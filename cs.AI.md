# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data](https://arxiv.org/abs/2402.08646) | 本研究提出了一个从数据中对各种类型的符号推理进行概率化描述的统一解释，并使用经典的推理关系、经验上的推理关系、最大一致集、最大可能集和最大似然估计来实现。这个理论为实现人类类似的机器智能提供了新的见解。 |
| [^2] | [A simple connection from loss flatness to compressed representations in neural networks.](http://arxiv.org/abs/2310.01770) | 该论文研究了深度神经网络中损失平坦性和神经表示压缩之间的关系，通过简单的数学关系，证明了损失平坦性与神经表示的压缩相关。 |

# 详细

[^1]: 从数据推理抽象的统一解释

    Inference of Abstraction for a Unified Account of Symbolic Reasoning from Data

    [https://arxiv.org/abs/2402.08646](https://arxiv.org/abs/2402.08646)

    本研究提出了一个从数据中对各种类型的符号推理进行概率化描述的统一解释，并使用经典的推理关系、经验上的推理关系、最大一致集、最大可能集和最大似然估计来实现。这个理论为实现人类类似的机器智能提供了新的见解。

    

    受到神经科学对贝叶斯方法在大脑功能方面的实证研究的启发，我们提供了一个统一的概率化解释，用于从数据中对各种类型的符号推理进行描述。我们使用经典的推理关系、经验上的推理关系、最大一致集、最大可能集和最大似然估计来对它们进行描述。这个理论为实现人类类似的机器智能提供了新的见解。

    Inspired by empirical work in neuroscience for Bayesian approaches to brain function, we give a unified probabilistic account of various types of symbolic reasoning from data. We characterise them in terms of formal logic using the classical consequence relation, an empirical consequence relation, maximal consistent sets, maximal possible sets and maximum likelihood estimation. The theory gives new insights into reasoning towards human-like machine intelligence.
    
[^2]: 损失平坦性与神经网络中压缩表示的简单联系

    A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])

    [http://arxiv.org/abs/2310.01770](http://arxiv.org/abs/2310.01770)

    该论文研究了深度神经网络中损失平坦性和神经表示压缩之间的关系，通过简单的数学关系，证明了损失平坦性与神经表示的压缩相关。

    

    对深度神经网络的泛化能力进行研究的方法有很多种，包括至少两种不同的方法：一种基于参数空间中损失景观的形状，另一种基于特征空间中表示流形的结构（即单位活动的空间）。这两种方法相关但很少同时进行研究和明确关联。在这里，我们提出了一种简单的分析方法来建立这种联系。我们展示了在深度神经网络学习的最后阶段，神经表示流形的体积压缩与正在进行的参数优化所探索的最小值周围的损失平坦性相关。我们证明了这可以由一个相对简单的数学关系来预测：损失平坦性意味着神经表示的压缩。我们的结果与\citet{ma_linear_2021}的先前研究密切相关，该研究展示了平坦性（即小特征值）与表示流形的体积压缩之间的关系。

    Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
    

