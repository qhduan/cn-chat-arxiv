# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Institutional Platform for Secure Self-Service Large Language Model Exploration](https://rss.arxiv.org/abs/2402.00913) | 这个论文介绍了一个用户友好型平台，旨在使大型定制语言模型更易于使用，通过最新的多LoRA推理技术和定制适配器，实现了数据隔离、加密和身份验证的安全服务。 |
| [^2] | [FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation](https://arxiv.org/abs/2403.20261) | FABind+通过改进口袋预测和姿态生成，提升分子对接表现 |
| [^3] | [One-Shot Domain Incremental Learning](https://arxiv.org/abs/2403.16707) | 提出了一种处理单次领域增量学习中批归一化层统计数据困难的技术，并展示了其有效性。 |
| [^4] | [Tur[k]ingBench: A Challenge Benchmark for Web Agents](https://arxiv.org/abs/2403.11905) | Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。 |
| [^5] | [KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents](https://arxiv.org/abs/2403.03101) | KnowAgent引入了显式动作知识，通过动作知识库和知识型自学习策略来增强LLM的规划能力，从而改善语言Agent的规划表现。 |
| [^6] | [BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection](https://arxiv.org/abs/2402.07536) | BreakGPT是第一个用于金融突破检测的大型语言模型，采用多阶段结构框架，提高了答案和理由的准确性。 |
| [^7] | [CIC: A framework for Culturally-aware Image Captioning](https://arxiv.org/abs/2402.05374) | CIC是一种面向文化感知图像字幕的框架，通过结合视觉问答和大型语言模型，它能够生成能描述图像中文化元素的详细字幕。 |
| [^8] | [MacGyver: Are Large Language Models Creative Problem Solvers?](https://arxiv.org/abs/2311.09682) | 通过创建MACGYVER数据集并与人类比较，研究发现大型语言模型在创意问题解决方面独具挑战性，在知识广度和可行性方面与人类存在独特差异，同时还展示了通过新的提示技术提升大型语言模型的问题解决能力潜力。 |
| [^9] | [Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering](https://arxiv.org/abs/2309.02233) | 该研究提出了一种名为LLMs增强医学教科书（LLM-AMT）的系统，通过插入式模块将权威医学教科书集成到LLMs的框架中，显著提高了LLMs在专业领域的能力。 |
| [^10] | [Cross-domain Random Pre-training with Prototypes for Reinforcement Learning](https://arxiv.org/abs/2302.05614) | 提出了CRPTpro框架，利用原型进行跨领域自监督随机预训练，提高预训练效率，并实现在不同领域中定义的视觉控制RL任务。 |
| [^11] | [Learning to Manipulate under Limited Information.](http://arxiv.org/abs/2401.16412) | 本研究通过训练神经网络在有限信息条件下学习如何利用不同投票方法进行操纵，发现某些投票方法在有限信息下容易被操纵，而其他方法不容易被操纵。 |
| [^12] | [From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces.](http://arxiv.org/abs/2310.19786) | 通过新的约化方法，我们改进了经典的Swap遗憾最小化算法，并提供了一个无外部遗憾算法的替代方法。对于学习专家建议问题，我们的算法可以在较少的轮数和更低的复杂度下达到相同的Swap遗憾限制。 |
| [^13] | [Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning.](http://arxiv.org/abs/2310.18209) | 提出了一种新颖的对比学习框架，用于学习高质量的图嵌入，并设计了对齐度量和均匀性度量来解决图领域的非欧几里德几何结构问题。 |
| [^14] | [ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery.](http://arxiv.org/abs/2309.17203) | ComSD提出了一种新方法，通过更合理的互信息估计和动态加权的内在奖励来平衡无监督技能发现中的行为质量和多样性。 |
| [^15] | [Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease.](http://arxiv.org/abs/2309.08916) | 本研究提出了一种双向图生成对抗网络（BGGAN），用于表示阿尔茨海默病（AD）的脑结构-功能连接。通过特殊设计的内部图卷积网络模块和平衡器模块，该方法能够准确地学习结构域和功能域之间的映射函数，并解决模式坍塌问题，同时学习结构和功能特征的互补性。 |
| [^16] | [Reinforcement Learning for Generative AI: A Survey.](http://arxiv.org/abs/2308.14328) | 该论文综述了强化学习在生成型人工智能中的应用。通过创建新的训练信号，强化学习展示了其从多个角度引入人类归纳偏好的强大和灵活性，以建立一个性能良好的模型。 |
| [^17] | [Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning.](http://arxiv.org/abs/2308.12219) | 本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。 |
| [^18] | [Empower Your Model with Longer and Better Context Comprehension.](http://arxiv.org/abs/2307.13365) | 本文研究了大语言模型（LLMs）内的信息传递，并提出了一种名为注意力转移的技术，该技术能够使模型在不增加训练或对生成流畅性的影响的情况下实现更长更好的上下文理解。 |
| [^19] | [Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning.](http://arxiv.org/abs/2307.04996) | 本文介绍了两种基于知识图谱的方法，一种使用强化学习，另一种使用XGBoost算法，用于个性化文章推荐。这些方法利用自动生成的知识图谱，并在一个大型跨国金融服务公司的客户中进行了实证研究。 |
| [^20] | [FedNoisy: Federated Noisy Label Learning Benchmark.](http://arxiv.org/abs/2306.11650) | FedNoisy是第一个标准化的联合噪声标签学习基准测试，并提供20个基本设置和标准化的仿真流程，以帮助研究人员探索联合学习中噪声标签的影响。 |
| [^21] | [How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?.](http://arxiv.org/abs/2305.06587) | 该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。 |

# 详细

[^1]: 用于安全自助大型语言模型探索的机构平台

    Institutional Platform for Secure Self-Service Large Language Model Exploration

    [https://rss.arxiv.org/abs/2402.00913](https://rss.arxiv.org/abs/2402.00913)

    这个论文介绍了一个用户友好型平台，旨在使大型定制语言模型更易于使用，通过最新的多LoRA推理技术和定制适配器，实现了数据隔离、加密和身份验证的安全服务。

    

    本文介绍了由肯塔基大学应用人工智能中心开发的用户友好型平台，旨在使大型定制语言模型（LLM）更易于使用。通过利用最近在多LoRA推理方面的进展，系统有效地适应了各类用户和项目的定制适配器。论文概述了系统的架构和关键特性，包括数据集策划、模型训练、安全推理和基于文本的特征提取。我们通过使用基于代理的方法建立了一个基于租户意识的计算网络，在安全地利用孤立资源岛的基础上形成了一个统一的系统。该平台致力于提供安全的LLM服务，强调过程和数据隔离、端到端加密以及基于角色的资源身份验证。该贡献与实现简化访问先进的AI模型和技术以支持科学发现的总体目标一致。

    This paper introduces a user-friendly platform developed by the University of Kentucky Center for Applied AI, designed to make large, customized language models (LLMs) more accessible. By capitalizing on recent advancements in multi-LoRA inference, the system efficiently accommodates custom adapters for a diverse range of users and projects. The paper outlines the system's architecture and key features, encompassing dataset curation, model training, secure inference, and text-based feature extraction.   We illustrate the establishment of a tenant-aware computational network using agent-based methods, securely utilizing islands of isolated resources as a unified system. The platform strives to deliver secure LLM services, emphasizing process and data isolation, end-to-end encryption, and role-based resource authentication. This contribution aligns with the overarching goal of enabling simplified access to cutting-edge AI models and technology in support of scientific discovery.
    
[^2]: FABind+: 通过改进口袋预测和姿态生成增强分子对接

    FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation

    [https://arxiv.org/abs/2403.20261](https://arxiv.org/abs/2403.20261)

    FABind+通过改进口袋预测和姿态生成，提升分子对接表现

    

    分子对接是药物发现中至关重要的过程。传统技术依赖于受物理原理支配的广泛采样和模拟，但这些方法往往速度慢且昂贵。基于深度学习的方法的出现显示出显著的前景，提供了精确性和效率的增长。建立在FABind的基础工作之上，这是一个专注于速度和准确性的模型，我们提出了FABind+，这是一个大大提升其前身性能的增强版。我们确定口袋预测是分子对接中的一个关键瓶颈，并提出了一种显著改进口袋预测的新方法，从而简化了对接过程。此外，我们对对接模块进行了修改，以增强其姿态生成能力。为了缩小与传统采样/生成方法之间的差距，我们结合了一个简单而有效的s

    arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
    
[^3]: 单次领域增量学习

    One-Shot Domain Incremental Learning

    [https://arxiv.org/abs/2403.16707](https://arxiv.org/abs/2403.16707)

    提出了一种处理单次领域增量学习中批归一化层统计数据困难的技术，并展示了其有效性。

    

    在以前关于用于分类的深度神经网络模型的研究中已经讨论了领域增量学习（DIL）。在DIL中，我们假设随着时间的推移观察新领域上的样本。模型必须对所有领域上的输入进行分类。然而，在实践中，我们可能会遇到这样一种情况，即我们需要在新领域的样本仅间歇性地被观察的约束下执行DIL。因此，在本研究中，我们考虑了一个极端情况，即我们只有一份来自新领域的样本，我们称之为单次DIL。我们首先经验性地表明现有的DIL方法在单次DIL中表现不佳。通过各种调查，我们分析了这种失败的原因。根据我们的分析，我们明确了单次DIL的困难是由批归一化层中的统计数据引起的。因此，我们提出了一种关于这些统计数据的技术，并展示了我们技术的有效性。

    arXiv:2403.16707v1 Announce Type: cross  Abstract: Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our tech
    
[^4]: Tur[k]ingBench：用于网络代理的挑战基准测试

    Tur[k]ingBench: A Challenge Benchmark for Web Agents

    [https://arxiv.org/abs/2403.11905](https://arxiv.org/abs/2403.11905)

    Tur[k]ingBench是一个挑战性的网络代理基准测试，用于评估最先进的多模态模型在处理包含文本指示和多模态上下文的复杂任务时的泛化能力。

    

    最近的聊天机器人展示了在原始文本形式下理解和交流的令人印象深刻的能力。然而，世界上不仅仅是原始文本。例如，人们在网页上花费大量时间，在这些网页上，文本与其他形式交织在一起，并以各种复杂互动的形式完成任务。最先进的多模型是否能够推广到这种复杂的领域呢？为了回答这个问题，我们介绍了TurkingBench，一个由包含多模态背景的文本说明制定的任务基准。与现有的使用人工合成的网页的工作不同，这里我们使用最初设计用于各种注释目的的自然HTML页面。每个任务的HTML说明也被实例化为各种值（从众包任务获得）以形成任务的新实例。这个基准包含32.2K个实例。

    arXiv:2403.11905v1 Announce Type: new  Abstract: Recent chatbots have demonstrated impressive ability to understand and communicate in raw-text form. However, there is more to the world than raw text. For example, humans spend long hours of their time on web pages, where text is intertwined with other modalities and tasks are accomplished in the form of various complex interactions. Can state-of-the-art multi-modal models generalize to such complex domains?   To address this question, we introduce TurkingBench, a benchmark of tasks formulated as web pages containing textual instructions with multi-modal context. Unlike existing work which employs artificially synthesized web pages, here we use natural HTML pages that were originally designed for crowdsourcing workers for various annotation purposes. The HTML instructions of each task are also instantiated with various values (obtained from the crowdsourcing tasks) to form new instances of the task. This benchmark contains 32.2K instanc
    
[^5]: KnowAgent: 知识增强规划用于基于LLM的Agent

    KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents

    [https://arxiv.org/abs/2403.03101](https://arxiv.org/abs/2403.03101)

    KnowAgent引入了显式动作知识，通过动作知识库和知识型自学习策略来增强LLM的规划能力，从而改善语言Agent的规划表现。

    

    大型语言模型(LLMs)在复杂推理任务中表现出巨大潜力，但在处理更复杂的挑战时仍有所不足，特别是与环境互动通过生成可执行动作时。这种不足主要来自于语言Agent中缺乏内置动作知识，导致在任务求解过程中无法有效引导规划轨迹，从而导致规划幻觉。为了解决这个问题，我们引入了KnowAgent，一种旨在通过整合显式动作知识来增强LLM规划能力的新方法。具体而言，KnowAgent采用了一个动作知识库和一个知识型自学习策略来限制规划过程中的行动路径，实现更合理的轨迹合成，进而提高语言Agent的计划性能。基于HotpotQA和ALFWorld的实验结果基于不同的主干模型。

    arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
    
[^6]: BreakGPT: 一种具有多阶段结构的大型语言模型用于金融突破检测

    BreakGPT: A Large Language Model with Multi-stage Structure for Financial Breakout Detection

    [https://arxiv.org/abs/2402.07536](https://arxiv.org/abs/2402.07536)

    BreakGPT是第一个用于金融突破检测的大型语言模型，采用多阶段结构框架，提高了答案和理由的准确性。

    

    交易区间突破（TRB）是金融交易技术分析中的一种关键方法，广泛应用于股票、期货和外汇等金融市场的交易者。然而，区分真假突破并提供正确的理由对投资者来说具有重要挑战。最近，大型语言模型在各种下游应用中取得了成功，但在金融突破检测领域的效果仍不理想。原因在于突破检测需要独特的数据和特定的知识。为了解决这些问题，我们引入了BreakGPT，这是第一个用于金融突破检测的大型语言模型。此外，我们还开发了一种名为多阶段结构的新颖框架，有效地减少了下游应用中的错误。实验结果表明，与GPT-3.5相比，BreakGPT的答案和理由的准确性提高了44%，有助于金融突破检测。

    Trading range breakout (TRB) is a key method in the technical analysis of financial trading, widely employed by traders in financial markets such as stocks, futures, and foreign exchange. However, distinguishing between true and false breakout and providing the correct rationale cause significant challenges to investors. Recently, large language models have achieved success in various downstream applications, but their effectiveness in the domain of financial breakout detection has been subpar. The reason is that the unique data and specific knowledge are required in breakout detection. To address these issues, we introduce BreakGPT, the first large language model for financial breakout detection. Furthermore, we have developed a novel framework for large language models, namely multi-stage structure, effectively reducing mistakes in downstream applications. Experimental results indicate that compared to GPT-3.5, BreakGPT improves the accuracy of answers and rational by 44%, with the m
    
[^7]: CIC：一种面向文化感知图像字幕的框架

    CIC: A framework for Culturally-aware Image Captioning

    [https://arxiv.org/abs/2402.05374](https://arxiv.org/abs/2402.05374)

    CIC是一种面向文化感知图像字幕的框架，通过结合视觉问答和大型语言模型，它能够生成能描述图像中文化元素的详细字幕。

    

    图像字幕通过使用视觉-语言预训练模型（VLPs）如BLIP从图像生成描述性句子，这种方法已经取得了很大的改进。然而，当前的方法缺乏对图像中所描绘的文化元素（例如亚洲文化群体的传统服装）生成详细描述性字幕的能力。在本文中，我们提出了一种新的框架，\textbf{面向文化感知图像字幕（CIC）}，该框架能够从代表不同文化的图像中生成字幕并描述文化元素。受到将视觉模态和大型语言模型（LLMs）通过适当的提示进行组合的方法的启发，我们的框架（1）根据图像中的文化类别生成问题，（2）利用生成的问题从视觉问答（VQA）中提取文化视觉元素，（3）使用带有提示的LLMs生成文化感知字幕。我们在4个不同大学的45名参与者上进行了人工评估。

    Image Captioning generates descriptive sentences from images using Vision-Language Pre-trained models (VLPs) such as BLIP, which has improved greatly. However, current methods lack the generation of detailed descriptive captions for the cultural elements depicted in the images, such as the traditional clothing worn by people from Asian cultural groups. In this paper, we propose a new framework, \textbf{Culturally-aware Image Captioning (CIC)}, that generates captions and describes cultural elements extracted from cultural visual elements in images representing cultures. Inspired by methods combining visual modality and Large Language Models (LLMs) through appropriate prompts, our framework (1) generates questions based on cultural categories from images, (2) extracts cultural visual elements from Visual Question Answering (VQA) using generated questions, and (3) generates culturally-aware captions using LLMs with the prompts. Our human evaluation conducted on 45 participants from 4 dif
    
[^8]: MacGyver：大型语言模型是否是创意问题解决者？

    MacGyver: Are Large Language Models Creative Problem Solvers?

    [https://arxiv.org/abs/2311.09682](https://arxiv.org/abs/2311.09682)

    通过创建MACGYVER数据集并与人类比较，研究发现大型语言模型在创意问题解决方面独具挑战性，在知识广度和可行性方面与人类存在独特差异，同时还展示了通过新的提示技术提升大型语言模型的问题解决能力潜力。

    

    我们在一个全新的约束设置中探究了现代大型语言模型的创意问题解决能力。为此，我们创建了MACGYVER，这是一个自动生成的数据集，包含超过1600个特意设计的现实世界问题，旨在引发物体的创新使用，并需要超越常规思维。我们随后向大型语言模型和人类展示我们的数据集，以比较和对比它们的问题解决能力。MACGYVER对这两个群体都具有挑战性，但以独特和互补的方式呈现。例如，人类擅长熟悉的任务，但在特定领域知识上有困难，导致更高的差异。相比之下，大型语言模型暴露于各种专业知识，尝试更广泛的问题，但在提出物理上不可行的行动时失败。最后，我们对大型语言模型进行了详细的错误分析，并展示了通过新的提示技术提高它们的问题解决能力的潜力。

    arXiv:2311.09682v2 Announce Type: replace-cross  Abstract: We explore the creative problem-solving capabilities of modern LLMs in a novel constrained setting. To this end, we create MACGYVER, an automatically generated dataset consisting of over 1,600 real-world problems deliberately designed to trigger innovative usage of objects and necessitate out-of-the-box thinking. We then present our collection to both LLMs and humans to compare and contrast their problem-solving abilities. MACGYVER is challenging for both groups, but in unique and complementary ways. For instance, humans excel in tasks they are familiar with but struggle with domain-specific knowledge, leading to a higher variance. In contrast, LLMs, exposed to a variety of specialized knowledge, attempt broader problems but fail by proposing physically-infeasible actions. Finally, we provide a detailed error analysis of LLMs, and demonstrate the potential of enhancing their problem-solving ability with novel prompting techniqu
    
[^9]: 用医学教科书增强黑盒LLMs进行临床问题回答

    Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering

    [https://arxiv.org/abs/2309.02233](https://arxiv.org/abs/2309.02233)

    该研究提出了一种名为LLMs增强医学教科书（LLM-AMT）的系统，通过插入式模块将权威医学教科书集成到LLMs的框架中，显著提高了LLMs在专业领域的能力。

    

    大规模语言模型（LLMs）如ChatGPT已经展示出根据人类指令生成响应的印象能力。然而，由于它们缺乏特定、深入的知识，它们在医学领域的应用可能具有挑战性。在这项研究中，我们提出了一种名为LLMs增强医学教科书（LLM-AMT）的系统，旨在增强LLMs在专业领域的能力。LLM-AMT通过插入式模块将权威医学教科书集成到LLMs的框架中。这些模块包括一个查询增强器、一个混合教科书检索器和一个知识自我完善。它们共同整合权威医学知识。此外，一个LLMs阅读器有助于上下文理解。我们在三个医学问答任务上的实验结果表明，LLMAMT显著提高了响应质量，准确率提高了11.6%到16.6%。值得注意的是，以GPT-4-Turbo为基础模型

    arXiv:2309.02233v2 Announce Type: replace-cross  Abstract: Large-scale language models (LLMs) like ChatGPT have demonstrated impressive abilities in generating responses based on human instructions. However, their use in the medical field can be challenging due to their lack of specific, in-depth knowledge. In this study, we present a system called LLMs Augmented with Medical Textbooks (LLM-AMT) designed to enhance the proficiency of LLMs in specialized domains. LLM-AMT integrates authoritative medical textbooks into the LLMs' framework using plug-and-play modules. These modules include a Query Augmenter, a Hybrid Textbook Retriever, and a Knowledge Self-Refiner. Together, they incorporate authoritative medical knowledge. Additionally, an LLM Reader aids in contextual understanding. Our experimental results on three medical QA tasks demonstrate that LLMAMT significantly improves response quality, with accuracy gains ranging from 11.6% to 16.6%. Notably, with GPT-4-Turbo as the base mod
    
[^10]: 具有原型的跨领域随机预训练用于强化学习

    Cross-domain Random Pre-training with Prototypes for Reinforcement Learning

    [https://arxiv.org/abs/2302.05614](https://arxiv.org/abs/2302.05614)

    提出了CRPTpro框架，利用原型进行跨领域自监督随机预训练，提高预训练效率，并实现在不同领域中定义的视觉控制RL任务。

    

    此工作已提交给IEEE进行可能的出版。 CRPTpro提出了一种用于基于图像的RL的跨领域自监督随机预训练框架，利用原型。 CRPTpro采用了跨领域随机策略，可以轻松快速地从多个领域中抽样多样化数据，以提高预训练效率。此外，通过提出一种新颖的内在损失进行原型表示学习，以在不同领域中预训练有效且通用的编码器。在没有微调的情况下，跨领域编码器可以高效地应用于不同领域中定义的具有挑战性的下游视觉控制RL任务。 与以前的方法如APT和Proto-RL相比，CRP

    arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
    
[^11]: 学习在有限信息下进行操纵

    Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])

    [http://arxiv.org/abs/2401.16412](http://arxiv.org/abs/2401.16412)

    本研究通过训练神经网络在有限信息条件下学习如何利用不同投票方法进行操纵，发现某些投票方法在有限信息下容易被操纵，而其他方法不容易被操纵。

    

    根据社会选择理论的经典结果，任何合理的偏好投票方法有时会给个体提供报告不真实偏好的激励。对于比较投票方法来说，不同投票方法在多大程度上更或者更少抵抗这种策略性操纵已成为一个关键考虑因素。在这里，我们通过神经网络在不同规模下对限制信息下学习如何利用给定投票方法进行操纵的成功程度来衡量操纵的抵抗力。我们训练了将近40,000个不同规模的神经网络来对抗8种不同的投票方法，在6种限制信息情况下，进行包含5-21名选民和3-6名候选人的委员会规模选举的操纵。我们发现，一些投票方法，如Borda方法，在有限信息下可以被神经网络高度操纵，而其他方法，如Instant Runoff方法，虽然被一个理想的操纵者利润化操纵，但在有限信息下不会受到操纵。

    By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
    
[^12]: 从外部到Swap遗憾2.0：针对大动作空间的高效约化和无知对手

    From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2310.19786](http://arxiv.org/abs/2310.19786)

    通过新的约化方法，我们改进了经典的Swap遗憾最小化算法，并提供了一个无外部遗憾算法的替代方法。对于学习专家建议问题，我们的算法可以在较少的轮数和更低的复杂度下达到相同的Swap遗憾限制。

    

    我们提供了一种新颖的从Swap遗憾最小化到外部遗憾最小化的约化方法，改进了Blum-Mansour和Stolz-Lugosi的经典约化方法，不需要行为空间的有限性。我们证明，只要存在某个假设类的无外部遗憾算法，就必然存在相同类别的无Swap遗憾算法。对于学习专家建议问题，我们的结果意味着可以保证在$\log(N)^{O(1/\epsilon)}$轮后，每次迭代复杂度为$O(N)$的情况下，Swap遗憾被限定为$\epsilon$，而Blum-Mansour和Stolz-Lugosi的经典约化方法需要$O(N/\epsilon^2)$轮和至少$\Omega(N^2)$的每次迭代复杂度。我们的结果伴随着一个相关的下界，与[BM07]不同，这个下界适用于无知和$\ell_1$-受限的对手和可以利用这个下界的学习者。

    We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
    
[^13]: 超卷曲图对比学习的对齐和外壳同构性

    Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning. (arXiv:2310.18209v1 [cs.LG])

    [http://arxiv.org/abs/2310.18209](http://arxiv.org/abs/2310.18209)

    提出了一种新颖的对比学习框架，用于学习高质量的图嵌入，并设计了对齐度量和均匀性度量来解决图领域的非欧几里德几何结构问题。

    

    学习对下游任务有益的自监督图表示是具有挑战性的。在各种方法中，对比学习具有竞争力的性能。对比学习的嵌入被排列在一个超球面上，从而使得在欧几里德空间中的余弦距离测量成为可能。然而，许多领域的潜在几何结构，如图形，展现了高度非欧几里德的特性。为此，我们提出了一种新颖的对比学习框架，用于学习高质量的图嵌入。具体而言，我们设计了一种有效捕捉层次数据不变性信息的对齐度量，同时我们提出了一种替代均匀度量来防止所谓的维度塌陷。我们证明，在双曲空间中，必须解决与树的属性相关的叶子和高度层面的均匀性，而在双曲流形的环境空间中，这些概念转化为对同构性的施加。

    Learning good self-supervised graph representations that are beneficial to downstream tasks is challenging. Among a variety of methods, contrastive learning enjoys competitive performance. The embeddings of contrastive learning are arranged on a hypersphere that enables the Cosine distance measurement in the Euclidean space. However, the underlying structure of many domains such as graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a novel contrastive learning framework to learn high-quality graph embedding. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity which are related to properties of trees, whereas in the ambient space of the hyperbolic manifold, these notions translate into imposing an isotro
    
[^14]: ComSD: 在无监督技能发现中平衡行为质量和多样性

    ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])

    [http://arxiv.org/abs/2309.17203](http://arxiv.org/abs/2309.17203)

    ComSD提出了一种新方法，通过更合理的互信息估计和动态加权的内在奖励来平衡无监督技能发现中的行为质量和多样性。

    

    无监督技能发现的理想方法能够在没有外部奖励的情况下产生多样且合格的技能，同时发现的技能集能够以各种方式高效地适应下游任务。本文提出了Contrastive multi-objectives Skill Discovery (ComSD)，通过更合理的互信息估计和动态加权的内在奖励来减轻发现的行为在质量和多样性之间的冲突。

    Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
    
[^15]: 双向图生成对抗网络：用于阿尔茨海默病的脑结构-功能连接的表示

    Bidirectional Graph GAN: Representing Brain Structure-Function Connections for Alzheimer's Disease. (arXiv:2309.08916v1 [cs.AI])

    [http://arxiv.org/abs/2309.08916](http://arxiv.org/abs/2309.08916)

    本研究提出了一种双向图生成对抗网络（BGGAN），用于表示阿尔茨海默病（AD）的脑结构-功能连接。通过特殊设计的内部图卷积网络模块和平衡器模块，该方法能够准确地学习结构域和功能域之间的映射函数，并解决模式坍塌问题，同时学习结构和功能特征的互补性。

    

    揭示脑疾病的发病机制，包括阿尔茨海默病（AD），脑结构与功能之间的关系至关重要。然而，由于各种原因，将脑结构-功能连接映射是一个巨大的挑战。本文提出了一种双向图生成对抗网络（BGGAN）来表示脑结构-功能连接。具体来说，通过设计一个内部图卷积网络（InnerGCN）模块，BGGAN的生成器可以利用直接和间接脑区域的特征来学习结构域和功能域之间的映射函数。此外，还设计了一个名为Balancer的新模块来平衡生成器和判别器之间的优化。通过将Balancer引入到BGGAN中，结构生成器和功能生成器不仅可以缓解模式坍塌问题，还可以学习结构和功能特征的互补性。实验结果表明该方法能够在AD中准确地表示脑结构-功能连接。

    The relationship between brain structure and function is critical for revealing the pathogenesis of brain disease, including Alzheimer's disease (AD). However, it is a great challenge to map brain structure-function connections due to various reasons. In this work, a bidirectional graph generative adversarial networks (BGGAN) is proposed to represent brain structure-function connections. Specifically, by designing a module incorporating inner graph convolution network (InnerGCN), the generators of BGGAN can employ features of direct and indirect brain regions to learn the mapping function between structural domain and functional domain. Besides, a new module named Balancer is designed to counterpoise the optimization between generators and discriminators. By introducing the Balancer into BGGAN, both the structural generator and functional generator can not only alleviate the issue of mode collapse but also learn complementarity of structural and functional features. Experimental result
    
[^16]: 强化学习在生成型人工智能中的应用：综述

    Reinforcement Learning for Generative AI: A Survey. (arXiv:2308.14328v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2308.14328](http://arxiv.org/abs/2308.14328)

    该论文综述了强化学习在生成型人工智能中的应用。通过创建新的训练信号，强化学习展示了其从多个角度引入人类归纳偏好的强大和灵活性，以建立一个性能良好的模型。

    

    深度生成型人工智能是机器学习界中一个长期存在的重要主题，可以影响到诸多应用领域，如文本生成和计算机视觉。训练生成模型的主要范式是最大似然估计，通过减小模型分布和目标分布之间的差异来推动学习器捕捉并逼近目标数据分布。这种公式成功地建立了生成任务的目标，然而却无法满足使用者对生成模型的所有需求。强化学习作为一种竞争性选择，通过创建新的目标来注入新的训练信号，展示了它的强大和灵活性，可以从多个角度引入人类归纳偏好，如对抗学习、手动设计规则和学习奖励模型，以建立一个性能良好的模型。

    Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning ha
    
[^17]: 扩展性和指导调优的扩散语言模型能够完成多种任务

    Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])

    [http://arxiv.org/abs/2308.12219](http://arxiv.org/abs/2308.12219)

    本文研究表明，通过扩展扩散语言模型的数据、规模和任务，可以有效使其成为强大的语言学习者。实验证明，扩展扩散语言模型在解决通用语言任务方面能够持续提高性能。

    

    最近生成式人工智能的兴起得益于扩散概率模型的生成能力和大规模语言模型的可扩展性。尽管具有潜力，但扩散语言模型是否能够解决与自回归模型相媲美的通用语言任务仍然不明确。本文证明了在数据、规模和任务方面扩展扩散模型能够有效使其成为强大的语言学习者。我们通过先通过掩码语言建模预训练从大规模数据中获取知识，再通过扩散适应将预训练的掩码语言模型改进为扩散语言模型，通过任务特定的微调和指导调优来发掘其在解决通用语言任务方面的多样性。实验证明，扩展扩散语言模型能够在下游语言任务中持续提高性能。

    The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
    
[^18]: 用更长更好的上下文理解将模型赋能

    Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v1 [cs.CL])

    [http://arxiv.org/abs/2307.13365](http://arxiv.org/abs/2307.13365)

    本文研究了大语言模型（LLMs）内的信息传递，并提出了一种名为注意力转移的技术，该技术能够使模型在不增加训练或对生成流畅性的影响的情况下实现更长更好的上下文理解。

    

    最近，随着大量的大语言模型（LLMs）的出现，人工智能的实现进入了一个新的时代。无论这些模型自身的容量和结构如何，都存在对LLMs具有更长更复杂上下文的增强理解的需求，而模型通常在处理超出其理解能力范围的句子序列时会遇到上限，导致产生离题或混乱的回答。虽然最近有几项工作试图以不同的方式解决这个问题，但它们很少关注“为什么模型无法自行弥补或增强自己的能力”。在本文中，我们对LLMs内的信息传递性质进行了深入研究，并提出了一种名为注意力转移的新技术。这种技术能够使模型在最小化额外训练或对生成流利性的影响的情况下实现更长更好的上下文理解。我们的实验证明了这一点。

    Recently, with the emergence of numerous Large Language Models (LLMs), the implementation of AI has entered a new era. Irrespective of these models' own capacity and structure, there is a growing demand for LLMs to possess enhanced comprehension of longer and more complex contexts with relatively smaller sizes. Models often encounter an upper limit when processing sequences of sentences that extend beyond their comprehension capacity and result in off-topic or even chaotic responses. While several recent works attempt to address this issue in various ways, they rarely focus on "why models are unable to compensate or strengthen their capabilities on their own". In this paper, we thoroughly investigate the nature of information transfer within LLMs and propose a novel technique called Attention Transition. This technique empowers models to achieve longer and better context comprehension with minimal additional training or impact on generation fluency. Our experiments are conducted in XSu
    
[^19]: 利用自动生成的知识图谱和强化学习增强推荐系统

    Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning. (arXiv:2307.04996v1 [cs.IR])

    [http://arxiv.org/abs/2307.04996](http://arxiv.org/abs/2307.04996)

    本文介绍了两种基于知识图谱的方法，一种使用强化学习，另一种使用XGBoost算法，用于个性化文章推荐。这些方法利用自动生成的知识图谱，并在一个大型跨国金融服务公司的客户中进行了实证研究。

    

    个性化推荐在直接营销中越来越重要，激发了通过知识图谱（KG）应用来提升客户体验的研究动机。例如，在金融服务领域，公司可以通过向客户提供相关金融文章来培养关系，促进客户参与和促进知情的金融决策。尽管一些方法专注于基于KG的推荐系统以改进内容，但在本研究中，我们专注于可解释的基于KG的推荐系统来进行决策。为此，我们提出了两种基于知识图谱的个性化文章推荐方法，用于一家大型跨国金融服务公司的一组客户。第一种方法使用强化学习，第二种方法使用XGBoost算法来向客户推荐文章。这两种方法都利用从结构化（表格数据）和非结构化数据（大量文本数据）生成的KG。

    Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications. For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions. While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company. The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers. Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body o
    
[^20]: FedNoisy: 分布式噪声标签学习基准测试

    FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2306.11650](http://arxiv.org/abs/2306.11650)

    FedNoisy是第一个标准化的联合噪声标签学习基准测试，并提供20个基本设置和标准化的仿真流程，以帮助研究人员探索联合学习中噪声标签的影响。

    

    联合学习已经因为无需对来自客户端的敏感数据进行聚合而变得受欢迎。但是，数据隔离的分布式和孤立性可能会受到数据质量的复杂性的影响，使其更容易受到噪声标签的干扰。许多努力都致力于在集中式或联合式环境中防御噪声标签的负面影响。然而，缺乏一个全面考虑各种典型联合学习场景中噪声标签影响的基准测试。在这项工作中，我们提供了第一个标准化的基准测试，可以帮助研究人员充分探索潜在的联合噪声设置。此外，我们进行了全面的实验，探索这些数据设置的特性，并揭示了联合学习中的挑战性场景，这可能指导未来的方法开发。我们强调我们基准测试中提出的20个基本设置，适用于5个以上的数据集，并提供了标准化的仿真流程。

    Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
    
[^21]: 面向时间序列预测的谱时图神经网络的表达能力研究

    How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])

    [http://arxiv.org/abs/2305.06587](http://arxiv.org/abs/2305.06587)

    该论文研究了谱时图神经网络的表达能力，并揭示了其具有线性谱时GNN是普适的、表现力受到离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。同时，论文提出了一个简单实例TGC，其在时间序列预测方面具有显著的性能优势。

    

    谱时图神经网络是大多数基于图神经网络(GNN)的时间序列预测模型的一个有前途的抽象。然而，我们需要更多关于这种方法的基础知识。本文建立了一个理论框架，揭示了谱时GNN的表现力。我们的结果表明，具有线性谱时GNN是普适的，在温和的假设下，它们的表现力受到我们的离散时间动态图扩展的第一阶Weisfeiler-Leman算法的限制。为了使我们的发现在实践中有用，我们详细讨论了相关限制，并概述了在谱域中设计空间和时间模块的理论蓝图。基于这些见解，并为了展示基于我们的框架，谱时GNN有多么强大，我们提出了一个名为 Temporal Graph GegenConv (TGC) 的简单实例，显著优于大多数已有的模型。

    Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
    

