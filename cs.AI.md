# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Uncertainty Quantification for Gradient-based Explanations in Neural Networks](https://arxiv.org/abs/2403.17224) | 本文提出了一种结合不确定性估计方法和解释方法来确定神经网络解释不确定性的流程，通过计算解释分布的变异系数，评估了解释的置信度并确定Guided Backpropagation方法生成的解释具有较低的不确定性。 |
| [^2] | [MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness.](http://arxiv.org/abs/2312.04960) | MIMIR提出了一种新颖的防御方法，通过在预训练中利用遮罩图像建模，构建了一个不同的对抗性训练方法。该方法旨在增强Vision Transformers（ViTs）对抗攻击的鲁棒性。 |
| [^3] | [LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving.](http://arxiv.org/abs/2310.03026) | 本文研究将大型语言模型（LLMs）作为复杂自动驾驶场景的决策组件，通过认知路径和算法来实现全面推理和可执行驾驶指令的转化。实验证明，LLMs能够在单车任务和复杂驾驶行为中表现出优越性能，这是因为其具有常识推理能力。 |
| [^4] | [Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss.](http://arxiv.org/abs/2309.06774) | 本文揭示了基于Hinge Loss训练的深度学习二分类器的基本测试性能限制。 |
| [^5] | [Temporal Difference Learning with Experience Replay.](http://arxiv.org/abs/2306.09746) | 本文提出具有经验回放的TD学习，在马尔科夫观测模型下，通过对噪声项的分解，提供了有限时间误差界限，可以通过调整回放缓冲区和小批量的大小来控制误差。 |
| [^6] | [Experiential Explanations for Reinforcement Learning.](http://arxiv.org/abs/2210.04723) | 该论文提出了一种经验解释技术，通过训练影响预测器来恢复强化学习系统中的信息，使得非AI专家能够更好地理解其决策过程。 |

# 详细

[^1]: 神经网络中基于梯度的解释的不确定性量化

    Uncertainty Quantification for Gradient-based Explanations in Neural Networks

    [https://arxiv.org/abs/2403.17224](https://arxiv.org/abs/2403.17224)

    本文提出了一种结合不确定性估计方法和解释方法来确定神经网络解释不确定性的流程，通过计算解释分布的变异系数，评估了解释的置信度并确定Guided Backpropagation方法生成的解释具有较低的不确定性。

    

    解释方法有助于理解模型预测的原因。这些方法越来越多地参与模型调试、性能优化，并获得对模型工作原理的洞见。鉴于这些方法的关键应用，衡量这些方法生成的解释的不确定性是至关重要的。在本文中，我们提出了一种结合不确定性估计方法和解释方法来确定神经网络解释不确定性的流程。我们利用这个流程为CIFAR-10、FER+和California Housing数据集生成解释分布。通过计算这些分布的变异系数，我们评估了解释的置信度，并确定使用引导反向传播生成的解释与低不确定性相关。此外，我们计算了修改的像素插入/删除度量来评价……

    arXiv:2403.17224v1 Announce Type: cross  Abstract: Explanation methods help understand the reasons for a model's prediction. These methods are increasingly involved in model debugging, performance optimization, and gaining insights into the workings of a model. With such critical applications of these methods, it is imperative to measure the uncertainty associated with the explanations generated by these methods. In this paper, we propose a pipeline to ascertain the explanation uncertainty of neural networks by combining uncertainty estimation methods and explanation methods. We use this pipeline to produce explanation distributions for the CIFAR-10, FER+, and California Housing datasets. By computing the coefficient of variation of these distributions, we evaluate the confidence in the explanation and determine that the explanations generated using Guided Backpropagation have low uncertainty associated with them. Additionally, we compute modified pixel insertion/deletion metrics to ev
    
[^2]: MIMIR: 基于互信息的对抗鲁棒性的遮罩图像建模

    MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v2 [cs.CV] UPDATED)

    [http://arxiv.org/abs/2312.04960](http://arxiv.org/abs/2312.04960)

    MIMIR提出了一种新颖的防御方法，通过在预训练中利用遮罩图像建模，构建了一个不同的对抗性训练方法。该方法旨在增强Vision Transformers（ViTs）对抗攻击的鲁棒性。

    

    视觉变压器（ViTs）相对于卷积神经网络（CNNs）在各种任务上实现了卓越的性能，但ViTs也容易受到对抗性攻击。对抗性训练是建立强大的CNN模型的最成功方法之一。因此，最近的研究探索了基于ViTs和CNNs之间的差异的对抗性训练的新方法，如更好的训练策略，防止注意力集中在单个块上，或丢弃低注意力的嵌入。然而，这些方法仍然遵循传统监督对抗训练的设计，限制了对ViTs的对抗训练的潜力。本文提出了一种新颖的防御方法MIMIR，旨在通过利用预训练中的遮罩图像建模构建不同的对抗性训练方法。我们创建了一个自编码器，它接受对抗性例子作为输入，但将干净的例子作为建模目标。然后，我们创建了一个互信息（MI）

    Vision Transformers (ViTs) achieve superior performance on various tasks compared to convolutional neural networks (CNNs), but ViTs are also vulnerable to adversarial attacks. Adversarial training is one of the most successful methods to build robust CNN models. Thus, recent works explored new methodologies for adversarial training of ViTs based on the differences between ViTs and CNNs, such as better training strategies, preventing attention from focusing on a single block, or discarding low-attention embeddings. However, these methods still follow the design of traditional supervised adversarial training, limiting the potential of adversarial training on ViTs. This paper proposes a novel defense method, MIMIR, which aims to build a different adversarial training methodology by utilizing Masked Image Modeling at pre-training. We create an autoencoder that accepts adversarial examples as input but takes the clean examples as the modeling target. Then, we create a mutual information (MI
    
[^3]: LanguageMPC：基于大型语言模型的自动驾驶决策者

    LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving. (arXiv:2310.03026v2 [cs.RO] UPDATED)

    [http://arxiv.org/abs/2310.03026](http://arxiv.org/abs/2310.03026)

    本文研究将大型语言模型（LLMs）作为复杂自动驾驶场景的决策组件，通过认知路径和算法来实现全面推理和可执行驾驶指令的转化。实验证明，LLMs能够在单车任务和复杂驾驶行为中表现出优越性能，这是因为其具有常识推理能力。

    

    现有基于学习的自动驾驶系统在理解高级信息、推广罕见事件和提供可解释性方面面临挑战。为解决这些问题，本研究将大型语言模型（LLMs）作为复杂自动驾驶场景的决策组件，需要人类常识理解。我们设计了认知路径，使LLMs能够进行全面推理，并开发了将LLM决策转化为可执行驾驶指令的算法。通过这种方式，LLM决策通过引导参数矩阵适应与低级控制器无缝集成。大量实验表明，我们提出的方法不仅在单车任务中始终超越基线方法，而且还能处理复杂的驾驶行为，甚至多车协调，这要归功于LLMs的常识推理能力。本文介绍了将LLMs作为有效决策者的初步步骤。

    Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-make
    
[^4]: 基于Hinge Loss训练的深度学习二分类器的基本限制

    Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss. (arXiv:2309.06774v1 [cs.LG])

    [http://arxiv.org/abs/2309.06774](http://arxiv.org/abs/2309.06774)

    本文揭示了基于Hinge Loss训练的深度学习二分类器的基本测试性能限制。

    

    深度学习在化学、计算机科学、电子工程、数学、医学、神经科学和物理学等多个领域取得了重大突破，但对于为什么和如何获得经验成功的全面理解仍然基本难以把握。为了解决这一根本问题并揭示深度学习背后的奥秘，已经在建立统一理论的方向上取得了重大创新。这些创新包括优化、泛化和近似等基础性进展。然而，迄今为止还没有一个工作提供了一种方法来量化深度学习算法在解决模式分类问题时的测试性能。为了在一定程度上克服这个基本挑战，本文揭示了基于Hinge Loss训练的深度学习二分类器的基本测试性能限制。

    Although deep learning (DL) has led to several breakthroughs in many disciplines as diverse as chemistry, computer science, electrical engineering, mathematics, medicine, neuroscience, and physics, a comprehensive understanding of why and how DL is empirically successful remains fundamentally elusive. To attack this fundamental problem and unravel the mysteries behind DL's empirical successes, significant innovations toward a unified theory of DL have been made. These innovations encompass nearly fundamental advances in optimization, generalization, and approximation. Despite these advances, however, no work to date has offered a way to quantify the testing performance of a DL-based algorithm employed to solve a pattern classification problem. To overcome this fundamental challenge in part, this paper exposes the fundamental testing performance limits of DL-based binary classifiers trained with hinge loss. For binary classifiers that are based on deep rectified linear unit (ReLU) feedf
    
[^5]: 《具有经验回放的时序差分学习》

    Temporal Difference Learning with Experience Replay. (arXiv:2306.09746v1 [cs.LG])

    [http://arxiv.org/abs/2306.09746](http://arxiv.org/abs/2306.09746)

    本文提出具有经验回放的TD学习，在马尔科夫观测模型下，通过对噪声项的分解，提供了有限时间误差界限，可以通过调整回放缓冲区和小批量的大小来控制误差。

    

    时序差分学习被普遍认为是强化学习领域中最受欢迎的算法之一。本文研究了其有限时间行为，包括均方误差和样本复杂度的有限时间界限。在经验方面，经验回放是深度强化学习算法成功的关键因素之一，但其在强化学习中的理论效应尚未被完全理解。本文提出了马尔科夫噪声项的简单分解，并为具有经验回放的TD学习提供了有限时间误差界限。具体而言，在马尔科夫观测模型下，我们证明了对于平均迭代和最终迭代情况下，常数步长引起的误差术语可以通过回放缓冲区的大小和从经验回放缓冲区中抽样的小批量来有效控制。

    Temporal-difference (TD) learning is widely regarded as one of the most popular algorithms in reinforcement learning (RL). Despite its widespread use, it has only been recently that researchers have begun to actively study its finite time behavior, including the finite time bound on mean squared error and sample complexity. On the empirical side, experience replay has been a key ingredient in the success of deep RL algorithms, but its theoretical effects on RL have yet to be fully understood. In this paper, we present a simple decomposition of the Markovian noise terms and provide finite-time error bounds for TD-learning with experience replay. Specifically, under the Markovian observation model, we demonstrate that for both the averaged iterate and final iterate cases, the error term induced by a constant step-size can be effectively controlled by the size of the replay buffer and the mini-batch sampled from the experience replay buffer.
    
[^6]: 强化学习的经验解释

    Experiential Explanations for Reinforcement Learning. (arXiv:2210.04723v3 [cs.AI] UPDATED)

    [http://arxiv.org/abs/2210.04723](http://arxiv.org/abs/2210.04723)

    该论文提出了一种经验解释技术，通过训练影响预测器来恢复强化学习系统中的信息，使得非AI专家能够更好地理解其决策过程。

    

    强化学习系统可能非常复杂和无法解释，这使得非人工智能专家难以理解或干预它们的决策。我们提出了一种经验解释技术，通过在强化学习策略旁边训练影响预测器来生成反事实解释。影响预测器是学习奖励来源如何影响代理在不同状态下的模型，从而恢复有关策略如何反映环境的信息。

    Reinforcement Learning (RL) systems can be complex and non-interpretable, making it challenging for non-AI experts to understand or intervene in their decisions. This is due, in part, to the sequential nature of RL in which actions are chosen because of future rewards. However, RL agents discard the qualitative features of their training, making it hard to recover user-understandable information for "why" an action is chosen. Proposed sentence chunking: We propose a technique Experiential Explanations to generate counterfactual explanations by training influence predictors alongside the RL policy. Influence predictors are models that learn how sources of reward affect the agent in different states, thus restoring information about how the policy reflects the environment. A human evaluation study revealed that participants presented with experiential explanations were better able to correctly guess what an agent would do than those presented with other standard types of explanations. Pa
    

