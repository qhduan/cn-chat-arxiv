# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning](https://arxiv.org/abs/2312.10107) | 分析了上下文感知领域泛化的条件，提出了理论分析和实证分析所需的标准，并展示了该方法可以检测非常数域的场景。 |

# 详细

[^1]: 迈向面向上下文感知领域泛化：理解边缘传递学习的好处和限制

    Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning

    [https://arxiv.org/abs/2312.10107](https://arxiv.org/abs/2312.10107)

    分析了上下文感知领域泛化的条件，提出了理论分析和实证分析所需的标准，并展示了该方法可以检测非常数域的场景。

    

    在这项工作中，我们分析了关于输入$X$的上下文信息如何改善深度学习模型在新领域中的预测的条件。在领域泛化中边缘传递学习的研究基础上，我们将上下文的概念形式化为一组数据点的排列不变表示，这些数据点来自于与输入本身相同的域。我们对这种方法在原则上可以产生好处的条件进行了理论分析，并制定了两个在实践中可以轻松验证的必要标准。此外，我们提供了关于边缘传递学习方法有望具有稳健性的分布变化类型的见解。实证分析表明我们的标准有效地区分了有利和不利的场景。最后，我们证明可以可靠地检测模型面临非常数域的场景。

    arXiv:2312.10107v2 Announce Type: replace-cross  Abstract: In this work, we analyze the conditions under which information about the context of an input $X$ can improve the predictions of deep learning models in new domains. Following work in marginal transfer learning in Domain Generalization (DG), we formalize the notion of context as a permutation-invariant representation of a set of data points that originate from the same domain as the input itself. We offer a theoretical analysis of the conditions under which this approach can, in principle, yield benefits, and formulate two necessary criteria that can be easily verified in practice. Additionally, we contribute insights into the kind of distribution shifts for which the marginal transfer learning approach promises robustness. Empirical analysis shows that our criteria are effective in discerning both favorable and unfavorable scenarios. Finally, we demonstrate that we can reliably detect scenarios where a model is tasked with unw
    

