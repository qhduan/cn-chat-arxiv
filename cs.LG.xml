<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#65288;CGM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#21151;&#33021;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12859</link><description>&lt;p&gt;
&#20855;&#26377;&#20989;&#25968;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#30340;&#21407;&#22987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Primal Methods for Variational Inequality Problems with Functional Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12859
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#65288;CGM&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#22810;&#20010;&#21151;&#33021;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#22240;&#20854;&#22312;&#21253;&#25324;&#26426;&#22120;&#23398;&#20064;&#21644;&#36816;&#31609;&#23398;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#32780;&#22791;&#21463;&#35748;&#21487;&#12290; &#39318;&#27425;&#26041;&#27861;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26631;&#20934;&#26041;&#27861;&#65292;&#22240;&#20854;&#31616;&#21333;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290; &#20256;&#32479;&#19978;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#25237;&#24433;&#25110;&#32447;&#24615;&#26368;&#23567;&#21270;&#23637;&#24320;&#22120;&#26469;&#23548;&#33322;&#21487;&#34892;&#38598;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20250;&#22312;&#20855;&#26377;&#22810;&#20010;&#21151;&#33021;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290; &#35299;&#20915;&#36825;&#20123;&#21151;&#33021;&#32422;&#26463;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;Lagrange&#20989;&#25968;&#30340;&#21407;&#22987;-&#23545;&#20598;&#31639;&#27861;&#19978;&#12290; &#36825;&#20123;&#31639;&#27861;&#21450;&#20854;&#29702;&#35770;&#20998;&#26512;&#36890;&#24120;&#38656;&#35201;&#23384;&#22312;&#24182;&#19988;&#20107;&#20808;&#20102;&#35299;&#26368;&#20339;&#25289;&#26684;&#26391;&#26085;&#20056;&#25968;&#12290; &#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#21407;&#22987;&#26041;&#27861;&#65292;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#65288;CGM&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#21151;&#33021;&#32422;&#26463;&#30340;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12859v1 Announce Type: cross  Abstract: Constrained variational inequality problems are recognized for their broad applications across various fields including machine learning and operations research. First-order methods have emerged as the standard approach for solving these problems due to their simplicity and scalability. However, they typically rely on projection or linear minimization oracles to navigate the feasible set, which becomes computationally expensive in practical scenarios featuring multiple functional constraints. Existing efforts to tackle such functional constrained variational inequality problems have centered on primal-dual algorithms grounded in the Lagrangian function. These algorithms along with their theoretical analysis often require the existence and prior knowledge of the optimal Lagrange multipliers. In this work, we propose a simple primal method, termed Constrained Gradient Method (CGM), for addressing functional constrained variational inequa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.09974</link><description>&lt;p&gt;
GET&#65306;&#35299;&#38145;CLIP&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
GET: Unlocking the Multi-modal Potential of CLIP for Generalized Category Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#25968;&#25454;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#65292;&#20197;&#35299;&#38145;CLIP&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#20219;&#21153;&#20013;&#30340;&#22810;&#27169;&#24577;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#21253;&#21547;&#26087;&#31867;&#21035;&#21644;&#26032;&#31867;&#21035;&#30340;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#65292;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#26088;&#22312;&#20934;&#30830;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#24182;&#27491;&#30830;&#20998;&#31867;&#26087;&#31867;&#21035;&#65292;&#21033;&#29992;&#20174;&#26377;&#26631;&#31614;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#31867;&#21035;&#27010;&#24565;&#12290;&#24403;&#21069;&#30340;GCD&#26041;&#27861;&#21482;&#20351;&#29992;&#21333;&#19968;&#30340;&#35270;&#35273;&#20449;&#24687;&#27169;&#24577;&#65292;&#23548;&#33268;&#22312;&#35270;&#35273;&#19978;&#30456;&#20284;&#31867;&#21035;&#30340;&#20998;&#31867;&#25928;&#26524;&#19981;&#20339;&#12290;&#34429;&#28982;&#26576;&#20123;&#31867;&#21035;&#22312;&#35270;&#35273;&#19978;&#23481;&#26131;&#28151;&#28102;&#65292;&#20294;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#21487;&#33021;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23558;&#25991;&#26412;&#20449;&#24687;&#24341;&#20837;&#21040;GCD&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#26080;&#26631;&#31614;&#25968;&#25454;&#32570;&#20047;&#31867;&#21035;&#21517;&#31216;&#65292;&#20351;&#24471;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25991;&#26412;&#23884;&#20837;&#21512;&#25104;&#22120;&#65288;TES&#65289;&#65292;&#29992;&#20110;&#20026;&#26080;&#26631;&#31614;&#26679;&#26412;&#29983;&#25104;&#20266;&#25991;&#26412;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;TES&#21033;&#29992;CLIP&#21487;&#20197;&#29983;&#25104;&#23545;&#40784;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29305;&#24449;&#36825;&#19968;&#29305;&#24615;&#65292;&#23558;&#35270;&#35273;&#23884;&#20837;&#36716;&#25442;&#20026;CLIP&#25991;&#26412;&#27169;&#22411;&#30340;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09974v1 Announce Type: cross  Abstract: Given unlabelled datasets containing both old and new categories, generalized category discovery (GCD) aims to accurately discover new classes while correctly classifying old classes, leveraging the class concepts learned from labeled samples. Current GCD methods only use a single visual modality of information, resulting in poor classification of visually similar classes. Though certain classes are visually confused, their text information might be distinct, motivating us to introduce text information into the GCD task. However, the lack of class names for unlabelled data makes it impractical to utilize text information. To tackle this challenging problem, in this paper, we propose a Text Embedding Synthesizer (TES) to generate pseudo text embeddings for unlabelled samples. Specifically, our TES leverages the property that CLIP can generate aligned vision-language features, converting visual embeddings into tokens of the CLIP's text e
&lt;/p&gt;</description></item><item><title>&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06586</link><description>&lt;p&gt;
ContextGPT: &#23558;LLMs&#30693;&#35782;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ContextGPT: Infusing LLMs Knowledge into Neuro-Symbolic Activity Recognition Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06586
&lt;/p&gt;
&lt;p&gt;
&#23558;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24120;&#35782;&#30693;&#35782;&#26377;&#25928;&#22320;&#27880;&#20837;&#31070;&#32463;&#31526;&#21495;&#27963;&#21160;&#35782;&#21035;&#27169;&#22411;&#65292;&#20197;&#32531;&#35299;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#24863;&#30693;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#31227;&#21160;&#35745;&#31639;&#20013;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25991;&#29486;&#20013;&#26368;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#22522;&#20110;&#30417;&#30563;&#24335;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#23454;&#38469;&#37096;&#32626;&#21463;&#21040;&#38656;&#35201;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#38480;&#21046;&#12290;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NeSy&#65289;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#21363;&#23558;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#21450;&#20854;&#21487;&#33021;&#21457;&#29983;&#30340;&#32972;&#26223;&#30340;&#24120;&#35782;&#30693;&#35782;&#27880;&#20837;HAR&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#20013;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;HAR&#30340;NeSy&#26041;&#27861;&#20381;&#36182;&#20110;&#36923;&#36753;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#30693;&#35782;&#65288;&#20363;&#22914;&#26412;&#20307;&#35770;&#65289;&#65292;&#20854;&#35774;&#35745;&#12289;&#23454;&#26045;&#21644;&#32500;&#25252;&#20197;&#25429;&#25417;&#26032;&#27963;&#21160;&#21644;&#19978;&#19979;&#25991;&#38656;&#35201;&#26174;&#33879;&#30340;&#20154;&#21147;&#24037;&#31243;&#21162;&#21147;&#12289;&#25216;&#26415;&#30693;&#35782;&#21644;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26377;&#25928;&#22320;&#32534;&#30721;&#20102;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06586v1 Announce Type: cross  Abstract: Context-aware Human Activity Recognition (HAR) is a hot research area in mobile computing, and the most effective solutions in the literature are based on supervised deep learning models. However, the actual deployment of these systems is limited by the scarcity of labeled data that is required for training. Neuro-Symbolic AI (NeSy) provides an interesting research direction to mitigate this issue, by infusing common-sense knowledge about human activities and the contexts in which they can be performed into HAR deep learning classifiers. Existing NeSy methods for context-aware HAR rely on knowledge encoded in logic-based models (e.g., ontologies) whose design, implementation, and maintenance to capture new activities and contexts require significant human engineering efforts, technical knowledge, and domain expertise. Recent works show that pre-trained Large Language Models (LLMs) effectively encode common-sense knowledge about human a
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00025</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
On the Challenges and Opportunities in Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00025
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#36817;&#24180;&#26469;&#22686;&#38271;&#36805;&#36895;&#32780;&#31283;&#23450;&#12290;&#38543;&#30528;&#28023;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#30340;&#36827;&#27493;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#23637;&#29616;&#20986;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#25991;&#26412;&#20197;&#21450;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#35270;&#39057;&#21644;&#20998;&#23376;&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#22823;&#35268;&#27169;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#33509;&#24178;&#22522;&#26412;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#30340;&#20851;&#38190;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35782;&#21035;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#25506;&#32034;&#26377;&#30410;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#35775;&#38382;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00025v1 Announce Type: cross  Abstract: The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;</title><link>https://arxiv.org/abs/2402.10079</link><description>&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Review of the Learning-based Camera and Lidar Simulation Methods for Autonomous Driving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#20256;&#24863;&#22120;&#65292;&#23588;&#20854;&#26159;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#65292;&#26159;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;(Autonomous Driving Systems&#65292;ADS)&#30340;&#20851;&#38190;&#20803;&#32032;&#65292;&#20351;&#20854;&#33021;&#22815;&#29702;&#35299;&#21608;&#22260;&#29615;&#22659;&#20197;&#20570;&#20986;&#26126;&#26234;&#30340;&#39550;&#39542;&#21644;&#25511;&#21046;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#36924;&#30495;&#30340;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#27169;&#25311;&#26041;&#27861;&#65292;&#20063;&#31216;&#20026;&#30456;&#26426;&#21644;&#28608;&#20809;&#38647;&#36798;&#27169;&#22411;&#65292;&#23545;&#20110;&#26377;&#25928;&#36827;&#34892;&#22522;&#20110;&#20223;&#30495;&#30340;ADS&#27979;&#35797;&#33267;&#20851;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#20419;&#36827;&#20102;&#24863;&#30693;&#20256;&#24863;&#22120;&#27169;&#22411;&#20316;&#20026;&#21512;&#25104;&#21508;&#31181;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#30340;&#26222;&#21450;&#12290;&#20256;&#32479;&#20256;&#24863;&#22120;&#20223;&#30495;&#26041;&#27861;&#20381;&#36182;&#20110;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#31995;&#32479;&#22914;ADS&#20013;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#30340;&#28508;&#21147;&#22312;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#21463;&#21040;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#21512;&#25104;&#39640;&#32500;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#30340;&#25512;&#21160;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#23398;&#20064;&#30340;&#20256;&#24863;&#22120;&#20223;&#30495;&#26041;&#27861;&#30340;&#26368;&#26032;&#30740;&#31350;&#29616;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10079v1 Announce Type: cross  Abstract: Perception sensors, particularly camera and Lidar, are key elements of Autonomous Driving Systems (ADS) that enable them to comprehend their surroundings for informed driving and control decisions. Therefore, developing realistic camera and Lidar simulation methods, also known as camera and Lidar models, is of paramount importance to effectively conduct simulation-based testing for ADS. Moreover, the rise of deep learning-based perception models has propelled the prevalence of perception sensor models as valuable tools for synthesising diverse training datasets. The traditional sensor simulation methods rely on computationally expensive physics-based algorithms, specifically in complex systems such as ADS. Hence, the current potential resides in learning-based models, driven by the success of deep generative models in synthesising high-dimensional data. This paper reviews the current state-of-the-art in learning-based sensor simulation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05935</link><description>&lt;p&gt;
SPHINX-X: &#25193;&#23637;&#25968;&#25454;&#21644;&#21442;&#25968;&#29992;&#20110;&#19968;&#31995;&#21015;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;SPHINX-X&#65292;&#19968;&#31181;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#12290;&#36890;&#36807;&#25913;&#36827;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#25104;&#21151;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#19981;&#21516;&#30340;MLLMs&#65292;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#26377;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;SPHINX-X&#65292;&#19968;&#31181;&#22522;&#20110;SPHINX&#24320;&#21457;&#30340;&#24191;&#27867;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#31995;&#21015;&#12290;&#20026;&#20102;&#25913;&#21892;&#26550;&#26500;&#21644;&#35757;&#32451;&#25928;&#29575;&#65292;&#25105;&#20204;&#36890;&#36807;&#31227;&#38500;&#20887;&#20313;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#12289;&#32469;&#36807;&#23436;&#20840;&#22635;&#20805;&#30340;&#23376;&#22270;&#20687;&#65292;&#24182;&#23558;&#22810;&#38454;&#27573;&#35757;&#32451;&#31616;&#21270;&#25104;&#20026;&#19968;&#38454;&#27573;&#30340;&#20840;&#38598;&#21512;&#27169;&#24335;&#65292;&#20462;&#25913;&#20102;SPHINX&#26694;&#26550;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;MLLM&#30340;&#28508;&#21147;&#65292;&#25105;&#20204;&#32452;&#35013;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#36328;&#35821;&#35328;&#12289;&#36328;&#35270;&#35273;&#21644;&#35270;&#35273;-&#35821;&#35328;&#20219;&#21153;&#30340;&#22810;&#39046;&#22495;&#12289;&#22810;&#27169;&#24577;&#30340;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#20844;&#24320;&#21487;&#29992;&#30340;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20351;&#29992;&#25105;&#20204;&#30340;OCR&#23494;&#38598;&#21644;Mark&#25968;&#25454;&#38598;&#20016;&#23500;&#36825;&#20010;&#25910;&#38598;&#65292;&#25193;&#23637;&#20102;&#22810;&#26679;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#36890;&#36807;&#23545;&#19981;&#21516;&#22522;&#30784;LLM&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;TinyLlama1.1B&#12289;InternLM2-7B&#12289;LLaMA2-13B&#21644;Mixtral8x7B&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31995;&#21015;&#21442;&#25968;&#22823;&#23567;&#21644;&#22810;&#35821;&#35328;&#33021;&#21147;&#21464;&#21270;&#30340;MLLMs&#12290;&#20840;&#38754;&#30340;&#22522;&#20934;&#27979;&#35797;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#24615;&#33021;&#19982;&#25968;&#25454;&#21644;&#21442;&#25968;&#35268;&#27169;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose SPHINX-X, an extensive Multimodality Large Language Model (MLLM) series developed upon SPHINX. To improve the architecture and training efficiency, we modify the SPHINX framework by removing redundant visual encoders, bypassing fully-padded sub-images with skip tokens, and simplifying multi-stage training into a one-stage all-in-one paradigm. To fully unleash the potential of MLLMs, we assemble a comprehensive multi-domain and multimodal dataset covering publicly available resources in language, vision, and vision-language tasks. We further enrich this collection with our curated OCR intensive and Set-of-Mark datasets, extending the diversity and generality. By training over different base LLMs including TinyLlama1.1B, InternLM2-7B, LLaMA2-13B, and Mixtral8x7B, we obtain a spectrum of MLLMs that vary in parameter size and multilingual capabilities. Comprehensive benchmarking reveals a strong correlation between the multi-modal performance with the data and parameter scales. 
&lt;/p&gt;</description></item><item><title>PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;</title><link>https://arxiv.org/abs/2402.05868</link><description>&lt;p&gt;
PromptCrypt: &#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23433;&#20840;&#36890;&#20449;&#30340;&#25552;&#31034;&#21152;&#23494;
&lt;/p&gt;
&lt;p&gt;
PromptCrypt: Prompt Encryption for Secure Communication with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05868
&lt;/p&gt;
&lt;p&gt;
PromptCrypt&#26159;&#19968;&#31181;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#30340;&#26426;&#21046;&#65292;&#20445;&#25252;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#29992;&#25143;&#30340;&#38544;&#31169;&#65292;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#21644;&#35299;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#22312;&#26085;&#24120;&#25805;&#20316;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#25104;&#20026;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#20123;&#27169;&#22411;&#22312;&#21487;&#35775;&#38382;&#24615;&#21644;&#21151;&#33021;&#24615;&#26041;&#38754;&#24102;&#26469;&#20102;&#37325;&#22823;&#22909;&#22788;&#65292;&#20294;&#23427;&#20204;&#20063;&#24341;&#20837;&#20102;&#37325;&#35201;&#30340;&#38544;&#31169;&#38382;&#39064;&#65306;&#22312;&#20113;&#22522;&#30784;&#26550;&#26500;&#20013;&#20256;&#36755;&#21644;&#23384;&#20648;&#29992;&#25143;&#25968;&#25454;&#20250;&#20135;&#29983;&#37325;&#22823;&#30340;&#25968;&#25454;&#27844;&#38706;&#21644;&#26410;&#32463;&#25480;&#26435;&#35775;&#38382;&#25935;&#24863;&#20449;&#24687;&#30340;&#39118;&#38505;&#65307;&#21363;&#20351;&#25968;&#25454;&#30340;&#20256;&#36755;&#21644;&#23384;&#20648;&#34987;&#21152;&#23494;&#65292;LLM&#26381;&#21153;&#25552;&#20379;&#21830;&#20173;&#28982;&#30693;&#36947;&#25968;&#25454;&#30340;&#30495;&#23454;&#20869;&#23481;&#65292;&#20174;&#32780;&#38459;&#27490;&#20010;&#20154;&#25110;&#23454;&#20307;&#25918;&#24515;&#20351;&#29992;&#27492;&#31867;LLM&#26381;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26426;&#21046;PromptCrypt&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#12290;&#23427;&#20351;&#29992;&#34920;&#24773;&#31526;&#21495;&#23545;&#29992;&#25143;&#36755;&#20837;&#36827;&#34892;&#21152;&#23494;&#65292;&#28982;&#21518;&#23558;&#20854;&#21457;&#36865;&#21040;LLM&#65292;&#26377;&#25928;&#22320;&#20351;&#20854;&#23545;&#20154;&#31867;&#25110;LLM&#30340;&#26816;&#26597;&#26080;&#27861;&#29702;&#35299;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#25552;&#31034;&#30340;&#24847;&#22270;&#65292;&#20174;&#32780;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cloud-based large language models (LLMs) such as ChatGPT have increasingly become integral to daily operations, serving as vital tools across various applications. While these models offer substantial benefits in terms of accessibility and functionality, they also introduce significant privacy concerns: the transmission and storage of user data in cloud infrastructures pose substantial risks of data breaches and unauthorized access to sensitive information; even if the transmission and storage of data is encrypted, the LLM service provider itself still knows the real contents of the data, preventing individuals or entities from confidently using such LLM services. To address these concerns, this paper proposes a simple yet effective mechanism PromptCrypt to protect user privacy. It uses Emoji to encrypt the user inputs before sending them to LLM, effectively rendering them indecipherable to human or LLM's examination while retaining the original intent of the prompt, thus ensuring the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2401.17173</link><description>&lt;p&gt;
&#36890;&#36807;&#20989;&#25968;&#32534;&#30721;&#22120;&#23454;&#29616;&#38646;-shot&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Reinforcement Learning via Function Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#20294;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#38590;&#28857;&#22312;&#20110;&#23547;&#25214;&#19968;&#20010;&#33391;&#22909;&#30340;&#34920;&#31034;&#26469;&#34920;&#36798;&#24403;&#21069;&#20219;&#21153;&#65292;&#20197;&#20415;&#20195;&#29702;&#31243;&#24207;&#29702;&#35299;&#23427;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#20989;&#25968;&#32534;&#30721;&#22120;&#26469;&#34920;&#31034;&#22870;&#21169;&#20989;&#25968;&#25110;&#36716;&#31227;&#20989;&#25968;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#19968;&#20010;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#26377;&#20851;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#36801;&#31227;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#23558;&#22522;&#26412;RL&#31639;&#27861;&#19982;&#20989;&#25968;&#32534;&#30721;&#22120;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;RL&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#28176;&#36817;&#24615;&#33021;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encod
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AutArch&#65292;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20174;&#36951;&#30041;&#36164;&#28304;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35760;&#24405;&#36136;&#37327;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2311.17978</link><description>&lt;p&gt;
AutArch&#65306;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
AutArch: An AI-assisted workflow for object detection and automated recording in archaeological catalogues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17978
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;AutArch&#65292;&#19968;&#31181;&#29992;&#20110;&#32771;&#21476;&#30446;&#24405;&#20013;&#29289;&#20307;&#26816;&#27979;&#21644;&#33258;&#21160;&#21270;&#35760;&#24405;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#24037;&#20316;&#27969;&#31243;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#21270;&#20174;&#36951;&#30041;&#36164;&#28304;&#20013;&#25552;&#21462;&#25968;&#25454;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#35760;&#24405;&#36136;&#37327;&#21644;&#26631;&#20934;&#19981;&#19968;&#33268;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30340;&#32972;&#26223;&#26159;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#22823;&#25968;&#25454;&#20174;&#24322;&#26500;&#30340;&#24050;&#21457;&#34920;&#36164;&#28304;&#20013;&#21019;&#24314;&#22823;&#35268;&#27169;&#32479;&#19968;&#30340;&#32771;&#21476;&#25968;&#25454;&#38598;&#65292;&#27604;&#22914;&#36951;&#29289;&#30446;&#24405;&#12290;&#35770;&#25991;&#20851;&#27880;&#30340;&#26159;&#19968;&#33268;&#32771;&#21476;&#25968;&#25454;&#32452;&#21512;&#30340;&#25361;&#25112;&#12290;&#30001;&#20110;&#29616;&#26377;&#35760;&#24405;&#22312;&#36136;&#37327;&#21644;&#35760;&#24405;&#26631;&#20934;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#25105;&#20204;&#26080;&#27861;&#31616;&#21333;&#22320;&#21512;&#24182;&#29616;&#26377;&#35760;&#24405;&#12290;&#22240;&#27492;&#65292;&#24517;&#39035;&#20174;&#24050;&#21457;&#34920;&#30340;&#32771;&#21476;&#25554;&#22270;&#20013;&#37325;&#26032;&#21019;&#24314;&#35760;&#24405;&#12290;&#21482;&#26377;&#36890;&#36807;&#33258;&#21160;&#21270;&#30340;&#24110;&#21161;&#65292;&#36825;&#25165;&#26159;&#21487;&#34892;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#19968;&#20010;&#26032;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#29992;&#20110;&#20174;&#32771;&#21476;&#36951;&#29289;&#30446;&#24405;&#20013;&#25910;&#38598;&#25968;&#25454;&#65292;&#36825;&#20123;&#30446;&#24405;&#20316;&#20026;&#36951;&#30041;&#36164;&#28304;&#23384;&#22312;&#65292;&#27604;&#22914;&#22823;&#22411;&#26410;&#25490;&#24207;&#30340;PDF&#25991;&#20214;&#20013;&#30340;&#32771;&#21476;&#32472;&#22270;&#21644;&#29031;&#29255;&#65307;&#35813;&#24037;&#20316;&#27969;&#31243;&#20381;&#36182;&#20110;&#25903;&#25345;&#22270;&#20687;&#22788;&#29702;&#12289;&#29289;&#20307;&#26816;&#27979;&#20197;&#21450;&#39564;&#35777;&#21644;&#35843;&#25972;&#33258;&#21160;&#33719;&#21462;&#25968;&#25454;&#30340;&#20132;&#20114;&#25163;&#27573;&#30340;&#33258;&#23450;&#20041;&#36719;&#20214;&#65288;AutArch&#65289;&#12290;&#25105;&#20204;&#38598;&#25104;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17978v2 Announce Type: replace-cross  Abstract: The context of this paper is the creation of large uniform archaeological datasets from heterogeneous published resources, such as find catalogues - with the help of AI and Big Data. The paper is concerned with the challenge of consistent assemblages of archaeological data. We cannot simply combine existing records, as they differ in terms of quality and recording standards. Thus, records have to be recreated from published archaeological illustrations. This is only a viable path with the help of automation. The contribution of this paper is a new workflow for collecting data from archaeological find catalogues available as legacy resources, such as archaeological drawings and photographs in large unsorted PDF files; the workflow relies on custom software (AutArch) supporting image processing, object detection, and interactive means of validating and adjusting automatically retrieved data. We integrate artificial intelligence (
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#20102;&#20016;&#23500;&#32780;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#27969;&#24418;&#20551;&#35774;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#20026;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2208.11665</link><description>&lt;p&gt;
&#32479;&#35745;&#23545;&#27969;&#24418;&#20551;&#35774;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Statistical exploration of the Manifold Hypothesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.11665
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#20174;&#25968;&#25454;&#20013;&#24471;&#20986;&#20102;&#20016;&#23500;&#32780;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#24182;&#25552;&#20379;&#20102;&#35299;&#37322;&#27969;&#24418;&#20551;&#35774;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#35813;&#30740;&#31350;&#20026;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#25552;&#20379;&#20102;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#20551;&#35774;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#20026;&#25509;&#21463;&#30340;&#29702;&#35770;&#65292;&#23427;&#35748;&#20026;&#21517;&#20041;&#19978;&#30340;&#39640;&#32500;&#25968;&#25454;&#23454;&#38469;&#19978;&#38598;&#20013;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#20302;&#32500;&#27969;&#24418;&#20013;&#12290;&#36825;&#31181;&#29616;&#35937;&#22312;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#20013;&#32463;&#39564;&#24615;&#22320;&#35266;&#23519;&#21040;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24050;&#32463;&#23548;&#33268;&#20102;&#22810;&#31181;&#32479;&#35745;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#24182;&#34987;&#35748;&#20026;&#26159;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36890;&#36807;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#36825;&#31181;&#36890;&#29992;&#19988;&#38750;&#24120;&#31616;&#21333;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#29983;&#25104;&#20016;&#23500;&#32780;&#26377;&#26102;&#22797;&#26434;&#30340;&#27969;&#24418;&#32467;&#26500;&#65292;&#36890;&#36807;&#28508;&#21464;&#37327;&#12289;&#30456;&#20851;&#24615;&#21644;&#24179;&#31283;&#24615;&#31561;&#22522;&#26412;&#27010;&#24565;&#12290;&#36825;&#20026;&#20026;&#20160;&#20040;&#27969;&#24418;&#20551;&#35774;&#22312;&#36825;&#20040;&#22810;&#24773;&#20917;&#19979;&#20284;&#20046;&#25104;&#31435;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32479;&#35745;&#35299;&#37322;&#12290;&#22312;&#28508;&#22312;&#24230;&#37327;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21457;&#29616;&#21644;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#20960;&#20309;&#32467;&#26500;&#20197;&#21450;&#25506;&#32034;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -- the Latent Metric Model -- via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism
&lt;/p&gt;</description></item><item><title>&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.14086</link><description>&lt;p&gt;
&#20351;&#29992;Sum-Product Networks&#29983;&#25104;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Generating Likely Counterfactuals Using Sum-Product Networks. (arXiv:2401.14086v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14086
&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#35201;&#27714;&#65292;&#38656;&#35201;&#23545;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#36827;&#34892;&#35299;&#37322;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sum-Product Networks&#27169;&#25311;&#23547;&#25214;&#39640;&#21487;&#33021;&#24615;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#25552;&#20379;&#28385;&#36275;&#22810;&#20010;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#20339;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29992;&#25143;&#38656;&#27714;&#21644;&#26368;&#36817;&#30340;&#27861;&#35268;&#65288;GDPR&#12289;AI&#27861;&#26696;&#65289;&#65292;&#38656;&#35201;&#35299;&#37322;AI&#31995;&#32479;&#25152;&#20570;&#20986;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#20915;&#31574;&#24448;&#24448;&#21482;&#33021;&#22312;&#20107;&#21518;&#35299;&#37322;&#65292;&#21453;&#20107;&#23454;&#25512;&#29702;&#25104;&#20026;&#24120;&#35265;&#30340;&#35299;&#37322;&#26041;&#24335;&#12290;&#20160;&#20040;&#26500;&#25104;&#20102;&#26368;&#20339;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#24517;&#39035;&#32771;&#34385;&#22810;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#8220;&#26679;&#26412;&#36317;&#31163;&#8221;&#26159;&#26368;&#24120;&#35265;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#19968;&#35201;&#27714;&#32463;&#24120;&#20250;&#23548;&#33268;&#19981;&#22826;&#21487;&#33021;&#19988;&#22240;&#27492;&#20215;&#20540;&#26377;&#38480;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#39640;&#21487;&#33021;&#24615;&#35299;&#37322;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#65288;MIO&#65289;&#27169;&#25311;&#23547;&#25214;&#28385;&#36275;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#35768;&#22810;&#24120;&#35265;&#35201;&#27714;&#30340;&#26368;&#26377;&#21487;&#33021;&#35299;&#37322;&#12290;&#22312;&#27492;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sum-Product Network&#65288;SPN&#65289;&#30340;MIO&#34920;&#36798;&#65292;&#24182;&#20351;&#29992;SPN&#20272;&#35745;&#21453;&#20107;&#23454;&#30340;&#21487;&#33021;&#24615;&#65292;&#36825;&#23545;&#29420;&#31435;&#30340;&#20852;&#36259;&#20063;&#26377;&#29992;&#12290;&#19982;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20960;&#31181;&#26041;&#27861;&#36827;&#34892;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to user demand and recent regulation (GDPR, AI Act), decisions made by AI systems need to be explained. These decisions are often explainable only post hoc, where counterfactual explanations are popular. The question of what constitutes the best counterfactual explanation must consider multiple aspects, where "distance from the sample" is the most common. We argue that this requirement frequently leads to explanations that are unlikely and, therefore, of limited value. Here, we present a system that provides high-likelihood explanations. We show that the search for the most likely explanations satisfying many common desiderata for counterfactual explanations can be modeled using mixed-integer optimization (MIO). In the process, we propose an MIO formulation of a Sum-Product Network (SPN) and use the SPN to estimate the likelihood of a counterfactual, which can be of independent interest. A numerical comparison against several methods for generating counterfactual explanations is pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SLOTS&#30340;&#21322;&#30417;&#30563;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#25509;&#25910;&#21322;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#22312;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#24494;&#35843;&#20013;&#32508;&#21512;&#21033;&#29992;&#23545;&#27604;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#32570;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.08848</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#31471;&#21040;&#31471;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised End-To-End Contrastive Learning For Time Series Classification. (arXiv:2310.08848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SLOTS&#30340;&#21322;&#30417;&#30563;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#12290;&#23427;&#36890;&#36807;&#25509;&#25910;&#21322;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#22312;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#19979;&#28216;&#24494;&#35843;&#20013;&#32508;&#21512;&#21033;&#29992;&#23545;&#27604;&#25439;&#22833;&#21644;&#20998;&#31867;&#25439;&#22833;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#32570;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#26159;&#37329;&#34701;&#12289;&#21307;&#30103;&#21644;&#20256;&#24863;&#22120;&#25968;&#25454;&#20998;&#26512;&#31561;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#26080;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#22312;&#20351;&#29992;&#26377;&#38480;&#26631;&#31614;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#25928;&#34920;&#31034;&#26041;&#38754;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#29616;&#26377;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#20013;&#26222;&#36941;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#29420;&#31435;&#30340;&#38454;&#27573;&#65306;&#22312;&#26080;&#26631;&#31614;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#65292;&#28982;&#21518;&#22312;&#23567;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#19978;&#23545;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#20363;&#22914;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#23545;&#27604;&#25439;&#22833;&#19981;&#33021;&#30452;&#25509;&#24433;&#21709;&#19979;&#28216;&#24494;&#35843;&#20998;&#31867;&#22120;&#65292;&#20197;&#21450;&#32570;&#20047;&#21033;&#29992;&#30001;&#26377;&#20215;&#20540;&#30340;&#30495;&#23454;&#26631;&#31614;&#24341;&#23548;&#30340;&#20998;&#31867;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SLOTS&#65288;Semi-supervised Learning fOr Time clasSification&#65289;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;SLOTS&#25509;&#25910;&#21322;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#25324;&#22823;&#37327;&#26080;&#26631;&#31614;&#26679;&#26412;&#21644;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series classification is a critical task in various domains, such as finance, healthcare, and sensor data analysis. Unsupervised contrastive learning has garnered significant interest in learning effective representations from time series data with limited labels. The prevalent approach in existing contrastive learning methods consists of two separate stages: pre-training the encoder on unlabeled datasets and fine-tuning the well-trained model on a small-scale labeled dataset. However, such two-stage approaches suffer from several shortcomings, such as the inability of unsupervised pre-training contrastive loss to directly affect downstream fine-tuning classifiers, and the lack of exploiting the classification loss which is guided by valuable ground truth. In this paper, we propose an end-to-end model called SLOTS (Semi-supervised Learning fOr Time clasSification). SLOTS receives semi-labeled datasets, comprising a large number of unlabeled samples and a small proportion of labele
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#36991;&#20813;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#12290;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#36824;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#20248;&#20808;&#24230;&#25351;&#26631;&#20248;&#20808;&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2306.15503</link><description>&lt;p&gt;
&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65306;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prioritized Trajectory Replay: A Replay Memory for Data-driven Reinforcement Learning. (arXiv:2306.15503v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#30340;&#22238;&#25918;&#35760;&#24518;&#26041;&#27861;&#65292;&#23558;&#25968;&#25454;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#24182;&#21033;&#29992;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#36991;&#20813;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#12290;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#36824;&#33021;&#26681;&#25454;&#19981;&#21516;&#30340;&#20248;&#20808;&#24230;&#25351;&#26631;&#20248;&#20808;&#37319;&#26679;&#25928;&#29575;&#26356;&#39640;&#30340;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#20063;&#31216;&#20026;&#31163;&#32447;RL&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#20855;&#26377;&#25552;&#21319;&#22312;&#32447;RL&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#31163;&#32447;RL&#20013;&#30340;&#25968;&#25454;&#37319;&#26679;&#25216;&#26415;&#30340;&#20316;&#29992;&#21364;&#34987;&#24573;&#35270;&#20102;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#23558;&#37319;&#26679;&#25216;&#26415;&#24212;&#29992;&#20110;&#29366;&#24577;&#36716;&#25442;&#24182;&#19981;&#33021;&#22987;&#32456;&#25552;&#39640;&#31163;&#32447;RL&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35760;&#24518;&#25216;&#26415;&#8212;&#8212;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65288;TR/PTR&#65289;&#65292;&#23427;&#23558;&#37319;&#26679;&#30340;&#35270;&#35282;&#25193;&#23637;&#21040;&#36712;&#36857;&#20013;&#65292;&#20197;&#20174;&#26377;&#38480;&#30340;&#25968;&#25454;&#20013;&#25552;&#21462;&#26356;&#20840;&#38754;&#30340;&#20449;&#24687;&#12290;TR&#36890;&#36807;&#21453;&#21521;&#37319;&#26679;&#36712;&#36857;&#26469;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#20248;&#21270;&#21518;&#32493;&#29366;&#24577;&#20449;&#24687;&#30340;&#20351;&#29992;&#12290;&#22312;TR&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#21152;&#26435;&#35780;&#35770;&#30446;&#26631;&#65292;&#20197;&#36991;&#20813;&#22312;&#31163;&#32447;&#35757;&#32451;&#20013;&#37319;&#26679;&#26410;&#35265;&#36807;&#30340;&#21160;&#20316;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#20248;&#20808;&#36712;&#36857;&#22238;&#25918;&#65288;PTR&#65289;&#26469;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36712;&#36857;&#37319;&#26679;&#65292;&#26681;&#25454;&#19981;&#21516;&#30340;&#36712;&#36857;&#20248;&#20808;&#24230;&#25351;&#26631;&#36827;&#34892;&#20248;&#20808;&#35774;&#32622;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
In recent years, data-driven reinforcement learning (RL), also known as offline RL, have gained significant attention. However, the role of data sampling techniques in offline RL has been overlooked despite its potential to enhance online RL performance. Recent research suggests applying sampling techniques directly to state-transitions does not consistently improve performance in offline RL. Therefore, in this study, we propose a memory technique, (Prioritized) Trajectory Replay (TR/PTR), which extends the sampling perspective to trajectories for more comprehensive information extraction from limited data. TR enhances learning efficiency by backward sampling of trajectories that optimizes the use of subsequent state information. Building on TR, we build the weighted critic target to avoid sampling unseen actions in offline training, and Prioritized Trajectory Replay (PTR) that enables more efficient trajectory sampling, prioritized by various trajectory priority metrics. We demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#30340;&#26579;&#33394;&#20307;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;Fred Hutchinson&#30284;&#30151;&#30740;&#31350;&#20013;&#24515;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25299;&#25169;&#35270;&#35273;&#36716;&#25442;&#22120;(TopViTs)&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#26579;&#33394;&#20307;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2211.14312</link><description>&lt;p&gt;
&#31934;&#20934;&#32959;&#30244;&#23398;&#30340;&#26579;&#33394;&#20307;AI
&lt;/p&gt;
&lt;p&gt;
Karyotype AI for Precision Oncology. (arXiv:2211.14312v3 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#31934;&#20934;&#32959;&#30244;&#23398;&#20013;&#30340;&#26579;&#33394;&#20307;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#20351;&#29992;Fred Hutchinson&#30284;&#30151;&#30740;&#31350;&#20013;&#24515;&#30340;&#22823;&#37327;&#25968;&#25454;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#25299;&#25169;&#35270;&#35273;&#36716;&#25442;&#22120;(TopViTs)&#65292;&#25104;&#21151;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#35782;&#21035;&#26579;&#33394;&#20307;&#24322;&#24120;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26579;&#33394;&#20307;&#20998;&#26512;&#23545;&#20110;&#35786;&#26029;&#36951;&#20256;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#20110;&#34880;&#28082;&#31995;&#32479;&#24694;&#24615;&#32959;&#30244;&#65292;&#36890;&#36807;&#26579;&#33394;&#20307;&#32452;&#22411;&#20998;&#26512;&#26469;&#21457;&#29616;&#20307;&#32454;&#32990;&#31361;&#21464;&#26159;&#26631;&#20934;&#30340;&#25252;&#29702;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26579;&#33394;&#20307;&#32452;&#22411;&#20998;&#26512;&#22240;&#20026;&#22823;&#37096;&#20998;&#26159;&#25163;&#21160;&#25805;&#20316;&#65292;&#19988;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#26469;&#35782;&#21035;&#21644;&#27880;&#37322;&#31361;&#21464;&#65292;&#25152;&#20197;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#20197;Fred Hutchinson&#30284;&#30151;&#30740;&#31350;&#20013;&#24515;&#36807;&#21435;&#20116;&#24180;&#30340;&#32422;10,000&#20010;&#24739;&#32773;&#26631;&#26412;&#21644;&#32422;50,000&#20010;&#26579;&#33394;&#20307;&#32452;&#22411;&#22270;&#29255;&#20316;&#20026;&#35757;&#32451;&#38598;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#32452;&#20195;&#34920;&#21333;&#20010;&#26579;&#33394;&#20307;&#30340;&#26631;&#35760;&#22270;&#29255;&#12290;&#36825;&#20123;&#21333;&#20010;&#26579;&#33394;&#20307;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#20998;&#31867;&#20154;&#31867;&#30340;24&#26465;&#26579;&#33394;&#20307;&#21644;&#35782;&#21035;&#26579;&#33394;&#20307;&#24322;&#24120;&#12290;&#20855;&#26377;&#26368;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#20351;&#29992;&#20102;&#26368;&#36817;&#24341;&#20837;&#30340;&#25299;&#25169;&#35270;&#35273;&#36716;&#25442;&#22120;(TopViTs)&#21644;&#20108;&#32423;&#22359;-&#25176;&#26222;&#21033;&#33576;&#33945;&#29256;&#65292;&#20197;&#34701;&#20837;&#32467;&#26500;&#24615;&#24402;&#32435;&#20559;&#32622;&#12290;TopViT&#30340;&#24615;&#33021;&#20248;&#20110;CNN(Inc)
&lt;/p&gt;
&lt;p&gt;
Chromosome analysis is essential for diagnosing genetic disorders. For hematologic malignancies, identification of somatic clonal aberrations by karyotype analysis remains the standard of care. However, karyotyping is costly and time-consuming because of the largely manual process and the expertise required in identifying and annotating aberrations. Efforts to automate karyotype analysis to date fell short in aberration detection. Using a training set of ~10k patient specimens and ~50k karyograms from over 5 years from the Fred Hutchinson Cancer Center, we created a labeled set of images representing individual chromosomes. These individual chromosomes were used to train and assess deep learning models for classifying the 24 human chromosomes and identifying chromosomal aberrations. The top-accuracy models utilized the recently introduced Topological Vision Transformers (TopViTs) with 2-level-block-Toeplitz masking, to incorporate structural inductive bias. TopViT outperformed CNN (Inc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2209.14790</link><description>&lt;p&gt;
&#22810;&#32452;&#20998;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sparse PCA With Multiple Components. (arXiv:2209.14790v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#38598;&#26041;&#24046;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#36825;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#31232;&#30095;&#24615;&#21644;&#27491;&#20132;&#24615;&#32422;&#26463;&#30340;&#20984;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#38750;&#24120;&#39640;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#19968;&#20010;&#31232;&#30095;&#20027;&#25104;&#20998;&#24182;&#32553;&#20943;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20294;&#22312;&#23547;&#25214;&#22810;&#20010;&#30456;&#20114;&#27491;&#20132;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#25152;&#24471;&#35299;&#30340;&#27491;&#20132;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25361;&#25112;&#36825;&#31181;&#29616;&#29366;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21478;&#19968;&#31181;&#26041;&#27861;&#26469;&#21152;&#24378;&#19978;&#30028;&#65292;&#25105;&#20204;&#20351;&#29992;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#26469;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we de
&lt;/p&gt;</description></item></channel></rss>