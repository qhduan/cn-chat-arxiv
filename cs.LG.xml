<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#33976;&#39311;&#20174;&#21463;&#27745;&#26579;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#25552;&#21462;&#33391;&#24615;&#30693;&#35782;&#65292;&#23558;&#20854;&#20256;&#36882;&#32473;&#26032;&#32534;&#30721;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#33976;&#39311;&#30340;&#26680;&#24515;&#32452;&#20214;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03846</link><description>&lt;p&gt;
&#22312;&#32531;&#35299;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#21518;&#38376;&#38382;&#39064;&#20013;&#33976;&#39311;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Effectiveness of Distillation in Mitigating Backdoors in Pre-trained Encoder
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03846
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;&#33976;&#39311;&#20174;&#21463;&#27745;&#26579;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#25552;&#21462;&#33391;&#24615;&#30693;&#35782;&#65292;&#23558;&#20854;&#20256;&#36882;&#32473;&#26032;&#32534;&#30721;&#22120;&#65292;&#25104;&#21151;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#25506;&#35752;&#20102;&#33976;&#39311;&#30340;&#26680;&#24515;&#32452;&#20214;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#29992;&#20110;SSL&#20013;&#38450;&#24481;&#21463;&#27745;&#26579;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#65292;&#21483;&#20570;&#33976;&#39311;&#65292;&#36825;&#20010;&#26041;&#27861;&#26368;&#21021;&#26159;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#38450;&#24481;&#26426;&#21046;&#12290;&#33976;&#39311;&#26088;&#22312;&#20174;&#32473;&#23450;&#27169;&#22411;&#65288;&#31216;&#20026;&#25945;&#24072;&#32593;&#32476;&#65289;&#20013;&#25552;&#28860;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#21478;&#19968;&#20010;&#27169;&#22411;&#65288;&#31216;&#20026;&#23398;&#29983;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#29616;&#22312;&#20351;&#29992;&#23427;&#20174;&#21463;&#27745;&#26579;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#20013;&#25552;&#28860;&#33391;&#24615;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#32473;&#19968;&#20010;&#26032;&#32534;&#30721;&#22120;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#24178;&#20928;&#30340;&#39044;&#35757;&#32451;&#32534;&#30721;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23545;&#33976;&#39311;&#23545;&#25239;&#21463;&#27745;&#26579;&#32534;&#30721;&#22120;&#30340;&#26377;&#25928;&#24615;&#21644;&#24615;&#33021;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#38024;&#23545;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#21644;&#22235;&#20010;&#24120;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#33976;&#39311;&#21487;&#20197;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;80.87%&#38477;&#20302;&#21040;27.51%&#65292;&#32780;&#20934;&#30830;&#29575;&#19979;&#38477;&#20102;6.35%&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#33976;&#39311;&#30340;&#19977;&#20010;&#26680;&#24515;&#32452;&#20214;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#65306;&#25945;&#24072;&#32593;&#32476;&#12289;&#23398;&#29983;&#32593;&#32476;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03846v1 Announce Type: new  Abstract: In this paper, we study a defense against poisoned encoders in SSL called distillation, which is a defense used in supervised learning originally. Distillation aims to distill knowledge from a given model (a.k.a the teacher net) and transfer it to another (a.k.a the student net). Now, we use it to distill benign knowledge from poisoned pre-trained encoders and transfer it to a new encoder, resulting in a clean pre-trained encoder. In particular, we conduct an empirical study on the effectiveness and performance of distillation against poisoned encoders. Using two state-of-the-art backdoor attacks against pre-trained image encoders and four commonly used image classification datasets, our experimental results show that distillation can reduce attack success rate from 80.87% to 27.51% while suffering a 6.35% loss in accuracy. Moreover, we investigate the impact of three core components of distillation on performance: teacher net, student n
&lt;/p&gt;</description></item><item><title>TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.</title><link>https://arxiv.org/abs/2402.12683</link><description>&lt;p&gt;
TorchCP&#65306;&#22522;&#20110;PyTorch&#30340;&#19968;&#31181;&#36866;&#29992;&#20110;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TorchCP: A Library for Conformal Prediction based on PyTorch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12683
&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#30340;Python&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#20102;&#29992;&#20110;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#21508;&#31181;&#23454;&#29616;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65288;&#21253;&#25324;&#22810;&#32500;&#36755;&#20986;&#65289;&#12290;TorchCP&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#24182;&#21033;&#29992;&#30697;&#38453;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#31616;&#27905;&#39640;&#25928;&#30340;&#25512;&#29702;&#23454;&#29616;&#12290;&#35813;&#20195;&#30721;&#37319;&#29992;LGPL&#35768;&#21487;&#35777;&#65292;&#24182;&#22312;$\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12683v1 Announce Type: new  Abstract: TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2310.05866</link><description>&lt;p&gt;
&#36890;&#36807;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#24615;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Generative quantum machine learning via denoising diffusion probabilistic models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05866
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;QuDDPM&#65289;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#20197;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#25991;&#26412;&#29983;&#25104;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#20197;&#21450;&#32467;&#26500;&#28789;&#27963;&#12289;&#35757;&#32451;&#31616;&#21333;&#30340;&#29305;&#28857;&#65292;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#37327;&#23376;&#29983;&#25104;&#27169;&#22411;&#21033;&#29992;&#32416;&#32544;&#21644;&#21472;&#21152;&#30340;&#33021;&#21147;&#20026;&#23398;&#20064;&#32463;&#20856;&#21644;&#37327;&#23376;&#25968;&#25454;&#24102;&#26469;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#21463;&#32463;&#20856;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#37327;&#23376;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#8221;&#65288;QuDDPM&#65289;&#65292;&#20197;&#23454;&#29616;&#23545;&#37327;&#23376;&#25968;&#25454;&#30340;&#39640;&#25928;&#21487;&#35757;&#32451;&#30340;&#29983;&#25104;&#23398;&#20064;&#12290;QuDDPM&#37319;&#29992;&#36275;&#22815;&#23618;&#25968;&#30340;&#30005;&#36335;&#26469;&#20445;&#35777;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#24341;&#20837;&#22810;&#20010;&#20013;&#38388;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;&#30446;&#26631;&#20998;&#24067;&#19982;&#22122;&#22768;&#20043;&#38388;&#30340;&#25554;&#20540;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#36991;&#20813;&#36139;&#30240;&#24179;&#21407;&#24182;&#20445;&#35777;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#23398;&#20064;&#35823;&#24046;&#30340;&#19978;&#30028;&#21644;...&#65288;&#26410;&#23436;&#24453;&#32493;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep generative models are key-enabling technology to computer vision, text generation and large language models. Denoising diffusion probabilistic models (DDPMs) have recently gained much attention due to their ability to generate diverse and high-quality samples in many computer vision tasks, as well as to incorporate flexible model architectures and relatively simple training scheme. Quantum generative models, empowered by entanglement and superposition, have brought new insight to learning classical and quantum data. Inspired by the classical counterpart, we propose the \emph{quantum denoising diffusion probabilistic model} (QuDDPM) to enable efficiently trainable generative learning of quantum data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity, while introduces multiple intermediate training tasks as interpolation between the target distribution and noise to avoid barren plateau and guarantee efficient training. We provide bounds on the learning error and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22836;&#37096;&#38598;&#21644;&#23614;&#37096;&#38598;&#30340;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#36830;&#32493;&#30340;&#27493;&#39588;&#65292;&#24182;&#21033;&#29992;&#23450;&#29702;&#35777;&#26126;&#25345;&#32493;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#20197;&#23398;&#20064;&#23614;&#37096;&#65292;&#21516;&#26102;&#19981;&#20250;&#24536;&#35760;&#22836;&#37096;&#12290;</title><link>http://arxiv.org/abs/2306.13275</link><description>&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#33021;&#25913;&#36827;&#38271;&#23614;&#35782;&#21035;&#21527;&#65311;&#36208;&#21521;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Can Continual Learning Improve Long-Tailed Recognition? Toward a Unified Framework. (arXiv:2306.13275v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#38271;&#23614;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22836;&#37096;&#38598;&#21644;&#23614;&#37096;&#38598;&#30340;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#36830;&#32493;&#30340;&#27493;&#39588;&#65292;&#24182;&#21033;&#29992;&#23450;&#29702;&#35777;&#26126;&#25345;&#32493;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#20197;&#23398;&#20064;&#23614;&#37096;&#65292;&#21516;&#26102;&#19981;&#20250;&#24536;&#35760;&#22836;&#37096;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#24230;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#19981;&#21516;&#31867;&#21035;&#20043;&#38388;&#30340;&#26679;&#26412;&#25968;&#37327;&#26497;&#24230;&#22833;&#34913;&#20250;&#20986;&#29616;&#38271;&#23614;&#35782;&#21035;&#65288;LTR&#65289;&#38382;&#39064;&#12290;LTR&#26041;&#27861;&#26088;&#22312;&#20934;&#30830;&#22320;&#23398;&#20064;&#21253;&#21547;&#19968;&#20010;&#36739;&#22823;&#8220;&#22836;&#8221;&#38598;&#21644;&#19968;&#20010;&#36739;&#23567;&#8220;&#23614;&#8221;&#38598;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#20551;&#35774;&#25439;&#22833;&#20989;&#25968;&#26159;&#24378;&#20984;&#30340;&#65292;&#37027;&#20040;&#23436;&#25972;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#22312;&#21516;&#19968;&#20010;&#23398;&#20064;&#32773;&#20005;&#26684;&#35757;&#32451;&#22836;&#38598;&#26102;&#30340;&#26435;&#37325;&#19978;&#38480;&#20043;&#20869;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#22768;&#31216;&#23558;&#22836;&#38598;&#21644;&#23614;&#38598;&#30340;&#23398;&#20064;&#35270;&#20026;&#20004;&#20010;&#29420;&#31435;&#30340;&#36830;&#32493;&#27493;&#39588;&#65292;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26356;&#26032;&#23398;&#20064;&#32773;&#30340;&#26435;&#37325;&#20197;&#23398;&#20064;&#23614;&#37096;&#65292;&#32780;&#19981;&#20250;&#24536;&#35760;&#22836;&#37096;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#29609;&#20855;MNIST-LT&#25968;&#25454;&#38598;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26631;&#20934;LTR&#22522;&#20934;&#65288;CIFAR100-LT&#21644;CIFAR10-L&#65289;&#30340;&#22810;&#20010;&#19981;&#24179;&#34913;&#21464;&#20307;&#19978;&#35780;&#20272;&#20102;&#20960;&#31181;CL&#31574;&#30053;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Long-Tailed Recognition (LTR) problem emerges in the context of learning from highly imbalanced datasets, in which the number of samples among different classes is heavily skewed. LTR methods aim to accurately learn a dataset comprising both a larger Head set and a smaller Tail set. We propose a theorem where under the assumption of strong convexity of the loss function, the weights of a learner trained on the full dataset are within an upper bound of the weights of the same learner trained strictly on the Head. Next, we assert that by treating the learning of the Head and Tail as two separate and sequential steps, Continual Learning (CL) methods can effectively update the weights of the learner to learn the Tail without forgetting the Head. First, we validate our theoretical findings with various experiments on the toy MNIST-LT dataset. We then evaluate the efficacy of several CL strategies on multiple imbalanced variations of two standard LTR benchmarks (CIFAR100-LT and CIFAR10-L
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#26679;&#22810;&#20803;&#37325;&#23614;&#20998;&#24067;&#30340;VAE&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#20803;&#26497;&#20540;&#24773;&#20917;&#65292;&#22914;&#27827;&#27969;&#27700;&#20301;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#35780;&#20272;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2306.10987</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22810;&#20803;&#26497;&#20540;&#30340;VAE&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A VAE Approach to Sample Multivariate Extremes. (arXiv:2306.10987v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#26679;&#22810;&#20803;&#37325;&#23614;&#20998;&#24067;&#30340;VAE&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#19990;&#30028;&#30340;&#22810;&#20803;&#26497;&#20540;&#24773;&#20917;&#65292;&#22914;&#27827;&#27969;&#27700;&#20301;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#35780;&#20272;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25105;&#20204;&#38656;&#35201;&#35780;&#20272;&#26410;&#26469;&#21487;&#33021;&#20250;&#20986;&#29616;&#30340;&#27604;&#24050;&#35266;&#23519;&#21040;&#30340;&#26497;&#20540;&#26356;&#22823;&#30340;&#26497;&#31471;&#24773;&#20917;&#30340;&#39118;&#38505;&#26102;&#65292;&#20174;&#35266;&#27979;&#25968;&#25454;&#38598;&#20013;&#20934;&#30830;&#22320;&#29983;&#25104;&#26497;&#20540;&#33267;&#20851;&#37325;&#35201;&#12290; &#24212;&#29992;&#33539;&#22260;&#21253;&#25324;&#33258;&#28982;&#28798;&#23475;&#21644;&#37329;&#34701;&#23849;&#28291;&#12290; &#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#29983;&#25104;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#26497;&#31471;&#26679;&#26412;&#65292;&#38656;&#35201;&#20180;&#32454;&#36866;&#24212;&#12290;&#27492;&#22806;&#65292;&#26497;&#20540;&#29702;&#35770;&#30340;&#28176;&#36827;&#32467;&#26524;&#25552;&#20379;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#23588;&#20854;&#26159;&#36890;&#36807;&#22810;&#20803;&#27491;&#21017;&#21464;&#21270;&#30340;&#27010;&#24565;&#26469;&#27169;&#25311;&#22810;&#20803;&#26497;&#31471;&#20107;&#20214;&#12290; &#36830;&#25509;&#36825;&#20004;&#20010;&#39046;&#22495;&#65292;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#25277;&#26679;&#22810;&#20803;&#37325;&#23614;&#20998;&#24067;&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26041;&#27861;&#65292;&#21363;&#21487;&#33021;&#20855;&#26377;&#29305;&#21035;&#22823;&#24378;&#24230;&#30340;&#26497;&#31471;&#20998;&#24067;&#12290; &#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#27839;&#22810;&#29785;&#27827;&#32593;&#32476;&#30340;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#30456;&#20851;&#24615;&#12290; &#21518;&#32773;&#26174;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#24050;&#35266;&#23519;&#21040;&#30340;&#26497;&#20540;&#20998;&#24067;&#20013;&#25277;&#26679;&#26469;&#27169;&#25311;&#27827;&#27969;&#27700;&#20301;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating accurate extremes from an observational data set is crucial when seeking to estimate risks associated with the occurrence of future extremes which could be larger than those already observed. Applications range from the occurrence of natural disasters to financial crashes. Generative approaches from the machine learning community do not apply to extreme samples without careful adaptation. Besides, asymptotic results from extreme value theory (EVT) give a theoretical framework to model multivariate extreme events, especially through the notion of multivariate regular variation. Bridging these two fields, this paper details a variational autoencoder (VAE) approach for sampling multivariate heavy-tailed distributions, i.e., distributions likely to have extremes of particularly large intensities. We illustrate the relevance of our approach on a synthetic data set and on a real data set of discharge measurements along the Danube river network. The latter shows the potential of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;LLM&#20195;&#29702;&#31243;&#24207;&#30340;&#24615;&#33021;&#26469;&#23454;&#29616;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.02662</link><description>&lt;p&gt;
&#22312;&#20132;&#20114;&#29615;&#22659;&#20013;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;
&lt;/p&gt;
&lt;p&gt;
Grounding Large Language Models in Interactive Environments with Online Reinforcement Learning. (arXiv:2302.02662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#21517;&#20026;GLAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#39640;LLM&#20195;&#29702;&#31243;&#24207;&#30340;&#24615;&#33021;&#26469;&#23454;&#29616;LLMs&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#65292;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25104;&#21151;&#22320;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25429;&#25417;&#19990;&#30028;&#29289;&#29702;&#30340;&#25277;&#35937;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#20915;&#31574;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;LLMs&#30340;&#30693;&#35782;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#23545;&#40784;&#21487;&#33021;&#26159;&#38169;&#35823;&#30340;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#32780;&#38480;&#21046;&#20102;&#20854;&#21151;&#33021;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#21151;&#33021;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#30340;&#26041;&#27861;&#65288;&#31216;&#20026;GLAM&#65289;&#65306;&#25105;&#20204;&#32771;&#34385;&#19968;&#20010;&#20351;&#29992;LLM&#20316;&#20026;&#31574;&#30053;&#30340;&#20195;&#29702;&#31243;&#24207;&#65292;&#38543;&#30528;&#20195;&#29702;&#31243;&#24207;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#32780;&#36880;&#27493;&#26356;&#26032;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#25552;&#39640;&#20854;&#35299;&#20915;&#30446;&#26631;&#30340;&#24615;&#33021;&#12290;&#20351;&#29992;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#25991;&#26412;&#29615;&#22659;&#35774;&#35745;&#26469;&#30740;&#31350;&#26356;&#39640;&#32423;&#24418;&#24335;&#30340;&#22522;&#30784;&#35774;&#26045;&#24314;&#35774;&#65292;&#20197;&#21450;&#19968;&#32452;&#31354;&#38388;&#21644;&#23548;&#33322;&#20219;&#21153;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#31185;&#23398;&#38382;&#39064;&#65306;1&#65289;LLMs&#33021;&#21542;&#25552;&#39640;&#21508;&#31181;RL&#20219;&#21153;&#30340;&#22312;&#32447;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#65311;2&#65289;&#23427;&#22914;&#20309;&#25552;&#39640;&#19981;&#21516;&#24418;&#24335;&#30340;&#27867;&#21270;&#65311;3&#65289;&#22312;&#32447;&#23398;&#20064;&#30340;&#24433;&#21709;&#26159;&#20160;&#20040;&#65311;&#25105;&#20204;&#36890;&#36807;&#21151;&#33021;&#26041;&#24335;&#30740;&#31350;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works successfully leveraged Large Language Models' (LLM) abilities to capture abstract knowledge about world's physics to solve decision-making problems. Yet, the alignment between LLMs' knowledge and the environment can be wrong and limit functional competence due to lack of grounding. In this paper, we study an approach (named GLAM) to achieve this alignment through functional grounding: we consider an agent using an LLM as a policy that is progressively updated as the agent interacts with the environment, leveraging online Reinforcement Learning to improve its performance to solve goals. Using an interactive textual environment designed to study higher-level forms of functional grounding, and a set of spatial and navigation tasks, we study several scientific questions: 1) Can LLMs boost sample efficiency for online learning of various RL tasks? 2) How can it boost different forms of generalization? 3) What is the impact of online learning? We study these questions by functio
&lt;/p&gt;</description></item></channel></rss>