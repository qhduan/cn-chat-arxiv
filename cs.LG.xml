<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#65292;&#36890;&#36807;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#21644;&#21464;&#20998;&#32467;&#26524;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#26377;&#38480;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.08750</link><description>&lt;p&gt;
&#31070;&#32463;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#21644;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
Neural reproducing kernel Banach spaces and representer theorems for deep networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#22312;&#36825;&#20123;&#31354;&#38388;&#20013;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#65292;&#36890;&#36807;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#21644;&#21464;&#20998;&#32467;&#26524;&#24471;&#20986;&#20102;&#36866;&#29992;&#20110;&#23454;&#38469;&#20013;&#24120;&#35265;&#26377;&#38480;&#28145;&#24230;&#32593;&#32476;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30001;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#30340;&#20989;&#25968;&#31354;&#38388;&#26377;&#21161;&#20110;&#29702;&#35299;&#30456;&#24212;&#30340;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#24402;&#32435;&#20559;&#24046;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23450;&#20041;&#20102;&#36866;&#24403;&#30340;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#65292;&#36825;&#20123;&#31354;&#38388;&#37197;&#22791;&#26377;&#24378;&#21046;&#31232;&#30095;&#24615;&#30340;&#33539;&#25968;&#65292;&#20351;&#20854;&#33021;&#22815;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#21450;&#20854;&#34920;&#31034;&#20013;&#28508;&#22312;&#32467;&#26500;&#12290;&#22522;&#20110;&#20877;&#29983;&#26680;&#24052;&#25343;&#36203;&#31354;&#38388;&#29702;&#35770;&#65292;&#32467;&#21512;&#21464;&#20998;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#35777;&#26126;&#22312;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#26377;&#38480;&#26550;&#26500;&#30340;&#34920;&#29616;&#23450;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25193;&#23637;&#20102;&#27973;&#23618;&#32593;&#32476;&#30340;&#31867;&#20284;&#32467;&#26524;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#26041;&#21521;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08750v1 Announce Type: cross  Abstract: Studying the function spaces defined by neural networks helps to understand the corresponding learning models and their inductive bias. While in some limits neural networks correspond to function spaces that are reproducing kernel Hilbert spaces, these regimes do not capture the properties of the networks used in practice. In contrast, in this paper we show that deep neural networks define suitable reproducing kernel Banach spaces.   These spaces are equipped with norms that enforce a form of sparsity, enabling them to adapt to potential latent structures within the input data and their representations. In particular, leveraging the theory of reproducing kernel Banach spaces, combined with variational results, we derive representer theorems that justify the finite architectures commonly employed in applications. Our study extends analogous results for shallow networks and can be seen as a step towards considering more practically plaus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00019</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion MRI with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#25955;&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#35780;&#20272;&#22823;&#33041;&#24494;&#32467;&#26500;&#21644;&#32467;&#26500;&#36830;&#25509;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;dMRI&#25968;&#25454;&#20197;&#25552;&#21462;&#20020;&#24202;&#21644;&#31185;&#23398;&#30446;&#30340;&#30340;&#26377;&#29992;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; dMRI&#27979;&#37327;&#36890;&#24120;&#21463;&#21040;&#24378;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#24178;&#25200;&#65292;&#25968;&#25454;&#20013;&#36890;&#24120;&#23384;&#22312;&#39640;&#30340;&#20250;&#35805;&#38388;&#21644;&#25195;&#25551;&#32773;&#38388;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#22823;&#33041;&#32467;&#26500;&#30340;&#30456;&#24403;&#22823;&#30340;&#20010;&#20307;&#38388;&#21464;&#24322;&#65292;&#24182;&#19988;&#27979;&#37327;&#21644;&#24863;&#20852;&#36259;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;dMRI&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#23581;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#24050;&#32463;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20027;&#35201;&#21457;&#29616;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-weighted magnetic resonance imaging (dMRI) offers unique capabilities such as noninvasive assessment of brain's micro-structure and structural connectivity. However, analyzing the dMRI data to extract useful information for clinical and scientific purposes is challenging. The dMRI measurements often suffer from strong noise and artifacts, there is usually high inter-session and inter-scanner heterogeneity in the data and considerable inter-subject variability in brain structure, and the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed micro-structure mapping, tractography, white matter tract analysis, as well as data preprocessing and harmonization. We summarize the main findings, strengths, and weaknesses of the existing methods and suggest topics for future re
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#20998;&#26512;Transformers&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#38271;&#26102;&#38388;&#19979;&#30340;&#38598;&#22242;&#24418;&#25104;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2312.10794</link><description>&lt;p&gt;
Transformers&#30340;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
A mathematical perspective on Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10794
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#23398;&#26694;&#26550;&#29992;&#20110;&#20998;&#26512;Transformers&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#38271;&#26102;&#38388;&#19979;&#30340;&#38598;&#22242;&#24418;&#25104;&#12290;&#36825;&#19968;&#30740;&#31350;&#20026;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#25105;&#20204;&#22522;&#20110;&#23558;Transformers&#35299;&#37322;&#20026;&#30456;&#20114;&#20316;&#29992;&#30340;&#31890;&#23376;&#31995;&#32479;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#26469;&#20998;&#26512;Transformers&#65292;&#25581;&#31034;&#20102;&#38271;&#26102;&#38388;&#19979;&#30340;&#38598;&#22242;&#24418;&#25104;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#28508;&#22312;&#30340;&#29702;&#35770;&#65292;&#24182;&#20026;&#25968;&#23398;&#23478;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#23478;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers play a central role in the inner workings of large language models. We develop a mathematical framework for analyzing Transformers based on their interpretation as interacting particle systems, which reveals that clusters emerge in long time. Our study explores the underlying theory and offers new perspectives for mathematicians as well as computer scientists.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#38454;&#27573;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#30340;&#31639;&#27861;&#21644;&#30456;&#24212;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#32553;&#25918;&#22240;&#23376;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#32422;&#26463;&#28385;&#36275;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#39640;&#27010;&#29575;&#21644;&#26399;&#26395;&#35774;&#32622;&#65292;&#24182;&#22312;&#22810;&#20010;&#32422;&#26463;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2401.08016</link><description>&lt;p&gt;
&#20855;&#26377;&#38454;&#27573;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Bandits with Stage-wise Constraints. (arXiv:2401.08016v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20855;&#26377;&#38454;&#27573;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#30340;&#31639;&#27861;&#21644;&#30456;&#24212;&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#32553;&#25918;&#22240;&#23376;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#32422;&#26463;&#28385;&#36275;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#36866;&#24212;&#39640;&#27010;&#29575;&#21644;&#26399;&#26395;&#35774;&#32622;&#65292;&#24182;&#22312;&#22810;&#20010;&#32422;&#26463;&#24773;&#20917;&#19979;&#24471;&#21040;&#20102;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#32422;&#26463;&#38382;&#39064;&#24517;&#39035;&#28385;&#36275;&#39640;&#27010;&#29575;&#21644;&#26399;&#26395;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#22312;&#38454;&#27573;&#32422;&#26463;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#26174;&#28982;&#65292;&#26399;&#26395;&#32422;&#26463;&#30340;&#35774;&#23450;&#26159;&#23545;&#39640;&#27010;&#29575;&#32422;&#26463;&#30340;&#25918;&#23485;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#32447;&#24615;&#24773;&#20917;&#24320;&#22987;&#65292;&#20854;&#20013;&#19978;&#19979;&#25991;&#36172;&#21338;&#26426;&#38382;&#39064;&#65288;&#22870;&#21169;&#20989;&#25968;&#65289;&#21644;&#38454;&#27573;&#32422;&#26463;&#65288;&#25104;&#26412;&#20989;&#25968;&#65289;&#37117;&#26159;&#32447;&#24615;&#30340;&#12290;&#22312;&#39640;&#27010;&#29575;&#21644;&#26399;&#26395;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#38480;&#32622;&#20449;&#21306;&#38388;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#27492;&#38382;&#39064;&#30340;T&#36718;&#36951;&#25022;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#24605;&#24819;&#26469;&#24179;&#34913;&#25506;&#32034;&#21644;&#32422;&#26463;&#28385;&#36275;&#65292;&#36890;&#36807;&#19981;&#21516;&#30340;&#32553;&#25918;&#22240;&#23376;&#32553;&#25918;&#22870;&#21169;&#21644;&#25104;&#26412;&#32622;&#20449;&#21306;&#38388;&#30340;&#21322;&#24452;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#35813;&#32422;&#26463;&#38382;&#39064;&#30340;&#19979;&#30028;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#22914;&#20309;&#25193;&#23637;&#21040;&#22810;&#20010;&#32422;&#26463;&#65292;&#24182;&#25552;&#20379;&#20102;&#27169;&#25311;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study contextual bandits in the presence of a stage-wise constraint (a constraint at each round), when the constraint must be satisfied both with high probability and in expectation. Obviously the setting where the constraint is in expectation is a relaxation of the one with high probability. We start with the linear case where both the contextual bandit problem (reward function) and the stage-wise constraint (cost function) are linear. In each of the high probability and in expectation settings, we propose an upper-confidence bound algorithm for the problem and prove a $T$-round regret bound for it. Our algorithms balance exploration and constraint satisfaction using a novel idea that scales the radii of the reward and cost confidence sets with different scaling factors. We also prove a lower-bound for this constrained problem, show how our algorithms and analyses can be extended to multiple constraints, and provide simulations to validate our theoretical results. In the high proba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20165;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21363;&#21487;&#22312;&#36817;&#32447;&#24615;&#26102;&#38388;&#20869;&#20272;&#35745;&#31232;&#30095;&#22343;&#20540;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.15276</link><description>&lt;p&gt;
&#22686;&#37327;&#23398;&#20064;&#19979;&#30340;&#31232;&#30095;&#22343;&#20540;&#40065;&#26834;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Sparse Mean Estimation via Incremental Learning. (arXiv:2305.15276v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#20165;&#38656;&#35201;&#36739;&#23569;&#30340;&#26679;&#26412;&#21363;&#21487;&#22312;&#36817;&#32447;&#24615;&#26102;&#38388;&#20869;&#20272;&#35745;&#31232;&#30095;&#22343;&#20540;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#20272;&#35745;&#22120;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31232;&#30095;&#22343;&#20540;&#30340;&#40065;&#26834;&#24615;&#20272;&#35745;&#38382;&#39064;&#65292;&#26088;&#22312;&#20272;&#35745;&#20174;&#37325;&#23614;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#37096;&#20998;&#25439;&#22351;&#26679;&#26412;&#30340;$k$-&#31232;&#30095;&#22343;&#20540;&#12290;&#29616;&#26377;&#20272;&#35745;&#22120;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#38754;&#20020;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#23427;&#20204;&#21463;&#21040;&#19968;&#20010;&#34987;&#25512;&#27979;&#30340;&#35745;&#31639;&#32479;&#35745;&#26435;&#34913;&#30340;&#38480;&#21046;&#65292;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#31639;&#27861;&#38656;&#35201;$\tilde\Omega(k^2)$&#20010;&#26679;&#26412;&#65292;&#32780;&#20854;&#22312;&#32479;&#35745;&#19978;&#26368;&#20248;&#30340;&#23545;&#24212;&#29289;&#21482;&#38656;&#35201;$\tilde O(k)$&#20010;&#26679;&#26412;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#30340;&#20272;&#35745;&#22120;&#35268;&#27169;&#38543;&#30528;&#29615;&#22659;&#30340;&#32500;&#24230;&#22686;&#21152;&#32780;&#24613;&#21095;&#19978;&#21319;&#65292;&#38590;&#20197;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#22312;&#36866;&#24230;&#30340;&#26465;&#20214;&#19979;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#25361;&#25112;&#65306;&#23427;&#22312;&#20960;&#20046;&#32447;&#24615;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#20013;&#36816;&#34892;&#65288;&#30456;&#23545;&#20110;&#29615;&#22659;&#32500;&#24230;&#65289;&#65292;&#21516;&#26102;&#21482;&#38656;&#35201;$\tilde O(k)$&#20010;&#26679;&#26412;&#26469;&#24674;&#22797;&#30495;&#23454;&#30340;&#22343;&#20540;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#22686;&#37327;&#23398;&#20064;&#29616;&#35937;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38750;&#20984;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#23558;&#22343;&#20540;&#20272;&#35745;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#22522;&#20110;&#22686;&#37327;&#23398;&#20064;&#30340;&#31639;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the problem of robust sparse mean estimation, where the goal is to estimate a $k$-sparse mean from a collection of partially corrupted samples drawn from a heavy-tailed distribution. Existing estimators face two critical challenges in this setting. First, they are limited by a conjectured computational-statistical tradeoff, implying that any computationally efficient algorithm needs $\tilde\Omega(k^2)$ samples, while its statistically-optimal counterpart only requires $\tilde O(k)$ samples. Second, the existing estimators fall short of practical use as they scale poorly with the ambient dimension. This paper presents a simple mean estimator that overcomes both challenges under moderate conditions: it runs in near-linear time and memory (both with respect to the ambient dimension) while requiring only $\tilde O(k)$ samples to recover the true mean. At the core of our method lies an incremental learning phenomenon: we introduce a simple nonconvex framework that ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#26041;&#27861;&#65292;&#20174;&#22240;&#26524;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#24433;&#21709;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#65292;&#21253;&#25324;&#20116;&#31181;&#20027;&#35201;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#12289;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#21644;&#31038;&#20132;&#23186;&#20307;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.16148</link><description>&lt;p&gt;
&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#22240;&#32032;&#30340;&#24314;&#27169;&#65306;&#19968;&#31181;&#36125;&#21494;&#26031;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Modelling Determinants of Cryptocurrency Prices: A Bayesian Network Approach. (arXiv:2303.16148v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#36125;&#21494;&#26031;&#32593;&#32476;&#26041;&#27861;&#65292;&#20174;&#22240;&#26524;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#24433;&#21709;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#65292;&#21253;&#25324;&#20116;&#31181;&#20027;&#35201;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#12289;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#21644;&#31038;&#20132;&#23186;&#20307;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#39044;&#27979;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24066;&#22330;&#24635;&#20540;&#21644;&#26367;&#20195;&#27604;&#29305;&#24065;&#30340;&#21152;&#23494;&#36135;&#24065;&#25968;&#37327;&#30340;&#22686;&#38271;&#25552;&#20379;&#20102;&#25237;&#36164;&#26426;&#20250;&#65292;&#21516;&#26102;&#20063;&#22686;&#21152;&#20102;&#39044;&#27979;&#20854;&#20215;&#26684;&#27874;&#21160;&#30340;&#22797;&#26434;&#24230;&#12290;&#22312;&#36825;&#20010;&#27874;&#21160;&#24615;&#30456;&#23545;&#36739;&#24369;&#30340;&#24066;&#22330;&#20013;&#65292;&#39044;&#27979;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#26159;&#38656;&#35201;&#30830;&#23450;&#24433;&#21709;&#20215;&#26684;&#30340;&#22240;&#32032;&#12290;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#26159;&#20174;&#22240;&#26524;&#20998;&#26512;&#30340;&#35282;&#24230;&#30740;&#31350;&#24433;&#21709;&#26367;&#20195;&#27604;&#29305;&#24065;&#20215;&#26684;&#30340;&#22240;&#32032;&#65292;&#29305;&#21035;&#22320;&#65292;&#30740;&#31350;&#20102;&#20116;&#20010;&#20027;&#35201;&#30340;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#65292;&#21253;&#25324;&#40644;&#37329;&#12289;&#30707;&#27833;&#21644;&#26631;&#20934;&#26222;&#23572;500&#25351;&#25968;&#31561;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#20197;&#21450;&#31038;&#20132;&#23186;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#30001;&#20116;&#20010;&#20256;&#32479;&#37329;&#34701;&#36164;&#20135;&#30340;&#21382;&#21490;&#20215;&#26684;&#25968;&#25454;&#12289;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#21644;&#26367;&#20195;&#21152;&#23494;&#36135;&#24065;&#20215;&#26684;&#25968;&#25454;&#26500;&#25104;&#30340;&#22240;&#26524;&#32593;&#32476;&#65292;&#36825;&#20123;&#32593;&#32476;&#29992;&#20110;&#22240;&#26524;&#25512;&#29702;&#21644;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growth of market capitalisation and the number of altcoins (cryptocurrencies other than Bitcoin) provide investment opportunities and complicate the prediction of their price movements. A significant challenge in this volatile and relatively immature market is the problem of predicting cryptocurrency prices which needs to identify the factors influencing these prices. The focus of this study is to investigate the factors influencing altcoin prices, and these factors have been investigated from a causal analysis perspective using Bayesian networks. In particular, studying the nature of interactions between five leading altcoins, traditional financial assets including gold, oil, and S\&amp;P 500, and social media is the research question. To provide an answer to the question, we create causal networks which are built from the historic price data of five traditional financial assets, social media data, and price data of altcoins. The ensuing networks are used for causal reasoning and diag
&lt;/p&gt;</description></item></channel></rss>