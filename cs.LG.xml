<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SVGCraft&#24341;&#20837;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#25551;&#32472;&#25972;&#20010;&#22330;&#26223;&#30340;&#30690;&#37327;&#22270;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24067;&#23616;&#29983;&#25104;&#12289;&#20135;&#29983;&#36974;&#32617;&#28508;&#21464;&#37327;&#20197;&#36827;&#34892;&#20934;&#30830;&#23545;&#35937;&#25918;&#32622;&#12289;&#34701;&#21512;&#27880;&#24847;&#21147;&#22270;&#20197;&#21450;&#20351;&#29992;&#25193;&#25955;U-Net&#36827;&#34892;&#21512;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;LPIPS&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2404.00412</link><description>&lt;p&gt;
SVGCraft:&#36229;&#36234;&#21333;&#20010;&#30446;&#26631;&#25991;&#23383;&#21040;SVG&#32508;&#21512;&#30011;&#24067;&#24067;&#23616;
&lt;/p&gt;
&lt;p&gt;
SVGCraft: Beyond Single Object Text-to-SVG Synthesis with Comprehensive Canvas Layout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00412
&lt;/p&gt;
&lt;p&gt;
SVGCraft&#24341;&#20837;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#25551;&#32472;&#25972;&#20010;&#22330;&#26223;&#30340;&#30690;&#37327;&#22270;&#65292;&#20854;&#20013;&#21253;&#25324;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#36827;&#34892;&#24067;&#23616;&#29983;&#25104;&#12289;&#20135;&#29983;&#36974;&#32617;&#28508;&#21464;&#37327;&#20197;&#36827;&#34892;&#20934;&#30830;&#23545;&#35937;&#25918;&#32622;&#12289;&#34701;&#21512;&#27880;&#24847;&#21147;&#22270;&#20197;&#21450;&#20351;&#29992;&#25193;&#25955;U-Net&#36827;&#34892;&#21512;&#25104;&#65292;&#21516;&#26102;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;LPIPS&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20174;&#25991;&#26412;&#25552;&#31034;&#21040;&#30690;&#37327;&#22270;&#30340;VectorArt&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#35273;&#20219;&#21153;&#65292;&#38656;&#35201;&#23545;&#24050;&#30693;&#21644;&#26410;&#30693;&#23454;&#20307;&#36827;&#34892;&#22810;&#26679;&#21270;&#32780;&#30495;&#23454;&#30340;&#25551;&#36848;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#23616;&#38480;&#20110;&#29983;&#25104;&#21333;&#20010;&#23545;&#35937;&#65292;&#32780;&#19981;&#26159;&#30001;&#22810;&#20010;&#20803;&#32032;&#32452;&#25104;&#30340;&#22330;&#26223;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;SVGCraft&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#25551;&#32472;&#25972;&#20010;&#22330;&#26223;&#30340;&#30690;&#37327;&#22270;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;LLM&#20174;&#25991;&#26412;&#25552;&#31034;&#29983;&#25104;&#24067;&#23616;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#36890;&#36807;&#29983;&#20135;&#29305;&#23450;&#36793;&#30028;&#26694;&#20013;&#30340;&#25513;&#33180;&#28508;&#21464;&#37327;&#23454;&#29616;&#20934;&#30830;&#30340;&#23545;&#35937;&#25918;&#32622;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#34701;&#21512;&#26426;&#21046;&#65292;&#29992;&#20110;&#38598;&#25104;&#27880;&#24847;&#21147;&#22270;&#65292;&#24182;&#20351;&#29992;&#25193;&#25955;U-Net&#36827;&#34892;&#36830;&#36143;&#30340;&#21512;&#25104;&#65292;&#21152;&#24555;&#32472;&#22270;&#36807;&#31243;&#12290;&#29983;&#25104;&#30340;SVG&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;&#21644;LPIPS&#25439;&#22833;&#36827;&#34892;&#20248;&#21270;&#65292;&#36890;&#36807;&#36879;&#26126;&#24230;&#35843;&#21046;&#26469;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00412v1 Announce Type: cross  Abstract: Generating VectorArt from text prompts is a challenging vision task, requiring diverse yet realistic depictions of the seen as well as unseen entities. However, existing research has been mostly limited to the generation of single objects, rather than comprehensive scenes comprising multiple elements. In response, this work introduces SVGCraft, a novel end-to-end framework for the creation of vector graphics depicting entire scenes from textual descriptions. Utilizing a pre-trained LLM for layout generation from text prompts, this framework introduces a technique for producing masked latents in specified bounding boxes for accurate object placement. It introduces a fusion mechanism for integrating attention maps and employs a diffusion U-Net for coherent composition, speeding up the drawing process. The resulting SVG is optimized using a pre-trained encoder and LPIPS loss with opacity modulation to maximize similarity. Additionally, th
&lt;/p&gt;</description></item><item><title>CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;</title><link>https://arxiv.org/abs/2403.07728</link><description>&lt;p&gt;
CAS: &#19968;&#31181;&#20855;&#26377;FCR&#25511;&#21046;&#30340;&#22312;&#32447;&#36873;&#25321;&#24615;&#31526;&#21512;&#39044;&#27979;&#30340;&#36890;&#29992;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAS: A General Algorithm for Online Selective Conformal Prediction with FCR Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07728
&lt;/p&gt;
&lt;p&gt;
CAS&#26694;&#26550;&#20801;&#35768;&#22312;&#22312;&#32447;&#36873;&#25321;&#24615;&#39044;&#27979;&#20013;&#25511;&#21046;FCR&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#21644;&#26657;&#20934;&#38598;&#26500;&#36896;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#26041;&#24335;&#19979;&#21518;&#36873;&#25321;&#39044;&#27979;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#23558;&#36164;&#28304;&#32791;&#36153;&#22312;&#19981;&#37325;&#35201;&#30340;&#21333;&#20301;&#19978;&#65292;&#22312;&#25253;&#21578;&#20854;&#39044;&#27979;&#21306;&#38388;&#20043;&#21069;&#23545;&#24403;&#21069;&#20010;&#20307;&#36827;&#34892;&#21021;&#27493;&#36873;&#25321;&#22312;&#22312;&#32447;&#39044;&#27979;&#20219;&#21153;&#20013;&#26159;&#24120;&#35265;&#19988;&#26377;&#24847;&#20041;&#30340;&#12290;&#30001;&#20110;&#22312;&#32447;&#36873;&#25321;&#23548;&#33268;&#25152;&#36873;&#39044;&#27979;&#21306;&#38388;&#20013;&#23384;&#22312;&#26102;&#38388;&#22810;&#37325;&#24615;&#65292;&#22240;&#27492;&#25511;&#21046;&#23454;&#26102;&#35823;&#35206;&#30422;&#38472;&#36848;&#29575;&#65288;FCR&#65289;&#26469;&#27979;&#37327;&#24179;&#22343;&#35823;&#35206;&#30422;&#35823;&#24046;&#26159;&#37325;&#35201;&#30340;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;CAS&#65288;&#36866;&#24212;&#24615;&#36873;&#25321;&#21518;&#26657;&#20934;&#65289;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#21253;&#35065;&#20219;&#20309;&#39044;&#27979;&#27169;&#22411;&#21644;&#22312;&#32447;&#36873;&#25321;&#35268;&#21017;&#65292;&#20197;&#36755;&#20986;&#21518;&#36873;&#25321;&#30340;&#39044;&#27979;&#21306;&#38388;&#12290;&#22914;&#26524;&#36873;&#25321;&#20102;&#24403;&#21069;&#20010;&#20307;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#21382;&#21490;&#25968;&#25454;&#36827;&#34892;&#33258;&#36866;&#24212;&#36873;&#25321;&#26469;&#26500;&#24314;&#26657;&#20934;&#38598;&#65292;&#28982;&#21518;&#20026;&#26410;&#35266;&#23519;&#21040;&#30340;&#26631;&#31614;&#36755;&#20986;&#31526;&#21512;&#39044;&#27979;&#21306;&#38388;&#12290;&#25105;&#20204;&#20026;&#26657;&#20934;&#38598;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26500;&#36896;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07728v1 Announce Type: cross  Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) to measure the averaged miscoverage error. We develop a general framework named CAS (Calibration after Adaptive Selection) that can wrap around any prediction model and online selection rule to output post-selection prediction intervals. If the current individual is selected, we first perform an adaptive selection on historical data to construct a calibration set, then output a conformal prediction interval for the unobserved label. We provide tractable constructions for the calibration set for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GuideGen&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#32852;&#21512;&#29983;&#25104;CT&#22270;&#20687;&#21644;&#33145;&#37096;&#22120;&#23448;&#20197;&#21450;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#25513;&#33180;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#26032;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.07247</link><description>&lt;p&gt;
GuideGen&#65306;&#19968;&#31181;&#29992;&#20110;&#32852;&#21512;CT&#20307;&#31215;&#21644;&#35299;&#21078;&#32467;&#26500;&#29983;&#25104;&#30340;&#25991;&#26412;&#24341;&#23548;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
GuideGen: A Text-guided Framework for Joint CT Volume and Anatomical structure Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07247
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GuideGen&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#32852;&#21512;&#29983;&#25104;CT&#22270;&#20687;&#21644;&#33145;&#37096;&#22120;&#23448;&#20197;&#21450;&#32467;&#30452;&#32928;&#30284;&#32452;&#32455;&#25513;&#33180;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#39046;&#22495;&#25552;&#20379;&#20102;&#19968;&#31181;&#29983;&#25104;&#25968;&#25454;&#38598;&#30340;&#26032;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07247v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#20026;&#20102;&#25910;&#38598;&#24102;&#26377;&#22270;&#20687;&#21644;&#30456;&#24212;&#26631;&#31614;&#30340;&#22823;&#22411;&#21307;&#23398;&#25968;&#25454;&#38598;&#32780;&#36827;&#34892;&#30340;&#27880;&#37322;&#36127;&#25285;&#21644;&#22823;&#37327;&#24037;&#20316;&#24456;&#23569;&#26159;&#21010;&#31639;&#19988;&#20196;&#20154;&#26395;&#32780;&#29983;&#30031;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#32570;&#20047;&#20016;&#23500;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#21066;&#24369;&#20102;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#21152;&#21095;&#20102;&#21307;&#23398;&#39046;&#22495;&#38754;&#20020;&#30340;&#22270;&#20687;&#20998;&#26512;&#25361;&#25112;&#12290;&#20316;&#20026;&#19968;&#31181;&#26435;&#23452;&#20043;&#35745;&#65292;&#37492;&#20110;&#29983;&#25104;&#24615;&#31070;&#32463;&#27169;&#22411;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#29616;&#22312;&#21487;&#20197;&#22312;&#22806;&#37096;&#32422;&#26463;&#30340;&#24341;&#23548;&#19979;&#20197;&#39640;&#20445;&#30495;&#24230;&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;GuideGen&#65306;&#19968;&#31181;&#32852;&#21512;&#29983;&#25104;&#33145;&#37096;&#22120;&#23448;&#21644;&#32467;&#30452;&#32928;&#30284;CT&#22270;&#20687;&#21644;&#32452;&#32455;&#25513;&#33180;&#30340;&#31649;&#32447;&#65292;&#20854;&#21463;&#25991;&#26412;&#25552;&#31034;&#26465;&#20214;&#32422;&#26463;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20307;&#31215;&#25513;&#33180;&#37319;&#26679;&#22120;&#65292;&#20197;&#36866;&#24212;&#25513;&#33180;&#26631;&#31614;&#30340;&#31163;&#25955;&#20998;&#24067;&#24182;&#29983;&#25104;&#20302;&#20998;&#36776;&#29575;3D&#32452;&#32455;&#25513;&#33180;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#22120;&#20250;&#22312;&#25910;&#21040;&#30456;&#24212;&#25991;&#26412;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#33258;&#22238;&#24402;&#29983;&#25104;CT&#20999;&#29255;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07247v1 Announce Type: cross  Abstract: The annotation burden and extensive labor for gathering a large medical dataset with images and corresponding labels are rarely cost-effective and highly intimidating. This results in a lack of abundant training data that undermines downstream tasks and partially contributes to the challenge image analysis faces in the medical field. As a workaround, given the recent success of generative neural models, it is now possible to synthesize image datasets at a high fidelity guided by external constraints. This paper explores this possibility and presents \textbf{GuideGen}: a pipeline that jointly generates CT images and tissue masks for abdominal organs and colorectal cancer conditioned on a text prompt. Firstly, we introduce Volumetric Mask Sampler to fit the discrete distribution of mask labels and generate low-resolution 3D tissue masks. Secondly, our Conditional Image Generator autoregressively generates CT slices conditioned on a corre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20869;&#22312;&#32467;&#26500;&#21644;jets&#20960;&#20309;&#27010;&#24565;&#30340;&#20272;&#35745;Koopman&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;JetDMD&#65292;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#25910;&#25947;&#29575;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65292;&#20026;Koopman&#31639;&#23376;&#30340;&#25968;&#20540;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.02524</link><description>&lt;p&gt;
&#22312;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20855;&#26377;&#20869;&#22312;&#21487;&#35266;&#27979;&#24615;&#30340;Koopman&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Koopman operators with intrinsic observables in rigged reproducing kernel Hilbert spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20869;&#22312;&#32467;&#26500;&#21644;jets&#20960;&#20309;&#27010;&#24565;&#30340;&#20272;&#35745;Koopman&#31639;&#23376;&#30340;&#26032;&#26041;&#27861;JetDMD&#65292;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#25910;&#25947;&#29575;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65292;&#20026;Koopman&#31639;&#23376;&#30340;&#25968;&#20540;&#20272;&#35745;&#25552;&#20379;&#20102;&#26356;&#31934;&#30830;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#65292;&#26377;&#21161;&#20110;&#28145;&#20837;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#35013;&#37197;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#23450;&#20041;&#30340;Koopman&#31639;&#23376;&#21450;&#20854;&#35889;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;Jet Dynamic Mode Decomposition&#65288;JetDMD&#65289;&#65292;&#21033;&#29992;RKHS&#30340;&#20869;&#22312;&#32467;&#26500;&#21644;&#31216;&#20026;jets&#30340;&#20960;&#20309;&#27010;&#24565;&#26469;&#22686;&#24378;Koopman&#31639;&#23376;&#30340;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#31934;&#30830;&#24230;&#19978;&#20248;&#21270;&#20102;&#20256;&#32479;&#30340;&#25193;&#23637;&#21160;&#24577;&#27169;&#24577;&#20998;&#35299;&#65288;EDMD&#65289;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#24449;&#20540;&#30340;&#25968;&#20540;&#20272;&#35745;&#26041;&#38754;&#12290;&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#30340;&#35823;&#24046;&#30028;&#21644;&#29305;&#27530;&#27491;&#23450;&#20869;&#26680;&#30340;&#25910;&#25947;&#29575;&#35777;&#26126;&#20102;JetDMD&#30340;&#20248;&#36234;&#24615;&#65292;&#20026;&#20854;&#24615;&#33021;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25105;&#20204;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;Koopman&#31639;&#23376;&#30340;&#35889;&#20998;&#26512;&#65292;&#22312;&#35013;&#37197;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26694;&#26550;&#20869;&#25552;&#20986;&#20102;&#25193;&#23637;Koopman&#31639;&#23376;&#30340;&#27010;&#24565;&#12290;&#36825;&#20010;&#27010;&#24565;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#20272;&#35745;&#30340;Koopman&#29305;&#24449;&#20989;&#25968;&#24182;&#25429;&#25417;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02524v1 Announce Type: cross  Abstract: This paper presents a novel approach for estimating the Koopman operator defined on a reproducing kernel Hilbert space (RKHS) and its spectra. We propose an estimation method, what we call Jet Dynamic Mode Decomposition (JetDMD), leveraging the intrinsic structure of RKHS and the geometric notion known as jets to enhance the estimation of the Koopman operator. This method refines the traditional Extended Dynamic Mode Decomposition (EDMD) in accuracy, especially in the numerical estimation of eigenvalues. This paper proves JetDMD's superiority through explicit error bounds and convergence rate for special positive definite kernels, offering a solid theoretical foundation for its performance. We also delve into the spectral analysis of the Koopman operator, proposing the notion of extended Koopman operator within a framework of rigged Hilbert space. This notion leads to a deeper understanding of estimated Koopman eigenfunctions and captu
&lt;/p&gt;</description></item><item><title>Conformer&#26159;&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#30340;&#26102;&#31354;&#36830;&#32493;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#23454;&#29616;&#36830;&#32493;&#24615;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#22825;&#27668;&#28436;&#21464;&#12290;</title><link>https://arxiv.org/abs/2402.17966</link><description>&lt;p&gt;
Conformer&#65306;&#23558;&#36830;&#32493;&#27880;&#24847;&#21147;&#23884;&#20837;&#35270;&#35273;Transformer&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Conformer: Embedding Continuous Attention in Vision Transformer for Weather Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17966
&lt;/p&gt;
&lt;p&gt;
Conformer&#26159;&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#30340;&#26102;&#31354;&#36830;&#32493;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#23454;&#29616;&#36830;&#32493;&#24615;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#22825;&#27668;&#28436;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25805;&#20316;&#24615;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#20381;&#36182;&#20110;&#35745;&#31639;&#26114;&#36149;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#12290;&#23613;&#31649;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#22825;&#27668;&#39044;&#27979;&#20013;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#65292;&#20294;Transformers&#26159;&#31163;&#25955;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#20854;&#23398;&#20064;&#21160;&#24577;&#22825;&#27668;&#31995;&#32479;&#36830;&#32493;&#26102;&#31354;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;Conformer&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22825;&#27668;&#39044;&#27979;&#30340;&#26102;&#31354;&#36830;&#32493;&#35270;&#35273;Transformer&#12290;Conformer&#26088;&#22312;&#36890;&#36807;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#23454;&#29616;&#36830;&#32493;&#24615;&#26469;&#23398;&#20064;&#26102;&#38388;&#19978;&#30340;&#36830;&#32493;&#22825;&#27668;&#28436;&#21464;&#12290;&#27880;&#24847;&#21147;&#26426;&#21046;&#34987;&#32534;&#30721;&#20026;Transformer&#26550;&#26500;&#20013;&#30340;&#21487;&#24494;&#20998;&#20989;&#25968;&#65292;&#20197;&#24314;&#27169;&#22797;&#26434;&#30340;&#22825;&#27668;&#21160;&#24577;&#12290;&#25105;&#20204;&#23558;Conformer&#19982;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#27169;&#22411;&#21644;&#20960;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;Conformer&#22312;&#25152;&#26377;&#21069;&#23548;&#26102;&#38388;&#19978;&#20248;&#20110;&#19968;&#20123;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17966v1 Announce Type: new  Abstract: Operational weather forecasting system relies on computationally expensive physics-based models. Although Transformers-based models have shown remarkable potential in weather forecasting, Transformers are discrete models which limit their ability to learn the continuous spatio-temporal features of the dynamical weather system. We address this issue with Conformer, a spatio-temporal Continuous Vision Transformer for weather forecasting. Conformer is designed to learn the continuous weather evolution over time by implementing continuity in the multi-head attention mechanism. The attention mechanism is encoded as a differentiable function in the transformer architecture to model the complex weather dynamics. We evaluate Conformer against a state-of-the-art Numerical Weather Prediction (NWP) model and several deep learning based weather forecasting models. Conformer outperforms some of the existing data-driven models at all lead times while 
&lt;/p&gt;</description></item><item><title>LCEN&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#12289;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#31232;&#30095;&#22320;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17120</link><description>&lt;p&gt;
LCEN&#65306;&#19968;&#31181;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#38750;&#32447;&#24615;&#30340;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LCEN: A Novel Feature Selection Algorithm for Nonlinear, Interpretable Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17120
&lt;/p&gt;
&lt;p&gt;
LCEN&#31639;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#12289;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26032;&#22411;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#33021;&#22815;&#26356;&#20934;&#30830;&#12289;&#26356;&#31232;&#30095;&#22320;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#26550;&#26500;&#30456;&#23545;&#20110;&#40657;&#30418;&#26550;&#26500;&#20855;&#26377;&#20248;&#21183;&#65292;&#22312;&#20851;&#38190;&#39046;&#22495;&#22914;&#33322;&#31354;&#25110;&#21307;&#23398;&#20013;&#65292;&#21487;&#35299;&#37322;&#24615;&#23545;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#26368;&#31616;&#21333;&#12289;&#26368;&#24120;&#29992;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#65288;&#22914;LASSO&#25110;EN&#65289;&#20165;&#38480;&#20110;&#32447;&#24615;&#39044;&#27979;&#65292;&#24182;&#19988;&#29305;&#24449;&#36873;&#25321;&#33021;&#21147;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;LASSO-Clip-EN&#65288;LCEN&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#21019;&#24314;&#38750;&#32447;&#24615;&#12289;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;LCEN&#22312;&#22810;&#31181;&#20154;&#24037;&#21644;&#23454;&#35777;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#29983;&#25104;&#27604;&#20854;&#20182;&#24120;&#29992;&#26550;&#26500;&#26356;&#20934;&#30830;&#12289;&#26356;&#31232;&#30095;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#23454;&#39564;&#34920;&#26126;&#65292;LCEN&#23545;&#25968;&#25454;&#38598;&#21644;&#24314;&#27169;&#20013;&#36890;&#24120;&#23384;&#22312;&#30340;&#35768;&#22810;&#38382;&#39064;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#22122;&#22768;&#12289;&#22810;&#37325;&#20849;&#32447;&#24615;&#12289;&#25968;&#25454;&#31232;&#32570;&#21644;&#36229;&#21442;&#25968;&#26041;&#24046;&#12290;LCEN&#36824;&#33021;&#22815;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#37325;&#26032;&#21457;&#29616;&#22810;&#20010;&#29289;&#29702;&#23450;&#24459;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17120v1 Announce Type: new  Abstract: Interpretable architectures can have advantages over black-box architectures, and interpretability is essential for the application of machine learning in critical settings, such as aviation or medicine. However, the simplest, most commonly used interpretable architectures (such as LASSO or EN) are limited to linear predictions and have poor feature selection capabilities. In this work, we introduce the LASSO-Clip-EN (LCEN) algorithm for the creation of nonlinear, interpretable machine learning models. LCEN is tested on a wide variety of artificial and empirical datasets, creating more accurate, sparser models than other commonly used architectures. These experiments reveal that LCEN is robust against many issues typically present in datasets and modeling, including noise, multicollinearity, data scarcity, and hyperparameter variance. LCEN is also able to rediscover multiple physical laws from empirical data and, for processes with no kn
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.14332</link><description>&lt;p&gt;
&#20174;&#22823;&#35268;&#27169;&#21040;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;&#29992;&#20110;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#30340;&#23610;&#23544;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
From Large to Small Datasets: Size Generalization for Clustering Algorithm Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14332
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30340;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#33021;&#22815;&#22312;&#23567;&#23454;&#20363;&#19978;&#20445;&#35777;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#20063;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#20934;&#30830;&#24230;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32858;&#31867;&#31639;&#27861;&#36873;&#25321;&#20013;&#65292;&#25105;&#20204;&#20250;&#24471;&#21040;&#19968;&#20010;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#24182;&#35201;&#26377;&#25928;&#22320;&#36873;&#25321;&#35201;&#20351;&#29992;&#30340;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#26377;&#19968;&#20010;&#26410;&#30693;&#30340;&#22522;&#20934;&#32858;&#31867;&#65292;&#25105;&#20204;&#21482;&#33021;&#36890;&#36807;&#26114;&#36149;&#30340;oracle&#26597;&#35810;&#26469;&#35775;&#38382;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#32858;&#31867;&#31639;&#27861;&#30340;&#36755;&#20986;&#23558;&#19982;&#22522;&#26412;&#20107;&#23454;&#32467;&#26500;&#19978;&#25509;&#36817;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#32858;&#31867;&#31639;&#27861;&#20934;&#30830;&#24615;&#30340;&#23610;&#23544;&#27867;&#21270;&#27010;&#24565;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23450;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#25105;&#20204;&#21487;&#20197;&#65288;1&#65289;&#23545;&#22823;&#35268;&#27169;&#32858;&#31867;&#23454;&#20363;&#36827;&#34892;&#23376;&#37319;&#26679;&#65292;&#65288;2&#65289;&#22312;&#36739;&#23567;&#23454;&#20363;&#19978;&#35780;&#20272;&#19968;&#32452;&#20505;&#36873;&#31639;&#27861;&#65292;&#65288;3&#65289;&#20445;&#35777;&#22312;&#23567;&#23454;&#20363;&#19978;&#20934;&#30830;&#24230;&#26368;&#39640;&#30340;&#31639;&#27861;&#23558;&#22312;&#21407;&#22987;&#22823;&#23454;&#20363;&#19978;&#25317;&#26377;&#26368;&#39640;&#30340;&#20934;&#30830;&#24230;&#12290;&#25105;&#20204;&#20026;&#19977;&#31181;&#32463;&#20856;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#20102;&#29702;&#35770;&#23610;&#23544;&#27867;&#21270;&#20445;&#35777;&#65306;&#21333;&#38142;&#25509;&#12289;k-means++&#21644;Gonzalez&#30340;k&#20013;&#24515;&#21551;&#21457;&#24335;&#65288;&#19968;&#31181;&#24179;&#28369;&#30340;&#21464;&#31181;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14332v1 Announce Type: new  Abstract: In clustering algorithm selection, we are given a massive dataset and must efficiently select which clustering algorithm to use. We study this problem in a semi-supervised setting, with an unknown ground-truth clustering that we can only access through expensive oracle queries. Ideally, the clustering algorithm's output will be structurally close to the ground truth. We approach this problem by introducing a notion of size generalization for clustering algorithm accuracy. We identify conditions under which we can (1) subsample the massive clustering instance, (2) evaluate a set of candidate algorithms on the smaller instance, and (3) guarantee that the algorithm with the best accuracy on the small instance will have the best accuracy on the original big instance. We provide theoretical size generalization guarantees for three classic clustering algorithms: single-linkage, k-means++, and (a smoothed variant of) Gonzalez's k-centers heuris
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#20351;&#29992;&#24418;&#24335;&#20026;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#26102;&#30340;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;MTW&#24352;&#37327;&#22312;&#38646;&#21521;&#37327;&#19978;&#20026;&#38646;&#30340;&#26465;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#32447;&#24615;ODE&#30340;&#31616;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#36870;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#21450;&#19968;&#20123;&#20855;&#20307;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.00953</link><description>&lt;p&gt;
&#25317;&#26377;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#36153;&#29992;&#26063;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Families of costs with zero and nonnegative MTW tensor in optimal transport. (arXiv:2401.00953v1 [math.AP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22312;&#26368;&#20248;&#20256;&#36865;&#20013;&#20351;&#29992;&#24418;&#24335;&#20026;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#26102;&#30340;&#38646;&#21644;&#38750;&#36127;MTW&#24352;&#37327;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;MTW&#24352;&#37327;&#22312;&#38646;&#21521;&#37327;&#19978;&#20026;&#38646;&#30340;&#26465;&#20214;&#20197;&#21450;&#30456;&#24212;&#30340;&#32447;&#24615;ODE&#30340;&#31616;&#21270;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#36870;&#20989;&#25968;&#30340;&#35299;&#26512;&#34920;&#36798;&#24335;&#20197;&#21450;&#19968;&#20123;&#20855;&#20307;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35745;&#31639;&#20102;&#22312;$\mathbb{R}^n$&#19978;&#20855;&#26377;&#24418;&#24335;$\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$&#30340;&#36153;&#29992;&#20989;&#25968;&#30340;&#26368;&#20248;&#20256;&#36865;&#38382;&#39064;&#30340;MTW&#24352;&#37327;&#65288;&#25110;&#20132;&#21449;&#26354;&#29575;&#65289;&#12290;&#20854;&#20013;&#65292;$\mathsf{u}$&#26159;&#19968;&#20010;&#20855;&#26377;&#36870;&#20989;&#25968;$\mathsf{s}$&#30340;&#26631;&#37327;&#20989;&#25968;&#65292;$x^{\ft}y$&#26159;&#23646;&#20110;$\mathbb{R}^n$&#24320;&#23376;&#38598;&#30340;&#21521;&#37327;$x&#65292;y$&#30340;&#38750;&#36864;&#21270;&#21452;&#32447;&#24615;&#37197;&#23545;&#12290;MTW&#24352;&#37327;&#22312;Kim-McCann&#24230;&#37327;&#19979;&#23545;&#20110;&#38646;&#21521;&#37327;&#30340;&#26465;&#20214;&#26159;&#19968;&#20010;&#22235;&#38454;&#38750;&#32447;&#24615;ODE&#65292;&#21487;&#20197;&#34987;&#31616;&#21270;&#20026;&#20855;&#26377;&#24120;&#25968;&#31995;&#25968;$P$&#21644;$S$&#30340;&#24418;&#24335;&#20026;$\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$&#30340;&#32447;&#24615;ODE&#12290;&#26368;&#32456;&#24471;&#21040;&#30340;&#36870;&#20989;&#25968;&#21253;&#25324;Lambert&#21644;&#24191;&#20041;&#21453;&#21452;&#26354;/&#19977;&#35282;&#20989;&#25968;&#12290;&#24179;&#26041;&#27431;&#27663;&#24230;&#37327;&#21644;$\log$&#22411;&#36153;&#29992;&#26159;&#36825;&#20123;&#35299;&#30340;&#23454;&#20363;&#12290;&#36825;&#20010;&#23478;&#26063;&#30340;&#26368;&#20248;&#26144;&#23556;&#20063;&#26159;&#26174;&#24335;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We compute explicitly the MTW tensor (or cross curvature) for the optimal transport problem on $\mathbb{R}^n$ with a cost function of form $\mathsf{c}(x, y) = \mathsf{u}(x^{\mathfrak{t}}y)$, where $\mathsf{u}$ is a scalar function with inverse $\mathsf{s}$, $x^{\ft}y$ is a nondegenerate bilinear pairing of vectors $x, y$ belonging to an open subset of $\mathbb{R}^n$. The condition that the MTW-tensor vanishes on null vectors under the Kim-McCann metric is a fourth-order nonlinear ODE, which could be reduced to a linear ODE of the form $\mathsf{s}^{(2)} - S\mathsf{s}^{(1)} + P\mathsf{s} = 0$ with constant coefficients $P$ and $S$. The resulting inverse functions include {\it Lambert} and {\it generalized inverse hyperbolic\slash trigonometric} functions. The square Euclidean metric and $\log$-type costs are equivalent to instances of these solutions. The optimal map for the family is also explicit. For cost functions of a similar form on a hyperboloid model of the hyperbolic space and u
&lt;/p&gt;</description></item><item><title>RandCom&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#36807;&#27010;&#29575;&#24615;&#26412;&#22320;&#26356;&#26032;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2310.07983</link><description>&lt;p&gt;
RandCom&#65306;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#29992;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RandCom: Random Communication Skipping Method for Decentralized Stochastic Optimization. (arXiv:2310.07983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07983
&lt;/p&gt;
&lt;p&gt;
RandCom&#26159;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#38543;&#26426;&#36890;&#20449;&#36339;&#36291;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#20013;&#36890;&#36807;&#27010;&#29575;&#24615;&#26412;&#22320;&#26356;&#26032;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#35774;&#32622;&#20013;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#38543;&#26426;&#36890;&#20449;&#36339;&#36807;&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#22240;&#20854;&#22312;&#21152;&#36895;&#36890;&#20449;&#22797;&#26434;&#24615;&#26041;&#38754;&#20855;&#26377;&#30340;&#20248;&#21183;&#32780;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#24378;&#20984;&#30830;&#23450;&#24615;&#35774;&#32622;&#30340;&#38598;&#20013;&#24335;&#36890;&#20449;&#21327;&#35758;&#19978;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RandCom&#30340;&#20998;&#24067;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#37319;&#29992;&#20102;&#27010;&#29575;&#24615;&#30340;&#26412;&#22320;&#26356;&#26032;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;RandCom&#22312;&#38543;&#26426;&#38750;&#20984;&#12289;&#20984;&#21644;&#24378;&#20984;&#35774;&#32622;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#33021;&#22815;&#36890;&#36807;&#36890;&#20449;&#27010;&#29575;&#26469;&#28176;&#36817;&#22320;&#20943;&#23569;&#36890;&#20449;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;RandCom&#33021;&#22815;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#22312;&#38543;&#26426;&#24378;&#20984;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;RandCom&#21487;&#20197;&#36890;&#36807;&#29420;&#31435;&#20110;&#32593;&#32476;&#30340;&#27493;&#38271;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;RandCom&#24212;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#29616;&#32447;&#24615;&#21152;&#36895;&#30340;&#28508;&#21147;&#30340;&#31215;&#26497;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed optimization methods with random communication skips are gaining increasing attention due to their proven benefits in accelerating communication complexity. Nevertheless, existing research mainly focuses on centralized communication protocols for strongly convex deterministic settings. In this work, we provide a decentralized optimization method called RandCom, which incorporates probabilistic local updates. We analyze the performance of RandCom in stochastic non-convex, convex, and strongly convex settings and demonstrate its ability to asymptotically reduce communication overhead by the probability of communication. Additionally, we prove that RandCom achieves linear speedup as the number of nodes increases. In stochastic strongly convex settings, we further prove that RandCom can achieve linear speedup with network-independent stepsizes. Moreover, we apply RandCom to federated learning and provide positive results concerning the potential for achieving linear speedup and
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15039</link><description>&lt;p&gt;
&#32467;&#21512;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data. (arXiv:2309.15039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#31929;&#30340;&#21307;&#23398;&#32959;&#30244;&#31579;&#26597;&#26041;&#27861;&#36890;&#24120;&#36153;&#29992;&#39640;&#26114;&#12289;&#32791;&#26102;&#38271;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#22312;&#30284;&#30151;&#26816;&#27979;&#26041;&#38754;&#21457;&#25381;&#20102;&#24040;&#22823;&#20316;&#29992;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#25110;&#28145;&#20837;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#30284;&#30151;&#31579;&#26597;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#23454;&#26045;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#24050;&#26377;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#23545;&#24739;&#32773;&#36827;&#34892;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#30284;&#30151;&#39118;&#38505;&#35780;&#20272;&#24212;&#29992;AI&#26041;&#27861;&#26159;&#19968;&#31181;&#39072;&#35206;&#24615;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#30340;&#25968;&#25454;&#36138;&#23146;&#31574;&#30053;&#33073;&#39062;&#32780;&#20986;&#65292;&#20165;&#38656;&#35201;&#26469;&#33258;EHR&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#21382;&#21490;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;175441&#21517;&#19981;&#35760;&#21517;&#30340;&#24739;&#32773;&#65288;&#20854;&#20013;2861&#21517;&#34987;&#35786;&#26029;&#20026;&#30284;&#30151;&#65289;&#12290;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects affect the mass implementation of cancer screening methods. For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.  This paper presents a novel method for mass cancer risk prediction using EHR data. Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR. We formulate the problem as a binary classification. This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we implement a solution based on a recurrent neural network (RNN). We propose a method that combine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#24182;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#65292;&#20063;&#32473;&#20986;&#20102;&#19968;&#20123;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#26356;&#39640;&#30340;$k-$WL&#21464;&#20307;&#21644;&#20998;&#35010;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.19659</link><description>&lt;p&gt;
&#21033;&#29992;&#23616;&#37096;&#21270;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressivity of Graph Neural Networks using Localization. (arXiv:2305.19659v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19659
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#29992;&#20110;&#35299;&#20915;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#24182;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#21516;&#26102;&#65292;&#20063;&#32473;&#20986;&#20102;&#19968;&#20123;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#26356;&#39640;&#30340;$k-$WL&#21464;&#20307;&#21644;&#20998;&#35010;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Weisfeiler-Leman (WL)&#31639;&#27861;&#30340;&#23616;&#37096;&#29256;&#26412;&#65292;&#26088;&#22312;&#22686;&#21152;&#34920;&#36798;&#33021;&#21147;&#24182;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#23376;&#22270;&#35745;&#25968;&#38382;&#39064;&#65292;&#24182;&#20026;&#20219;&#24847;$k$&#32473;&#20986;$k-$WL&#30340;&#23616;&#37096;&#29256;&#26412;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;Local $k-$WL&#30340;&#20316;&#29992;&#65292;&#24182;&#35777;&#26126;&#20854;&#27604;$k-$WL&#26356;&#20855;&#34920;&#29616;&#21147;&#65292;&#24182;&#19988;&#33267;&#22810;&#19982;$(k+1)-$WL&#19968;&#26679;&#20855;&#26377;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#19968;&#20123;&#27169;&#24335;&#30340;&#29305;&#24449;&#65292;&#22914;&#26524;&#20004;&#20010;&#22270;&#26159;Local $k-$WL&#31561;&#20215;&#30340;&#65292;&#21017;&#23427;&#20204;&#30340;&#23376;&#22270;&#21644;&#35825;&#23548;&#23376;&#22270;&#30340;&#35745;&#25968;&#26159;&#19981;&#21464;&#30340;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;$k-$WL&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#23618;$k-$WL&#21644;&#36882;&#24402;$k-$WL&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#25928;&#29575;&#27604;&#22312;&#25972;&#20010;&#22270;&#19978;&#24212;&#29992;$k-$WL&#26356;&#39640;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35010;&#25216;&#26415;&#65292;&#20351;&#29992;$1-$WL&#21363;&#21487;&#20445;&#35777;&#25152;&#26377;&#22823;&#23567;&#19981;&#36229;&#36807;4&#30340;&#35825;&#23548;&#23376;&#22270;&#30340;&#20934;&#30830;&#35745;&#25968;&#12290;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#20351;&#29992;$k&gt;1$&#36827;&#19968;&#27493;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#36824;&#23558;Local $k-$WL&#30340;&#34920;&#29616;&#21147;&#19982;&#20854;&#20182;GNN&#23618;&#27425;&#32467;&#26500;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose localized versions of Weisfeiler-Leman (WL) algorithms in an effort to both increase the expressivity, as well as decrease the computational overhead. We focus on the specific problem of subgraph counting and give localized versions of $k-$WL for any $k$. We analyze the power of Local $k-$WL and prove that it is more expressive than $k-$WL and at most as expressive as $(k+1)-$WL. We give a characterization of patterns whose count as a subgraph and induced subgraph are invariant if two graphs are Local $k-$WL equivalent. We also introduce two variants of $k-$WL: Layer $k-$WL and recursive $k-$WL. These methods are more time and space efficient than applying $k-$WL on the whole graph. We also propose a fragmentation technique that guarantees the exact count of all induced subgraphs of size at most 4 using just $1-$WL. The same idea can be extended further for larger patterns using $k&gt;1$. We also compare the expressive power of Local $k-$WL with other GNN hierarc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#38598;&#65292;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10805</link><description>&lt;p&gt;
RPLKG: &#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RPLKG: Robust Prompt Learning with Knowledge Graph. (arXiv:2304.10805v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#38598;&#65292;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#21487;&#36801;&#31227;&#30340;&#65292;&#24182;&#19988;&#23545;&#26410;&#30693;&#25968;&#25454;&#38598;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#35832;&#22914;CLIP&#20043;&#31867;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#24403;&#26631;&#35760;&#25968;&#25454;&#38598;&#26377;&#38480;&#26102;&#65292;&#26032;&#25968;&#25454;&#38598;&#25110;&#39046;&#22495;&#30340;&#27867;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#21508;&#31181;&#21162;&#21147;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#21644;&#36866;&#37197;&#22120;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#33258;&#36866;&#24212;&#26041;&#27861;&#19981;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#65288;RPLKG&#65289;&#12290;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#33258;&#21160;&#35774;&#35745;&#20986;&#21508;&#31181;&#21487;&#35299;&#37322;&#21644;&#26377;&#24847;&#20041;&#30340;&#25552;&#31034;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19968;&#27425;&#27491;&#21521;&#20256;&#36882;&#21518;&#33719;&#24471;&#25552;&#31034;&#38598;&#30340;&#32531;&#23384;&#23884;&#20837;&#12290;&#20043;&#21518;&#65292;&#27169;&#22411;&#20351;&#29992;GumbelSoftmax&#20248;&#21270;&#25552;&#31034;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained models have been known that they are transferable, and they generalize well on the unseen dataset. Recently, multimodal pre-trained models such as CLIP show significant performance improvement in diverse experiments. However, when the labeled dataset is limited, the generalization of a new dataset or domain is still challenging. To improve the generalization performance on few-shot learning, there have been diverse efforts, such as prompt learning and adapter. However, the current few-shot adaptation methods are not interpretable, and they require a high computation cost for adaptation. In this study, we propose a new method, robust prompt learning with knowledge graph (RPLKG). Based on the knowledge graph, we automatically design diverse interpretable and meaningful prompt sets. Our model obtains cached embeddings of prompt sets after one forwarding from a large pre-trained model. After that, model optimizes the prompt selection processes with GumbelSoftmax. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#30340;&#35843;&#24230;&#31574;&#30053;&#21644;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#20449;&#36947;&#24863;&#30693;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#35843;&#24230;&#31574;&#30053;&#21644;&#8220;&#24180;&#40836;&#24863;&#30693;&#8221;&#30340;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#26469;&#35299;&#20915;FL&#31995;&#32479;&#20013;&#30340;&#8220;&#25302;&#27795;&#8221;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.07356</link><description>&lt;p&gt;
&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#22312;&#26080;&#32447;&#32593;&#32476;&#20013;&#30340;&#35843;&#24230;&#21644;&#32858;&#21512;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scheduling and Aggregation Design for Asynchronous Federated Learning over Wireless Networks. (arXiv:2212.07356v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;&#32852;&#37030;&#23398;&#20064;&#30340;&#35843;&#24230;&#31574;&#30053;&#21644;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#65292;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#20449;&#36947;&#24863;&#30693;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#35843;&#24230;&#31574;&#30053;&#21644;&#8220;&#24180;&#40836;&#24863;&#30693;&#8221;&#30340;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#26469;&#35299;&#20915;FL&#31995;&#32479;&#20013;&#30340;&#8220;&#25302;&#27795;&#8221;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20223;&#30495;&#35777;&#23454;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#21327;&#20316;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#35774;&#22791;&#19978;&#30340;&#35757;&#32451;&#21644;&#22522;&#20110;&#26381;&#21153;&#22120;&#30340;&#32858;&#21512;&#26469;&#22312;&#20998;&#24067;&#24335;&#20195;&#29702;&#38388;&#35757;&#32451;&#36890;&#29992;&#30340;ML&#27169;&#22411;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#27493;FL&#35774;&#35745;&#65292;&#37319;&#29992;&#21608;&#26399;&#24615;&#30340;&#32858;&#21512;&#26469;&#35299;&#20915;FL&#31995;&#32479;&#20013;&#30340;&#8220;&#25302;&#27795;&#8221;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#26377;&#38480;&#30340;&#26080;&#32447;&#36890;&#20449;&#36164;&#28304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#35843;&#24230;&#31574;&#30053;&#21644;&#32858;&#21512;&#35774;&#35745;&#23545;&#25910;&#25947;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#38477;&#20302;&#32858;&#21512;&#27169;&#22411;&#26356;&#26032;&#30340;&#20559;&#24046;&#21644;&#26041;&#24046;&#30340;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35843;&#24230;&#31574;&#30053;&#65292;&#23427;&#21516;&#26102;&#32771;&#34385;&#20102;&#29992;&#25143;&#35774;&#22791;&#30340;&#20449;&#36947;&#36136;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#34920;&#31034;&#12290;&#36890;&#36807;&#20223;&#30495;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#20449;&#36947;&#24863;&#30693;&#25968;&#25454;&#37325;&#35201;&#24615;&#30340;&#35843;&#24230;&#31574;&#30053;&#30456;&#23545;&#20110;&#21516;&#27493;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#30340;&#29616;&#26377;&#26368;&#26032;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#19968;&#31181;&#8220;&#24180;&#40836;&#24863;&#30693;&#8221;&#30340;&#32858;&#21512;&#21152;&#26435;&#35774;&#35745;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a collaborative machine learning (ML) framework that combines on-device training and server-based aggregation to train a common ML model among distributed agents. In this work, we propose an asynchronous FL design with periodic aggregation to tackle the straggler issue in FL systems. Considering limited wireless communication resources, we investigate the effect of different scheduling policies and aggregation designs on the convergence performance. Driven by the importance of reducing the bias and variance of the aggregated model updates, we propose a scheduling policy that jointly considers the channel quality and training data representation of user devices. The effectiveness of our channel-aware data-importance-based scheduling policy, compared with state-of-the-art methods proposed for synchronous FL, is validated through simulations. Moreover, we show that an ``age-aware'' aggregation weighting design can significantly improve the learning performance i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2209.14790</link><description>&lt;p&gt;
&#22810;&#32452;&#20998;&#30340;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sparse PCA With Multiple Components. (arXiv:2209.14790v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#26159;&#19968;&#31181;&#29992;&#20110;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#35299;&#37322;&#39640;&#32500;&#25968;&#25454;&#38598;&#26041;&#24046;&#30340;&#22522;&#26412;&#25216;&#26415;&#12290;&#36825;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#31232;&#30095;&#24615;&#21644;&#27491;&#20132;&#24615;&#32422;&#26463;&#30340;&#20984;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20854;&#35745;&#31639;&#22797;&#26434;&#24230;&#38750;&#24120;&#39640;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35745;&#31639;&#19968;&#20010;&#31232;&#30095;&#20027;&#25104;&#20998;&#24182;&#32553;&#20943;&#21327;&#26041;&#24046;&#30697;&#38453;&#26469;&#35299;&#20915;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#65292;&#20294;&#22312;&#23547;&#25214;&#22810;&#20010;&#30456;&#20114;&#27491;&#20132;&#30340;&#20027;&#25104;&#20998;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#25152;&#24471;&#35299;&#30340;&#27491;&#20132;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25361;&#25112;&#36825;&#31181;&#29616;&#29366;&#65292;&#36890;&#36807;&#23558;&#27491;&#20132;&#24615;&#26465;&#20214;&#37325;&#26032;&#34920;&#36848;&#20026;&#31209;&#32422;&#26463;&#65292;&#24182;&#21516;&#26102;&#23545;&#31232;&#30095;&#24615;&#21644;&#31209;&#32422;&#26463;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#32039;&#20945;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#26469;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#19978;&#30028;&#65292;&#24403;&#27599;&#20010;&#20027;&#25104;&#20998;&#30340;&#20010;&#20307;&#31232;&#30095;&#24615;&#34987;&#25351;&#23450;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#21152;&#24378;&#19978;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#21478;&#19968;&#31181;&#26041;&#27861;&#26469;&#21152;&#24378;&#19978;&#30028;&#65292;&#25105;&#20204;&#20351;&#29992;&#39069;&#22806;&#30340;&#20108;&#38454;&#38181;&#19981;&#31561;&#24335;&#26469;&#21152;&#24378;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sparse Principal Component Analysis (sPCA) is a cardinal technique for obtaining combinations of features, or principal components (PCs), that explain the variance of high-dimensional datasets in an interpretable manner. This involves solving a sparsity and orthogonality constrained convex maximization problem, which is extremely computationally challenging. Most existing works address sparse PCA via methods-such as iteratively computing one sparse PC and deflating the covariance matrix-that do not guarantee the orthogonality, let alone the optimality, of the resulting solution when we seek multiple mutually orthogonal PCs. We challenge this status by reformulating the orthogonality conditions as rank constraints and optimizing over the sparsity and rank constraints simultaneously. We design tight semidefinite relaxations to supply high-quality upper bounds, which we strengthen via additional second-order cone inequalities when each PC's individual sparsity is specified. Further, we de
&lt;/p&gt;</description></item></channel></rss>