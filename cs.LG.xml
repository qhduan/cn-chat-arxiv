<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.16218</link><description>&lt;p&gt;
CoverUp&#65306;&#22522;&#20110;&#35206;&#30422;&#29575;&#24341;&#23548;&#30340;LLM&#27979;&#35797;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoverUp: Coverage-Guided LLM-Based Test Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16218
&lt;/p&gt;
&lt;p&gt;
CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CoverUp&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32467;&#21512;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#12290;CoverUp&#36890;&#36807;&#36845;&#20195;&#25913;&#21892;&#35206;&#30422;&#29575;&#65292;&#23558;&#35206;&#30422;&#29575;&#20998;&#26512;&#19982;LLM&#23545;&#35805;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#20415;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#23578;&#26410;&#28085;&#30422;&#30340;&#20195;&#30721;&#34892;&#21644;&#20998;&#25903;&#19978;&#12290;&#26368;&#32456;&#30340;&#27979;&#35797;&#22871;&#20214;&#30456;&#27604;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#26174;&#33879;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#65306;&#19982;CodaMosa&#30456;&#27604;&#65292;&#19968;&#31181;&#28151;&#21512;LLM / &#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#31995;&#32479;&#65292;CoverUp&#22312;&#21508;&#26041;&#38754;&#37117;&#22823;&#24133;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#12290;&#20197;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;CoverUp&#23454;&#29616;&#20102;81%&#30340;&#20013;&#20301;&#32447;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;62%&#65289;&#12289;53%&#30340;&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;35%&#65289;&#21644;78%&#30340;&#32447;+&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;55%&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoverUp&#30340;&#36845;&#20195;&#12289;&#35206;&#30422;&#29575;&#24341;&#23548;&#26041;&#27861;&#23545;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#20854;&#25104;&#21151;&#30340;&#36817;&#19968;&#21322;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16218v1 Announce Type: cross  Abstract: This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DSM&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20854;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#27425;&#26799;&#24230;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11565</link><description>&lt;p&gt;
&#22522;&#20110;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#27861;&#30340;&#38750;&#24179;&#28369;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Decentralized Stochastic Subgradient Methods for Nonsmooth Nonconvex Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11565
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DSM&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#20998;&#26512;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#23637;&#31034;&#20854;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#27425;&#26799;&#24230;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#20855;&#26377;&#38750;&#20984;&#21644;&#38750;&#24179;&#28369;&#30446;&#26631;&#20989;&#25968;&#30340;&#21435;&#20013;&#24515;&#21270;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#20851;&#27880;&#38750;&#24179;&#28369;&#31070;&#32463;&#32593;&#32476;&#30340;&#21435;&#20013;&#24515;&#21270;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;DSM&#65292;&#29992;&#20110;&#20998;&#26512;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#36890;&#36807;&#24314;&#31435;&#29983;&#25104;&#24207;&#21015;&#28176;&#36817;&#36924;&#36817;&#20854;&#20851;&#32852;&#24494;&#20998;&#21253;&#21547;&#30340;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#28085;&#30422;&#20102;&#21508;&#31181;&#29616;&#26377;&#39640;&#25928;&#30340;&#21435;&#20013;&#24515;&#21270;&#27425;&#26799;&#24230;&#26041;&#27861;&#65292;&#21253;&#25324;&#21435;&#20013;&#24515;&#21270;&#38543;&#26426;&#27425;&#26799;&#24230;&#19979;&#38477;&#65288;DSGD&#65289;&#65292;&#20855;&#26377;&#26799;&#24230;&#36319;&#36394;&#25216;&#26415;&#30340;DSGD&#65288;DSGD-T&#65289;&#21644;&#24102;&#21160;&#37327;&#30340;DSGD&#65288;DSGDm&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;SignSGD&#65292;&#37319;&#29992;&#31526;&#21495;&#26144;&#23556;&#26469;&#27491;&#21017;&#21270;DSGDm&#20013;&#30340;&#26356;&#26032;&#26041;&#21521;&#65292;&#24182;&#34920;&#26126;&#23427;&#34987;&#21253;&#21547;&#22312;&#25105;&#20204;&#30340;&#25552;&#35758;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11565v1 Announce Type: cross  Abstract: In this paper, we concentrate on decentralized optimization problems with nonconvex and nonsmooth objective functions, especially on the decentralized training of nonsmooth neural networks. We introduce a unified framework, named DSM, to analyze the global convergence of decentralized stochastic subgradient methods. We prove the global convergence of our proposed framework under mild conditions, by establishing that the generated sequence asymptotically approximates the trajectories of its associated differential inclusion. Furthermore, we establish that our proposed framework encompasses a wide range of existing efficient decentralized subgradient methods, including decentralized stochastic subgradient descent (DSGD), DSGD with gradient-tracking technique (DSGD-T), and DSGD with momentum (DSGDm). In addition, we introduce SignSGD employing the sign map to regularize the update directions in DSGDm, and show it is enclosed in our propos
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#24212;&#29992;&#25193;&#25955;-&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39640;&#32423;&#25193;&#25955;&#27169;&#22411;&#21644;&#20302;&#32423;RL&#31639;&#27861;&#65292;&#23454;&#29616;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#39640;&#25928;&#29575;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.10794</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#30340;&#25193;&#25955;-&#24378;&#21270;&#23398;&#20064;&#20998;&#23618;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Diffusion-Reinforcement Learning Hierarchical Motion Planning in Adversarial Multi-agent Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10794
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25239;&#24615;&#22810;&#26234;&#33021;&#20307;&#28216;&#25103;&#20013;&#24212;&#29992;&#25193;&#25955;-&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#39640;&#32423;&#25193;&#25955;&#27169;&#22411;&#21644;&#20302;&#32423;RL&#31639;&#27861;&#65292;&#23454;&#29616;&#27604;&#22522;&#20934;&#26041;&#27861;&#26356;&#39640;&#25928;&#29575;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;-&#22522;&#20110;&#30340;&#36816;&#21160;&#35268;&#21010;&#26368;&#36817;&#23637;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#30340;&#28508;&#21147;&#65292;&#20174;&#33258;&#20027;&#23548;&#33322;&#21040;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#37096;&#20998;&#21487;&#35266;&#23519;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#36861;&#36880;&#36867;&#36991;&#28216;&#25103;&#65288;PEG&#65289;&#20013;&#23545;&#36867;&#36991;&#30446;&#26631;&#30340;&#36816;&#21160;&#35268;&#21010;&#20219;&#21153;&#12290;&#36825;&#20123;&#36861;&#36880;&#36867;&#36991;&#38382;&#39064;&#19982;&#21508;&#31181;&#24212;&#29992;&#30456;&#20851;&#65292;&#20363;&#22914;&#25628;&#32034;&#21644;&#25937;&#25588;&#34892;&#21160;&#20197;&#21450;&#30417;&#35270;&#26426;&#22120;&#20154;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#24517;&#39035;&#26377;&#25928;&#35268;&#21010;&#20182;&#20204;&#30340;&#34892;&#21160;&#26469;&#25910;&#38598;&#24773;&#25253;&#25110;&#23436;&#25104;&#20219;&#21153;&#65292;&#21516;&#26102;&#36991;&#20813;&#34987;&#20390;&#26597;&#25110;&#34987;&#20440;&#34383;&#33258;&#24049;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#26550;&#26500;&#65292;&#35813;&#26550;&#26500;&#25972;&#21512;&#20102;&#19968;&#20010;&#39640;&#32423;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#35268;&#21010;&#23545;&#29615;&#22659;&#25968;&#25454;&#25935;&#24863;&#30340;&#20840;&#23616;&#36335;&#24452;&#65292;&#21516;&#26102;&#20302;&#32423;RL&#31639;&#27861;&#25512;&#29702;&#38378;&#36991;&#34892;&#20026;&#19982;&#20840;&#23616;&#36335;&#24452;&#36319;&#38543;&#34892;&#20026;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24341;&#23548;RL&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#22522;&#32447;&#25552;&#39640;&#20102;51.2&#65285;&#65292;&#23454;&#29616;&#26356;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10794v1 Announce Type: cross  Abstract: Reinforcement Learning- (RL-)based motion planning has recently shown the potential to outperform traditional approaches from autonomous navigation to robot manipulation. In this work, we focus on a motion planning task for an evasive target in a partially observable multi-agent adversarial pursuit-evasion games (PEG). These pursuit-evasion problems are relevant to various applications, such as search and rescue operations and surveillance robots, where robots must effectively plan their actions to gather intelligence or accomplish mission tasks while avoiding detection or capture themselves. We propose a hierarchical architecture that integrates a high-level diffusion model to plan global paths responsive to environment data while a low-level RL algorithm reasons about evasive versus global path-following behavior. Our approach outperforms baselines by 51.2% by leveraging the diffusion model to guide the RL algorithm for more efficien
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#37325;&#24314;&#30340;&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#22122;&#22768;&#20256;&#25773;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.17410</link><description>&lt;p&gt;
&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#29992;&#20110;&#22122;&#22768;&#20256;&#25773;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A novel image space formalism of Fourier domain interpolation neural networks for noise propagation analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;MRI&#37325;&#24314;&#30340;&#20613;&#37324;&#21494;&#22495;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#20998;&#26512;&#20102;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#22122;&#22768;&#20256;&#25773;&#30340;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#20026;MRI&#37325;&#24314;&#20013;&#30340;&#22270;&#20687;&#22495;&#25554;&#20540;&#24320;&#21457;&#22810;&#23618;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#22270;&#20687;&#31354;&#38388;&#24418;&#24335;&#20027;&#20041;&#65292;&#24182;&#22312;CNN&#25512;&#26029;&#36807;&#31243;&#20013;&#23545;&#22122;&#22768;&#20256;&#25773;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#20351;&#29992;&#22797;&#20540;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;&#22312;&#20613;&#37324;&#21494;&#22495;&#65288;&#20063;&#31216;&#20026;k&#31354;&#38388;&#65289;&#20013;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#65292;&#23558;&#20854;&#34920;&#31034;&#20026;&#19982;&#28608;&#27963;&#25513;&#27169;&#30340;&#36880;&#20803;&#32032;&#20056;&#27861;&#12290;&#36825;&#31181;&#25805;&#20316;&#22312;&#22270;&#20687;&#31354;&#38388;&#20013;&#36716;&#25442;&#20026;&#21367;&#31215;&#12290;&#22312;k&#31354;&#38388;&#32593;&#32476;&#35757;&#32451;&#21518;&#65292;&#36825;&#31181;&#26041;&#27861;&#20026;&#30456;&#23545;&#20110;&#21035;&#21517;&#32447;&#22280;&#22270;&#20687;&#30340;&#37325;&#24314;&#22270;&#20687;&#30340;&#23548;&#25968;&#25552;&#20379;&#20102;&#19968;&#20010;&#20195;&#25968;&#34920;&#36798;&#24335;&#65292;&#36825;&#20123;&#21035;&#21517;&#32447;&#22280;&#22270;&#20687;&#20316;&#20026;&#22270;&#20687;&#31354;&#38388;&#20013;&#32593;&#32476;&#30340;&#36755;&#20837;&#24352;&#37327;&#12290;&#36825;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#20998;&#26512;&#20272;&#35745;&#32593;&#32476;&#25512;&#26029;&#20013;&#30340;&#26041;&#24046;&#65292;&#24182;&#29992;&#20110;&#25551;&#36848;&#22122;&#22768;&#29305;&#24615;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#27169;&#25311;&#21644;&#22522;&#20110;&#33258;&#21160;&#24494;&#20998;&#30340;&#25968;&#20540;&#26041;&#27861;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17410v1 Announce Type: cross  Abstract: Purpose: To develop an image space formalism of multi-layer convolutional neural networks (CNNs) for Fourier domain interpolation in MRI reconstructions and analytically estimate noise propagation during CNN inference. Theory and Methods: Nonlinear activations in the Fourier domain (also known as k-space) using complex-valued Rectifier Linear Units are expressed as elementwise multiplication with activation masks. This operation is transformed into a convolution in the image space. After network training in k-space, this approach provides an algebraic expression for the derivative of the reconstructed image with respect to the aliased coil images, which serve as the input tensors to the network in the image space. This allows the variance in the network inference to be estimated analytically and to be used to describe noise characteristics. Monte-Carlo simulations and numerical approaches based on auto-differentiation were used for val
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10088</link><description>&lt;p&gt;
&#20998;&#23618;&#28151;&#21512;&#24314;&#27169;&#29992;&#20110;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical hybrid modeling for flexible tool use
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#32452;&#21512;&#31163;&#25955;&#21644;&#36830;&#32493;&#27169;&#22411;&#20197;&#23454;&#29616;&#28789;&#27963;&#24037;&#20855;&#20351;&#29992;&#65292;&#25511;&#21046;&#21644;&#35268;&#21010;&#12290;&#22312;&#38750;&#24179;&#20961;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#25552;&#20986;&#30340;&#20027;&#21160;&#25512;&#29702;&#35745;&#31639;&#26694;&#26550;&#20013;&#65292;&#31163;&#25955;&#27169;&#22411;&#21487;&#20197;&#19982;&#36830;&#32493;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#20197;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12290;&#20174;&#21478;&#19968;&#20010;&#35282;&#24230;&#26469;&#30475;&#65292;&#31616;&#21333;&#30340;&#20195;&#29702;&#21487;&#20197;&#32452;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#19990;&#30028;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22914;&#20309;&#23558;&#36825;&#20004;&#20010;&#29305;&#28857;&#32467;&#21512;&#36215;&#26469;&#23454;&#29616;&#39640;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#34892;&#20026;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#30001;&#22810;&#20010;&#28151;&#21512; - &#36830;&#32493;&#21644;&#31163;&#25955; - &#21333;&#20803;&#32452;&#25104;&#65292;&#22797;&#21046;&#20195;&#29702;&#30340;&#37197;&#32622;&#65292;&#30001;&#39640;&#32423;&#31163;&#25955;&#27169;&#22411;&#25511;&#21046;&#65292;&#23454;&#29616;&#21160;&#24577;&#35268;&#21010;&#21644;&#21516;&#27493;&#34892;&#20026;&#12290;&#27599;&#20010;&#23618;&#27425;&#20869;&#37096;&#30340;&#36827;&#19968;&#27493;&#20998;&#35299;&#21487;&#20197;&#20197;&#20998;&#23618;&#26041;&#24335;&#34920;&#31034;&#19982;self&#30456;&#20851;&#30340;&#20854;&#20182;&#20195;&#29702;&#21644;&#23545;&#35937;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#31181;&#20998;&#23618;&#28151;&#21512;&#27169;&#22411;&#65306;&#22312;&#25342;&#21462;&#19968;&#20010;&#31227;&#21160;&#24037;&#20855;&#21518;&#21040;&#36798;&#19968;&#20010;&#31227;&#21160;&#29289;&#20307;&#12290;&#36825;&#39033;&#30740;&#31350;&#25193;&#23637;&#20102;&#20197;&#25512;&#29702;&#20026;&#25511;&#21046;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10088v1 Announce Type: cross  Abstract: In a recent computational framework called active inference, discrete models can be linked to their continuous counterparts to perform decision-making in changing environments. From another perspective, simple agents can be combined to better capture the causal relationships of the world. How can we use these two features together to achieve efficient goal-directed behavior? We present an architecture composed of several hybrid -- continuous and discrete -- units replicating the agent's configuration, controlled by a high-level discrete model that achieves dynamic planning and synchronized behavior. Additional factorizations within each level allow to represent hierarchically other agents and objects in relation to the self. We evaluate this hierarchical hybrid model on a non-trivial task: reaching a moving object after having picked a moving tool. This study extends past work on control as inference and proposes an alternative directi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00045</link><description>&lt;p&gt;
&#26816;&#27979;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Detecting Multimedia Generated by Large AI Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#26816;&#27979;&#26041;&#27861;&#21644;&#30740;&#31350;&#36827;&#23637;&#65292;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#31867;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#32431;&#26816;&#27979;&#21644;&#24212;&#29992;&#22330;&#26223;&#20004;&#20010;&#35282;&#24230;&#26469;&#22686;&#24378;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;LAIMs&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26631;&#24535;&#30528;&#19968;&#31181;&#26032;&#30340;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25972;&#21512;&#21040;&#26085;&#24120;&#29983;&#27963;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#23613;&#31649;&#22312;&#35768;&#22810;&#39046;&#22495;&#26377;&#30410;&#65292;&#20294;&#36825;&#20123;&#20869;&#23481;&#20063;&#24102;&#26469;&#20102;&#37325;&#22823;&#39118;&#38505;&#65292;&#21253;&#25324;&#28508;&#22312;&#30340;&#28389;&#29992;&#12289;&#31038;&#20250;&#30772;&#22351;&#21644;&#20262;&#29702;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#30001;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#30456;&#20851;&#30740;&#31350;&#20063;&#22823;&#24133;&#22686;&#21152;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#26126;&#26174;&#30340;&#38382;&#39064;&#65292;&#21363;&#32570;&#20047;&#31995;&#32479;&#24615;&#30340;&#35843;&#26597;&#30740;&#31350;&#65292;&#19987;&#38376;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#20840;&#38754;&#28085;&#30422;&#29616;&#26377;&#30740;&#31350;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#37325;&#28857;&#20851;&#27880;&#26816;&#27979;LAIMs&#29983;&#25104;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#22810;&#27169;&#24577;&#20869;&#23481;&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#26041;&#27861;&#20998;&#31867;&#27861;&#65292;&#25353;&#23186;&#20307;&#24418;&#24335;&#20998;&#31867;&#65292;&#24182;&#19982;&#32431;&#26816;&#27979;&#65288;&#26088;&#22312;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#65289;&#21644;&#24212;&#29992;&#22330;&#26223;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of Large AI Models (LAIMs), particularly diffusion models and large language models, has marked a new era where AI-generated multimedia is increasingly integrated into various aspects of daily life. Although beneficial in numerous fields, this content presents significant risks, including potential misuse, societal disruptions, and ethical concerns. Consequently, detecting multimedia generated by LAIMs has become crucial, with a marked rise in related research. Despite this, there remains a notable gap in systematic surveys that focus specifically on detecting LAIM-generated multimedia. Addressing this, we provide the first survey to comprehensively cover existing research on detecting multimedia (such as text, images, videos, audio, and multimodal content) created by LAIMs. Specifically, we introduce a novel taxonomy for detection methods, categorized by media modality, and aligned with two perspectives: pure detection (aiming to enhance detection performance) an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.05363</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#32423;&#22495;&#23545;&#40784;&#23454;&#29616;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
Generalizable Sleep Staging via Multi-level Domain Alignment. (arXiv:2401.05363v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22495;&#27867;&#21270;&#27010;&#24565;&#65292;&#32467;&#21512;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#30340;&#24605;&#24819;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#23545;&#20110;&#30561;&#30496;&#35780;&#20272;&#21644;&#30142;&#30149;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#20381;&#36182;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#30456;&#21516;&#25968;&#25454;&#38598;&#30340;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#22495;&#27867;&#21270;&#27010;&#24565;&#21040;&#33258;&#21160;&#30561;&#30496;&#20998;&#26399;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#36890;&#29992;&#30340;&#30561;&#30496;&#20998;&#26399;&#20219;&#21153;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#26410;&#35265;&#36807;&#30340;&#25968;&#25454;&#38598;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#21463;&#21040;&#29616;&#26377;&#30340;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#24449;&#23545;&#40784;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SleepDG&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#32771;&#34385;&#21040;&#23616;&#37096;&#26174;&#33879;&#29305;&#24449;&#21644;&#26102;&#24207;&#29305;&#24449;&#23545;&#20110;&#30561;&#30496;&#20998;&#26399;&#37117;&#24456;&#37325;&#35201;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#32423;&#29305;&#24449;&#23545;&#40784;&#65292;&#23558;&#26102;&#20195;&#32423;&#21644;&#24207;&#21015;&#32423;&#29305;&#24449;&#23545;&#40784;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26102;&#20195;&#32423;&#29305;&#24449;&#23545;&#40784;&#26041;&#27861;&#65292;&#23545;&#19981;&#21516;&#30561;&#30496;&#26102;&#20195;&#30340;&#29305;&#24449;&#20998;&#24067;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic sleep staging is essential for sleep assessment and disorder diagnosis. Most existing methods depend on one specific dataset and are limited to be generalized to other unseen datasets, for which the training data and testing data are from the same dataset. In this paper, we introduce domain generalization into automatic sleep staging and propose the task of generalizable sleep staging which aims to improve the model generalization ability to unseen datasets. Inspired by existing domain generalization methods, we adopt the feature alignment idea and propose a framework called SleepDG to solve it. Considering both of local salient features and sequential features are important for sleep staging, we propose a Multi-level Feature Alignment combining epoch-level and sequence-level feature alignment to learn domain-invariant feature representations. Specifically, we design an Epoch-level Feature Alignment to align the feature distribution of each single sleep epoch among different 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Rydberg&#21407;&#23376;&#38453;&#21015;&#19978;&#36827;&#34892;&#25968;&#23383;-&#27169;&#25311;&#37327;&#23376;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36817;&#26399;&#23454;&#29616;&#36825;&#19968;&#31639;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#25968;&#23383;&#23398;&#20064;&#26041;&#26696;&#65292;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#38656;&#35201;&#26356;&#30701;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#29616;&#23454;&#35823;&#24046;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#36825;&#20351;&#24471;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#25104;&#20026;&#20013;&#26399;&#25913;&#36827;&#21464;&#20998;&#37327;&#23376;&#23398;&#20064;&#23454;&#39564;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.02940</link><description>&lt;p&gt;
Rydberg&#21407;&#23376;&#38453;&#21015;&#30340;&#25968;&#23383;-&#27169;&#25311;&#37327;&#23376;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Digital-analog quantum learning on Rydberg atom arrays. (arXiv:2401.02940v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02940
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;Rydberg&#21407;&#23376;&#38453;&#21015;&#19978;&#36827;&#34892;&#25968;&#23383;-&#27169;&#25311;&#37327;&#23376;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36817;&#26399;&#23454;&#29616;&#36825;&#19968;&#31639;&#27861;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#19988;&#30456;&#27604;&#20110;&#25968;&#23383;&#23398;&#20064;&#26041;&#26696;&#65292;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#38656;&#35201;&#26356;&#30701;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#29616;&#23454;&#35823;&#24046;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#36825;&#20351;&#24471;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#25104;&#20026;&#20013;&#26399;&#25913;&#36827;&#21464;&#20998;&#37327;&#23376;&#23398;&#20064;&#23454;&#39564;&#30340;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Rydberg&#21407;&#23376;&#38453;&#21015;&#19978;&#30340;&#28151;&#21512;&#25968;&#23383;-&#27169;&#25311;&#37327;&#23376;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#37327;&#23376;&#23398;&#20064;&#30340;&#28508;&#22312;&#23454;&#29992;&#24615;&#21644;&#36817;&#26399;&#21487;&#23454;&#29616;&#24615;&#20197;&#21450;&#20013;&#24615;&#21407;&#23376;&#30340;&#24555;&#36895;&#25193;&#23637;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#26500;&#24314;&#21482;&#38656;&#22312;&#25968;&#23383;&#35774;&#32622;&#20013;&#36827;&#34892;&#21333;&#37327;&#23376;&#27604;&#29305;&#25805;&#20316;&#65292;&#24182;&#26681;&#25454;Rydberg&#21704;&#23494;&#39039;&#37327;&#36827;&#34892;&#27169;&#25311;&#35774;&#32622;&#20013;&#30340;&#20840;&#23616;&#39537;&#21160;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#21644;&#37327;&#23376;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25968;&#20540;&#30740;&#31350;&#65292;&#20998;&#21035;&#36890;&#36807;&#25163;&#20889;&#25968;&#23383;&#20998;&#31867;&#21644;&#26080;&#30417;&#30563;&#37327;&#23376;&#30456;&#36793;&#30028;&#23398;&#20064;&#26469;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#36825;&#20004;&#20010;&#20856;&#22411;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#19981;&#20165;&#22312;&#36817;&#26399;&#26159;&#21487;&#34892;&#30340;&#65292;&#32780;&#19988;&#19982;&#25968;&#23383;&#23398;&#20064;&#26041;&#26696;&#30456;&#27604;&#65292;&#38656;&#35201;&#26356;&#30701;&#30340;&#30005;&#36335;&#28145;&#24230;&#65292;&#24182;&#19988;&#23545;&#20110;&#29616;&#23454;&#35823;&#24046;&#27169;&#22411;&#26356;&#20855;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#20013;&#26399;&#25552;&#39640;&#21464;&#20998;&#37327;&#23376;&#23398;&#20064;&#23454;&#39564;&#30340;&#25928;&#26524;&#26041;&#38754;&#65292;&#25968;&#23383;-&#27169;&#25311;&#23398;&#20064;&#25171;&#24320;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose hybrid digital-analog learning algorithms on Rydberg atom arrays, combining the potentially practical utility and near-term realizability of quantum learning with the rapidly scaling architectures of neutral atoms. Our construction requires only single-qubit operations in the digital setting and global driving according to the Rydberg Hamiltonian in the analog setting. We perform a comprehensive numerical study of our algorithm on both classical and quantum data, given respectively by handwritten digit classification and unsupervised quantum phase boundary learning. We show in the two representative problems that digital-analog learning is not only feasible in the near term, but also requires shorter circuit depths and is more robust to realistic error models as compared to digital learning schemes. Our results suggest that digital-analog learning opens a promising path towards improved variational quantum learning experiments in the near term.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03568</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#35270;&#39057;&#35328;&#35821;&#24773;&#24863;&#35782;&#21035;&#30340;&#30690;&#37327;&#37327;&#21270;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
A vector quantized masked autoencoder for audiovisual speech emotion recognition. (arXiv:2305.03568v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20840;&#38754;&#30417;&#30563;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#38899;&#35270;&#39057;&#35328;&#35821;&#24773;&#24863;&#35782;&#21035;&#65288;SER&#65289;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#26631;&#35760;&#25968;&#25454;&#30340;&#26377;&#38480;&#24615;&#20173;&#28982;&#26159;&#35813;&#39046;&#22495;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#22914;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAEs&#65289;&#65292;&#24050;&#25104;&#20026;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#21035;&#20026;&#38899;&#35270;&#39057;&#35328;&#35821;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#35774;&#35745;&#30340;&#30690;&#37327;&#37327;&#21270;MAE&#27169;&#22411;&#65288;VQ-MAE-AV&#65289;&#12290;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#21407;&#22987;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#22788;&#29702;&#30340;&#22810;&#27169;&#24577;MAEs&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22522;&#20110;&#20004;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#31163;&#25955;&#38899;&#39057;&#21644;&#35270;&#35273;&#35328;&#35821;&#34920;&#31034;&#30340;&#33258;&#30417;&#30563;&#33539;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;VoxCeleb2&#25968;&#25454;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#26631;&#20934;&#24773;&#24863;&#38899;&#35270;&#39057;&#35328;&#35821;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#38899;&#35270;&#39057;SER&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
While fully-supervised models have been shown to be effective for audiovisual speech emotion recognition (SER), the limited availability of labeled data remains a major challenge in the field. To address this issue, self-supervised learning approaches, such as masked autoencoders (MAEs), have gained popularity as potential solutions. In this paper, we propose the VQ-MAE-AV model, a vector quantized MAE specifically designed for audiovisual speech self-supervised representation learning. Unlike existing multimodal MAEs that rely on the processing of the raw audiovisual speech data, the proposed method employs a self-supervised paradigm based on discrete audio and visual speech representations learned by two pre-trained vector quantized variational autoencoders. Experimental results show that the proposed approach, which is pre-trained on the VoxCeleb2 database and fine-tuned on standard emotional audiovisual speech datasets, outperforms the state-of-the-art audiovisual SER methods.
&lt;/p&gt;</description></item></channel></rss>