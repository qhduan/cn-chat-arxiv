<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#39640;&#25928;&#35757;&#32451;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#20856;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#20855;&#26377;&#20219;&#24847;&#24120;&#25968;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#20869;&#23384;&#20998;&#31163;&#12290;&#27599;&#20010;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#37117;&#21487;&#20197;&#22312;&#24120;&#25968;&#26102;&#38388;&#20869;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.08606</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20219;&#24847;&#22810;&#39033;&#24335;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Arbitrary Polynomial Separations in Trainable Quantum Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#39640;&#25928;&#35757;&#32451;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#20856;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#20855;&#26377;&#20219;&#24847;&#24120;&#25968;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#20869;&#23384;&#20998;&#31163;&#12290;&#27599;&#20010;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#37117;&#21487;&#20197;&#22312;&#24120;&#25968;&#26102;&#38388;&#20869;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35757;&#32451;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26222;&#36941;&#30340;&#26435;&#34913;&#65307;&#20316;&#20026;&#36825;&#20123;&#32467;&#26524;&#30340;&#25512;&#35770;&#65292;&#23454;&#38469;&#19978;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#23454;&#29616;&#25351;&#25968;&#32423;&#30340;&#36229;&#36234;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#31163;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;QNN&#35757;&#32451;&#26102;&#38388;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#39640;&#25928;&#35757;&#32451;&#30340;QNNs&#26469;&#32469;&#24320;&#36825;&#20123;&#36127;&#38754;&#32467;&#26524;&#65292;&#22312;&#25191;&#34892;&#32463;&#20856;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;QNNs&#21487;&#20197;&#23637;&#31034;&#20986;&#20219;&#24847;&#24120;&#25968;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#20869;&#23384;&#20998;&#31163;&#65292;&#19988;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#27599;&#20010;&#21333;&#20803;&#26684;&#37117;&#21487;&#20197;&#22312;&#24120;&#25968;&#26102;&#38388;&#20869;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20998;&#31163;&#36866;&#29992;&#20110;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#22312;&#20869;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#32463;&#20856;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#37327;&#23376;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical results in quantum machine learning have demonstrated a general trade-off between the expressive power of quantum neural networks (QNNs) and their trainability; as a corollary of these results, practical exponential separations in expressive power over classical machine learning models are believed to be infeasible as such QNNs take a time to train that is exponential in the model size. We here circumvent these negative results by constructing a hierarchy of efficiently trainable QNNs that exhibit unconditionally provable, polynomial memory separations of arbitrary constant degree over classical neural networks in performing a classical sequence modeling task. Furthermore, each unit cell of the introduced class of QNNs is computationally efficient, implementable in constant time on a quantum device. The classical networks we prove a separation over include well-known examples such as recurrent neural networks and Transformers. We show that quantum contextuality is th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02501</link><description>&lt;p&gt;
&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#32467;&#26500;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
The cell signaling structure function. (arXiv:2401.02501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;$(x,y,z,channel,time)$&#35270;&#39057;&#26174;&#31034;&#20102;&#32454;&#32990;&#36816;&#21160;&#21644;&#20449;&#21495;&#21160;&#21147;&#23398;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#19968;&#31181;&#22312;&#20116;&#32500;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#19981;&#38656;&#35201;&#39044;&#20808;&#20102;&#35299;&#39044;&#26399;&#30340;&#27169;&#24335;&#21160;&#21147;&#23398;&#20197;&#21450;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#26159;&#19968;&#31181;Kolmogorov&#32467;&#26500;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#26680;&#24515;&#21306;&#22495;&#30456;&#23545;&#20110;&#21608;&#22260;&#32454;&#32990;&#36136;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#26469;&#26368;&#20248;&#22320;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24230;&#37327;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;NCD&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#22312;&#20302;&#32500;&#23884;&#20837;&#20013;&#20316;&#20026;&#28857;&#30340;Hilbert&#31354;&#38388;&#30340;&#20877;&#29983;&#26680;&#65292;&#21487;&#20197;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display patterns of cellular motion and signaling dynamics. We present here an approach to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell microscopy movies unique in requiring no \emph{a priori} knowledge of expected pattern dynamics, and no training data. The proposed cell signaling structure function (SSF) is a Kolmogorov structure function that optimally measures cell signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a significant improvement compared to the current state-of-the-art cytonuclear ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value, or a functional output such as velocity. Patterns of similarity are identified via the metric normalized compression distance (NCD). The NCD is a reproducing kernel for a Hilbert space that represents the input SSF kymographs as points in a low dimensional embedding that optimally captures the pattern
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#65288;PNNP&#65289;&#29992;&#20110;&#20934;&#30830;&#22122;&#22768;&#24314;&#27169;&#21644;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#65292;&#38598;&#25104;&#20102;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#35299;&#32806;&#12289;&#29289;&#29702;&#24341;&#23548;&#20195;&#29702;&#27169;&#22411;&#21644;&#21487;&#24494;&#20998;&#20998;&#24067;&#23548;&#21521;&#25439;&#22833;&#31561;&#39640;&#25928;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.09126</link><description>&lt;p&gt;
&#29289;&#29702;&#24341;&#23548;&#30340;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#29992;&#20110;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Physics-guided Noise Neural Proxy for Low-light Raw Image Denoising. (arXiv:2310.09126v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#65288;PNNP&#65289;&#29992;&#20110;&#20934;&#30830;&#22122;&#22768;&#24314;&#27169;&#21644;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#65292;&#38598;&#25104;&#20102;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#35299;&#32806;&#12289;&#29289;&#29702;&#24341;&#23548;&#20195;&#29702;&#27169;&#22411;&#21644;&#21487;&#24494;&#20998;&#20998;&#24067;&#23548;&#21521;&#25439;&#22833;&#31561;&#39640;&#25928;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#22312;&#31227;&#21160;&#25668;&#24433;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#23398;&#20064;&#26041;&#27861;&#24050;&#25104;&#20026;&#20027;&#27969;&#26041;&#27861;&#12290;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#25104;&#20026;&#26367;&#20195;&#23545;&#24212;&#30495;&#23454;&#25968;&#25454;&#30340;&#39640;&#25928;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#36136;&#37327;&#21463;&#22122;&#22768;&#27169;&#22411;&#31934;&#24230;&#30340;&#38480;&#21046;&#65292;&#38477;&#20302;&#20102;&#20302;&#20809;&#21407;&#22987;&#22270;&#20687;&#21435;&#22122;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20934;&#30830;&#22122;&#22768;&#24314;&#27169;&#26694;&#26550;&#65292;&#23398;&#20064;&#19968;&#20010;&#20174;&#26263;&#22330;&#20013;&#33719;&#24471;&#30340;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#65288;PNNP&#65289;&#12290;PNNP&#38598;&#25104;&#20102;&#19977;&#31181;&#39640;&#25928;&#25216;&#26415;&#65306;&#29289;&#29702;&#24341;&#23548;&#22122;&#22768;&#35299;&#32806;&#65288;PND&#65289;&#65292;&#29289;&#29702;&#24341;&#23548;&#20195;&#29702;&#27169;&#22411;&#65288;PPM&#65289;&#21644;&#21487;&#24494;&#20998;&#20998;&#24067;&#23548;&#21521;&#25439;&#22833;&#65288;DDL&#65289;&#12290;PND&#23558;&#26263;&#22330;&#35299;&#32806;&#20026;&#19981;&#21516;&#30340;&#32452;&#20998;&#65292;&#24182;&#20197;&#28789;&#27963;&#30340;&#26041;&#24335;&#22788;&#29702;&#19981;&#21516;&#27700;&#24179;&#30340;&#22122;&#22768;&#65292;&#38477;&#20302;&#20102;&#22122;&#22768;&#31070;&#32463;&#20195;&#29702;&#30340;&#22797;&#26434;&#24230;&#12290;PPM&#36890;&#36807;&#24341;&#20837;&#29289;&#29702;&#20808;&#39564;&#26377;&#25928;&#22320;&#32422;&#26463;&#29983;&#25104;&#30340;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-light raw image denoising plays a crucial role in mobile photography, and learning-based methods have become the mainstream approach. Training the learning-based methods with synthetic data emerges as an efficient and practical alternative to paired real data. However, the quality of synthetic data is inherently limited by the low accuracy of the noise model, which decreases the performance of low-light raw image denoising. In this paper, we develop a novel framework for accurate noise modeling that learns a physics-guided noise neural proxy (PNNP) from dark frames. PNNP integrates three efficient techniques: physics-guided noise decoupling (PND), physics-guided proxy model (PPM), and differentiable distribution-oriented loss (DDL). The PND decouples the dark frame into different components and handles different levels of noise in a flexible manner, which reduces the complexity of the noise neural proxy. The PPM incorporates physical priors to effectively constrain the generated no
&lt;/p&gt;</description></item></channel></rss>