<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20844;&#24179;&#25512;&#33616;&#31995;&#32479;&#65292;&#21517;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20844;&#24179;&#27880;&#24847;&#21147;&#21644;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#20004;&#20010;&#32452;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.03365</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#30064;&#36136;&#21451;&#21892;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heterophily-Aware Fair Recommendation using Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20844;&#24179;&#25512;&#33616;&#31995;&#32479;&#65292;&#21517;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20844;&#24179;&#27880;&#24847;&#21147;&#21644;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#20004;&#20010;&#32452;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#35774;&#35745;&#20026;&#20026;&#26368;&#32456;&#29992;&#25143;&#26381;&#21153;&#65292;&#36824;&#35201;&#35753;&#20854;&#20182;&#21442;&#19982;&#32773;&#65288;&#22914;&#39033;&#30446;&#21644;&#39033;&#30446;&#20379;&#24212;&#21830;&#65289;&#20174;&#20013;&#21463;&#30410;&#12290;&#36825;&#20123;&#21442;&#19982;&#32773;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#25110;&#20914;&#31361;&#30340;&#30446;&#26631;&#21644;&#21033;&#30410;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#24615;&#21644;&#27969;&#34892;&#24230;&#20559;&#24046;&#32771;&#34385;&#30340;&#38656;&#27714;&#12290;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#26041;&#27861;&#20063;&#38754;&#20020;&#19981;&#20844;&#24179;&#24615;&#21644;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#25361;&#25112;&#65292;&#20854;&#24402;&#19968;&#21270;&#21644;&#32858;&#21512;&#36807;&#31243;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#32452;&#20214;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#65306;i&#65289;&#20844;&#24179;&#27880;&#24847;&#21147;&#65292;&#23427;&#22312;GNN&#30340;&#24402;&#19968;&#21270;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;&#28857;&#31215;&#65292;&#20197;&#20943;&#23569;&#33410;&#28857;&#24230;&#25968;&#30340;&#24433;&#21709;&#65307;ii&#65289;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#65292;&#20026;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve the end users, but also to benefit other participants, such as items and items providers. These participants may have different or conflicting goals and interests, which raise the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve items' side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) fairness-aware attention which incorporates dot product in the normalization process of GNNs, to decrease the effect of nodes' degrees, and ii) heterophily feature weighting to assign distinct weights to 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23618;&#30340;&#24490;&#29615;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#21516;&#26102;&#35299;&#37322;&#31995;&#32479;&#32423;&#21644;&#20010;&#20307;&#32423;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32676;&#20307;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.14973</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#24490;&#29615;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#21457;&#29616;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32676;&#20307;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Discovering group dynamics in synchronous time series via hierarchical recurrent switching-state models. (arXiv:2401.14973v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14973
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#30340;&#24490;&#29615;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#21516;&#26102;&#35299;&#37322;&#31995;&#32479;&#32423;&#21644;&#20010;&#20307;&#32423;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32676;&#20307;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#23545;&#21516;&#19968;&#26102;&#38388;&#27573;&#20869;&#22810;&#20010;&#23454;&#20307;&#30456;&#20114;&#20316;&#29992;&#32780;&#20135;&#29983;&#30340;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#36827;&#34892;&#24314;&#27169;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24314;&#27169;&#20010;&#20307;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#23545;&#25105;&#20204;&#30340;&#39044;&#26399;&#24212;&#29992;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#20854;&#20013;&#38598;&#20307;&#31995;&#32479;&#32423;&#34892;&#20026;&#24433;&#21709;&#30528;&#20010;&#20307;&#23454;&#20307;&#30340;&#36712;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#65292;&#21516;&#26102;&#35299;&#37322;&#31995;&#32479;&#32423;&#21644;&#20010;&#20307;&#32423;&#30340;&#21160;&#24577;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#31995;&#32479;&#32423;&#31163;&#25955;&#29366;&#24577;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#39537;&#21160;&#30528;&#38544;&#21547;&#30340;&#23454;&#20307;&#32423;&#38142;&#65292;&#36827;&#32780;&#25511;&#21046;&#27599;&#20010;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#12290;&#35266;&#27979;&#32467;&#26524;&#22312;&#23454;&#20307;&#21644;&#31995;&#32479;&#32423;&#30340;&#38142;&#20043;&#38388;&#36827;&#34892;&#21453;&#39304;&#65292;&#36890;&#36807;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#29366;&#24577;&#36716;&#25442;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#23618;&#20999;&#25442;&#24490;&#29615;&#21160;&#21147;&#23398;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#21464;&#20998;&#22352;&#26631;&#19978;&#21319;&#26356;&#26032;&#26469;&#23398;&#20064;&#65292;&#20854;&#22312;&#20010;&#20307;&#25968;&#37327;&#19978;&#21576;&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to model a collection of time series arising from multiple entities interacting over the same time period. Recent work focused on modeling individual time series is inadequate for our intended applications, where collective system-level behavior influences the trajectories of individual entities. To address such problems, we present a new hierarchical switching-state model that can be trained in an unsupervised fashion to simultaneously explain both system-level and individual-level dynamics. We employ a latent system-level discrete state Markov chain that drives latent entity-level chains which in turn govern the dynamics of each observed time series. Feedback from the observations to the chains at both the entity and system levels improves flexibility via context-dependent state transitions. Our hierarchical switching recurrent dynamical models can be learned via closed-form variational coordinate ascent updates to all latent chains that scale linearly in the number of indivi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.14961</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-To-End Set-Based Training for Neural Network Verification. (arXiv:2401.14961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21363;&#24494;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20135;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#38656;&#35201;&#23545;&#36755;&#20837;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#39318;&#27425;&#37319;&#29992;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#35757;&#32451;&#20986;&#21487;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#31616;&#21270;&#24050;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#32493;&#24418;&#24335;&#21270;&#40065;&#26834;&#24615;&#39564;&#35777;&#36807;&#31243;&#12290;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#35745;&#31639;&#26469;&#35757;&#32451;&#25972;&#20010;&#25200;&#21160;&#36755;&#20837;&#38598;&#21512;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network. Safety-critical environments require neural networks that are robust against input perturbations. However, training and formally verifying robust neural networks is challenging. We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network. While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs. Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify. In many cases, set-based trai
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02562</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32852;&#21512;&#34920;&#31034;&#36827;&#34892;&#39135;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Food Classification using Joint Representation of Visual and Textual Data. (arXiv:2308.02562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#20998;&#31867;&#26159;&#20581;&#24247;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#21516;&#26102;&#20351;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;UPMC Food-101&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#31532;&#20108;&#26368;&#22909;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
&lt;/p&gt;</description></item></channel></rss>