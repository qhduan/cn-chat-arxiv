<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.05811</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
Statistical Efficiency of Distributional Temporal Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05811
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;&#26102;&#38388;&#24046;&#20998;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(DRL)&#20851;&#27880;&#30340;&#26159;&#36820;&#22238;&#30340;&#23436;&#25972;&#20998;&#24067;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22343;&#20540;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#32463;&#39564;&#25104;&#21151;&#12290;&#39046;&#22495;DRL&#20013;&#30340;&#26680;&#24515;&#20219;&#21153;&#20043;&#19968;&#26159;&#20998;&#24067;&#24335;&#31574;&#30053;&#35780;&#20272;&#65292;&#28041;&#21450;&#20272;&#35745;&#32473;&#23450;&#31574;&#30053;pi&#30340;&#36820;&#22238;&#20998;&#24067;&#951;^pi&#12290;&#30456;&#24212;&#22320;&#25552;&#20986;&#20102;&#20998;&#24067;&#26102;&#38388;&#24046;&#20998;(TD)&#31639;&#27861;&#65292;&#36825;&#26159;&#32463;&#20856;RL&#25991;&#29486;&#20013;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;&#30340;&#24310;&#20280;&#12290;&#22312;&#34920;&#26684;&#26696;&#20363;&#20013;&#65292;citet{rowland2018analysis}&#21644;citet{rowland2023analysis}&#20998;&#21035;&#35777;&#26126;&#20102;&#20004;&#20010;&#20998;&#24067;&#24335;TD&#23454;&#20363;&#21363;&#20998;&#31867;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;(CTD)&#21644;&#20998;&#20301;&#25968;&#26102;&#38388;&#24046;&#20998;&#31639;&#27861;(QTD)&#30340;&#28176;&#36817;&#25910;&#25947;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#20998;&#24067;&#24335;TD&#30340;&#26377;&#38480;&#26679;&#26412;&#24615;&#33021;&#12290;&#20026;&#20102;&#20419;&#36827;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38750;&#21442;&#25968;&#30340; dis
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05811v1 Announce Type: cross  Abstract: Distributional reinforcement learning (DRL), which cares about the full distribution of returns instead of just the mean, has achieved empirical success in various domains. One of the core tasks in the field of DRL is distributional policy evaluation, which involves estimating the return distribution $\eta^\pi$ for a given policy $\pi$. A distributional temporal difference (TD) algorithm has been accordingly proposed, which is an extension of the temporal difference algorithm in the classic RL literature. In the tabular case, \citet{rowland2018analysis} and \citet{rowland2023analysis} proved the asymptotic convergence of two instances of distributional TD, namely categorical temporal difference algorithm (CTD) and quantile temporal difference algorithm (QTD), respectively. In this paper, we go a step further and analyze the finite-sample performance of distributional TD. To facilitate theoretical analysis, we propose non-parametric dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;Moreau&#21253;&#32476;&#26469;&#23545;&#27979;&#24230;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;Wasserstein&#26799;&#24230;&#27969;&#12290;</title><link>https://arxiv.org/abs/2402.04613</link><description>&lt;p&gt;
&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;Moreau&#21253;&#32476;&#30340;f-&#24046;&#24322;&#30340;Wasserstein&#26799;&#24230;&#27969;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;Moreau&#21253;&#32476;&#26469;&#23545;&#27979;&#24230;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;Wasserstein&#26799;&#24230;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#27979;&#24230;f-&#24046;&#24322;&#65292;&#20363;&#22914;Kullback-Leibler&#24046;&#24322;&#65292;&#23545;&#20110;&#25152;&#28041;&#21450;&#30340;&#27979;&#24230;&#30340;&#25903;&#25345;&#23384;&#22312;&#38480;&#21046;&#12290;&#35299;&#20915;&#21150;&#27861;&#26159;&#36890;&#36807;&#19982;&#29305;&#24449;&#26680;K&#30456;&#20851;&#30340;&#24179;&#26041;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;(MMD)&#23545;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25152;&#35859;&#30340;&#26680;&#22343;&#20540;&#23884;&#20837;&#26469;&#26174;&#31034;&#30456;&#24212;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#37325;&#20889;&#20026;&#19982;K&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#26576;&#20123;&#20989;&#25968;&#30340;Moreau&#21253;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20851;&#20110;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;Moreau&#21253;&#32476;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#32467;&#26524;&#26469;&#35777;&#26126;MMD&#27491;&#21017;&#21270;&#30340;f-&#24046;&#24322;&#21450;&#20854;&#26799;&#24230;&#30340;&#23646;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26469;&#20998;&#26512;&#21463;MMD&#27491;&#21017;&#21270;&#30340;f-&#24046;&#24322;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#32463;&#39564;&#27979;&#24230;&#24320;&#22987;&#30340;Wasserstein&#26799;&#24230;&#27969;&#65292;&#24182;&#25552;&#20379;&#20351;&#29992;Tsallis-$\alpha$&#24046;&#24322;&#30340;&#27010;&#24565;&#24615;&#25968;&#20540;&#31034;&#20363;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler divergence, are subject to limitations regarding the support of the involved measures. A remedy consists of regularizing the $f$-divergence by a squared maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In this paper, we use the so-called kernel mean embedding to show that the corresponding regularization can be rewritten as the Moreau envelope of some function in the reproducing kernel Hilbert space associated with $K$. Then, we exploit well-known results on Moreau envelopes in Hilbert spaces to prove properties of the MMD-regularized $f$-divergences and, in particular, their gradients. Subsequently, we use our findings to analyze Wasserstein gradient flows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein gradient flows starting from empirical measures and provide proof-of-the-concept numerical examples with Tsallis-$\alpha$ divergences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#19982;&#32447;&#24615;&#22810;&#26680;&#22312;&#22810;&#32500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26684;&#28857;&#35889;&#28151;&#21512;&#26680;&#20844;&#24335;&#65292;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20248;&#21270;&#32467;&#26500;&#21644;&#36924;&#36817;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#20351;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#21464;&#24471;&#21487;&#34892;&#12290;</title><link>http://arxiv.org/abs/2309.08201</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#19982;&#32447;&#24615;&#22810;&#26680;&#65306;&#39057;&#35889;&#35774;&#35745;&#21644;&#22810;&#32500;&#25968;&#25454;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Gaussian Processes with Linear Multiple Kernel: Spectrum Design and Distributed Learning for Multi-Dimensional Data. (arXiv:2309.08201v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#36807;&#31243;&#19982;&#32447;&#24615;&#22810;&#26680;&#22312;&#22810;&#32500;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26684;&#28857;&#35889;&#28151;&#21512;&#26680;&#20844;&#24335;&#65292;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#20248;&#21270;&#32467;&#26500;&#21644;&#36924;&#36817;&#33021;&#21147;&#12290;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#20351;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#21464;&#24471;&#21487;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;GP&#24314;&#27169;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#26680;&#20989;&#25968;&#30340;&#36873;&#25321;&#65292;&#32447;&#24615;&#22810;&#26680;&#65288;LMKs&#65289;&#22240;&#20854;&#24378;&#22823;&#30340;&#24314;&#27169;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#25104;&#20026;&#19968;&#20010;&#21560;&#24341;&#20154;&#30340;&#26680;&#20989;&#25968;&#31867;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#26684;&#28857;&#35889;&#28151;&#21512;&#65288;GSM&#65289;&#26680;&#65292;&#23427;&#26159;&#19968;&#31181;&#21487;&#20197;&#36817;&#20284;&#20219;&#24847;&#24179;&#31283;&#26680;&#30340;LMK&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GSM&#26680;&#20844;&#24335;&#65292;&#29992;&#20110;&#22810;&#32500;&#25968;&#25454;&#65292;&#30456;&#27604;&#29616;&#26377;&#20844;&#24335;&#20943;&#23569;&#20102;&#36229;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#26377;&#21033;&#30340;&#20248;&#21270;&#32467;&#26500;&#21644;&#36924;&#36817;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20351;GSM&#26680;&#20013;&#30340;&#22823;&#35268;&#27169;&#36229;&#21442;&#25968;&#20248;&#21270;&#21464;&#24471;&#21487;&#34892;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#20998;&#24067;&#24335;SCA&#65288;DSCA&#65289;&#31639;&#27861;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22522;&#20110;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#26694;&#26550;&#25552;&#20986;&#20102;&#21452;&#37325;&#20998;&#24067;&#24335;SCA&#65288;D$^2$SCA&#65289;&#31639;&#27861;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21512;&#20316;&#22320;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gaussian processes (GPs) have emerged as a prominent technique for machine learning and signal processing. A key component in GP modeling is the choice of kernel, and linear multiple kernels (LMKs) have become an attractive kernel class due to their powerful modeling capacity and interpretability. This paper focuses on the grid spectral mixture (GSM) kernel, an LMK that can approximate arbitrary stationary kernels. Specifically, we propose a novel GSM kernel formulation for multi-dimensional data that reduces the number of hyper-parameters compared to existing formulations, while also retaining a favorable optimization structure and approximation capability. In addition, to make the large-scale hyper-parameter optimization in the GSM kernel tractable, we first introduce the distributed SCA (DSCA) algorithm. Building on this, we propose the doubly distributed SCA (D$^2$SCA) algorithm based on the alternating direction method of multipliers (ADMM) framework, which allows us to cooperativ
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.09377</link><description>&lt;p&gt;
&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Language Aligned Visual Representations Predict Human Behavior in Naturalistic Learning Tasks. (arXiv:2306.09377v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09377
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#27604;&#32431;&#35270;&#35273;&#34920;&#31034;&#26041;&#24335;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#22791;&#35782;&#21035;&#21644;&#27010;&#25324;&#33258;&#28982;&#29289;&#20307;&#30456;&#20851;&#29305;&#24449;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#24773;&#22659;&#20013;&#26377;&#25152;&#24110;&#21161;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#29616;&#35937;&#24182;&#30830;&#23450;&#26368;&#26377;&#25928;&#30340;&#34920;&#31034;&#26041;&#24335;&#20197;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20004;&#20010;&#28041;&#21450;&#31867;&#21035;&#23398;&#20064;&#21644;&#22870;&#21169;&#23398;&#20064;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#20351;&#29992;&#36924;&#30495;&#30340;&#22270;&#20687;&#20316;&#20026;&#21050;&#28608;&#29289;&#65292;&#24182;&#35201;&#27714;&#21442;&#19982;&#32773;&#22522;&#20110;&#25152;&#26377;&#35797;&#39564;&#30340;&#26032;&#22411;&#21050;&#28608;&#29289;&#20316;&#20986;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#22240;&#27492;&#38656;&#35201;&#27867;&#21270;&#12290;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#65292;&#24213;&#23618;&#35268;&#21017;&#26159;&#20351;&#29992;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#25552;&#21462;&#30340;&#21050;&#28608;&#32500;&#24230;&#29983;&#25104;&#30340;&#31616;&#21333;&#32447;&#24615;&#20989;&#25968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21442;&#19982;&#32773;&#22312;&#20960;&#27425;&#35797;&#39564;&#20869;&#23601;&#25104;&#21151;&#22320;&#30830;&#23450;&#20102;&#30456;&#20851;&#30340;&#21050;&#28608;&#29305;&#24449;&#65292;&#35777;&#26126;&#20102;&#26377;&#25928;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#22411;&#27604;&#36739;&#65292;&#35780;&#20272;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#31034;&#23545;&#20154;&#31867;&#36873;&#25321;&#30340;&#36880;&#27425;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65288;&#22914;&#35821;&#35328;&#24314;&#27169;&#21644;&#26426;&#22120;&#32763;&#35793;&#65289;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#20248;&#20110;&#35270;&#35273;&#20219;&#21153;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#31034;&#65292;&#34920;&#26126;&#23545;&#40784;&#35821;&#35328;&#30340;&#35270;&#35273;&#34920;&#31034;&#21487;&#33021;&#26356;&#26377;&#25928;&#22320;&#39044;&#27979;&#20154;&#31867;&#22312;&#33258;&#28982;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models train
&lt;/p&gt;</description></item><item><title>PeFLL&#26159;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#65292;PeFLL&#33021;&#22815;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20854;&#23427;&#26032;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.05515</link><description>&lt;p&gt;
&#19968;&#31181;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#32456;&#36523;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PeFLL: A Lifelong Learning Approach to Personalized Federated Learning. (arXiv:2306.05515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05515
&lt;/p&gt;
&lt;p&gt;
PeFLL&#26159;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#30340;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#65292;PeFLL&#33021;&#22815;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#22312;&#20854;&#23427;&#26032;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#24050;&#25104;&#20026;&#24212;&#23545;&#21442;&#19982;&#23458;&#25143;&#31471;&#25968;&#25454;&#20998;&#24067;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#25361;&#25112;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;pFL&#19981;&#26159;&#23398;&#20064;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#32780;&#26159;&#26088;&#22312;&#23398;&#20064;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20010;&#20307;&#27169;&#22411;&#65292;&#21516;&#26102;&#20173;&#28982;&#21033;&#29992;&#20854;&#20182;&#23458;&#25143;&#31471;&#21487;&#29992;&#30340;&#25968;&#25454;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PeFLL&#65292;&#36825;&#26159;&#19968;&#31181;&#26681;&#26893;&#20110;&#32456;&#36523;&#23398;&#20064;&#30340;&#26032;&#22411;pFL&#26041;&#27861;&#65292;&#19981;&#20165;&#22312;&#35757;&#32451;&#38454;&#27573;&#23384;&#22312;&#30340;&#23458;&#25143;&#31471;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780;&#19988;&#22312;&#26410;&#26469;&#21487;&#33021;&#20986;&#29616;&#30340;&#23458;&#25143;&#31471;&#19978;&#20063;&#34920;&#29616;&#33391;&#22909;&#12290;PeFLL&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#23884;&#20837;&#32593;&#32476;&#21644;&#36229;&#32593;&#32476;&#26469;&#23398;&#20064;&#36755;&#20986;&#29305;&#23450;&#20110;&#23458;&#25143;&#31471;&#30340;&#27169;&#22411;&#12290;&#23884;&#20837;&#32593;&#32476;&#23398;&#20064;&#20197;&#19968;&#31181;&#21453;&#26144;&#23427;&#20204;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#28508;&#22312;&#25551;&#36848;&#31526;&#31354;&#38388;&#20013;&#34920;&#31034;&#23458;&#25143;&#31471;&#12290;&#36229;&#32593;&#32476;&#23398;&#20064;&#20174;&#36825;&#20010;&#28508;&#22312;&#31354;&#38388;&#21040;&#21487;&#33021;&#30340;&#23458;&#25143;&#27169;&#22411;&#31354;&#38388;&#30340;&#26144;&#23556;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;PeFLL&#20135;&#29983;&#20102;&#26356;&#39640;&#20934;&#30830;&#29575;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized federated learning (pFL) has emerged as a popular approach to dealing with the challenge of statistical heterogeneity between the data distributions of the participating clients. Instead of learning a single global model, pFL aims to learn an individual model for each client while still making use of the data available at other clients. In this work, we present PeFLL, a new pFL approach rooted in lifelong learning that performs well not only on clients present during its training phase, but also on any that may emerge in the future. PeFLL learns to output client specific models by jointly training an embedding network and a hypernetwork. The embedding network learns to represent clients in a latent descriptor space in a way that reflects their similarity to each other. The hypernetwork learns a mapping from this latent space to the space of possible client models. We demonstrate experimentally that PeFLL produces models of superior accuracy compared to previous methods, es
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#23558;&#37327;&#23376;&#32858;&#31867;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#28508;&#22312;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.14641</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#31639;&#27861;&#36827;&#34892;&#22270;&#20998;&#26512;&#65306;&#37327;&#23376;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Graphy Analysis Using a GPU-based Parallel Algorithm: Quantum Clustering. (arXiv:2305.14641v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#23558;&#37327;&#23376;&#32858;&#31867;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#20013;&#65292;&#20351;&#29992;&#22522;&#20110;GPU&#30340;&#24182;&#34892;&#31639;&#27861;&#26469;&#35745;&#31639;&#28508;&#22312;&#20540;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#37327;&#23376;&#32858;&#31867;&#24212;&#29992;&#20110;&#22270;&#32467;&#26500;&#30340;&#26032;&#26041;&#27861;&#12290;&#37327;&#23376;&#32858;&#31867;&#65288;QC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#28508;&#22312;&#20989;&#25968;&#26469;&#30830;&#23450;&#32858;&#31867;&#20013;&#24515;&#12290;&#22312;&#35813;&#26041;&#27861;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#25214;&#21040;&#32858;&#31867;&#20013;&#24515;&#12290;GPU&#24182;&#34892;&#21270;&#29992;&#20110;&#35745;&#31639;&#28508;&#22312;&#20540;&#12290;&#25105;&#20204;&#36824;&#23545;&#20116;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;&#22235;&#20010;&#25351;&#26631;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#32467;&#26524;&#26174;&#31034;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;$\sigma$&#23545;&#23454;&#39564;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The article introduces a new method for applying Quantum Clustering to graph structures. Quantum Clustering (QC) is a novel density-based unsupervised learning method that determines cluster centers by constructing a potential function. In this method, we use the Graph Gradient Descent algorithm to find the centers of clusters. GPU parallelization is utilized for computing potential values. We also conducted experiments on five widely used datasets and evaluated using four indicators. The results show superior performance of the method. Finally, we discuss the influence of $\sigma$ on the experimental results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;smoothed PLDA&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#22788;&#29702;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#22797;&#26434;&#24230;&#20026;O(epsilon^(-2/3))&#12290;</title><link>http://arxiv.org/abs/2209.10825</link><description>&lt;p&gt;
&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#30340;&#20840;&#23616;&#25910;&#25947;&#29575;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Rate Analysis of Nonsmooth Nonconvex-Nonconcave Minimax Optimization. (arXiv:2209.10825v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;smoothed PLDA&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#22788;&#29702;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#22797;&#26434;&#24230;&#20026;O(epsilon^(-2/3))&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#20248;&#21270;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#26799;&#24230;&#19979;&#38477;-&#19978;&#21319;&#65288;GDA&#65289;&#31639;&#27861;&#30340;&#21508;&#31181;&#21464;&#20307;&#19978;&#65292;&#36825;&#20123;&#31639;&#27861;&#20165;&#36866;&#29992;&#20110;&#24179;&#28369;&#30340;&#38750;&#20984;&#20985;&#22330;&#26223;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21517;&#20026;&#24179;&#28369;&#30340;&#36817;&#31471;&#32447;&#24615;&#19979;&#38477;&#19978;&#21319;&#65288;smoothed PLDA&#65289;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#22788;&#29702;&#24191;&#27867;&#30340;&#32467;&#26500;&#21270;&#38750;&#20809;&#28369;&#38750;&#20984;&#38750;&#20985;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#21407;&#22987;&#20989;&#25968;&#20855;&#26377;&#38750;&#20809;&#28369;&#22797;&#21512;&#32467;&#26500;&#65292;&#23545;&#20598;&#20989;&#25968;&#20855;&#26377;Kurdyka-L{o}jasiewicz&#65288;K\L{}&#65289;&#24615;&#36136;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25910;&#25947;&#20998;&#26512;&#26694;&#26550;&#26469;&#20998;&#26512;smoothed PLDA&#31639;&#27861;&#65292;&#20854;&#20013;&#20851;&#38190;&#32452;&#20214;&#26159;&#25105;&#20204;&#26368;&#26032;&#24320;&#21457;&#30340;&#38750;&#20809;&#28369;&#21407;&#22987;&#35823;&#24046;&#30028;&#21644;&#23545;&#20598;&#35823;&#24046;&#30028;&#23646;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;smoothed PLDA&#21487;&#20197;&#22312;&#20855;&#26377;&#38750;&#20809;&#28369;&#22797;&#21512;&#21407;&#22987;&#20989;&#25968;&#21644;KL&#23545;&#20598;&#20989;&#25968;&#30340;&#24191;&#27867;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#20013;&#25214;&#21040;$\varepsilon$-game-stationary&#28857;&#21644;$\varepsilon$-&#26368;&#20248;&#21270;&#31283;&#23450;&#28857;&#65292;&#20854;&#22797;&#26434;&#24230;&#20026;$\mathcal{O}(\varepsilon^{-2/3})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonconvex-nonconcave minimax optimization has gained widespread interest over the last decade. However, most existing work focuses on variants of gradient descent-ascent (GDA) algorithms, which are only applicable in smooth nonconvex-concave settings. To address this limitation, we propose a novel algorithm named smoothed proximal linear descent-ascent (smoothed PLDA), which can effectively handle a broad range of structured nonsmooth nonconvex-nonconcave minimax problems. Specifically, we consider the setting where the primal function has a nonsmooth composite structure and the dual function possesses the Kurdyka-\L{}ojasiewicz (K\L{}) property with exponent $\theta \in [0,1)$. We introduce a novel convergence analysis framework for smoothed PLDA, the key components of which are our newly developed nonsmooth primal error bound and dual error bound properties. Using this framework, we show that smoothed PLDA can find both $\epsilon$-game-stationary points and $\epsilon$-optimization-st
&lt;/p&gt;</description></item></channel></rss>