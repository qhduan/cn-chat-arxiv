<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#30340;GAN&#65292;&#21363;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;</title><link>https://arxiv.org/abs/2404.00081</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Molecular Generative Adversarial Network with Multi-Property Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00081
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#24378;&#21270;&#23398;&#20064;&#30340;GAN&#65292;&#21363;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#65292;&#24182;&#21033;&#29992;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#24050;&#34987;&#24212;&#29992;&#20110;&#33647;&#29289;&#21457;&#29616;&#20013;$de~novo$&#20998;&#23376;&#29983;&#25104;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#30740;&#31350;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#65292;&#26469;&#22788;&#29702;GANs&#20013;&#20998;&#23376;&#34920;&#31034;&#30340;&#31163;&#25955;&#29305;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;GANs&#21644;RL&#27169;&#22411;&#30340;&#22266;&#26377;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#65292;&#20197;&#21450;&#19982;MCTS&#37319;&#26679;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;MCTS RL-based GANs&#38590;&#20197;&#25193;&#23637;&#21040;&#22823;&#22411;&#21270;&#23398;&#25968;&#25454;&#24211;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24102;&#21363;&#26102;&#21644;&#20840;&#23616;&#22870;&#21169;&#30340;&#28436;&#21592;-&#35780;&#35770;&#23478;RL&#30340;&#26032;&#22411;GAN&#65292;&#31216;&#20026;InstGAN&#65292;&#20197;&#22312;&#20196;&#29260;&#32423;&#21035;&#19978;&#29983;&#25104;&#20855;&#26377;&#22810;&#23646;&#24615;&#20248;&#21270;&#30340;&#20998;&#23376;&#12290;&#27492;&#22806;&#65292;&#26368;&#22823;&#21270;&#20449;&#24687;&#29109;&#34987;&#21033;&#29992;&#26469;&#32531;&#35299;&#27169;&#24335;&#23849;&#28291;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;InstGAN&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#65292;&#36798;&#21040;&#20102;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00081v1 Announce Type: cross  Abstract: Deep generative models, such as generative adversarial networks (GANs), have been employed for $de~novo$ molecular generation in drug discovery. Most prior studies have utilized reinforcement learning (RL) algorithms, particularly Monte Carlo tree search (MCTS), to handle the discrete nature of molecular representations in GANs. However, due to the inherent instability in training GANs and RL models, along with the high computational cost associated with MCTS sampling, MCTS RL-based GANs struggle to scale to large chemical databases. To tackle these challenges, this study introduces a novel GAN based on actor-critic RL with instant and global rewards, called InstGAN, to generate molecules at the token-level with multi-property optimization. Furthermore, maximized information entropy is leveraged to alleviate the mode collapse. The experimental results demonstrate that InstGAN outperforms other baselines, achieves comparable performance
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;&#21463;&#25511;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#28040;&#38500;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#20559;&#24046;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19508</link><description>&lt;p&gt;
&#29992;&#21463;&#25511;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#28040;&#38500;&#24515;&#33039;&#25104;&#20687;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Debiasing Cardiac Imaging with Controlled Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;&#21463;&#25511;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#28040;&#38500;&#24515;&#33039;&#25104;&#20687;&#20013;&#30340;&#20559;&#24046;&#21644;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#23545;&#22522;&#20110;&#24515;&#33039;&#30913;&#20849;&#25391;&#25104;&#20687;&#36827;&#34892;&#30142;&#30149;&#35786;&#26029;&#21644;&#39044;&#21518;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#36827;&#23637;&#21463;&#21040;&#35757;&#32451;&#25968;&#25454;&#39640;&#24230;&#19981;&#24179;&#34913;&#21644;&#20559;&#24046;&#30340;&#38459;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#12289;&#24180;&#40836;&#12289;&#20307;&#37325;&#25351;&#25968;&#21644;&#20581;&#24247;&#29366;&#20917;&#65289;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#26469;&#20943;&#36731;&#25968;&#25454;&#38598;&#20013;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#24615;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;ControlNet&#26469;&#20197;&#24739;&#32773;&#20803;&#25968;&#25454;&#21644;&#20351;&#29992;&#22823;&#22411;&#38431;&#21015;&#30740;&#31350;&#65288;&#20855;&#20307;&#26469;&#35828;&#26159;UK Biobank&#65289;&#20013;&#20998;&#21106;&#25513;&#27169;&#23548;&#20986;&#30340;&#24515;&#33039;&#20960;&#20309;&#24418;&#29366;&#20026;&#26465;&#20214;&#29983;&#25104;&#25991;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#24050;&#24314;&#31435;&#30340;&#23450;&#37327;&#25351;&#26631;&#35780;&#20272;&#29983;&#25104;&#22270;&#20687;&#30340;&#36924;&#30495;&#31243;&#24230;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#19979;&#28216;&#20998;&#31867;&#20219;&#21153;&#65292;&#26088;&#22312;&#36890;&#36807;&#21512;&#25104;&#29983;&#25104;&#30340;&#26679;&#26412;&#32416;&#27491;&#20195;&#34920;&#24615;&#19981;&#36275;&#32676;&#20307;&#20869;&#30340;&#19981;&#24179;&#34913;&#26469;&#25913;&#27491;&#20998;&#31867;&#22120;&#30340;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31034;&#33539;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19508v1 Announce Type: cross  Abstract: The progress in deep learning solutions for disease diagnosis and prognosis based on cardiac magnetic resonance imaging is hindered by highly imbalanced and biased training data. To address this issue, we propose a method to alleviate imbalances inherent in datasets through the generation of synthetic data based on sensitive attributes such as sex, age, body mass index, and health condition. We adopt ControlNet based on a denoising diffusion probabilistic model to condition on text assembled from patient metadata and cardiac geometry derived from segmentation masks using a large-cohort study, specifically, the UK Biobank. We assess our method by evaluating the realism of the generated images using established quantitative metrics. Furthermore, we conduct a downstream classification task aimed at debiasing a classifier by rectifying imbalances within underrepresented groups through synthetically generated samples. Our experiments demons
&lt;/p&gt;</description></item><item><title>&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;</title><link>https://arxiv.org/abs/2403.13748</link><description>&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#20013;&#22240;&#23376;&#21270;&#39640;&#26031;&#36817;&#20284;&#30340;&#24046;&#24322;&#25490;&#24207;
&lt;/p&gt;
&lt;p&gt;
An Ordering of Divergences for Variational Inference with Factorized Gaussian Approximations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13748
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#25955;&#24230;&#25490;&#24207;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#24182;&#19988;&#22240;&#23376;&#21270;&#36817;&#20284;&#26080;&#27861;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#25512;&#26029;&#65288;VI&#65289;&#20013;&#65292;&#32473;&#23450;&#19968;&#20010;&#38590;&#20197;&#22788;&#29702;&#30340;&#20998;&#24067;$p$&#65292;&#38382;&#39064;&#26159;&#20174;&#19968;&#20123;&#26356;&#26131;&#22788;&#29702;&#30340;&#26063;$\mathcal{Q}$&#20013;&#35745;&#31639;&#26368;&#20339;&#36817;&#20284;$q$&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#36817;&#20284;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;Kullback-Leibler (KL)&#25955;&#24230;&#26469;&#25214;&#21040;&#30340;&#12290;&#28982;&#32780;&#65292;&#23384;&#22312;&#20854;&#20182;&#26377;&#25928;&#30340;&#25955;&#24230;&#36873;&#25321;&#65292;&#24403;$\mathcal{Q}$&#19981;&#21253;&#21547;$p$&#26102;&#65292;&#27599;&#20010;&#25955;&#24230;&#37117;&#25903;&#25345;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#39640;&#26031;&#30340;&#23494;&#38598;&#21327;&#26041;&#24046;&#30697;&#38453;&#34987;&#23545;&#35282;&#21327;&#26041;&#24046;&#30697;&#38453;&#30340;&#39640;&#26031;&#36817;&#20284;&#25152;&#24433;&#21709;&#30340;VI&#32467;&#26524;&#20013;&#65292;&#25955;&#24230;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;VI&#32467;&#26524;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19981;&#21516;&#30340;&#25955;&#24230;&#21487;&#20197;&#36890;&#36807;&#23427;&#20204;&#30340;&#21464;&#20998;&#36817;&#20284;&#35823;&#20272;&#19981;&#30830;&#23450;&#24615;&#30340;&#21508;&#31181;&#24230;&#37327;&#65292;&#22914;&#26041;&#24046;&#12289;&#31934;&#24230;&#21644;&#29109;&#65292;&#36827;&#34892;\textit{&#25490;&#24207;}&#12290;&#25105;&#20204;&#36824;&#24471;&#20986;&#19968;&#20010;&#19981;&#21487;&#33021;&#23450;&#29702;&#65292;&#34920;&#26126;&#26080;&#27861;&#36890;&#36807;&#22240;&#23376;&#21270;&#36817;&#20284;&#21516;&#26102;&#21305;&#37197;&#36825;&#20123;&#24230;&#37327;&#20013;&#30340;&#20219;&#24847;&#20004;&#20010;&#65307;&#22240;&#27492;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13748v1 Announce Type: cross  Abstract: Given an intractable distribution $p$, the problem of variational inference (VI) is to compute the best approximation $q$ from some more tractable family $\mathcal{Q}$. Most commonly the approximation is found by minimizing a Kullback-Leibler (KL) divergence. However, there exist other valid choices of divergences, and when $\mathcal{Q}$ does not contain~$p$, each divergence champions a different solution. We analyze how the choice of divergence affects the outcome of VI when a Gaussian with a dense covariance matrix is approximated by a Gaussian with a diagonal covariance matrix. In this setting we show that different divergences can be \textit{ordered} by the amount that their variational approximations misestimate various measures of uncertainty, such as the variance, precision, and entropy. We also derive an impossibility theorem showing that no two of these measures can be simultaneously matched by a factorized approximation; henc
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03726</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Diffusion on language model embeddings for protein sequence generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03726
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#35774;&#35745;&#38656;&#35201;&#23545;&#34507;&#30333;&#36136;&#23431;&#23449;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#23613;&#31649;&#35768;&#22810;&#24037;&#20316;&#20542;&#21521;&#20110;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#25110;&#19987;&#27880;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;&#23478;&#26063;&#65292;&#20294;&#26080;&#26465;&#20214;&#29983;&#25104;&#30340;&#22522;&#30784;&#20219;&#21153;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#21644;&#37325;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#36825;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;DiMA&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#20174;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;ESM-2&#34893;&#29983;&#30340;&#23884;&#20837;&#36827;&#34892;&#36830;&#32493;&#25193;&#25955;&#20197;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;DiMA&#36229;&#36234;&#20102;&#21253;&#25324;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#35828;&#26126;&#20102;&#23548;&#33268;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36328;&#22810;&#31181;&#24418;&#24335;&#24191;&#27867;&#35780;&#20272;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#12289;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#29983;&#29289;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20135;&#29983;&#26032;&#39062;&#12289;&#22810;&#26679;&#21270;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#31934;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03726v1 Announce Type: cross  Abstract: Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accura
&lt;/p&gt;</description></item><item><title>&#22238;&#22768;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36755;&#20837;&#26469;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#21253;&#21547;&#21518;&#32493;&#20196;&#29260;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#20805;&#20998;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.15449</link><description>&lt;p&gt;
&#37325;&#22797;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Repetition Improves Language Model Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15449
&lt;/p&gt;
&lt;p&gt;
&#22238;&#22768;&#23884;&#20837;&#26041;&#27861;&#36890;&#36807;&#37325;&#22797;&#36755;&#20837;&#26469;&#25552;&#21462;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#26080;&#27861;&#21253;&#21547;&#21518;&#32493;&#20196;&#29260;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#20805;&#20998;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25913;&#36827;&#20174;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#25552;&#21462;&#25991;&#26412;&#23884;&#20837;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#25913;&#36827;&#25968;&#25454;&#12289;&#39592;&#24178;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25110;&#36890;&#36807;&#25351;&#20196;&#25913;&#36827;&#20219;&#21153;&#24046;&#24322;&#21270;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#19968;&#20010;&#26550;&#26500;&#38480;&#21046;&#65306;&#20196;&#29260;&#23884;&#20837;&#19981;&#33021;&#21253;&#21547;&#26469;&#33258;&#36755;&#20837;&#20013;&#21518;&#32493;&#20196;&#29260;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#8220;&#22238;&#22768;&#23884;&#20837;&#8221;&#65292;&#20854;&#20013;&#25105;&#20204;&#22312;&#19978;&#19979;&#25991;&#20013;&#23558;&#36755;&#20837;&#37325;&#22797;&#20004;&#27425;&#65292;&#24182;&#20174;&#31532;&#20108;&#27425;&#20986;&#29616;&#20013;&#25552;&#21462;&#23884;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26089;&#26399;&#20196;&#29260;&#30340;&#22238;&#22768;&#23884;&#20837;&#21487;&#20197;&#32534;&#30721;&#20851;&#20110;&#21518;&#32493;&#20196;&#29260;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#20351;&#25105;&#20204;&#33021;&#22815;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;LLMs&#36827;&#34892;&#23884;&#20837;&#12290;&#22312;MTEB&#25490;&#34892;&#27036;&#19978;&#65292;&#22238;&#22768;&#23884;&#20837;&#22312;&#38646;&#23556;&#20987;&#20013;&#27604;&#32463;&#20856;&#23884;&#20837;&#25552;&#39640;&#20102;&#36229;&#36807;9%&#65292;&#22312;&#24494;&#35843;&#26102;&#25552;&#39640;&#20102;&#32422;0.7%&#12290;&#20351;&#29992;Mistral-7B&#27169;&#22411;&#30340;&#22238;&#22768;&#23884;&#20837;&#23454;&#29616;&#20102;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15449v1 Announce Type: new  Abstract: Recent approaches to improving the extraction of text embeddings from autoregressive large language models (LLMs) have largely focused on improvements to data, backbone pretrained language models, or improving task-differentiation via instructions. In this work, we address an architectural limitation of autoregressive models: token embeddings cannot contain information from tokens that appear later in the input. To address this limitation, we propose a simple approach, "echo embeddings," in which we repeat the input twice in context and extract embeddings from the second occurrence. We show that echo embeddings of early tokens can encode information about later tokens, allowing us to maximally leverage high-quality LLMs for embeddings. On the MTEB leaderboard, echo embeddings improve over classical embeddings by over 9% zero-shot and by around 0.7% when fine-tuned. Echo embeddings with a Mistral-7B model achieve state-of-the-art compared
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;</title><link>https://arxiv.org/abs/2402.12391</link><description>&lt;p&gt;
&#23454;&#29616;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#31185;&#23398;&#21457;&#29616;&#30340;AI&#31185;&#23398;&#23478;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Toward a Team of AI-made Scientists for Scientific Discovery from Gene Expression Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12391
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#65292;&#30001;&#27169;&#25311;&#35282;&#33394;&#21327;&#20316;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#24050;&#25104;&#20026;&#31185;&#23398;&#21457;&#29616;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#20174;&#22797;&#26434;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21517;&#20026;AI&#31185;&#23398;&#23478;&#22242;&#38431;&#65288;TAIS&#65289;&#65292;&#26088;&#22312;&#31616;&#21270;&#31185;&#23398;&#21457;&#29616;&#27969;&#31243;&#12290;TAIS&#21253;&#25324;&#27169;&#25311;&#35282;&#33394;&#65292;&#21253;&#25324;&#39033;&#30446;&#32463;&#29702;&#12289;&#25968;&#25454;&#24037;&#31243;&#24072;&#21644;&#39046;&#22495;&#19987;&#23478;&#65292;&#27599;&#20010;&#35282;&#33394;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#34920;&#12290;&#36825;&#20123;&#35282;&#33394;&#21327;&#20316;&#20197;&#22797;&#21046;&#25968;&#25454;&#31185;&#23398;&#23478;&#36890;&#24120;&#25191;&#34892;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#20851;&#27880;&#20110;&#35782;&#21035;&#20855;&#26377;&#30142;&#30149;&#39044;&#27979;&#20215;&#20540;&#30340;&#22522;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12391v1 Announce Type: cross  Abstract: Machine learning has emerged as a powerful tool for scientific discovery, enabling researchers to extract meaningful insights from complex datasets. For instance, it has facilitated the identification of disease-predictive genes from gene expression data, significantly advancing healthcare. However, the traditional process for analyzing such datasets demands substantial human effort and expertise for the data selection, processing, and analysis. To address this challenge, we introduce a novel framework, a Team of AI-made Scientists (TAIS), designed to streamline the scientific discovery pipeline. TAIS comprises simulated roles, including a project manager, data engineer, and domain expert, each represented by a Large Language Model (LLM). These roles collaborate to replicate the tasks typically performed by data scientists, with a specific focus on identifying disease-predictive genes. Furthermore, we have curated a benchmark dataset t
&lt;/p&gt;</description></item><item><title>AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12226</link><description>&lt;p&gt;
AnyGPT&#65306;&#32479;&#19968;&#30340;&#22810;&#27169;&#24335;&#31163;&#25955;&#24207;&#21015;&#24314;&#27169;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12226
&lt;/p&gt;
&lt;p&gt;
AnyGPT&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#31163;&#25955;&#34920;&#31034;&#23454;&#29616;&#21508;&#31181;&#27169;&#24577;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#33021;&#22815;&#22312;&#19981;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#25110;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#31283;&#23450;&#35757;&#32451;&#65292;&#20026;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#25972;&#21512;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; AnyGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#20219;&#24847;&#22810;&#27169;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;&#31163;&#25955;&#34920;&#31034;&#32479;&#19968;&#22788;&#29702;&#21508;&#31181;&#27169;&#24577;&#65292;&#21253;&#25324;&#35821;&#38899;&#12289;&#25991;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#20048;&#12290;AnyGPT &#21487;&#20197;&#31283;&#23450;&#35757;&#32451;&#65292;&#26080;&#38656;&#23545;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26550;&#26500;&#25110;&#35757;&#32451;&#33539;&#24335;&#36827;&#34892;&#20219;&#20309;&#25913;&#21160;&#12290;&#30456;&#21453;&#65292;&#23427;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#32423;&#39044;&#22788;&#29702;&#65292;&#20419;&#36827;&#20102;&#26032;&#27169;&#24577;&#30340;&#26080;&#32541;&#38598;&#25104;&#21040;LLM&#20013;&#65292;&#31867;&#20284;&#20110;&#26032;&#35821;&#35328;&#30340;&#25972;&#21512;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24335;&#25991;&#26412;&#20013;&#24515;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#22810;&#27169;&#24335;&#23545;&#40784;&#39044;&#35757;&#32451;&#12290;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#21512;&#25104;&#20102;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#20219;&#24847;&#22810;&#27169;&#24335;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#23427;&#21253;&#25324;108k&#20010;&#22810;&#36718;&#23545;&#35805;&#31034;&#20363;&#65292;&#31934;&#32454;&#22320;&#20132;&#32455;&#21508;&#31181;&#27169;&#24577;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#22788;&#29702;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#20219;&#24847;&#32452;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AnyGPT&#33021;&#22815;&#20419;&#36827;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12226v1 Announce Type: cross  Abstract: We introduce AnyGPT, an any-to-any multimodal language model that utilizes discrete representations for the unified processing of various modalities, including speech, text, images, and music. AnyGPT can be trained stably without any alterations to the current large language model (LLM) architecture or training paradigms. Instead, it relies exclusively on data-level preprocessing, facilitating the seamless integration of new modalities into LLMs, akin to the incorporation of new languages. We build a multimodal text-centric dataset for multimodal alignment pre-training. Utilizing generative models, we synthesize the first large-scale any-to-any multimodal instruction dataset. It consists of 108k samples of multi-turn conversations that intricately interweave various modalities, thus equipping the model to handle arbitrary combinations of multimodal inputs and outputs. Experimental results demonstrate that AnyGPT is capable of facilitat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#65292;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#38544;&#31169;&#24615;&#21644;&#23454;&#29992;&#24615;&#31561;&#26032;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#23545;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#24635;&#32467;&#12290;</title><link>https://arxiv.org/abs/2402.06806</link><description>&lt;p&gt;
&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#30340;&#21407;&#21017;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Towards Principled Assessment of Tabular Data Synthesis Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#65292;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#38544;&#31169;&#24615;&#21644;&#23454;&#29992;&#24615;&#31561;&#26032;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#35780;&#20272;&#25351;&#26631;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#36825;&#20010;&#26694;&#26550;&#65292;&#23545;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21512;&#25104;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#21033;&#29992;&#25968;&#25454;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22823;&#37327;&#30340;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#21512;&#25104;&#22120;&#65289;&#12290;&#19968;&#20123;&#21512;&#25104;&#22120;&#28385;&#36275;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#26088;&#22312;&#20197;&#21551;&#21457;&#24335;&#30340;&#26041;&#24335;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#12290;&#30001;&#20110;&#32570;&#20047;&#21407;&#21017;&#24615;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#23545;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#21644;&#26368;&#26032;&#30340;&#22522;&#20110;&#36793;&#38469;&#30340;&#21512;&#25104;&#22120;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38754;&#23545;&#38754;&#27604;&#36739;&#30340;&#26032;&#24320;&#21457;&#30340;&#21512;&#25104;&#22120;&#30340;&#29702;&#35299;&#23578;&#19981;&#20840;&#38754;&#65292;&#23545;&#36825;&#20123;&#21512;&#25104;&#22120;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#30340;&#20840;&#38754;&#20102;&#35299;&#20173;&#28982;&#38590;&#20197;&#23454;&#29616;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21407;&#21017;&#24615;&#21644;&#31995;&#32479;&#21270;&#30340;&#35780;&#20272;&#26694;&#26550;&#26469;&#35780;&#20272;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26816;&#26597;&#21644;&#25209;&#35780;&#29616;&#26377;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#32452;&#26032;&#30340;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#20854;&#38480;&#21046;&#65292;&#21253;&#25324;&#20445;&#30495;&#24230;&#12289;&#38544;&#31169;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#22522;&#20110;&#25552;&#20986;&#30340;&#25351;&#26631;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35780;&#20272;&#32452;&#32455;&#26694;&#26550;&#65292;&#20197;&#23545;&#19981;&#21516;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#24182;&#36827;&#34892;&#27604;&#36739;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data synthesis has been advocated as an important approach for utilizing data while protecting data privacy. A large number of tabular data synthesis algorithms (which we call synthesizers) have been proposed. Some synthesizers satisfy Differential Privacy, while others aim to provide privacy in a heuristic fashion. A comprehensive understanding of the strengths and weaknesses of these synthesizers remains elusive due to lacking principled evaluation metrics and missing head-to-head comparisons of newly developed synthesizers that take advantage of diffusion models and large language models with state-of-the-art marginal-based synthesizers.   In this paper, we present a principled and systematic evaluation framework for assessing tabular data synthesis algorithms. Specifically, we examine and critique existing evaluation metrics, and introduce a set of new metrics in terms of fidelity, privacy, and utility to address their limitations. Based on the proposed metrics, we also devise a un
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21464;&#20998;Shapley&#32593;&#32476;&#65292;&#36890;&#36807;&#27010;&#29575;&#21270;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#35745;&#31639;Shapley&#20540;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#20272;&#35745;&#27169;&#22411;&#36793;&#38469;&#20540;&#21644;&#22788;&#29702;&#35299;&#37322;&#21487;&#21464;&#24615;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04211</link><description>&lt;p&gt;
&#21464;&#20998;Shapley&#32593;&#32476;&#65306;&#19968;&#31181;&#27010;&#29575;&#21270;&#30340;&#26041;&#27861;&#29992;&#20110;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#33258;&#35299;&#37322;Shapley&#20540;
&lt;/p&gt;
&lt;p&gt;
Variational Shapley Network: A Probabilistic Approach to Self-Explaining Shapley values with Uncertainty Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#21464;&#20998;Shapley&#32593;&#32476;&#65292;&#36890;&#36807;&#27010;&#29575;&#21270;&#30340;&#26041;&#27861;&#31616;&#21270;&#20102;&#35745;&#31639;Shapley&#20540;&#30340;&#36807;&#31243;&#65292;&#24182;&#35299;&#20915;&#20102;&#20272;&#35745;&#27169;&#22411;&#36793;&#38469;&#20540;&#21644;&#22788;&#29702;&#35299;&#37322;&#21487;&#21464;&#24615;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley&#20540;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#38416;&#26126;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#30340;&#22522;&#30784;&#24037;&#20855;&#12290;&#23613;&#31649;&#23427;&#20204;&#34987;&#24191;&#27867;&#37319;&#29992;&#24182;&#20855;&#26377;&#28385;&#36275;&#37325;&#35201;&#21487;&#35299;&#37322;&#24615;&#20844;&#29702;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#20294;&#22312;&#20272;&#35745;&#36807;&#31243;&#20013;&#20173;&#28982;&#23384;&#22312;&#35745;&#31639;&#25361;&#25112;&#65292;&#21253;&#25324;&#65288;i&#65289;&#23545;&#27169;&#22411;&#22312;&#25152;&#26377;&#21487;&#33021;&#30340;&#36755;&#20837;&#29305;&#24449;&#32452;&#21512;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#65288;ii&#65289;&#20272;&#35745;&#27169;&#22411;&#30340;&#36793;&#38469;&#20540;&#65292;&#20197;&#21450;&#65288;iii&#65289;&#22788;&#29702;&#35299;&#37322;&#30340;&#21487;&#21464;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#35299;&#37322;&#26041;&#27861;&#65292;&#26174;&#33879;&#31616;&#21270;&#20102;Shapley&#20540;&#30340;&#35745;&#31639;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#21069;&#21521;&#20256;&#36882;&#12290;&#37492;&#20110;Shapley&#20540;&#30340;&#30830;&#23450;&#24615;&#22788;&#29702;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#38480;&#21046;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#27010;&#29575;&#26694;&#26550;&#32435;&#20837;&#20854;&#20013;&#20197;&#25429;&#25417;&#35299;&#37322;&#20013;&#22266;&#26377;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#20854;&#20182;&#26367;&#20195;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#19981;&#30452;&#25509;&#20381;&#36182;&#20110;&#35266;&#27979;&#25968;&#25454;&#31354;&#38388;&#26469;&#20272;&#35745;&#36793;&#38469;&#20540;&#65307;&#30456;&#21453;&#65292;&#23427;&#20351;&#29992;&#20174;&#28508;&#22312;&#30340;&#12289;&#29305;&#23450;&#20110;&#29305;&#24449;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#27966;&#29983;&#20986;&#30340;&#21487;&#36866;&#24212;&#30340;&#22522;&#32447;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shapley values have emerged as a foundational tool in machine learning (ML) for elucidating model decision-making processes. Despite their widespread adoption and unique ability to satisfy essential explainability axioms, computational challenges persist in their estimation when ($i$) evaluating a model over all possible subset of input feature combinations, ($ii$) estimating model marginals, and ($iii$) addressing variability in explanations. We introduce a novel, self-explaining method that simplifies the computation of Shapley values significantly, requiring only a single forward pass. Recognizing the deterministic treatment of Shapley values as a limitation, we explore incorporating a probabilistic framework to capture the inherent uncertainty in explanations. Unlike alternatives, our technique does not rely directly on the observed data space to estimate marginals; instead, it uses adaptable baseline values derived from a latent, feature-specific embedding space, generated by a no
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#25918;&#23485;&#26465;&#20214;&#19979;&#30340;&#20998;&#21306;&#20998;&#31867;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20986;&#20102;&#32477;&#23545;&#36830;&#32493;&#20998;&#37327;&#30340;&#26032;&#29305;&#24615;&#65292;&#35745;&#31639;&#20102;&#20998;&#31867;&#38169;&#35823;&#27010;&#29575;&#30340;&#31934;&#30830;&#25910;&#25947;&#29575;</title><link>https://arxiv.org/abs/2312.14889</link><description>&lt;p&gt;
&#35770;&#20174;&#21487;&#35266;&#27979;&#21644;&#31169;&#23494;&#25968;&#25454;&#20013;&#23454;&#29616;&#36895;&#29575;&#26368;&#20248;&#20998;&#21306;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
On Rate-Optimal Partitioning Classification from Observable and from Privatised Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14889
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#25918;&#23485;&#26465;&#20214;&#19979;&#30340;&#20998;&#21306;&#20998;&#31867;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20986;&#20102;&#32477;&#23545;&#36830;&#32493;&#20998;&#37327;&#30340;&#26032;&#29305;&#24615;&#65292;&#35745;&#31639;&#20102;&#20998;&#31867;&#38169;&#35823;&#27010;&#29575;&#30340;&#31934;&#30830;&#25910;&#25947;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#21306;&#20998;&#31867;&#30340;&#32463;&#20856;&#26041;&#27861;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25918;&#23485;&#26465;&#20214;&#19979;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#21253;&#25324;&#21487;&#35266;&#27979;&#65288;&#38750;&#31169;&#23494;&#65289;&#21644;&#31169;&#23494;&#25968;&#25454;&#12290;&#25105;&#20204;&#20551;&#35774;&#29305;&#24449;&#21521;&#37327;$X$&#21462;&#20540;&#20110;$\mathbb{R}^d$&#65292;&#20854;&#26631;&#31614;&#20026;$Y$&#12290;&#20043;&#21069;&#20851;&#20110;&#20998;&#21306;&#20998;&#31867;&#22120;&#30340;&#32467;&#26524;&#22522;&#20110;&#24378;&#23494;&#24230;&#20551;&#35774;&#65292;&#36825;&#31181;&#20551;&#35774;&#38480;&#21046;&#36739;&#22823;&#65292;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#30340;&#20363;&#23376;&#21152;&#20197;&#35777;&#26126;&#12290;&#25105;&#20204;&#20551;&#35774;$X$&#30340;&#20998;&#24067;&#26159;&#32477;&#23545;&#36830;&#32493;&#20998;&#24067;&#21644;&#31163;&#25955;&#20998;&#24067;&#30340;&#28151;&#21512;&#20307;&#65292;&#20854;&#20013;&#32477;&#23545;&#36830;&#32493;&#20998;&#37327;&#38598;&#20013;&#20110;&#19968;&#20010;$d_a$&#32500;&#23376;&#31354;&#38388;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#22312;&#26356;&#23485;&#26494;&#30340;&#26465;&#20214;&#19979;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#38500;&#20102;&#26631;&#20934;&#30340;Lipschitz&#21644;&#36793;&#38469;&#26465;&#20214;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#32477;&#23545;&#36830;&#32493;&#20998;&#37327;&#30340;&#19968;&#20010;&#26032;&#29305;&#24615;&#65292;&#36890;&#36807;&#35813;&#29305;&#24615;&#35745;&#31639;&#20102;&#20998;&#31867;&#38169;&#35823;&#27010;&#29575;&#30340;&#31934;&#30830;&#25910;&#25947;&#29575;&#65292;&#23545;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14889v2 Announce Type: replace-cross  Abstract: In this paper we revisit the classical method of partitioning classification and study its convergence rate under relaxed conditions, both for observable (non-privatised) and for privatised data. Let the feature vector $X$ take values in $\mathbb{R}^d$ and denote its label by $Y$. Previous results on the partitioning classifier worked with the strong density assumption, which is restrictive, as we demonstrate through simple examples. We assume that the distribution of $X$ is a mixture of an absolutely continuous and a discrete distribution, such that the absolutely continuous component is concentrated to a $d_a$ dimensional subspace. Here, we study the problem under much milder assumptions: in addition to the standard Lipschitz and margin conditions, a novel characteristic of the absolutely continuous component is introduced, by which the exact convergence rate of the classification error probability is calculated, both for the
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#30697;&#35282;&#22797;&#21512;&#20307;&#30340;&#21516;&#35843;&#27010;&#24565;&#23450;&#20041;&#20102;&#21452;&#20998;&#32423;&#25345;&#20037;&#24615;&#26465;&#30721;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#23450;&#29702;</title><link>https://arxiv.org/abs/2303.14694</link><description>&lt;p&gt;
&#19968;&#20010;&#20851;&#20110;&#21452;&#20998;&#32423;&#25345;&#20037;&#24615;&#26465;&#30721;&#30340;&#31283;&#23450;&#24615;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;
A stability theorem for bigraded persistence barcodes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14694
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30697;&#35282;&#22797;&#21512;&#20307;&#30340;&#21516;&#35843;&#27010;&#24565;&#23450;&#20041;&#20102;&#21452;&#20998;&#32423;&#25345;&#20037;&#24615;&#26465;&#30721;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#31283;&#23450;&#24615;&#23450;&#29702;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#36234;&#25176;&#37324;&#26031;-&#37324;&#26222;&#26031;&#28388;&#27874;&#30340;&#30697;&#35282;&#22797;&#21512;&#20307;&#30340;&#26222;&#36890;&#21644;&#21452;&#37325;&#21516;&#35843;&#23450;&#20041;&#20102;&#26377;&#38480;&#20266;&#24230;&#37327;&#31354;&#38388;X&#30340;&#21452;&#20998;&#32423;&#25345;&#20037;&#21516;&#35843;&#27169;&#21644;&#21452;&#20998;&#32423;&#26465;&#30721;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20851;&#20110;&#21452;&#20998;&#32423;&#25345;&#20037;&#24615;&#21452;&#21516;&#35843;&#27169;&#21644;&#26465;&#30721;&#30340;&#31283;&#23450;&#24615;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14694v2 Announce Type: replace-cross  Abstract: We define bigraded persistent homology modules and bigraded barcodes of a finite pseudo-metric space X using the ordinary and double homology of the moment-angle complex associated with the Vietoris-Rips filtration of X. We prove a stability theorem for the bigraded persistent double homology modules and barcodes.
&lt;/p&gt;</description></item><item><title>Multi-EuP&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25552;&#20379;&#20102;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#12290;</title><link>http://arxiv.org/abs/2311.01870</link><description>&lt;p&gt;
&#22810;&#35821;&#35328;&#27431;&#27954;&#35758;&#20250;&#25968;&#25454;&#38598;&#29992;&#20110;&#20998;&#26512;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Multi-EuP: The Multilingual European Parliament Dataset for Analysis of Bias in Information Retrieval. (arXiv:2311.01870v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01870
&lt;/p&gt;
&lt;p&gt;
Multi-EuP&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#26088;&#22312;&#30740;&#31350;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25552;&#20379;&#20102;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#21644;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#21487;&#29992;&#20110;&#35780;&#20272;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Multi-EuP&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#27431;&#27954;&#35758;&#20250;&#30340;22K&#20010;&#22810;&#35821;&#35328;&#25991;&#26723;&#65292;&#28085;&#30422;&#20102;24&#31181;&#35821;&#35328;&#12290;&#35813;&#25968;&#25454;&#38598;&#26088;&#22312;&#30740;&#31350;&#22810;&#35821;&#35328;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#29615;&#22659;&#19979;&#30340;&#20844;&#24179;&#24615;&#65292;&#20197;&#20998;&#26512;&#22312;&#25490;&#21517;&#19978;&#30340;&#35821;&#35328;&#21644;&#20154;&#21475;&#20559;&#35265;&#12290;&#23427;&#25317;&#26377;&#19968;&#20010;&#30495;&#23454;&#30340;&#22810;&#35821;&#35328;&#35821;&#26009;&#24211;&#65292;&#20854;&#20013;&#30340;&#20027;&#39064;&#34987;&#32763;&#35793;&#25104;&#20102;&#25152;&#26377;24&#31181;&#35821;&#35328;&#65292;&#24182;&#25552;&#20379;&#36328;&#35821;&#35328;&#30456;&#20851;&#24615;&#35780;&#21028;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#19982;&#25991;&#26723;&#30456;&#20851;&#30340;&#20016;&#23500;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#65292;&#20415;&#20110;&#30740;&#31350;&#20154;&#21475;&#20559;&#35265;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;Multi-EuP&#22312;&#21333;&#35821;&#21644;&#22810;&#35821;&#20449;&#24687;&#26816;&#32034;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#19968;&#39033;&#20851;&#20110;&#26631;&#35760;&#21270;&#31574;&#30053;&#36873;&#25321;&#24341;&#36215;&#30340;&#35821;&#35328;&#20559;&#35265;&#30340;&#21021;&#27493;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Multi-EuP, a new multilingual benchmark dataset, comprising 22K multi-lingual documents collected from the European Parliament, spanning 24 languages. This dataset is designed to investigate fairness in a multilingual information retrieval (IR) context to analyze both language and demographic bias in a ranking context. It boasts an authentic multilingual corpus, featuring topics translated into all 24 languages, as well as cross-lingual relevance judgments. Furthermore, it offers rich demographic information associated with its documents, facilitating the study of demographic bias. We report the effectiveness of Multi-EuP for benchmarking both monolingual and multilingual IR. We also conduct a preliminary experiment on language bias caused by the choice of tokenization strategy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#36830;&#32493;Galerkin&#26041;&#27861;&#20013;&#21152;&#20837;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#35745;&#31639;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.18897</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#22686;&#24378;&#20302;&#38454;&#19981;&#36830;&#32493;Galerkin&#26041;&#27861;&#22312;&#21487;&#21387;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-Order Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier--Stokes Equations. (arXiv:2310.18897v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#36830;&#32493;Galerkin&#26041;&#27861;&#20013;&#21152;&#20837;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#35745;&#31639;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#38271;&#65292;&#27169;&#25311;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#21644;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#36890;&#24120;&#20250;&#36816;&#34892;&#19968;&#20010;&#20302;&#20445;&#30495;&#24230;&#27169;&#22411;&#24182;&#37319;&#29992;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#65292;&#20294;&#36873;&#25321;&#36866;&#24403;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#24182;&#23545;&#20854;&#36827;&#34892;&#35843;&#33410;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#22312;&#19981;&#36830;&#32493;Galerkin&#65288;DG&#65289;&#31354;&#38388;&#31163;&#25955;&#21270;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#25311;&#20013;&#24341;&#20837;&#31070;&#32463;&#24120;&#24494;&#20998;&#31639;&#23376;&#26469;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36830;&#32493;&#32423;&#21035;&#19978;&#23398;&#20064;&#20302;&#38454;DG&#27714;&#35299;&#22120;&#20013;&#32570;&#22833;&#30340;&#23610;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20302;&#38454;DG&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20197;&#19968;&#23450;&#31243;&#24230;&#30340;&#31934;&#24230;&#21152;&#36895;&#28388;&#27874;&#39640;&#38454;DG&#27169;&#25311;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing computing power over the years has enabled simulations to become more complex and accurate. While immensely valuable for scientific discovery and problem-solving, however, high-fidelity simulations come with significant computational demands. As a result, it is common to run a low-fidelity model with a subgrid-scale model to reduce the computational cost, but selecting the appropriate subgrid-scale models and tuning them are challenging. We propose a novel method for learning the subgrid-scale model effects when simulating partial differential equations augmented by neural ordinary differential operators in the context of discontinuous Galerkin (DG) spatial discretization. Our approach learns the missing scales of the low-order DG solver at a continuous level and hence improves the accuracy of the low-order DG approximations as well as accelerates the filtered high-order DG simulations with a certain degree of precision. We demonstrate the performance of our approach throug
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2310.07917</link><description>&lt;p&gt;
&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#21644;&#26410;&#26469;&#36235;&#21183;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Machine Learning Techniques in Imbalanced Data and Future Trends. (arXiv:2310.07917v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07917
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#65292;&#26088;&#22312;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#22312;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#20013;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#26816;&#27979;&#32597;&#35265;&#20107;&#20214;&#19968;&#30452;&#26159;&#25968;&#25454;&#25366;&#25496;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#38382;&#39064;&#28608;&#21457;&#20102;&#30740;&#31350;&#20154;&#21592;&#36827;&#19968;&#27493;&#25913;&#36827;&#25968;&#25454;&#22788;&#29702;&#21644;&#31639;&#27861;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#38750;&#24179;&#34913;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#35770;&#25991;&#25910;&#38598;&#21644;&#23457;&#26597;&#20102;258&#31687;&#26469;&#33258;&#26399;&#21002;&#21644;&#20250;&#35758;&#35770;&#25991;&#30340;&#21516;&#34892;&#35780;&#23457;&#35770;&#25991;&#65292;&#26088;&#22312;&#20174;&#25216;&#26415;&#21644;&#24212;&#29992;&#35282;&#24230;&#28145;&#20837;&#23457;&#26597;&#38750;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#21508;&#31181;&#26041;&#27861;&#12290;&#35813;&#24037;&#20316;&#26088;&#22312;&#20026;&#22312;&#23398;&#26415;&#30028;&#25110;&#24037;&#19994;&#30028;&#24076;&#26395;&#28145;&#20837;&#23398;&#20064;&#22823;&#35268;&#27169;&#38750;&#24179;&#34913;&#25968;&#25454;&#19979;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#32508;&#36848;&#65292;&#24182;&#20026;&#20182;&#20204;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
For over two decades, detecting rare events has been a challenging task among researchers in the data mining and machine learning domain. Real-life problems inspire researchers to navigate and further improve data processing and algorithmic approaches to achieve effective and computationally efficient methods for imbalanced learning. In this paper, we have collected and reviewed 258 peer-reviewed papers from archival journals and conference papers in an attempt to provide an in-depth review of various approaches in imbalanced learning from technical and application perspectives. This work aims to provide a structured review of methods used to address the problem of imbalanced data in various domains and create a general guideline for researchers in academia or industry who want to dive into the broad field of machine learning using large-scale imbalanced data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2309.14394</link><description>&lt;p&gt;
&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation. (arXiv:2309.14394v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#22122;&#22768;&#25193;&#25955;&#27169;&#22411;&#65288;MDD&#65289;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#32423;&#21035;&#26469;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#20219;&#24847;&#22495;&#20043;&#38388;&#30340;&#32763;&#35793;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22495;&#38388;&#32763;&#35793;&#28041;&#21450;&#22312;&#32473;&#23450;&#28304;&#22495;&#26465;&#20214;&#19979;&#29983;&#25104;&#30446;&#26631;&#22495;&#26679;&#26412;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#38598;&#20013;&#22312;&#22266;&#23450;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#19978;&#65292;&#21363;&#23427;&#20204;&#20165;&#36866;&#29992;&#20110;&#29305;&#23450;&#30340;&#37197;&#32622;&#65288;&#20363;&#22914;&#23545;&#20110;&#20004;&#20010;&#22495;&#65292;&#35201;&#20040;$D_1\rightarrow{}D_2$&#65292;&#35201;&#20040;$D_2\rightarrow{}D_1$&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Multi-Domain Diffusion&#65288;MDD&#65289;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#22495;&#32763;&#35793;&#30340;&#26465;&#20214;&#25193;&#25955;&#26694;&#26550;&#12290;&#19982;&#20197;&#24448;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;MDD&#19981;&#38656;&#35201;&#23450;&#20041;&#36755;&#20837;&#21644;&#36755;&#20986;&#22495;&#65292;&#20801;&#35768;&#22312;&#19968;&#32452;&#22495;&#30340;&#20219;&#20309;&#20998;&#21306;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65288;&#20363;&#22914;$(D_1, D_2)\rightarrow{}D_3$&#65292;$D_2\rightarrow{}(D_1, D_3)$&#65292;$D_3\rightarrow{}D_1$&#31561;&#65289;&#65292;&#32780;&#26080;&#38656;&#20026;&#27599;&#20010;&#22495;&#37197;&#32622;&#35757;&#32451;&#21333;&#29420;&#30340;&#27169;&#22411;&#12290;MDD&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22122;&#22768;&#24418;&#24335;&#65292;&#36890;&#36807;&#20026;&#27599;&#20010;&#22495;&#24341;&#20837;&#19968;&#20010;&#22122;&#22768;&#32423;&#21035;&#65292;&#20197;&#33258;&#28982;&#30340;&#26041;&#24335;&#23545;&#32570;&#22833;&#30340;&#22495;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#23558;&#20256;&#32479;&#30340;&#32763;&#35793;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#36890;&#36807;&#22122;&#22768;&#24314;&#27169;&#26469;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain-to-domain translation involves generating a target domain sample given a condition in the source domain. Most existing methods focus on fixed input and output domains, i.e. they only work for specific configurations (i.e. for two domains, either $D_1\rightarrow{}D_2$ or $D_2\rightarrow{}D_1$). This paper proposes Multi-Domain Diffusion (MDD), a conditional diffusion framework for multi-domain translation in a semi-supervised context. Unlike previous methods, MDD does not require defining input and output domains, allowing translation between any partition of domains within a set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for 3 domains), without the need to train separate models for each domain configuration. The key idea behind MDD is to leverage the noise formulation of diffusion models by incorporating one noise level per domain, which allows missing domains to be modeled with noise in a natural way. This transforms the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#32039;&#26694;&#26550;&#26500;&#36896;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#35843;&#25972;&#26694;&#26550;&#30340;&#28040;&#22833;&#30697;&#21644;&#20854;&#20182;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#36335;&#24452;&#25903;&#25345;&#30340;&#22270;&#20449;&#21495;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#22312;&#38750;&#32447;&#24615;&#36924;&#36817;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.03537</link><description>&lt;p&gt;
&#22522;&#20110;&#23376;&#22270;&#30340;&#32039;&#26694;&#26550;&#22312;&#20855;&#26377;&#32039;&#33268;&#25903;&#25345;&#21644;&#28176;&#28040;&#30952;&#30340;&#22270;&#19978;
&lt;/p&gt;
&lt;p&gt;
Subgraph-based Tight Frames on Graphs with Compact Supports and Vanishing Moments. (arXiv:2309.03537v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#32039;&#26694;&#26550;&#26500;&#36896;&#26041;&#27861;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#35843;&#25972;&#26694;&#26550;&#30340;&#28040;&#22833;&#30697;&#21644;&#20854;&#20182;&#23646;&#24615;&#65292;&#23454;&#29616;&#23545;&#20855;&#26377;&#36335;&#24452;&#25903;&#25345;&#30340;&#22270;&#20449;&#21495;&#30340;&#39640;&#25928;&#34920;&#31034;&#65292;&#22312;&#38750;&#32447;&#24615;&#36924;&#36817;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#19968;&#31995;&#21015;&#20998;&#23618;&#20998;&#21306;&#26500;&#24314;&#20855;&#26377;&#32039;&#33268;&#25903;&#25345;&#30340;&#22270;&#19978;&#30340;&#32039;&#26694;&#26550;&#12290;&#20174;&#25105;&#20204;&#30340;&#25277;&#35937;&#26500;&#36896;&#24320;&#22987;&#65292;&#25105;&#20204;&#33021;&#22815;&#28789;&#27963;&#22320;&#23558;&#23376;&#22270;Laplacians&#32435;&#20837;&#21040;&#25105;&#20204;&#30340;&#22270;&#26694;&#26550;&#35774;&#35745;&#20013;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#36890;&#29992;&#26041;&#27861;&#20801;&#35768;&#35843;&#25972;&#26694;&#26550;&#30340;&#65288;&#23376;&#22270;&#65289;&#28040;&#22833;&#30697;&#21644;&#20854;&#20182;&#23646;&#24615;&#65292;&#22914;&#26041;&#21521;&#24615;&#65292;&#20197;&#26377;&#25928;&#22320;&#34920;&#31034;&#20855;&#26377;&#36335;&#24452;&#25903;&#25345;&#30340;&#22270;&#20449;&#21495;&#12290;&#25105;&#20204;&#26126;&#30830;&#23450;&#20041;&#24182;&#27979;&#35797;&#20102;&#20960;&#20010;&#21464;&#20307;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#22270;&#26694;&#26550;&#22312;&#38750;&#32447;&#24615;&#36924;&#36817;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we proposed a novel and general method to construct tight frames on graphs with compact supports based on a series of hierarchical partitions. Starting from our abstract construction that generalizes previous methods based on partition trees, we are able to flexibly incorporate subgraph Laplacians into our design of graph frames. Consequently, our general methods permit adjusting the (subgraph) vanishing moments of the framelets and extra properties, such as directionality, for efficiently representing graph signals with path-like supports. Several variants are explicitly defined and tested. Experimental results show our proposed graph frames perform superiorly in non-linear approximation tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#20197;&#32553;&#23567;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#8221;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.16589</link><description>&lt;p&gt;
&#20855;&#26377;&#29983;&#25104;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#20998;&#24067;&#40065;&#26834;&#24615;&#30340;&#21487;&#30097;&#20215;&#26684;
&lt;/p&gt;
&lt;p&gt;
The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model. (arXiv:2305.16589v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#20197;&#32553;&#23567;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#8221;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#65292;&#20197;&#20943;&#23569;&#22312;&#23454;&#36341;&#20013;&#30340;&#27169;&#25311;&#19982;&#30495;&#23454;&#24046;&#36317;&#12290;&#25105;&#20204;&#37319;&#29992;&#20998;&#24067;&#40065;&#26834;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;RMDPs&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#31574;&#30053;&#65292;&#22312;&#37096;&#32626;&#29615;&#22659;&#33853;&#22312;&#39044;&#23450;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#20869;&#26102;&#65292;&#20248;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#20102;&#19968;&#20123;&#21162;&#21147;&#65292;&#20294;RMDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#27809;&#26377;&#24471;&#21040;&#35299;&#20915;&#65292;&#26080;&#35770;&#20351;&#29992;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26159;&#20160;&#20040;&#12290;&#19981;&#28165;&#26970;&#20998;&#24067;&#40065;&#26834;&#24615;&#19982;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#26159;&#21542;&#20855;&#26377;&#32479;&#35745;&#23398;&#19978;&#30340;&#24433;&#21709;&#12290;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#26681;&#25454;&#21517;&#20041;MDP&#32472;&#21046;&#26679;&#26412;&#65292;&#25105;&#20204;&#23558;&#25551;&#36848;RMDPs&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24403;&#30001;&#24635;&#21464;&#24046;&#65288;TV&#65289;&#36317;&#31163;&#25110;$\chi^2$&#20998;&#27495;&#25351;&#23450;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26102;&#12290;&#22312;&#36825;&#37324;&#30740;&#31350;&#30340;&#31639;&#27861;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#40065;&#26834;&#20540;&#36845;&#20195;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25972;&#20010;&#33539;&#22260;&#20869;&#37117;&#26159;&#36817;&#20046;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates model robustness in reinforcement learning (RL) to reduce the sim-to-real gap in practice. We adopt the framework of distributionally robust Markov decision processes (RMDPs), aimed at learning a policy that optimizes the worst-case performance when the deployed environment falls within a prescribed uncertainty set around the nominal MDP. Despite recent efforts, the sample complexity of RMDPs remained mostly unsettled regardless of the uncertainty set in use. It was unclear if distributional robustness bears any statistical consequences when benchmarked against standard RL.  Assuming access to a generative model that draws samples based on the nominal MDP, we characterize the sample complexity of RMDPs when the uncertainty set is specified via either the total variation (TV) distance or $\chi^2$ divergence. The algorithm studied here is a model-based method called {\em distributionally robust value iteration}, which is shown to be near-optimal for the full range
&lt;/p&gt;</description></item></channel></rss>