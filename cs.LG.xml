<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#22521;&#20859;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#39046;&#22495;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;</title><link>http://arxiv.org/abs/2312.17300</link><description>&lt;p&gt;
&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36890;&#36807;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#23398;&#20064;&#25913;&#21892;&#20837;&#20405;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Intrusion Detection with Domain-Invariant Representation Learning in Latent Space. (arXiv:2312.17300v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#22521;&#20859;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#29305;&#24449;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#29305;&#24449;&#65292;&#20197;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#39046;&#22495;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#32858;&#28966;&#20110;&#21033;&#29992;&#26469;&#33258;&#20855;&#26377;&#20016;&#23500;&#35757;&#32451;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#22810;&#20010;&#30456;&#20851;&#39046;&#22495;&#30340;&#30693;&#35782;&#65292;&#22686;&#24378;&#23545;&#26410;&#30693;&#20998;&#24067;&#65288;IN&#65289;&#21644;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#39046;&#22495;&#30340;&#25512;&#29702;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20004;&#38454;&#27573;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#65292;&#20351;&#29992;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36825;&#31181;&#26041;&#27861;&#26088;&#22312;&#20174;&#36328;&#36234;&#22810;&#20010;&#39046;&#22495;&#30340;&#29305;&#24449;&#20013;&#22521;&#20859;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#65292;&#21253;&#25324;&#26412;&#22320;&#21644;&#36328;&#39046;&#22495;&#65292;&#20197;&#22686;&#24378;&#23545;IN&#21644;OOD&#39046;&#22495;&#30340;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#26368;&#23567;&#21270;&#20808;&#39564;&#19982;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#20998;&#31163;&#28508;&#22312;&#31354;&#38388;&#65292;&#26377;&#25928;&#28040;&#38500;&#34394;&#20551;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#32508;&#21512;&#32780;&#35328;&#65292;&#32852;&#21512;&#20248;&#21270;&#23558;&#20419;&#36827;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#20998;&#31867;&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#22312;&#22810;&#20010;&#32593;&#32476;&#23433;&#20840;&#25968;&#25454;&#38598;&#19978;&#30340;&#25928;&#33021;&#65292;&#23545;&#27604;&#20102;&#29616;&#20195;&#39046;&#22495;&#27867;&#21270;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization focuses on leveraging knowledge from multiple related domains with ample training data and labels to enhance inference on unseen in-distribution (IN) and out-of-distribution (OOD) domains. In our study, we introduce a two-phase representation learning technique using multi-task learning. This approach aims to cultivate a latent space from features spanning multiple domains, encompassing both native and cross-domains, to amplify generalization to IN and OOD territories. Additionally, we attempt to disentangle the latent space by minimizing the mutual information between the prior and latent space, effectively de-correlating spurious feature correlations. Collectively, the joint optimization will facilitate domain-invariant feature learning. We assess the model's efficacy across multiple cybersecurity datasets, using standard classification metrics on both unseen IN and OOD sets, and juxtapose the results with contemporary domain generalization methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#39640;&#32500;&#20960;&#20309;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24378;&#21046;&#26045;&#21152;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#25551;&#36848;&#20102;&#28145;&#24230;&#32593;&#32476;&#21098;&#26525;&#27604;&#29575;&#30340;&#30456;&#21464;&#28857;&#65292;&#35813;&#28857;&#31561;&#20110;&#26576;&#20123;&#20984;&#20307;&#30340;&#24179;&#26041;&#39640;&#26031;&#23485;&#24230;&#38500;&#20197;&#21442;&#25968;&#30340;&#21407;&#22987;&#32500;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.05857</link><description>&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#21487;&#20197;&#34987;&#21098;&#26525;&#21040;&#22810;&#20040;&#31232;&#30095;&#65306;&#20960;&#20309;&#35270;&#35282;&#19979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Sparse Can We Prune A Deep Network: A Geometric Viewpoint. (arXiv:2306.05857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#39640;&#32500;&#20960;&#20309;&#30340;&#35282;&#24230;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24378;&#21046;&#26045;&#21152;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#25551;&#36848;&#20102;&#28145;&#24230;&#32593;&#32476;&#21098;&#26525;&#27604;&#29575;&#30340;&#30456;&#21464;&#28857;&#65292;&#35813;&#28857;&#31561;&#20110;&#26576;&#20123;&#20984;&#20307;&#30340;&#24179;&#26041;&#39640;&#26031;&#23485;&#24230;&#38500;&#20197;&#21442;&#25968;&#30340;&#21407;&#22987;&#32500;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#21442;&#25968;&#21270;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#20043;&#19968;&#12290;&#34429;&#28982;&#23427;&#21487;&#20197;&#25552;&#20379;&#20986;&#33394;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#20294;&#21516;&#26102;&#20063;&#24378;&#21152;&#20102;&#37325;&#22823;&#30340;&#23384;&#20648;&#36127;&#25285;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#30740;&#31350;&#32593;&#32476;&#21098;&#26525;&#12290;&#19968;&#20010;&#33258;&#28982;&#32780;&#22522;&#26412;&#30340;&#38382;&#39064;&#26159;&#65306;&#25105;&#20204;&#33021;&#21098;&#26525;&#19968;&#20010;&#28145;&#24230;&#32593;&#32476;&#21040;&#22810;&#20040;&#31232;&#30095;&#65288;&#20960;&#20046;&#19981;&#24433;&#21709;&#24615;&#33021;&#65289;&#65311;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;&#31532;&#19968;&#21407;&#29702;&#26041;&#27861;&#65292;&#20855;&#20307;&#22320;&#65292;&#21482;&#36890;&#36807;&#22312;&#21407;&#22987;&#25439;&#22833;&#20989;&#25968;&#20013;&#24378;&#21046;&#26045;&#21152;&#31232;&#30095;&#24615;&#32422;&#26463;&#65292;&#25105;&#20204;&#33021;&#22815;&#20174;&#39640;&#32500;&#20960;&#20309;&#30340;&#35282;&#24230;&#25551;&#36848;&#21098;&#26525;&#27604;&#29575;&#30340;&#23574;&#38160;&#30456;&#21464;&#28857;&#65292;&#35813;&#28857;&#23545;&#24212;&#20110;&#21487;&#34892;&#21644;&#19981;&#21487;&#34892;&#20043;&#38388;&#30340;&#36793;&#30028;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21098;&#26525;&#27604;&#29575;&#30340;&#30456;&#21464;&#28857;&#31561;&#20110;&#26576;&#20123;&#20984;&#20307;&#30340;&#24179;&#26041;&#39640;&#26031;&#23485;&#24230;&#65292;&#36825;&#20123;&#20984;&#20307;&#26159;&#30001;$l_1$-&#35268;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#24471;&#20986;&#30340;&#65292;&#38500;&#20197;&#21442;&#25968;&#30340;&#21407;&#22987;&#32500;&#24230;&#12290;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21098;&#26525;&#36807;&#31243;&#20013;&#21442;&#25968;&#30340;&#20998;&#24067;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Overparameterization constitutes one of the most significant hallmarks of deep neural networks. Though it can offer the advantage of outstanding generalization performance, it meanwhile imposes substantial storage burden, thus necessitating the study of network pruning. A natural and fundamental question is: How sparse can we prune a deep network (with almost no hurt on the performance)? To address this problem, in this work we take a first principles approach, specifically, by merely enforcing the sparsity constraint on the original loss function, we're able to characterize the sharp phase transition point of pruning ratio, which corresponds to the boundary between the feasible and the infeasible, from the perspective of high-dimensional geometry. It turns out that the phase transition point of pruning ratio equals the squared Gaussian width of some convex body resulting from the $l_1$-regularized loss function, normalized by the original dimension of parameters. As a byproduct, we pr
&lt;/p&gt;</description></item></channel></rss>