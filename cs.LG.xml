<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06925</link><description>&lt;p&gt;
Transformers&#23398;&#20064;&#20302;&#25935;&#24863;&#24615;&#20989;&#25968;&#30340;&#31616;&#21333;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Simplicity Bias of Transformers to Learn Low Sensitivity Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06925
&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#20855;&#26377;&#20302;&#25935;&#24863;&#24615;&#65292;&#36825;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#26377;&#21161;&#20110;&#35299;&#37322;&#20854;&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformers&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23545;&#23427;&#20204;&#20855;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#20197;&#21450;&#36825;&#20123;&#20559;&#24046;&#22914;&#20309;&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19981;&#21516;&#30340;&#29702;&#35299;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#27169;&#22411;&#23545;&#36755;&#20837;&#20013;&#30340;&#38543;&#26426;&#26356;&#25913;&#30340;&#25935;&#24863;&#24615;&#27010;&#24565;&#21270;&#20026;&#19968;&#31181;&#31616;&#21333;&#24615;&#20559;&#24046;&#30340;&#27010;&#24565;&#65292;&#36825;&#20026;&#35299;&#37322;transformers&#22312;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#19978;&#30340;&#31616;&#21333;&#24615;&#21644;&#35889;&#20559;&#24046;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;transformers&#22312;&#35270;&#35273;&#21644;&#35821;&#35328;&#20219;&#21153;&#20013;&#27604;&#20854;&#20182;&#26367;&#20195;&#26550;&#26500;&#65288;&#22914;LSTMs&#12289;MLPs&#21644;CNNs&#65289;&#20855;&#26377;&#26356;&#20302;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20302;&#25935;&#24863;&#24615;&#20559;&#24046;&#19982;&#25913;&#36827;&#24615;&#33021;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06925v1 Announce Type: cross  Abstract: Transformers achieve state-of-the-art accuracy and robustness across many tasks, but an understanding of the inductive biases that they have and how those biases are different from other neural network architectures remains elusive. Various neural network architectures such as fully connected networks have been found to have a simplicity bias towards simple functions of the data; one version of this simplicity bias is a spectral bias to learn simple functions in the Fourier space. In this work, we identify the notion of sensitivity of the model to random changes in the input as a notion of simplicity bias which provides a unified metric to explain the simplicity and spectral bias of transformers across different data modalities. We show that transformers have lower sensitivity than alternative architectures, such as LSTMs, MLPs and CNNs, across both vision and language tasks. We also show that low-sensitivity bias correlates with impro
&lt;/p&gt;</description></item><item><title>&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17767</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;
&lt;/p&gt;
&lt;p&gt;
Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17767
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#65288;Stretch RE2&#65289;&#33021;&#22815;&#22312;&#22810;&#26679;&#30340;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25289;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#12290;&#25105;&#20204;&#22312;31&#20010;&#19981;&#21516;&#30340;&#29289;&#20307;&#21644;13&#20010;&#19981;&#21516;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;4&#22825;&#30340;&#23454;&#38469;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#38646;&#20987;&#25171;&#19979;&#65292;&#23545;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#26032;&#39062;&#30340;&#27249;&#26588;&#21644;&#25277;&#23625;&#30340;&#25171;&#24320;&#29575;&#36798;&#21040;61%&#12290;&#23545;&#22833;&#36133;&#27169;&#24335;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#25105;&#20204;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17767v1 Announce Type: cross  Abstract: Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and bui
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;</title><link>https://arxiv.org/abs/2402.06855</link><description>&lt;p&gt;
&#26356;&#22909;&#36824;&#26159;&#26356;&#24046;&#65311;&#36890;&#36807;&#26631;&#31614;&#22686;&#24378;&#23398;&#20064;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
For Better or For Worse? Learning Minimum Variance Features With Label Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06855
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#31614;&#22686;&#24378;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#25968;&#25454;&#22686;&#24378;&#23545;&#20110;&#25104;&#21151;&#22320;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#21457;&#25381;&#20102;&#20851;&#38190;&#20316;&#29992;&#12290;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#31867;-&#21253;&#25324;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup-&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#20462;&#25913;&#36755;&#20837;&#25968;&#25454;&#21644;&#36755;&#20837;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27492;&#31867;&#26041;&#27861;&#20013;&#26631;&#31614;&#22686;&#24378;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#19978;&#20351;&#29992;&#26631;&#31614;&#22686;&#24378;&#35757;&#32451;&#30340;&#32447;&#24615;&#27169;&#22411;&#21482;&#33021;&#23398;&#20064;&#21040;&#26368;&#23567;&#26041;&#24046;&#29305;&#24449;&#65292;&#32780;&#26631;&#20934;&#35757;&#32451;&#65288;&#21253;&#25324;&#26435;&#37325;&#34928;&#20943;&#65289;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#26041;&#24046;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#19968;&#20010;&#37325;&#35201;&#21518;&#26524;&#26159;&#28040;&#26497;&#30340;&#65306;&#19982;&#26631;&#20934;&#35757;&#32451;&#30456;&#27604;&#65292;&#26631;&#31614;&#24179;&#28369;&#21644;Mixup&#23545;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#23545;&#25239;&#25200;&#21160;&#21487;&#33021;&#19981;&#22826;&#40065;&#26834;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#22270;&#20687;&#20998;&#31867;&#22522;&#20934;&#30340;&#19968;&#31995;&#21015;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#19982;&#23454;&#36341;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation has been pivotal in successfully training deep learning models on classification tasks over the past decade. An important subclass of data augmentation techniques - which includes both label smoothing and Mixup - involves modifying not only the input data but also the input label during model training. In this work, we analyze the role played by the label augmentation aspect of such methods. We prove that linear models on linearly separable data trained with label augmentation learn only the minimum variance features in the data, while standard training (which includes weight decay) can learn higher variance features. An important consequence of our results is negative: label smoothing and Mixup can be less robust to adversarial perturbations of the training data when compared to standard training. We verify that our theory reflects practice via a range of experiments on synthetic data and image classification benchmarks.
&lt;/p&gt;</description></item><item><title>Sym-Q&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#21644;&#22870;&#21169;&#20449;&#21495;&#65292;Sym-Q&#33021;&#22815;&#26681;&#25454;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#25913;&#36827;&#34920;&#36798;&#24335;&#12290;</title><link>https://arxiv.org/abs/2402.05306</link><description>&lt;p&gt;
Sym-Q&#65306;&#36890;&#36807;&#39034;&#24207;&#20915;&#31574;&#36827;&#34892;&#33258;&#36866;&#24212;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Sym-Q: Adaptive Symbolic Regression via Sequential Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05306
&lt;/p&gt;
&lt;p&gt;
Sym-Q&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#26469;&#35299;&#20915;&#29616;&#26377;&#27169;&#22411;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#21644;&#22870;&#21169;&#20449;&#21495;&#65292;Sym-Q&#33021;&#22815;&#26681;&#25454;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#25913;&#36827;&#34920;&#36798;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#20855;&#26377;&#20174;&#23454;&#35777;&#25968;&#25454;&#20013;&#25581;&#31034;&#28508;&#22312;&#25968;&#23398;&#21644;&#29289;&#29702;&#20851;&#31995;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#36825;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#22312;&#27867;&#21270;&#24615;&#21644;&#36866;&#24212;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#36890;&#24120;&#65292;&#24403;&#36755;&#20986;&#34920;&#36798;&#24335;&#19981;&#36275;&#20197;&#36866;&#24212;&#23454;&#39564;&#25968;&#25454;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#26377;&#25928;&#30340;&#26426;&#21046;&#26469;&#36866;&#24212;&#25110;&#20462;&#25913;&#34920;&#36798;&#24335;&#12290;&#36825;&#31181;&#32570;&#20047;&#28789;&#27963;&#24615;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#21457;&#29616;&#26410;&#30693;&#30340;&#29289;&#29702;&#25110;&#29983;&#29289;&#20851;&#31995;&#26041;&#38754;&#12290;&#21463;&#21040;&#20154;&#31867;&#19987;&#23478;&#22914;&#20309;&#25913;&#36827;&#21644;&#35843;&#25972;&#34920;&#36798;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;Symbolic Q-network&#65288;Sym-Q&#65289;&#65292;&#23558;&#31526;&#21495;&#22238;&#24402;&#37325;&#26032;&#23450;&#20041;&#20026;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#12290;Sym-Q&#21033;&#29992;&#30417;&#30563;&#28436;&#31034;&#24182;&#26681;&#25454;&#22870;&#21169;&#20449;&#21495;&#26469;&#25913;&#36827;&#34920;&#36798;&#24335;&#65292;&#22870;&#21169;&#20449;&#21495;&#25351;&#31034;&#25311;&#21512;&#31934;&#24230;&#30340;&#36136;&#37327;&#12290;&#23427;&#29420;&#29305;&#30340;&#33021;&#21147;&#21487;&#20197;&#22788;&#29702;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression holds great potential for uncovering underlying mathematical and physical relationships from empirical data. While existing transformer-based models have recently achieved significant success in this domain, they face challenges in terms of generalizability and adaptability. Typically, in cases where the output expressions do not adequately fit experimental data, the models lack efficient mechanisms to adapt or modify the expression. This inflexibility hinders their application in real-world scenarios, particularly in discovering unknown physical or biological relationships. Inspired by how human experts refine and adapt expressions, we introduce Symbolic Q-network (Sym-Q), a novel reinforcement learning-based model that redefines symbolic regression as a sequential decision-making task. Sym-Q leverages supervised demonstrations and refines expressions based on reward signals indicating the quality of fitting precision. Its distinctive ability to manage the complexi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03664</link><description>&lt;p&gt;
&#39640;&#25928;&#27714;&#35299;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficient Solvers for Partial Gromov-Wasserstein
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24046;Gromov-Wasserstein&#65288;PGW&#65289;&#38382;&#39064;&#21487;&#20197;&#27604;&#36739;&#20855;&#26377;&#19981;&#22343;&#21248;&#36136;&#37327;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#21644;&#37096;&#20998;&#21305;&#37197;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;Gromov-Wasserstein&#38382;&#39064;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31867;&#20284;&#20110;&#25226;&#20559;&#24046;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#12290;&#36825;&#20010;&#36716;&#21270;&#23548;&#33268;&#20102;&#20004;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#65292;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#65292;&#25968;&#23398;&#21644;&#35745;&#31639;&#19978;&#31561;&#20215;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;PGW&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#22312;&#24418;&#29366;&#21305;&#37197;&#21644;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces. In this paper, we demonstrate that the PGW problem can be transformed into a variant of the Gromov-Wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem. This transformation leads to two new solvers, mathematically and computationally equivalent, based on the Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We further establish that the PGW problem constitutes a metric for metric measure spaces. Finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21452;&#20869;&#28857;&#20248;&#21270;&#23398;&#20064;&#21644;&#21452;&#36229;&#26799;&#24230;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#26377;&#30028;&#21464;&#37327;&#30340;&#21442;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#32422;&#26463;&#23545;&#24212;&#30340;&#23545;&#20598;&#21464;&#37327;&#65292;&#30830;&#20445;&#23545;&#20598;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#39640;&#20445;&#30495;&#24230;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#21644;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.02596</link><description>&lt;p&gt;
&#21452;&#20869;&#28857;&#20248;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual Interior-Point Optimization Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21452;&#20869;&#28857;&#20248;&#21270;&#23398;&#20064;&#21644;&#21452;&#36229;&#26799;&#24230;&#23398;&#20064;&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#26377;&#30028;&#21464;&#37327;&#30340;&#21442;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#39044;&#27979;&#32422;&#26463;&#23545;&#24212;&#30340;&#23545;&#20598;&#21464;&#37327;&#65292;&#30830;&#20445;&#23545;&#20598;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#39640;&#20445;&#30495;&#24230;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#21644;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21452;&#20869;&#28857;&#23398;&#20064;&#65288;DIPL&#65289;&#21644;&#21452;&#36229;&#26799;&#24230;&#23398;&#20064;&#65288;DSL&#65289;&#65292;&#20197;&#23398;&#20064;&#24102;&#26377;&#26377;&#30028;&#21464;&#37327;&#30340;&#21442;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#65292;&#36825;&#22312;&#35768;&#22810;&#34892;&#19994;&#20013;&#37117;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;DIPL&#27169;&#25311;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#20598;&#20869;&#28857;&#31639;&#27861;&#65292;&#32780;DSL&#21017;&#27169;&#25311;&#20102;&#32463;&#20856;&#30340;&#23545;&#20598;&#36229;&#26799;&#24230;&#19978;&#21319;&#31639;&#27861;&#12290;&#36890;&#36807;&#39044;&#27979;&#19982;&#32422;&#26463;&#20851;&#32852;&#30340;&#23545;&#20598;&#21464;&#37327;&#65292;DIPL&#21644;DSL&#20445;&#35777;&#23545;&#20598;&#21487;&#34892;&#24615;&#65292;&#28982;&#21518;&#21033;&#29992;&#23545;&#20110;&#32422;&#26463;&#30028;&#38480;&#30340;&#23545;&#20598;&#30340;&#28789;&#27963;&#24615;&#12290;DIPL&#21644;DSL&#36890;&#36807;&#25552;&#20379;&#36136;&#37327;&#35777;&#26126;&#26469;&#34917;&#20805;&#29616;&#26377;&#30340;&#21407;&#22987;&#23398;&#20064;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#23427;&#20204;&#33021;&#22815;&#20026;&#22823;&#35268;&#27169;&#26368;&#20248;&#21151;&#29575;&#27969;&#38382;&#39064;&#20135;&#29983;&#39640;&#20445;&#30495;&#24230;&#30340;&#23545;&#20598;&#21487;&#34892;&#35299;&#65292;&#24182;&#22312;0.5%&#30340;&#20248;&#21270;&#24046;&#36317;&#19979;&#25552;&#20379;&#26377;&#25928;&#30340;&#23545;&#20598;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces Dual Interior Point Learning (DIPL) and Dual Supergradient Learning (DSL) to learn dual feasible solutions to parametric linear programs with bounded variables, which are pervasive across many industries. DIPL mimics a novel dual interior point algorithm while DSL mimics classical dual supergradient ascent. DIPL and DSL ensure dual feasibility by predicting dual variables associated with the constraints then exploiting the flexibility of the duals of the bound constraints. DIPL and DSL complement existing primal learning methods by providing a certificate of quality. They are shown to produce high-fidelity dual-feasible solutions to large-scale optimal power flow problems providing valid dual bounds under 0.5% optimality gap.
&lt;/p&gt;</description></item><item><title>&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02364</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#21457;&#23637;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
The Developmental Landscape of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02364
&lt;/p&gt;
&lt;p&gt;
&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;transformers&#20013;&#65292;&#24403;&#23427;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#25110;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22914;&#20309;&#20197;&#31163;&#25955;&#30340;&#21457;&#23637;&#38454;&#27573;&#20986;&#29616;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#38548;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#65292;&#36890;&#36807;&#25506;&#27979;&#21442;&#25968;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#31181;&#32676;&#25439;&#22833;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#30740;&#31350;&#36825;&#20123;&#26032;&#26041;&#27861;&#25581;&#31034;&#30340;&#38454;&#27573;&#65292;&#20197;&#24314;&#31435;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#26041;&#27861;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;</title><link>https://arxiv.org/abs/2402.00626</link><description>&lt;p&gt;
Vision-LLMs&#36890;&#36807;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#21487;&#20197;&#33258;&#27450;&#27450;&#20154;
&lt;/p&gt;
&lt;p&gt;
Vision-LLMs Can Fool Themselves with Self-Generated Typographic Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00626
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#23545;&#20110;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#26356;&#26377;&#25928;&#30340;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#26041;&#27861;&#65292;&#20026;&#27492;&#35774;&#35745;&#20102;&#19968;&#20010;&#29420;&#29305;&#30340;&#27979;&#35797;&#22522;&#20934;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#22522;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65307;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20840;&#26032;&#31867;&#21035;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;LVLM&#23545;&#20110;&#28041;&#21450;&#23558;&#35823;&#23548;&#24615;&#25991;&#26412;&#21472;&#21152;&#21040;&#22270;&#20687;&#19978;&#30340;&#20174;&#25490;&#29256;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#25915;&#20987;&#24615;&#21364;&#27809;&#26377;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#25490;&#29256;&#25915;&#20987;&#20381;&#36182;&#20110;&#20174;&#39044;&#23450;&#20041;&#31867;&#21035;&#38598;&#21512;&#20013;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#35823;&#23548;&#24615;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#36873;&#25321;&#30340;&#31867;&#21035;&#21487;&#33021;&#19981;&#26159;&#26368;&#26377;&#25928;&#30340;&#25915;&#20987;&#31867;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#29420;&#29305;&#35774;&#35745;&#30340;&#26032;&#39062;&#22522;&#20934;&#26469;&#27979;&#35797;LVLM&#23545;&#25490;&#29256;&#25915;&#20987;&#30340;&#23481;&#26131;&#21463;&#25915;&#20987;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#32780;&#26356;&#26377;&#25928;&#30340;&#25490;&#29256;&#25915;&#20987;&#65306;&#33258;&#21160;&#29983;&#25104;&#30340;&#25490;&#29256;&#25915;&#20987;&#12290;&#23454;&#38469;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#31616;&#21333;&#22320;&#25552;&#31034;GPT-4V&#31561;&#27169;&#22411;&#21033;&#29992;&#20854;&#24378;&#22823;&#30340;&#35821;&#35328;&#33021;&#21147;&#25512;&#33616;&#19968;&#31181;&#25490;&#29256;&#25915;&#20987;&#26469;&#20026;&#32473;&#23450;&#30340;&#22270;&#20687;&#29983;&#25104;&#25915;&#20987;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26032;&#39062;&#22522;&#20934;&#65292;&#25105;&#20204;&#21457;&#29616;&#25490;&#29256;&#25915;&#20987;&#23545;LVLM&#26500;&#25104;&#20102;&#37325;&#22823;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, significant progress has been made on Large Vision-Language Models (LVLMs); a new class of VL models that make use of large pre-trained language models. Yet, their vulnerability to Typographic attacks, which involve superimposing misleading text onto an image remain unstudied. Furthermore, prior work typographic attacks rely on sampling a random misleading class from a predefined set of classes. However, the random chosen class might not be the most effective attack. To address these issues, we first introduce a novel benchmark uniquely designed to test LVLMs vulnerability to typographic attacks. Furthermore, we introduce a new and more effective typographic attack: Self-Generated typographic attacks. Indeed, our method, given an image, make use of the strong language capabilities of models like GPT-4V by simply prompting them to recommend a typographic attack. Using our novel benchmark, we uncover that typographic attacks represent a significant threat against LVLM(s). Furth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#22312;&#23398;&#20064;&#31639;&#27861;&#20013;&#32771;&#34385;&#20102;&#26080;&#32447;&#36890;&#36947;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#20998;&#26512;&#38590;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17460</link><description>&lt;p&gt;
&#20351;&#26080;&#32447;&#29615;&#22659;&#23545;&#26799;&#24230;&#20272;&#35745;&#22120;&#26377;&#29992;&#65306;&#19968;&#31181;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Rendering Wireless Environments Useful for Gradient Estimators: A Zero-Order Stochastic Federated Learning Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#38543;&#26426;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#22312;&#23398;&#20064;&#31639;&#27861;&#20013;&#32771;&#34385;&#20102;&#26080;&#32447;&#36890;&#36947;&#65292;&#36991;&#20813;&#20102;&#36164;&#28304;&#30340;&#28010;&#36153;&#21644;&#20998;&#26512;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20801;&#35768;&#22810;&#20010;&#36793;&#32536;&#35774;&#22791;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20844;&#24320;&#21407;&#22987;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#24403;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#36890;&#36807;&#26080;&#32447;&#20449;&#36947;&#36890;&#20449;&#26102;&#65292;&#35813;&#26041;&#27861;&#38754;&#20020;&#30528;&#36890;&#20449;&#21644;&#35745;&#31639;&#29942;&#39048;&#12290;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#36890;&#20449;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#38454;&#65288;ZO&#65289;&#26041;&#27861;&#65292;&#37319;&#29992;&#19968;&#28857;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#21033;&#29992;&#26080;&#32447;&#36890;&#20449;&#36890;&#36947;&#30340;&#29305;&#24615;&#65292;&#32780;&#26080;&#38656;&#30693;&#36947;&#36890;&#36947;&#29366;&#24577;&#31995;&#25968;&#12290;&#36825;&#26159;&#31532;&#19968;&#31181;&#23558;&#26080;&#32447;&#36890;&#36947;&#21253;&#21547;&#22312;&#23398;&#20064;&#31639;&#27861;&#26412;&#36523;&#20013;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#28010;&#36153;&#36164;&#28304;&#26469;&#20998;&#26512;&#21644;&#28040;&#38500;&#20854;&#24433;&#21709;&#12290;&#36825;&#39033;&#24037;&#20316;&#30340;&#20004;&#20010;&#20027;&#35201;&#22256;&#38590;&#26159;&#65292;&#22312;FL&#20013;&#65292;&#30446;&#26631;&#20989;&#25968;&#36890;&#24120;&#19981;&#26159;&#20984;&#30340;&#65292;&#36825;&#20351;&#24471;&#23558;FL&#25193;&#23637;&#21040;ZO&#26041;&#27861;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#20197;&#21450;&#21253;&#25324;&#24433;&#21709;&#30340;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is a novel approach to machine learning that allows multiple edge devices to collaboratively train a model without disclosing their raw data. However, several challenges hinder the practical implementation of this approach, especially when devices and the server communicate over wireless channels, as it suffers from communication and computation bottlenecks in this case. By utilizing a communication-efficient framework, we propose a novel zero-order (ZO) method with a one-point gradient estimator that harnesses the nature of the wireless communication channel without requiring the knowledge of the channel state coefficient. It is the first method that includes the wireless channel in the learning algorithm itself instead of wasting resources to analyze it and remove its impact. The two main difficulties of this work are that in FL, the objective function is usually not convex, which makes the extension of FL to ZO methods challenging, and that including the impa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;</title><link>https://arxiv.org/abs/2208.04284</link><description>&lt;p&gt;
&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#19968;&#33324;&#21270;&#30028;&#38480;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Rademacher Complexity-based Generalization Bounds for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.04284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#36827;&#34892;&#20998;&#31867;&#23569;&#37327;&#31867;&#21035;&#22270;&#20687;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#30340;&#21457;&#23637;&#23545;&#20110;&#39640;&#32500;&#26144;&#23556;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#26159;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Rademacher&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;CNNs&#30340;&#32593;&#32476;&#38271;&#24230;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35832;&#22914;ReLU&#65292;Leaky ReLU&#65292;Parametric Rectifier Linear Unit&#65292;Sigmoid&#21644;Tanh&#31561;&#29305;&#23450;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the Rademacher complexity-based approach can generate non-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for classifying a small number of classes of images. The development of new Talagrand's contraction lemmas for high-dimensional mappings between function spaces and CNNs for general Lipschitz activation functions is a key technical contribution. Our results show that the Rademacher complexity does not depend on the network length for CNNs with some special types of activation functions such as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#32416;&#27491;&#20154;&#31867;&#38169;&#35823;&#26041;&#38754;&#36215;&#21040;&#20102;&#31215;&#26497;&#20316;&#29992;&#65292;&#20294;&#27492;&#20030;&#20063;&#28508;&#22312;&#23548;&#33268;&#24515;&#29702;&#25104;&#26412;&#65292;&#24182;&#24433;&#21709;&#20154;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#30740;&#31350;&#32593;&#29699;&#27604;&#36187;&#20013;&#30340;Hawk-Eye&#23457;&#26597;&#31995;&#32479;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#20837;AI&#30417;&#30563;&#21518;&#65292;&#35009;&#21028;&#21592;&#30340;&#38169;&#35823;&#29575;&#19979;&#38477;&#65292;&#24515;&#29702;&#25104;&#26412;&#23548;&#33268;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#23558;&#29699;&#21028;&#20026;&#36827;&#30028;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#31867;&#22411;&#38169;&#21028;&#30340;&#36716;&#21464;&#12290;</title><link>http://arxiv.org/abs/2401.16754</link><description>&lt;p&gt;
AI&#30417;&#30563;&#21644;&#20154;&#31867;&#38169;&#35823;&#65306;&#26469;&#33258;&#20013;&#24515;&#27861;&#24237;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
AI Oversight and Human Mistakes: Evidence from Centre Court. (arXiv:2401.16754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16754
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#32416;&#27491;&#20154;&#31867;&#38169;&#35823;&#26041;&#38754;&#36215;&#21040;&#20102;&#31215;&#26497;&#20316;&#29992;&#65292;&#20294;&#27492;&#20030;&#20063;&#28508;&#22312;&#23548;&#33268;&#24515;&#29702;&#25104;&#26412;&#65292;&#24182;&#24433;&#21709;&#20154;&#30340;&#20915;&#31574;&#12290;&#36890;&#36807;&#30740;&#31350;&#32593;&#29699;&#27604;&#36187;&#20013;&#30340;Hawk-Eye&#23457;&#26597;&#31995;&#32479;&#65292;&#25105;&#20204;&#21457;&#29616;&#24341;&#20837;AI&#30417;&#30563;&#21518;&#65292;&#35009;&#21028;&#21592;&#30340;&#38169;&#35823;&#29575;&#19979;&#38477;&#65292;&#24515;&#29702;&#25104;&#26412;&#23548;&#33268;&#20182;&#20204;&#26356;&#20542;&#21521;&#20110;&#23558;&#29699;&#21028;&#20026;&#36827;&#30028;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#31867;&#22411;&#38169;&#21028;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#19981;&#26029;&#25552;&#21319;&#30340;&#39537;&#21160;&#19979;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31995;&#32479;&#24050;&#32463;&#24320;&#22987;&#22312;&#35768;&#22810;&#22330;&#21512;&#29992;&#20110;&#32416;&#27491;&#20154;&#31867;&#38169;&#35823;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#39318;&#20010;&#23454;&#22320;&#35777;&#25454;&#65292;&#35777;&#26126;&#36825;&#31181;AI&#30417;&#30563;&#20250;&#20135;&#29983;&#24515;&#29702;&#25104;&#26412;&#65292;&#24433;&#21709;&#20154;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;AI&#30417;&#30563;&#21457;&#29983;&#30340;&#26368;&#39640;&#21487;&#35265;&#24615;&#22330;&#26223;&#20043;&#19968;&#65306;&#39030;&#32423;&#32593;&#29699;&#27604;&#36187;&#20013;&#35009;&#21028;&#30340;Hawk-Eye&#23457;&#26597;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24341;&#20837;Hawk-Eye&#23457;&#26597;&#21518;&#65292;&#35009;&#21028;&#30340;&#25972;&#20307;&#38169;&#35823;&#29575;&#38477;&#20302;&#65292;&#31526;&#21512;&#24515;&#29702;&#25104;&#26412;&#34987;AI&#21542;&#23450;&#30340;&#21512;&#29702;&#24573;&#35270;&#29616;&#35937;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#35009;&#21028;&#22686;&#21152;&#20102;&#23545;&#29699;&#20837;&#20869;&#30340;&#21028;&#23450;&#29575;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#20174;II&#31867;&#38169;&#35823;&#65288;&#23558;&#29699;&#21028;&#20026;&#20986;&#30028;&#65292;&#23454;&#38469;&#19978;&#26159;&#36827;&#30028;&#65289;&#21040;I&#31867;&#38169;&#35823;&#65288;&#23558;&#29699;&#21028;&#20026;&#36827;&#30028;&#65292;&#23454;&#38469;&#19978;&#26159;&#20986;&#30028;&#65289;&#30340;&#36716;&#21464;&#12290;&#36890;&#36807;&#23545;&#29702;&#24615;&#19981;&#27880;&#24847;&#30340;&#35009;&#21028;&#27169;&#22411;&#36827;&#34892;&#24515;&#29702;&#25104;&#26412;&#30340;&#32467;&#26500;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;AI&#21542;&#23450;&#30340;&#24515;&#29702;&#25104;&#26412;&#65292;&#35009;&#21028;&#21592;&#38477;&#20302;&#20102;&#38169;&#35823;&#21028;&#23450;&#30340;&#39118;&#38505;&#24182;&#25552;&#39640;&#20102;&#29699;&#20837;&#20869;&#30340;&#21028;&#23450;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have begun to be used to overrule human mistakes in many settings. We provide the first field evidence this AI oversight carries psychological costs that can impact human decision-making. We investigate one of the highest visibility settings in which AI oversight has occurred: the Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs of being overruled by AI. We also find that umpires increased the rate at which they called balls in, which produced a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires, and our results suggest that because 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.13835</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20013;&#27169;&#22411;&#21644;&#20154;&#31867;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#26657;&#20934;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
The Calibration Gap between Model and Human Confidence in Large Language Models. (arXiv:2401.13835v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13835
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20256;&#36798;&#32622;&#20449;&#24230;&#26041;&#38754;&#27169;&#22411;&#21644;&#20154;&#31867;&#20043;&#38388;&#23384;&#22312;&#30340;&#24046;&#36317;&#65292;&#24182;&#21457;&#29616;&#40664;&#35748;&#35299;&#37322;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#33021;&#22815;&#33719;&#24471;&#20154;&#31867;&#30340;&#20449;&#20219;&#65292;&#23427;&#20204;&#38656;&#35201;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#23454;&#29616;&#33391;&#22909;&#30340;&#26657;&#20934;&#65292;&#21363;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#21644;&#20256;&#36798;&#23427;&#20204;&#30340;&#39044;&#27979;&#27491;&#30830;&#30340;&#21487;&#33021;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20851;&#27880;&#20102;LLM&#20869;&#37096;&#32622;&#20449;&#24230;&#35780;&#20272;&#30340;&#36136;&#37327;&#65292;&#20294;&#38382;&#39064;&#20173;&#28982;&#26159;LLM&#33021;&#22815;&#22914;&#20309;&#23558;&#36825;&#31181;&#20869;&#37096;&#27169;&#22411;&#32622;&#20449;&#24230;&#20256;&#36798;&#32473;&#20154;&#31867;&#29992;&#25143;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#31867;&#23545;LLM&#21709;&#24212;&#30340;&#22806;&#37096;&#32622;&#20449;&#24230;&#19982;&#27169;&#22411;&#20869;&#37096;&#32622;&#20449;&#24230;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#28041;&#21450;&#22810;&#39033;&#36873;&#25321;&#39064;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#26816;&#26597;&#20102;&#20154;&#31867;&#29992;&#25143;&#35782;&#21035;LLM&#36755;&#20986;&#21487;&#20449;&#24230;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#20998;&#20026;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#35780;&#20272;&#29992;&#25143;&#23545;&#30495;&#23454;LLM&#32622;&#20449;&#24230;&#30340;&#24863;&#30693;&#21644;&#65288;2&#65289;&#35843;&#26597;&#20010;&#24615;&#21270;&#35299;&#37322;&#23545;&#35813;&#24863;&#30693;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#30340;&#40664;&#35748;&#35299;&#37322;&#24448;&#24448;&#20250;&#23548;&#33268;&#29992;&#25143;&#36807;&#39640;&#20272;&#35745;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#20462;&#25913;&#35299;&#37322;&#30340;&#26041;&#24335;&#21487;&#20197;&#20943;&#23567;&#36825;&#31181;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the expl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#38598;&#25104;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22810;&#20010;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#65292;&#20197;&#36827;&#34892;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.03388</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#22810;&#26680;&#22270;&#23398;&#20064;&#30340;&#33258;&#38381;&#30151;&#39044;&#27979;&#19982;&#29983;&#29289;&#26631;&#24535;&#29289;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Multi-kernel Graph Learning for Autism Prediction and Biomarker Discovery. (arXiv:2303.03388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#22810;&#27169;&#24577;&#38598;&#25104;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22810;&#20010;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#65292;&#20197;&#36827;&#34892;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#38598;&#25104;&#21644;&#20998;&#31867;&#26159;&#30142;&#30149;&#39044;&#27979;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38556;&#30861;&#20043;&#19968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMKGL&#30340;&#26032;&#26041;&#27861;&#26469;&#26377;&#25928;&#25269;&#28040;&#22810;&#27169;&#24577;&#38598;&#25104;&#36807;&#31243;&#20013;&#21508;&#27169;&#24577;&#20043;&#38388;&#36127;&#38754;&#24433;&#21709;&#65292;&#24182;&#20174;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#22270;&#23884;&#20837;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#23398;&#20064;&#29983;&#25104;&#22810;&#20010;&#22270;&#65292;&#28982;&#21518;&#25552;&#20986;&#22810;&#26680;&#22270;&#23398;&#20064;&#27169;&#22359;&#65292;&#20174;&#22810;&#27169;&#24577;&#22270;&#20013;&#25552;&#21462;&#24322;&#36136;&#20449;&#24687;&#12290;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#32858;&#21512;&#22810;&#27169;&#24577;&#22270;&#20013;&#30340;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#23545;&#33258;&#38381;&#30151;&#30340;&#39044;&#27979;&#21644;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to its complexity, graph learning-based multi-modal integration and classification is one of the most challenging obstacles for disease prediction. To effectively offset the negative impact between modalities in the process of multi-modal integration and extract heterogeneous information from graphs, we propose a novel method called MMKGL (Multi-modal Multi-Kernel Graph Learning). For the problem of negative impact between modalities, we propose a multi-modal graph embedding module to construct a multi-modal graph. Different from conventional methods that manually construct static graphs for all modalities, each modality generates a separate graph by adaptive learning, where a function graph and a supervision graph are introduced for optimization during the multi-graph fusion embedding process. We then propose a multi-kernel graph learning module to extract heterogeneous information from the multi-modal graph. The information in the multi-modal graph at different levels is aggregat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#35299;&#20915;&#20102;&#38480;&#21046;&#24179;&#34913;&#28857;Gram&#30697;&#38453;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.05797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Global Convergence Rate of Deep Equilibrium Models with General Activations. (arXiv:2302.05797v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19968;&#33324;&#28608;&#27963;&#20989;&#25968;&#30340;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#30340;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#35299;&#20915;&#20102;&#38480;&#21046;&#24179;&#34913;&#28857;Gram&#30697;&#38453;&#26368;&#23567;&#29305;&#24449;&#20540;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#19968;&#31687;&#35770;&#25991;&#20013;&#65292;Ling&#31561;&#20154;&#30740;&#31350;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#36807;&#21442;&#25968;&#21270;&#28145;&#24230;&#24179;&#34913;&#27169;&#22411;&#65288;DEQ&#65289;&#12290;&#20182;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#65292;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20197;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#20110;&#20855;&#26377;&#20219;&#20309;&#20855;&#26377;&#26377;&#30028;&#19968;&#38454;&#21644;&#20108;&#38454;&#23548;&#25968;&#30340;&#28608;&#27963;&#20989;&#25968;&#30340;DEQ&#65292;&#35813;&#20107;&#23454;&#20173;&#28982;&#25104;&#31435;&#12290;&#30001;&#20110;&#26032;&#30340;&#28608;&#27963;&#20989;&#25968;&#36890;&#24120;&#26159;&#38750;&#32447;&#24615;&#30340;&#65292;&#38480;&#21046;&#24179;&#34913;&#28857;&#30340;Gram&#30697;&#38453;&#30340;&#26368;&#23567;&#29305;&#24449;&#20540;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#38656;&#35201;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#24635;&#20307;Gram&#30697;&#38453;&#65292;&#24182;&#24320;&#21457;&#19968;&#31181;&#20855;&#26377;Hermite&#22810;&#39033;&#24335;&#23637;&#24320;&#30340;&#26032;&#24418;&#24335;&#30340;&#21452;&#37325;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a recent paper, Ling et al. investigated the over-parametrized Deep Equilibrium Model (DEQ) with ReLU activation. They proved that the gradient descent converges to a globally optimal solution at a linear convergence rate for the quadratic loss function. This paper shows that this fact still holds for DEQs with any general activation that has bounded first and second derivatives. Since the new activation function is generally non-linear, bounding the least eigenvalue of the Gram matrix of the equilibrium point is particularly challenging. To accomplish this task, we need to create a novel population Gram matrix and develop a new form of dual activation with Hermite polynomial expansion.
&lt;/p&gt;</description></item></channel></rss>