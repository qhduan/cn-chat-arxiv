<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03726</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Diffusion on language model embeddings for protein sequence generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03726
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#35774;&#35745;&#38656;&#35201;&#23545;&#34507;&#30333;&#36136;&#23431;&#23449;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#23613;&#31649;&#35768;&#22810;&#24037;&#20316;&#20542;&#21521;&#20110;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#25110;&#19987;&#27880;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;&#23478;&#26063;&#65292;&#20294;&#26080;&#26465;&#20214;&#29983;&#25104;&#30340;&#22522;&#30784;&#20219;&#21153;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#21644;&#37325;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#36825;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;DiMA&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#20174;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;ESM-2&#34893;&#29983;&#30340;&#23884;&#20837;&#36827;&#34892;&#36830;&#32493;&#25193;&#25955;&#20197;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;DiMA&#36229;&#36234;&#20102;&#21253;&#25324;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#35828;&#26126;&#20102;&#23548;&#33268;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36328;&#22810;&#31181;&#24418;&#24335;&#24191;&#27867;&#35780;&#20272;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#12289;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#29983;&#29289;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20135;&#29983;&#26032;&#39062;&#12289;&#22810;&#26679;&#21270;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#31934;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03726v1 Announce Type: cross  Abstract: Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accura
&lt;/p&gt;</description></item><item><title>Moco&#26159;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#20803;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26356;&#26032;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#21644;&#35745;&#31639;&#39044;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.04915</link><description>&lt;p&gt;
Moco: &#19968;&#31181;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#20803;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Moco: A Learnable Meta Optimizer for Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04915
&lt;/p&gt;
&lt;p&gt;
Moco&#26159;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#20803;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#23398;&#20064;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#26356;&#26032;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#21644;&#35745;&#31639;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65288;COPs&#65289;&#36890;&#24120;&#26159;NP&#38590;&#30340;&#12290;&#36807;&#21435;&#65292;&#36825;&#20123;&#38382;&#39064;&#20027;&#35201;&#26159;&#36890;&#36807;&#20154;&#24037;&#35774;&#35745;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#35299;&#20915;&#30340;&#65292;&#20294;&#26159;&#31070;&#32463;&#32593;&#32476;&#30340;&#36827;&#23637;&#20419;&#20351;&#20154;&#20204;&#24320;&#21457;&#20102;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#35768;&#22810;&#26041;&#27861;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30452;&#25509;&#26500;&#24314;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#22312;&#25512;&#29702;&#26102;&#26080;&#27861;&#36827;&#19968;&#27493;&#25913;&#36827;&#24050;&#32463;&#26500;&#24314;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;Moco&#23398;&#20064;&#20102;&#19968;&#20010;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#26681;&#25454;&#20174;&#24403;&#21069;&#25628;&#32034;&#29366;&#24577;&#25552;&#21462;&#30340;&#29305;&#24449;&#26469;&#26356;&#26032;&#35299;&#20915;&#26041;&#26696;&#26500;&#24314;&#36807;&#31243;&#12290;&#36825;&#31181;&#20803;&#35757;&#32451;&#36807;&#31243;&#20197;&#25628;&#32034;&#36807;&#31243;&#20013;&#25214;&#21040;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#20026;&#30446;&#26631;&#65292;&#32473;&#23450;&#25628;&#32034;&#39044;&#31639;&#31561;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;Moco&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#30340;&#24773;&#20917;&#65292;&#20363;&#22914;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#12290;Moco&#26159;&#19968;&#20010;&#23436;&#20840;&#21487;&#23398;&#20064;&#30340;&#20803;&#20248;&#21270;&#22120;&#65292;&#19981;&#20351;&#29992;&#20219;&#20309;&#29305;&#23450;&#38382;&#39064;&#30340;&#23616;&#37096;&#25628;&#32034;&#25110;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#26368;&#22823;&#26368;&#23567;&#36153;&#29992;&#27969;&#38382;&#39064;&#20013;&#27979;&#35797;&#20102;Moco&#12290;
&lt;/p&gt;
&lt;p&gt;
Relevant combinatorial optimization problems (COPs) are often NP-hard. While they have been tackled mainly via handcrafted heuristics in the past, advances in neural networks have motivated the development of general methods to learn heuristics from data. Many approaches utilize a neural network to directly construct a solution, but are limited in further improving based on already constructed solutions at inference time. Our approach, Moco, learns a graph neural network that updates the solution construction procedure based on features extracted from the current search state. This meta training procedure targets the overall best solution found during the search procedure given information such as the search budget. This allows Moco to adapt to varying circumstances such as different computational budgets. Moco is a fully learnable meta optimizer that does not utilize any problem specific local search or decomposition. We test Moco on the Traveling Salesman Problem (TSP) and Maximum In
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#29702;&#24615;&#19982;&#38750;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#26410;&#35299;&#38382;&#39064;&#12290;&#37325;&#28857;&#35752;&#35770;&#20102;&#34892;&#20026;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#38750;&#29702;&#24615;&#34892;&#20026;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#22788;&#29702;&#38750;&#29702;&#24615;&#20195;&#29702;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.17165</link><description>&lt;p&gt;
(&#38750;)&#29702;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65306;&#29616;&#29366;&#12289;&#30740;&#31350;&#25361;&#25112;&#21644;&#26410;&#35299;&#20043;&#38382;
&lt;/p&gt;
&lt;p&gt;
(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17165
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#29702;&#24615;&#19982;&#38750;&#29702;&#24615;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#26410;&#35299;&#38382;&#39064;&#12290;&#37325;&#28857;&#35752;&#35770;&#20102;&#34892;&#20026;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#30340;&#38750;&#29702;&#24615;&#34892;&#20026;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#26469;&#22788;&#29702;&#38750;&#29702;&#24615;&#20195;&#29702;&#65292;&#20294;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#24615;&#27010;&#24565;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#21344;&#25454;&#30528;&#37325;&#35201;&#22320;&#20301;&#12290;&#26080;&#35770;&#26159;&#27169;&#25311;&#20154;&#31867;&#25512;&#29702;&#36824;&#26159;&#36861;&#27714;&#26377;&#38480;&#26368;&#20248;&#24615;&#65292;&#25105;&#20204;&#36890;&#24120;&#24076;&#26395;&#20351;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#23613;&#21487;&#33021;&#29702;&#24615;&#12290;&#23613;&#31649;&#36825;&#20010;&#27010;&#24565;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#38750;&#24120;&#26680;&#24515;&#65292;&#20294;&#23545;&#20110;&#20160;&#20040;&#26500;&#25104;&#29702;&#24615;&#20195;&#29702;&#24182;&#27809;&#26377;&#32479;&#19968;&#30340;&#23450;&#20041;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#29702;&#24615;&#19982;&#38750;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#20010;&#39046;&#22495;&#30340;&#26410;&#35299;&#38382;&#39064;&#12290;&#22312;&#20854;&#20182;&#39046;&#22495;&#23545;&#29702;&#24615;&#30340;&#29702;&#35299;&#23545;&#20854;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#27010;&#24565;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#32463;&#27982;&#23398;&#12289;&#21746;&#23398;&#21644;&#24515;&#29702;&#23398;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#30528;&#37325;&#32771;&#34385;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#26576;&#20123;&#24773;&#22659;&#20013;&#38750;&#29702;&#24615;&#34892;&#20026;&#21487;&#33021;&#26159;&#26368;&#20248;&#30340;&#24773;&#20917;&#12290;&#20851;&#20110;&#22788;&#29702;&#38750;&#29702;&#24615;&#20195;&#29702;&#30340;&#26041;&#27861;&#24050;&#32463;&#24471;&#21040;&#20102;&#19968;&#20123;&#21457;&#23637;&#65292;&#21253;&#25324;&#35782;&#21035;&#21644;&#20132;&#20114;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17165v2 Announce Type: replace Abstract: The concept of rationality is central to the field of artificial intelligence. Whether we are seeking to simulate human reasoning, or the goal is to achieve bounded optimality, we generally seek to make artificial agents as rational as possible. Despite the centrality of the concept within AI, there is no unified definition of what constitutes a rational agent. This article provides a survey of rationality and irrationality in artificial intelligence, and sets out the open questions in this area. The understanding of rationality in other fields has influenced its conception within artificial intelligence, in particular work in economics, philosophy and psychology. Focusing on the behaviour of artificial agents, we consider irrational behaviours that can prove to be optimal in certain scenarios. Some methods have been developed to deal with irrational agents, both in terms of identification and interaction, however work in this area re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20445;&#25345;&#37051;&#23621;&#30456;&#20284;&#24615;&#23454;&#29616;&#20102;&#26222;&#36866;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#24322;&#31867;&#22270;&#19978;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36127;&#20998;&#31867;&#25439;&#22833;&#30340;&#26356;&#26032;&#19982;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#21576;&#36127;&#30456;&#20851;&#65292;&#35299;&#37322;&#20102;&#22270;&#25915;&#20987;&#32773;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#23545;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26032;&#39062;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.09754</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#25345;&#37051;&#23621;&#30456;&#20284;&#24615;&#23454;&#29616;&#26222;&#36866;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Universally Robust Graph Neural Networks by Preserving Neighbor Similarity. (arXiv:2401.09754v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20445;&#25345;&#37051;&#23621;&#30456;&#20284;&#24615;&#23454;&#29616;&#20102;&#26222;&#36866;&#40065;&#26834;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312;&#24322;&#31867;&#22270;&#19978;&#25506;&#32034;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#33030;&#24369;&#24615;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36127;&#20998;&#31867;&#25439;&#22833;&#30340;&#26356;&#26032;&#19982;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#21576;&#36127;&#30456;&#20851;&#65292;&#35299;&#37322;&#20102;&#22270;&#25915;&#20987;&#32773;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#23545;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26032;&#39062;&#22320;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#20851;&#31995;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#31867;&#22270;&#19978;&#23481;&#26131;&#21463;&#21040;&#32467;&#26500;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#40065;&#26834;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21516;&#31867;&#22270;&#19978;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24322;&#31867;&#22270;&#19978;&#30340;&#33030;&#24369;&#24615;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#26410;&#35299;&#20043;&#35868;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24320;&#22987;&#25506;&#32034;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#31867;&#22270;&#19978;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36127;&#20998;&#31867;&#25439;&#22833;&#30340;&#26356;&#26032;&#19982;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#30340;&#24130;&#21644;&#32858;&#21512;&#30340;&#25104;&#23545;&#30456;&#20284;&#24615;&#21576;&#36127;&#30456;&#20851;&#12290;&#36825;&#19968;&#29702;&#35770;&#35777;&#26126;&#35299;&#37322;&#20102;&#23454;&#35777;&#35266;&#23519;&#65292;&#21363;&#22270;&#25915;&#20987;&#32773;&#20542;&#21521;&#20110;&#22522;&#20110;&#37051;&#23621;&#29305;&#24449;&#32780;&#19981;&#26159;&#20010;&#20307;&#29305;&#24449;&#36830;&#25509;&#19981;&#30456;&#20284;&#33410;&#28857;&#23545;&#65292;&#26080;&#35770;&#26159;&#22312;&#21516;&#31867;&#22270;&#36824;&#26159;&#24322;&#31867;&#22270;&#19978;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#26032;&#39062;&#22320;&#24341;&#20837;&#20102;&#19968;&#31181;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Despite the tremendous success of graph neural networks in learning relational data, it has been widely investigated that graph neural networks are vulnerable to structural attacks on homophilic graphs. Motivated by this, a surge of robust models is crafted to enhance the adversarial robustness of graph neural networks on homophilic graphs. However, the vulnerability based on heterophilic graphs remains a mystery to us. To bridge this gap, in this paper, we start to explore the vulnerability of graph neural networks on heterophilic graphs and theoretically prove that the update of the negative classification loss is negatively correlated with the pairwise similarities based on the powered aggregated neighbor features. This theoretical proof explains the empirical observations that the graph attacker tends to connect dissimilar node pairs based on the similarities of neighbor features instead of ego features both on homophilic and heterophilic graphs. In this way, we novelly introduce a
&lt;/p&gt;</description></item></channel></rss>