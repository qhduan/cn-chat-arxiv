<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24555;&#36895;&#37327;&#23376;&#31639;&#27861;&#36827;&#34892;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#20197;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.08045</link><description>&lt;p&gt;
&#24555;&#36895;&#37327;&#23376;&#31639;&#27861;&#29992;&#20110;&#27880;&#24847;&#21147;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Fast Quantum Algorithm for Attention Computation. (arXiv:2307.08045v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08045
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#24555;&#36895;&#37327;&#23376;&#31639;&#27861;&#36827;&#34892;&#27880;&#24847;&#21147;&#35745;&#31639;&#65292;&#20197;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#34920;&#29616;&#20986;&#33394;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#24322;&#24120;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#30001;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#39537;&#21160;&#65292;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#24182;&#22312;&#21508;&#31181;&#19982;&#35821;&#35328;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#12290;LLMs &#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#29983;&#25104;&#12289;&#25991;&#26412;&#20998;&#31867;&#12289;&#35821;&#35328;&#24314;&#27169;&#31561;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#23427;&#20204;&#22312;&#25429;&#25417;&#22797;&#26434;&#30340;&#35821;&#35328;&#27169;&#24335;&#12289;&#29702;&#35299;&#32972;&#26223;&#12289;&#29983;&#25104;&#36830;&#36143;&#19988;&#30456;&#20851;&#30340;&#25991;&#26412;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#26696;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#30340;&#26550;&#26500;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#23427;&#26159;&#19968;&#20010;&#22522;&#26412;&#32452;&#20214;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#22312;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26377;&#25928;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#21152;&#24555;&#27880;&#24847;&#21147;&#35745;&#31639;&#26041;&#26696;&#30340;&#36895;&#24230;&#26159;&#21152;&#36895;LLMs&#35745;&#31639;&#30340;&#26680;&#24515;&#38382;&#39064;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated exceptional performance across a wide range of tasks. These models, powered by advanced deep learning techniques, have revolutionized the field of natural language processing (NLP) and have achieved remarkable results in various language-related tasks.  LLMs have excelled in tasks such as machine translation, sentiment analysis, question answering, text generation, text classification, language modeling, and more. They have proven to be highly effective in capturing complex linguistic patterns, understanding context, and generating coherent and contextually relevant text. The attention scheme plays a crucial role in the architecture of large language models (LLMs). It is a fundamental component that enables the model to capture and utilize contextual information during language processing tasks effectively. Making the attention scheme computation faster is one of the central questions to speed up the LLMs computation. It is well-known that
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.12344</link><description>&lt;p&gt;
&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#35777;&#26126;&#31934;&#30830;&#30340;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An efficient, provably exact algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12344
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#31639;&#27861;&#20855;&#26377;&#24736;&#20037;&#30340;&#21382;&#21490;&#65292;&#33267;&#23569;&#21487;&#20197;&#36861;&#28335;&#21040;1936&#24180;&#30340;&#32447;&#24615;&#21028;&#21035;&#20998;&#26512;&#12290;&#23545;&#20110;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#35768;&#22810;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#24471;&#21040;&#30456;&#24212;&#30340;0-1&#25439;&#22833;&#20998;&#31867;&#38382;&#39064;&#30340;&#31934;&#30830;&#35299;&#65292;&#20294;&#23545;&#20110;&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;&#24050;&#32463;&#35777;&#26126;&#36825;&#20010;&#38382;&#39064;&#22312;&#23436;&#20840;&#33539;&#22260;&#20869;&#26159;NP&#38590;&#30340;&#12290;&#25152;&#26377;&#26367;&#20195;&#26041;&#27861;&#37117;&#28041;&#21450;&#26576;&#31181;&#24418;&#24335;&#30340;&#36817;&#20284;&#65292;&#21253;&#25324;&#20351;&#29992;0-1&#25439;&#22833;&#30340;&#20195;&#29702;&#65288;&#20363;&#22914;hinge&#25110;logistic&#25439;&#22833;&#65289;&#25110;&#36817;&#20284;&#30340;&#32452;&#21512;&#25628;&#32034;&#65292;&#36825;&#20123;&#37117;&#19981;&#33021;&#20445;&#35777;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#12290;&#25214;&#21040;&#35299;&#20915;&#23450;&#32500;&#24230;0-1&#25439;&#22833;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#26377;&#25928;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#31639;&#27861;&#30340;&#26500;&#24314;&#36807;&#31243;&#65292;&#22686;&#37327;&#21333;&#20803;&#26522;&#20030;&#65288;ICE&#65289;&#65292;&#23427;&#21487;&#20197;&#31934;&#30830;&#35299;&#20915;0-1&#25439;&#22833;&#20998;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms for solving the linear classification problem have a long history, dating back at least to 1936 with linear discriminant analysis. For linearly separable data, many algorithms can obtain the exact solution to the corresponding 0-1 loss classification problem efficiently, but for data which is not linearly separable, it has been shown that this problem, in full generality, is NP-hard. Alternative approaches all involve approximations of some kind, including the use of surrogates for the 0-1 loss (for example, the hinge or logistic loss) or approximate combinatorial search, none of which can be guaranteed to solve the problem exactly. Finding efficient algorithms to obtain an exact i.e. globally optimal solution for the 0-1 loss linear classification problem with fixed dimension, remains an open problem. In research we report here, we detail the construction of a new algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss classification problem exactly in po
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36716;&#25442;&#22120;&#30340;&#32463;&#39564;&#20139;&#20048;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#37327;&#26410;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25968;&#25454;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#20135;&#21697;&#30340;&#20139;&#20048;&#20215;&#26684;&#21644;&#27966;&#29983;&#25351;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.00044</link><description>&lt;p&gt;
&#30001;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20139;&#20048;&#20215;&#26684;&#21644;&#36136;&#37327;&#35843;&#25972;&#20215;&#26684;&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Hedonic Prices and Quality Adjusted Price Indices Powered by AI. (arXiv:2305.00044v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36716;&#25442;&#22120;&#30340;&#32463;&#39564;&#20139;&#20048;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#37327;&#26410;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25968;&#25454;&#65292;&#20934;&#30830;&#22320;&#20272;&#35745;&#20135;&#21697;&#30340;&#20139;&#20048;&#20215;&#26684;&#21644;&#27966;&#29983;&#25351;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#32463;&#27982;&#29615;&#22659;&#19979;&#65292;&#20351;&#29992;&#30005;&#23376;&#35760;&#24405;&#20934;&#30830;&#22320;&#23454;&#26102;&#27979;&#37327;&#20215;&#26684;&#25351;&#25968;&#30340;&#21464;&#21270;&#23545;&#20110;&#36319;&#36394;&#36890;&#32960;&#21644;&#29983;&#20135;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#32463;&#39564;&#20139;&#20048;&#27169;&#22411;&#65292;&#33021;&#22815;&#22788;&#29702;&#22823;&#37327;&#26410;&#32467;&#26500;&#21270;&#30340;&#20135;&#21697;&#25968;&#25454;&#65288;&#25991;&#26412;&#12289;&#22270;&#20687;&#12289;&#20215;&#26684;&#21644;&#25968;&#37327;&#65289;&#65292;&#24182;&#36755;&#20986;&#31934;&#30830;&#30340;&#20139;&#20048;&#20215;&#26684;&#20272;&#35745;&#21644;&#27966;&#29983;&#25351;&#25968;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#22270;&#20687;&#20013;&#29983;&#25104;&#25277;&#35937;&#30340;&#20135;&#21697;&#23646;&#24615;&#25110;&#8221;&#29305;&#24449;&#8220;&#65292;&#28982;&#21518;&#20351;&#29992;&#36825;&#20123;&#23646;&#24615;&#26469;&#20272;&#31639;&#20139;&#20048;&#20215;&#26684;&#20989;&#25968;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;transformers&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#26377;&#20851;&#20135;&#21697;&#30340;&#25991;&#26412;&#20449;&#24687;&#36716;&#25442;&#20026;&#25968;&#23383;&#29305;&#24449;&#65292;&#20351;&#29992;&#35757;&#32451;&#25110;&#24494;&#35843;&#36807;&#30340;&#20135;&#21697;&#25551;&#36848;&#20449;&#24687;&#65292;&#20351;&#29992;&#27531;&#24046;&#32593;&#32476;&#27169;&#22411;&#23558;&#20135;&#21697;&#22270;&#20687;&#36716;&#25442;&#20026;&#25968;&#23383;&#29305;&#24449;&#12290;&#20026;&#20102;&#20135;&#29983;&#20272;&#35745;&#30340;&#20139;&#20048;&#20215;&#26684;&#20989;&#25968;&#65292;&#25105;&#20204;&#20877;&#27425;&#20351;&#29992;&#22810;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#65292;&#35757;&#32451;&#20197;&#22312;&#25152;&#26377;&#26102;&#38388;&#27573;&#21516;&#26102;&#39044;&#27979;&#20135;&#21697;&#30340;&#20215;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate, real-time measurements of price index changes using electronic records are essential for tracking inflation and productivity in today's economic environment. We develop empirical hedonic models that can process large amounts of unstructured product data (text, images, prices, quantities) and output accurate hedonic price estimates and derived indices. To accomplish this, we generate abstract product attributes, or ``features,'' from text descriptions and images using deep neural networks, and then use these attributes to estimate the hedonic price function. Specifically, we convert textual information about the product to numeric features using large language models based on transformers, trained or fine-tuned using product descriptions, and convert the product image to numeric features using a residual network model. To produce the estimated hedonic price function, we again use a multi-task neural network trained to predict a product's price in all time periods simultaneousl
&lt;/p&gt;</description></item></channel></rss>