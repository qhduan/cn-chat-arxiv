<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;RL-MUL&#65292;&#21033;&#29992;&#30697;&#38453;&#21644;&#24352;&#37327;&#34920;&#31034;&#20056;&#27861;&#22120;&#30340;&#21387;&#32553;&#26641;&#65292;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22870;&#21169;&#23454;&#29616;&#21306;&#22495;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#25193;&#23637;&#21040;&#20248;&#21270;&#34701;&#21512;&#20056;-&#32047;&#21152;&#65288;MAC&#65289;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2404.00639</link><description>&lt;p&gt;
RL-MUL&#65306;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
RL-MUL: Multiplier Design Optimization with Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00639
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;RL-MUL&#65292;&#21033;&#29992;&#30697;&#38453;&#21644;&#24352;&#37327;&#34920;&#31034;&#20056;&#27861;&#22120;&#30340;&#21387;&#32553;&#26641;&#65292;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22870;&#21169;&#23454;&#29616;&#21306;&#22495;&#21644;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#21516;&#26102;&#25193;&#23637;&#21040;&#20248;&#21270;&#34701;&#21512;&#20056;-&#32047;&#21152;&#65288;MAC&#65289;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20056;&#27861;&#26159;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#20056;&#27861;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#30005;&#36335;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35774;&#35745;&#31354;&#38388;&#24040;&#22823;&#65292;&#20248;&#21270;&#20056;&#27861;&#22120;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#38750;&#24179;&#20961;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL-MUL&#65292;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20056;&#27861;&#22120;&#35774;&#35745;&#20248;&#21270;&#26694;&#26550;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#30697;&#38453;&#21644;&#24352;&#37327;&#34920;&#31034;&#20056;&#27861;&#22120;&#30340;&#21387;&#32553;&#26641;&#65292;&#22522;&#20110;&#36825;&#19968;&#34920;&#31034;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26080;&#32541;&#22320;&#38598;&#25104;&#20026;&#20195;&#29702;&#32593;&#32476;&#12290;&#20195;&#29702;&#21487;&#20197;&#23398;&#20064;&#26681;&#25454;&#23450;&#21046;&#21270;&#30340;&#21487;&#23481;&#24525;&#21306;&#22495;&#19982;&#24310;&#36831;&#20043;&#38388;&#30340;&#26435;&#34913;&#20851;&#31995;&#26469;&#20248;&#21270;&#20056;&#27861;&#22120;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;RL-MUL&#30340;&#33021;&#21147;&#34987;&#25193;&#23637;&#21040;&#20248;&#21270;&#34701;&#21512;&#20056;-&#32047;&#21152;&#65288;MAC&#65289;&#35774;&#35745;&#12290;&#23454;&#39564;&#22312;&#19981;&#21516;&#20301;&#23485;&#30340;&#20056;&#27861;&#22120;&#19978;&#36827;&#34892;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;RL-MUL&#29983;&#25104;&#30340;&#20056;&#27861;&#22120;&#33021;&#22815;&#36229;&#36234;&#25152;&#26377;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00639v1 Announce Type: cross  Abstract: Multiplication is a fundamental operation in many applications, and multipliers are widely adopted in various circuits. However, optimizing multipliers is challenging and non-trivial due to the huge design space. In this paper, we propose RL-MUL, a multiplier design optimization framework based on reinforcement learning. Specifically, we utilize matrix and tensor representations for the compressor tree of a multiplier, based on which the convolutional neural networks can be seamlessly incorporated as the agent network. The agent can learn to optimize the multiplier structure based on a Pareto-driven reward which is customized to accommodate the trade-off between area and delay. Additionally, the capability of RL-MUL is extended to optimize the fused multiply-accumulator (MAC) designs. Experiments are conducted on different bit widths of multipliers. The results demonstrate that the multipliers produced by RL-MUL can dominate all baseli
&lt;/p&gt;</description></item><item><title>Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;</title><link>https://arxiv.org/abs/2404.00399</link><description>&lt;p&gt;
Aurora-M: &#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#65292;&#31532;&#19968;&#20010;&#24320;&#28304;&#30340;&#22810;&#35821;&#35328;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32418;&#38431;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Aurora-M: The First Open Source Multilingual Language Model Red-teamed according to the U.S. Executive Order
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00399
&lt;/p&gt;
&lt;p&gt;
Aurora-M &#26159;&#31532;&#19968;&#20010;&#26681;&#25454;&#32654;&#22269;&#34892;&#25919;&#21629;&#20196;&#36827;&#34892;&#32418;&#38431;&#27979;&#35797;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#19978;&#35757;&#32451;&#65292;&#19981;&#26029;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;&#20102;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807; 2 &#19975;&#20159;&#20010;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#25903;&#25345;&#22810;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#26102;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#38480;&#21046;&#20102;&#21487;&#35775;&#38382;&#24615;&#12290;BLOOM &#21644; StarCoder &#31561;&#20513;&#35758;&#26088;&#22312;&#20351;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#20110;&#21327;&#20316;&#31038;&#21306;&#24320;&#21457;&#26356;&#20855;&#27665;&#20027;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23384;&#22312;&#30340;&#27169;&#22411;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#65306;&#22810;&#35821;&#35328;&#33021;&#21147;&#26377;&#38480;&#65292;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#39044;&#35757;&#32451;&#21448;&#20855;&#26377;&#39640;&#26114;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#38656;&#35201;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#23433;&#20840;&#21644;&#21457;&#23637;&#27861;&#24459;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; Aurora-M&#65292;&#19968;&#20010;&#21253;&#21547; 15B &#21442;&#25968;&#30340;&#22810;&#35821;&#35328;&#24320;&#28304;&#27169;&#22411;&#65292;&#35757;&#32451;&#35821;&#35328;&#21253;&#25324;&#33521;&#35821;&#12289;&#33452;&#20848;&#35821;&#12289;&#21360;&#22320;&#35821;&#12289;&#26085;&#35821;&#12289;&#36234;&#21335;&#35821;&#21644;&#20195;&#30721;&#12290;Aurora-M &#19981;&#26029;&#20174; StarCoderPlus &#19978;&#39044;&#35757;&#32451;&#65292;&#39069;&#22806;&#35757;&#32451;&#20102; 4350 &#20159;&#20010; token&#65292;&#24635;&#35757;&#32451; token &#25968;&#36229;&#36807;&#20102; 2 &#19975;&#20159;&#20010;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#22312;&#20154;&#24037;&#23457;&#26680;&#30340;&#23433;&#20840;&#35828;&#26126;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#24320;&#28304;&#22810;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#24320;&#21457;&#19982;&#20256;&#32479;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00399v1 Announce Type: cross  Abstract: Pretrained language models underpin several AI applications, but their high computational cost for training limits accessibility. Initiatives such as BLOOM and StarCoder aim to democratize access to pretrained models for collaborative community development. However, such existing models face challenges: limited multilingual capabilities, continual pretraining causing catastrophic forgetting, whereas pretraining from scratch is computationally expensive, and compliance with AI safety and development laws. This paper presents Aurora-M, a 15B parameter multilingual open-source model trained on English, Finnish, Hindi, Japanese, Vietnamese, and code. Continually pretrained from StarCoderPlus on 435 billion additional tokens, Aurora-M surpasses 2 trillion tokens in total training token count. It is the first open-source multilingual model fine-tuned on human-reviewed safety instructions, thus aligning its development not only with conventio
&lt;/p&gt;</description></item><item><title>&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.17919</link><description>&lt;p&gt;
LISA&#65306;&#29992;&#20110;&#39640;&#25928;&#20869;&#23384;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17919
&lt;/p&gt;
&lt;p&gt;
&#36880;&#23618;&#37325;&#35201;&#24615;&#37319;&#26679;&#30340;&#26032;&#26041;&#27861;LISA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35760;&#24518;&#25104;&#26412;&#20302;&#19988;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#33258;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#28982;&#32780;&#23427;&#20204;&#24040;&#22823;&#30340;&#20869;&#23384;&#28040;&#32791;&#24050;&#25104;&#20026;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35832;&#22914;&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#20043;&#31867;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#20294;&#22312;&#22823;&#22810;&#25968;&#22823;&#35268;&#27169;&#24494;&#35843;&#35774;&#32622;&#20013;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#20173;&#26080;&#27861;&#19982;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#30456;&#21305;&#37197;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#19981;&#36275;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LoRA&#22312;&#24494;&#35843;&#20219;&#21153;&#20013;&#30340;&#36880;&#23618;&#29305;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#19981;&#21516;&#23618;&#20043;&#38388;&#26435;&#37325;&#33539;&#25968;&#30340;&#24322;&#24120;&#20559;&#26012;&#12290;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#31616;&#21333;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#35760;&#24518;&#25104;&#26412;&#20302;&#20110;LoRA&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#24191;&#27867;&#30340;&#35774;&#32622;&#20013;&#20248;&#20110;LoRA&#21644;&#23436;&#25972;&#21442;&#25968;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;Layerwise Importance Sampled AdamW&#65288;LISA&#65289;&#65292;&#36825;&#26159;LoRA&#30340;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#24212;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17919v1 Announce Type: cross  Abstract: The machine learning community has witnessed impressive advancements since the first appearance of large language models (LLMs), yet their huge memory consumption has become a major roadblock to large-scale training. Parameter Efficient Fine-Tuning techniques such as Low-Rank Adaptation (LoRA) have been proposed to alleviate this problem, but their performance still fails to match full parameter training in most large-scale fine-tuning settings. Attempting to complement this deficiency, we investigate layerwise properties of LoRA on fine-tuning tasks and observe an uncommon skewness of weight norms across different layers. Utilizing this key observation, a surprisingly simple training strategy is discovered, which outperforms both LoRA and full parameter training in a wide range of settings with memory costs as low as LoRA. We name it Layerwise Importance Sampled AdamW (LISA), a promising alternative for LoRA, which applies the idea of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;RAMDA&#31639;&#27861;&#29992;&#20110;&#35757;&#32451;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20351;&#29992;&#36817;&#20284;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#25910;&#25947;&#28857;&#38468;&#36817;RAMDA&#30340;&#36845;&#20195;&#36798;&#21040;&#20102;&#26368;&#20248;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.14398</link><description>&lt;p&gt;
&#29992;&#25928;&#29575;&#20302;&#19979;&#30340;&#36817;&#20284;&#23376;&#38382;&#39064;&#35299;&#31639;&#22120;&#35757;&#32451;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#21160;&#37327;&#21452;&#24179;&#22343;
&lt;/p&gt;
&lt;p&gt;
Regularized Adaptive Momentum Dual Averaging with an Efficient Inexact Subproblem Solver for Training Structured Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14398
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;RAMDA&#31639;&#27861;&#29992;&#20110;&#35757;&#32451;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;&#20351;&#29992;&#36817;&#20284;&#35299;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#25910;&#25947;&#28857;&#38468;&#36817;RAMDA&#30340;&#36845;&#20195;&#36798;&#21040;&#20102;&#26368;&#20248;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#32467;&#26500;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#21160;&#37327;&#21452;&#24179;&#22343;&#65288;RAMDA&#65289;&#31639;&#27861;&#12290;&#19982;&#29616;&#26377;&#30340;&#27491;&#21017;&#21270;&#33258;&#36866;&#24212;&#26041;&#27861;&#31867;&#20284;&#65292;RAMDA&#30340;&#26356;&#26032;&#26041;&#21521;&#35745;&#31639;&#23376;&#38382;&#39064;&#28041;&#21450;&#38750;&#20809;&#28369;&#27491;&#21017;&#21270;&#39033;&#21644;&#23545;&#35282;&#39044;&#22788;&#29702;&#22120;&#65292;&#22240;&#27492;&#19968;&#33324;&#32780;&#35328;&#27809;&#26377;&#23553;&#38381;&#24418;&#24335;&#30340;&#35299;&#12290;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#23454;&#29616;&#30340;&#36817;&#20284;&#26465;&#20214;&#65292;&#20445;&#30041;&#20102;&#31867;&#20284;&#20110;&#31934;&#30830;&#29256;&#26412;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#37197;&#22871;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#65292;&#29992;&#20110;&#20351;RAMDA&#21644;&#29616;&#26377;&#26041;&#27861;&#30340;&#23376;&#38382;&#39064;&#22312;&#23454;&#36341;&#20013;&#21487;&#34892;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#20998;&#20998;&#26512;&#20013;&#30340;&#27969;&#24418;&#35782;&#21035;&#29702;&#35770;&#34920;&#26126;&#65292;&#21363;&#20351;&#23384;&#22312;&#36825;&#31181;&#36817;&#20284;&#24615;&#65292;RAMDA&#30340;&#36845;&#20195;&#22312;&#28176;&#36817;&#25910;&#25947;&#30340;&#31283;&#23450;&#28857;&#22788;&#36798;&#21040;&#30001;&#27491;&#21017;&#21270;&#39033;&#35825;&#23548;&#30340;&#29702;&#24819;&#32467;&#26500;&#12290;&#22312;&#25910;&#25947;&#28857;&#38468;&#36817;&#65292;&#36825;&#31181;&#32467;&#26500;&#22312;&#23616;&#37096;&#19978;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14398v1 Announce Type: new  Abstract: We propose a Regularized Adaptive Momentum Dual Averaging (RAMDA) algorithm for training structured neural networks. Similar to existing regularized adaptive methods, the subproblem for computing the update direction of RAMDA involves a nonsmooth regularizer and a diagonal preconditioner, and therefore does not possess a closed-form solution in general. We thus also carefully devise an implementable inexactness condition that retains convergence guarantees similar to the exact versions, and propose a companion efficient solver for the subproblems of both RAMDA and existing methods to make them practically feasible. We leverage the theory of manifold identification in variational analysis to show that, even in the presence of such inexactness, the iterates of RAMDA attain the ideal structure induced by the regularizer at the stationary point of asymptotic convergence. This structure is locally optimal near the point of convergence, so RAM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12511</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#21521;&#26799;&#24230;&#30340;Frank-Wolfe&#20248;&#21270;&#29992;&#20110;&#39640;&#25928;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Forward Gradient-Based Frank-Wolfe Optimization for Memory Efficient Deep Neural Network Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38656;&#35201;&#22312;&#27599;&#20010;&#32423;&#21035;&#35745;&#31639;&#26799;&#24230;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#25110;&#21453;&#21521;&#27169;&#24335;&#24494;&#20998;&#35745;&#31639;&#26799;&#24230;&#38656;&#35201;&#28040;&#32791;&#22823;&#37327;&#20869;&#23384;&#65292;&#20351;&#21453;&#21521;&#20256;&#25773;&#25104;&#20026;&#35745;&#31639;&#26799;&#24230;&#30340;&#19968;&#31181;&#20302;&#25928;&#26041;&#27861;&#12290;&#26412;&#25991;&#37325;&#28857;&#20998;&#26512;&#20102;&#33879;&#21517;&#30340;Frank-Wolfe&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#21363;&#26377;&#26465;&#20214;&#30340;&#26799;&#24230;&#31639;&#27861;&#65292;&#36890;&#36807;&#35775;&#38382;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#20197;&#35745;&#31639;&#26799;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#26174;&#31034;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#36890;&#36807;&#35775;&#38382;&#22312;&#21069;&#21521;&#33258;&#21160;&#24494;&#20998;&#20013;&#33719;&#24471;&#30340;&#30495;&#26799;&#24230;&#30340;&#26377;&#22122;&#22768;&#20272;&#35745;&#65292; &#21363;&#31216;&#20026;Projected Forward Gradient&#65292;&#25910;&#25947;&#20110;&#26368;&#20248;&#35299;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;&#27425;&#32447;&#24615;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26631;&#20934;&#30340;Frank-Wolfe&#31639;&#27861;&#65292;&#22312;&#25552;&#20379;Projected Fors
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12511v1 Announce Type: new  Abstract: Training a deep neural network using gradient-based methods necessitates the calculation of gradients at each level. However, using backpropagation or reverse mode differentiation, to calculate the gradients necessities significant memory consumption, rendering backpropagation an inefficient method for computing gradients. This paper focuses on analyzing the performance of the well-known Frank-Wolfe algorithm, a.k.a. conditional gradient algorithm by having access to the forward mode of automatic differentiation to compute gradients. We provide in-depth technical details that show the proposed Algorithm does converge to the optimal solution with a sub-linear rate of convergence by having access to the noisy estimate of the true gradient obtained in the forward mode of automated differentiation, referred to as the Projected Forward Gradient. In contrast, the standard Frank-Wolfe algorithm, when provided with access to the Projected Forwar
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102; CodeUltraFeedback &#25968;&#25454;&#38598;&#65292;&#36890;&#36807; AI &#21453;&#39304;&#20351; 14 &#31181;&#19981;&#21516;&#30340; LLMs &#23545; 10,000 &#20010;&#22797;&#26434;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#20351;&#29992; LLM-as-a-Judge &#26041;&#27861;&#35780;&#20272;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272; LLM &#23545;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934; CODAL-Bench&#12290;</title><link>https://arxiv.org/abs/2403.09032</link><description>&lt;p&gt;
CodeUltraFeedback&#65306;&#19968;&#31181;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;LLM&#20316;&#20026;&#27861;&#23448;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language Models to Coding Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09032
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102; CodeUltraFeedback &#25968;&#25454;&#38598;&#65292;&#36890;&#36807; AI &#21453;&#39304;&#20351; 14 &#31181;&#19981;&#21516;&#30340; LLMs &#23545; 10,000 &#20010;&#22797;&#26434;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#24182;&#20351;&#29992; LLM-as-a-Judge &#26041;&#27861;&#35780;&#20272;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272; LLM &#23545;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934; CODAL-Bench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#29992;&#25143;&#23450;&#20041;&#30340;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24615;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#65292;&#38656;&#35201;&#35780;&#20272;&#22797;&#26434;&#25991;&#26412;LLMs&#30340;&#36755;&#20986;&#12290;&#29616;&#26377;&#22522;&#20934;&#20208;&#36182;&#33258;&#21160;&#21270;&#25351;&#26631;&#21644;&#38745;&#24577;&#20998;&#26512;&#24037;&#20855;&#65292;&#26410;&#33021;&#35780;&#20272;&#29992;&#25143;&#25351;&#20196;&#21644;LLM&#36755;&#20986;&#20013;&#30340;&#24494;&#22937;&#20043;&#22788;&#65292;&#31361;&#26174;&#20102;&#23545;LLM&#20559;&#22909;&#23545;&#40784;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CodeUltraFeedback&#65292;&#19968;&#20010;&#21253;&#21547;10,000&#20010;&#22797;&#26434;&#25351;&#20196;&#30340;&#20559;&#22909;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;AI&#21453;&#39304;&#26469;&#35843;&#25972;&#21644;&#23545;&#40784;LLMs&#19982;&#32534;&#31243;&#20559;&#22909;&#12290;&#25105;&#20204;&#20351;&#29992;14&#31181;&#19981;&#21516;&#30340;LLMs&#23545;&#36825;&#20123;&#25351;&#20196;&#29983;&#25104;&#21709;&#24212;&#65292;&#28982;&#21518;&#26681;&#25454;&#23427;&#20204;&#19982;&#20116;&#31181;&#32534;&#31243;&#20559;&#22909;&#30340;&#23545;&#40784;&#24773;&#20917;&#36827;&#34892;&#27880;&#37322;&#65292;&#20351;&#29992;GPT-3.5&#30340;LLM&#20316;&#20026;&#27861;&#23448;&#26041;&#27861;&#20135;&#29983;&#25968;&#23383;&#21644;&#25991;&#26412;&#21453;&#39304;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CODAL-Bench&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLM&#19982;&#36825;&#20123;&#32534;&#31243;&#20559;&#22909;&#23545;&#40784;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;C
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09032v1 Announce Type: cross  Abstract: Evaluating the alignment of large language models (LLMs) with user-defined coding preferences is a challenging endeavour that requires assessing intricate textual LLMs' outputs. By relying on automated metrics and static analysis tools, existing benchmarks fail to assess nuances in user instructions and LLM outputs, highlighting the need for large-scale datasets and benchmarks for LLM preference alignment. In this paper, we introduce CodeUltraFeedback, a preference dataset of 10,000 complex instructions to tune and align LLMs to coding preferences through AI feedback. We generate responses to the instructions using a pool of 14 diverse LLMs, which we then annotate according to their alignment with five coding preferences using the LLM-as-a-Judge approach with GPT-3.5, producing both numerical and textual feedback. We also present CODAL-Bench, a benchmark for assessing LLM alignment with these coding preferences. Our results show that C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#21517;&#20026;&#35748;&#30693;&#23433;&#20840;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#20998;&#26512;&#31070;&#32463;&#25216;&#26415;&#23545;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#30456;&#20851;&#38382;&#39064;&#25551;&#36848;&#21644;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.07945</link><description>&lt;p&gt;
&#19968;&#20010;&#35299;&#20915;&#31070;&#32463;&#25216;&#26415;&#35748;&#30693;&#23433;&#20840;&#38382;&#39064;&#30340;&#25968;&#23398;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#21517;&#20026;&#35748;&#30693;&#23433;&#20840;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#20998;&#26512;&#31070;&#32463;&#25216;&#26415;&#23545;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#30456;&#20851;&#38382;&#39064;&#25551;&#36848;&#21644;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#31070;&#32463;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#31070;&#32463;&#25216;&#26415;&#21644;&#23433;&#20840;&#20043;&#38388;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20851;&#38190;&#20132;&#21449;&#28857;&#12290;&#26893;&#20837;&#24335;&#35774;&#22791;&#12289;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#21644;&#38750;&#20405;&#20837;&#24335;&#27835;&#30103;&#37117;&#24102;&#26469;&#20102;&#36829;&#21453;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#30340;&#21069;&#26223;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#23478;&#21644;&#21307;&#29983;&#21628;&#21505;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064; -- &#25105;&#20204;&#31216;&#20043;&#20026;&#35748;&#30693;&#23433;&#20840; -- &#20294;&#24212;&#29992;&#24037;&#20316;&#21463;&#21040;&#38480;&#21046;&#12290;&#38459;&#30861;&#31185;&#23398;&#21644;&#24037;&#31243;&#21162;&#21147;&#35299;&#20915;&#35748;&#30693;&#23433;&#20840;&#38382;&#39064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#28165;&#26224;&#25551;&#36848;&#21644;&#20998;&#26512;&#30456;&#20851;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35748;&#30693;&#23433;&#20840;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#37492;&#22810;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#65292;&#23454;&#29616;&#36825;&#31181;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#23545;&#35748;&#30693;&#23433;&#20840;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#25551;&#36848;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07945v1 Announce Type: cross  Abstract: The rapid advancement in neurotechnology in recent years has created an emerging critical intersection between neurotechnology and security. Implantable devices, non-invasive monitoring, and non-invasive therapies all carry with them the prospect of violating the privacy and autonomy of individuals' cognition. A growing number of scientists and physicians have made calls to address this issue -- which we term Cognitive Security -- but applied efforts have been limited. A major barrier hampering scientific and engineering efforts to address Cognitive Security is the lack of a clear means of describing and analyzing relevant problems. In this paper we develop Cognitive Security, a mathematical framework which enables such description and analysis by drawing on methods and results from multiple fields. We demonstrate certain statistical properties which have significant implications for Cognitive Security, and then present descriptions of
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34507;&#30333;&#36136;&#30340;&#22522;&#22240;&#34920;&#31034;&#20316;&#20026;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#32467;&#26500;&#30456;&#20851;&#30340;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;Masked Gene Modeling&#65288;MGM&#65289;&#21644;Triple Enhanced Metagenomic Contrastive Learning&#65288;TEM-CL&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23439;&#22522;&#22240;&#32452;&#35821;&#35328;&#27169;&#22411;FGBERT&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22522;&#22240;&#24207;&#21015;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.16901</link><description>&lt;p&gt;
FGBERT&#65306;&#22522;&#20110;&#21151;&#33021;&#39537;&#21160;&#30340;&#23439;&#22522;&#22240;&#32452;&#39044;&#35757;&#32451;&#22522;&#22240;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FGBERT: Function-Driven Pre-trained Gene Language Model for Metagenomics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#34507;&#30333;&#36136;&#30340;&#22522;&#22240;&#34920;&#31034;&#20316;&#20026;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#32467;&#26500;&#30456;&#20851;&#30340;&#26631;&#35760;&#22120;&#65292;&#36890;&#36807;Masked Gene Modeling&#65288;MGM&#65289;&#21644;Triple Enhanced Metagenomic Contrastive Learning&#65288;TEM-CL&#65289;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#23439;&#22522;&#22240;&#32452;&#35821;&#35328;&#27169;&#22411;FGBERT&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#22522;&#22240;&#24207;&#21015;&#19982;&#21151;&#33021;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer representations, limiting the capture of structurally relevant gene contexts. To address these limitations and further our understanding of complex relationships between metagenomic sequences and their functions, we introduce a protein-based gene representation as a context-aware and structure-relevant tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene group-level pre-training, providing insights into inter-gene contextual information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for gene-level pre-training to model gene sequence-function relationships. MGM and TEM-CL constitute our novel metagenomic language model FGBERT, pre-trained on 100 million metagenomic sequences.
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16901v1 Announce Type: cross  Abstract: Metagenomic data, comprising mixed multi-species genomes, are prevalent in diverse environments like oceans and soils, significantly impacting human health and ecological functions. However, current research relies on K-mer representations, limiting the capture of structurally relevant gene contexts. To address these limitations and further our understanding of complex relationships between metagenomic sequences and their functions, we introduce a protein-based gene representation as a context-aware and structure-relevant tokenizer. Our approach includes Masked Gene Modeling (MGM) for gene group-level pre-training, providing insights into inter-gene contextual information, and Triple Enhanced Metagenomic Contrastive Learning (TEM-CL) for gene-level pre-training to model gene sequence-function relationships. MGM and TEM-CL constitute our novel metagenomic language model {\NAME}, pre-trained on 100 million metagenomic sequences. We demon
&lt;/p&gt;</description></item><item><title>AutoMMLab&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#22686;&#24378;AutoML&#31995;&#32479;&#65292;&#36890;&#36807;&#29992;&#25143;&#30340;&#35821;&#35328;&#25351;&#20196;&#26469;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#25972;&#20010;&#27169;&#22411;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#38750;&#19987;&#23478;&#20010;&#20307;&#26356;&#23481;&#26131;&#26500;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.15351</link><description>&lt;p&gt;
AutoMMLab&#65306;&#20174;&#35821;&#35328;&#25351;&#20196;&#33258;&#21160;&#29983;&#25104;&#21487;&#37096;&#32626;&#27169;&#22411;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15351
&lt;/p&gt;
&lt;p&gt;
AutoMMLab&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#22686;&#24378;AutoML&#31995;&#32479;&#65292;&#36890;&#36807;&#29992;&#25143;&#30340;&#35821;&#35328;&#25351;&#20196;&#26469;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#25972;&#20010;&#27169;&#22411;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#65292;&#20351;&#38750;&#19987;&#23478;&#20010;&#20307;&#26356;&#23481;&#26131;&#26500;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15351v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25552;&#35201;&#65306;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#26159;&#19968;&#32452;&#26088;&#22312;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#36807;&#31243;&#30340;&#25216;&#26415;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;AutoML&#26041;&#27861;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#27169;&#22411;&#24320;&#21457;&#30340;&#20960;&#20010;&#20851;&#38190;&#27493;&#39588;&#65288;&#20363;&#22914;&#36229;&#21442;&#25968;&#20248;&#21270;&#65289;&#65292;&#20294;&#32570;&#20047;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#21270;&#25972;&#20010;&#31471;&#21040;&#31471;&#27169;&#22411;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#30340;AutoML&#31995;&#32479;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoMMLab&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;LLM&#22686;&#24378;AutoML&#31995;&#32479;&#65292;&#25353;&#29031;&#29992;&#25143;&#30340;&#35821;&#35328;&#25351;&#20196;&#26469;&#33258;&#21160;&#21270;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#25972;&#20010;&#27169;&#22411;&#29983;&#25104;&#24037;&#20316;&#27969;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;AutoMMLab&#31995;&#32479;&#26377;&#25928;&#22320;&#21033;&#29992;LLM&#20316;&#20026;&#36830;&#25509;AutoML&#21644;OpenMMLab&#31038;&#21306;&#30340;&#26725;&#26753;&#65292;&#20351;&#38750;&#19987;&#23478;&#20010;&#20307;&#33021;&#22815;&#36890;&#36807;&#29992;&#25143;&#21451;&#22909;&#30340;&#35821;&#35328;&#30028;&#38754;&#36731;&#26494;&#26500;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;RU-LLaMA&#26469;&#29702;&#35299;&#29992;&#25143;&#30340;&#35831;&#27714;&#24182;&#23433;&#25490;&#25972;&#20010;&#27969;&#27700;&#32447;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#22120; c
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15351v1 Announce Type: new  Abstract: Automated machine learning (AutoML) is a collection of techniques designed to automate the machine learning development process. While traditional AutoML approaches have been successfully applied in several critical steps of model development (e.g. hyperparameter optimization), there lacks a AutoML system that automates the entire end-to-end model production workflow. To fill this blank, we present AutoMMLab, a general-purpose LLM-empowered AutoML system that follows user's language instructions to automate the whole model production workflow for computer vision tasks. The proposed AutoMMLab system effectively employs LLMs as the bridge to connect AutoML and OpenMMLab community, empowering non-expert individuals to easily build task-specific models via a user-friendly language interface. Specifically, we propose RU-LLaMA to understand users' request and schedule the whole pipeline, and propose a novel LLM-based hyperparameter optimizer c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#39046;&#22495;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#25955;&#24674;&#22797;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#39640;&#32423;&#21644;&#20302;&#32423;&#35270;&#35273;&#29305;&#24449;&#30340;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.14401</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#35270;&#35273;&#34917;&#20607;&#24341;&#23548;&#21644;&#35270;&#35273;&#24046;&#24322;&#20998;&#26512;&#29992;&#20110;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Diffusion Model Based Visual Compensation Guidance and Visual Difference Analysis for No-Reference Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25193;&#25955;&#27169;&#22411;&#24341;&#20837;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;&#39046;&#22495;&#65292;&#35774;&#35745;&#20102;&#26032;&#30340;&#25193;&#25955;&#24674;&#22797;&#32593;&#32476;&#65292;&#25552;&#39640;&#20102;&#23398;&#20064;&#39640;&#32423;&#21644;&#20302;&#32423;&#35270;&#35273;&#29305;&#24449;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#33258;&#30001;&#33021;&#24341;&#23548;&#30340;&#26080;&#21442;&#32771;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;(NR-IQA)&#26041;&#27861;&#20173;&#28982;&#22312;&#25214;&#21040;&#22312;&#22270;&#20687;&#30340;&#20687;&#32032;&#32423;&#23398;&#20064;&#29305;&#24449;&#20449;&#24687;&#21644;&#25429;&#33719;&#39640;&#32423;&#29305;&#24449;&#20449;&#24687;&#20043;&#38388;&#36798;&#21040;&#24179;&#34913;&#20197;&#21450;&#39640;&#32423;&#29305;&#24449;&#20449;&#24687;&#30340;&#26377;&#25928;&#21033;&#29992;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#20808;&#25216;&#26415;(SOTA)&#29983;&#25104;&#27169;&#22411;&#31867;&#21035;&#65292;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#24314;&#27169;&#22797;&#26434;&#20851;&#31995;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#20840;&#38754;&#29702;&#35299;&#22270;&#20687;&#65292;&#24182;&#20855;&#26377;&#26356;&#22909;&#22320;&#23398;&#20064;&#39640;&#32423;&#21644;&#20302;&#32423;&#35270;&#35273;&#29305;&#24449;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23558;&#25193;&#25955;&#27169;&#22411;&#25506;&#32034;&#21040;NR-IQA&#39046;&#22495;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#25193;&#25955;&#24674;&#22797;&#32593;&#32476;&#65292;&#21033;&#29992;&#29983;&#25104;&#30340;&#22686;&#24378;&#22270;&#20687;&#21644;&#21253;&#21547;&#22122;&#22768;&#30340;&#22270;&#20687;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#21435;&#22122;&#36807;&#31243;&#20013;&#33719;&#24471;&#30340;&#38750;&#32447;&#24615;&#29305;&#24449;&#20316;&#20026;&#39640;&#32423;&#35270;&#35273;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14401v1 Announce Type: cross  Abstract: Existing free-energy guided No-Reference Image Quality Assessment (NR-IQA) methods still suffer from finding a balance between learning feature information at the pixel level of the image and capturing high-level feature information and the efficient utilization of the obtained high-level feature information remains a challenge. As a novel class of state-of-the-art (SOTA) generative model, the diffusion model exhibits the capability to model intricate relationships, enabling a comprehensive understanding of images and possessing a better learning of both high-level and low-level visual features. In view of these, we pioneer the exploration of the diffusion model into the domain of NR-IQA. Firstly, we devise a new diffusion restoration network that leverages the produced enhanced image and noise-containing images, incorporating nonlinear features obtained during the denoising process of the diffusion model, as high-level visual informat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.08925</link><description>&lt;p&gt;
MaxMin-RLHF:&#38754;&#21521;&#20855;&#26377;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20844;&#24179;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
MaxMin-RLHF: Towards Equitable Alignment of Large Language Models with Diverse Human Preferences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08925
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#23545;&#40784;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#24182;&#20351;&#29992;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#20154;&#31867;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;(RLHF)&#36890;&#36807;&#20351;&#29992;&#20174;&#20559;&#22909;&#25968;&#25454;&#20013;&#27966;&#29983;&#30340;&#21333;&#19968;&#22870;&#21169;&#27169;&#22411;&#26469;&#23545;&#40784;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#20174;&#22810;&#20010;&#29992;&#25143;&#25910;&#38598;&#30340;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#30340;&#20016;&#23500;&#22810;&#26679;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20986;&#20102;&#20351;&#29992;&#21333;&#19968;&#22870;&#21169;RLHF&#36827;&#34892;&#23545;&#40784;&#30340;&#19981;&#21487;&#33021;&#24615;&#32467;&#26524;&#65292;&#20174;&#32780;&#20984;&#26174;&#20102;&#20854;&#26080;&#27861;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#20026;&#20102;&#25552;&#20379;&#19968;&#20010;&#20844;&#24179;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#36890;&#36807;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#23398;&#20064;&#20102;&#19968;&#31181;&#28151;&#21512;&#20559;&#22909;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20013;&#30340;&#24179;&#31561;&#21407;&#21017;&#21551;&#21457;&#30340;MaxMin&#23545;&#40784;&#30446;&#26631;&#26469;&#26356;&#22909;&#22320;&#34920;&#31034;&#22810;&#26679;&#30340;&#20154;&#31867;&#20559;&#22909;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#19982;&#20998;&#24067;&#31283;&#20581;&#20248;&#21270;&#21644;&#36890;&#29992;&#25928;&#29992;RL&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#31361;&#26174;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08925v1 Announce Type: cross Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns language models to human preferences by employing a singular reward model derived from preference data. However, such an approach overlooks the rich diversity of human preferences inherent in data collected from multiple users. In this work, we first derive an impossibility result of alignment with single reward RLHF, thereby highlighting its insufficiency in representing diverse human preferences. To provide an equitable solution to the problem, we learn a mixture of preference distributions via an expectation-maximization algorithm and propose a MaxMin alignment objective for policy learning inspired by the Egalitarian principle in social choice theory to better represent diverse human preferences. We elucidate the connection of our proposed approach to distributionally robust optimization and general utility RL, thereby highlighting the generality and robustness of our proposed
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#22270;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#27169;&#22359;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37051;&#25509;&#20449;&#24687;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04046</link><description>&lt;p&gt;
&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#30340;&#32852;&#21512;&#25193;&#25955;&#65292;&#23454;&#29616;&#22270;&#24418;&#30340;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling of Graphs via Joint Diffusion of Node and Edge Attributes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04046
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32852;&#21512;&#25193;&#25955;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#24418;&#29983;&#25104;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#22270;&#32452;&#20214;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#27169;&#22359;&#21644;&#30456;&#20114;&#20381;&#36182;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#37051;&#25509;&#20449;&#24687;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#29983;&#25104;&#26159;&#21508;&#31181;&#24037;&#31243;&#21644;&#31185;&#23398;&#23398;&#31185;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#24573;&#35270;&#20102;&#36793;&#23646;&#24615;&#30340;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#20851;&#38190;&#24212;&#29992;&#20013;&#36793;&#23646;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#36825;&#20351;&#24471;&#20808;&#21069;&#30340;&#26041;&#27861;&#22312;&#36825;&#20123;&#24773;&#22659;&#20013;&#21487;&#33021;&#19981;&#36866;&#29992;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#23384;&#22312;&#19968;&#20123;&#31616;&#21333;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#20294;&#32463;&#39564;&#35843;&#26597;&#26174;&#31034;&#23427;&#20204;&#30340;&#25928;&#26524;&#26377;&#38480;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#24456;&#22909;&#22320;&#27169;&#25311;&#22270;&#32452;&#20214;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33410;&#28857;&#21644;&#36793;&#30340;&#32852;&#21512;&#35780;&#20998;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#24418;&#29983;&#25104;&#65292;&#32771;&#34385;&#20102;&#25152;&#26377;&#22270;&#32452;&#20214;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#21019;&#26032;&#28857;&#65306;(i) &#23558;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#32467;&#21512;&#22312;&#19968;&#20010;&#27880;&#24847;&#27169;&#22359;&#20013;&#65292;&#22522;&#20110;&#36825;&#20004;&#20010;&#22240;&#32032;&#29983;&#25104;&#26679;&#26412;&#65307;(ii) &#22312;&#22270;&#24418;&#25193;&#25955;&#36807;&#31243;&#20013;&#65292;&#33410;&#28857;&#12289;&#36793;&#21644;&#37051;&#25509;&#20449;&#24687;&#30456;&#20114;&#20381;&#36182;&#12290;&#25105;&#20204;&#22312;&#28041;&#21450;&#23454;&#38469;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#21547;&#36793;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation is integral to various engineering and scientific disciplines. Nevertheless, existing methodologies tend to overlook the generation of edge attributes. However, we identify critical applications where edge attributes are essential, making prior methods potentially unsuitable in such contexts. Moreover, while trivial adaptations are available, empirical investigations reveal their limited efficacy as they do not properly model the interplay among graph components. To address this, we propose a joint score-based model of nodes and edges for graph generation that considers all graph components. Our approach offers two key novelties: (i) node and edge attributes are combined in an attention module that generates samples based on the two ingredients; and (ii) node, edge and adjacency information are mutually dependent during the graph diffusion process. We evaluate our method on challenging benchmarks involving real-world and synthetic datasets in which edge features are cr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.02352</link><description>&lt;p&gt;
&#36870;&#21521;&#23398;&#20064;&#65306;&#36890;&#36807;&#25441;&#21462;&#23398;&#20064;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Working Backwards: Learning to Place by Picking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#65288;PvP&#65289;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#20027;&#25910;&#38598;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#25918;&#32622;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#28436;&#31034;&#65292;&#20854;&#20013;&#29289;&#20307;&#24517;&#39035;&#34987;&#25805;&#32437;&#21040;&#29305;&#23450;&#30340;&#25509;&#35302;&#38480;&#21046;&#20301;&#32622;&#12290;&#36890;&#36807;PvP&#65292;&#25105;&#20204;&#36890;&#36807;&#39072;&#20498;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#65292;&#25509;&#36817;&#20110;&#26426;&#22120;&#20154;&#29289;&#20307;&#25918;&#32622;&#28436;&#31034;&#30340;&#25910;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#19968;&#32452;&#26368;&#21021;&#20301;&#20110;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#30340;&#29289;&#20307;&#30340;&#25235;&#21462;&#24207;&#21015;&#20013;&#33719;&#24471;&#25918;&#32622;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#30334;&#20010;&#28436;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#65292;&#36825;&#26159;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#27169;&#22359;&#23454;&#29616;&#30340;&#65306;&#35302;&#35273;&#37325;&#26032;&#25235;&#21462;&#21644;&#29992;&#20110;&#25235;&#21462;&#30340;&#39034;&#20174;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#30452;&#25509;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36890;&#36807;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35757;&#32451;&#31574;&#30053;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#31574;&#30053;&#21487;&#20197;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#29615;&#22659;&#33539;&#22260;&#30340;&#29289;&#20307;&#25918;&#32622;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02352v2 Announce Type: replace-cross  Abstract: We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention by combining two modules: tactile regrasping and compliant control for grasps. We train a policy directly from visual observations through behavioral cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the tra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2305.13168</link><description>&lt;p&gt;
LLMs&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#65306;&#26368;&#26032;&#21151;&#33021;&#19982;&#26410;&#26469;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
LLMs for Knowledge Graph Construction and Reasoning: Recent Capabilities and Future Opportunities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.13168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20840;&#38754;&#35780;&#20272;&#20102;LLMs&#22312;&#30693;&#35782;&#22270;&#35889;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;GPT-4&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#26500;&#24314;&#21644;&#25512;&#29702;&#20013;&#30340;&#25968;&#37327;&#21270;&#21644;&#36136;&#21270;&#35780;&#20272;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#20843;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37325;&#28857;&#20851;&#27880;&#28085;&#30422;&#23454;&#20307;&#21644;&#20851;&#31995;&#25552;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#12289;&#38142;&#25509;&#39044;&#27979;&#21644;&#38382;&#31572;&#22235;&#20010;&#20856;&#22411;&#20219;&#21153;&#65292;&#20174;&#32780;&#20840;&#38754;&#25506;&#32034;&#20102;LLMs&#22312;&#26500;&#24314;&#21644;&#25512;&#29702;&#39046;&#22495;&#30340;&#34920;&#29616;&#12290;&#32463;&#39564;&#24615;&#30740;&#31350;&#21457;&#29616;&#65292;&#20197;GPT-4&#20026;&#20195;&#34920;&#30340;LLMs&#26356;&#36866;&#21512;&#20316;&#20026;&#25512;&#29702;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#34429;&#28982;GPT-4&#22312;&#19982;KG&#26500;&#24314;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#20986;&#33394;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#31934;&#35843;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35843;&#26597;&#36824;&#25193;&#23637;&#21040;LLMs&#22312;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#30340;&#28508;&#22312;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#34394;&#25311;&#30693;&#35782;&#25552;&#21462;&#30340;&#26500;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.13168v2 Announce Type: replace-cross  Abstract: This paper presents an exhaustive quantitative and qualitative evaluation of Large Language Models (LLMs) for Knowledge Graph (KG) construction and reasoning. We engage in experiments across eight diverse datasets, focusing on four representative tasks encompassing entity and relation extraction, event extraction, link prediction, and question-answering, thereby thoroughly exploring LLMs' performance in the domain of construction and inference. Empirically, our findings suggest that LLMs, represented by GPT-4, are more suited as inference assistants rather than few-shot information extractors. Specifically, while GPT-4 exhibits good performance in tasks related to KG construction, it excels further in reasoning tasks, surpassing fine-tuned models in certain cases. Moreover, our investigation extends to the potential generalization ability of LLMs for information extraction, leading to the proposition of a Virtual Knowledge Extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2401.11174</link><description>&lt;p&gt;
&#20687;&#32032;&#32423;&#21035;&#35782;&#21035;&#29992;&#20110;&#25972;&#20307;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Pixel-Wise Recognition for Holistic Surgical Scene Understanding. (arXiv:2401.11174v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#19968;&#20010;&#22522;&#20110;&#21464;&#24418;&#22120;&#30340;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26377;&#25928;&#22320;&#32467;&#21512;&#20102;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#21644;&#23616;&#37096;&#22120;&#26800;&#20998;&#21106;&#65292;&#21487;&#29992;&#20110;&#22810;&#23618;&#27425;&#29702;&#35299;&#22806;&#31185;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Prostatectomies&#30340;&#25972;&#20307;&#21644;&#22810;&#31890;&#24230;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#65288;GraSP&#65289;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#23545;&#22806;&#31185;&#22330;&#26223;&#29702;&#35299;&#36827;&#34892;&#20102;&#23618;&#27425;&#21270;&#24314;&#27169;&#65292;&#21253;&#25324;&#19981;&#21516;&#31890;&#24230;&#30340;&#20114;&#34917;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#22806;&#31185;&#27963;&#21160;&#30340;&#22810;&#23618;&#27425;&#29702;&#35299;&#65292;&#21253;&#25324;&#22806;&#31185;&#38454;&#27573;&#21644;&#27493;&#39588;&#30340;&#35782;&#21035;&#20197;&#21450;&#21253;&#25324;&#22806;&#31185;&#22120;&#26800;&#20998;&#21106;&#21644;&#21407;&#23376;&#21487;&#35270;&#21160;&#20316;&#26816;&#27979;&#22312;&#20869;&#30340;&#30701;&#26399;&#20219;&#21153;&#12290;&#20026;&#20102;&#21033;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#21464;&#24418;&#22120;&#65288;Transformers&#65289;&#30340;&#34892;&#21160;&#12289;&#38454;&#27573;&#12289;&#27493;&#39588;&#21644;&#22120;&#26800;&#20998;&#21106;&#65288;TAPIS&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#20840;&#23616;&#35270;&#39057;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#26469;&#33258;&#22120;&#26800;&#20998;&#21106;&#27169;&#22411;&#30340;&#23616;&#37096;&#21306;&#22495;&#24314;&#35758;&#30456;&#32467;&#21512;&#65292;&#20197;&#24212;&#23545;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#22810;&#31890;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#30701;&#26399;&#35782;&#21035;&#20219;&#21153;&#20013;&#21253;&#25324;&#20998;&#21106;&#27880;&#37322;&#30340;&#24433;&#21709;&#65292;&#24182;&#31361;&#26174;&#20102;&#19981;&#21516;&#30340;&#31890;&#24230;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the Holistic and Multi-Granular Surgical Scene Understanding of Prostatectomies (GraSP) dataset, a curated benchmark that models surgical scene understanding as a hierarchy of complementary tasks with varying levels of granularity. Our approach enables a multi-level comprehension of surgical activities, encompassing long-term tasks such as surgical phases and steps recognition and short-term tasks including surgical instrument segmentation and atomic visual actions detection. To exploit our proposed benchmark, we introduce the Transformers for Actions, Phases, Steps, and Instrument Segmentation (TAPIS) model, a general architecture that combines a global video feature extractor with localized region proposals from an instrument segmentation model to tackle the multi-granularity of our benchmark. Through extensive experimentation, we demonstrate the impact of including segmentation annotations in short-term recognition tasks, highlight the varying granularity require
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#33258;&#36866;&#24212;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#20013;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#35843;&#25972;&#19982;&#22122;&#22768;&#39044;&#27979;&#38590;&#24230;&#30456;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.13307</link><description>&lt;p&gt;
&#24182;&#38750;&#25152;&#26377;&#27493;&#39588;&#37117;&#30456;&#31561;&#65306;&#36827;&#23637;&#25193;&#25955;&#27169;&#22411;&#30340;&#39640;&#25928;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Not All Steps are Equal: Efficient Generation with Progressive Diffusion Models. (arXiv:2312.13307v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.13307
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#27493;&#39588;&#33258;&#36866;&#24212;&#35757;&#32451;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#25193;&#25955;&#27169;&#22411;&#20013;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#20914;&#31361;&#38382;&#39064;&#65292;&#23558;&#27169;&#22411;&#22823;&#23567;&#35843;&#25972;&#19982;&#22122;&#22768;&#39044;&#27979;&#38590;&#24230;&#30456;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#33394;&#30340;&#25928;&#33021;&#65292;&#20855;&#26377;&#21435;&#22122;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#30446;&#21069;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#25152;&#26377;&#26102;&#38388;&#27493;&#19978;&#37117;&#37319;&#29992;&#32479;&#19968;&#30340;&#21435;&#22122;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#22122;&#22768;&#28508;&#22312;&#21464;&#21270;&#23548;&#33268;&#20102;&#35757;&#32451;&#20013;&#30340;&#20914;&#31361;&#65292;&#38480;&#21046;&#20102;&#25193;&#25955;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#31216;&#20026;&#27493;&#39588;&#33258;&#36866;&#24212;&#35757;&#32451;&#12290;&#22312;&#21021;&#22987;&#38454;&#27573;&#65292;&#35757;&#32451;&#19968;&#20010;&#22522;&#30784;&#30340;&#21435;&#22122;&#27169;&#22411;&#26469;&#21253;&#25324;&#25152;&#26377;&#30340;&#26102;&#38388;&#27493;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#27493;&#20998;&#20026;&#19981;&#21516;&#30340;&#32452;&#65292;&#23545;&#27599;&#20010;&#32452;&#20869;&#30340;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#36798;&#21040;&#19987;&#38376;&#30340;&#21435;&#22122;&#33021;&#21147;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#65292;&#19981;&#21516;&#26102;&#38388;&#27493;&#30340;&#22122;&#22768;&#39044;&#27979;&#22256;&#38590;&#31243;&#24230;&#26159;&#19981;&#21516;&#30340;&#65292;&#25152;&#20197;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#26679;&#30340;&#27169;&#22411;&#22823;&#23567;&#35201;&#27714;&#12290;&#25105;&#20204;&#36890;&#36807;&#20272;&#35745;&#27599;&#20010;&#26102;&#38388;&#27493;&#30340;&#20449;&#22122;&#27604;&#26469;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#22823;&#23567;&#65292;&#20197;&#36827;&#34892;&#24494;&#35843;&#20043;&#21069;&#12290;&#27492;&#35843;&#25972;&#31616;&#21270;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#27969;&#31243;&#24182;&#25552;&#39640;&#20102;&#29983;&#25104;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have demonstrated remarkable efficacy in various generative tasks with the predictive prowess of denoising model. Currently, these models employ a uniform denoising approach across all timesteps. However, the inherent variations in noisy latents at each timestep lead to conflicts during training, constraining the potential of diffusion models. To address this challenge, we propose a novel two-stage training strategy termed Step-Adaptive Training. In the initial stage, a base denoising model is trained to encompass all timesteps. Subsequently, we partition the timesteps into distinct groups, fine-tuning the model within each group to achieve specialized denoising capabilities. Recognizing that the difficulties of predicting noise at different timesteps vary, we introduce a diverse model size requirement. We dynamically adjust the model size for each timestep by estimating task difficulty based on its signal-to-noise ratio before fine-tuning. This adjustment is facilitat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;PRGD&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#20998;&#31867;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#26102;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#24555;&#36895;&#36793;&#30028;&#26368;&#22823;&#21270;&#65292;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2311.14387</link><description>&lt;p&gt;
&#36890;&#36807;&#28176;&#36827;&#33539;&#25968;&#37325;&#26032;&#32553;&#25918;&#23454;&#29616;&#25351;&#25968;&#32423;&#24555;&#36895;&#36793;&#30028;&#26368;&#22823;&#21270;
&lt;/p&gt;
&lt;p&gt;
Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling. (arXiv:2311.14387v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14387
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;PRGD&#31639;&#27861;&#65292;&#25105;&#20204;&#22312;&#20998;&#31867;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#26102;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#24555;&#36895;&#36793;&#30028;&#26368;&#22823;&#21270;&#65292;&#19982;&#29616;&#26377;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#22312;&#20998;&#31867;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#26102;&#34920;&#29616;&#20986;&#30340;&#36793;&#30028;&#26368;&#22823;&#21270;&#20559;&#24046;&#12290;&#25105;&#20204;&#23545;&#19982;&#65288;&#24402;&#19968;&#21270;&#30340;&#65289;&#26799;&#24230;&#30456;&#20851;&#30340;&#36895;&#24230;&#22330;&#30340;&#29305;&#24615;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#22312;&#36793;&#30028;&#26368;&#22823;&#21270;&#20013;&#30340;&#20316;&#29992;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28176;&#36827;&#37325;&#26032;&#32553;&#25918;&#26799;&#24230;&#19979;&#38477;&#65288;PRGD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;PRGD&#21487;&#20197;&#20197;&#25351;&#25968;&#32423;&#24555;&#36895;&#22686;&#22823;&#36793;&#30028;&#12290;&#36825;&#19982;&#30446;&#21069;&#25152;&#26377;&#29616;&#26377;&#31639;&#27861;&#24418;&#25104;&#20102;&#40092;&#26126;&#23545;&#27604;&#65292;&#21518;&#32773;&#20197;&#32531;&#24930;&#30340;&#22810;&#39033;&#24335;&#36895;&#29575;&#26368;&#22823;&#21270;&#36793;&#30028;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#25968;&#25454;&#20998;&#24067;&#30340;&#28201;&#21644;&#26465;&#20214;&#65292;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#20687;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#21644;&#24402;&#19968;&#21270;&#26799;&#24230;&#19979;&#38477;&#65288;NGD&#65289;&#36825;&#26679;&#30340;&#29616;&#26377;&#31639;&#27861;&#22312;&#39640;&#25928;&#26368;&#22823;&#21270;&#36793;&#30028;&#26102;&#20250;&#20986;&#29616;&#22833;&#36133;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PRGD&#22312;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the margin-maximization bias exhibited by gradient-based algorithms in classifying linearly separable data. We present an in-depth analysis of the specific properties of the velocity field associated with (normalized) gradients, focusing on their role in margin maximization. Inspired by this analysis, we propose a novel algorithm called Progressive Rescaling Gradient Descent (PRGD) and show that PRGD can maximize the margin at an {\em exponential rate}. This stands in stark contrast to all existing algorithms, which maximize the margin at a slow {\em polynomial rate}. Specifically, we identify mild conditions on data distribution under which existing algorithms such as gradient descent (GD) and normalized gradient descent (NGD) {\em provably fail} in maximizing the margin efficiently. To validate our theoretical findings, we present both synthetic and real-world experiments. Notably, PRGD also shows promise in enhancing the generalization performance when a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#20132;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.12487</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#20132;&#27880;&#24847;&#21147;&#25552;&#21319;&#36816;&#31639;&#31526;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improved Operator Learning by Orthogonal Attention. (arXiv:2310.12487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#20132;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#65292;&#36890;&#36807;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36816;&#31639;&#31526;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#20195;&#29702;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#65292;&#21463;&#21040;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#36816;&#31639;&#31526;&#24050;&#25104;&#20026;&#30456;&#20851;&#30740;&#31350;&#30340;&#20027;&#27969;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#24847;&#26426;&#21046;&#20013;&#21442;&#25968;&#25968;&#37327;&#24040;&#22823;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19978;&#36807;&#25311;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#26680;&#31215;&#20998;&#31639;&#23376;&#30340;&#29305;&#24449;&#20998;&#35299;&#21644;&#31070;&#32463;&#36817;&#20284;&#30340;&#29305;&#24449;&#20989;&#25968;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#27491;&#20132;&#27880;&#24847;&#21147;&#12290;&#27491;&#20132;&#21270;&#33258;&#28982;&#22320;&#23545;&#32467;&#26524;&#31070;&#32463;&#36816;&#31639;&#31526;&#26045;&#21152;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#25928;&#26524;&#65292;&#26377;&#21161;&#20110;&#25269;&#25239;&#36807;&#25311;&#21512;&#21644;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#21253;&#25324;&#27491;&#24120;&#21644;&#38750;&#27491;&#24120;&#20960;&#20309;&#24418;&#29366;&#30340;&#20845;&#20010;&#26631;&#20934;&#31070;&#32463;&#36816;&#31639;&#31526;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#32988;&#36807;&#31454;&#20105;&#23545;&#25163;&#65292;&#24182;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural operators, as an efficient surrogate model for learning the solutions of PDEs, have received extensive attention in the field of scientific machine learning. Among them, attention-based neural operators have become one of the mainstreams in related research. However, existing approaches overfit the limited training data due to the considerable number of parameters in the attention mechanism. To address this, we develop an orthogonal attention based on the eigendecomposition of the kernel integral operator and the neural approximation of eigenfunctions. The orthogonalization naturally poses a proper regularization effect on the resulting neural operator, which aids in resisting overfitting and boosting generalization. Experiments on six standard neural operator benchmark datasets comprising both regular and irregular geometries show that our method can outperform competing baselines with decent margins.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#31283;&#23450;&#32593;&#32476;(EStable-Net)&#29992;&#20110;&#35299;&#20915;&#26799;&#24230;&#27969;&#26041;&#31243;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#38477;&#20302;&#31163;&#25955;&#33021;&#37327;&#24182;&#29983;&#25104;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2309.10002</link><description>&lt;p&gt;
&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#33021;&#37327;&#31283;&#23450;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Energy stable neural network for gradient flow equations. (arXiv:2309.10002v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33021;&#37327;&#31283;&#23450;&#32593;&#32476;(EStable-Net)&#29992;&#20110;&#35299;&#20915;&#26799;&#24230;&#27969;&#26041;&#31243;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#38477;&#20302;&#31163;&#25955;&#33021;&#37327;&#24182;&#29983;&#25104;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27714;&#35299;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#33021;&#37327;&#31283;&#23450;&#32593;&#32476;&#65288;EStable-Net&#65289;&#12290;&#25105;&#20204;&#30340;&#31070;&#32463;&#32593;&#32476;EStable-Net&#30340;&#35299;&#26356;&#26032;&#26041;&#26696;&#21463;&#21040;&#20102;&#26799;&#24230;&#27969;&#26041;&#31243;&#22522;&#20110;&#36741;&#21161;&#21464;&#37327;&#30340;&#31561;&#20215;&#24418;&#24335;&#30340;&#21551;&#21457;&#12290;EStable-Net&#33021;&#22815;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#38477;&#20302;&#31163;&#25955;&#33021;&#37327;&#65292;&#19982;&#26799;&#24230;&#27969;&#26041;&#31243;&#30340;&#28436;&#21270;&#36807;&#31243;&#30340;&#24615;&#36136;&#20445;&#25345;&#19968;&#33268;&#12290;&#31070;&#32463;&#32593;&#32476;EStable-Net&#30340;&#26550;&#26500;&#21253;&#25324;&#20960;&#20010;&#33021;&#37327;&#34928;&#20943;&#27169;&#22359;&#65292;&#27599;&#20010;&#27169;&#22359;&#30340;&#36755;&#20986;&#21487;&#20197;&#35299;&#37322;&#20026;&#26799;&#24230;&#27969;&#26041;&#31243;&#28436;&#21270;&#36807;&#31243;&#30340;&#20013;&#38388;&#29366;&#24577;&#12290;&#36825;&#31181;&#35774;&#35745;&#25552;&#20379;&#20102;&#19968;&#20010;&#31283;&#23450;&#12289;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#32593;&#32476;&#32467;&#26500;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#32593;&#32476;&#33021;&#22815;&#29983;&#25104;&#39640;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose an energy stable network (EStable-Net) for solving gradient flow equations. The solution update scheme in our neural network EStable-Net is inspired by a proposed auxiliary variable based equivalent form of the gradient flow equation. EStable-Net enables decreasing of a discrete energy along the neural network, which is consistent with the property in the evolution process of the gradient flow equation. The architecture of the neural network EStable-Net consists of a few energy decay blocks, and the output of each block can be interpreted as an intermediate state of the evolution process of the gradient flow equation. This design provides a stable, efficient and interpretable network structure. Numerical experimental results demonstrate that our network is able to generate high accuracy and stable predictions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;</title><link>http://arxiv.org/abs/2308.12143</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#29983;&#25104;&#27169;&#22411;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Fluctuation based Membership Inference Attack for Generative Models. (arXiv:2308.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#29983;&#25104;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27874;&#21160;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#27874;&#21160;&#24615;&#26469;&#25512;&#26029;&#27169;&#22411;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35757;&#32451;&#35760;&#24405;&#30340;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIA)&#36890;&#36807;&#26597;&#35810;&#27169;&#22411;&#26469;&#35782;&#21035;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#26576;&#26465;&#35760;&#24405;&#12290;&#23545;&#32463;&#20856;&#20998;&#31867;&#27169;&#22411;&#30340;MIA&#24050;&#26377;&#24456;&#22810;&#30740;&#31350;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#24320;&#22987;&#25506;&#32034;&#22914;&#20309;&#23558;MIA&#24212;&#29992;&#21040;&#29983;&#25104;&#27169;&#22411;&#19978;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#26377;&#30340;&#38754;&#21521;&#29983;&#25104;&#27169;&#22411;&#30340;MIA&#20027;&#35201;&#20381;&#36182;&#20110;&#30446;&#26631;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#36807;&#25311;&#21512;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#36991;&#20813;&#65292;&#32780;&#29616;&#26377;&#30340;MIA&#22312;&#23454;&#36341;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#19982;&#36807;&#25311;&#21512;&#19981;&#21516;&#65292;&#35760;&#24518;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#26368;&#20339;&#24615;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26356;&#20026;&#26222;&#36941;&#30340;&#29616;&#35937;&#12290;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#35760;&#24518;&#23548;&#33268;&#29983;&#25104;&#35760;&#24405;&#30340;&#27010;&#29575;&#20998;&#24067;&#21576;&#29616;&#20986;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#27874;&#21160;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#26041;&#27861;(PFAMI)&#65292;&#23427;&#26159;&#19968;&#31181;&#40657;&#30418;MIA&#65292;&#36890;&#36807;&#26816;&#27979;&#27010;&#29575;&#27874;&#21160;&#26469;&#25512;&#26029;&#25104;&#21592;&#36523;&#20221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attack (MIA) identifies whether a record exists in a machine learning model's training set by querying the model. MIAs on the classic classification models have been well-studied, and recent works have started to explore how to transplant MIA onto generative models. Our investigation indicates that existing MIAs designed for generative models mainly depend on the overfitting in target models. However, overfitting can be avoided by employing various regularization techniques, whereas existing MIAs demonstrate poor performance in practice. Unlike overfitting, memorization is essential for deep learning models to attain optimal performance, making it a more prevalent phenomenon. Memorization in generative models leads to an increasing trend in the probability distribution of generating records around the member record. Therefore, we propose a Probabilistic Fluctuation Assessing Membership Inference Attack (PFAMI), a black-box MIA that infers memberships by detecting t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21512;&#29702;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2308.10462</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models. (arXiv:2308.10462v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#19979;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#65292;&#24182;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21512;&#29702;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#19987;&#38376;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#29305;&#23450;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#21363;&#21487;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#24847;&#22270;&#29983;&#25104;&#20934;&#30830;&#30340;&#20195;&#30721;&#29255;&#27573;&#30340;&#21360;&#35937;&#33021;&#21147;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#31361;&#20986;&#20102;&#24494;&#35843;LLMs&#30340;&#20248;&#21183;&#65292;&#20294;&#36825;&#20010;&#36807;&#31243;&#20195;&#20215;&#39640;&#65292;&#23545;&#20110;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22312;&#36164;&#28304;&#31232;&#32570;&#30340;&#29615;&#22659;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#20316;&#20026;&#19968;&#31181;&#31574;&#30053;&#65292;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#25552;&#31034;&#31034;&#20363;&#25351;&#23548;LLM&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;ICL&#24341;&#20837;&#20102;&#19968;&#20123;&#19981;&#20415;&#20043;&#22788;&#65292;&#27604;&#22914;&#38656;&#35201;&#35774;&#35745;&#19978;&#19979;&#25991;&#30456;&#20851;&#30340;&#25552;&#31034;&#21644;&#27809;&#26377;&#23398;&#20064;&#20219;&#21153;&#29305;&#23450;&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#39044;&#35265;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21512;&#29702;&#36164;&#28304;&#28040;&#32791;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23558;LLM&#19987;&#38376;&#29992;&#20110;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) demonstrate impressive capabilities to generate accurate code snippets given natural language intents in zero-shot, i.e., without the need for specific fine-tuning. While prior studies have highlighted the advantages of fine-tuning LLMs, this process incurs high computational costs, making it impractical in resource-scarce environments, particularly for models with billions of parameters. To address these challenges, previous research explored In-Context Learning (ICL) as a strategy to guide the LLM generative process with task-specific prompt examples. However, ICL introduces inconveniences, such as the need for designing contextually relevant prompts and the absence of learning task-specific parameters, thereby limiting downstream task performance. In this context, we foresee Parameter-Efficient Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize LLMs to task-specific data while maintaining reasonable resource consumption. In t
&lt;/p&gt;</description></item><item><title>U-Turn&#25193;&#25955;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;U-Turn Diffusion&#25216;&#26415;&#26469;&#25913;&#36827;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#25216;&#26415;&#32467;&#21512;&#20102;&#21069;&#21521;&#12289;U-Turn&#21644;&#21453;&#21521;&#36807;&#31243;&#65292;&#36890;&#36807;&#35299;&#26500;&#24555;&#36895;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#29983;&#25104;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.07421</link><description>&lt;p&gt;
U-Turn&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
U-Turn Diffusion. (arXiv:2308.07421v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07421
&lt;/p&gt;
&lt;p&gt;
U-Turn&#25193;&#25955;&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;AI&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;U-Turn Diffusion&#25216;&#26415;&#26469;&#25913;&#36827;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#36825;&#31181;&#25216;&#26415;&#32467;&#21512;&#20102;&#21069;&#21521;&#12289;U-Turn&#21644;&#21453;&#21521;&#36807;&#31243;&#65292;&#36890;&#36807;&#35299;&#26500;&#24555;&#36895;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#29983;&#25104;&#36807;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#22270;&#20687;&#30340;AI&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#30001;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#39537;&#21160;&#30340;&#21160;&#24577;&#36741;&#21161;&#26102;&#38388;&#26426;&#21046;&#65292;&#22312;&#36755;&#20837;&#22270;&#20687;&#20013;&#33719;&#21462;&#20998;&#25968;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#35780;&#20272;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#25928;&#29575;&#30340;&#26631;&#20934;&#65306;&#29983;&#25104;&#36807;&#31243;&#30340;&#33021;&#21147;&#21462;&#20915;&#20110;&#22312;&#21453;&#21521;/&#21435;&#22122;&#38454;&#27573;&#35299;&#26500;&#24555;&#36895;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#30340;&#21512;&#25104;&#22270;&#20687;&#36136;&#37327;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;&#8220;U-Turn Diffusion&#8221;&#30340;&#26041;&#27861;&#12290;U-Turn Diffusion&#25216;&#26415;&#20174;&#26631;&#20934;&#30340;&#21069;&#21521;&#25193;&#25955;&#36807;&#31243;&#24320;&#22987;&#65292;&#23613;&#31649;&#30456;&#23545;&#20110;&#20256;&#32479;&#35774;&#32622;&#65292;&#23427;&#30340;&#25345;&#32493;&#26102;&#38388;&#26356;&#30701;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25191;&#34892;&#26631;&#20934;&#30340;&#21453;&#21521;&#21160;&#21147;&#23398;&#65292;&#20197;&#21069;&#21521;&#36807;&#31243;&#30340;&#26368;&#32456;&#37197;&#32622;&#20026;&#21021;&#22987;&#20540;&#12290;&#36825;&#31181;&#32467;&#21512;&#20102;&#21069;&#21521;&#12289;U-Turn&#21644;&#21453;&#21521;&#36807;&#31243;&#30340;U-Turn Diffusion&#36807;&#31243;&#21019;&#24314;&#19968;&#20010;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive examination of score-based diffusion models of AI for generating synthetic images. These models hinge upon a dynamic auxiliary time mechanism driven by stochastic differential equations, wherein the score function is acquired from input images. Our investigation unveils a criterion for evaluating efficiency of the score-based diffusion models: the power of the generative process depends on the ability to de-construct fast correlations during the reverse/de-noising phase. To improve the quality of the produced synthetic images, we introduce an approach coined "U-Turn Diffusion". The U-Turn Diffusion technique starts with the standard forward diffusion process, albeit with a condensed duration compared to conventional settings. Subsequently, we execute the standard reverse dynamics, initialized with the concluding configuration from the forward process. This U-Turn Diffusion procedure, combining forward, U-turn, and reverse processes, creates a synthetic image 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.00107</link><description>&lt;p&gt;
MERT:&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training. (arXiv:2306.00107v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00107
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;MERT&#65292;&#21033;&#29992;&#20102;&#25945;&#24072;&#27169;&#22411;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#30340;&#32452;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26368;&#36817;&#22312;&#35270;&#35273;&#12289;&#25991;&#26412;&#21644;&#35821;&#38899;&#39046;&#22495;&#20013;&#24050;&#34987;&#35777;&#26126;&#26159;&#35757;&#32451;&#36890;&#29992;&#27169;&#22411;&#30340;&#19968;&#31181;&#24456;&#26377;&#21069;&#26223;&#30340;&#33539;&#20363;&#65292;&#23545;&#20110;&#36328;&#36234;&#38899;&#20048;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#35843;&#24615;&#21644;&#38899;&#39640;&#36825;&#26679;&#30340;&#29305;&#27530;&#38899;&#20048;&#30693;&#35782;&#30340;&#24314;&#27169;&#39047;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#35268;&#27169;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#22768;&#23398;&#38899;&#20048;&#29702;&#35299;&#27169;&#22411;&#65292;&#21363;MERT&#12290;&#22312;&#25105;&#20204;&#30340;&#25506;&#32034;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26356;&#20248;&#31168;&#30340;&#25945;&#24072;&#27169;&#22411;&#32452;&#21512;&#65292;&#36825;&#31181;&#32452;&#21512;&#26041;&#27861;&#22312;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#35821;&#38899;&#21644;&#38899;&#39057;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has recently emerged as a promising paradigm for training generalisable models on large-scale data in the fields of vision, text, and speech. Although SSL has been proven effective in speech and audio, its application to music audio has yet to be thoroughly explored. This is primarily due to the distinctive challenges associated with modelling musical knowledge, particularly its tonal and pitched characteristics of music. To address this research gap, we propose an acoustic Music undERstanding model with large-scale self-supervised Training (MERT), which incorporates teacher models to provide pseudo labels in the masked language modelling (MLM) style acoustic pre-training. In our exploration, we identified a superior combination of teacher models, which outperforms conventional speech and audio approaches in terms of performance. This combination includes an acoustic teacher based on Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14032</link><description>&lt;p&gt;
&#24102;&#26377;&#38899;&#39057;&#20809;&#35889;&#21464;&#25442;&#22120;&#30340; Patch-Mix &#23545;&#27604;&#23398;&#20064;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Patch-Mix Contrastive Learning with Audio Spectrogram Transformer on Respiratory Sound Classification. (arXiv:2305.14032v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#36807;&#22312;&#38899;&#39057;&#25968;&#25454;&#19978;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21628;&#21560;&#22768;&#21253;&#21547;&#26089;&#26399;&#35786;&#26029;&#33268;&#21629;&#32954;&#37096;&#30142;&#30149;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#33258; COVID-19 &#30123;&#24773;&#20197;&#26469;&#65292;&#22522;&#20110;&#30005;&#23376;&#21548;&#35786;&#22120;&#30340;&#26080;&#25509;&#35302;&#21307;&#30103;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20026;&#27492;&#65292;&#24320;&#21457;&#20102;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35786;&#26029;&#32954;&#37096;&#30142;&#30149;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#21307;&#23398;&#25968;&#25454;&#30340;&#31232;&#32570;&#65292;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#22823;&#35268;&#27169;&#35270;&#35273;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#21628;&#21560;&#38899;&#20998;&#31867;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340; Patch-Mix &#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#28151;&#21512;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#30340;&#34917;&#19969;&#65292;&#19982; Audio Spectrogram Transformer (AST) &#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340; Patch-Mix &#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21306;&#20998;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#28151;&#21512;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312; ICBHI &#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#39640;&#24471;&#20998; 4.08%&#12290;
&lt;/p&gt;
&lt;p&gt;
Respiratory sound contains crucial information for the early diagnosis of fatal lung diseases. Since the COVID-19 pandemic, there has been a growing interest in contact-free medical care based on electronic stethoscopes. To this end, cutting-edge deep learning models have been developed to diagnose lung diseases; however, it is still challenging due to the scarcity of medical data. In this study, we demonstrate that the pretrained model on large-scale visual and audio datasets can be generalized to the respiratory sound classification task. In addition, we introduce a straightforward Patch-Mix augmentation, which randomly mixes patches between different samples, with Audio Spectrogram Transformer (AST). We further propose a novel and effective Patch-Mix Contrastive Learning to distinguish the mixed representations in the latent space. Our method achieves state-of-the-art performance on the ICBHI dataset, outperforming the prior leading score by an improvement of 4.08%.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35299;&#37322;&#21402;&#24230;&#21644;&#31283;&#23450;&#39030;&#37096;&#26174;&#33879;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#35299;&#37322;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#21644;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.14106</link><description>&lt;p&gt;
&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Provable Robust Saliency-based Explanations. (arXiv:2212.14106v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.14106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#22522;&#20110;&#26174;&#33879;&#24615;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35299;&#37322;&#21402;&#24230;&#21644;&#31283;&#23450;&#39030;&#37096;&#26174;&#33879;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#35299;&#37322;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#31283;&#23450;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#21644;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#35299;&#37322;&#23545;&#20110;&#24314;&#31435;&#20154;&#31867;&#23545;&#27169;&#22411;&#30340;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20351;&#29992;&#39030;&#37096;-k&#30340;&#20132;&#38598;&#26469;&#35780;&#20272;&#35299;&#37322;&#30340;&#40065;&#26834;&#24615;&#26159;&#24120;&#29992;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#37117;&#22522;&#20110;$\ell_p$&#33539;&#25968;&#65292;&#20174;&#32780;&#22312;&#35780;&#20272;&#21644;&#20248;&#21270;&#30446;&#26631;&#20043;&#38388;&#23384;&#22312;&#19981;&#21305;&#37197;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#35299;&#37322;&#30340;&#21402;&#24230;&#26469;&#34913;&#37327;&#39030;&#37096;-k&#26174;&#33879;&#29305;&#24449;&#25490;&#21517;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#21487;&#34892;&#30340;&#26367;&#20195;&#30446;&#26631;&#30340;R2ET&#31639;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#26368;&#22823;&#21270;&#21402;&#24230;&#24182;&#31283;&#23450;&#39030;&#37096;&#26174;&#33879;&#29305;&#24449;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;R2ET&#21644;&#23545;&#25239;&#35757;&#32451;&#20043;&#38388;&#30340;&#32852;&#31995;&#65307;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20844;&#24335;&#21644;&#27867;&#21270;&#35823;&#24046;&#30028;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26367;&#20195;&#30446;&#26631;&#21487;&#20197;&#25913;&#36827;&#35299;&#37322;&#30340;&#25968;&#20540;&#21644;&#32479;&#35745;&#31283;&#23450;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#21644;&#25968;&#25454;&#27169;&#24577;&#36827;&#34892;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;R2ET&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robust explanations of machine learning models are critical to establishing human trust in the models. The top-$k$ intersection is widely used to evaluate the robustness of explanations. However, most existing attacking and defense strategies are based on $\ell_p$ norms, thus creating a mismatch between the evaluation and optimization objectives. To this end, we define explanation thickness for measuring top-$k$ salient features ranking stability, and design the \textit{R2ET} algorithm based on a novel tractable surrogate to maximize the thickness and stabilize the top salient features efficiently. Theoretically, we prove a connection between R2ET and adversarial training; using a novel multi-objective optimization formulation and a generalization error bound, we further prove that the surrogate objective can improve both the numerical and statistical stability of the explanations. Experiments with a wide spectrum of network architectures and data modalities demonstrate that R2ET attai
&lt;/p&gt;</description></item></channel></rss>