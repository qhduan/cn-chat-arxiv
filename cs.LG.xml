<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#20013;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#12289;&#39044;&#27979;&#12289;&#25512;&#33616;&#12289;&#20998;&#31867;&#12289;&#20272;&#35745;&#21644;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.14151</link><description>&lt;p&gt;
&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#35843;&#26597;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#20013;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#12289;&#39044;&#27979;&#12289;&#25512;&#33616;&#12289;&#20998;&#31867;&#12289;&#20272;&#35745;&#21644;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14151v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25277;&#35937;&#65306;&#36712;&#36857;&#35745;&#31639;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#28085;&#30422;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#21644;&#25366;&#25496;&#65292;&#22240;&#20854;&#22312;&#35832;&#22914;&#20301;&#32622;&#26381;&#21153;&#12289;&#22478;&#24066;&#20132;&#36890;&#21644;&#20844;&#20849;&#23433;&#20840;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#26041;&#27861;&#20391;&#37325;&#20110;&#31616;&#21333;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#38754;&#20020;&#22797;&#26434;&#35745;&#31639;&#12289;&#26377;&#38480;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#19981;&#36275;&#20197;&#36866;&#24212;&#29616;&#23454;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36712;&#36857;&#35745;&#31639;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65288;DL4Traj&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#24182;&#31616;&#35201;&#20171;&#32461;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#31649;&#29702;&#65288;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65289;&#21644;&#25366;&#25496;&#65288;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#39044;&#27979;&#12289;&#36712;&#36857;&#30456;&#20851;&#30340;&#25512;&#33616;&#12289;&#36712;&#36857;&#20998;&#31867;&#12289;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#12289;&#24322;&#24120;&#26816;&#27979;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14151v1 Announce Type: cross  Abstract: Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities. In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj). We first define trajectory data and provide a brief overview of widely-used deep learning models. Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related recommendation, trajectory classification, travel time estimation, anomaly detecti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#65288;FTN&#65289;&#65292;&#23427;&#21487;&#20197;&#20811;&#26381;&#22810;&#20219;&#21153;&#22810;&#39046;&#22495;&#23398;&#20064;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#21033;&#29992;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#23384;&#20648;&#25104;&#26412;&#12289;&#35745;&#31639;&#37327;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#31561;&#26041;&#38754;&#23454;&#29616;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FTN&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#30340;&#30446;&#26631;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.06124</link><description>&lt;p&gt;
&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#29992;&#20110;&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Factorized Tensor Networks for Multi-Task and Multi-Domain Learning. (arXiv:2310.06124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#65288;FTN&#65289;&#65292;&#23427;&#21487;&#20197;&#20811;&#26381;&#22810;&#20219;&#21153;&#22810;&#39046;&#22495;&#23398;&#20064;&#20013;&#30340;&#20849;&#20139;&#20449;&#24687;&#21033;&#29992;&#25361;&#25112;&#65292;&#24182;&#22312;&#20934;&#30830;&#24615;&#12289;&#23384;&#20648;&#25104;&#26412;&#12289;&#35745;&#31639;&#37327;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#31561;&#26041;&#38754;&#23454;&#29616;&#39640;&#25928;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FTN&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#26356;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#30340;&#30446;&#26631;&#39046;&#22495;&#21644;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#21644;&#22810;&#39046;&#22495;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#20351;&#29992;&#21333;&#20010;&#32479;&#19968;&#30340;&#32593;&#32476;&#20849;&#21516;&#23398;&#20064;&#22810;&#20010;&#20219;&#21153;/&#39046;&#22495;&#65292;&#25110;&#32773;&#20808;&#21518;&#23398;&#20064;&#23427;&#20204;&#12290;&#20851;&#38190;&#25361;&#25112;&#21644;&#26426;&#20250;&#26159;&#21033;&#29992;&#20219;&#21153;&#21644;&#39046;&#22495;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#25552;&#39640;&#32479;&#19968;&#32593;&#32476;&#30340;&#25928;&#29575;&#65292;&#21253;&#25324;&#20934;&#30830;&#24615;&#12289;&#23384;&#20648;&#25104;&#26412;&#12289;&#35745;&#31639;&#37327;&#25110;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#35299;&#24352;&#37327;&#32593;&#32476;&#65288;FTN&#65289;&#65292;&#21487;&#20197;&#36890;&#36807;&#22686;&#21152;&#23569;&#37327;&#38468;&#21152;&#21442;&#25968;&#23454;&#29616;&#19982;&#29420;&#31435;&#21333;&#20219;&#21153;/&#39046;&#22495;&#32593;&#32476;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#12290;FTN&#20351;&#29992;&#28304;&#27169;&#22411;&#30340;&#20923;&#32467;&#20027;&#24178;&#32593;&#32476;&#65292;&#24182;&#36880;&#27493;&#28155;&#21152;&#20219;&#21153;/&#39046;&#22495;&#29305;&#23450;&#30340;&#20302;&#31209;&#24352;&#37327;&#22240;&#23376;&#21040;&#20849;&#20139;&#30340;&#20923;&#32467;&#32593;&#32476;&#20013;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#22823;&#37327;&#30446;&#26631;&#39046;&#22495;&#21644;&#20219;&#21153;&#65292;&#32780;&#19981;&#20250;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;FTN&#38656;&#35201;&#36739;&#23569;&#30340;&#20219;&#21153;&#29305;&#23450;&#21442;&#25968;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#39046;&#22495;&#21644;&#22810;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task and multi-domain learning methods seek to learn multiple tasks/domains, jointly or one after another, using a single unified network. The key challenge and opportunity is to exploit shared information across tasks and domains to improve the efficiency of the unified network. The efficiency can be in terms of accuracy, storage cost, computation, or sample complexity. In this paper, we propose a factorized tensor network (FTN) that can achieve accuracy comparable to independent single-task/domain networks with a small number of additional parameters. FTN uses a frozen backbone network from a source model and incrementally adds task/domain-specific low-rank tensor factors to the shared frozen network. This approach can adapt to a large number of target domains and tasks without catastrophic forgetting. Furthermore, FTN requires a significantly smaller number of task-specific parameters compared to existing methods. We performed experiments on widely used multi-domain and multi-
&lt;/p&gt;</description></item></channel></rss>