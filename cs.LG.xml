<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#23545;&#25239;&#35774;&#32622;&#19979;&#37325;&#24314;&#25104;&#21151;&#29575;&#30340;&#27491;&#24335;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#26356;&#26126;&#26234;&#22320;&#36873;&#25321;&#38544;&#31169;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.12861</link><description>&lt;p&gt;
&#22312;&#27809;&#26377;&#25968;&#25454;&#20808;&#39564;&#26465;&#20214;&#19979;&#38480;&#21046;&#23545;&#25239;&#32773;&#37325;&#24314;&#25915;&#20987;&#25104;&#21151;&#29575;
&lt;/p&gt;
&lt;p&gt;
Bounding Reconstruction Attack Success of Adversaries Without Data Priors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#23545;&#25239;&#35774;&#32622;&#19979;&#37325;&#24314;&#25104;&#21151;&#29575;&#30340;&#27491;&#24335;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#65292;&#26377;&#21161;&#20110;&#26356;&#26126;&#26234;&#22320;&#36873;&#25321;&#38544;&#31169;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#24314;&#25915;&#20987;&#23384;&#22312;&#27844;&#28431;&#25935;&#24863;&#25968;&#25454;&#30340;&#39118;&#38505;&#12290;&#22312;&#29305;&#23450;&#24773;&#22659;&#19979;&#65292;&#23545;&#25163;&#21487;&#20197;&#20351;&#29992;&#27169;&#22411;&#30340;&#26799;&#24230;&#20960;&#20046;&#23436;&#32654;&#22320;&#37325;&#24314;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#12290;&#22312;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26102;&#65292;&#21487;&#20197;&#25552;&#20379;&#23545;&#36825;&#31181;&#37325;&#24314;&#25915;&#20987;&#25104;&#21151;&#29575;&#30340;&#27491;&#24335;&#19978;&#38480;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#20123;&#19978;&#38480;&#26159;&#22312;&#21487;&#33021;&#19981;&#31526;&#21512;&#39640;&#24230;&#29616;&#23454;&#23454;&#29992;&#24615;&#30340;&#26368;&#22351;&#24773;&#20917;&#20551;&#35774;&#19979;&#21046;&#23450;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#24046;&#20998;&#38544;&#31169;&#35757;&#32451;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#22312;&#29616;&#23454;&#23545;&#25239;&#35774;&#32622;&#19979;&#30340;&#37325;&#24314;&#25104;&#21151;&#29575;&#27491;&#24335;&#19978;&#38480;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#32467;&#26524;&#25903;&#25345;&#36825;&#20123;&#19978;&#38480;&#12290;&#36890;&#36807;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#29616;&#23454;&#24773;&#22659;&#20013;&#65292;&#65288;a&#65289;&#39044;&#26399;&#30340;&#37325;&#24314;&#25104;&#21151;&#29575;&#21487;&#20197;&#22312;&#19981;&#21516;&#32972;&#26223;&#21644;&#19981;&#21516;&#24230;&#37327;&#19979;&#24471;&#21040;&#36866;&#24403;&#30340;&#38480;&#21046;&#65292;&#36825;&#65288;b&#65289;&#26377;&#21161;&#20110;&#26356;&#26126;&#26234;&#22320;&#36873;&#25321;&#38544;&#31169;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12861v1 Announce Type: new  Abstract: Reconstruction attacks on machine learning (ML) models pose a strong risk of leakage of sensitive data. In specific contexts, an adversary can (almost) perfectly reconstruct training data samples from a trained model using the model's gradients. When training ML models with differential privacy (DP), formal upper bounds on the success of such reconstruction attacks can be provided. So far, these bounds have been formulated under worst-case assumptions that might not hold high realistic practicality. In this work, we provide formal upper bounds on reconstruction success under realistic adversarial settings against ML models trained with DP and support these bounds with empirical results. With this, we show that in realistic scenarios, (a) the expected reconstruction success can be bounded appropriately in different contexts and by different metrics, which (b) allows for a more educated choice of a privacy parameter.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#23450;&#20301;&#31070;&#32463;&#32593;&#32476;&#65288;P-NN&#65289;&#65292;&#36890;&#36807;&#26368;&#23567;&#25551;&#36848;&#29305;&#24449;&#38477;&#20302;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#32447;&#23450;&#20301;&#20013;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;</title><link>https://arxiv.org/abs/2402.09580</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26080;&#32447;&#23450;&#20301;&#20013;&#30340;&#22797;&#26434;&#24230;&#38477;&#20302;&#65306;&#26368;&#23567;&#25551;&#36848;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Complexity Reduction in Machine Learning-Based Wireless Positioning: Minimum Description Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#23450;&#20301;&#31070;&#32463;&#32593;&#32476;&#65288;P-NN&#65289;&#65292;&#36890;&#36807;&#26368;&#23567;&#25551;&#36848;&#29305;&#24449;&#38477;&#20302;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#32447;&#23450;&#20301;&#20013;&#30340;&#22797;&#26434;&#24230;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#19968;&#30452;&#33268;&#21147;&#20110;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26080;&#32447;&#23450;&#20301;&#65288;WP&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;WP&#31639;&#27861;&#22312;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#34920;&#29616;&#20986;&#20102;&#39640;&#31934;&#24230;&#21644;&#40065;&#26834;&#24615;&#65292;&#20294;&#23427;&#20204;&#20063;&#23384;&#22312;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#65306;&#23427;&#20204;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#29305;&#24449;&#65292;&#36825;&#23545;&#20110;&#31227;&#21160;&#24212;&#29992;&#26469;&#35828;&#21487;&#33021;&#26159;&#31105;&#27490;&#30340;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#20301;&#31070;&#32463;&#32593;&#32476;&#65288;P-NN&#65289;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#26368;&#23567;&#25551;&#36848;&#29305;&#24449;&#65292;&#22823;&#22823;&#38477;&#20302;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;WP&#30340;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#29305;&#24449;&#36873;&#25321;&#22522;&#20110;&#26368;&#22823;&#21151;&#29575;&#27979;&#37327;&#21450;&#20854;&#26102;&#38388;&#20301;&#32622;&#65292;&#20197;&#20256;&#36798;&#36827;&#34892;WP&#25152;&#38656;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#29305;&#24449;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22312;&#20449;&#21495;&#20108;&#36827;&#21046;&#36873;&#25321;&#19978;&#20351;&#29992;&#20449;&#24687;&#35770;&#24230;&#37327;&#65292;&#20248;&#21270;&#20102;&#26399;&#26395;&#26377;&#29992;&#20449;&#24687;&#37327;&#21644;&#20998;&#31867;&#33021;&#21147;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09580v1 Announce Type: new  Abstract: A recent line of research has been investigating deep learning approaches to wireless positioning (WP). Although these WP algorithms have demonstrated high accuracy and robust performance against diverse channel conditions, they also have a major drawback: they require processing high-dimensional features, which can be prohibitive for mobile applications. In this work, we design a positioning neural network (P-NN) that substantially reduces the complexity of deep learning-based WP through carefully crafted minimum description features. Our feature selection is based on maximum power measurements and their temporal locations to convey information needed to conduct WP. We also develop a novel methodology for adaptively selecting the size of feature space, which optimizes over balancing the expected amount of useful information and classification capability, quantified using information-theoretic measures on the signal bin selection. Numeri
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20307;&#26041;&#27861;&#26469;&#24418;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#20307;&#30340;&#24403;&#21069;&#23494;&#24230;&#26469;&#25351;&#23548;&#25512;&#33616;&#30340;&#34892;&#21160;&#65292;&#35299;&#20915;&#20102;&#20010;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#30340;&#26032;&#30340;&#31454;&#20105;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#25104;&#26412;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#20102;&#32463;&#20856;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26399;&#26395;&#12290;</title><link>https://arxiv.org/abs/2402.04579</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#20248;&#20256;&#36755;&#23454;&#29616;&#38598;&#20307;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Collective Counterfactual Explanations via Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20307;&#26041;&#27861;&#26469;&#24418;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#36890;&#36807;&#21033;&#29992;&#20010;&#20307;&#30340;&#24403;&#21069;&#23494;&#24230;&#26469;&#25351;&#23548;&#25512;&#33616;&#30340;&#34892;&#21160;&#65292;&#35299;&#20915;&#20102;&#20010;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#30340;&#26032;&#30340;&#31454;&#20105;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#25104;&#26412;&#38382;&#39064;&#65292;&#24182;&#25913;&#36827;&#20102;&#32463;&#20856;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#25552;&#20379;&#20010;&#20307;&#30340;&#25104;&#26412;&#26368;&#20248;&#34892;&#21160;&#65292;&#20197;&#25913;&#21464;&#26631;&#31614;&#20026;&#25152;&#38656;&#30340;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#22823;&#37327;&#23454;&#20363;&#23547;&#27714;&#29366;&#24577;&#20462;&#25913;&#65292;&#36825;&#31181;&#20010;&#20307;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#26032;&#30340;&#31454;&#20105;&#21644;&#24847;&#24819;&#19981;&#21040;&#30340;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25512;&#33616;&#24573;&#35270;&#20102;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#24314;&#35758;&#29992;&#25143;&#35748;&#20026;&#26159;&#24322;&#24120;&#20540;&#30340;&#34892;&#21160;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#20307;&#26041;&#27861;&#26469;&#24418;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#37325;&#28857;&#26159;&#21033;&#29992;&#20010;&#20307;&#30340;&#24403;&#21069;&#23494;&#24230;&#26469;&#25351;&#23548;&#25512;&#33616;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#38382;&#39064;&#33258;&#28982;&#22320;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290;&#20511;&#37492;&#26368;&#20248;&#20256;&#36755;&#30340;&#24191;&#27867;&#25991;&#29486;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#36825;&#31181;&#38598;&#20307;&#26041;&#27861;&#22914;&#20309;&#25913;&#36827;&#32463;&#20856;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#27169;&#25311;&#25903;&#25345;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#19982;&#32463;&#20856;&#26041;&#27861;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide individuals with cost-optimal actions that can alter their labels to desired classes. However, if substantial instances seek state modification, such individual-centric methods can lead to new competitions and unanticipated costs. Furthermore, these recommendations, disregarding the underlying data distribution, may suggest actions that users perceive as outliers. To address these issues, our work proposes a collective approach for formulating counterfactual explanations, with an emphasis on utilizing the current density of the individuals to inform the recommended actions. Our problem naturally casts as an optimal transport problem. Leveraging the extensive literature on optimal transport, we illustrate how this collective method improves upon the desiderata of classical counterfactual explanations. We support our proposal with numerical simulations, illustrating the effectiveness of the proposed approach and its relation to classic methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;</title><link>https://arxiv.org/abs/2402.00751</link><description>&lt;p&gt;
&#26080;&#27861;&#23398;&#20064;&#30340;&#31639;&#27861;&#29992;&#20110;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unlearnable Algorithms for In-context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#26469;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#31934;&#30830;&#21435;&#23398;&#20064;&#65292;&#24182;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#27169;&#22411;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#37096;&#32626;&#22312;&#26410;&#30693;&#26469;&#28304;&#30340;&#25968;&#25454;&#19978;&#65292;&#26426;&#22120;&#21435;&#23398;&#20064;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35201;&#23454;&#29616;&#31934;&#30830;&#30340;&#21435;&#23398;&#20064;&#8212;&#8212;&#22312;&#27809;&#26377;&#20351;&#29992;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#19982;&#27169;&#22411;&#20998;&#24067;&#21305;&#37197;&#30340;&#27169;&#22411;&#8212;&#8212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#25110;&#20302;&#25928;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#39044;&#20808;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20219;&#21153;&#36866;&#24212;&#38454;&#27573;&#30340;&#39640;&#25928;&#21435;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;LLM&#36827;&#34892;&#20219;&#21153;&#36866;&#24212;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#21487;&#20197;&#23454;&#29616;&#20219;&#21153;&#36866;&#24212;&#35757;&#32451;&#25968;&#25454;&#30340;&#39640;&#25928;&#31934;&#30830;&#21435;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#36873;&#25321;&#23569;&#37327;&#35757;&#32451;&#31034;&#20363;&#21152;&#21040;LLM&#30340;&#25552;&#31034;&#21069;&#38754;&#65288;&#29992;&#20110;&#20219;&#21153;&#36866;&#24212;&#65289;&#65292;&#21517;&#20026;ERASE&#65292;&#23427;&#30340;&#21435;&#23398;&#20064;&#25805;&#20316;&#25104;&#26412;&#19982;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#26080;&#20851;&#65292;&#24847;&#21619;&#30528;&#23427;&#36866;&#29992;&#20110;&#22823;&#22411;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#35752;&#35770;&#20102;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36825;&#20351;&#25105;&#20204;&#24471;&#21040;&#20102;&#20197;&#19979;&#32467;&#35770;&#65306;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning is a desirable operation as models get increasingly deployed on data with unknown provenance. However, achieving exact unlearning -- obtaining a model that matches the model distribution when the data to be forgotten was never used -- is challenging or inefficient, often requiring significant retraining. In this paper, we focus on efficient unlearning methods for the task adaptation phase of a pretrained large language model (LLM). We observe that an LLM's ability to do in-context learning for task adaptation allows for efficient exact unlearning of task adaptation training data. We provide an algorithm for selecting few-shot training examples to prepend to the prompt given to an LLM (for task adaptation), ERASE, whose unlearning operation cost is independent of model and dataset size, meaning it scales to large models and datasets. We additionally compare our approach to fine-tuning approaches and discuss the trade-offs between the two approaches. This leads us to p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.00848</link><description>&lt;p&gt;
&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;-&#19968;&#31181;&#22522;&#20110;YOLOv8&#30340;&#38598;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach. (arXiv:2309.00848v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#30340;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#21644;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#12290;&#35813;&#26041;&#27861;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#24182;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#65292;&#26377;&#21161;&#20110;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20391;&#37325;&#20110;&#21033;&#29992;YOLOv8&#27169;&#22411;&#21644;&#21019;&#26032;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25552;&#21319;&#23391;&#21152;&#25289;&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#65288;DLA&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#20197;&#24212;&#23545;&#23391;&#21152;&#25289;&#22797;&#26434;&#25991;&#23383;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#32463;&#36807;&#20005;&#26684;&#30340;&#39564;&#35777;&#38598;&#35780;&#20272;&#65292;&#23545;&#23436;&#25972;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#23454;&#29616;&#20934;&#30830;&#30340;&#20803;&#32032;&#20998;&#21106;&#30340;&#20004;&#38454;&#27573;&#39044;&#27979;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#27169;&#22411;&#32467;&#21512;&#21518;&#22788;&#29702;&#24615;&#33021;&#20248;&#20110;&#21333;&#20010;&#22522;&#30784;&#26550;&#26500;&#65292;&#35299;&#20915;&#20102;BaDLAD&#25968;&#25454;&#38598;&#20013;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#26088;&#22312;&#25512;&#21160;&#23391;&#21152;&#25289;&#25991;&#26723;&#20998;&#26512;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;OCR&#21644;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#65292;&#21516;&#26102;BaDLAD&#20316;&#20026;&#22522;&#30784;&#36164;&#28304;&#26377;&#21161;&#20110;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#20026;&#23558;&#26032;&#31574;&#30053;&#32435;&#20837;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using the YOLOv8 model and innovative post-processing techniques. We tackle challenges unique to the complex Bengali script by employing data augmentation for model robustness. After meticulous validation set evaluation, we fine-tune our approach on the complete dataset, leading to a two-stage prediction strategy for accurate element segmentation. Our ensemble model, combined with post-processing, outperforms individual base architectures, addressing issues identified in the BaDLAD dataset. By leveraging this approach, we aim to advance Bengali document analysis, contributing to improved OCR and document comprehension and BaDLAD serves as a foundational resource for this endeavor, aiding future research in the field. Furthermore, our experiments provided key insights to incorporate new strategies into the established solution.
&lt;/p&gt;</description></item><item><title>RobustNeuralNetworks.jl&#26159;&#19968;&#20010;&#29992;Julia&#32534;&#20889;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21253;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#40065;&#26834;&#24615;&#32422;&#26463;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;</title><link>http://arxiv.org/abs/2306.12612</link><description>&lt;p&gt;
RobustNeuralNetworks.jl&#65306;&#24102;&#26377;&#35748;&#35777;&#40065;&#26834;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;
RobustNeuralNetworks.jl: a Package for Machine Learning and Data-Driven Control with Certified Robustness. (arXiv:2306.12612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12612
&lt;/p&gt;
&lt;p&gt;
RobustNeuralNetworks.jl&#26159;&#19968;&#20010;&#29992;Julia&#32534;&#20889;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#39537;&#21160;&#25511;&#21046;&#21253;&#65292;&#23427;&#36890;&#36807;&#33258;&#28982;&#28385;&#36275;&#29992;&#25143;&#23450;&#20041;&#30340;&#40065;&#26834;&#24615;&#32422;&#26463;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#23545;&#20110;&#24494;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#65292;&#23548;&#33268;&#20986;&#29616;&#24847;&#22806;&#25110;&#33030;&#24369;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RobustNeuralNetworks.jl&#65306;&#19968;&#20010;Julia&#21253;&#65292;&#29992;&#20110;&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33258;&#28982;&#22320;&#28385;&#36275;&#19968;&#32452;&#29992;&#25143;&#23450;&#20041;&#30340;&#40065;&#26834;&#24615;&#32422;&#26463;&#26465;&#20214;&#12290;&#35813;&#21253;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;Recurrent Equilibrium Network&#65288;REN&#65289;&#21644;Lipschitz-Bounded Deep Network&#65288;LBDN&#65289;&#27169;&#22411;&#31867;&#65292;&#24182;&#26088;&#22312;&#30452;&#25509;&#19982;Julia&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#21253;Flux.jl&#25509;&#21475;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27169;&#22411;&#21442;&#25968;&#21270;&#32972;&#21518;&#30340;&#29702;&#35770;&#65292;&#27010;&#36848;&#20102;&#35813;&#21253;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25945;&#31243;&#65292;&#28436;&#31034;&#20102;&#20854;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#38750;&#32447;&#24615;&#29366;&#24577;&#35266;&#27979;&#22120;&#35774;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are typically sensitive to small input perturbations, leading to unexpected or brittle behaviour. We present RobustNeuralNetworks.jl: a Julia package for neural network models that are constructed to naturally satisfy a set of user-defined robustness constraints. The package is based on the recently proposed Recurrent Equilibrium Network (REN) and Lipschitz-Bounded Deep Network (LBDN) model classes, and is designed to interface directly with Julia's most widely-used machine learning package, Flux.jl. We discuss the theory behind our model parameterization, give an overview of the package, and provide a tutorial demonstrating its use in image classification, reinforcement learning, and nonlinear state-observer design.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11127</link><description>&lt;p&gt;
&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65306;&#29702;&#35299;&#20854;&#31639;&#27861;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#22312;&#25552;&#39640;&#23454;&#35777;&#34920;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#27714;&#26356;&#21152;&#22797;&#26434;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23454;&#39564;&#36890;&#24120;&#26377;&#35768;&#22810;&#21442;&#25968;&#65292;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#12290;Tree-structured Parzen estimator (TPE) &#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#26368;&#36817;&#30340;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25511;&#21046;&#21442;&#25968;&#30340;&#35282;&#33394;&#21644;&#31639;&#27861;&#30452;&#35273;&#23578;&#26410;&#24471;&#21040;&#35752;&#35770;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#27599;&#20010;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#20174;&#21078;&#26512;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#25512;&#33616;&#35774;&#32622;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#25512;&#33616;&#35774;&#32622;&#25552;&#39640;&#20102;TPE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TPE&#23454;&#29616;&#21487;&#22312;https://github.com/nabenabe0928/tpe/tree/single-opt&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.10300</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21442;&#25968;&#20248;&#21270;&#20013;&#30340;&#26377;&#25928;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#19982;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge. (arXiv:2208.10300v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#36890;&#24120;&#20551;&#23450;&#24050;&#26377;&#25928;&#29992;&#20989;&#25968;&#12289;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#25110;&#23581;&#35797;&#30830;&#23450;&#23436;&#25972;&#30340;Pareto&#21069;&#27839;&#26469;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#65292;&#32467;&#26524;&#24448;&#24448;&#22522;&#20110;&#38544;&#21547;&#21644;&#26174;&#24615;&#30340;&#19987;&#23478;&#30693;&#35782;&#65292;&#38590;&#20197;&#23450;&#20041;&#19968;&#20010;&#25928;&#29992;&#20989;&#25968;&#65292;&#32780;&#20114;&#21160;&#23398;&#20064;&#25110;&#21518;&#32493;&#21551;&#21457;&#24335;&#38656;&#35201;&#21453;&#22797;&#24182;&#19988;&#26114;&#36149;&#22320;&#19987;&#23478;&#21442;&#19982;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#65288;&#25104;&#23545;&#30340;&#65289;&#32467;&#26524;&#20559;&#22909;&#65292;&#32780;&#19988;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#24456;&#23569;&#30340;&#32467;&#26524;&#26102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#21040;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art in multi-objective optimization assumes either a given utility function, learns a utility function interactively or tries to determine the complete Pareto front, requiring a post elicitation of the preferred result. However, result elicitation in real world problems is often based on implicit and explicit expert knowledge, making it difficult to define a utility function, whereas interactive learning or post elicitation requires repeated and expensive expert involvement. To mitigate this, we learn a utility function offline, using expert knowledge by means of preference learning. In contrast to other works, we do not only use (pairwise) result preferences, but also coarse information about the utility function space. This enables us to improve the utility function estimate, especially when using very few results. Additionally, we model the occurring uncertainties in the utility function learning task and propagate them through the whole optimization chain. 
&lt;/p&gt;</description></item></channel></rss>