<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.17329</link><description>&lt;p&gt;
&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;
&lt;/p&gt;
&lt;p&gt;
Deep Support Vectors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#32034;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#27010;&#24565;&#65292;&#20171;&#32461;&#20102;DeepKKT&#26465;&#20214;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#31867;&#20284;&#65292;&#20026;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#25552;&#20379;&#20102;&#26041;&#27861;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#25104;&#21151;&#36890;&#24120;&#34987;&#24402;&#22240;&#20110;&#20854;&#19982;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#31561;&#20215;&#24615;&#65292;&#20294;&#36825;&#31181;&#20851;&#31995;&#30340;&#23454;&#38469;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20840;&#38754;&#25506;&#35752;&#12290;&#26412;&#25991;&#22312;&#36825;&#19968;&#39046;&#22495;&#24320;&#23637;&#20102;&#19968;&#39033;&#25506;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#28145;&#24230;&#25903;&#25345;&#21521;&#37327;&#65288;DSVs&#65289;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DeepKKT&#26465;&#20214;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#28145;&#24230;&#23398;&#20064;&#37327;&#36523;&#23450;&#21046;&#30340;&#20256;&#32479;Karush-Kuhn-Tucker&#65288;KKT&#65289;&#26465;&#20214;&#30340;&#35843;&#25972;&#29256;&#26412;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;DSVs&#19982;SVM&#20013;&#30340;&#25903;&#25345;&#21521;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#26631;&#20934;&#30340;&#20999;&#23454;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20351;&#29992;DSVs&#37325;&#26500;&#27169;&#22411;&#65292;&#31867;&#20284;&#20110;SVM&#20013;&#30340;&#36807;&#31243;&#12290;&#20195;&#30721;&#23558;&#20250;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17329v1 Announce Type: cross  Abstract: While the success of deep learning is commonly attributed to its theoretical equivalence with Support Vector Machines (SVM), the practical implications of this relationship have not been thoroughly explored. This paper pioneers an exploration in this domain, specifically focusing on the identification of Deep Support Vectors (DSVs) within deep learning models. We introduce the concept of DeepKKT conditions, an adaptation of the traditional Karush-Kuhn-Tucker (KKT) conditions tailored for deep learning. Through empirical investigations, we illustrate that DSVs exhibit similarities to support vectors in SVM, offering a tangible method to interpret the decision-making criteria of models. Additionally, our findings demonstrate that models can be effectively reconstructed using DSVs, resembling the process in SVM. The code will be available.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CounterfacTS&#24037;&#20855;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#25506;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03508</link><description>&lt;p&gt;
&#25506;&#31350;&#20351;&#29992;CounterfacTS&#25506;&#31350;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Probing the Robustness of Time-series Forecasting Models with CounterfacTS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03508
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CounterfacTS&#24037;&#20855;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#25506;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26102;&#38754;&#20020;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#25968;&#25454;&#20998;&#24067;&#30340;&#26102;&#38388;&#28436;&#21270;&#65288;&#21363;&#27010;&#24565;&#28418;&#31227;&#65289;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#21453;&#26144;&#36825;&#20123;&#21464;&#21270;&#65292;&#27169;&#22411;&#22312;&#26032;&#30340;&#20998;&#24067;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#24456;&#24046;&#65292;&#22240;&#27492;&#65292;&#27492;&#31867;&#20107;&#20214;&#30340;&#24433;&#21709;&#20107;&#21069;&#26080;&#27861;&#21487;&#38752;&#22320;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20844;&#24320;&#21457;&#24067;CounterfacTS&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#21453;&#20107;&#23454;&#25506;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#12290;CounterfacTS&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20801;&#35768;&#29992;&#25143;&#21487;&#35270;&#21270;&#12289;&#27604;&#36739;&#21644;&#37327;&#21270;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21450;&#20854;&#39044;&#27979;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#23545;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#21508;&#31181;&#21464;&#25442;&#65292;&#24182;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#25506;&#32034;&#39044;&#27979;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#31034;&#20363;&#26696;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;CounterfacTS&#22914;&#20309;&#29992;&#20110;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03508v1 Announce Type: new  Abstract: A common issue for machine learning models applied to time-series forecasting is the temporal evolution of the data distributions (i.e., concept drift). Because most of the training data does not reflect such changes, the models present poor performance on the new out-of-distribution scenarios and, therefore, the impact of such events cannot be reliably anticipated ahead of time. We present and publicly release CounterfacTS, a tool to probe the robustness of deep learning models in time-series forecasting tasks via counterfactuals. CounterfacTS has a user-friendly interface that allows the user to visualize, compare and quantify time series data and their forecasts, for a number of datasets and deep learning models. Furthermore, the user can apply various transformations to the time series and explore the resulting changes in the forecasts in an interpretable manner. Through example cases, we illustrate how CounterfacTS can be used to i)
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#25552;&#21462;&#20449;&#24687;&#65292;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.12993</link><description>&lt;p&gt;
&#29992;&#20110;&#21270;&#23398;&#25991;&#29486;&#25968;&#25454;&#25366;&#25496;&#30340;&#33258;&#20027;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Autonomous Large Language Model Agent for Chemical Literature Data Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12993
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#20174;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#25552;&#21462;&#20449;&#24687;&#65292;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#25552;&#39640;&#24037;&#20316;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21512;&#25104;&#23545;&#20110;&#25512;&#21160;&#26448;&#26009;&#21512;&#25104;&#21644;&#33647;&#29289;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#65292;&#24433;&#21709;&#30528;&#21253;&#25324;&#29615;&#22659;&#31185;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#22312;&#20869;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#21270;&#23398;&#39046;&#22495;&#30340;&#25216;&#26415;&#19978;&#21319;&#20351;&#24471;&#20135;&#29983;&#20102;&#22823;&#37327;&#30340;&#21270;&#23398;&#25968;&#25454;&#65292;&#25361;&#25112;&#30740;&#31350;&#20154;&#21592;&#21435;&#35782;&#21035;&#27169;&#24335;&#24182;&#32454;&#21270;&#21512;&#25104;&#36807;&#31243;&#12290;&#20154;&#24037;&#26234;&#33021;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#26469;&#20248;&#21270;&#21512;&#25104;&#24182;&#25552;&#39640;&#20135;&#37327;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#22788;&#29702;&#25991;&#29486;&#25968;&#25454;&#26041;&#38754;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#21270;&#23398;&#25991;&#29486;&#30340;&#32467;&#26500;&#19981;&#35268;&#25972;&#65292;&#20889;&#20316;&#39118;&#26684;&#22810;&#26679;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26694;&#26550;&#65292;&#33021;&#22815;&#20174;&#24191;&#27867;&#30340;&#21270;&#23398;&#25991;&#29486;&#20013;&#39640;&#20445;&#30495;&#22320;&#25552;&#21462;&#20449;&#24687;&#12290;&#36825;&#20010;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24555;&#36895;&#29983;&#25104;&#21644;&#36845;&#20195;&#20248;&#21270;&#12290;&#23427;&#20805;&#24403;&#21270;&#23398;&#21161;&#25163;&#30340;&#35282;&#33394;&#65292;&#33258;&#21160;&#21270;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#33410;&#30465;&#20154;&#21147;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12993v1 Announce Type: cross  Abstract: Chemical synthesis, which is crucial for advancing material synthesis and drug discovery, impacts various sectors including environmental science and healthcare. The rise of technology in chemistry has generated extensive chemical data, challenging researchers to discern patterns and refine synthesis processes. Artificial intelligence (AI) helps by analyzing data to optimize synthesis and increase yields. However, AI faces challenges in processing literature data due to the unstructured format and diverse writing style of chemical literature. To overcome these difficulties, we introduce an end-to-end AI agent framework capable of high-fidelity extraction from extensive chemical literature. This AI agent employs large language models (LLMs) for prompt generation and iterative optimization. It functions as a chemistry assistant, automating data collection and analysis, thereby saving manpower and enhancing performance. Our framework's ef
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#40065;&#26834;&#30340;&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LR-GCL&#65289;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#36716;&#23548;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20302;&#31209;&#27491;&#35268;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#36716;&#23548;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.09600</link><description>&lt;p&gt;
&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Graph Contrastive Learning for Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#40065;&#26834;&#30340;&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LR-GCL&#65289;&#31639;&#27861;&#65292;&#24212;&#29992;&#20110;&#36716;&#23548;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#20302;&#31209;&#27491;&#35268;&#21270;&#30340;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#32534;&#30721;&#22120;&#65292;&#24182;&#20351;&#29992;&#29983;&#25104;&#30340;&#29305;&#24449;&#36827;&#34892;&#32447;&#24615;&#36716;&#23548;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#22312;&#33410;&#28857;&#20998;&#31867;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22270;&#25968;&#25454;&#20013;&#19981;&#21487;&#36991;&#20813;&#22320;&#23384;&#22312;&#22122;&#22768;&#65292;&#36825;&#20250;&#20005;&#37325;&#38477;&#20302;GNNs&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#19988;&#40065;&#26834;&#30340;GNN&#32534;&#30721;&#22120;&#65292;&#21363;&#20302;&#31209;&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;LR-GCL&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20004;&#20010;&#27493;&#39588;&#36827;&#34892;&#36716;&#23548;&#33410;&#28857;&#20998;&#31867;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#20302;&#31209;&#27491;&#24120;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#19968;&#20010;&#21517;&#20026;LR-GCL&#30340;&#20302;&#31209;GCL&#32534;&#30721;&#22120;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;LR-GCL&#29983;&#25104;&#30340;&#29305;&#24449;&#65292;&#20351;&#29992;&#32447;&#24615;&#36716;&#23548;&#20998;&#31867;&#31639;&#27861;&#23545;&#22270;&#20013;&#30340;&#26410;&#26631;&#35760;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;LR-GCL&#21463;&#21040;&#22270;&#25968;&#25454;&#21644;&#20854;&#26631;&#31614;&#30340;&#20302;&#39057;&#24615;&#36136;&#30340;&#21551;&#31034;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#21463;&#21040;&#25105;&#20204;&#20851;&#20110;&#36716;&#23548;&#23398;&#20064;&#30340;&#23574;&#38160;&#27867;&#21270;&#30028;&#38480;&#30340;&#25512;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09600v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have been widely used to learn node representations and with outstanding performance on various tasks such as node classification. However, noise, which inevitably exists in real-world graph data, would considerably degrade the performance of GNNs revealed by recent studies. In this work, we propose a novel and robust GNN encoder, Low-Rank Graph Contrastive Learning (LR-GCL). Our method performs transductive node classification in two steps. First, a low-rank GCL encoder named LR-GCL is trained by prototypical contrastive learning with low-rank regularization. Next, using the features produced by LR-GCL, a linear transductive classification algorithm is used to classify the unlabeled nodes in the graph. Our LR-GCL is inspired by the low frequency property of the graph data and its labels, and it is also theoretically motivated by our sharp generalization bound for transductive learning. To the best of our kno
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;</title><link>https://arxiv.org/abs/2402.02242</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65288;PVMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;PVMs&#36798;&#21040;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#65292;&#26631;&#20934;&#30340;&#20840;&#38754;&#24494;&#35843;&#33539;&#24335;&#30001;&#20110;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#65292;&#26088;&#22312;&#20197;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#35270;&#35273;PEFT&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#23545;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;PEFT&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#22522;&#20110;&#28155;&#21152;&#30340;&#12289;&#22522;&#20110;&#37096;&#20998;&#30340;&#21644;&#22522;&#20110;&#32479;&#19968;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;&#35813;&#32508;&#36848;&#36824;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01782</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Spiking Neural Network Learning Methods with Varying Locality
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#19981;&#21516;&#23616;&#37096;&#24615;&#23545;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25552;&#20379;&#26356;&#30495;&#23454;&#30340;&#31070;&#32463;&#21160;&#21147;&#23398;&#65292;&#22312;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#19982;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#20449;&#24687;&#22312;SNN&#20013;&#20197;&#33033;&#20914;&#24418;&#24335;&#36827;&#34892;&#22788;&#29702;&#65292;&#37319;&#29992;&#20107;&#20214;&#39537;&#21160;&#26426;&#21046;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#33021;&#28304;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33033;&#20914;&#26426;&#21046;&#30340;&#38750;&#21487;&#24494;&#24615;&#65292;&#35757;&#32451;SNN&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#26102;&#38388;&#21453;&#21521;&#20256;&#25773;&#65288;BPTT&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#19968;&#23450;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#29983;&#29289;&#23398;&#19978;&#19981;&#21487;&#34892;&#12290;&#30456;&#21453;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#20855;&#26377;&#19981;&#21516;&#23616;&#37096;&#24615;&#30340;&#26367;&#20195;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26377;&#30456;&#20284;&#20043;&#22788;&#65292;&#21516;&#26102;&#22312;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;SNN&#30340;&#38544;&#24335;&#24490;&#29615;&#29305;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs), providing more realistic neuronal dynamics, have shown to achieve performance comparable to Artificial Neural Networks (ANNs) in several machine learning tasks. Information is processed as spikes within SNNs in an event-based mechanism that significantly reduces energy consumption. However, training SNNs is challenging due to the non-differentiable nature of the spiking mechanism. Traditional approaches, such as Backpropagation Through Time (BPTT), have shown effectiveness but comes with additional computational and memory costs and are biologically implausible. In contrast, recent works propose alternative learning methods with varying degrees of locality, demonstrating success in classification tasks. In this work, we show that these methods share similarities during the training process, while they present a trade-off between biological plausibility and performance. Further, this research examines the implicitly recurrent nature of SNNs and investigat
&lt;/p&gt;</description></item><item><title>CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00450</link><description>&lt;p&gt;
CPT: &#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#33021; &#21147;&#36882;&#36827;&#24335;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
CPT: Competence-progressive Training Strategy for Few-shot Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00450
&lt;/p&gt;
&lt;p&gt;
CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#25104;&#21151;&#20173;&#28982;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#33410;&#28857;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#65292;&#26631;&#31614;&#31232;&#30095;&#65292;&#24378;&#35843;&#20102;GNN&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20256;&#32479;&#30340;&#24773;&#33410;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#38480;&#21046;&#65306;&#38543;&#26426;&#21644;&#22343;&#21248;&#20219;&#21153;&#20998;&#37197;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#21040;&#27425;&#20248;&#35299;&#65292;&#24573;&#35270;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#27700;&#24179;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#20803;&#23398;&#20064;&#22120;&#36807;&#26089;&#22320;&#38754;&#20020;&#22797;&#26434;&#20219;&#21153;&#65292;&#38459;&#30861;&#20102;&#27491;&#24120;&#30340;&#23398;&#20064;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20803;&#23398;&#20064;&#22120;&#24212;&#35813;&#20174;&#31616;&#21333;&#27010;&#24565;&#24320;&#22987;&#65292;&#36880;&#28176;&#36827;&#20837;&#26356;&#22797;&#26434;&#30340;&#27010;&#24565;&#65292;&#23601;&#20687;&#20154;&#31867;&#23398;&#20064;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#38590;&#24230;&#19982;&#20803;&#23398;&#20064;&#22120;&#30340;&#36882;&#36827;&#33021;&#21147;&#30456;&#21305;&#37197;&#65292;&#22686;&#24378;&#20102;&#20803;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhanci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15952</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#24341;&#23548;&#25193;&#25955;&#21644;&#23884;&#22871;&#38598;&#25104;&#25913;&#36827;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Robustness and Reliability in Medical Image Classification with Latent-Guided Diffusion and Nested-Ensembles. (arXiv:2310.15952v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#26469;&#25913;&#21892;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#23545;&#23454;&#38469;&#24212;&#29992;&#20013;&#24120;&#35265;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#30495;&#23454;&#20020;&#24202;&#29615;&#22659;&#20013;&#37096;&#32626;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#23427;&#20204;&#23545;&#25152;&#33719;&#21462;&#30340;&#22270;&#20687;&#30340;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#35768;&#22810;&#26041;&#27861;&#20250;&#23545;&#35757;&#32451;&#25968;&#25454;&#24212;&#29992;&#39044;&#23450;&#20041;&#30340;&#36716;&#25442;&#65292;&#20197;&#22686;&#24378;&#27979;&#35797;&#26102;&#30340;&#40065;&#26834;&#24615;&#65292;&#20294;&#36825;&#20123;&#36716;&#25442;&#21487;&#33021;&#26080;&#27861;&#30830;&#20445;&#27169;&#22411;&#23545;&#24739;&#32773;&#22270;&#20687;&#20013;&#30340;&#22810;&#26679;&#24615;&#21464;&#24322;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21464;&#25442;&#22120;&#21644;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#19977;&#38454;&#27573;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#23545;&#23454;&#36341;&#20013;&#24120;&#35265;&#30340;&#25104;&#20687;&#21464;&#24322;&#24615;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#26080;&#38656;&#39044;&#20808;&#30830;&#23450;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22810;&#20010;&#22270;&#20687;&#32534;&#30721;&#22120;&#39318;&#20808;&#23398;&#20064;&#20998;&#23618;&#29305;&#24449;&#34920;&#31034;&#26469;&#26500;&#24314;&#36776;&#21035;&#28508;&#22312;&#31354;&#38388;&#12290;&#25509;&#19979;&#26469;&#65292;&#19968;&#20010;&#30001;&#28508;&#22312;&#20195;&#30721;&#24341;&#23548;&#30340;&#36870;&#25193;&#25955;&#36807;&#31243;&#20316;&#29992;&#20110;&#26377;&#20449;&#24687;&#20808;&#39564;&#65292;&#24182;&#25552;&#20986;&#39044;&#27979;&#20505;&#36873;&#12290;
&lt;/p&gt;
&lt;p&gt;
While deep learning models have achieved remarkable success across a range of medical image analysis tasks, deployment of these models in real clinical contexts requires that they be robust to variability in the acquired images. While many methods apply predefined transformations to augment the training data to enhance test-time robustness, these transformations may not ensure the model's robustness to the diverse variability seen in patient images. In this paper, we introduce a novel three-stage approach based on transformers coupled with conditional diffusion models, with the goal of improving model robustness to the kinds of imaging variability commonly encountered in practice without the need for pre-determined data augmentation strategies. To this end, multiple image encoders first learn hierarchical feature representations to build discriminative latent spaces. Next, a reverse diffusion process, guided by the latent code, acts on an informative prior and proposes prediction candi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2310.11594</link><description>&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Unhardening via Backdoor Attacks in Federated Learning. (arXiv:2310.11594v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#65292;&#36890;&#36807;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#30340;&#25968;&#25454;&#39537;&#21160;&#29615;&#22659;&#20013;&#65292;&#32500;&#25252;&#29992;&#25143;&#38544;&#31169;&#21644;&#37322;&#25918;&#25968;&#25454;&#28508;&#21147;&#20043;&#38388;&#24494;&#22937;&#30340;&#24179;&#34913;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#12290;&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20197;&#38544;&#31169;&#20026;&#20013;&#24515;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#23454;&#29616;&#20102;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#36825;&#31181;&#20998;&#25955;&#24335;&#26041;&#27861;&#24102;&#26469;&#20102;&#23433;&#20840;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24694;&#24847;&#23454;&#20307;&#27880;&#20837;&#25439;&#22351;&#25968;&#25454;&#30340;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26368;&#21021;&#21463;&#21040;&#27979;&#35797;&#26102;&#38388;&#36867;&#36991;&#25915;&#20987;&#30340;&#21551;&#21457;&#65292;&#25506;&#35752;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#20132;&#21449;&#28857;&#65292;&#24341;&#20837;&#20102;Adversarial Robustness Unhardening&#65288;ARU&#65289;&#12290;ARU&#34987;&#19968;&#37096;&#20998;&#23545;&#25163;&#20351;&#29992;&#65292;&#20197;&#26377;&#24847;&#20171;&#20837;&#20998;&#25955;&#24335;&#35757;&#32451;&#36807;&#31243;&#20013;&#30772;&#22351;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#20351;&#27169;&#22411;&#26356;&#23481;&#26131;&#21463;&#21040;&#26356;&#24191;&#27867;&#30340;&#36867;&#36991;&#25915;&#20987;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;ARU&#23545;&#23545;&#25239;&#24615;&#35757;&#32451;&#21644;&#29616;&#26377;&#30340;&#40065;&#26834;&#32858;&#21512;&#38450;&#24481;&#31574;&#30053;&#23545;&#20013;&#27602;&#21644;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In today's data-driven landscape, the delicate equilibrium between safeguarding user privacy and unleashing data potential stands as a paramount concern. Federated learning, which enables collaborative model training without necessitating data sharing, has emerged as a privacy-centric solution. This decentralized approach brings forth security challenges, notably poisoning and backdoor attacks where malicious entities inject corrupted data. Our research, initially spurred by test-time evasion attacks, investigates the intersection of adversarial training and backdoor attacks within federated learning, introducing Adversarial Robustness Unhardening (ARU). ARU is employed by a subset of adversaries to intentionally undermine model robustness during decentralized training, rendering models susceptible to a broader range of evasion attacks. We present extensive empirical experiments evaluating ARU's impact on adversarial training and existing robust aggregation defenses against poisoning a
&lt;/p&gt;</description></item><item><title>&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#65292;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#24456;&#23569;&#26159;&#29616;&#23454;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#31283;&#20581;&#20844;&#24179;&#30340;&#31639;&#27861;&#25512;&#33616;&#26469;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03647</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethinking Fairness for Human-AI Collaboration. (arXiv:2310.03647v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03647
&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#65292;&#38656;&#35201;&#37325;&#26032;&#24605;&#32771;&#20844;&#24179;&#24615;&#65292;&#22240;&#20026;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#24456;&#23569;&#26159;&#29616;&#23454;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#35774;&#35745;&#31283;&#20581;&#20844;&#24179;&#30340;&#31639;&#27861;&#25512;&#33616;&#26469;&#25552;&#21319;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26041;&#27861;&#26088;&#22312;&#30830;&#20445;&#20154;&#31867;&#20915;&#31574;&#32773;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#26102;&#23454;&#29616;&#20844;&#24179;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21512;&#20316;&#20013;&#65292;&#23436;&#20840;&#36981;&#23432;&#31639;&#27861;&#20915;&#31574;&#24456;&#23569;&#26159;&#29616;&#23454;&#25110;&#29702;&#24819;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20844;&#24179;&#31639;&#27861;&#30340;&#36873;&#25321;&#24615;&#36981;&#23432;&#20250;&#30456;&#23545;&#20110;&#20154;&#31867;&#20197;&#21069;&#30340;&#25919;&#31574;&#22686;&#21152;&#27495;&#35270;&#12290;&#22240;&#27492;&#65292;&#30830;&#20445;&#20844;&#24179;&#32467;&#26524;&#38656;&#35201;&#22522;&#26412;&#19981;&#21516;&#30340;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#65292;&#20197;&#30830;&#20445;&#23545;&#20915;&#31574;&#32773;&#65288;&#20107;&#20808;&#19981;&#30693;&#36947;&#65289;&#30340;&#36981;&#23432;&#27169;&#24335;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#36981;&#23432;&#31283;&#20581;&#20844;&#24179;&#30340;&#31639;&#27861;&#25512;&#33616;&#65292;&#26080;&#35770;&#20154;&#31867;&#30340;&#36981;&#23432;&#27169;&#24335;&#22914;&#20309;&#65292;&#23427;&#20204;&#37117;&#33021;&#30830;&#20445;&#22312;&#20915;&#31574;&#20013;&#25913;&#21892;&#20844;&#24179;&#24615;&#65288;&#24369;&#24418;&#24847;&#20041;&#19978;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20248;&#21270;&#31574;&#30053;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#24615;&#33021;&#25913;&#36827;&#36981;&#23432;&#31283;&#20581;&#20844;&#24179;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#35774;&#35745;&#31639;&#27861;&#25512;&#33616;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing approaches to algorithmic fairness aim to ensure equitable outcomes if human decision-makers comply perfectly with algorithmic decisions. However, perfect compliance with the algorithm is rarely a reality or even a desirable outcome in human-AI collaboration. Yet, recent studies have shown that selective compliance with fair algorithms can amplify discrimination relative to the prior human policy. As a consequence, ensuring equitable outcomes requires fundamentally different algorithmic design principles that ensure robustness to the decision-maker's (a priori unknown) compliance pattern. We define the notion of compliance-robustly fair algorithmic recommendations that are guaranteed to (weakly) improve fairness in decisions, regardless of the human's compliance pattern. We propose a simple optimization strategy to identify the best performance-improving compliance-robustly fair policy. However, we show that it may be infeasible to design algorithmic recommendations that are s
&lt;/p&gt;</description></item><item><title>&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;VCSC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#30697;&#38453;&#23384;&#20648;&#26684;&#24335;&#65292;&#33021;&#22815;&#21033;&#29992;&#39640;&#20887;&#20313;&#24615;&#23558;&#25968;&#25454;&#36827;&#19968;&#27493;&#21387;&#32553;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#22686;&#37327;&#32534;&#30721;&#21644;&#23383;&#33410;&#25171;&#21253;&#21387;&#32553;&#32034;&#24341;&#25968;&#32452;&#65292;IVCSC&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#33410;&#30465;&#12290;</title><link>http://arxiv.org/abs/2309.04355</link><description>&lt;p&gt;
&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;VCSC&#65289;&#65306;&#20887;&#20313;&#25968;&#25454;&#30340;&#31232;&#30095;&#30697;&#38453;&#23384;&#20648;
&lt;/p&gt;
&lt;p&gt;
Value-Compressed Sparse Column (VCSC): Sparse Matrix Storage for Redundant Data. (arXiv:2309.04355v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04355
&lt;/p&gt;
&lt;p&gt;
&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;VCSC&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#31232;&#30095;&#30697;&#38453;&#23384;&#20648;&#26684;&#24335;&#65292;&#33021;&#22815;&#21033;&#29992;&#39640;&#20887;&#20313;&#24615;&#23558;&#25968;&#25454;&#36827;&#19968;&#27493;&#21387;&#32553;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#27809;&#26377;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#36890;&#36807;&#22686;&#37327;&#32534;&#30721;&#21644;&#23383;&#33410;&#25171;&#21253;&#21387;&#32553;&#32034;&#24341;&#25968;&#32452;&#65292;IVCSC&#23454;&#29616;&#20102;&#26356;&#22823;&#30340;&#23384;&#20648;&#31354;&#38388;&#33410;&#30465;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;CSC&#65289;&#21644;&#22352;&#26631;&#65288;COO&#65289;&#26159;&#31232;&#30095;&#30697;&#38453;&#30340;&#24120;&#29992;&#21387;&#32553;&#26684;&#24335;&#12290;&#28982;&#32780;&#65292;CSC&#21644;COO&#37117;&#26159;&#36890;&#29992;&#26684;&#24335;&#65292;&#19981;&#33021;&#21033;&#29992;&#38500;&#31232;&#30095;&#24615;&#20197;&#22806;&#30340;&#25968;&#25454;&#29305;&#24615;&#65292;&#22914;&#25968;&#25454;&#20887;&#20313;&#24615;&#12290;&#39640;&#24230;&#20887;&#20313;&#30340;&#31232;&#30095;&#25968;&#25454;&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20363;&#22914;&#22522;&#22240;&#32452;&#23398;&#65292;&#22312;&#20256;&#32479;&#30340;&#31232;&#30095;&#23384;&#20648;&#26684;&#24335;&#19979;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#22826;&#22823;&#26080;&#27861;&#36827;&#34892;&#20869;&#23384;&#35745;&#31639;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25193;&#23637;&#30340;CSC&#26684;&#24335;&#65306;&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;VCSC&#65289;&#21644;&#32034;&#24341;&#21644;&#20540;&#21387;&#32553;&#30340;&#31232;&#30095;&#21015;&#65288;IVCSC&#65289;&#12290;VCSC&#21033;&#29992;&#21015;&#20869;&#30340;&#39640;&#20887;&#20313;&#24615;&#65292;&#23558;&#25968;&#25454;&#36827;&#19968;&#27493;&#21387;&#32553;&#20102;3&#20493;&#20197;&#19978;&#65292;&#30456;&#27604;COO&#21387;&#32553;&#20102;2.25&#20493;&#65292;&#32780;&#24615;&#33021;&#29305;&#24449;&#27809;&#26377;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;IVCSC&#36890;&#36807;&#22686;&#37327;&#32534;&#30721;&#21644;&#23383;&#33410;&#25171;&#21253;&#21387;&#32553;&#32034;&#24341;&#25968;&#32452;&#65292;&#20351;&#20869;&#23384;&#20351;&#29992;&#37327;&#27604;COO&#20943;&#23569;&#20102;10&#20493;&#65292;&#27604;CSC&#20943;&#23569;&#20102;7.5&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compressed Sparse Column (CSC) and Coordinate (COO) are popular compression formats for sparse matrices. However, both CSC and COO are general purpose and cannot take advantage of any of the properties of the data other than sparsity, such as data redundancy. Highly redundant sparse data is common in many machine learning applications, such as genomics, and is often too large for in-core computation using conventional sparse storage formats. In this paper, we present two extensions to CSC: (1) Value-Compressed Sparse Column (VCSC) and (2) Index- and Value-Compressed Sparse Column (IVCSC). VCSC takes advantage of high redundancy within a column to further compress data up to 3-fold over COO and 2.25-fold over CSC, without significant negative impact to performance characteristics. IVCSC extends VCSC by compressing index arrays through delta encoding and byte-packing, achieving a 10-fold decrease in memory usage over COO and 7.5-fold decrease over CSC. Our benchmarks on simulated and rea
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;</title><link>http://arxiv.org/abs/2305.16264</link><description>&lt;p&gt;
&#32553;&#25918;&#25968;&#25454;&#21463;&#38480;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Data-Constrained Language Models. (arXiv:2305.16264v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#30740;&#31350;&#20102;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#25193;&#23637;&#35821;&#35328;&#27169;&#22411;&#30340;&#36235;&#21183;&#28041;&#21450;&#22686;&#21152;&#21442;&#25968;&#35745;&#25968;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#25512;&#26029;&#36825;&#20010;&#36235;&#21183;&#34920;&#26126;&#65292;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#21487;&#33021;&#24456;&#24555;&#23601;&#20250;&#21463;&#21040;&#20114;&#32852;&#32593;&#19978;&#21487;&#29992;&#25991;&#26412;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#20986;&#20110;&#27492;&#38480;&#21046;&#30340;&#21160;&#26426;&#65292;&#25105;&#20204;&#30740;&#31350;&#22312;&#25968;&#25454;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#32553;&#25918;&#35821;&#35328;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36816;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#65292;&#21464;&#21270;&#25968;&#25454;&#37325;&#22797;&#31243;&#24230;&#21644;&#35745;&#31639;&#39044;&#31639;&#65292;&#33539;&#22260;&#36798;&#21040;&#20102;9000&#20159;&#20010;&#35757;&#32451;&#20196;&#29260;&#21644;9&#20159;&#21442;&#25968;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#39640;&#36798;4&#27425;&#37325;&#22797;&#25968;&#25454;&#30340;&#35757;&#32451;&#19982;&#20351;&#29992;&#21807;&#19968;&#25968;&#25454;&#30456;&#27604;&#23545;&#25439;&#22833;&#30340;&#36129;&#29486;&#24494;&#19981;&#36275;&#36947;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#26356;&#22810;&#30340;&#37325;&#22797;&#25968;&#25454;&#65292;&#28155;&#21152;&#35745;&#31639;&#30340;&#20215;&#20540;&#26368;&#32456;&#20250;&#34928;&#20943;&#20026;&#38646;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#35777;&#20102;&#19968;&#20010;&#35745;&#31639;&#26368;&#20248;&#24615;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#32771;&#34385;&#21040;&#37325;&#22797;&#20196;&#29260;&#21644;&#36807;&#37327;&#21442;&#25968;&#30340;&#20215;&#20540;&#36882;&#20943;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.10442</link><description>&lt;p&gt;
CBAGAN-RRT: &#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network for Sampling-Based Path Planning. (arXiv:2305.10442v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;RRT&#31639;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#21021;&#22987;&#36335;&#24452;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#19988;&#25910;&#25947;&#36895;&#24230;&#36807;&#24930;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#65292;&#20197;&#35774;&#35745;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;GAN&#27169;&#22411;&#29983;&#25104;&#30340;&#36335;&#24452;&#27010;&#29575;&#20998;&#24067;&#29992;&#20110;&#24341;&#23548;RRT&#31639;&#27861;&#30340;&#37319;&#26679;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#30001; \cite {zhang2021generative} &#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#65288;&#22914;IOU&#20998;&#25968;&#65292;Dice&#20998;&#25968;&#65289;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#65288;&#22914;&#36335;&#24452;&#38271;&#24230;&#21644;&#25104;&#21151;&#29575;&#65289;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling-based path planning algorithms play an important role in autonomous robotics. However, a common problem among the RRT-based algorithms is that the initial path generated is not optimal and the convergence is too slow to be used in real-world applications. In this paper, we propose a novel image-based learning algorithm (CBAGAN-RRT) using a Convolutional Block Attention Generative Adversarial Network with a combination of spatial and channel attention and a novel loss function to design the heuristics, find a better optimal path, and improve the convergence of the algorithm both concerning time and speed. The probability distribution of the paths generated from our GAN model is used to guide the sampling process for the RRT algorithm. We train and test our network on the dataset generated by \cite{zhang2021generative} and demonstrate that our algorithm outperforms the previous state-of-the-art algorithms using both the image quality generation metrics like IOU Score, Dice Score
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#26377;&#25928;&#22320;&#35745;&#31639;&#21644;&#23454;&#29616;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#21338;&#24328;&#31867;&#21035;&#20013;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#30340;&#31038;&#20250;&#31119;&#21033;&#65292;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.05005</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#21644;&#26080;&#24724;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Bayes correlated equilibria and no-regret dynamics. (arXiv:2304.05005v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#30340;&#19968;&#20010;&#27010;&#24565;&#65292;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#26377;&#25928;&#22320;&#35745;&#31639;&#21644;&#23454;&#29616;&#65292;&#24182;&#22312;&#24191;&#27867;&#30340;&#21338;&#24328;&#31867;&#21035;&#20013;&#36798;&#21040;&#36817;&#20284;&#26368;&#20248;&#30340;&#31038;&#20250;&#31119;&#21033;&#65292;&#22312;&#23454;&#39564;&#20013;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#21338;&#24328;&#30340;&#22343;&#34913;&#27010;&#24565;&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#22522;&#26412;&#21338;&#24328;&#27169;&#22411;&#12290;&#25105;&#20204;&#26088;&#22312;&#23454;&#29616;&#19977;&#31181;&#29702;&#24819;&#30340;&#24179;&#34913;&#24615;&#36136;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#22312;&#21338;&#24328;&#20013;&#24341;&#20837;&#35843;&#35299;&#32773;&#26469;&#33258;&#28982;&#22320;&#23454;&#29616;&#22343;&#34913;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#26377;&#25928;&#22320;&#35745;&#31639;&#22343;&#34913;&#12290;&#31532;&#19977;&#65292;&#23545;&#20110;&#24191;&#27867;&#30340;&#21338;&#24328;&#31867;&#21035;&#65292;&#35813;&#31867;&#22343;&#34913;&#36817;&#20284;&#22320;&#26368;&#22823;&#21270;&#31038;&#20250;&#31119;&#21033;&#65292;&#21363;&#36890;&#36807;&#28798;&#21464;&#20195;&#20215;&#26469;&#24230;&#37327;&#12290;&#36825;&#19977;&#31181;&#23646;&#24615;&#20801;&#35768;&#29609;&#23478;&#35745;&#31639;&#22343;&#34913;&#65292;&#24182;&#36890;&#36807;&#35843;&#35299;&#32773;&#20351;&#20854;&#23454;&#29616;&#65292;&#20174;&#32780;&#22312;&#36817;&#20046;&#26368;&#20248;&#30340;&#31038;&#20250;&#31119;&#21033;&#20013;&#36798;&#25104;&#31283;&#23450;&#29366;&#24577;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23384;&#22312;&#19968;&#20010;&#22343;&#34913;&#27010;&#24565;&#65292;&#28385;&#36275;&#36825;&#19977;&#31181;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#21508;&#31181;&#65288;&#19981;&#31561;&#20215;&#30340;&#65289;&#30456;&#20851;&#22343;&#34913;&#25193;&#23637;&#65292;&#32479;&#31216;&#20026;&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#20419;&#36827;&#29609;&#23478;&#20043;&#38388;&#20132;&#27969;&#30340;&#27807;&#36890;&#22343;&#34913;&#65288;&#20063;&#31216;&#20026;&#21327;&#35843;&#26426;&#21046;&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24724;&#21160;&#24577;&#65292;&#20197;&#20998;&#24067;&#24335;&#26041;&#24335;&#25910;&#25947;&#20110;&#36125;&#21494;&#26031;&#30456;&#20851;&#22343;&#34913;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#24179;&#34913;&#27010;&#24565;&#28385;&#36275;&#19977;&#31181;&#29702;&#24819;&#30340;&#24179;&#34913;&#24615;&#36136;&#65292;&#24182;&#21576;&#29616;&#20102;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores equilibrium concepts for Bayesian games, which are fundamental models of games with incomplete information. We aim at three desirable properties of equilibria. First, equilibria can be naturally realized by introducing a mediator into games. Second, an equilibrium can be computed efficiently in a distributed fashion. Third, any equilibrium in that class approximately maximizes social welfare, as measured by the price of anarchy, for a broad class of games. These three properties allow players to compute an equilibrium and realize it via a mediator, thereby settling into a stable state with approximately optimal social welfare. Our main result is the existence of an equilibrium concept that satisfies these three properties.  Toward this goal, we characterize various (non-equivalent) extensions of correlated equilibria, collectively known as Bayes correlated equilibria. In particular, we focus on communication equilibria (also known as coordination mechanisms), which 
&lt;/p&gt;</description></item></channel></rss>