<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#22312;&#30740;&#31350;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#35268;&#27169;&#29305;&#24615;&#26102;&#21457;&#29616;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25512;&#29702;&#39044;&#31639;&#19979;&#24448;&#24448;&#27604;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2404.01367</link><description>&lt;p&gt;
&#22823;&#24182;&#38750;&#24635;&#26159;&#26356;&#22909;&#65306;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#35268;&#27169;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bigger is not Always Better: Scaling Properties of Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01367
&lt;/p&gt;
&lt;p&gt;
&#22312;&#30740;&#31350;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#35268;&#27169;&#29305;&#24615;&#26102;&#21457;&#29616;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#22312;&#30456;&#21516;&#25512;&#29702;&#39044;&#31639;&#19979;&#24448;&#24448;&#27604;&#36739;&#22823;&#30340;&#27169;&#22411;&#26356;&#26377;&#25928;&#22320;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65288;LDMs&#65289;&#30340;&#35268;&#27169;&#29305;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#23427;&#20204;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#23613;&#31649;&#25913;&#36827;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#25512;&#29702;&#31639;&#27861;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#26377;&#25928;&#25552;&#21319;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#25928;&#29575;&#65292;&#20294;&#27169;&#22411;&#22823;&#23567;&#30340;&#20316;&#29992;&#8212;&#8212;&#37319;&#26679;&#25928;&#29575;&#30340;&#20851;&#38190;&#20915;&#23450;&#22240;&#32032;&#8212;&#8212;&#23578;&#26410;&#21463;&#21040;&#24443;&#24213;&#30340;&#23457;&#26597;&#12290;&#36890;&#36807;&#23545;&#24050;&#24314;&#31435;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#27169;&#22411;&#22823;&#23567;&#22914;&#20309;&#24433;&#21709;&#22312;&#19981;&#21516;&#37319;&#26679;&#27493;&#39588;&#19979;&#30340;&#37319;&#26679;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#36235;&#21183;&#65306;&#22312;&#32473;&#23450;&#25512;&#29702;&#39044;&#31639;&#19979;&#36816;&#34892;&#26102;&#65292;&#36739;&#23567;&#30340;&#27169;&#22411;&#32463;&#24120;&#32988;&#36807;&#20854;&#36739;&#22823;&#30340;&#31561;&#20215;&#29289;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#32467;&#26524;&#19978;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#24212;&#29992;&#21508;&#31181;&#25193;&#25955;&#37319;&#26679;&#22120;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#35780;&#20272;&#21518;&#31934;&#39311;&#27169;&#22411;&#65292;&#20197;&#21450;&#36827;&#34892;&#27604;&#36739;&#65292;&#26469;&#23637;&#31034;&#36825;&#20123;&#21457;&#29616;&#30340;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01367v1 Announce Type: cross  Abstract: We study the scaling properties of latent diffusion models (LDMs) with an emphasis on their sampling efficiency. While improved network architecture and inference algorithms have shown to effectively boost sampling efficiency of diffusion models, the role of model size -- a critical determinant of sampling efficiency -- has not been thoroughly examined. Through empirical analysis of established text-to-image diffusion models, we conduct an in-depth investigation into how model size influences sampling efficiency across varying sampling steps. Our findings unveil a surprising trend: when operating under a given inference budget, smaller models frequently outperform their larger equivalents in generating high-quality results. Moreover, we extend our study to demonstrate the generalizability of the these findings by applying various diffusion samplers, exploring diverse downstream tasks, evaluating post-distilled models, as well as compar
&lt;/p&gt;</description></item><item><title>&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17768</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#22797;&#26434;&#24615;&#21040;&#20844;&#20247;&#21465;&#20107;&#65306;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SciNews: From Scholarly Complexities to Public Narratives -- A Dataset for Scientific News Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17768
&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#25552;&#39640;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#23398;&#26415;&#20986;&#29256;&#29289;&#21644;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#20316;&#20026;&#19968;&#20010;&#26725;&#26753;&#65292;&#24039;&#22937;&#22320;&#23558;&#22797;&#26434;&#30340;&#30740;&#31350;&#25991;&#31456;&#32763;&#35793;&#25104;&#19982;&#26356;&#24191;&#27867;&#30340;&#20844;&#20247; resonant &#30340;&#25253;&#36947;&#12290;&#36825;&#31181;&#21465;&#20107;&#30340;&#33258;&#21160;&#29983;&#25104;&#22686;&#24378;&#20102;&#23398;&#26415;&#35265;&#35299;&#30340;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35821;&#26009;&#24211;&#26469;&#20419;&#36827;&#36825;&#31181;&#33539;&#24335;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#35821;&#26009;&#24211;&#21253;&#25324;&#20061;&#20010;&#23398;&#31185;&#39046;&#22495;&#20013;&#23398;&#26415;&#20986;&#29256;&#29289;&#21450;&#20854;&#30456;&#24212;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#30340;&#24179;&#34892;&#32534;&#35793;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#25968;&#25454;&#38598;&#30340;&#23454;&#29992;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#31185;&#23398;&#26032;&#38395;&#21465;&#20107;&#21644;&#23398;&#26415;&#25991;&#31295;&#20043;&#38388;&#30340;&#21487;&#35835;&#24615;&#21644;&#31616;&#27905;&#24615;&#24046;&#24322;&#12290;&#25105;&#20204;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#36807;&#31243;&#21253;&#25324;&#33258;&#21160;&#35780;&#20272;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#20026;&#26410;&#26469;&#25506;&#32034;&#33258;&#21160;&#29983;&#25104;&#31185;&#23398;&#26032;&#38395;&#25253;&#36947;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17768v1 Announce Type: cross  Abstract: Scientific news reports serve as a bridge, adeptly translating complex research articles into reports that resonate with the broader public. The automated generation of such narratives enhances the accessibility of scholarly insights. In this paper, we present a new corpus to facilitate this paradigm development. Our corpus comprises a parallel compilation of academic publications and their corresponding scientific news reports across nine disciplines. To demonstrate the utility and reliability of our dataset, we conduct an extensive analysis, highlighting the divergences in readability and brevity between scientific news narratives and academic manuscripts. We benchmark our dataset employing state-of-the-art text generation models. The evaluation process involves both automatic and human evaluation, which lays the groundwork for future explorations into the automated generation of scientific news reports. The dataset and code related 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#28151;&#21512;LiFi&#21644;WiFi&#32593;&#32476;&#20013;&#22522;&#20110;&#29992;&#25143;&#20013;&#24515;&#30340;&#36127;&#36733;&#24179;&#34913;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#32593;&#32476;&#20013;&#24515;&#21270;&#26041;&#27861;&#22312;&#22788;&#29702;&#31227;&#21160;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16823</link><description>&lt;p&gt;
&#28151;&#21512;LiFi&#21644;WiFi&#32593;&#32476;&#20013;&#30340;&#36164;&#28304;&#21644;&#31227;&#21160;&#31649;&#29702;&#65306;&#22522;&#20110;&#29992;&#25143;&#20013;&#24515;&#30340;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resource and Mobility Management in Hybrid LiFi and WiFi Networks: A User-Centric Learning Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16823
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#28151;&#21512;LiFi&#21644;WiFi&#32593;&#32476;&#20013;&#22522;&#20110;&#29992;&#25143;&#20013;&#24515;&#30340;&#36127;&#36733;&#24179;&#34913;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#32593;&#32476;&#20013;&#24515;&#21270;&#26041;&#27861;&#22312;&#22788;&#29702;&#31227;&#21160;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#20809;&#36890;&#20449;&#65288;LiFi&#65289;&#21644;&#26080;&#32447;&#23616;&#22495;&#32593;&#65288;WiFi&#65289;&#32593;&#32476;&#65288;HLWNets&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#23460;&#20869;&#26080;&#32447;&#36890;&#20449;&#33539;&#24335;&#65292;&#32467;&#21512;&#20102;LiFi&#30340;&#23485;&#23637;&#20809;&#35889;&#20248;&#21183;&#21644;WiFi&#30340;&#26080;&#22788;&#19981;&#22312;&#30340;&#35206;&#30422;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36127;&#36733;&#24179;&#34913;&#65288;LB&#65289;&#25104;&#20026;&#36825;&#31181;&#28151;&#21512;&#32593;&#32476;&#36164;&#28304;&#31649;&#29702;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#36127;&#36733;&#24179;&#34913;&#26041;&#27861;&#22823;&#22810;&#26159;&#32593;&#32476;&#20013;&#24515;&#21270;&#30340;&#65292;&#20381;&#36182;&#20013;&#22830;&#21333;&#20803;&#19968;&#27425;&#24615;&#20026;&#25152;&#26377;&#29992;&#25143;&#21046;&#23450;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#19981;&#35770;&#29992;&#25143;&#30340;&#31227;&#21160;&#29366;&#24577;&#22914;&#20309;&#65292;&#35299;&#20915;&#26041;&#26696;&#37117;&#38656;&#35201;&#21516;&#27493;&#26356;&#26032;&#25152;&#26377;&#29992;&#25143;&#12290;&#36825;&#20250;&#24433;&#21709;&#32593;&#32476;&#24615;&#33021;&#30340;&#20004;&#20010;&#26041;&#38754;&#65306;i&#65289;&#24403;&#26356;&#26032;&#39057;&#29575;&#36739;&#20302;&#26102;&#65292;&#20250;&#24433;&#21709;&#24555;&#36895;&#31227;&#21160;&#29992;&#25143;&#30340;&#36830;&#25509;&#24615;&#65307;ii&#65289;&#24403;&#26356;&#26032;&#39057;&#29575;&#36739;&#39640;&#26102;&#65292;&#20250;&#23548;&#33268;&#23545;&#20110;&#24930;&#36895;&#31227;&#21160;&#29992;&#25143;&#19981;&#24517;&#35201;&#30340;&#20999;&#25442;&#20197;&#21450;&#24040;&#22823;&#30340;&#21453;&#39304;&#25104;&#26412;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20801;&#35768;&#29992;&#25143;&#26356;&#26032;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16823v1 Announce Type: cross  Abstract: Hybrid light fidelity (LiFi) and wireless fidelity (WiFi) networks (HLWNets) are an emerging indoor wireless communication paradigm, which combines the advantages of the capacious optical spectra of LiFi and ubiquitous coverage of WiFi. Meanwhile, load balancing (LB) becomes a key challenge in resource management for such hybrid networks. The existing LB methods are mostly network-centric, relying on a central unit to make a solution for the users all at once. Consequently, the solution needs to be updated for all users at the same pace, regardless of their moving status. This would affect the network performance in two aspects: i) when the update frequency is low, it would compromise the connectivity of fast-moving users; ii) when the update frequency is high, it would cause unnecessary handovers as well as hefty feedback costs for slow-moving users. Motivated by this, we investigate user-centric LB which allows users to update their 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26356;&#38590;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09472</link><description>&lt;p&gt;
&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#65306;&#36229;&#36234;&#20154;&#31867;&#30417;&#30563;&#30340;&#21487;&#25193;&#23637;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Easy-to-Hard Generalization: Scalable Alignment Beyond Human Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09472
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#23398;&#20064;&#65292;&#23454;&#29616;&#23545;&#26356;&#38590;&#25512;&#29702;&#20219;&#21153;&#30340;&#26377;&#25928;&#27867;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#27861;&#20381;&#36182;&#20110;&#20154;&#31867;&#25552;&#20379;&#30340;&#28436;&#31034;&#25110;&#21028;&#26029;&#65292;&#30001;&#20110;&#36825;&#31181;&#26041;&#27861;&#65292;AI&#31995;&#32479;&#23398;&#20064;&#21040;&#30340;&#33021;&#21147;&#23558;&#21463;&#21040;&#20154;&#31867;&#33021;&#21147;&#30340;&#19978;&#30028;&#38480;&#21046;&#12290;&#36825;&#23601;&#24102;&#26469;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#24403;&#31995;&#32479;&#30340;&#33021;&#21147;&#36229;&#36807;&#20154;&#31867;&#27700;&#24179;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#32487;&#32493;&#25913;&#36827;&#36825;&#20123;&#31995;&#32479;&#65311;&#26412;&#25991;&#22312;&#35299;&#20915;&#38590;&#24230;&#25512;&#29702;&#20219;&#21153;&#65288;&#22914;4-5&#32423;&#25968;&#23398;&#38382;&#39064;&#65289;&#30340;&#32972;&#26223;&#19979;&#22238;&#31572;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#20174;&#26356;&#31616;&#21333;&#30340;&#20219;&#21153;&#65288;&#22914;1-3&#32423;&#25968;&#23398;&#38382;&#39064;&#65289;&#20013;&#23398;&#20064;&#20154;&#31867;&#27880;&#37322;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#8220;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#8221;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#28857;&#26159;&#65292;&#19968;&#20010;&#22312;&#26356;&#31616;&#21333;&#20219;&#21153;&#30340;&#30417;&#30563;&#19979;&#35757;&#32451;&#30340;&#35780;&#20272;&#22120;&#65288;&#22870;&#21169;&#27169;&#22411;&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#29992;&#20110;&#35780;&#20998;&#26356;&#38590;&#20219;&#21153;&#30340;&#20505;&#36873;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#20419;&#36827;&#22312;&#19981;&#21516;&#38590;&#24230;&#20219;&#21153;&#38388;&#30340;&#26131;&#20110;&#38590;&#30340;&#27867;&#21270;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#23545;&#40784;&#26041;&#27861;&#65292;&#39318;&#20808;&#35757;&#32451;&#22788;&#29702;&#30563;&#23548;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09472v1 Announce Type: cross  Abstract: Current AI alignment methodologies rely on human-provided demonstrations or judgments, and the learned capabilities of AI systems would be upper-bounded by human capabilities as a result. This raises a challenging research question: How can we keep improving the systems when their capabilities have surpassed the levels of humans? This paper answers this question in the context of tackling hard reasoning tasks (e.g., level 4-5 MATH problems) via learning from human annotations on easier tasks (e.g., level 1-3 MATH problems), which we term as \textit{easy-to-hard generalization}. Our key insight is that an evaluator (reward model) trained on supervisions for easier tasks can be effectively used for scoring candidate solutions of harder tasks and hence facilitating easy-to-hard generalization over different levels of tasks. Based on this insight, we propose a novel approach to scalable alignment, which firstly trains the process-supervise
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#29983;&#25104;/&#20248;&#21270;&#20219;&#21153;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.03425</link><description>&lt;p&gt;
&#22312;3D&#20013;&#22609;&#36896;&#20998;&#23376;&#65306;&#38754;&#21521;&#25991;&#26412;&#30340;&#20998;&#23376;&#20248;&#21270;&#28789;&#27963;&#23376;&#32467;&#26500;&#24863;&#30693;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Sculpting Molecules in 3D: A Flexible Substructure Aware Framework for Text-Oriented Molecular Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03425
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#27169;&#24577;&#24341;&#23548;&#29983;&#25104;/&#20248;&#21270;&#20219;&#21153;&#35299;&#20915;&#20998;&#23376;&#35774;&#35745;&#38382;&#39064;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#28145;&#24230;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;AI-Generated Content&#65292;&#19982;&#20174;&#31532;&#19968;&#21407;&#29702;&#35745;&#31639;&#20013;&#24471;&#20986;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#25104;&#20026;&#25913;&#21464;&#31185;&#23398;&#30740;&#31350;&#26684;&#23616;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#26082;&#21253;&#21547;&#22810;&#27169;&#24577;&#20808;&#39564;&#30693;&#35782;&#21448;&#20855;&#26377;&#20851;&#38190;&#21644;&#22797;&#26434;&#24615;&#30340;&#20998;&#23376;&#33647;&#29289;&#25110;&#26448;&#26009;&#30340;&#25361;&#25112;&#20381;&#28982;&#26159;&#19968;&#39033;&#20851;&#38190;&#32780;&#22797;&#26434;&#30340;&#24037;&#20316;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#19968;&#36870;&#35774;&#35745;&#38382;&#39064;&#65292;&#23558;&#20854;&#26500;&#36896;&#20026;&#19968;&#31181;&#22810;&#27169;&#24577;&#23548;&#21521;&#29983;&#25104;/&#20248;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#19968;&#20010;&#38754;&#21521;&#25991;&#26412;-&#32467;&#26500;&#23545;&#40784;&#30340;&#23545;&#31216;&#25193;&#25955;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#20998;&#23376;&#29983;&#25104;/&#20248;&#21270;&#20219;&#21153;&#65292;&#21363;3DToMolo.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03425v1 Announce Type: new  Abstract: The integration of deep learning, particularly AI-Generated Content, with high-quality data derived from ab initio calculations has emerged as a promising avenue for transforming the landscape of scientific research. However, the challenge of designing molecular drugs or materials that incorporate multi-modality prior knowledge remains a critical and complex undertaking. Specifically, achieving a practical molecular design necessitates not only meeting the diversity requirements but also addressing structural and textural constraints with various symmetries outlined by domain experts. In this article, we present an innovative approach to tackle this inverse design problem by formulating it as a multi-modality guidance generation/optimization task. Our proposed solution involves a textural-structure alignment symmetric diffusion framework for the implementation of molecular generation/optimization tasks, namely 3DToMolo. 3DToMolo aims to 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.12921</link><description>&lt;p&gt;
&#20934;&#26102;&#21040;&#20301;&#65306;&#36890;&#36807;&#38480;&#21046;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#35299;&#37322;&#26469;&#20462;&#35746;&#23427;&#20204;
&lt;/p&gt;
&lt;p&gt;
Right on Time: Revising Time Series Models by Constraining their Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12921
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#32463;&#24120;&#20250;&#21463;&#21040;&#20854;&#20381;&#36182;&#28151;&#26434;&#22240;&#32032;&#30340;&#20542;&#21521;&#30340;&#25439;&#23475;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26032;&#35760;&#24405;&#30340;&#12289;&#33258;&#28982;&#28151;&#26434;&#30340;&#25968;&#25454;&#38598;P2S&#26469;&#33258;&#30495;&#23454;&#30340;&#26426;&#26800;&#29983;&#20135;&#32447;&#65292;&#24378;&#35843;&#20102;&#36825;&#19968;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#28151;&#26434;&#22240;&#32032;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20934;&#26102;&#21040;&#20301;&#65288;RioT&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#27169;&#22411;&#35299;&#37322;&#22312;&#26102;&#38388;&#21644;&#39057;&#29575;&#22495;&#20043;&#38388;&#36827;&#34892;&#20132;&#20114;&#12290;&#28982;&#21518;&#21033;&#29992;&#20004;&#20010;&#22495;&#20869;&#30340;&#35299;&#37322;&#21453;&#39304;&#26469;&#32422;&#26463;&#27169;&#22411;&#65292;&#20351;&#20854;&#36828;&#31163;&#26631;&#27880;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#22312;&#22788;&#29702;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#20013;&#28151;&#26434;&#22240;&#32032;&#26041;&#38754;&#65292;&#21452;&#22495;&#20132;&#20114;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20973;&#32463;&#39564;&#35777;&#26126;&#65292;RioT&#33021;&#22815;&#26377;&#25928;&#22320;&#24341;&#23548;&#27169;&#22411;&#36828;&#31163;P2S&#20197;&#21450;&#27969;&#34892;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#39044;&#27979;&#25968;&#25454;&#38598;&#20013;&#30340;&#38169;&#35823;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12921v1 Announce Type: new  Abstract: The reliability of deep time series models is often compromised by their tendency to rely on confounding factors, which may lead to misleading results. Our newly recorded, naturally confounded dataset named P2S from a real mechanical production line emphasizes this. To tackle the challenging problem of mitigating confounders in time series data, we introduce Right on Time (RioT). Our method enables interactions with model explanations across both the time and frequency domain. Feedback on explanations in both domains is then used to constrain the model, steering it away from the annotated confounding factors. The dual-domain interaction strategy is crucial for effectively addressing confounders in time series datasets. We empirically demonstrate that RioT can effectively guide models away from the wrong reasons in P2S as well as popular time series classification and forecasting datasets.
&lt;/p&gt;</description></item><item><title>Higgs&#37492;&#21035;&#30340;&#24341;&#23548;&#37327;&#23376;&#21387;&#32553;&#27169;&#22411;&#23558;&#39044;&#22788;&#29702;&#21644;&#37327;&#23376;&#20998;&#31867;&#31639;&#27861;&#32479;&#19968;&#20026;&#21487;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23548;&#33268;&#20998;&#31867;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#37492;&#21035;LHC&#20013;&#30340;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#12290;</title><link>https://arxiv.org/abs/2402.09524</link><description>&lt;p&gt;
Higgs&#37492;&#21035;&#30340;&#24341;&#23548;&#37327;&#23376;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Guided Quantum Compression for Higgs Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09524
&lt;/p&gt;
&lt;p&gt;
Higgs&#37492;&#21035;&#30340;&#24341;&#23548;&#37327;&#23376;&#21387;&#32553;&#27169;&#22411;&#23558;&#39044;&#22788;&#29702;&#21644;&#37327;&#23376;&#20998;&#31867;&#31639;&#27861;&#32479;&#19968;&#20026;&#21487;&#35757;&#32451;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#23548;&#33268;&#20998;&#31867;&#24615;&#33021;&#38477;&#20302;&#30340;&#38382;&#39064;&#65292;&#33021;&#22815;&#26377;&#25928;&#37492;&#21035;LHC&#20013;&#30340;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.09524v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#26412;&#26032;&#39062;&#19988;&#26377;&#21069;&#26223;&#30340;&#25968;&#25454;&#20998;&#26512;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#25968;&#25454;&#38598;&#23545;&#24403;&#21069;&#21487;&#29992;&#30340;&#37327;&#23376;&#35745;&#31639;&#26426;&#26469;&#35828;&#36807;&#20110;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#19978;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#36890;&#36807;&#20351;&#29992;&#38477;&#32500;&#31639;&#27861;&#65288;&#22914;&#33258;&#21160;&#32534;&#30721;&#22120;&#65289;&#22312;&#36890;&#36807;&#37327;&#23376;&#27169;&#22411;&#20043;&#21069;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#32463;&#20856;&#33258;&#21160;&#32534;&#30721;&#22120;&#20316;&#20026;&#29420;&#31435;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#23558;&#39044;&#22788;&#29702;&#21644;&#37327;&#23376;&#20998;&#31867;&#31639;&#27861;&#32479;&#19968;&#21040;&#21333;&#20010;&#21487;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#26550;&#26500;&#65306;&#24341;&#23548;&#37327;&#23376;&#21387;&#32553;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#35813;&#27169;&#22411;&#22312;LHC&#30340;&#36136;&#23376;-&#36136;&#23376;&#30896;&#25758;&#20013;&#37492;&#21035;&#24076;&#26684;&#26031;&#29627;&#33394;&#23376;&#30340;&#23454;&#29992;&#24615;&#24471;&#21040;&#20102;&#35777;&#26126;&#65292;&#32780;&#20256;&#32479;&#26041;&#27861;&#21017;&#26080;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09524v1 Announce Type: cross  Abstract: Quantum machine learning provides a fundamentally novel and promising approach to analyzing data. However, many data sets are too complex for currently available quantum computers. Consequently, quantum machine learning applications conventionally resort to dimensionality reduction algorithms, e.g., auto-encoders, before passing data through the quantum models. We show that using a classical auto-encoder as an independent preprocessing step can significantly decrease the classification performance of a quantum machine learning algorithm. To ameliorate this issue, we design an architecture that unifies the preprocessing and quantum classification algorithms into a single trainable model: the guided quantum compression model. The utility of this model is demonstrated by using it to identify the Higgs boson in proton-proton collisions at the LHC, where the conventional approach proves ineffective. Conversely, the guided quantum compressio
&lt;/p&gt;</description></item><item><title>&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#36825;&#31181;&#25200;&#21160;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36755;&#20837;&#32500;&#24230;&#36229;&#36807;&#36755;&#20986;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#36830;&#32493;&#30340;&#26080;&#23475;&#25200;&#21160;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#19968;&#26063;&#36890;&#29992;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#19968;&#33268;&#22320;&#24433;&#21709;&#32593;&#32476;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25200;&#21160;&#21487;&#33021;&#19981;&#20250;&#24433;&#21709;&#20854;&#35782;&#21035;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02095</link><description>&lt;p&gt;
&#30524;&#35265;&#26410;&#24517;&#20026;&#23454;&#65306;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Seeing is not always believing: The Space of Harmless Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02095
&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#36825;&#31181;&#25200;&#21160;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36755;&#20837;&#32500;&#24230;&#36229;&#36807;&#36755;&#20986;&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#36830;&#32493;&#30340;&#26080;&#23475;&#25200;&#21160;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#36824;&#35299;&#20915;&#20102;&#19968;&#26063;&#36890;&#29992;&#25200;&#21160;&#65292;&#36825;&#20123;&#25200;&#21160;&#19968;&#33268;&#22320;&#24433;&#21709;&#32593;&#32476;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20154;&#31867;&#35748;&#20026;&#37325;&#35201;&#30340;&#25200;&#21160;&#21487;&#33021;&#19981;&#20250;&#24433;&#21709;&#20854;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#21363;&#25200;&#21160;&#20250;&#20351;&#32593;&#32476;&#36755;&#20986;&#23436;&#20840;&#19981;&#21464;&#12290;&#26080;&#35770;&#36825;&#20123;&#25200;&#21160;&#22312;&#24212;&#29992;&#20110;&#22270;&#20687;&#26102;&#30340;&#22823;&#23567;&#22914;&#20309;&#65292;&#21482;&#35201;&#23427;&#20204;&#20301;&#20110;&#26080;&#23475;&#25200;&#21160;&#31354;&#38388;&#20869;&#65292;&#23601;&#19981;&#20250;&#23545;&#21407;&#22987;&#22270;&#20687;&#30340;&#32593;&#32476;&#36755;&#20986;&#20135;&#29983;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#32593;&#32476;&#20013;&#30340;&#20219;&#20309;&#32447;&#24615;&#23618;&#65292;&#36755;&#20837;&#32500;&#24230;$n$&#36229;&#36807;&#36755;&#20986;&#32500;&#24230;$m$&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36830;&#32493;&#26080;&#23475;&#25200;&#21160;&#23376;&#31354;&#38388;&#30340;&#23384;&#22312;&#65292;&#20854;&#32500;&#24230;&#20026;$(n-m)$&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#26063;&#19968;&#33268;&#24433;&#21709;&#32593;&#32476;&#36755;&#20986;&#30340;&#36890;&#29992;&#25200;&#21160;&#65292;&#32780;&#19981;&#35770;&#23427;&#20204;&#30340;&#22823;&#23567;&#22914;&#20309;&#12290;&#22522;&#20110;&#36825;&#20123;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#23475;&#25200;&#21160;&#22312;&#20445;&#25252;&#38544;&#31169;&#25968;&#25454;&#20351;&#29992;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21363;&#34987;&#20154;&#31867;&#25429;&#25417;&#21040;&#30340;&#37325;&#35201;&#25200;&#21160;&#21487;&#33021;&#19981;&#20250;&#24433;&#21709;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35782;&#21035;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of deep neural networks, we expose the existence of a harmless perturbation space, where perturbations leave the network output entirely unaltered. Perturbations within this harmless perturbation space, regardless of their magnitude when applied to images, exhibit no impact on the network's outputs of the original images. Specifically, given any linear layer within the network, where the input dimension $n$ exceeds the output dimension $m$, we demonstrate the existence of a continuous harmless perturbation subspace with a dimension of $(n-m)$. Inspired by this, we solve for a family of general perturbations that consistently influence the network output, irrespective of their magnitudes. With these theoretical findings, we explore the application of harmless perturbations for privacy-preserving data usage. Our work reveals the difference between DNNs and human perception that the significant perturbations captured by humans may not affect the recognition of DNNs. As a re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2402.01865</link><description>&lt;p&gt;
&#25105;&#30340;&#27169;&#22411;&#20250;&#24536;&#35760;&#20160;&#20040;&#65311;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20013;&#30340;&#34987;&#36951;&#24536;&#23454;&#20363;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#36951;&#24536;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#19978;&#28216;&#23454;&#20363;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#20197;&#25913;&#36827;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#26681;&#25454;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#21464;&#21270;&#19982;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#27492;&#22806;&#65292;&#36824;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#35821;&#35328;&#27169;&#22411;&#20250;&#20986;&#29616;&#38169;&#35823;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#36890;&#36807;&#23558;&#27169;&#22411;&#26356;&#26032;&#20026;&#32416;&#27491;&#38169;&#35823;&#23454;&#20363;&#65292;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;&#36951;&#24536;&#65292;&#26356;&#26032;&#21518;&#30340;&#27169;&#22411;&#22312;&#25351;&#23548;&#24494;&#35843;&#25110;&#19978;&#28216;&#35757;&#32451;&#38454;&#27573;&#20013;&#23398;&#21040;&#30340;&#23454;&#20363;&#19978;&#20986;&#29616;&#38169;&#35823;&#12290;&#38543;&#26426;&#37325;&#25773;&#19978;&#28216;&#25968;&#25454;&#30340;&#25928;&#26524;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#24448;&#24448;&#20276;&#38543;&#30528;&#36739;&#39640;&#30340;&#26041;&#24046;&#21644;&#36739;&#24046;&#30340;&#21487;&#25511;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#37325;&#25773;&#36807;&#31243;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#35797;&#22270;&#39044;&#27979;&#30001;&#20110;&#27169;&#22411;&#26356;&#26032;&#32780;&#36951;&#24536;&#30340;&#19978;&#28216;&#23454;&#20363;&#12290;&#25105;&#20204;&#26681;&#25454;&#19968;&#32452;&#22312;&#32447;&#23398;&#20064;&#30340;&#23454;&#20363;&#21644;&#30456;&#24212;&#34987;&#36951;&#24536;&#30340;&#19978;&#28216;&#39044;&#35757;&#32451;&#23454;&#20363;&#35757;&#32451;&#39044;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37096;&#20998;&#21487;&#35299;&#37322;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#36825;&#26679;&#30340;&#35266;&#23519;&#32467;&#26524;&#65306;&#39044;&#35757;&#32451;&#23454;&#20363;&#30340;&#39044;-softmax&#23545;&#25968;&#20960;&#29575;&#20998;&#25968;&#30340;&#21464;&#21270;&#31867;&#20284;&#20110;&#22312;&#32447;&#23398;&#20064;&#23454;&#20363;&#30340;&#21464;&#21270;&#65292;&#36825;&#22312;BART&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#19981;&#38169;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;T5&#27169;&#22411;&#19978;&#22833;&#36133;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#22522;&#20110;&#20869;&#31215;&#30340;&#40657;&#30418;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Language models deployed in the wild make errors. However, simply updating the model with the corrected error instances causes catastrophic forgetting -- the updated model makes errors on instances learned during the instruction tuning or upstream training phase. Randomly replaying upstream data yields unsatisfactory performance and often comes with high variance and poor controllability. To this end, we try to forecast upstream examples that will be forgotten due to a model update for improved controllability of the replay process and interpretability. We train forecasting models given a collection of online learned examples and corresponding forgotten upstream pre-training examples. We propose a partially interpretable forecasting model based on the observation that changes in pre-softmax logit scores of pretraining examples resemble that of online learned examples, which performs decently on BART but fails on T5 models. We further show a black-box classifier based on inner products 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22788;&#29702;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#20551;&#35774;&#36716;&#31227;&#21407;&#22240;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#26410;&#30693;&#30340;&#23646;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20998;&#23618;&#25277;&#26679;&#30340;&#26694;&#26550;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#30475;&#20316;&#20998;&#24067;&#22806;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>https://arxiv.org/abs/2311.18575</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#65306;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Class Distribution Shifts in Zero-Shot Learning: Learning Robust Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18575
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#22788;&#29702;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#20551;&#35774;&#36716;&#31227;&#21407;&#22240;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26159;&#26410;&#30693;&#30340;&#23646;&#24615;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20998;&#23618;&#25277;&#26679;&#30340;&#26694;&#26550;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#65292;&#25105;&#20204;&#33021;&#22815;&#23558;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#30475;&#20316;&#20998;&#24067;&#22806;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#23545;&#38646;&#26679;&#26412;&#20998;&#31867;&#22120;&#26469;&#35828;&#23588;&#20026;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#20174;&#35757;&#32451;&#31867;&#21035;&#23398;&#21040;&#30340;&#34920;&#31034;&#65292;&#20294;&#37096;&#32626;&#22312;&#26032;&#30340;&#12289;&#26410;&#30693;&#30340;&#31867;&#21035;&#19978;&#12290;&#24120;&#35265;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#21407;&#22240;&#26159;&#19982;&#31867;&#21035;&#30456;&#20851;&#30340;&#23646;&#24615;&#30340;&#25913;&#21464;&#65292;&#27604;&#22914;&#22312;&#20154;&#29289;&#35782;&#21035;&#20013;&#30340;&#31181;&#26063;&#25110;&#24615;&#21035;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#20010;&#37319;&#29992;&#36825;&#20010;&#35774;&#32622;&#30340;&#27169;&#22411;&#65292;&#20551;&#35774;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#30693;&#23548;&#33268;&#36716;&#31227;&#30340;&#23646;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#23398;&#20064;&#23545;&#36825;&#31181;&#36716;&#31227;&#40065;&#26834;&#30340;&#25968;&#25454;&#34920;&#31034;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#25277;&#26679;&#30340;&#26694;&#26550;&#26469;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#29615;&#22659;&#12290;&#23613;&#31649;&#20004;&#31181;&#35774;&#32622;&#20043;&#38388;&#23384;&#22312;&#20851;&#38190;&#24046;&#24322;&#65292;&#20294;&#36825;&#20010;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#20998;&#24067;&#36716;&#31227;&#36716;&#21270;&#20026;&#20998;&#24067;&#22806;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#40065;&#26834;&#34920;&#31034;&#30340;&#31639;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#19981;&#21516;&#31867;&#21035;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class distribution shifts are particularly challenging for zero-shot classifiers, which rely on representations learned from training classes but are deployed on new, unseen ones. Common causes for such shifts are changes in attributes associated with classes, such as race or gender in person identification. In this work, we propose and analyze a model that adopts this setting, assuming that the attribute responsible for the shift is unknown during training. To address the challenge of learning data representations robust to such shifts, we introduce a framework based on hierarchical sampling to construct synthetic data environments. Despite key differences between the settings, this framework allows us to formulate class distribution shifts in zero-shot learning as out-of-distribution problems. Consequently, we present an algorithm for learning robust representations, and show that our approach significantly improves generalization to diverse class distributions in both simulations an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#21305;&#37197;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#27493;&#38271;&#21442;&#25968;&#21644;&#21160;&#24577;softassign&#31574;&#30053;&#65292;&#33021;&#22815;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2208.08233</link><description>&lt;p&gt;
&#22270;&#21305;&#37197;&#30340;&#21160;&#24577;softassign&#21644;&#33258;&#36866;&#24212;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Dynamical softassign and adaptive parameter tuning for graph matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.08233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#21305;&#37197;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#33258;&#36866;&#24212;&#27493;&#38271;&#21442;&#25968;&#21644;&#21160;&#24577;softassign&#31574;&#30053;&#65292;&#33021;&#22815;&#25552;&#39640;&#25910;&#25947;&#24615;&#21644;&#25928;&#29575;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#31216;&#20026;&#32422;&#26463;&#26799;&#24230;&#26041;&#27861;&#30340;&#22270;&#21305;&#37197;&#38382;&#39064;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#35813;&#26694;&#26550;&#20869;&#30340;&#27969;&#34892;&#31639;&#27861;&#21253;&#25324;&#36882;&#24402;&#20998;&#37197;&#65288;GA&#65289;&#12289;&#25972;&#25968;&#25237;&#24433;&#22266;&#23450;&#28857;&#27861;&#65288;IPFP&#65289;&#21644;&#21452;&#38543;&#26426;&#25237;&#24433;&#22266;&#23450;&#28857;&#27861;&#65288;DSPFP&#65289;&#12290; &#36825;&#20123;&#31639;&#27861;&#22312;&#27493;&#38271;&#21442;&#25968;&#21644;&#32422;&#26463;&#31639;&#23376;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#27493;&#38271;&#21442;&#25968;&#21487;&#20197;&#20445;&#35777;&#22522;&#30784;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#22686;&#24378;&#23427;&#20204;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;&#21021;&#27493;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#23436;&#20840;&#36830;&#25509;&#30340;&#22270;&#21305;&#37197;&#20013;&#65292;&#26368;&#20248;&#27493;&#38271;&#21442;&#25968;&#26377;&#24456;&#39640;&#30340;&#27010;&#29575;&#20026;1&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#31574;&#30053;&#26469;&#22788;&#29702;softassign&#36825;&#19968;&#27969;&#34892;&#32422;&#26463;&#31639;&#23376;&#22312;&#33410;&#28857;&#22522;&#25968;&#21644;&#28322;&#20986;&#39118;&#38505;&#26041;&#38754;&#30340;&#25935;&#24863;&#24615;&#12290; &#32467;&#21512;&#33258;&#36866;&#24212;&#27493;&#38271;&#21442;&#25968;&#21644;&#21160;&#24577;softassign&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#21305;&#37197;&#31639;&#27861;&#65306;softas
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.08233v3 Announce Type: replace-cross  Abstract: This paper studies a unified framework for graph matching problems called the constrained gradient method. Popular algorithms within this framework include graduated assignment (GA), integer projected fixed-point method (IPFP), and doubly stochastic projected fixed-point method (DSPFP). These algorithms differ from the step size parameter and constrained operator. Our contributed adaptive step size parameter can guarantee the underlying algorithms' convergence and enhance their efficiency and accuracy. A preliminary analysis suggests that the optimal step size parameter has a high probability of being 1 in fully connected graph matching. Secondly, we propose a dynamic strategy for softassign, a popular constrained operator, to address its sensitivity concerning nodes' cardinality and risk of overflow. Combining the adaptive step size parameter and the dynamical softassign, we propose a novel graph matching algorithm: the softas
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.14707</link><description>&lt;p&gt;
&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#30340;&#29305;&#24449;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Mitigating Feature Gap for Adversarial Robustness by Feature Disentanglement. (arXiv:2401.14707v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14707
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#35299;&#32544;&#26469;&#32531;&#35299;&#23545;&#25239;&#40065;&#26834;&#24615;&#20013;&#29305;&#24449;&#24046;&#36317;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26126;&#30830;&#24314;&#27169;&#21644;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#65292;&#26377;&#25928;&#25552;&#21319;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#23545;&#25239;&#24494;&#35843;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#24050;&#32463;&#22312;&#33258;&#28982;&#24773;&#20917;&#19979;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24335;&#24494;&#35843;&#26469;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#25239;&#26679;&#26412;&#20013;&#30340;&#19968;&#20123;&#28508;&#22312;&#29305;&#24449;&#34987;&#23545;&#25239;&#25200;&#21160;&#25152;&#28151;&#28102;&#65292;&#24182;&#23548;&#33268;&#33258;&#28982;&#26679;&#26412;&#21644;&#23545;&#25239;&#26679;&#26412;&#22312;&#26368;&#21518;&#19968;&#23618;&#38544;&#34255;&#23618;&#30340;&#29305;&#24449;&#20043;&#38388;&#20986;&#29616;&#24847;&#22806;&#22686;&#21152;&#30340;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35299;&#32544;&#30340;&#26041;&#27861;&#26469;&#26126;&#30830;&#24314;&#27169;&#21644;&#36827;&#19968;&#27493;&#28040;&#38500;&#23548;&#33268;&#29305;&#24449;&#24046;&#36317;&#30340;&#28508;&#22312;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29305;&#24449;&#35299;&#32544;&#22120;&#65292;&#23558;&#23545;&#25239;&#26679;&#26412;&#30340;&#28508;&#22312;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#30340;&#29305;&#24449;&#20998;&#31163;&#24320;&#26469;&#65292;&#20174;&#32780;&#36890;&#36807;&#28040;&#38500;&#28508;&#22312;&#29305;&#24449;&#26469;&#25552;&#21319;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#19982;&#23545;&#25239;&#26679;&#26412;&#22312;&#24494;&#35843;&#27169;&#22411;&#20013;&#30340;&#29305;&#24449;&#23545;&#40784;&#65292;&#36827;&#19968;&#27493;&#20174;&#33258;&#28982;&#26679;&#26412;&#30340;&#29305;&#24449;&#20013;&#33719;&#30410;&#65292;&#36991;&#20813;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are vulnerable to adversarial samples. Adversarial fine-tuning methods aim to enhance adversarial robustness through fine-tuning the naturally pre-trained model in an adversarial training manner. However, we identify that some latent features of adversarial samples are confused by adversarial perturbation and lead to an unexpectedly increasing gap between features in the last hidden layer of natural and adversarial samples. To address this issue, we propose a disentanglement-based approach to explicitly model and further remove the latent features that cause the feature gap. Specifically, we introduce a feature disentangler to separate out the latent features from the features of the adversarial samples, thereby boosting robustness by eliminating the latent features. Besides, we align features in the pre-trained model with features of adversarial samples in the fine-tuned model, to further benefit from the features from natural samples without confusion. Empirical 
&lt;/p&gt;</description></item><item><title>eipy&#26159;&#19968;&#20010;&#24320;&#28304;Python&#21253;&#65292;&#29992;&#20110;&#24320;&#21457;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#25104;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#20005;&#26684;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26469;&#35780;&#20272;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.09582</link><description>&lt;p&gt;
eipy: &#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#38598;&#25104;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#24320;&#28304;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
eipy: An Open-Source Python Package for Multi-modal Data Integration using Heterogeneous Ensembles. (arXiv:2401.09582v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09582
&lt;/p&gt;
&lt;p&gt;
eipy&#26159;&#19968;&#20010;&#24320;&#28304;Python&#21253;&#65292;&#29992;&#20110;&#24320;&#21457;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#25104;&#20998;&#31867;&#27169;&#22411;&#12290;&#23427;&#25552;&#20379;&#20102;&#20005;&#26684;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#26469;&#35780;&#20272;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;eipy&#65292;&#19968;&#31181;&#29992;&#20110;&#24320;&#21457;&#26377;&#25928;&#30340;&#22810;&#27169;&#24577;&#24322;&#26500;&#38598;&#25104;&#20998;&#31867;&#30340;&#24320;&#28304;Python&#21253;&#12290;eipy&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#20005;&#26684;&#32780;&#29992;&#25143;&#21451;&#22909;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#31995;&#32479;&#35780;&#20272;&#23427;&#20204;&#22312;&#23884;&#22871;&#20132;&#21449;&#39564;&#35777;&#20013;&#30340;&#24615;&#33021;&#26469;&#27604;&#36739;&#21644;&#36873;&#25321;&#26368;&#20339;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#25104;&#21644;&#39044;&#27979;&#24314;&#27169;&#26041;&#27861;&#12290;&#35813;&#21253;&#26088;&#22312;&#21033;&#29992;&#31867;&#20284;scikit-learn&#30340;&#20272;&#35745;&#22120;&#20316;&#20026;&#32452;&#20214;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;&#39044;&#27979;&#27169;&#22411;&#12290;eipy&#30340;&#26368;&#26032;&#29992;&#25143;&#25351;&#21335;&#65292;&#21253;&#25324;API&#21442;&#32771;&#21644;&#25945;&#31243;&#65292;&#35831;&#21442;&#38405;https://eipy.readthedocs.io&#12290;&#35813;&#39033;&#30446;&#30340;&#20027;&#35201;&#23384;&#20648;&#24211;&#20301;&#20110;GitHub&#19978;&#30340;https://github.com/GauravPandeyLab/eipy&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce eipy--an open-source Python package for developing effective, multi-modal heterogeneous ensembles for classification. eipy simultaneously provides both a rigorous, and user-friendly framework for comparing and selecting the best-performing multi-modal data integration and predictive modeling methods by systematically evaluating their performance using nested cross-validation. The package is designed to leverage scikit-learn-like estimators as components to build multi-modal predictive models. An up-to-date user guide, including API reference and tutorials, for eipy is maintained at https://eipy.readthedocs.io . The main repository for this project can be found on GitHub at https://github.com/GauravPandeyLab/eipy .
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31867;&#20284;ResNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;Langevin Monte Carlo&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#31616;&#21333;&#21442;&#32771;&#20998;&#24067;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#26469;&#36827;&#34892;&#37319;&#26679;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#36924;&#36817;&#36895;&#24230;&#21644;&#34920;&#36798;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.03242</link><description>&lt;p&gt;
&#20351;&#29992;&#31867;&#20284;ResNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36817;&#20284;Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
Approximating Langevin Monte Carlo with ResNet-like Neural Network architectures. (arXiv:2311.03242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31867;&#20284;ResNet&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#36817;&#20284;Langevin Monte Carlo&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#26469;&#33258;&#31616;&#21333;&#21442;&#32771;&#20998;&#24067;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#26469;&#36827;&#34892;&#37319;&#26679;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#36924;&#36817;&#36895;&#24230;&#21644;&#34920;&#36798;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#23558;&#26469;&#33258;&#31616;&#21333;&#21442;&#32771;&#20998;&#24067;&#65288;&#22914;&#26631;&#20934;&#27491;&#24577;&#20998;&#24067;&#65289;&#30340;&#26679;&#26412;&#26144;&#23556;&#21040;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#20013;&#65292;&#20174;&#32780;&#20174;&#32473;&#23450;&#30340;&#30446;&#26631;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#21463;Langevin Monte Carlo (LMC)&#31639;&#27861;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;&#22522;&#20110;LMC&#25200;&#21160;&#32467;&#26524;&#65292;&#22312;Wasserstein-2&#36317;&#31163;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26550;&#26500;&#23545;&#20110;&#24179;&#28369;&#30340;&#23545;&#25968;&#20985;&#30446;&#26631;&#20998;&#24067;&#30340;&#36924;&#36817;&#36895;&#24230;&#12290;&#20998;&#26512;&#20005;&#37325;&#20381;&#36182;&#20110;&#25200;&#21160;LMC&#36807;&#31243;&#30340;&#20013;&#38388;&#24230;&#37327;&#30340;&#20122;&#39640;&#26031;&#24615;&#27010;&#24565;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#26681;&#25454;&#19981;&#21516;&#25200;&#21160;&#20551;&#35774;&#25512;&#23548;&#20986;&#20102;&#20013;&#38388;&#26041;&#24046;&#20195;&#29702;&#30340;&#22686;&#38271;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;&#28145;&#24230;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#36817;&#20284;&#26679;&#26412;&#19982;&#30446;&#26631;&#20998;&#24067;&#26144;&#23556;&#30340;&#34920;&#36798;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We sample from a given target distribution by constructing a neural network which maps samples from a simple reference, e.g. the standard normal distribution, to samples from the target. To that end, we propose using a neural network architecture inspired by the Langevin Monte Carlo (LMC) algorithm. Based on LMC perturbation results, we show approximation rates of the proposed architecture for smooth, log-concave target distributions measured in the Wasserstein-$2$ distance. The analysis heavily relies on the notion of sub-Gaussianity of the intermediate measures of the perturbed LMC process. In particular, we derive bounds on the growth of the intermediate variance proxies under different assumptions on the perturbations. Moreover, we propose an architecture similar to deep residual neural networks and derive expressivity results for approximating the sample to target distribution map.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;</title><link>http://arxiv.org/abs/2308.11480</link><description>&lt;p&gt;
&#23545;&#24191;&#27867;&#30340;&#20998;&#24067;&#22806;&#26816;&#27979;&#30340;&#26399;&#26395;&#65306;&#26399;&#26395;&#20043;&#22806;&#30340;&#26410;&#30693;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection. (arXiv:2308.11480v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11480
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#20998;&#24067;&#22806;&#26816;&#27979;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#39640;&#37096;&#32626;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#36890;&#24120;&#28041;&#21450;&#24320;&#21457;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#30340;&#36755;&#20837;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#24120;&#24120;&#29421;&#31364;&#22320;&#20851;&#27880;&#35757;&#32451;&#38598;&#20013;&#32570;&#22833;&#30340;&#31867;&#21035;&#26679;&#26412;&#65292;&#24573;&#30053;&#20102;&#20854;&#20182;&#31867;&#22411;&#30340;&#21487;&#33021;&#20998;&#24067;&#21464;&#21270;&#12290;&#36825;&#31181;&#38480;&#21046;&#38477;&#20302;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#22240;&#20026;&#31995;&#32479;&#20250;&#36935;&#21040;&#21508;&#31181;&#21508;&#26679;&#30340;&#24322;&#24120;&#36755;&#20837;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#20116;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23545;&#26368;&#36817;&#30340;OOD&#26816;&#27979;&#26041;&#27861;&#22312;&#27599;&#19968;&#31181;&#20998;&#24067;&#21464;&#21270;&#19978;&#36827;&#34892;&#20102;&#20851;&#38190;&#35780;&#20272;&#12290;&#25105;&#20204;&#20197;BROAD&#65288;Benchmarking Resilience Over Anomaly Diversity&#65289;&#30340;&#21517;&#20041;&#20844;&#24320;&#21457;&#24067;&#25105;&#20204;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#26816;&#27979;&#26410;&#30693;&#31867;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#36935;&#21040;&#20854;&#20182;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#26102;&#24615;&#33021;&#19981;&#19968;&#33268;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#23427;&#20204;&#21482;&#33021;&#21487;&#38752;&#22320;&#26816;&#27979;&#21040;&#23427;&#20204;&#29305;&#21035;&#35774;&#35745;&#26469;&#39044;&#26399;&#30340;&#24847;&#22806;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the reliability of deployed machine learning systems often involves developing methods to detect out-of-distribution (OOD) inputs. However, existing research often narrowly focuses on samples from classes that are absent from the training set, neglecting other types of plausible distribution shifts. This limitation reduces the applicability of these methods in real-world scenarios, where systems encounter a wide variety of anomalous inputs. In this study, we categorize five distinct types of distribution shifts and critically evaluate the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). Our findings reveal that while these methods excel in detecting unknown classes, their performance is inconsistent when encountering other types of distribution shifts. In other words, they only reliably detect unexpected inputs that they have been specifically designed to expec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fuzz4All&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#38024;&#23545;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#35821;&#35328;&#21644;&#36825;&#20123;&#35821;&#35328;&#30340;&#35768;&#22810;&#19981;&#21516;&#21151;&#33021;&#36827;&#34892;&#27169;&#31946;&#27979;&#35797;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2308.04748</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#36890;&#29992;&#27169;&#31946;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Universal Fuzzing via Large Language Models. (arXiv:2308.04748v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04748
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fuzz4All&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#38024;&#23545;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#35821;&#35328;&#21644;&#36825;&#20123;&#35821;&#35328;&#30340;&#35768;&#22810;&#19981;&#21516;&#21151;&#33021;&#36827;&#34892;&#27169;&#31946;&#27979;&#35797;&#30340;&#36890;&#29992;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#31946;&#27979;&#35797;&#22312;&#21457;&#29616;&#21508;&#31181;&#36719;&#20214;&#31995;&#32479;&#20013;&#30340;&#28431;&#27934;&#21644;&#33030;&#24369;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#25509;&#21463;&#32534;&#31243;&#25110;&#24418;&#24335;&#35821;&#35328;&#20316;&#20026;&#36755;&#20837;&#30340;&#27979;&#35797;&#31995;&#32479;&#65288;SUTs&#65289;&#65292;&#22914;&#32534;&#35793;&#22120;&#65292;&#36816;&#34892;&#26102;&#24341;&#25806;&#65292;&#32422;&#26463;&#27714;&#35299;&#22120;&#21644;&#20855;&#26377;&#21487;&#35775;&#38382;API&#30340;&#36719;&#20214;&#24211;&#65292;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#36719;&#20214;&#24320;&#21457;&#30340;&#22522;&#26412;&#26500;&#24314;&#22359;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#29616;&#26377;&#27169;&#31946;&#27979;&#35797;&#24037;&#20855;&#36890;&#24120;&#38024;&#23545;&#29305;&#23450;&#35821;&#35328;&#65292;&#22240;&#27492;&#26080;&#27861;&#36731;&#26131;&#24212;&#29992;&#20110;&#20854;&#20182;&#35821;&#35328;&#29978;&#33267;&#21516;&#19968;&#35821;&#35328;&#30340;&#20854;&#20182;&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#27169;&#31946;&#27979;&#35797;&#24037;&#20855;&#29983;&#25104;&#30340;&#36755;&#20837;&#36890;&#24120;&#23616;&#38480;&#20110;&#36755;&#20837;&#35821;&#35328;&#30340;&#29305;&#23450;&#21151;&#33021;&#65292;&#22240;&#27492;&#24456;&#38590;&#25581;&#31034;&#19982;&#20854;&#20182;&#21151;&#33021;&#30456;&#20851;&#30340;&#28431;&#27934;&#25110;&#26032;&#21151;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Fuzz4All&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36890;&#29992;&#30340;&#27169;&#31946;&#27979;&#35797;&#24037;&#20855;&#65292;&#23427;&#21487;&#20197;&#38024;&#23545;&#35768;&#22810;&#19981;&#21516;&#30340;&#36755;&#20837;&#35821;&#35328;&#21644;&#36825;&#20123;&#35821;&#35328;&#30340;&#35768;&#22810;&#19981;&#21516;&#21151;&#33021;&#36827;&#34892;&#27979;&#35797;&#12290;Fuzz4All&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#36755;&#20837;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fuzzing has achieved tremendous success in discovering bugs and vulnerabilities in various software systems. Systems under test (SUTs) that take in programming or formal language as inputs, e.g., compilers, runtime engines, constraint solvers, and software libraries with accessible APIs, are especially important as they are fundamental building blocks of software development. However, existing fuzzers for such systems often target a specific language, and thus cannot be easily applied to other languages or even other versions of the same language. Moreover, the inputs generated by existing fuzzers are often limited to specific features of the input language, and thus can hardly reveal bugs related to other or new features. This paper presents Fuzz4All, the first fuzzer that is universal in the sense that it can target many different input languages and many different features of these languages. The key idea behind Fuzz4All is to leverage large language models (LLMs) as an input genera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21463;&#25968;&#35770;&#26041;&#27861;&#21551;&#21457;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#25554;&#20540;&#28857;&#26469;&#25552;&#39640;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.13869</link><description>&lt;p&gt;
&#20248;&#33391;&#26684;&#35757;&#32451;: &#20511;&#21161;&#25968;&#35770;&#21152;&#36895;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Good Lattice Training: Physics-Informed Neural Networks Accelerated by Number Theory. (arXiv:2307.13869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21463;&#25968;&#35770;&#26041;&#27861;&#21551;&#21457;&#65292;&#36890;&#36807;&#36873;&#25321;&#36866;&#24403;&#30340;&#25554;&#20540;&#28857;&#26469;&#25552;&#39640;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;(PINNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#39640;&#25928;&#30340;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#30340;&#26041;&#27861;&#12290;&#23427;&#20204;&#30340;&#25104;&#21151;&#22312;&#20110;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#28385;&#36275;&#32473;&#23450;&#28857;&#19978;&#30340;PDE&#65292;&#24182;&#23545;&#35299;&#36827;&#34892;&#36924;&#36817;&#12290;&#28982;&#32780;&#65292;PDE&#30340;&#35299;&#22312;&#26412;&#36136;&#19978;&#26159;&#26080;&#38480;&#32500;&#30340;&#65292;&#24182;&#19988;&#36755;&#20986;&#19982;&#35299;&#20043;&#38388;&#30340;&#36317;&#31163;&#26159;&#23450;&#20041;&#22312;&#25972;&#20010;&#22495;&#19978;&#30340;&#31215;&#20998;&#12290;&#22240;&#27492;&#65292;&#29289;&#29702;&#20449;&#24687;&#25439;&#22833;&#20989;&#25968;&#20165;&#25552;&#20379;&#26377;&#38480;&#30340;&#36924;&#36817;&#12290;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#25554;&#20540;&#28857;&#26041;&#38754;&#21017;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#23613;&#31649;&#36825;&#19968;&#26041;&#38754;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#65292;&#31216;&#20026;&#20248;&#33391;&#26684;&#35757;&#32451;(GLT)&#65292;&#29992;&#20110;PINNs&#65292;&#21463;&#25968;&#20540;&#20998;&#26512;&#20013;&#30340;&#25968;&#35770;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;GLT&#25552;&#20379;&#20102;&#19968;&#32452;&#21363;&#20351;&#22312;&#23569;&#37327;&#28857;&#21644;&#22810;&#32500;&#31354;&#38388;&#20013;&#20063;&#38750;&#24120;&#26377;&#25928;&#30340;&#25554;&#20540;&#28857;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;GLT&#21482;&#38656;&#35201;2-20&#20493;&#30340;&#28857;&#25968;
&lt;/p&gt;
&lt;p&gt;
Physics-informed neural networks (PINNs) offer a novel and efficient approach to solving partial differential equations (PDEs). Their success lies in the physics-informed loss, which trains a neural network to satisfy a given PDE at specific points and to approximate the solution. However, the solutions to PDEs are inherently infinite-dimensional, and the distance between the output and the solution is defined by an integral over the domain. Therefore, the physics-informed loss only provides a finite approximation, and selecting appropriate collocation points becomes crucial to suppress the discretization errors, although this aspect has often been overlooked. In this paper, we propose a new technique called good lattice training (GLT) for PINNs, inspired by number theoretic methods for numerical analysis. GLT offers a set of collocation points that are effective even with a small number of points and for multi-dimensional spaces. Our experiments demonstrate that GLT requires 2--20 tim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#30340;ConvResNeXts&#65292;&#21487;&#20197;&#38544;&#21547;&#22320;&#23454;&#29616;&#23545;&#27169;&#22359;&#30340;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#33021;&#22815;&#36866;&#24212;&#20302;&#32500;&#27969;&#24418;&#30340;&#24179;&#28369;&#24615;&#21644;&#32467;&#26500;&#65292;&#24182;&#39640;&#25928;&#22320;&#23398;&#20064;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01649</link><description>&lt;p&gt;
&#20302;&#32500;&#27969;&#24418;&#19978;&#36807;&#21442;&#25968;&#21270;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#30340;&#38750;&#21442;&#25968;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Classification on Low Dimensional Manifolds using Overparameterized Convolutional Residual Networks. (arXiv:2307.01649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21367;&#31215;&#27531;&#24046;&#32593;&#32476;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#30340;ConvResNeXts&#65292;&#21487;&#20197;&#38544;&#21547;&#22320;&#23454;&#29616;&#23545;&#27169;&#22359;&#30340;&#31232;&#30095;&#24615;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#33021;&#22815;&#36866;&#24212;&#20302;&#32500;&#27969;&#24418;&#30340;&#24179;&#28369;&#24615;&#21644;&#32467;&#26500;&#65292;&#24182;&#39640;&#25928;&#22320;&#23398;&#20064;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;(ConvResNets)&#34429;&#28982;&#36807;&#21442;&#25968;&#21270;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#33021;&#22815;&#33719;&#24471;&#26174;&#33879;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#36825;&#19981;&#33021;&#34987;&#24120;&#35268;&#26234;&#24935;&#24456;&#22909;&#22320;&#35299;&#37322;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20174;&#38750;&#21442;&#25968;&#20998;&#31867;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#20351;&#29992;&#26435;&#37325;&#34928;&#20943;&#35757;&#32451;&#30340;ConvResNeXts&#65288;&#35206;&#30422;ConvResNets&#20316;&#20026;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65289;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#20801;&#35768;ConvResNeXts&#20013;&#26377;&#26080;&#38480;&#22810;&#30340;&#26500;&#24314;&#27169;&#22359;&#65292;&#24182;&#26174;&#31034;&#26435;&#37325;&#34928;&#20943;&#38544;&#21547;&#22320;&#24378;&#21046;&#36825;&#20123;&#27169;&#22359;&#30340;&#31232;&#30095;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#24179;&#28369;&#30446;&#26631;&#20989;&#25968;&#65292;&#28982;&#21518;&#35777;&#26126;ConvResNeXts&#21487;&#20197;&#36866;&#24212;&#20989;&#25968;&#30340;&#24179;&#28369;&#24615;&#21644;&#20302;&#32500;&#32467;&#26500;&#65292;&#24182;&#19988;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#20989;&#25968;&#32780;&#19981;&#21463;&#32500;&#24230;&#35781;&#21650;&#30340;&#22256;&#25200;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#37096;&#20998;&#35777;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#30340;ConvResNeXts&#30456;&#23545;&#20110;&#24120;&#35268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional residual neural networks (ConvResNets), though overparameterized, can achieve remarkable prediction performance in practice, which cannot be well explained by conventional wisdom. To bridge this gap, we study the performance of ConvResNeXts, which cover ConvResNets as a special case, trained with weight decay from the perspective of nonparametric classification. Our analysis allows for infinitely many building blocks in ConvResNeXts, and shows that weight decay implicitly enforces sparsity on these blocks. Specifically, we consider a smooth target function supported on a low-dimensional manifold, then prove that ConvResNeXts can adapt to the function smoothness and low-dimensional structures and efficiently learn the function without suffering from the curse of dimensionality. Our findings partially justify the advantage of overparameterized ConvResNeXts over conventional machine learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;</title><link>http://arxiv.org/abs/2306.07799</link><description>&lt;p&gt;
ChatGPT&#19982;&#20154;&#24037;&#25776;&#20889;&#25991;&#26412;&#65306;&#21487;&#25511;&#25991;&#26412;&#25688;&#35201;&#21644;&#21477;&#23376;&#39118;&#26684;&#36716;&#31227;&#30340;&#27934;&#23519;
&lt;/p&gt;
&lt;p&gt;
ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer. (arXiv:2306.07799v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#21644;&#20889;&#20316;&#39118;&#26684;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#19968;&#20123;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;ChatGPT&#65289;&#20197;&#20854;&#20986;&#33394;&#30340;&#33021;&#21147;&#20174;&#31616;&#30701;&#30340;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#29983;&#25104;&#36830;&#36143;&#30340;&#25991;&#26412;&#24341;&#36215;&#20102;&#23186;&#20307;&#30340;&#37325;&#35270;&#12290;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#26816;&#26597;ChatGPT&#22312;&#20004;&#20010;&#21487;&#25511;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#21363;ChatGPT&#33021;&#21542;&#36866;&#24212;&#19981;&#21516;&#30340;&#30446;&#26631;&#21463;&#20247;&#65288;&#19987;&#23478;&#19982;&#19968;&#33324;&#20154;&#65289;&#21644;&#20889;&#20316;&#39118;&#26684;&#65288;&#27491;&#24335;&#19982;&#38750;&#27491;&#24335;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#25104;&#25991;&#26412;&#30340;&#24544;&#23454;&#24230;&#65292;&#24182;&#23558;&#27169;&#22411;&#30340;&#34920;&#29616;&#19982;&#20154;&#24037;&#25776;&#20889;&#30340;&#25991;&#26412;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20154;&#31867;&#20135;&#29983;&#30340;&#25991;&#20307;&#21464;&#21270;&#27604;ChatGPT&#34920;&#29616;&#20986;&#30340;&#26356;&#22823;&#65292;&#32780;&#29983;&#25104;&#30340;&#25991;&#26412;&#22312;&#35832;&#22914;&#21333;&#35789;&#31867;&#22411;&#20998;&#24067;&#31561;&#20960;&#20010;&#29305;&#24449;&#19978;&#19982;&#20154;&#31867;&#26679;&#26412;&#26377;&#25152;&#19981;&#21516;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403; ChatGPT &#23558;&#25991;&#26412;&#36866;&#24212;&#29305;&#23450;&#39118;&#26684;&#26102;&#65292;&#26377;&#26102;&#20250;&#21253;&#21547;&#20107;&#23454;&#38169;&#35823;&#25110;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale language models, like ChatGPT, have garnered significant media attention and stunned the public with their remarkable capacity for generating coherent text from short natural language prompts. In this paper, we aim to conduct a systematic inspection of ChatGPT's performance in two controllable generation tasks, with respect to ChatGPT's ability to adapt its output to different target audiences (expert vs. layman) and writing styles (formal vs. informal). Additionally, we evaluate the faithfulness of the generated text, and compare the model's performance with human-authored texts. Our findings indicate that the stylistic variations produced by humans are considerably larger than those demonstrated by ChatGPT, and the generated texts diverge from human samples in several characteristics, such as the distribution of word types. Moreover, we observe that ChatGPT sometimes incorporates factual errors or hallucinations when adapting the text to suit a specific style.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.01631</link><description>&lt;p&gt;
Gode -- &#23558;&#29983;&#29289;&#21270;&#23398;&#30693;&#35782;&#22270;&#35889;&#38598;&#25104;&#21040;&#20998;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#20013;
&lt;/p&gt;
&lt;p&gt;
Gode -- Integrating Biochemical Knowledge Graph into Pre-training Molecule Graph Neural Network. (arXiv:2306.01631v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01631
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#23376;&#32467;&#26500;&#21644;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889;&#20013;&#38598;&#25104;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#65292;&#36890;&#36807;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23646;&#24615;&#30340;&#20934;&#30830;&#39044;&#27979;&#23545;&#20110;&#20419;&#36827;&#21019;&#26032;&#27835;&#30103;&#26041;&#27861;&#30340;&#21457;&#23637;&#21644;&#29702;&#35299;&#21270;&#23398;&#29289;&#36136;&#21644;&#29983;&#29289;&#31995;&#32479;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#21333;&#20010;&#20998;&#23376;&#32467;&#26500;&#30340;&#22270;&#34920;&#31034;&#19982;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#22270;&#35889; (KG) &#30340;&#22810;&#20010;&#39046;&#22495;&#20449;&#24687;&#36827;&#34892;&#38598;&#25104;&#12290;&#36890;&#36807;&#38598;&#25104;&#20004;&#20010;&#32423;&#21035;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#31574;&#30053;&#39044;&#20808;&#35757;&#32451;&#26356;&#24191;&#27867;&#21644;&#26356;&#24378;&#22823;&#30340;&#34920;&#31034;&#65292;&#29992;&#20110;&#20998;&#23376;&#32423;&#21644; KG &#32423;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#24615;&#33021;&#35780;&#20272;&#26041;&#38754;&#65292;&#25105;&#20204;&#22312; 11 &#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#21270;&#23398;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#19978;&#24494;&#35843;&#25105;&#20204;&#39044;&#20808;&#35757;&#32451;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#24494;&#35843;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
The precise prediction of molecular properties holds paramount importance in facilitating the development of innovative treatments and comprehending the intricate interplay between chemicals and biological systems. In this study, we propose a novel approach that integrates graph representations of individual molecular structures with multi-domain information from biomedical knowledge graphs (KGs). Integrating information from both levels, we can pre-train a more extensive and robust representation for both molecule-level and KG-level prediction tasks with our novel self-supervision strategy. For performance evaluation, we fine-tune our pre-trained model on 11 challenging chemical property prediction tasks. Results from our framework demonstrate our fine-tuned models outperform existing state-of-the-art models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16784</link><description>&lt;p&gt;
&#32467;&#21512;&#35805;&#35821;&#32467;&#26500;&#20998;&#24067;&#30340;&#38271;&#25991;&#26412;&#33258;&#21160;&#25688;&#35201;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Incorporating Distributions of Discourse Structure for Long Document Abstractive Summarization. (arXiv:2305.16784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16784
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20197;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#20026;&#22522;&#30784;&#65292;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#25991;&#26412;&#25688;&#35201;&#65292;&#35805;&#35821;&#32467;&#26500;&#22312;&#36776;&#35782;&#25991;&#26412;&#26680;&#24515;&#20869;&#23481;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#21487;&#24796;&#30340;&#26159;&#65292;&#20043;&#21069;&#23558;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#24341;&#20837;&#22522;&#20110;transformer&#30340;&#33258;&#21160;&#25688;&#35201;&#27169;&#22411;&#30340;&#30740;&#31350;&#20165;&#32771;&#34385;&#20102;&#26680;&#24515;&#37096;&#20998;&#30340;&#27880;&#37322;&#65292;&#20174;&#32780;&#24573;&#30053;&#20102;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#35805;&#35821;&#20851;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;'RSTformer'&#30340;&#26032;&#22411;&#25688;&#35201;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20840;&#38754;&#34701;&#21512;&#20102;&#35805;&#35821;&#20851;&#31995;&#31867;&#22411;&#21644;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;RST-attention&#26426;&#21046;&#26159;&#22522;&#20110;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#30340;Longformer&#26694;&#26550;&#30340;&#25193;&#23637;&#12290;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#27169;&#22411;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#27169;&#22411;&#65292;&#20984;&#26174;&#20986;&#20854;&#22312;&#22810;&#20010;&#33258;&#21160;&#35780;&#20272;&#25351;&#26631;&#21644;&#20154;&#24037;&#35780;&#20272;&#19978;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
For text summarization, the role of discourse structure is pivotal in discerning the core content of a text. Regrettably, prior studies on incorporating Rhetorical Structure Theory (RST) into transformer-based summarization models only consider the nuclearity annotation, thereby overlooking the variety of discourse relation types. This paper introduces the 'RSTformer', a novel summarization model that comprehensively incorporates both the types and uncertainty of rhetorical relations. Our RST-attention mechanism, rooted in document-level rhetorical structure, is an extension of the recently devised Longformer framework. Through rigorous evaluation, the model proposed herein exhibits significant superiority over state-of-the-art models, as evidenced by its notable performance on several automatic metrics and human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#21333;&#27169;&#24577;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;VAE&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15708</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#22810;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Score-Based Multimodal Autoencoders. (arXiv:2305.15708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#24314;&#27169;&#21333;&#27169;&#24577;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#19968;&#33268;&#24615;&#25972;&#21512;&#65292;&#25552;&#39640;&#20102;&#22810;&#27169;&#24577;VAE&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26159;&#19968;&#31867;&#33021;&#22815;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#26500;&#24314;&#21487;&#22788;&#29702;&#21518;&#39564;&#30340;&#26377;&#21069;&#36884;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22810;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#12290;&#20294;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#27599;&#19968;&#20010;&#27169;&#24577;&#30340;&#29983;&#25104;&#36136;&#37327;&#37117;&#20250;&#38477;&#20302;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#21333;&#27169;&#24577;VAE&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;&#22810;&#27169;&#24577;VAE&#30340;&#29983;&#25104;&#24615;&#33021;&#12290;&#20998;&#25968;&#27169;&#22411;&#30340;&#20316;&#29992;&#26159;&#36890;&#36807;&#23398;&#20064;&#28508;&#22312;&#21464;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#30340;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#32467;&#21512;&#20102;&#21333;&#27169;&#24577;VAE&#21331;&#36234;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#23545;&#19981;&#21516;&#27169;&#24577;&#30340;&#19968;&#33268;&#24615;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Variational Autoencoders (VAEs) represent a promising group of generative models that facilitate the construction of a tractable posterior within the latent space, given multiple modalities. Daunhawer et al. (2022) demonstrate that as the number of modalities increases, the generative quality of each modality declines. In this study, we explore an alternative approach to enhance the generative performance of multimodal VAEs by jointly modeling the latent space of unimodal VAEs using score-based models (SBMs). The role of the SBM is to enforce multimodal coherence by learning the correlation among the latent variables. Consequently, our model combines the superior generative quality of unimodal VAEs with coherent integration across different modalities.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;</title><link>http://arxiv.org/abs/2305.15557</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#23398;&#20064;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#29575;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence. (arXiv:2305.15557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#26469;&#35782;&#21035;&#38750;&#32447;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#33539;&#24335;&#20381;&#36182;&#20110;&#29366;&#24577;&#30340;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#30456;&#24212;&#30340;Fokker-Planck&#26041;&#31243;&#30340;&#22522;&#20110;RKHS&#30340;&#36817;&#20284;&#25311;&#21512;&#21040;&#36825;&#20123;&#35266;&#27979;&#20540;&#65292;&#20174;&#32780;&#24471;&#20986;&#29702;&#35770;&#23398;&#20064;&#36895;&#29575;&#30340;&#20272;&#35745;&#20540;&#65292;&#36825;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#24403;&#26410;&#30693;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#36234;&#39640;&#26102;&#65292;&#29702;&#35770;&#20272;&#35745;&#20540;&#36234;&#26469;&#36234;&#32039;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#20869;&#26680;&#30340;&#65292;&#22240;&#27492;&#31163;&#32447;&#39044;&#22788;&#29702;&#21487;&#20197;&#22312;&#21407;&#21017;&#19978;&#24471;&#21040;&#26377;&#25928;&#30340;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel non-parametric learning paradigm for the identification of drift and diffusion coefficients of non-linear stochastic differential equations, which relies upon discrete-time observations of the state. The key idea essentially consists of fitting a RKHS-based approximation of the corresponding Fokker-Planck equation to such observations, yielding theoretical estimates of learning rates which, unlike previous works, become increasingly tighter when the regularity of the unknown drift and diffusion coefficients becomes higher. Our method being kernel-based, offline pre-processing may in principle be profitably leveraged to enable efficient numerical implementation.
&lt;/p&gt;</description></item><item><title>ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01973</link><description>&lt;p&gt;
ERM++&#65306;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ERM++: An Improved Baseline for Domain Generalization. (arXiv:2304.01973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01973
&lt;/p&gt;
&lt;p&gt;
ERM++&#26159;&#19968;&#20010;&#29992;&#20110;&#22495;&#36890;&#29992;&#24615;&#30340;&#25913;&#36827;&#22522;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#31561;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#27604;&#26631;&#20934;ERM&#26356;&#26377;&#25928;&#65292;&#21516;&#26102;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#34920;&#29616;&#20063;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#22495;&#36890;&#29992;&#24615;&#65288;DG&#65289;&#34913;&#37327;&#20998;&#31867;&#22120;&#23545;&#20110;&#23427;&#27809;&#26377;&#25509;&#21463;&#36807;&#35757;&#32451;&#30340;&#26032;&#25968;&#25454;&#20998;&#24067;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#32771;&#34385;&#20102;&#22810;&#20010;&#35757;&#32451;&#22495;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#28304;DG&#26041;&#27861;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#22495;&#26631;&#31614;&#22686;&#21152;&#20102;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#35757;&#32451;&#36807;&#31243;&#65292;&#21363;&#22312;&#28304;&#22495;&#19978;&#31616;&#21333;&#22320;&#26368;&#23567;&#21270;&#32463;&#39564;&#39118;&#38505;&#65292;&#21487;&#20197;&#32988;&#36807;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;DG&#26041;&#27861;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#20010;&#20851;&#38190;&#20505;&#36873;&#25216;&#26415;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;ERM&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#26356;&#22909;&#22320;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#12289;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#21644;&#26435;&#37325;&#31354;&#38388;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#31216;&#20026;ERM ++&#65292;&#24182;&#23637;&#31034;&#23427;&#30456;&#23545;&#20110;&#26631;&#20934;ERM&#22312;&#20116;&#20010;&#22810;&#28304;&#25968;&#25454;&#38598;&#19978;&#23558;DG&#30340;&#24615;&#33021;&#26174;&#30528;&#25552;&#39640;&#20102;5&#65285;&#20197;&#19978;&#65292;&#24182;&#19988;&#23613;&#31649;&#35745;&#31639;&#22797;&#26434;&#24230;&#26356;&#20302;&#65292;&#20294;&#20987;&#36133;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;ERM ++&#22312;WILDS-FMOW&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-source Domain Generalization (DG) measures a classifier's ability to generalize to new distributions of data it was not trained on, given several training domains. While several multi-source DG methods have been proposed, they incur additional complexity during training by using domain labels. Recent work has shown that a well-tuned Empirical Risk Minimization (ERM) training procedure, that is simply minimizing the empirical risk on the source domains, can outperform most existing DG methods. We identify several key candidate techniques to further improve ERM performance, such as better utilization of training data, model parameter selection, and weight-space regularization. We call the resulting method ERM++, and show it significantly improves the performance of DG on five multi-source datasets by over 5% compared to standard ERM, and beats state-of-the-art despite being less computationally expensive. Additionally, we demonstrate the efficacy of ERM++ on the WILDS-FMOW dataset,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ChatRepair&#30340;&#26032;&#22411;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#8220;&#29983;&#25104;&#21644;&#39564;&#35777;&#8221;&#33539;&#24335;&#19981;&#21516;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23545;&#35805;&#39118;&#26684;&#23454;&#29616;&#21363;&#26102;&#21453;&#39304;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#28431;&#27934;&#20462;&#22797;&#30340;&#25928;&#29575;&#21644;&#34917;&#19969;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.00385</link><description>&lt;p&gt;
&#35753;&#23545;&#35805;&#32487;&#32493;&#65306;&#20351;&#29992;ChatGPT&#20165;&#20197;0.42&#32654;&#20803;&#30340;&#20215;&#26684;&#20462;&#22797;&#20102;337&#20010;&#28431;&#27934;&#20013;&#30340;162&#20010;
&lt;/p&gt;
&lt;p&gt;
Keep the Conversation Going: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. (arXiv:2304.00385v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ChatRepair&#30340;&#26032;&#22411;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#65292;&#19982;&#20256;&#32479;&#30340;&#8220;&#29983;&#25104;&#21644;&#39564;&#35777;&#8221;&#33539;&#24335;&#19981;&#21516;&#65292;&#23427;&#33021;&#22815;&#36890;&#36807;&#23545;&#35805;&#39118;&#26684;&#23454;&#29616;&#21363;&#26102;&#21453;&#39304;&#65292;&#20174;&#32780;&#26174;&#30528;&#25552;&#39640;&#28431;&#27934;&#20462;&#22797;&#30340;&#25928;&#29575;&#21644;&#34917;&#19969;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#26377;&#20851;&#26377;&#28431;&#27934;&#31243;&#24207;&#30340;&#20462;&#34917;&#31243;&#24207;&#12290;&#26368;&#36817;&#30340;APR&#24037;&#20316;&#38598;&#20013;&#20110;&#21033;&#29992;&#29616;&#20195;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#29983;&#25104;APR&#30340;&#34917;&#19969;&#12290;&#36825;&#31181;&#22522;&#20110;LLM&#30340;APR&#24037;&#20855;&#30340;&#24037;&#20316;&#26041;&#27861;&#26159;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#30001;&#21407;&#22987;&#26377;&#28431;&#27934;&#20195;&#30721;&#26500;&#24314;&#30340;&#36755;&#20837;&#25552;&#31034;&#65292;&#28982;&#21518;&#26597;&#35810;LLM&#29983;&#25104;&#34917;&#19969;&#12290;&#34429;&#28982;&#22522;&#20110;LLM&#30340;APR&#24037;&#20855;&#33021;&#22815;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20173;&#28982;&#36981;&#24490;&#8220;&#29983;&#25104;&#21644;&#39564;&#35777;&#8221;&#20462;&#22797;&#33539;&#24335;&#65292;&#21363;&#39318;&#20808;&#29983;&#25104;&#22823;&#37327;&#30340;&#34917;&#19969;&#65292;&#28982;&#21518;&#36880;&#20010;&#39564;&#35777;&#27599;&#20010;&#34917;&#19969;&#12290;&#36825;&#19981;&#20165;&#20250;&#23548;&#33268;&#35768;&#22810;&#37325;&#22797;&#30340;&#19981;&#27491;&#30830;&#30340;&#34917;&#19969;&#65292;&#32780;&#19988;&#36824;&#20250;&#38169;&#36807;&#27979;&#35797;&#22833;&#36133;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#20197;&#21450;&#21487;&#34892;&#30340;&#34917;&#19969;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatRepair&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#23545;&#35805;&#39537;&#21160;&#30340;APR&#26041;&#27861;&#65292;&#23427;&#23558;&#34917;&#19969;&#29983;&#25104;&#19982;&#21363;&#26102;&#21453;&#39304;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#20197;&#23545;&#35805;&#39118;&#26684;&#25191;&#34892;APR&#12290;ChatRepair&#39318;&#20808;&#23558;&#30456;&#20851;&#30340;&#27979;&#35797;&#22833;&#36133;&#20449;&#24687;&#39304;&#20837;LLM&#20013;&#65292;&#28982;&#21518;&#22312;&#34917;&#19969;&#29983;&#25104;&#36807;&#31243;&#20013;&#20351;&#29992;&#20132;&#20114;&#24335;&#23545;&#35805;&#65292;&#20197;&#38598;&#20013;&#26041;&#24335;&#29983;&#25104;&#34917;&#19969;&#12290;&#27492;&#22806;&#65292;ChatRepair&#36824;&#21033;&#29992;&#20102;&#27979;&#35797;&#32467;&#26524;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#26356;&#22909;&#30340;&#34917;&#19969;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Program Repair (APR) aims to automatically generate patches for buggy programs. Recent APR work has been focused on leveraging modern Large Language Models (LLMs) to directly generate patches for APR. Such LLM-based APR tools work by first constructing an input prompt built using the original buggy code and then queries the LLM to generate patches. While the LLM-based APR tools are able to achieve state-of-the-art results, it still follows the classic Generate and Validate repair paradigm of first generating lots of patches and then validating each one afterwards. This not only leads to many repeated patches that are incorrect but also miss the crucial information in test failures as well as in plausible patches.  To address these limitations, we propose ChatRepair, the first fully automated conversation-driven APR approach that interleaves patch generation with instant feedback to perform APR in a conversational style. ChatRepair first feeds the LLM with relevant test failur
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#20013;&#30340;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;APR&#30340;&#26032;&#26041;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#20256;&#32479;APR&#24037;&#20855;&#22312;&#19981;&#21516;&#39033;&#30446;&#20013;&#26080;&#27861;&#20135;&#29983;&#22810;&#26679;&#21270;&#20462;&#34917;&#31243;&#24207;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.10494</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#37325;&#26032;&#23457;&#35270;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Plastic Surgery Hypothesis via Large Language Models. (arXiv:2303.10494v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#20013;&#30340;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;&#65292;&#24182;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;APR&#30340;&#26032;&#26041;&#27861;&#65292;&#20027;&#35201;&#35299;&#20915;&#20102;&#20256;&#32479;APR&#24037;&#20855;&#22312;&#19981;&#21516;&#39033;&#30446;&#20013;&#26080;&#27861;&#20135;&#29983;&#22810;&#26679;&#21270;&#20462;&#34917;&#31243;&#24207;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#26088;&#22312;&#33258;&#21160;&#29983;&#25104;&#36755;&#20837;&#38169;&#35823;&#31243;&#24207;&#30340;&#34917;&#19969;&#12290;&#20256;&#32479;APR&#24037;&#20855;&#36890;&#24120;&#19987;&#27880;&#20110;&#29305;&#23450;&#30340;&#38169;&#35823;&#31867;&#22411;&#21644;&#20462;&#22797;&#26041;&#24335;&#65292;&#36890;&#36807;&#20351;&#29992;&#27169;&#26495;&#12289;&#21551;&#21457;&#24335;&#21644;&#27491;&#24335;&#35268;&#33539;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#22312;&#38169;&#35823;&#31867;&#22411;&#21644;&#20462;&#34917;&#31243;&#24207;&#30340;&#22810;&#26679;&#21270;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;APR&#24037;&#20855;&#65292;&#26368;&#36817;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#30452;&#25509;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;APR&#12290;&#34429;&#28982;&#22522;&#20110;LLM&#30340;APR&#24037;&#20855;&#33021;&#22815;&#22312;&#35768;&#22810;&#20462;&#22797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#29992;&#20110;&#30452;&#25509;&#20462;&#22797;&#30340;LLMs&#24182;&#27809;&#26377;&#23436;&#20840;&#20102;&#35299;&#39033;&#30446;&#29305;&#23450;&#20449;&#24687;&#65292;&#22914;&#29420;&#29305;&#30340;&#21464;&#37327;&#25110;&#26041;&#27861;&#21517;&#31216;&#12290;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;&#26159;APR&#30340;&#19968;&#20010;&#33879;&#21517;&#30340;&#35265;&#35299;&#65292;&#23427;&#25351;&#20986;&#20462;&#22797;&#38169;&#35823;&#30340;&#20195;&#30721;&#37096;&#20998;&#36890;&#24120;&#24050;&#32463;&#23384;&#22312;&#20110;&#21516;&#19968;&#39033;&#30446;&#20013;&#12290;&#20256;&#32479;&#30340;APR&#24037;&#20855;&#20027;&#35201;&#36890;&#36807;&#35774;&#35745;&#25163;&#21160;&#25110;&#22522;&#20110;&#21551;&#21457;&#30340;&#26041;&#27861;&#26469;&#21033;&#29992;&#25972;&#24418;&#25163;&#26415;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated Program Repair (APR) aspires to automatically generate patches for an input buggy program. Traditional APR tools typically focus on specific bug types and fixes through the use of templates, heuristics, and formal specifications. However, these techniques are limited in terms of the bug types and patch variety they can produce. As such, researchers have designed various learning-based APR tools with recent work focused on directly using Large Language Models (LLMs) for APR. While LLM-based APR tools are able to achieve state-of-the-art performance on many repair datasets, the LLMs used for direct repair are not fully aware of the project-specific information such as unique variable or method names.  The plastic surgery hypothesis is a well-known insight for APR, which states that the code ingredients to fix the bug usually already exist within the same project. Traditional APR tools have largely leveraged the plastic surgery hypothesis by designing manual or heuristic-based a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#28418;&#31227;&#39033;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;&#36890;&#36807;&#27425;&#27969;&#24418;&#20551;&#35774;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22343;&#20540;&#28418;&#31227;&#20989;&#25968;&#21644;&#24471;&#20998;&#20989;&#25968;&#28176;&#36817;&#21457;&#25955;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.07882</link><description>&lt;p&gt;
&#22522;&#20110;&#27425;&#27969;&#24418;&#20551;&#35774;&#19979;&#25193;&#25955;&#27169;&#22411;&#22855;&#24322;&#24615;&#30340;&#25968;&#23398;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mathematical analysis of singularities in the diffusion model under the submanifold assumption. (arXiv:2301.07882v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07882
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#25193;&#25955;&#27169;&#22411;&#20013;&#28418;&#31227;&#39033;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;&#36890;&#36807;&#27425;&#27969;&#24418;&#20551;&#35774;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#22788;&#29702;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#65292;&#35299;&#20915;&#20102;&#22343;&#20540;&#28418;&#31227;&#20989;&#25968;&#21644;&#24471;&#20998;&#20989;&#25968;&#28176;&#36817;&#21457;&#25955;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#23398;&#20998;&#26512;&#12290;&#20197;&#26465;&#20214;&#26399;&#26395;&#34920;&#31034;&#21453;&#21521;&#37319;&#26679;&#27969;&#31243;&#30340;&#28418;&#31227;&#39033;&#65292;&#20854;&#20013;&#28041;&#21450;&#25968;&#25454;&#20998;&#24067;&#21644;&#21069;&#21521;&#25193;&#25955;&#12290;&#35757;&#32451;&#36807;&#31243;&#26088;&#22312;&#36890;&#36807;&#26368;&#23567;&#21270;&#19982;&#26465;&#20214;&#26399;&#26395;&#30456;&#20851;&#30340;&#22343;&#26041;&#27531;&#24046;&#26469;&#23547;&#25214;&#27492;&#31867;&#28418;&#31227;&#20989;&#25968;&#12290;&#20351;&#29992;&#21069;&#21521;&#25193;&#25955;&#30340;Green&#20989;&#25968;&#30340;&#23567;&#26102;&#38388;&#36817;&#20284;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DDPM&#20013;&#30340;&#35299;&#26512;&#22343;&#20540;&#28418;&#31227;&#20989;&#25968;&#21644;SGM&#20013;&#30340;&#24471;&#20998;&#20989;&#25968;&#22312;&#37319;&#26679;&#36807;&#31243;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#23545;&#20110;&#20687;&#37027;&#20123;&#38598;&#20013;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#30340;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#32780;&#35328;&#65292;&#28176;&#36817;&#22320;&#21457;&#25955;&#65292;&#22240;&#27492;&#38590;&#20197;&#36890;&#36807;&#32593;&#32476;&#36827;&#34892;&#36924;&#36817;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#30456;&#20851;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#21363;&#20351;&#22312;&#22788;&#29702;&#22855;&#24322;&#25968;&#25454;&#20998;&#24067;&#26102;&#20173;&#28982;&#20445;&#25345;&#26377;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#26469;&#35828;&#26126;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper provide several mathematical analyses of the diffusion model in machine learning. The drift term of the backwards sampling process is represented as a conditional expectation involving the data distribution and the forward diffusion. The training process aims to find such a drift function by minimizing the mean-squared residue related to the conditional expectation. Using small-time approximations of the Green's function of the forward diffusion, we show that the analytical mean drift function in DDPM and the score function in SGM asymptotically blow up in the final stages of the sampling process for singular data distributions such as those concentrated on lower-dimensional manifolds, and is therefore difficult to approximate by a network. To overcome this difficulty, we derive a new target function and associated loss, which remains bounded even for singular data distributions. We illustrate the theoretical findings with several numerical examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#33258;&#20027;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#23545;&#25239;&#24615;&#25200;&#21160;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;CADet&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21516;&#19968;&#26679;&#26412;&#30340;&#23545;&#27604;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;OOD&#26816;&#27979;&#65292;&#24182;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#35782;&#21035;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.01742</link><description>&lt;p&gt;
CADet:&#20840;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#29992;&#20110;&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#33258;&#20027;&#20998;&#24067;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CADet: Fully Self-Supervised Out-Of-Distribution Detection With Contrastive Learning. (arXiv:2210.01742v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01742
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#24102;&#26377;&#23545;&#27604;&#23398;&#20064;&#30340;&#20840;&#33258;&#20027;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#21516;&#26102;&#26816;&#27979;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#23545;&#25239;&#24615;&#25200;&#21160;&#26679;&#26412;&#12290;&#36890;&#36807;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;CADet&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#21516;&#19968;&#26679;&#26412;&#30340;&#23545;&#27604;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#36827;&#34892;OOD&#26816;&#27979;&#65292;&#24182;&#22312;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#35782;&#21035;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#24102;&#26377;&#20998;&#24067;&#20043;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#29616;&#23454;&#19990;&#30028;&#37096;&#32626;&#20013;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21516;&#26102;&#26816;&#27979;&#20004;&#31181;&#31867;&#22411;&#30340;OOD&#26679;&#26412;&#65306;&#26410;&#35265;&#36807;&#30340;&#31867;&#21035;&#21644;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#19982;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#21452;&#26679;&#26412;&#26816;&#39564;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#40065;&#26834;&#22320;&#27979;&#35797;&#20004;&#20010;&#29420;&#31435;&#26679;&#26412;&#38598;&#26159;&#21542;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#36739;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#23427;&#22312;&#21306;&#20998;CIFAR-10&#21644;CIFAR-10.1&#26102;&#20855;&#26377;&#26356;&#39640;&#30340;&#32622;&#20449;&#24230;&#12290;&#22312;&#27492;&#25104;&#21151;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CADet&#65288;&#23545;&#27604;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#21333;&#26679;&#26412;OOD&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;CADet&#20511;&#37492;&#20102;MMD&#30340;&#24605;&#24819;&#65292;&#20294;&#21033;&#29992;&#20102;&#21516;&#19968;&#26679;&#26412;&#30340;&#23545;&#27604;&#21464;&#25442;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;CADet&#22312;&#35782;&#21035;&#34987;&#23545;&#25239;&#24615;&#25200;&#21160;&#24178;&#25200;&#30340;&#26679;&#26412;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Handling out-of-distribution (OOD) samples has become a major stake in the real-world deployment of machine learning systems. This work explores the use of self-supervised contrastive learning to the simultaneous detection of two types of OOD samples: unseen classes and adversarial perturbations. First, we pair self-supervised contrastive learning with the maximum mean discrepancy (MMD) two-sample test. This approach enables us to robustly test whether two independent sets of samples originate from the same distribution, and we demonstrate its effectiveness by discriminating between CIFAR-10 and CIFAR-10.1 with higher confidence than previous work. Motivated by this success, we introduce CADet (Contrastive Anomaly Detection), a novel method for OOD detection of single samples. CADet draws inspiration from MMD, but leverages the similarity between contrastive transformations of a same sample. CADet outperforms existing adversarial detection methods in identifying adversarially perturbed
&lt;/p&gt;</description></item></channel></rss>