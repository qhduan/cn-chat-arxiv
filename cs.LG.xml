<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#29289;&#32852;&#32593;&#29992;&#25143;&#35774;&#22791;&#65288;IIoT UEs&#65289;&#30340;&#24847;&#22270;&#24863;&#30693;DRL&#19978;&#34892;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#23398;&#20064;&#22914;&#20309;&#35843;&#24230;&#36890;&#20449;&#36164;&#28304;&#65292;&#24182;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#31616;&#21270;&#26041;&#26696;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#35843;&#24230;&#26041;&#26696;&#65292;&#33021;&#26377;&#25928;&#20445;&#35777;IIoT UEs&#30340;&#24847;&#22270;&#34920;&#36798;&#12290;</title><link>https://arxiv.org/abs/2403.18364</link><description>&lt;p&gt;
&#38754;&#21521;5G-NR&#30340;&#24847;&#22270;&#24863;&#30693;DRL&#19978;&#34892;&#21160;&#24577;&#35843;&#24230;&#22120;
&lt;/p&gt;
&lt;p&gt;
Intent-Aware DRL-Based Uplink Dynamic Scheduler for 5G-NR
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18364
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#24037;&#19994;&#29289;&#32852;&#32593;&#29992;&#25143;&#35774;&#22791;&#65288;IIoT UEs&#65289;&#30340;&#24847;&#22270;&#24863;&#30693;DRL&#19978;&#34892;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#23398;&#20064;&#22914;&#20309;&#35843;&#24230;&#36890;&#20449;&#36164;&#28304;&#65292;&#24182;&#21033;&#29992;&#22270;&#32467;&#26500;&#30340;&#31616;&#21270;&#26041;&#26696;&#21152;&#36895;&#25910;&#25947;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#35843;&#24230;&#26041;&#26696;&#65292;&#33021;&#26377;&#25928;&#20445;&#35777;IIoT UEs&#30340;&#24847;&#22270;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25903;&#25345;&#24037;&#19994;&#29289;&#32852;&#32593;&#29992;&#25143;&#35774;&#22791;&#65288;IIoT UEs&#65289;&#20855;&#26377;&#24847;&#22270;&#65288;&#21363;&#25152;&#35831;&#27714;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#65289;&#21644;&#38543;&#26426;&#27969;&#37327;&#21040;&#36798;&#30340;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#38598;&#20013;&#21160;&#24577;&#35843;&#24230;&#22120;&#65292;&#29992;&#20110;&#23398;&#20064;&#22914;&#20309;&#22312;IIoT UEs&#20043;&#38388;&#35843;&#24230;&#21487;&#29992;&#36890;&#20449;&#36164;&#28304;&#30340;&#26102;&#38388;&#39057;&#29575;&#36164;&#28304;&#12290;&#25152;&#25552;&#20986;&#30340;&#35843;&#24230;&#22120;&#21033;&#29992;RL&#26694;&#26550;&#26469;&#36866;&#24212;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#21644;&#27969;&#37327;&#21040;&#36798;&#20013;&#30340;&#21160;&#24577;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#31616;&#21270;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;RL&#26694;&#26550;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#23398;&#20064;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20960;&#31181;&#20256;&#32479;&#35843;&#24230;&#26041;&#26696;&#65288;&#22914;&#36718;&#35810;&#12289;&#21322;&#38745;&#24577;&#21644;&#21551;&#21457;&#24335;&#26041;&#27861;&#65289;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26234;&#33021;&#35843;&#24230;&#22120;&#22312;&#20445;&#35777;IIoT UEs&#25152;&#34920;&#36798;&#30340;&#24847;&#22270;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18364v1 Announce Type: cross  Abstract: We investigate the problem of supporting Industrial Internet of Things user equipment (IIoT UEs) with intent (i.e., requested quality of service (QoS)) and random traffic arrival. A deep reinforcement learning (DRL) based centralized dynamic scheduler for time-frequency resources is proposed to learn how to schedule the available communication resources among the IIoT UEs. The proposed scheduler leverages an RL framework to adapt to the dynamic changes in the wireless communication system and traffic arrivals. Moreover, a graph-based reduction scheme is proposed to reduce the state and action space of the RL framework to allow fast convergence and a better learning strategy. Simulation results demonstrate the effectiveness of the proposed intelligent scheduler in guaranteeing the expressed intent of IIoT UEs compared to several traditional scheduling schemes, such as round-robin, semi-static, and heuristic approaches. The proposed sche
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#30340;&#24433;&#21709;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#25506;&#31350;&#23398;&#20064;&#21160;&#24577;&#21644;&#29992;&#20110;&#35843;&#26597;&#30340;&#28176;&#36827;&#35299;&#20923;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15210</link><description>&lt;p&gt;
&#35757;&#32451;&#26089;&#26399;&#24433;&#21709;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Early Period of Training Impacts Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15210
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#30340;&#24433;&#21709;&#23545;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#36890;&#36807;&#25506;&#31350;&#23398;&#20064;&#21160;&#24577;&#21644;&#29992;&#20110;&#35843;&#26597;&#30340;&#28176;&#36827;&#35299;&#20923;&#26041;&#27861;&#65292;&#25581;&#31034;&#20102;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#30340;&#24046;&#24322;&#26174;&#33879;&#24433;&#21709;&#20998;&#24067;&#20869;&#65288;ID&#65289;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#24448;&#24448;&#23545;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#25935;&#24863;&#65292;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#21464;&#24471;&#19981;&#22826;&#21487;&#38752;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#22797;&#26434;&#24615;&#21644;&#32570;&#20047;&#26377;&#25928;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#26089;&#26399;&#35757;&#32451;&#38454;&#27573;&#23545;OOD&#27867;&#21270;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23398;&#20064;&#21160;&#24577;&#21644;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#30340;OOD&#27867;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21033;&#29992;Fisher&#20449;&#24687;&#30340;&#30165;&#36857;&#21644;&#38160;&#21033;&#24230;&#65292;&#37325;&#28857;&#20851;&#27880;&#28176;&#36827;&#35299;&#20923;&#65288;&#21363;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#35299;&#20923;&#21442;&#25968;&#65289;&#20316;&#20026;&#30740;&#31350;&#26041;&#27861;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#35777;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;1&#65289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36873;&#25321;&#19981;&#21516;&#26102;&#38388;&#28857;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65292;&#21363;&#36880;&#28176;&#35299;&#20923;&#30340;&#23454;&#29616;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15210v1 Announce Type: new  Abstract: Prior research has found that differences in the early period of neural network training significantly impact the performance of in-distribution (ID) tasks. However, neural networks are often sensitive to out-of-distribution (OOD) data, making them less reliable in downstream applications. Yet, the impact of the early training period on OOD generalization remains understudied due to its complexity and lack of effective analytical methodologies. In this work, we investigate the relationship between learning dynamics and OOD generalization during the early period of neural network training. We utilize the trace of Fisher Information and sharpness, with a focus on gradual unfreezing (i.e. progressively unfreezing parameters during training) as the methodology for investigation. Through a series of empirical experiments, we show that 1) selecting the number of trainable parameters at different times during training, i.e. realized by gradual 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.11780</link><description>&lt;p&gt;
Prompt-Singer: &#24102;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#21487;&#25511;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Prompt-Singer: Controllable Singing-Voice-Synthesis with Natural Language Prompt
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11780
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;&#26041;&#27861;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#21644;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#38899;&#39640;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30340;&#21809;&#27468;&#22768;&#38899;&#21512;&#25104;(SVS)&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#38899;&#39057;&#36136;&#37327;&#21644;&#33258;&#28982;&#24230;&#65292;&#28982;&#32780;&#23427;&#20204;&#32570;&#20047;&#26174;&#24335;&#25511;&#21046;&#21512;&#25104;&#21809;&#27468;&#39118;&#26684;&#23646;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Prompt-Singer&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#29992;&#33258;&#28982;&#35821;&#35328;&#25511;&#21046;&#27468;&#25163;&#24615;&#21035;&#12289;&#38899;&#22495;&#21644;&#38899;&#37327;&#30340;SVS&#26041;&#27861;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#20165;&#35299;&#30721;&#22120;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26550;&#26500;&#65292;&#20855;&#26377;&#22810;&#23610;&#24230;&#23618;&#27425;&#32467;&#26500;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#31163;&#38899;&#39640;&#34920;&#31034;&#30340;&#33539;&#22260;&#26059;&#24459;&#35299;&#32806;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22522;&#20110;&#25991;&#26412;&#30340;&#38899;&#22495;&#25511;&#21046;&#21516;&#26102;&#20445;&#25345;&#20102;&#26059;&#24459;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21253;&#25324;&#19981;&#21516;&#31867;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#25991;&#26412;&#32534;&#30721;&#22120;&#24494;&#35843;&#65292;&#20197;&#21450;&#24341;&#20837;&#35821;&#38899;&#25968;&#25454;&#20197;&#20943;&#36731;&#25968;&#25454;&#31232;&#32570;&#24615;&#65292;&#26088;&#22312;&#20419;&#36827;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#33391;&#22909;&#30340;&#25511;&#21046;&#33021;&#21147;&#21644;&#38899;&#39057;&#36136;&#37327;&#12290;&#38899;&#39057;&#31034;&#20363;&#21487;&#35775;&#38382; http://prompt-singer.
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11780v1 Announce Type: cross  Abstract: Recent singing-voice-synthesis (SVS) methods have achieved remarkable audio quality and naturalness, yet they lack the capability to control the style attributes of the synthesized singing explicitly. We propose Prompt-Singer, the first SVS method that enables attribute controlling on singer gender, vocal range and volume with natural language. We adopt a model architecture based on a decoder-only transformer with a multi-scale hierarchy, and design a range-melody decoupled pitch representation that enables text-conditioned vocal range control while keeping melodic accuracy. Furthermore, we explore various experiment settings, including different types of text representations, text encoder fine-tuning, and introducing speech data to alleviate data scarcity, aiming to facilitate further research. Experiments show that our model achieves favorable controlling ability and audio quality. Audio samples are available at http://prompt-singer.
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.08352</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21450;&#19982;&#20256;&#32479;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08352
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#34987;&#35748;&#20026;&#26159;&#24120;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26368;&#37325;&#35201;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#23427;&#20027;&#35201;&#28041;&#21450;&#24212;&#29992;&#36866;&#24403;&#30340;&#25968;&#25454;&#36716;&#25442;&#25805;&#20316;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#19968;&#36807;&#31243;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25163;&#21160;&#21019;&#24314;&#21644;&#27979;&#35797;&#19981;&#21516;&#20505;&#36873;&#22686;&#24378;&#21450;&#20854;&#36229;&#21442;&#25968;&#38656;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21407;&#21017;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20110;AutoML&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;AutoML&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#25805;&#20316;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08352v1 Announce Type: cross  Abstract: Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of technique
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#24182;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#28789;&#27963;&#23398;&#20064;&#36825;&#20123;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2403.01499</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;
&lt;/p&gt;
&lt;p&gt;
Normalising Flow-based Differentiable Particle Filters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#35268;&#21270;&#27969;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#26694;&#26550;&#65292;&#33021;&#22815;&#23454;&#29616;&#26377;&#25928;&#30340;&#27010;&#29575;&#23494;&#24230;&#20272;&#35745;&#24182;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#28789;&#27963;&#23398;&#20064;&#36825;&#20123;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#23558;&#31070;&#32463;&#32593;&#32476;&#24341;&#20837;&#31890;&#23376;&#28388;&#27874;&#22120;&#20013;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#21152;&#65292;&#20363;&#22914;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#65292;&#20197;&#23454;&#29616;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23545;&#38750;&#32447;&#24615;&#38750;&#39640;&#26031;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#36827;&#34892;&#32852;&#21512;&#39034;&#24207;&#29366;&#24577;&#20272;&#35745;&#21644;&#27169;&#22411;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#20027;&#35201;&#30001;&#26222;&#36890;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#19981;&#20801;&#35768;&#23494;&#24230;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#35201;&#20040;&#21463;&#38480;&#20110;&#33258;&#20030;&#31890;&#23376;&#28388;&#27874;&#26694;&#26550;&#65292;&#35201;&#20040;&#37319;&#29992;&#39044;&#23450;&#20041;&#30340;&#20998;&#24067;&#31995;&#21015;&#65288;&#20363;&#22914;&#39640;&#26031;&#20998;&#24067;&#65289;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26356;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#65288;&#26465;&#20214;&#65289;&#27491;&#35268;&#21270;&#27969;&#26500;&#24314;&#21160;&#24577;&#27169;&#22411;&#12289;&#25552;&#35758;&#20998;&#24067;&#21644;&#27979;&#37327;&#27169;&#22411;&#30340;&#21487;&#24494;&#31890;&#23376;&#28388;&#27874;&#22120;&#26694;&#26550;&#12290;&#36825;&#19981;&#20165;&#20351;&#20854;&#33021;&#22815;&#20135;&#29983;&#26377;&#25928;&#30340;&#27010;&#29575;&#23494;&#24230;&#65292;&#36824;&#20351;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#28789;&#27963;&#22320;&#23398;&#20064;&#36825;&#20123;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01499v1 Announce Type: new  Abstract: Recently, there has been a surge of interest in incorporating neural networks into particle filters, e.g. differentiable particle filters, to perform joint sequential state estimation and model learning for non-linear non-Gaussian state-space models in complex environments. Existing differentiable particle filters are mostly constructed with vanilla neural networks that do not allow density estimation. As a result, they are either restricted to a bootstrap particle filtering framework or employ predefined distribution families (e.g. Gaussian distributions), limiting their performance in more complex real-world scenarios. In this paper we present a differentiable particle filtering framework that uses (conditional) normalising flows to build its dynamic model, proposal distribution, and measurement model. This not only enables valid probability densities but also allows the proposed method to adaptively learn these modules in a flexible w
&lt;/p&gt;</description></item><item><title>&#38543;&#30528;GateLoop&#12289;Mamba&#21644;GLA&#31561;&#20855;&#26377;&#20056;&#27861;&#20132;&#20114;&#30340;&#32447;&#24615;&#36882;&#24402;&#39537;&#21160;&#19979;&#30340;&#28145;&#24230;SSM&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.19047</link><description>&lt;p&gt;
&#28145;&#24230;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Theoretical Foundations of Deep Selective State-Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19047
&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;GateLoop&#12289;Mamba&#21644;GLA&#31561;&#20855;&#26377;&#20056;&#27861;&#20132;&#20114;&#30340;&#32447;&#24615;&#36882;&#24402;&#39537;&#21160;&#19979;&#30340;&#28145;&#24230;SSM&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#36229;&#36234;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;&#22914;S4&#65292;&#28304;&#33258;Gu&#31561;&#20154;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65292;&#20316;&#20026;&#24314;&#27169;&#24207;&#21015;&#25968;&#25454;&#30340;&#26377;&#25928;&#26041;&#27861;&#32780;&#26085;&#30410;&#21463;&#21040;&#38738;&#30544;&#12290;&#28145;&#24230;SSM&#22312;&#21508;&#31181;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;transformers&#65292;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#38477;&#20302;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22914;&#26524;&#39537;&#21160;SSM&#30340;&#32447;&#24615;&#36882;&#24402;&#20801;&#35768;&#36755;&#20837;&#21644;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#20056;&#27861;&#20132;&#20114;&#65288;&#22914;GateLoop&#65292;Mamba&#65292;GLA&#65289;&#65292;&#37027;&#20040;&#25152;&#24471;&#21040;&#30340;&#26550;&#26500;&#21487;&#20197;&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#19978;&#36229;&#36234;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25991;&#26412;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#21442;&#25968;&#35268;&#27169;&#36798;&#21040;&#21313;&#20159;&#32423;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;Rough Path Theory&#30340;&#24037;&#20855;&#65292;&#20026;&#36825;&#19968;&#26368;&#36817;&#30340;&#21457;&#29616;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#65306;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#38543;&#26426;&#32447;&#24615;&#36882;&#24402;&#37197;&#22791;&#31616;&#21333;&#30340;&#36755;&#20837;&#25511;&#21046;&#36716;&#25442;&#65288;&#36873;&#25321;&#24615;&#26426;&#21046;&#65289;&#26102;&#65292;&#38544;&#34255;&#29366;&#24577;&#21487;&#34987;&#35777;&#26126;&#26159;&#20302;&#32500;&#30340;&#25237;&#24433;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19047v1 Announce Type: new  Abstract: Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional proj
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.15402</link><description>&lt;p&gt;
&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#65306;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;
&lt;/p&gt;
&lt;p&gt;
Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy Structure Prior
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15402
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#31574;&#30053;&#32467;&#26500;&#20808;&#39564;&#30340;&#39640;&#25928;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#65292;&#36890;&#36807;&#20869;&#22806;&#29615;&#30340;&#23398;&#20064;&#65292;&#23454;&#29616;&#20102;&#25235;&#21462;&#12289;&#35266;&#23519;&#21644;&#25918;&#32622;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#20219;&#21153;&#65292;&#21363;&#26426;&#22120;&#20154;&#24212;&#37325;&#26032;&#37197;&#32622;&#29289;&#20307;&#21040;&#30001;RGB-D&#22270;&#20687;&#25351;&#23450;&#30340;&#26399;&#26395;&#30446;&#26631;&#37197;&#32622;&#20013;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#25972;&#21512;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22359;&#26469;&#25506;&#32034;&#26410;&#30693;&#29289;&#20307;&#37325;&#26032;&#25490;&#21015;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#24863;&#30693;&#35823;&#24046;&#25935;&#24863;&#65292;&#24182;&#19988;&#36739;&#23569;&#20851;&#27880;&#20219;&#21153;&#32423;&#24615;&#33021;&#12290;&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26377;&#25928;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#24863;&#30693;&#22122;&#22768;&#20013;&#37325;&#26032;&#25490;&#21015;&#26410;&#30693;&#29289;&#20307;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;&#22122;&#22768;&#24863;&#30693;&#22914;&#20309;&#20197;&#20998;&#31163;&#30340;&#26041;&#24335;&#24433;&#21709;&#25235;&#21462;&#21644;&#25918;&#32622;&#65292;&#24182;&#23637;&#31034;&#36825;&#26679;&#30340;&#20998;&#31163;&#32467;&#26500;&#19981;&#23481;&#26131;&#25913;&#21892;&#20219;&#21153;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#20998;&#31163;&#32467;&#26500;&#20316;&#20026;&#20808;&#39564;&#30340;GSP&#65292;&#19968;&#20010;&#21452;&#29615;&#31995;&#32479;&#12290;&#23545;&#20110;&#20869;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#20027;&#21160;&#35266;&#23519;&#31574;&#30053;&#20197;&#25552;&#39640;&#25918;&#32622;&#30340;&#24863;&#30693;&#12290;&#23545;&#20110;&#22806;&#29615;&#65292;&#25105;&#20204;&#23398;&#20064;&#19968;&#20010;&#25235;&#21462;&#31574;&#30053;&#65292;&#24847;&#35782;&#21040;&#29289;&#20307;&#21305;&#37197;&#21644;&#25235;&#21462;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15402v1 Announce Type: cross  Abstract: We focus on the task of unknown object rearrangement, where a robot is supposed to re-configure the objects into a desired goal configuration specified by an RGB-D image. Recent works explore unknown object rearrangement systems by incorporating learning-based perception modules. However, they are sensitive to perception error, and pay less attention to task-level performance. In this paper, we aim to develop an effective system for unknown object rearrangement amidst perception noise. We theoretically reveal the noisy perception impacts grasp and place in a decoupled way, and show such a decoupled structure is non-trivial to improve task optimality. We propose GSP, a dual-loop system with the decoupled structure as prior. For the inner loop, we learn an active seeing policy for self-confident object matching to improve the perception of place. For the outer loop, we learn a grasp policy aware of object matching and grasp capability gu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;LLMs&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.15173</link><description>&lt;p&gt;
&#26080;&#30171;&#20154;&#24037;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20108;&#38454;&#24494;&#35843;&#65306;&#19968;&#31181;&#22522;&#20110;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;LLMs&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32972;&#21521;&#20256;&#25773;&#36807;&#31243;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;GPU&#20869;&#23384;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36716;&#21521;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21463;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#21442;&#25968;&#26354;&#29575;&#30340;&#24322;&#36136;&#24615;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#21033;&#29992;&#23545;&#35282;Hessian&#22686;&#24378;&#38646;&#38454;&#20248;&#21270;&#22120;&#36827;&#34892;LLMs&#24494;&#35843;&#30340;&#24037;&#20316;&#12290;HiZOO&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#19988;&#27599;&#27493;&#21482;&#22686;&#21152;&#20102;&#19968;&#20010;&#21069;&#21521;&#20256;&#36882;&#12290;&#23545;&#21508;&#31181;&#27169;&#22411;&#65288;350M&#12316;66B&#21442;&#25968;&#65289;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;HiZOO&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35757;&#32451;&#27493;&#39588;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;HiZOO&#22312;&#27979;&#35797;&#20989;&#25968;&#19978;&#30340;&#20248;&#21270;&#36712;&#36857;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15173v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, il
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14746</link><description>&lt;p&gt;
&#25193;&#23637;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14746
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#21363;&#22823;&#37096;&#20998;&#21442;&#25968;&#20026;&#38646;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21363;&#37027;&#20123;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#35757;&#32451;&#25439;&#22833;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20272;&#35745;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#20013;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#19978;&#19979;&#30028;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#65306;(1)&#35201;&#22312;&#35757;&#32451;&#35821;&#26009;&#20013;&#34920;&#31034;&#30340;&#25216;&#33021;&#25968;&#37327;&#32763;&#20493;&#65292;&#38656;&#35201;&#23558;&#35821;&#26009;&#35268;&#27169;&#22823;&#32422;&#25193;&#23637;&#19977;&#21040;&#20116;&#20493;&#65292;(2)&#23545;&#20110;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;$N$&#21644;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;$D$&#28385;&#36275;$N \sim D^{0.58}$&#30340;&#20851;&#31995;&#65292;(3)&#22914;&#26524;&#19968;&#20010;LLM&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#65292;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#20986;&#26032;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#26367;&#20195;&#26041;&#27861;&#20013;&#65292;&#20851;&#20110;&#20302;&#32500;&#36817;&#20284;&#31209;&#30340;&#19968;&#20010;&#19979;&#30028;&#65292;&#20174;&#32780;&#20445;&#35777;&#21487;&#38752;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#23558;&#26377;&#25928;&#32500;&#24230;&#19982;&#26368;&#22823;&#32479;&#35745;&#26464;&#26438;&#24471;&#20998;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>https://arxiv.org/abs/2402.12885</link><description>&lt;p&gt;
&#23545;&#26368;&#22823;&#36793;&#38469;&#33258;&#30001;&#24230;&#30340;&#19968;&#20010;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
A Bound on the Maximal Marginal Degrees of Freedom
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12885
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#26367;&#20195;&#26041;&#27861;&#20013;&#65292;&#20851;&#20110;&#20302;&#32500;&#36817;&#20284;&#31209;&#30340;&#19968;&#20010;&#19979;&#30028;&#65292;&#20174;&#32780;&#20445;&#35777;&#21487;&#38752;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#23558;&#26377;&#25928;&#32500;&#24230;&#19982;&#26368;&#22823;&#32479;&#35745;&#26464;&#26438;&#24471;&#20998;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12885v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#36890;&#29992;&#26680;&#23725;&#22238;&#24402;&#22312;&#20869;&#23384;&#20998;&#37197;&#21644;&#35745;&#31639;&#26102;&#38388;&#19978;&#25104;&#26412;&#39640;&#26114;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#26680;&#23725;&#22238;&#24402;&#30340;&#20302;&#31209;&#36817;&#20284;&#21644;&#26367;&#20195;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#36825;&#20123;&#22256;&#38590;&#12290;&#26412;&#25991;&#30340;&#22522;&#26412;&#36129;&#29486;&#22312;&#20110;&#23545;&#20302;&#32500;&#36817;&#20284;&#30340;&#31209;&#25552;&#20986;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#35201;&#27714;&#20854;&#20445;&#25345;&#21487;&#38752;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#35813;&#30028;&#38480;&#23558;&#26377;&#25928;&#32500;&#24230;&#19982;&#26368;&#22823;&#32479;&#35745;&#26464;&#26438;&#24471;&#20998;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#36890;&#36807;&#28041;&#21450;&#26680;&#30340;&#27491;&#21017;&#24615;&#26469;&#34920;&#24449;&#26377;&#25928;&#32500;&#24230;&#21450;&#20854;&#38543;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#22686;&#38271;&#34892;&#20026;&#12290;&#23545;&#20110;&#36866;&#24403;&#36873;&#25321;&#30340;&#26680;&#65292;&#36825;&#31181;&#22686;&#38271;&#34987;&#35777;&#26126;&#26159;&#23545;&#25968;&#28176;&#36817;&#30340;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#20302;&#31209;&#36817;&#20284;&#20316;&#20026;Nystro&#776;m&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12885v1 Announce Type: cross  Abstract: Common kernel ridge regression is expensive in memory allocation and computation time. This paper addresses low rank approximations and surrogates for kernel ridge regression, which bridge these difficulties. The fundamental contribution of the paper is a lower bound on the rank of the low dimensional approximation, which is required such that the prediction power remains reliable. The bound relates the effective dimension with the largest statistical leverage score. We characterize the effective dimension and its growth behavior with respect to the regularization parameter by involving the regularity of the kernel. This growth is demonstrated to be asymptotically logarithmic for suitably chosen kernels, justifying low-rank approximations as the Nystr\"om method.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.12499</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#23454;&#29616;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Automated Security Response through Online Learning with Adaptive Conjectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12499
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#33258;&#36866;&#24212;&#29468;&#24819;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#26041;&#27861;&#65292;&#20854;&#20013;&#28216;&#25103;&#21442;&#19982;&#32773;&#36890;&#36807;Bayesian&#23398;&#20064;&#35843;&#25972;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#31574;&#30053;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#26368;&#20339;&#25311;&#21512;&#65292;&#25552;&#39640;&#20102;&#25512;&#28436;&#22312;&#29468;&#24819;&#27169;&#22411;&#19979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38024;&#23545;IT&#22522;&#30784;&#35774;&#26045;&#30340;&#33258;&#21160;&#21270;&#23433;&#20840;&#21709;&#24212;&#65292;&#24182;&#23558;&#25915;&#20987;&#32773;&#21644;&#38450;&#24481;&#32773;&#20043;&#38388;&#30340;&#20114;&#21160;&#24418;&#24335;&#34920;&#36848;&#20026;&#19968;&#20010;&#37096;&#20998;&#35266;&#27979;&#12289;&#38750;&#24179;&#31283;&#21338;&#24328;&#12290;&#25105;&#20204;&#25918;&#23485;&#20102;&#28216;&#25103;&#27169;&#22411;&#27491;&#30830;&#35268;&#23450;&#30340;&#26631;&#20934;&#20551;&#35774;&#65292;&#24182;&#32771;&#34385;&#27599;&#20010;&#21442;&#19982;&#32773;&#23545;&#27169;&#22411;&#26377;&#19968;&#20010;&#27010;&#29575;&#24615;&#29468;&#24819;&#65292;&#21487;&#33021;&#22312;&#26576;&#31181;&#24847;&#20041;&#19978;&#38169;&#35823;&#35268;&#23450;&#65292;&#21363;&#30495;&#23454;&#27169;&#22411;&#30340;&#27010;&#29575;&#20026;0&#12290;&#36825;&#31181;&#24418;&#24335;&#20801;&#35768;&#25105;&#20204;&#25429;&#25417;&#20851;&#20110;&#22522;&#30784;&#35774;&#26045;&#21644;&#21442;&#19982;&#32773;&#24847;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#22312;&#32447;&#23398;&#20064;&#26377;&#25928;&#30340;&#28216;&#25103;&#31574;&#30053;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20010;&#21442;&#19982;&#32773;&#36890;&#36807;&#36125;&#21494;&#26031;&#23398;&#20064;&#36845;&#20195;&#22320;&#35843;&#25972;&#20854;&#29468;&#24819;&#65292;&#24182;&#36890;&#36807;&#25512;&#28436;&#26356;&#26032;&#20854;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29468;&#24819;&#20250;&#25910;&#25947;&#21040;&#26368;&#20339;&#25311;&#21512;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#20855;&#26377;&#29468;&#27979;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#25512;&#28436;&#23454;&#29616;&#24615;&#33021;&#25913;&#36827;&#30340;&#19978;&#38480;&#12290;&#20026;&#20102;&#21051;&#30011;&#28216;&#25103;&#30340;&#31283;&#23450;&#29366;&#24577;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Berk-Nash&#24179;&#34913;&#30340;&#19968;&#20010;&#21464;&#31181;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12499v1 Announce Type: cross  Abstract: We study automated security response for an IT infrastructure and formulate the interaction between an attacker and a defender as a partially observed, non-stationary game. We relax the standard assumption that the game model is correctly specified and consider that each player has a probabilistic conjecture about the model, which may be misspecified in the sense that the true model has probability 0. This formulation allows us to capture uncertainty about the infrastructure and the intents of the players. To learn effective game strategies online, we design a novel method where a player iteratively adapts its conjecture using Bayesian learning and updates its strategy through rollout. We prove that the conjectures converge to best fits, and we provide a bound on the performance improvement that rollout enables with a conjectured model. To characterize the steady state of the game, we propose a variant of the Berk-Nash equilibrium. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.05966</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#21644;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethink Model Re-Basin and the Linear Mode Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22823;&#37096;&#20998;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#35299;&#21487;&#20197;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22522;&#24213;&#65292;&#21482;&#26159;&#39034;&#24207;&#21487;&#33021;&#19981;&#21516;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#38454;&#27573;&#65292;&#23545;&#20110;&#27169;&#22411;&#24179;&#22343;&#21270;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37325;&#26032;&#22522;&#24213;&#31574;&#30053;&#22312;&#25928;&#26524;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23545;&#24213;&#23618;&#26426;&#21046;&#30340;&#29702;&#35299;&#19981;&#22815;&#20840;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#20934;&#20570;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#39057;&#32321;&#19981;&#36275;&#20043;&#22788;&#65292;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#30452;&#25509;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21305;&#37197;&#31639;&#27861;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#35266;&#28857;&#19981;&#20165;&#28548;&#28165;&#21644;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#36824;&#20419;&#36827;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;&#20363;&#22914;&#65292;&#23427;&#23558;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;&#19982;&#21098;&#26525;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#65292;&#21487;&#20197;&#30452;&#25509;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21098;&#26525;&#25216;&#26415;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
&lt;/p&gt;</description></item><item><title>Polyp-DDPM&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#21106;&#25513;&#30721;&#29983;&#25104;&#36924;&#30495;&#30340;&#32963;&#32928;&#36947;&#24687;&#32905;&#22270;&#20687;&#65292;&#25552;&#21319;&#20102;&#20998;&#21106;&#25928;&#26524;&#65292;&#24182;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20998;&#21106;&#27169;&#22411;&#36798;&#21040;&#19982;&#30495;&#23454;&#22270;&#20687;&#30456;&#27604;&#21487;&#27604;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04031</link><description>&lt;p&gt;
Polyp-DDPM: &#22522;&#20110;&#25193;&#25955;&#30340;&#35821;&#20041;&#24687;&#32905;&#21512;&#25104;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#20998;&#21106;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Polyp-DDPM: Diffusion-Based Semantic Polyp Synthesis for Enhanced Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04031
&lt;/p&gt;
&lt;p&gt;
Polyp-DDPM&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#21106;&#25513;&#30721;&#29983;&#25104;&#36924;&#30495;&#30340;&#32963;&#32928;&#36947;&#24687;&#32905;&#22270;&#20687;&#65292;&#25552;&#21319;&#20102;&#20998;&#21106;&#25928;&#26524;&#65292;&#24182;&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#25552;&#20379;&#20102;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20351;&#24471;&#20998;&#21106;&#27169;&#22411;&#36798;&#21040;&#19982;&#30495;&#23454;&#22270;&#20687;&#30456;&#27604;&#21487;&#27604;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Polyp-DDPM&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26465;&#20214;&#25513;&#30721;&#19978;&#29983;&#25104;&#36924;&#30495;&#30340;&#24687;&#32905;&#22270;&#20687;&#65292;&#26088;&#22312;&#22686;&#24378;&#32963;&#32928;&#36947;&#24687;&#32905;&#30340;&#20998;&#21106;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38480;&#21046;&#12289;&#39640;&#26114;&#30340;&#27880;&#37322;&#25104;&#26412;&#21644;&#38544;&#31169;&#38382;&#39064;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#27169;&#22411;&#26465;&#20214;&#21270;&#20110;&#20998;&#21106;&#25513;&#30721;&#65288;&#34920;&#31034;&#24322;&#24120;&#21306;&#22495;&#30340;&#20108;&#36827;&#21046;&#25513;&#30721;&#65289;&#65292;Polyp-DDPM&#22312;&#22270;&#20687;&#36136;&#37327;&#21644;&#20998;&#21106;&#24615;&#33021;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65288;Frechet Inception Distance (FID) &#35780;&#20998;&#20026;78.47&#65292;&#32780;&#39640;&#20110;83.79&#30340;&#35780;&#20998;&#65307;Intersection over Union (IoU) &#20026;0.7156&#65292;&#32780;&#22522;&#20934;&#27169;&#22411;&#21512;&#25104;&#22270;&#20687;&#20026;0.6694&#20197;&#19979;&#65292;&#30495;&#23454;&#25968;&#25454;&#20026;0.7067&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#20102;&#39640;&#36136;&#37327;&#12289;&#22810;&#26679;&#21270;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#29992;&#20110;&#35757;&#32451;&#65292;&#20174;&#32780;&#25552;&#21319;&#20102;&#24687;&#32905;&#20998;&#21106;&#27169;&#22411;&#19982;&#30495;&#23454;&#22270;&#20687;&#30340;&#21487;&#27604;&#24615;&#65292;&#24182;&#25552;&#20379;&#26356;&#22823;&#30340;&#25968;&#25454;&#22686;&#24378;&#33021;&#21147;&#20197;&#25913;&#21892;&#20998;&#21106;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study introduces Polyp-DDPM, a diffusion-based method for generating realistic images of polyps conditioned on masks, aimed at enhancing the segmentation of gastrointestinal (GI) tract polyps. Our approach addresses the challenges of data limitations, high annotation costs, and privacy concerns associated with medical images. By conditioning the diffusion model on segmentation masks-binary masks that represent abnormal areas-Polyp-DDPM outperforms state-of-the-art methods in terms of image quality (achieving a Frechet Inception Distance (FID) score of 78.47, compared to scores above 83.79) and segmentation performance (achieving an Intersection over Union (IoU) of 0.7156, versus less than 0.6694 for synthetic images from baseline models and 0.7067 for real data). Our method generates a high-quality, diverse synthetic dataset for training, thereby enhancing polyp segmentation models to be comparable with real images and offering greater data augmentation capabilities to improve seg
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#28165;&#26224;&#30340;&#23450;&#20041;&#21644;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.01749</link><description>&lt;p&gt;
&#36808;&#21521;&#22478;&#24066;&#26234;&#33021;&#65306;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#32508;&#36848;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Towards Urban General Intelligence: A Review and Outlook of Urban Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#22312;&#26234;&#33021;&#22478;&#24066;&#21457;&#23637;&#20013;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;&#26041;&#27861;&#12290;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#28165;&#26224;&#30340;&#23450;&#20041;&#21644;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#29616;&#24050;&#25104;&#20026;&#26234;&#33021;&#22478;&#24066;&#26381;&#21153;&#36827;&#27493;&#30340;&#26680;&#24515;&#65292;&#23545;&#25552;&#39640;&#22478;&#24066;&#29615;&#22659;&#30340;&#25928;&#29575;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#23452;&#23621;&#24615;&#36215;&#21040;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;ChatGPT&#31561;&#22522;&#30784;&#27169;&#22411;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#20010;&#38761;&#21629;&#24615;&#30340;&#36716;&#21464;&#12290;&#23427;&#20204;&#22312;&#19978;&#19979;&#25991;&#29702;&#35299;&#12289;&#38382;&#39064;&#35299;&#20915;&#21644;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#30340;&#26080;&#19982;&#20262;&#27604;&#30340;&#33021;&#21147;&#34920;&#26126;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25972;&#21512;&#21040;&#22478;&#24066;&#39046;&#22495;&#20013;&#21487;&#33021;&#23545;&#26234;&#33021;&#22478;&#24066;&#30340;&#21457;&#23637;&#20135;&#29983;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;&#23613;&#31649;&#23545;&#22478;&#24066;&#22522;&#30784;&#27169;&#22411;&#65288;UFMs&#65289;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#38754;&#20020;&#30528;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#32570;&#20047;&#28165;&#26224;&#30340;&#23450;&#20041;&#12289;&#31995;&#32479;&#24615;&#30340;&#32508;&#36848;&#21644;&#21487;&#26222;&#36941;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;UFM&#30340;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20102;&#26500;&#24314;&#23427;&#20204;&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#23545;&#24403;&#21069;&#19982;UFM&#30456;&#20851;&#30340;&#24037;&#20316;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques are now integral to the advancement of intelligent urban services, playing a crucial role in elevating the efficiency, sustainability, and livability of urban environments. The recent emergence of foundation models such as ChatGPT marks a revolutionary shift in the fields of machine learning and artificial intelligence. Their unparalleled capabilities in contextual understanding, problem solving, and adaptability across a wide range of tasks suggest that integrating these models into urban domains could have a transformative impact on the development of smart cities. Despite growing interest in Urban Foundation Models~(UFMs), this burgeoning field faces challenges such as a lack of clear definitions, systematic reviews, and universalizable solutions. To this end, this paper first introduces the concept of UFM and discusses the unique challenges involved in building them. We then propose a data-centric taxonomy that categorizes current UFM-related works, base
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#38750;&#24179;&#31283;&#24615;&#24182;&#24320;&#21457;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2203.01707</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#27979;&#35797;&#24179;&#31283;&#24615;&#21644;&#21464;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Testing Stationarity and Change Point Detection in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.01707
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#35797;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#38750;&#24179;&#31283;&#24615;&#24182;&#24320;&#21457;&#24207;&#36143;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#21487;&#33021;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#12290;&#35768;&#22810;&#25991;&#29486;&#20013;&#29616;&#26377;&#30340;RL&#31639;&#27861;&#20381;&#36182;&#20110;&#38656;&#35201;&#31995;&#32479;&#36716;&#25442;&#21644;&#22870;&#21169;&#20989;&#25968;&#38543;&#26102;&#38388;&#20445;&#25345;&#24658;&#23450;&#30340;&#24179;&#31283;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#20013;&#24179;&#31283;&#24615;&#20551;&#35774;&#26159;&#26377;&#38480;&#21046;&#30340;&#65292;&#24182;&#19988;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#21487;&#33021;&#34987;&#36829;&#21453;&#65292;&#21253;&#25324;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#31227;&#21160;&#20581;&#24247;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#19968;&#33268;&#30340;&#31243;&#24207;&#65292;&#22522;&#20110;&#39044;&#20808;&#25910;&#38598;&#30340;&#21382;&#21490;&#25968;&#25454;&#27979;&#35797;&#26368;&#20248;Q&#20989;&#25968;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#22312;&#32447;&#25968;&#25454;&#25910;&#38598;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;&#26816;&#39564;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#39034;&#24207;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#28982;&#22320;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;RL&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#36827;&#34892;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#29702;&#35770;&#32467;&#26524;&#12289;&#20223;&#30495;&#30740;&#31350;&#21644;&#23454;&#36341;&#20013;&#30340;&#26696;&#20363;&#24471;&#21040;&#20102;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.01707v3 Announce Type: replace-cross  Abstract: We consider offline reinforcement learning (RL) methods in possibly nonstationary environments. Many existing RL algorithms in the literature rely on the stationarity assumption that requires the system transition and the reward function to be constant over time. However, the stationarity assumption is restrictive in practice and is likely to be violated in a number of applications, including traffic signal control, robotics and mobile health. In this paper, we develop a consistent procedure to test the nonstationarity of the optimal Q-function based on pre-collected historical data, without additional online data collection. Based on the proposed test, we further develop a sequential change point detection method that can be naturally coupled with existing state-of-the-art RL methods for policy optimization in nonstationary environments. The usefulness of our method is illustrated by theoretical results, simulation studies, an
&lt;/p&gt;</description></item><item><title>SymbolNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#65292;&#21516;&#26102;&#20248;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#22238;&#24402;&#12290;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#36866;&#24212;&#35843;&#25972;&#33258;&#36523;&#30340;&#24378;&#24230;&#65292;&#24182;&#25910;&#25947;&#21040;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SymbolNet&#33021;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#36229;&#36807;10&#20010;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.09949</link><description>&lt;p&gt;
SymbolNet: &#33258;&#36866;&#24212;&#21160;&#24577;&#20462;&#21098;&#30340;&#31070;&#32463;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
SymbolNet: Neural Symbolic Regression with Adaptive Dynamic Pruning. (arXiv:2401.09949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09949
&lt;/p&gt;
&lt;p&gt;
SymbolNet&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#65292;&#21516;&#26102;&#20248;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#20102;&#31526;&#21495;&#22238;&#24402;&#12290;&#36890;&#36807;&#24341;&#20837;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#36866;&#24212;&#35843;&#25972;&#33258;&#36523;&#30340;&#24378;&#24230;&#65292;&#24182;&#25910;&#25947;&#21040;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SymbolNet&#33021;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#36229;&#36807;10&#20010;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#36951;&#20256;&#32534;&#31243;&#30340;&#20351;&#29992;&#30456;&#21453;&#65292;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21487;&#22312;&#39640;&#36755;&#20837;&#32500;&#24230;&#19979;&#26377;&#25928;&#25193;&#23637;&#65292;&#24182;&#21033;&#29992;&#26799;&#24230;&#26041;&#27861;&#21152;&#36895;&#26041;&#31243;&#25628;&#32034;&#12290;&#24120;&#35265;&#30340;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#32422;&#26463;&#26041;&#27861;&#20381;&#36182;&#20110;&#22810;&#38454;&#27573;&#20462;&#21098;&#26041;&#27861;&#36827;&#34892;&#24494;&#35843;&#65292;&#20294;&#36825;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#24615;&#33021;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#21363;SymbolNet&#65292;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#23454;&#29616;&#31526;&#21495;&#22238;&#24402;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22312;&#21333;&#20010;&#35757;&#32451;&#20013;&#21160;&#24577;&#20462;&#21098;&#27169;&#22411;&#26435;&#37325;&#12289;&#36755;&#20837;&#29305;&#24449;&#21644;&#25968;&#23398;&#36816;&#31639;&#31526;&#65292;&#21516;&#26102;&#20248;&#21270;&#35757;&#32451;&#25439;&#22833;&#21644;&#34920;&#36798;&#24335;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27599;&#20010;&#20462;&#21098;&#31867;&#22411;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#39033;&#65292;&#35813;&#39033;&#21487;&#20197;&#33258;&#36866;&#24212;&#35843;&#25972;&#33258;&#36523;&#30340;&#24378;&#24230;&#65292;&#24182;&#23548;&#33268;&#25910;&#25947;&#21040;&#30446;&#26631;&#31232;&#30095;&#24230;&#27700;&#24179;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#26080;&#27861;&#39640;&#25928;&#22788;&#29702;&#20855;&#26377;&#36229;&#36807;10&#20010;&#36755;&#20837;&#30340;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrary to the use of genetic programming, the neural network approach to symbolic regression can scale well with high input dimension and leverage gradient methods for faster equation searching. Common ways of constraining expression complexity have relied on multistage pruning methods with fine-tuning, but these often lead to significant performance loss. In this work, we propose SymbolNet, a neural network approach to symbolic regression in a novel framework that enables dynamic pruning of model weights, input features, and mathematical operators in a single training, where both training loss and expression complexity are optimized simultaneously. We introduce a sparsity regularization term per pruning type, which can adaptively adjust its own strength and lead to convergence to a target sparsity level. In contrast to most existing symbolic regression methods that cannot efficiently handle datasets with more than $O$(10) inputs, we demonstrate the effectiveness of our model on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05857</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Improving Summarization with Human Edits. (arXiv:2310.05857v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#30340;&#21453;&#39304;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#25216;&#26415;&#23558;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#19982;&#27169;&#22411;&#29983;&#25104;&#25968;&#25454;&#32467;&#21512;&#36215;&#26469;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#33539;&#24335;&#23398;&#20064;&#21487;&#20197;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#25991;&#26412;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#22312;&#36890;&#29992;&#39046;&#22495;&#25277;&#35937;&#21270;&#25688;&#35201;&#29983;&#25104;&#20013;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24182;&#33719;&#24471;&#20102;&#36229;&#36234;&#20256;&#32479;&#20284;&#28982;&#35757;&#32451;&#30340;&#25688;&#35201;&#36136;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#19968;&#31181;&#36739;&#23569;&#25506;&#32034;&#30340;&#20154;&#31867;&#21453;&#39304;&#24418;&#24335;&#8212;&#8212;&#20154;&#24037;&#32534;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25216;&#26415;&#8212;&#8212;&#24207;&#21015;&#23545;&#40784;&#65288;&#19981;&#65289;&#20284;&#28982;&#35757;&#32451;(SALT)&#65292;&#22312;&#35757;&#32451;&#24490;&#29615;&#20013;&#21516;&#26102;&#20351;&#29992;&#20154;&#24037;&#32534;&#36753;&#21644;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#29616;&#26377;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#22522;&#20934;&#25688;&#35201;&#26469;&#27169;&#25311;&#20154;&#24037;&#32534;&#36753;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#21518;&#33719;&#21462;&#30340;&#27169;&#22411;&#29983;&#25104;&#25688;&#35201;&#65292;&#20197;&#20943;&#23569;&#23545;&#26114;&#36149;&#30340;&#20154;&#24037;&#32534;&#36753;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#21453;&#39304;&#30340;&#25506;&#32034;&#20174;&#36890;&#29992;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#25193;&#23637;&#21040;&#21307;&#23398;&#39046;&#22495;&#25688;&#35201;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;SALT&#22312;&#25913;&#36827;&#25688;&#35201;&#29983;&#25104;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown the promise of learning with human feedback paradigms to produce human-determined high-quality text. Existing works use human feedback to train large language models (LLMs) in general domain abstractive summarization and have obtained summary quality exceeding traditional likelihood training. In this paper, we focus on a less explored form of human feedback -- Human Edits. We propose Sequence Alignment (un)Likelihood Training (SALT), a novel technique to use both the human-edited and model-generated data together in the training loop. In addition, we demonstrate simulating Human Edits with ground truth summaries coming from existing training data -Imitation edits, along with the model-generated summaries obtained after the training, to reduce the need for expensive human-edit data. In our experiments, we extend human feedback exploration from general domain summarization to medical domain summarization. Our results demonstrate the effectiveness of SALT in improv
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RLSQM&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#26102;&#21487;&#33021;&#29983;&#25104;&#19981;&#33391;&#20195;&#30721;&#24322;&#21619;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#30340;&#22870;&#21169;&#27169;&#22411;&#21644;&#21033;&#29992;PPO&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#36136;&#37327;&#25351;&#26631;&#21644;&#25972;&#20307;&#36136;&#37327;&#30340;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.02368</link><description>&lt;p&gt;
&#20174;&#33258;&#21160;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21333;&#20803;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Automatic Feedback for High-Quality Unit Test Generation. (arXiv:2310.02368v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RLSQM&#65289;&#30340;&#26032;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#33258;&#21160;&#29983;&#25104;&#27979;&#35797;&#29992;&#20363;&#26102;&#21487;&#33021;&#29983;&#25104;&#19981;&#33391;&#20195;&#30721;&#24322;&#21619;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35757;&#32451;&#29305;&#23450;&#30340;&#22870;&#21169;&#27169;&#22411;&#21644;&#21033;&#29992;PPO&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#21333;&#20010;&#36136;&#37327;&#25351;&#26631;&#21644;&#25972;&#20307;&#36136;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#27979;&#35797;&#26159;&#36719;&#20214;&#24320;&#21457;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21019;&#24314;&#31526;&#21512;&#26368;&#20339;&#23454;&#36341;&#30340;&#39640;&#36136;&#37327;&#27979;&#35797;&#23545;&#20110;&#26377;&#25928;&#30340;&#32500;&#25252;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#21253;&#25324;&#33258;&#21160;&#21019;&#24314;&#27979;&#35797;&#29992;&#20363;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;LLM&#36890;&#24120;&#22312;&#22823;&#37327;&#20844;&#24320;&#21487;&#29992;&#30340;&#20195;&#30721;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21487;&#33021;&#21253;&#21547;&#19981;&#31526;&#21512;&#26368;&#20339;&#23454;&#36341;&#29978;&#33267;&#21253;&#21547;&#27979;&#35797;&#20195;&#30721;&#24322;&#21619;&#65288;&#21453;&#27169;&#24335;&#65289;&#30340;&#27979;&#35797;&#29992;&#20363;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#24378;&#21270;&#23398;&#20064;&#65288;RLSQM&#65289;&#30340;&#26032;&#25216;&#26415;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;LLM&#29983;&#25104;&#30340;&#21453;&#27169;&#24335;&#65292;&#24182;&#23637;&#31034;&#20102;LLM&#21487;&#20197;&#29983;&#25104;&#19981;&#33391;&#30340;&#27979;&#35797;&#20195;&#30721;&#24322;&#21619;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#38745;&#24577;&#36136;&#37327;&#25351;&#26631;&#35757;&#32451;&#20102;&#19987;&#38376;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#21033;&#29992;Proximal Policy Optimization &#65288;PPO&#65289;&#26469;&#35757;&#32451;&#36880;&#20010;&#20248;&#21270;&#21333;&#20010;&#36136;&#37327;&#25351;&#26631;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#22870;&#21169;&#34701;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#22870;&#21169;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#23545;&#25972;&#20307;&#36136;&#37327;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software testing is a crucial aspect of software development, and the creation of high-quality tests that adhere to best practices is essential for effective maintenance. Recently, Large Language Models (LLMs) have gained popularity for code generation, including the automated creation of test cases. However, these LLMs are often trained on vast amounts of publicly available code, which may include test cases that do not adhere to best practices and may even contain test smells (anti-patterns). To address this issue, we propose a novel technique called Reinforcement Learning from Static Quality Metrics (RLSQM). To begin, we analyze the anti-patterns generated by the LLM and show that LLMs can generate undesirable test smells. Thus, we train specific reward models for each static quality metric, then utilize Proximal Policy Optimization (PPO) to train models for optimizing a single quality metric at a time. Furthermore, we amalgamate these rewards into a unified reward model aimed at ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#39046;&#22495;&#20013;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#32570;&#38519;&#30340;&#26032;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#22823;&#22411;&#27169;&#22411;&#25216;&#26415;&#36335;&#32447;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.09721</link><description>&lt;p&gt;
&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#26032;&#35299;&#20915;&#26041;&#26696;&#21644;&#20855;&#20307;&#23454;&#26045;&#27493;&#39588;
&lt;/p&gt;
&lt;p&gt;
A new solution and concrete implementation steps for Artificial General Intelligence. (arXiv:2308.09721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#22312;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#39046;&#22495;&#20013;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#32570;&#38519;&#30340;&#26032;&#26041;&#26696;&#65292;&#24182;&#20998;&#26512;&#20102;&#22823;&#22411;&#27169;&#22411;&#25216;&#26415;&#36335;&#32447;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20027;&#27969;&#20154;&#24037;&#26234;&#33021;&#36890;&#24120;&#37319;&#29992;&#8220;&#27880;&#24847;&#26426;&#21046;+&#28145;&#24230;&#23398;&#20064;&#8221;+&#8220;&#24378;&#21270;&#23398;&#20064;&#8221;&#30340;&#25216;&#26415;&#36335;&#24452;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#25472;&#36215;&#20102;&#22823;&#27169;&#22411;&#30340;&#25216;&#26415;&#28010;&#28526;&#12290;&#20294;&#22312;&#28041;&#21450;&#19982;&#23454;&#38469;&#29615;&#22659;&#20132;&#20114;&#30340;&#39046;&#22495;&#65292;&#22914;&#20859;&#32769;&#25252;&#29702;&#12289;&#23478;&#24237;&#20445;&#22982;&#12289;&#20892;&#19994;&#29983;&#20135;&#21644;&#36710;&#36742;&#39550;&#39542;&#31561;&#65292;&#35797;&#38169;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#35797;&#38169;&#30340;&#24378;&#21270;&#23398;&#20064;&#36807;&#31243;&#24456;&#38590;&#23454;&#29616;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#24212;&#29992;&#20110;&#20219;&#20309;&#39046;&#22495;&#30340;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#65288;AGI&#65289;&#65292;&#25105;&#20204;&#38656;&#35201;&#21516;&#26102;&#21033;&#29992;&#29616;&#26377;&#25216;&#26415;&#24182;&#35299;&#20915;&#29616;&#26377;&#25216;&#26415;&#30340;&#32570;&#38519;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#30340;&#25216;&#26415;&#28010;&#28526;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#22823;&#22411;&#27169;&#22411;&#25216;&#26415;&#36335;&#32447;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
At present, the mainstream artificial intelligence generally adopts the technical path of "attention mechanism + deep learning" + "reinforcement learning". It has made great progress in the field of AIGC (Artificial Intelligence Generated Content), setting off the technical wave of big models[ 2][13 ]. But in areas that need to interact with the actual environment, such as elderly care, home nanny, agricultural production, and vehicle driving, trial and error are expensive and a reinforcement learning process that requires much trial and error is difficult to achieve. Therefore, in order to achieve Artificial General Intelligence(AGI) that can be applied to any field, we need to use both existing technologies and solve the defects of existing technologies, so as to further develop the technological wave of artificial intelligence. In this paper, we analyze the limitations of the technical route of large models, and by addressing these limitations, we propose solutions, thus solving the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.08941</link><description>&lt;p&gt;
NTK-&#36817;&#20284;MLP&#34701;&#21512;&#29992;&#20110;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
NTK-approximating MLP Fusion for Efficient Language Model Fine-tuning. (arXiv:2307.08941v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08941
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#20999;&#21521;&#26680;&#36817;&#20284;MLP&#34701;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#38477;&#20302;&#35745;&#31639;&#21644;&#23384;&#20648;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#25345;&#36739;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#24050;&#25104;&#20026;&#20027;&#35201;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26159;&#24494;&#35843;PLM&#21644;&#36827;&#34892;&#25512;&#29702;&#20063;&#26159;&#26114;&#36149;&#30340;&#65292;&#29305;&#21035;&#26159;&#22312;&#35745;&#31639;&#33021;&#21147;&#36739;&#20302;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#24050;&#32463;&#24191;&#27867;&#30740;&#31350;&#20102;&#19968;&#20123;&#36890;&#29992;&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;&#37327;&#21270;&#21644;&#33976;&#39311;&#65289;&#26469;&#20943;&#23569;PLM&#24494;&#35843;&#30340;&#35745;&#31639;/&#23384;&#20648;&#24320;&#38144;&#65292;&#20294;&#24456;&#23569;&#26377;&#19968;&#27425;&#24615;&#21387;&#32553;&#25216;&#26415;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;(MLP)&#27169;&#22359;&#20013;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLM)&#30340;&#31070;&#32463;&#20999;&#21521;&#26680;(NTK)&#65292;&#24182;&#25552;&#20986;&#36890;&#36807;NTK&#36817;&#20284;MLP&#34701;&#21512;&#26469;&#21019;&#24314;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;PLM&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;MLP&#37325;&#26032;&#35270;&#20026;&#19968;&#26463;&#23376;MLP&#65292;&#24182;&#23558;&#23427;&#20204;&#32858;&#31867;&#20026;&#32473;&#23450;&#25968;&#37327;&#30340;&#36136;&#24515;&#65292;&#28982;&#21518;&#23558;&#20854;&#24674;&#22797;&#20026;&#21387;&#32553;&#30340;MLP&#65292;&#24182;&#24847;&#22806;&#22320;&#26174;&#31034;&#20986;&#23545;&#21407;&#22987;PLM&#30340;NTK&#36827;&#34892;&#33391;&#22909;&#36817;&#20284;&#30340;&#25928;&#26524;&#12290;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#39564;&#35777;PLM&#24494;&#35843;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning a pre-trained language model (PLM) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the PLMs and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of PLM fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel (NTK)--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons (MLP) modules in a PLM and propose to coin a lightweight PLM through NTK-approximating MLP fusion. To achieve this, we reconsider the MLP as a bundle of sub-MLPs, and cluster them into a given number of centroids, which can then be restored as a compressed MLP and surprisingly shown to well approximate the NTK of the original PLM. Extensive experiments of PLM fine-tuning on both natural l
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#21040;&#35270;&#39057;&#27969;&#30340;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;TTT&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#21644;&#31163;&#32447;TTT&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#20840;&#26223;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2307.05014</link><description>&lt;p&gt;
&#35270;&#39057;&#27969;&#19978;&#30340;&#27979;&#35797;&#26102;&#22521;&#35757;
&lt;/p&gt;
&lt;p&gt;
Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05014
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#21040;&#35270;&#39057;&#27969;&#30340;&#35774;&#32622;&#20013;&#65292;&#25552;&#20986;&#20102;&#22312;&#32447;TTT&#26041;&#27861;&#65292;&#30456;&#23545;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#21644;&#31163;&#32447;TTT&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#37117;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#21253;&#25324;&#23454;&#20363;&#21644;&#20840;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#23558;&#27979;&#35797;&#26102;&#22521;&#35757;&#65288;TTT&#65289;&#30830;&#23450;&#20026;&#19968;&#31181;&#22312;&#27979;&#35797;&#26102;&#36827;&#19968;&#27493;&#25913;&#36827;&#35757;&#32451;&#27169;&#22411;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#22312;&#23545;&#27599;&#20010;&#27979;&#35797;&#23454;&#20363;&#36827;&#34892;&#39044;&#27979;&#20043;&#21069;&#65292;&#27169;&#22411;&#20250;&#20351;&#29992;&#33258;&#30417;&#30563;&#20219;&#21153;&#65288;&#20363;&#22914;&#20351;&#29992;&#25513;&#34109;&#33258;&#21160;&#32534;&#30721;&#22120;&#36827;&#34892;&#22270;&#20687;&#37325;&#24314;&#65289;&#22312;&#21516;&#19968;&#23454;&#20363;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;TTT&#25193;&#23637;&#21040;&#27969;&#24335;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#22810;&#20010;&#27979;&#35797;&#23454;&#20363;&#65288;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#20026;&#35270;&#39057;&#24103;&#65289;&#25353;&#26102;&#38388;&#39034;&#24207;&#21040;&#36798;&#12290;&#25105;&#20204;&#30340;&#25193;&#23637;&#26159;&#22312;&#32447;TTT&#65306;&#24403;&#21069;&#27169;&#22411;&#20174;&#19978;&#20010;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#28982;&#21518;&#22312;&#24403;&#21069;&#24103;&#21644;&#21069;&#20960;&#20010;&#24103;&#30340;&#23567;&#31383;&#21475;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#32447;TTT&#22312;&#22235;&#20010;&#20219;&#21153;&#19978;&#26126;&#26174;&#20248;&#20110;&#22266;&#23450;&#27169;&#22411;&#22522;&#32447;&#65292;&#22312;&#19977;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#30456;&#23545;&#25913;&#36827;&#20998;&#21035;&#20026;45%&#21644;66%&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#32447;TTT&#20063;&#20248;&#20110;&#20854;&#31163;&#32447;&#29256;&#26412;&#65292;&#21518;&#32773;&#35775;&#38382;&#26356;&#22810;&#20449;&#24687;&#65292;&#21487;&#20197;&#35757;&#32451;&#25152;&#26377;&#24103;&#32780;&#19981;&#32771;&#34385;&#26102;&#38388;&#39034;&#24207;&#12290;&#36825;&#19982;&#20808;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has established test-time training (TTT) as a general framework to further improve a trained model at test time. Before making a prediction on each test instance, the model is trained on the same instance using a self-supervised task, such as image reconstruction with masked autoencoders. We extend TTT to the streaming setting, where multiple test instances - video frames in our case - arrive in temporal order. Our extension is online TTT: The current model is initialized from the previous model, then trained on the current frame and a small window of frames immediately before. Online TTT significantly outperforms the fixed-model baseline for four tasks, on three real-world datasets. The relative improvement is 45% and 66% for instance and panoptic segmentation. Surprisingly, online TTT also outperforms its offline variant that accesses more information, training on all frames from the entire test video regardless of temporal order. This differs from previous findings using 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#20381;&#36182;&#36328;&#22495;&#22240;&#26524;&#26426;&#21046;&#30340;&#26368;&#23567;&#25913;&#21464;&#23646;&#24615;&#65292;&#22312;&#20445;&#25345;&#29305;&#23450;&#32452;&#20998;&#36328;&#22495;&#19981;&#21464;&#30340;&#21069;&#25552;&#19979;&#26368;&#23567;&#21270;&#20998;&#24067;&#36716;&#31227;&#30340;&#19981;&#24517;&#35201;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.06510</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Partial Identifiability for Domain Adaptation. (arXiv:2306.06510v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#37096;&#20998;&#21487;&#35782;&#21035;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#20381;&#36182;&#36328;&#22495;&#22240;&#26524;&#26426;&#21046;&#30340;&#26368;&#23567;&#25913;&#21464;&#23646;&#24615;&#65292;&#22312;&#20445;&#25345;&#29305;&#23450;&#32452;&#20998;&#36328;&#22495;&#19981;&#21464;&#30340;&#21069;&#25552;&#19979;&#26368;&#23567;&#21270;&#20998;&#24067;&#36716;&#31227;&#30340;&#19981;&#24517;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#23545;&#20110;&#35768;&#22810;&#27809;&#26377;&#30446;&#26631;&#22495;&#26631;&#31614;&#20449;&#24687;&#30340;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#27809;&#26377;&#36827;&#19968;&#27493;&#30340;&#20551;&#35774;&#65292;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#32852;&#21512;&#20998;&#24067;&#22312;&#30446;&#26631;&#22495;&#20013;&#26159;&#19981;&#21487;&#35782;&#21035;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20381;&#36182;&#36328;&#22495;&#22240;&#26524;&#26426;&#21046;&#30340;&#26368;&#23567;&#25913;&#21464;&#23646;&#24615;&#65292;&#20197;&#26368;&#23567;&#21270;&#20998;&#24067;&#36716;&#31227;&#30340;&#19981;&#24517;&#35201;&#24433;&#21709;&#12290;&#20026;&#20102;&#32534;&#30721;&#36825;&#20010;&#23646;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#20004;&#20010;&#20998;&#21306;&#28508;&#21464;&#37327;&#23376;&#31354;&#38388;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#26469;&#21046;&#23450;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65306;&#19981;&#21464;&#37096;&#20998;&#30340;&#20998;&#24067;&#22312;&#36328;&#22495;&#26102;&#20445;&#25345;&#19981;&#21464;&#65292;&#32780;&#31232;&#30095;&#30340;&#21487;&#21464;&#37096;&#20998;&#20250;&#22312;&#19981;&#21516;&#30340;&#22495;&#20013;&#21457;&#29983;&#21464;&#21270;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#22495;&#31227;&#20301;&#23545;&#21487;&#21464;&#37096;&#20998;&#30340;&#24433;&#21709;&#12290;&#22312;&#28201;&#21644;&#30340;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#37096;&#20998;&#21487;&#35782;&#21035;&#30340;&#28508;&#21464;&#37327;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#30446;&#26631;&#22495;&#20013;&#25968;&#25454;&#21644;&#26631;&#31614;&#30340;&#32852;&#21512;&#20998;&#24067;&#20063;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation is critical to many real-world applications where label information is unavailable in the target domain. In general, without further assumptions, the joint distribution of the features and the label is not identifiable in the target domain. To address this issue, we rely on the property of minimal changes of causal mechanisms across domains to minimize unnecessary influences of distribution shifts. To encode this property, we first formulate the data-generating process using a latent variable model with two partitioned latent subspaces: invariant components whose distributions stay the same across domains and sparse changing components that vary across domains. We further constrain the domain shift to have a restrictive influence on the changing components. Under mild conditions, we show that the latent variables are partially identifiable, from which it follows that the joint distribution of data and labels in the target domain is also identifiable. Give
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26641;&#29366;&#24352;&#37327;&#32593;&#32476;&#30340;CP&#31209;&#32422;&#26463;&#21644;&#24352;&#37327;&#20002;&#24323;&#65292;&#26469;&#26500;&#24314;&#20302;&#31209;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#26102;&#23578;-MNIST&#22270;&#20687;&#20998;&#31867;&#20013;&#23637;&#31034;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.19440</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#24352;&#37327;&#32593;&#32476;&#12289;CP&#31209;&#32422;&#26463;&#21644;&#24352;&#37327;&#20002;&#24323;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning with tree tensor networks, CP rank constraints, and tensor dropout. (arXiv:2305.19440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19440
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#26641;&#29366;&#24352;&#37327;&#32593;&#32476;&#30340;CP&#31209;&#32422;&#26463;&#21644;&#24352;&#37327;&#20002;&#24323;&#65292;&#26469;&#26500;&#24314;&#20302;&#31209;&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#26102;&#23578;-MNIST&#22270;&#20687;&#20998;&#31867;&#20013;&#23637;&#31034;&#20986;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#38477;&#20302;&#33258;&#30001;&#24230;&#26469;&#36817;&#20284;&#34920;&#31034;$N$&#38454;&#24352;&#37327;&#65292;&#24182;&#26500;&#25104;&#19968;&#31995;&#21015;&#21387;&#32553;&#30340;&#23567;&#24352;&#37327;&#32593;&#32476;&#12290;&#22312;[arXiv:2205.15296]&#25991;&#31456;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#21487;&#20197;&#36890;&#36807;&#23545;&#24352;&#37327;&#32593;&#32476;&#20013;&#30340;&#24352;&#37327;&#30340;CP&#31209;&#38468;&#21152;&#32422;&#26463;&#65292;&#36827;&#19968;&#27493;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#26088;&#22312;&#23637;&#31034;&#22914;&#20309;&#21033;&#29992;&#22522;&#20110;&#26641;&#29366;&#24352;&#37327;&#32593;&#32476;(TTN)&#30340;CP&#31209;&#32422;&#26463;&#21644;&#24352;&#37327;&#20002;&#24323;&#30340;&#26041;&#27861;&#26469;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#65292;&#24182;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#26102;&#23578;-MNIST&#22270;&#20687;&#20998;&#31867;&#20013;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#24352;&#37327;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#24403;&#20998;&#25903;&#31995;&#25968;$b=4$&#26102;&#65292;&#20302;&#31209;TTN&#20998;&#31867;&#22120;&#36798;&#21040;&#20102;&#27979;&#35797;&#38598;&#20934;&#30830;&#29575;90.3\%&#65292;&#21516;&#26102;&#25317;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#22522;&#20110;&#32447;&#24615;&#20803;&#32032;&#26500;&#25104;&#30340;&#24352;&#37327;&#32593;&#32476;&#20998;&#31867;&#22120;&#36991;&#20813;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#12290;CP&#31209;&#32422;&#26463;&#36824;&#26377;&#20854;&#20182;&#20248;&#28857;&#65306;&#21487;&#20197;&#20943;&#23569;&#21644;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor networks approximate order-$N$ tensors with a reduced number of degrees of freedom that is only polynomial in $N$ and arranged as a network of partially contracted smaller tensors. As suggested in [arXiv:2205.15296] in the context of quantum many-body physics, computation costs can be further substantially reduced by imposing constraints on the canonical polyadic (CP) rank of the tensors in such networks. Here we demonstrate how tree tensor networks (TTN) with CP rank constraints and tensor dropout can be used in machine learning. The approach is found to outperform other tensor-network based methods in Fashion-MNIST image classification. A low-rank TTN classifier with branching ratio $b=4$ reaches test set accuracy 90.3\% with low computation costs. Consisting of mostly linear elements, tensor network classifiers avoid the vanishing gradient problem of deep neural networks. The CP rank constraints have additional advantages: The number of parameters can be decreased and tuned m
&lt;/p&gt;</description></item><item><title>&#28155;&#21152;&#20869;&#37096;&#23618;&#36830;&#25509;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20462;&#25913;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#65292;&#20351;&#24471;&#24102;&#26377;&#20869;&#37096;&#23618;&#36830;&#25509;&#30340;&#27973;&#23618;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#28145;&#23618;&#32593;&#32476;&#30340;&#19968;&#20123;&#22256;&#38590;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.07037</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#37096;&#23618;&#36830;&#25509;&#37325;&#26032;&#24605;&#32771;&#28145;&#24230;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Rethink Depth Separation with Intra-layer Links. (arXiv:2305.07037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07037
&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#20869;&#37096;&#23618;&#36830;&#25509;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#24182;&#20462;&#25913;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#65292;&#20351;&#24471;&#24102;&#26377;&#20869;&#37096;&#23618;&#36830;&#25509;&#30340;&#27973;&#23618;&#32593;&#32476;&#21487;&#20197;&#34920;&#31034;&#28145;&#23618;&#32593;&#32476;&#30340;&#19968;&#20123;&#22256;&#38590;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#29616;&#22312;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20248;&#36234;&#24615;&#30340;&#19968;&#20010;&#26377;&#25928;&#35299;&#37322;&#65292;&#23427;&#30001;&#20004;&#37096;&#20998;&#32452;&#25104;&#65306;i&#65289;&#23384;&#22312;&#19968;&#31181;&#21487;&#20197;&#30001;&#28145;&#24230;&#32593;&#32476;&#34920;&#31034;&#30340;&#20989;&#25968;&#65307;ii&#65289;&#36825;&#26679;&#30340;&#20989;&#25968;&#19981;&#33021;&#30001;&#23485;&#24230;&#20302;&#20110;&#26576;&#19968;&#38408;&#20540;&#30340;&#27973;&#23618;&#32593;&#32476;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#29702;&#35770;&#26159;&#24314;&#31435;&#22312;&#21069;&#39304;&#32593;&#32476;&#19978;&#30340;&#12290;&#24456;&#23569;&#26377;&#30740;&#31350;&#22312;&#21521;&#35299;&#20915;&#29616;&#23454;&#38382;&#39064;&#30340;&#26368;&#24120;&#35265;&#30340;&#32593;&#32476;&#31867;&#22411;&#8212;&#8212;&#24555;&#25463;&#32593;&#32476;&#20013;&#32771;&#34385;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#28155;&#21152;&#20869;&#37096;&#23618;&#36830;&#25509;&#21487;&#20197;&#20462;&#25913;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#36890;&#36807;&#30028;&#38480;&#20272;&#35745;&#12289;&#26174;&#24335;&#26500;&#36896;&#21644;&#21151;&#33021;&#31354;&#38388;&#20998;&#26512;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#20869;&#37096;&#23618;&#36830;&#25509;&#26174;&#33879;&#25552;&#39640;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#19968;&#20010;&#24102;&#26377;&#20869;&#37096;&#23618;&#36830;&#25509;&#30340;&#27973;&#23618;&#32593;&#32476;&#19981;&#38656;&#35201;&#20687;&#20043;&#21069;&#19968;&#26679;&#21464;&#24471;&#23485;&#26469;&#34920;&#31034;&#30001;&#28145;&#23618;&#32593;&#32476;&#26500;&#36896;&#30340;&#19968;&#20123;&#22256;&#38590;&#20989;&#25968;&#26469;&#20462;&#25913;&#28145;&#24230;&#20998;&#31163;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The depth separation theory is nowadays widely accepted as an effective explanation for the power of depth, which consists of two parts: i) there exists a function representable by a deep network; ii) such a function cannot be represented by a shallow network whose width is lower than a threshold. However, this theory is established for feedforward networks. Few studies, if not none, considered the depth separation theory in the context of shortcuts which are the most common network types in solving real-world problems. Here, we find that adding intra-layer links can modify the depth separation theory. First, we report that adding intra-layer links can greatly improve a network's representation capability through bound estimation, explicit construction, and functional space analysis. Then, we modify the depth separation theory by showing that a shallow network with intra-layer links does not need to go as wide as before to express some hard functions constructed by a deep network. Such
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.03890</link><description>&lt;p&gt;
&#38750;&#23545;&#31216;&#32593;&#32476;&#36924;&#36817;&#29992;&#20110;&#36328;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Approximation by non-symmetric networks for cross-domain learning. (arXiv:2305.03890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#36827;&#34892;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#21487;&#20197;&#22312;&#36328;&#22495;&#23398;&#20064;&#20013;&#26174;&#33879;&#25552;&#39640;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;30&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#22312;&#20247;&#22810;&#36807;&#31243;&#65288;&#22914;&#65306;&#27973;&#23618;&#25110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#12289;&#24452;&#21521;&#22522;&#20989;&#25968;&#32593;&#32476;&#21644;&#21508;&#31181;&#20869;&#26680;&#26041;&#27861;&#65289;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#34920;&#36798;&#33021;&#21147;&#65289;&#30740;&#31350;&#20013;&#20419;&#36827;&#20102;&#22823;&#37327;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#38024;&#23545;&#19981;&#21464;&#23398;&#20064;&#12289;&#20256;&#36882;&#23398;&#20064;&#21644;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25104;&#20687;&#31561;&#24212;&#29992;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#26469;&#30740;&#31350;&#22522;&#20110;&#20869;&#26680;&#32593;&#32476;&#36924;&#36817;&#33021;&#21147;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#19968;&#32452;&#20869;&#26680;&#30340;&#26356;&#19968;&#33324;&#26041;&#27861;&#65292;&#22914;&#24191;&#20041;&#24179;&#31227;&#32593;&#32476;&#65288;&#20854;&#20013;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#21644;&#24179;&#31227;&#19981;&#21464;&#26680;&#20316;&#20026;&#29305;&#27530;&#24773;&#20917;&#65289;&#21644;&#26059;&#36716;&#21306;&#20989;&#25968;&#26680;&#12290;&#19982;&#20256;&#32479;&#30340;&#22522;&#20110;&#20869;&#26680;&#30340;&#36924;&#36817;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#33021;&#35201;&#27714;&#20869;&#26680;&#26159;&#27491;&#23450;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#38750;&#23545;&#31216;&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20869;&#26680;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#21487;&#33021;&#22312;&#20998;&#24067;&#19978;&#19981;&#21516;&#30340;&#36328;&#22495;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
For the past 30 years or so, machine learning has stimulated a great deal of research in the study of approximation capabilities (expressive power) of a multitude of processes, such as approximation by shallow or deep neural networks, radial basis function networks, and a variety of kernel based methods. Motivated by applications such as invariant learning, transfer learning, and synthetic aperture radar imaging, we initiate in this paper a general approach to study the approximation capabilities of kernel based networks using non-symmetric kernels. While singular value decomposition is a natural instinct to study such kernels, we consider a more general approach to include the use of a family of kernels, such as generalized translation networks (which include neural networks and translation invariant kernels as special cases) and rotated zonal function kernels. Naturally, unlike traditional kernel based approximation, we cannot require the kernels to be positive definite. Our results 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#27969;&#31243;&#65292;&#36890;&#36807;&#32472;&#21046;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#23376;&#32593;&#32476;&#65292;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#20256;&#36755;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11834</link><description>&lt;p&gt;
&#40065;&#26834;&#38376;&#31080;&#33021;&#22815;&#26356;&#22909;&#22320;&#20256;&#36755;: &#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#32472;&#21046;&#26356;&#20855;&#20256;&#36755;&#24615;&#30340;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Robust Tickets Can Transfer Better: Drawing More Transferable Subnetworks in Transfer Learning. (arXiv:2304.11834v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11834
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#27969;&#31243;&#65292;&#36890;&#36807;&#32472;&#21046;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#23376;&#32593;&#32476;&#65292;&#25913;&#36827;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#20256;&#36755;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36801;&#31227;&#23398;&#20064;&#21033;&#29992;&#22312;&#20016;&#23500;&#25968;&#25454;&#30340;&#28304;&#20219;&#21153;&#19978;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#36171;&#20104;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#25928;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#24448;&#24448;&#36807;&#20110;&#24222;&#22823;&#65292;&#26080;&#27861;&#25552;&#20379;&#21487;&#25512;&#24191;&#30340;&#34920;&#31034;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36801;&#31227;&#23398;&#20064;&#27969;&#31243;&#65292;&#21033;&#29992;&#25105;&#20204;&#30340;&#21457;&#29616;&#65306;&#40065;&#26834;&#38376;&#31080;&#33021;&#22815;&#26356;&#22909;&#22320;&#20256;&#36755;&#65292;&#21363;&#36890;&#36807;&#36866;&#24403;&#24341;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#24335;&#32472;&#21046;&#30340;&#23376;&#32593;&#32476;&#21487;&#20197;&#22312;&#20256;&#32479;&#30340;&#24425;&#31080;&#38376;&#31080;&#23376;&#32593;&#32476;&#19978;&#21462;&#24471;&#26356;&#22909;&#30340;&#20256;&#36755;&#33021;&#21147;&#12290;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#36801;&#31227;&#23398;&#20064;&#27969;&#31243;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#21644;&#31232;&#30095;&#27169;&#24335;&#20013;&#37117;&#33021;&#23454;&#29616;&#22686;&#24378;&#30340;&#20934;&#30830;&#24230;-&#31232;&#30095;&#24230;&#26435;&#34913;&#65292;&#36827;&#19968;&#27493;&#20016;&#23500;&#20102;&#24425;&#31080;&#38376;&#31080;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer learning leverages feature representations of deep neural networks (DNNs) pretrained on source tasks with rich data to empower effective finetuning on downstream tasks. However, the pretrained models are often prohibitively large for delivering generalizable representations, which limits their deployment on edge devices with constrained resources. To close this gap, we propose a new transfer learning pipeline, which leverages our finding that robust tickets can transfer better, i.e., subnetworks drawn with properly induced adversarial robustness can win better transferability over vanilla lottery ticket subnetworks. Extensive experiments and ablation studies validate that our proposed transfer learning pipeline can achieve enhanced accuracy-sparsity trade-offs across both diverse downstream tasks and sparsity patterns, further enriching the lottery ticket hypothesis.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;</title><link>http://arxiv.org/abs/2304.05805</link><description>&lt;p&gt;
&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65306;&#38271;&#26399;&#35760;&#24518;&#26377;&#22810;&#22823;&#30340;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
GDP nowcasting with artificial neural networks: How much does long-term memory matter?. (arXiv:2304.05805v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05805
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27604;&#36739;&#22235;&#31181;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#23545;&#32654;&#22269;GDP&#23395;&#24230;&#22686;&#38271;&#30340;&#39044;&#27979;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#20294;&#26159;&#36825;&#31181;&#25928;&#26524;&#20250;&#22312;&#19981;&#21040;&#20004;&#24180;&#30340;&#26102;&#38388;&#20869;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26102;&#26399;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#21464;&#24471;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#19981;&#21516;&#30340;&#32479;&#35745;&#27169;&#22411;&#24212;&#29992;&#20110;&#32654;&#22269;&#32463;&#27982;&#23395;&#24230;&#22269;&#20869;&#29983;&#20135;&#24635;&#20540;&#65288;GDP&#65289;&#22686;&#38271;&#39044;&#27979;&#12290;&#20351;&#29992;&#27599;&#26376;&#30340;FRED-MD&#25968;&#25454;&#24211;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21160;&#24577;&#22240;&#23376;&#27169;&#22411;&#65288;DFM&#65289;&#21644;&#22235;&#20010;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#39044;&#27979;&#34920;&#29616;&#65306;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#12289;&#19968;&#32500;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;1D CNN&#65289;&#12289;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;LSTM&#65289;&#21644;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#12290;&#23454;&#35777;&#20998;&#26512;&#21576;&#29616;&#20102;&#20004;&#20010;&#19981;&#21516;&#35780;&#20272;&#21608;&#26399;&#30340;&#32467;&#26524;&#12290;&#31532;&#19968;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2019&#24180;&#31532;4&#23395;&#24230;&#65289;&#20855;&#26377;&#24179;&#34913;&#30340;&#32463;&#27982;&#22686;&#38271;&#65292;&#32780;&#31532;&#20108;&#20010;&#21608;&#26399;&#65288;2010&#24180;&#31532;1&#23395;&#24230;&#33267;2022&#24180;&#31532;3&#23395;&#24230;&#65289;&#36824;&#21253;&#25324;COVID-19&#34928;&#36864;&#26399;&#38388;&#30340;&#26102;&#38388;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#32467;&#26524;&#65292;&#26356;&#38271;&#30340;&#36755;&#20837;&#24207;&#21015;&#22312;&#24179;&#34913;&#32463;&#27982;&#22686;&#38271;&#26399;&#38388;&#33021;&#22815;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#19968;&#20010;&#30456;&#23545;&#36739;&#20302;&#30340;&#38408;&#20540;&#20540;&#65288;&#32422;&#20845;&#20010;&#23395;&#24230;&#25110;&#21313;&#20843;&#20010;&#26376;&#65289;&#20197;&#21518;&#65292;&#36825;&#31181;&#25928;&#24212;&#20250;&#28040;&#22833;&#12290;&#22312;&#32463;&#27982;&#21160;&#33633;&#26399;&#65288;&#22914;COVID-19&#34928;&#36864;&#26399;&#38388;&#65289;&#65292;&#38271;&#26399;&#35760;&#24518;&#30340;&#25928;&#26524;&#20250;&#21464;&#24471;&#36739;&#20026;&#26126;&#26174;&#12290;
&lt;/p&gt;
&lt;p&gt;
In our study, we apply different statistical models to nowcast quarterly GDP growth for the US economy. Using the monthly FRED-MD database, we compare the nowcasting performance of the dynamic factor model (DFM) and four artificial neural networks (ANNs): the multilayer perceptron (MLP), the one-dimensional convolutional neural network (1D CNN), the long short-term memory network (LSTM), and the gated recurrent unit (GRU). The empirical analysis presents the results from two distinctively different evaluation periods. The first (2010:Q1 -- 2019:Q4) is characterized by balanced economic growth, while the second (2010:Q1 -- 2022:Q3) also includes periods of the COVID-19 recession. According to our results, longer input sequences result in more accurate nowcasts in periods of balanced economic growth. However, this effect ceases above a relatively low threshold value of around six quarters (eighteen months). During periods of economic turbulence (e.g., during the COVID-19 recession), long
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#27491;&#21017;&#21270;5&#215;2&#20132;&#21449;&#39564;&#35777;McNemar&#26816;&#39564;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#33539;&#21270;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#37325;&#21472;&#35760;&#24405;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35823;&#24046;&#29575;&#20272;&#35745;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#30041;&#32622;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#21151;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.03990</link><description>&lt;p&gt;
&#27604;&#36739;&#20004;&#31181;&#20998;&#31867;&#31639;&#27861;&#30340;&#22359;&#27491;&#21017;&#21270;5&#215;2&#20132;&#21449;&#39564;&#35777;McNemar&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Block-regularized 5$\times$2 Cross-validated McNemar's Test for Comparing Two Classification Algorithms. (arXiv:2304.03990v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03990
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#27491;&#21017;&#21270;5&#215;2&#20132;&#21449;&#39564;&#35777;McNemar&#26816;&#39564;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35268;&#33539;&#21270;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#37325;&#21472;&#35760;&#24405;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35823;&#24046;&#29575;&#20272;&#35745;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#30041;&#32622;&#26041;&#27861;&#26377;&#26356;&#39640;&#30340;&#21151;&#29575;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27604;&#36739;&#20004;&#31181;&#20998;&#31867;&#31639;&#27861;&#30340;&#20219;&#21153;&#20013;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;McNemar&#26816;&#39564;&#26088;&#22312;&#25512;&#26029;&#20986;&#20004;&#31181;&#20998;&#31867;&#31639;&#27861;&#30340;&#38169;&#35823;&#29575;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#24322;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;McNemar&#26816;&#39564;&#30340;&#21151;&#29575;&#36890;&#24120;&#19981;&#22826;&#29702;&#24819;&#65292;&#22240;&#20026;&#27979;&#35797;&#20013;&#30340;&#30041;&#32622;&#65288;HO&#65289;&#26041;&#27861;&#20165;&#20351;&#29992;&#19968;&#27425;&#35757;&#32451;&#39564;&#35777;&#25286;&#20998;&#65292;&#36825;&#36890;&#24120;&#20250;&#20135;&#29983;&#39640;&#24230;&#21464;&#21270;&#30340;&#38169;&#35823;&#29575;&#20272;&#35745;&#12290;&#30456;&#21453;&#65292;&#20132;&#21449;&#39564;&#35777;&#65288;CV&#65289;&#26041;&#27861;&#22312;&#22810;&#27425;&#37325;&#22797;HO&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#20135;&#29983;&#31283;&#23450;&#30340;&#20272;&#35745;&#65292;&#22240;&#27492;CV&#26041;&#27861;&#22312;&#25552;&#39640;McNemar&#26816;&#39564;&#21151;&#29575;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#20248;&#21183;&#12290;&#22312;&#25152;&#26377;&#31867;&#22411;&#30340;CV&#26041;&#27861;&#20013;&#65292;&#22359;&#27491;&#21017;&#21270;5&#215;2 CV&#65288;BCV&#65289;&#22312;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#27604;&#20854;&#20182;CV&#26041;&#27861;&#22312;&#31639;&#27861;&#27604;&#36739;&#20219;&#21153;&#20013;&#26356;&#20026;&#20248;&#36234;&#65292;&#22240;&#20026;5&#215;2 BCV&#21487;&#20197;&#36890;&#36807;&#20351;&#25152;&#26377;&#35757;&#32451;&#38598;&#20043;&#38388;&#30340;&#37325;&#21472;&#35760;&#24405;&#25968;&#35268;&#33539;&#21270;&#26469;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#35823;&#24046;&#29575;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the task of comparing two classification algorithms, the widely-used McNemar's test aims to infer the presence of a significant difference between the error rates of the two classification algorithms. However, the power of the conventional McNemar's test is usually unpromising because the hold-out (HO) method in the test merely uses a single train-validation split that usually produces a highly varied estimation of the error rates. In contrast, a cross-validation (CV) method repeats the HO method in multiple times and produces a stable estimation. Therefore, a CV method has a great advantage to improve the power of McNemar's test. Among all types of CV methods, a block-regularized 5$\times$2 CV (BCV) has been shown in many previous studies to be superior to the other CV methods in the comparison task of algorithms because the 5$\times$2 BCV can produce a high-quality estimator of the error rate by regularizing the numbers of overlapping records between all training sets. In this stu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#26412;&#22320;&#37051;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#31232;&#30095;&#37319;&#26679;&#25968;&#25454;&#20013;&#39640;&#25928;&#37325;&#24314;MR&#22270;&#20687;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.00775</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#36866;&#24212;&#26412;&#22320;&#37051;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20174;&#31232;&#30095;&#37319;&#26679;&#25968;&#25454;&#20013;&#37325;&#24314;MR&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Adaptive Local Neighborhood-based Neural Networks for MR Image Reconstruction from Undersampled Data. (arXiv:2206.00775v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#26412;&#22320;&#37051;&#22495;&#30340;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#65292;&#29992;&#20110;&#20174;&#31232;&#30095;&#37319;&#26679;&#25968;&#25454;&#20013;&#39640;&#25928;&#37325;&#24314;MR&#22270;&#20687;&#65292;&#35813;&#25216;&#26415;&#20855;&#26377;&#36739;&#24378;&#30340;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#21307;&#23398;&#22270;&#20687;&#37325;&#24314;&#25216;&#26415;&#33268;&#21147;&#20110;&#20197;&#23613;&#21487;&#33021;&#20302;&#30340;&#25104;&#26412;&#21644;&#23545;&#24739;&#32773;&#20135;&#29983;&#26368;&#23567;&#19981;&#33391;&#24433;&#21709;&#30340;&#26041;&#24335;&#29983;&#25104;&#36866;&#29992;&#20110;&#20020;&#24202;&#20351;&#29992;&#30340;&#39640;&#36136;&#37327;&#21307;&#23398;&#22270;&#20687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21487;&#20197;&#20174;&#31232;&#30095;&#37319;&#26679;&#30340;k&#31354;&#38388;&#25968;&#25454;&#20013;&#37325;&#24314;MR&#22270;&#20687;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#37325;&#24314;&#26102;&#36890;&#36807;&#23545;&#35757;&#32451;&#38598;&#30340;&#23567;&#21306;&#22495;&#36827;&#34892;&#36866;&#24212;&#24615;&#20272;&#35745;&#26469;&#24555;&#36895;&#20272;&#35745;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#25628;&#32034;&#19982;&#27979;&#35797;&#37325;&#24314;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#37051;&#23621;&#21644;&#35757;&#32451;&#36825;&#20123;&#37051;&#23621;&#19978;&#30340;&#23616;&#37096;&#32593;&#32476;&#65292;&#28982;&#21518;&#26356;&#26032;&#27979;&#35797;&#37325;&#24314;&#20043;&#38388;&#36827;&#34892;&#20132;&#26367;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#37325;&#24314;&#27169;&#22411;&#26159;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#19982;&#27491;&#22312;&#37325;&#24314;&#30340;&#22270;&#20687;&#30456;&#20284;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#32780;&#19981;&#26159;&#22312;&#22823;&#35268;&#27169;&#22810;&#26679;&#30340;&#35757;&#32451;&#38598;&#19978;&#25311;&#21512;&#30340;&#65292;&#22240;&#27492;&#23427;&#23545;&#26032;&#30340;&#25195;&#25551;&#26356;&#20855;&#36866;&#24212;&#24615;&#12290;&#23427;&#36824;&#21487;&#20197;&#22788;&#29702;&#35757;&#32451;&#38598;&#20013;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent medical image reconstruction techniques focus on generating high-quality medical images suitable for clinical use at the lowest possible cost and with the fewest possible adverse effects on patients. Recent works have shown significant promise for reconstructing MR images from sparsely sampled k-space data using deep learning. In this work, we propose a technique that rapidly estimates deep neural networks directly at reconstruction time by fitting them on small adaptively estimated neighborhoods of a training set. In brief, our algorithm alternates between searching for neighbors in a data set that are similar to the test reconstruction, and training a local network on these neighbors followed by updating the test reconstruction. Because our reconstruction model is learned on a dataset that is in some sense similar to the image being reconstructed rather than being fit on a large, diverse training set, it is more adaptive to new scans. It can also handle changes in training set
&lt;/p&gt;</description></item><item><title>Auto-NBA&#26159;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#25628;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#19977;&#20010;&#32806;&#21512;&#30340;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2106.06575</link><description>&lt;p&gt;
Auto-NBA: &#38024;&#23545;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#19977;&#20010;&#32852;&#21512;&#31354;&#38388;&#36827;&#34892;&#39640;&#25928;&#25628;&#32034;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Auto-NBA: Efficient and Effective Search Over the Joint Space of Networks, Bitwidths, and Accelerators. (arXiv:2106.06575v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.06575
&lt;/p&gt;
&lt;p&gt;
Auto-NBA&#26159;&#19968;&#31181;&#33021;&#22815;&#39640;&#25928;&#25628;&#32034;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#21516;&#26102;&#32771;&#34385;&#20248;&#21270;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#19977;&#20010;&#32806;&#21512;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#21516;&#26102;&#20248;&#21270;&#21644;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#38656;&#35201;&#32852;&#21512;&#32771;&#34385;&#19977;&#20010;&#19981;&#21516;&#21364;&#39640;&#24230;&#32806;&#21512;&#30340;&#26041;&#38754;&#65306;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#12290;&#28982;&#32780;&#65292;&#32852;&#21512;&#25628;&#32034;&#38754;&#20020;&#30340;&#25361;&#25112;&#23578;&#26410;&#34987;&#23436;&#20840;&#29702;&#35299;&#21644;&#35299;&#20915;&#12290;&#36825;&#20123;&#20851;&#38190;&#25361;&#25112;&#21253;&#25324;&#65288;1&#65289;&#26159;&#21542;&#25193;&#22823;&#30001;&#20110;&#24040;&#22823;&#32852;&#21512;&#31354;&#38388;&#32780;&#23548;&#33268;&#30340;&#20869;&#23384;&#28040;&#32791;&#65292;&#36824;&#26159;&#37319;&#29992;&#27425;&#20248;&#35774;&#35745;&#65292;&#65288;2&#65289;&#21152;&#36895;&#22120;&#35774;&#35745;&#31354;&#38388;&#30340;&#31163;&#25955;&#24615;&#19982;&#32593;&#32476;&#21644;&#27604;&#29305;&#23485;&#24230;&#35774;&#35745;&#31354;&#38388;&#30456;&#32806;&#21512;&#19988;&#19981;&#21516;&#65292;&#20197;&#21450;&#65288;3&#65289;&#32593;&#32476;-&#21152;&#36895;&#22120;&#32852;&#21512;&#25628;&#32034;&#20013;&#30340;&#8220;&#40481;&#29983;&#34507;&#34507;&#29983;&#40481;&#8221;&#38382;&#39064;&#65306;&#21363;&#32852;&#21512;&#25628;&#32034;&#38656;&#35201;&#35745;&#31639;&#25805;&#20316;&#30340;&#30828;&#20214;&#25104;&#26412;&#65292;&#28982;&#32780;&#22312;&#25628;&#32034;&#26399;&#38388;&#65292;&#23578;&#19981;&#30693;&#36947;&#25972;&#20010;&#32593;&#32476;&#30340;&#26368;&#20339;&#21152;&#36895;&#22120;&#65292;&#22240;&#27492;&#26080;&#27861;&#35745;&#31639;&#36825;&#20123;&#25104;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Auto-NBA&#30340;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#38024;&#23545;&#32593;&#32476;&#12289;&#27604;&#29305;&#23485;&#24230;&#21644;&#21152;&#36895;&#22120;&#19977;&#20010;&#32852;&#21512;&#31354;&#38388;&#30340;&#39640;&#25928;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
While maximizing deep neural networks' (DNNs') acceleration efficiency requires a joint search/design of three different yet highly coupled aspects, including the networks, bitwidths, and accelerators, the challenges associated with such a joint search have not yet been fully understood and addressed. The key challenges include (1) the dilemma of whether to explode the memory consumption due to the huge joint space or achieve sub-optimal designs, (2) the discrete nature of the accelerator design space that is coupled yet different from that of the networks and bitwidths, and (3) the chicken and egg problem associated with network-accelerator co-search, i.e., co-search requires operation-wise hardware cost, which is lacking during search as the optimal accelerator depending on the whole network is still unknown during search. To tackle these daunting challenges towards optimal and fast development of DNN accelerators, we propose a framework dubbed Auto-NBA to enable jointly searching fo
&lt;/p&gt;</description></item></channel></rss>