<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16970</link><description>&lt;p&gt;
&#32852;&#21512;&#33016;&#37096;X&#20809;&#35786;&#26029;&#21644;&#20020;&#24202;&#35270;&#35273;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20020;&#24202;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#39046;&#22495;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#22312;&#25918;&#23556;&#23398;&#31579;&#26597;&#36807;&#31243;&#20013;&#20020;&#24202;&#21307;&#29983;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#22270;&#20026;&#25552;&#20379;&#37325;&#35201;&#27934;&#23519;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#36164;&#20135;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#36825;&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#65292;&#21033;&#29992;&#20102;DenseNet201&#20027;&#24178;&#21644;&#22522;&#20110;&#27531;&#24046;&#21644;&#33192;&#32960;&#28608;&#21169;&#22359;&#30340;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#29992;&#20110;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#36827;&#34892;&#30142;&#30149;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;</title><link>https://arxiv.org/abs/2403.09918</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Attention-based Class-Conditioned Alignment for Multi-Source Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09918
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65292;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#65288;OD&#65289;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#33268;&#21147;&#20110;&#36890;&#36807;&#20419;&#36827;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#29305;&#24449;&#23545;&#40784;&#26469;&#32531;&#35299;&#20998;&#24067;&#36716;&#31227;&#30340;&#24433;&#21709;&#12290;&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#20801;&#35768;&#21033;&#29992;&#22810;&#20010;&#24102;&#27880;&#37322;&#30340;&#28304;&#25968;&#25454;&#38598;&#21644;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#25968;&#25454;&#26469;&#25552;&#39640;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;OD MSDA&#26041;&#27861;&#20197;&#19968;&#31181;&#19982;&#31867;&#21035;&#26080;&#20851;&#30340;&#26041;&#24335;&#25191;&#34892;&#29305;&#24449;&#23545;&#40784;&#12290;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#21407;&#22411;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#25353;&#31867;&#21035;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20294;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#23548;&#33268;&#38169;&#35823;&#31215;&#32047;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#19981;&#24179;&#34913;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31867;&#21035;&#26465;&#20214;&#23545;&#40784;&#26041;&#26696;&#65292;&#29992;&#20110;MSDA&#65292;&#35813;&#26041;&#26696;&#22312;&#36328;&#39046;&#22495;&#23545;&#40784;&#27599;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09918v1 Announce Type: cross  Abstract: Domain adaptation methods for object detection (OD) strive to mitigate the impact of distribution shifts by promoting feature alignment across source and target domains. Multi-source domain adaptation (MSDA) allows leveraging multiple annotated source datasets, and unlabeled target data to improve the accuracy and robustness of the detection model. Most state-of-the-art MSDA methods for OD perform feature alignment in a class-agnostic manner. This is challenging since the objects have unique modal information due to variations in object appearance across domains. A recent prototype-based approach proposed a class-wise alignment, yet it suffers from error accumulation due to noisy pseudo-labels which can negatively affect adaptation with imbalanced data. To overcome these limitations, we propose an attention-based class-conditioned alignment scheme for MSDA that aligns instances of each object category across domains. In particular, an 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2403.03551</link><description>&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#20026;&#39640;&#26031;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#65292;&#29992;&#20110;&#22270;&#20687;&#22686;&#24378;&#30340;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#65292;&#30001;&#20110;&#20854;&#22522;&#20110;&#30005;&#31163;&#36752;&#23556;&#65292;&#22240;&#27492;&#24076;&#26395;&#23613;&#37327;&#20943;&#23569;&#36752;&#23556;&#21058;&#37327;&#12290;&#28982;&#32780;&#65292;&#38477;&#20302;&#36752;&#23556;&#21058;&#37327;&#20250;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#65292;&#20174;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#25968;&#25454;&#37325;&#24314;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20540;&#24471;&#36827;&#34892;&#30740;&#31350;&#12290;&#26681;&#25454;LoDoPaB-CT&#22522;&#20934;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#28041;&#21450;UNet&#22411;&#26550;&#26500;&#30340;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25490;&#21517;&#31532;&#19968;&#30340;&#26041;&#27861;ItNet&#20351;&#29992;&#21253;&#25324;&#28388;&#27874;&#21453;&#25237;&#24433;&#65288;FBP&#65289;&#12289;&#22312;CT&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;UNet&#21644;&#36845;&#20195;&#32454;&#21270;&#27493;&#39588;&#30340;&#19977;&#38454;&#27573;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#31532;&#19968;&#38454;&#27573;&#20063;&#20351;&#29992;&#20102;FBP&#65292;&#32780;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29305;&#28857;&#26159;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#26159;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03551v1 Announce Type: cross  Abstract: Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose. However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures. Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step. In this paper, we propose a less complex two-stage method. The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage. The crucial point of our approach is that the neural network is pretrained
&lt;/p&gt;</description></item><item><title>CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.17363</link><description>&lt;p&gt;
CGGM&#65306;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#30340;&#26465;&#20214;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17363
&lt;/p&gt;
&lt;p&gt;
CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#26816;&#27979;&#29289;&#32852;&#32593;&#20013;&#33410;&#28857;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30340;&#32422;&#26463;&#21253;&#25324;&#37051;&#25509;&#20851;&#31995;&#30340;&#21333;&#35843;&#24615;&#65292;&#20026;&#33410;&#28857;&#26500;&#24314;&#22810;&#32500;&#29305;&#24449;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#32570;&#20047;&#31471;&#21040;&#31471;&#29983;&#25104;&#22810;&#31867;&#33410;&#28857;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CGGM&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#23569;&#25968;&#31867;&#21035;&#20013;&#26356;&#22810;&#33410;&#28857;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#30340;&#26426;&#21046;&#22686;&#24378;&#20102;&#20854;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#29305;&#24449;&#29983;&#25104;&#27169;&#22359;&#21517;&#20026;&#22810;&#32500;&#29305;&#24449;&#29983;&#25104;&#22120;&#65288;MFG&#65289;&#65292;&#21487;&#29983;&#25104;&#21253;&#25324;&#25299;&#25169;&#20449;&#24687;&#22312;&#20869;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#26631;&#31614;&#34987;&#36716;&#25442;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#29992;&#20316;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17363v1 Announce Type: cross  Abstract: Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as condition
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.16048</link><description>&lt;p&gt;
LLMs&#24102;&#26377;&#24605;&#32500;&#38142;&#26465;&#26159;&#38750;&#22240;&#26524;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
LLMs with Chain-of-Thought Are Non-Causal Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#65292;&#21457;&#29616;LLMs&#22312;&#31572;&#26696;&#29983;&#25104;&#36807;&#31243;&#20013;&#19982;&#20154;&#31867;&#25512;&#29702;&#23384;&#22312;&#24046;&#24322;&#65292;&#30456;&#20851;&#22240;&#32032;&#21253;&#25324;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25512;&#29702;&#20013;&#24605;&#32500;&#38142;&#26465;&#65288;CoT&#65289;&#30340;&#20316;&#29992;&#12290;&#23613;&#31649;&#23427;&#26377;&#25913;&#21892;&#20219;&#21153;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#22312;LLMs&#20013;&#27491;&#30830;&#31572;&#26696;&#36319;&#38543;&#19981;&#27491;&#30830;CoTs&#30340;&#39057;&#29575;&#21450;&#21453;&#20043;&#12290;&#25105;&#20204;&#37319;&#29992;&#22240;&#26524;&#20998;&#26512;&#26469;&#35780;&#20272;CoTs/&#25351;&#20196;&#19982;LLMs&#31572;&#26696;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#25581;&#31034;LLMs&#36817;&#20284;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#12290;&#36890;&#36807;&#27604;&#36739;&#26263;&#31034;SCM&#19982;&#20154;&#31867;&#25512;&#29702;&#30340;SCM&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;LLM&#21644;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#24433;&#21709;&#26263;&#31034;SCM&#22240;&#26524;&#32467;&#26500;&#30340;&#22240;&#32032;&#65292;&#25581;&#31034;&#20102;&#35821;&#22659;&#23398;&#20064;&#12289;&#26377;&#30417;&#30563;&#24494;&#35843;&#20197;&#21450;&#23545;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26174;&#33879;&#24433;&#21709;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#22312;https://github.com/StevenZHB/CoT_Causal_Analysis&#21457;&#24067;&#20102;&#20195;&#30721;&#21644;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16048v1 Announce Type: cross  Abstract: This paper explores the role of the Chain of Thought (CoT) in Large Language Models (LLMs) reasoning. Despite its potential to improve task performance, our analysis reveals a surprising frequency of correct answers following incorrect CoTs and vice versa. We employ causal analysis to assess the cause-effect relationship between CoTs/instructions and answers in LLMs, uncovering the Structural Causal Model (SCM) that LLMs approximate. By comparing the implied SCM with that of human reasoning, we highlight discrepancies between LLM and human reasoning processes. We further examine the factors influencing the causal structure of the implied SCM, revealing that in-context learning, supervised fine-tuning, and reinforcement learning on human feedback significantly impact the causal relations. We release the code and results at https://github.com/StevenZHB/CoT_Causal_Analysis.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.13516</link><description>&lt;p&gt;
ProSparse: &#24341;&#20837;&#21644;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20869;&#37096;&#28608;&#27963;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
ProSparse: Introducing and Enhancing Intrinsic Activation Sparsity within Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Activation sparsity&#25351;&#30340;&#26159;&#28608;&#27963;&#36755;&#20986;&#20013;&#23384;&#22312;&#35768;&#22810;&#24369;&#36129;&#29486;&#20803;&#32032;&#12290;&#20316;&#20026;&#20351;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#27169;&#22411;&#30340;&#26222;&#36941;&#23646;&#24615;&#65292;&#24050;&#34987;&#35777;&#26126;&#26159;&#25552;&#39640;&#27169;&#22411;&#25512;&#29702;&#25928;&#29575;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37319;&#29992;&#20102;&#27809;&#26377;&#20869;&#22312;&#28608;&#27963;&#31232;&#30095;&#24615;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#20363;&#22914;GELU&#21644;Swish&#65289;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#21162;&#21147;&#23581;&#35797;&#24341;&#20837;ReLU&#25110;&#20854;&#21464;&#20307;&#20316;&#20026;&#26367;&#20195;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;LLMs&#23454;&#29616;&#28608;&#27963;&#31232;&#30095;&#24615;&#21644;&#25512;&#29702;&#21152;&#36895;&#65292;&#20294;&#24456;&#23569;&#33021;&#21516;&#26102;&#33719;&#24471;&#39640;&#31232;&#30095;&#24230;&#21644;&#21487;&#27604;&#36739;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;"ProSparse"&#30340;&#26377;&#25928;&#31232;&#30095;&#21270;&#26041;&#27861;&#65292;&#20197;&#25512;&#21160;LLMs&#23454;&#29616;&#26356;&#39640;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#32780;&#19981;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23558;LLMs&#30340;&#28608;&#27963;&#20989;&#25968;&#26367;&#25442;&#20026;ReLU&#21518;&#65292;ProSparse&#37319;&#29992;&#28176;&#36827;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13516v1 Announce Type: cross  Abstract: Activation sparsity refers to the existence of considerable weakly-contributed elements among activation outputs. As a prevalent property of the models using the ReLU activation function, it has been proven a promising paradigm to boost model inference efficiency. Nevertheless, most large language models (LLMs) adopt activation functions without intrinsic activation sparsity (e.g., GELU and Swish). Some recent efforts have explored introducing ReLU or its variants as the substitutive activation function to help LLMs achieve activation sparsity and inference acceleration, but few can simultaneously obtain high sparsity and comparable model performance. This paper introduces an effective sparsification method named "ProSparse" to push LLMs for higher activation sparsity without decreasing model performance. Specifically, after substituting the activation function of LLMs with ReLU, ProSparse adopts progressive sparsity regularization wit
&lt;/p&gt;</description></item><item><title>TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.</title><link>https://arxiv.org/abs/2402.12683</link><description>&lt;p&gt;
TorchCP&#65306;&#22522;&#20110;PyTorch&#30340;&#19968;&#31181;&#36866;&#29992;&#20110;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TorchCP: A Library for Conformal Prediction based on PyTorch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12683
&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#30340;Python&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#20102;&#29992;&#20110;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#21508;&#31181;&#23454;&#29616;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65288;&#21253;&#25324;&#22810;&#32500;&#36755;&#20986;&#65289;&#12290;TorchCP&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#24182;&#21033;&#29992;&#30697;&#38453;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#31616;&#27905;&#39640;&#25928;&#30340;&#25512;&#29702;&#23454;&#29616;&#12290;&#35813;&#20195;&#30721;&#37319;&#29992;LGPL&#35768;&#21487;&#35777;&#65292;&#24182;&#22312;$\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12683v1 Announce Type: new  Abstract: TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.11940</link><description>&lt;p&gt;
AICAttack&#65306;&#22522;&#20110;&#27880;&#24847;&#21147;&#20248;&#21270;&#30340;&#23545;&#25239;&#24615;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11940
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;AICAttack&#65292;&#26088;&#22312;&#36890;&#36807;&#24494;&#23567;&#30340;&#22270;&#20687;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#65292;&#22312;&#40657;&#30418;&#25915;&#20987;&#24773;&#26223;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#21462;&#24471;&#20102;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31561;&#35768;&#22810;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;CV&#21644;NLP&#20132;&#21449;&#28857;&#19978;&#30340;&#22270;&#20687;&#23383;&#24149;&#38382;&#39064;&#20013;&#65292;&#30456;&#20851;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#25915;&#20987;&#31574;&#30053;&#65292;&#31216;&#20026;AICAttack&#65288;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22270;&#20687;&#23383;&#24149;&#25915;&#20987;&#65289;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#25200;&#21160;&#26469;&#25915;&#20987;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#12290;&#22312;&#40657;&#30418;&#25915;&#20987;&#29615;&#22659;&#20013;&#36816;&#34892;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#35775;&#38382;&#30446;&#26631;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#21442;&#25968;&#25110;&#26799;&#24230;&#20449;&#24687;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20505;&#36873;&#36873;&#25321;&#26426;&#21046;&#65292;&#21487;&#35782;&#21035;&#26368;&#20339;&#20687;&#32032;&#36827;&#34892;&#25915;&#20987;&#65292;&#28982;&#21518;&#37319;&#29992;&#24046;&#20998;&#36827;&#21270;&#65288;DE&#65289;&#26469;&#25200;&#20081;&#20687;&#32032;&#30340;RGB&#20540;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;AICAttack&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11940v1 Announce Type: cross  Abstract: Recent advances in deep learning research have shown remarkable achievements across many tasks in computer vision (CV) and natural language processing (NLP). At the intersection of CV and NLP is the problem of image captioning, where the related models' robustness against adversarial attacks has not been well studied. In this paper, we present a novel adversarial attack strategy, which we call AICAttack (Attention-based Image Captioning Attack), designed to attack image captioning models through subtle perturbations on images. Operating within a black-box attack scenario, our algorithm requires no access to the target model's architecture, parameters, or gradient information. We introduce an attention-based candidate selection mechanism that identifies the optimal pixels to attack, followed by Differential Evolution (DE) for perturbing pixels' RGB values. We demonstrate AICAttack's effectiveness through extensive experiments on benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;</title><link>https://arxiv.org/abs/2402.06126</link><description>&lt;p&gt;
&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;&#65306;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learn To be Efficient: Build Structured Sparsity in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)"&#65292;&#25552;&#20986;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20013;&#26500;&#24314;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#21462;&#24471;&#26356;&#22909;&#30340;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20197;&#20854;&#21313;&#20159;&#32423;&#21442;&#25968;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#20135;&#29983;&#20102;&#39640;&#26114;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;&#22312;LLM&#20013;&#20986;&#29616;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#20026;&#36890;&#36807;&#20165;&#28041;&#21450;&#37096;&#20998;&#21442;&#25968;&#36827;&#34892;&#25512;&#29702;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#20943;&#23569;&#36825;&#31181;&#25104;&#26412;&#12290;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21033;&#29992;&#36825;&#31181;&#33258;&#28982;&#24418;&#25104;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#65292;&#24573;&#35270;&#20102;&#36827;&#19968;&#27493;&#25918;&#22823;&#36825;&#31181;&#22266;&#26377;&#31232;&#30095;&#24615;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;LLM&#21487;&#20197;&#36890;&#36807;&#23454;&#29616;&#26356;&#32467;&#26500;&#21270;&#30340;&#28608;&#27963;&#31232;&#30095;&#24615;&#26469;&#23398;&#20064;&#39640;&#25928;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;"Learn-To-be-Efficient(LTE)", &#26088;&#22312;&#35757;&#32451;&#39640;&#25928;&#24847;&#35782;&#30340;LLM&#23398;&#20064;&#28608;&#27963;&#26356;&#23569;&#30340;&#31070;&#32463;&#20803;&#65292;&#24182;&#22312;&#31232;&#30095;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#26356;&#22909;&#30340;&#25240;&#34935;&#12290;&#27492;&#22806;&#65292;&#19982;&#20027;&#35201;&#20851;&#27880;&#22522;&#20110;ReLU&#27169;&#22411;&#30340;SOTA MoEfication&#26041;&#27861;&#19981;&#21516;&#65292;LTE&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#20687;GPT&#21644;LLaMA&#36825;&#26679;&#20855;&#26377;&#36719;&#28608;&#27963;&#20989;&#25968;&#30340;LLM&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#27169;&#22411;&#21644;&#21313;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LTE&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have achieved remarkable success with their billion-level parameters, yet they incur high inference overheads. The emergence of activation sparsity in LLMs provides a natural approach to reduce this cost by involving only parts of the parameters for inference. Existing methods only focus on utilizing this naturally formed activation sparsity, overlooking the potential for further amplifying this inherent sparsity. In this paper, we hypothesize that LLMs can learn to be efficient by achieving more structured activation sparsity.To achieve this, we introduce a novel algorithm, Learn-To-be-Efficient (LTE), designed to train efficiency-aware LLMs to learn to activate fewer neurons and achieve a better trade-off between sparsity and performance. Furthermore, unlike SOTA MoEfication methods, which mainly focus on ReLU-based models, LTE can also be applied to LLMs like GPT and LLaMA with soft activation functions. We evaluate LTE on four models and eleven datasets
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#32858;&#21512;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05541</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#40065;&#26834;&#21644;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#30340;&#20652;&#21270;&#21058;&#65306;&#35299;&#23494;&#23458;&#25143;&#36129;&#29486;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning as a Catalyst for Robust and Fair Federated Learning: Deciphering the Dynamics of Client Contributions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#8212;&#8212;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#32858;&#21512;&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20135;&#29983;&#20102;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#22810;&#20010;&#20998;&#25955;&#30340;&#35774;&#22791;&#25110;&#31995;&#32479;&#19978;&#35757;&#32451;&#26469;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#24182;&#20445;&#30041;&#26412;&#22320;&#25968;&#25454;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31574;&#30053;&#32463;&#24120;&#24573;&#35270;&#32479;&#35745;&#24322;&#36136;&#24615;&#21644;&#23545;&#25932;&#23545;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#25152;&#24102;&#26469;&#30340;&#22256;&#38590;&#65292;&#36825;&#20123;&#22240;&#32032;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;&#20010;&#24615;&#21270;&#30340;FL&#31574;&#30053;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#26469;&#36866;&#24212;&#20010;&#21035;&#23458;&#25143;&#30340;&#29305;&#28857;&#65292;&#20294;&#24448;&#24448;&#24573;&#35270;&#20102;&#26381;&#21153;&#22120;&#31471;&#32858;&#21512;&#30340;&#33030;&#24369;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24378;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;RFL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#36866;&#24212;&#20248;&#21270;&#32858;&#21512;&#36807;&#31243;&#20013;&#23458;&#25143;&#36129;&#29486;&#30340;&#26032;&#26694;&#26550;&#65292;&#20174;&#32780;&#22686;&#24378;&#24694;&#24847;&#23458;&#25143;&#19979;&#30340;&#27169;&#22411;&#40065;&#26834;&#24615;&#21644;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#20844;&#24179;&#24615;&#22312;&#38750;&#30456;&#21516;&#20998;&#24067;&#29615;&#22659;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#22522;&#20110;&#28145;&#24230;&#30830;&#23450;&#24615;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#21327;&#21516;&#35757;&#32451;&#65292;&#20197;&#20248;&#21270;&#23458;&#25143;&#36129;&#29486;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in federated learning (FL) have produced models that retain user privacy by training across multiple decentralized devices or systems holding local data samples. However, these strategies often neglect the inherent challenges of statistical heterogeneity and vulnerability to adversarial attacks, which can degrade model robustness and fairness. Personalized FL strategies offer some respite by adjusting models to fit individual client profiles, yet they tend to neglect server-side aggregation vulnerabilities. To address these issues, we propose Reinforcement Federated Learning (RFL), a novel framework that leverages deep reinforcement learning to adaptively optimize client contribution during aggregation, thereby enhancing both model robustness against malicious clients and fairness across participants under non-identically distributed settings. To achieve this goal, we propose a meticulous approach involving a Deep Deterministic Policy Gradient-based algorithm for co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.02229</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#34920;&#29616;&#20986;&#33394;
&lt;/p&gt;
&lt;p&gt;
Vanilla Bayesian Optimization Performs Great in High Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#24773;&#20917;&#19979;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#20808;&#39564;&#20551;&#35774;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#65292;&#20351;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#39640;&#32500;&#38382;&#39064;&#19968;&#30452;&#34987;&#35748;&#20026;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#30340;&#36719;&#32907;&#12290;&#21463;&#21040;&#32500;&#24230;&#22122;&#38899;&#30340;&#21050;&#28608;&#65292;&#35768;&#22810;&#31639;&#27861;&#26088;&#22312;&#36890;&#36807;&#23545;&#30446;&#26631;&#24212;&#29992;&#21508;&#31181;&#31616;&#21270;&#20551;&#35774;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#35782;&#21035;&#23548;&#33268;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#20219;&#21153;&#20013;&#19981;&#36866;&#29992;&#30340;&#36864;&#21270;&#29616;&#35937;&#65292;&#24182;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#22914;&#20309;&#36890;&#36807;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#26469;&#24212;&#23545;&#36825;&#20123;&#36864;&#21270;&#29616;&#35937;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#26222;&#36890;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#20013;&#20856;&#22411;&#20808;&#39564;&#20551;&#35774;&#30340;&#25913;&#36827;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#23545;&#30446;&#26631;&#26045;&#21152;&#32467;&#26500;&#24615;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#23558;&#22797;&#26434;&#24615;&#38477;&#20302;&#21040;&#21487;&#31649;&#29702;&#30340;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#20462;&#25913;&#26041;&#27861;&#8212;&#8212;&#36890;&#36807;&#32500;&#24230;&#23545;&#39640;&#26031;&#36807;&#31243;&#38271;&#24230;&#20808;&#39564;&#36827;&#34892;&#31616;&#21333;&#30340;&#32553;&#25918;&#8212;&#8212;&#25581;&#31034;&#20102;&#26631;&#20934;&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#26126;&#30830;&#34920;&#26126;&#20854;&#25928;&#26524;&#36828;&#36828;&#36229;&#20986;&#20197;&#24448;&#30340;&#39044;&#26399;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-dimensional problems have long been considered the Achilles' heel of Bayesian optimization algorithms. Spurred by the curse of dimensionality, a large collection of algorithms aim to make it more performant in this setting, commonly by imposing various simplifying assumptions on the objective. In this paper, we identify the degeneracies that make vanilla Bayesian optimization poorly suited to high-dimensional tasks, and further show how existing algorithms address these degeneracies through the lens of lowering the model complexity. Moreover, we propose an enhancement to the prior assumptions that are typical to vanilla Bayesian optimization algorithms, which reduces the complexity to manageable levels without imposing structural restrictions on the objective. Our modification - a simple scaling of the Gaussian process lengthscale prior with the dimensionality - reveals that standard Bayesian optimization works drastically better than previously thought in high dimensions, clearly
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#24102;&#26377;Bandit&#21453;&#39304;&#30340;&#26497;&#23567;&#26497;&#22823;&#27425;&#27169;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#26368;&#23567;&#26368;&#22823;&#19979;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#19982;&#19979;&#38480;&#36951;&#25022;&#30456;&#21305;&#37197;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.18465</link><description>&lt;p&gt;
&#24102;&#26377;Bandit&#21453;&#39304;&#30340;&#26497;&#23567;&#26497;&#22823;&#27425;&#27169;&#20248;&#21270;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Minimax Optimal Submodular Optimization with Bandit Feedback. (arXiv:2310.18465v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18465
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#24102;&#26377;Bandit&#21453;&#39304;&#30340;&#26497;&#23567;&#26497;&#22823;&#27425;&#27169;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#31532;&#19968;&#20010;&#26368;&#23567;&#26368;&#22823;&#19979;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#19982;&#19979;&#38480;&#36951;&#25022;&#30456;&#21305;&#37197;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#38543;&#26426;Bandit&#21453;&#39304;&#19979;&#65292;&#26368;&#22823;&#21270;&#19968;&#20010;&#21333;&#35843;&#27425;&#27169;&#38598;&#20989;&#25968;$f&#65306;2 ^ {[n]} \rightarrow [0,1]$&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;$f$&#23545;&#20110;&#23398;&#20064;&#32773;&#26159;&#26410;&#30693;&#30340;&#65292;&#20294;&#26159;&#22312;&#27599;&#20010;&#26102;&#38388;$t=1,\dots,T$&#65292;&#23398;&#20064;&#32773;&#36873;&#25321;&#19968;&#20010;&#38598;&#21512;$S_t \subset [n]$&#65292;&#20854;&#20013;$|S_t|\leq k$&#65292;&#24182;&#25509;&#25910;&#22870;&#21169;$f(S_t)+\eta_t$&#65292;&#20854;&#20013;$\eta_t$&#26159;&#22343;&#20540;&#20026;&#38646;&#30340;&#27425;&#39640;&#26031;&#22122;&#22768;&#12290;&#30446;&#26631;&#26159;&#22312;$T$&#27425;&#20013;&#20351;&#24471;&#23398;&#20064;&#32773;&#23545;&#20110;&#24102;&#26377;$|S_*|=k$&#30340;&#26368;&#22823;$f(S_*)$&#30340;($1-e^{-1}$)&#36817;&#20284;&#30340;&#26368;&#23567;&#36951;&#25022;&#65292;&#36890;&#36807;&#23545;$f$&#30340;&#36138;&#23146;&#26368;&#22823;&#21270;&#26469;&#36798;&#21040;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#25991;&#29486;&#20013;&#26368;&#22909;&#30340;&#36951;&#25022;&#36793;&#30028;&#25353;&#29031;$k n^{1/3} T^{2/3}$&#30340;&#27604;&#20363;&#32553;&#25918;&#12290;&#36890;&#36807;&#23558;&#27599;&#20010;&#38598;&#21512;&#31616;&#21333;&#22320;&#35270;&#20026;&#19968;&#20010;&#21807;&#19968;&#30340;arm&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;$\sqrt{{n \choose k} T}$&#20063;&#26159;&#21487;&#23454;&#29616;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#31532;&#19968;&#20010;&#26497;&#23567;&#26497;&#22823;&#19979;&#38480;&#65292;&#20854;&#25353;&#29031;$\mathcal{O}(\min_{i \le k}(in^{1/3}T^{2/3} + \sqrt{n^{k-i}T}))$&#30340;&#27604;&#20363;&#32553;&#25918;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#19982;&#19979;&#38480;&#36951;&#25022;&#30456;&#21305;&#37197;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider maximizing a monotonic, submodular set function $f: 2^{[n]} \rightarrow [0,1]$ under stochastic bandit feedback. Specifically, $f$ is unknown to the learner but at each time $t=1,\dots,T$ the learner chooses a set $S_t \subset [n]$ with $|S_t| \leq k$ and receives reward $f(S_t) + \eta_t$ where $\eta_t$ is mean-zero sub-Gaussian noise. The objective is to minimize the learner's regret over $T$ times with respect to ($1-e^{-1}$)-approximation of maximum $f(S_*)$ with $|S_*| = k$, obtained through greedy maximization of $f$. To date, the best regret bound in the literature scales as $k n^{1/3} T^{2/3}$. And by trivially treating every set as a unique arm one deduces that $\sqrt{ {n \choose k} T }$ is also achievable. In this work, we establish the first minimax lower bound for this setting that scales like $\mathcal{O}(\min_{i \le k}(in^{1/3}T^{2/3} + \sqrt{n^{k-i}T}))$. Moreover, we propose an algorithm that is capable of matching the lower bound regret.
&lt;/p&gt;</description></item><item><title>OneAdapt&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#65292;&#28385;&#36275;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#22312;&#37197;&#32622;&#21442;&#25968;&#26041;&#38754;&#30340;&#19977;&#20010;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2310.02422</link><description>&lt;p&gt;
OneAdapt&#65306;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#23454;&#29616;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#30340;&#24555;&#36895;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
OneAdapt: Fast Adaptation for Deep Learning Applications via Backpropagation. (arXiv:2310.02422v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02422
&lt;/p&gt;
&lt;p&gt;
OneAdapt&#36890;&#36807;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#65292;&#28385;&#36275;&#20102;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#22312;&#37197;&#32622;&#21442;&#25968;&#26041;&#38754;&#30340;&#19977;&#20010;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#27969;&#23186;&#20307;&#25968;&#25454;&#30340;&#25512;&#26029;&#26041;&#38754;&#24050;&#32463;&#26222;&#21450;&#65292;&#22914;&#35270;&#39057;&#20013;&#30340;&#30446;&#26631;&#26816;&#27979;&#12289;LiDAR&#25968;&#25454;&#21644;&#38899;&#39057;&#27874;&#24418;&#20013;&#30340;&#25991;&#26412;&#25552;&#21462;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25512;&#26029;&#20934;&#30830;&#24615;&#65292;&#36825;&#20123;&#24212;&#29992;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#32593;&#32476;&#24102;&#23485;&#26469;&#25910;&#38598;&#39640;&#20445;&#30495;&#25968;&#25454;&#65292;&#24182;&#19988;&#38656;&#35201;&#24191;&#27867;&#30340;GPU&#36164;&#28304;&#26469;&#36816;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#12290;&#23613;&#31649;&#36890;&#36807;&#20248;&#21270;&#37197;&#32622;&#21442;&#25968;&#65288;&#22914;&#35270;&#39057;&#20998;&#36776;&#29575;&#21644;&#24103;&#29575;&#65289;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#23545;&#32593;&#32476;&#24102;&#23485;&#21644;GPU&#36164;&#28304;&#30340;&#38656;&#27714;&#65292;&#20294;&#30446;&#21069;&#30340;&#33258;&#36866;&#24212;&#25216;&#26415;&#26080;&#27861;&#21516;&#26102;&#28385;&#36275;&#19977;&#20010;&#35201;&#27714;&#65306;&#65288;i&#65289;&#20197;&#26368;&#23567;&#30340;&#39069;&#22806;GPU&#25110;&#24102;&#23485;&#24320;&#38144;&#26469;&#33258;&#36866;&#24212;&#37197;&#32622;&#65307;&#65288;ii&#65289;&#22522;&#20110;&#25968;&#25454;&#23545;&#26368;&#32456;DNN&#30340;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#26469;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#20915;&#31574;&#65307;&#65288;iii&#65289;&#38024;&#23545;&#19968;&#31995;&#21015;&#37197;&#32622;&#21442;&#25968;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;OneAdapt&#65292;&#36890;&#36807;&#21033;&#29992;&#26799;&#24230;&#19978;&#21319;&#31574;&#30053;&#26469;&#33258;&#36866;&#24212;&#37197;&#32622;&#21442;&#25968;&#65292;&#28385;&#36275;&#20102;&#36825;&#20123;&#35201;&#27714;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#20805;&#20998;&#21033;&#29992;DNN&#30340;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
Deep learning inference on streaming media data, such as object detection in video or LiDAR feeds and text extraction from audio waves, is now ubiquitous. To achieve high inference accuracy, these applications typically require significant network bandwidth to gather high-fidelity data and extensive GPU resources to run deep neural networks (DNNs). While the high demand for network bandwidth and GPU resources could be substantially reduced by optimally adapting the configuration knobs, such as video resolution and frame rate, current adaptation techniques fail to meet three requirements simultaneously: adapt configurations (i) with minimum extra GPU or bandwidth overhead; (ii) to reach near-optimal decisions based on how the data affects the final DNN's accuracy, and (iii) do so for a range of configuration knobs. This paper presents OneAdapt, which meets these requirements by leveraging a gradient-ascent strategy to adapt configuration knobs. The key idea is to embrace DNNs' different
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15257</link><description>&lt;p&gt;
STARC:&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#24046;&#24322;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
STARC: A General Framework For Quantifying Differences Between Reward Functions. (arXiv:2309.15257v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65288;STARC&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#22635;&#34917;&#20102;&#22870;&#21169;&#23398;&#20064;&#29702;&#35770;&#22522;&#30784;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#20219;&#21153;&#65292;&#38656;&#35201;&#23558;&#20219;&#21153;&#30340;&#30446;&#26631;&#24418;&#24335;&#21270;&#20026;&#22870;&#21169;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26469;&#35828;&#65292;&#25163;&#21160;&#25351;&#23450;&#19968;&#20010;&#27704;&#19981;&#28608;&#21169;&#19981;&#33391;&#34892;&#20026;&#30340;&#22870;&#21169;&#20989;&#25968;&#38750;&#24120;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#26469;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22870;&#21169;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#23578;&#26410;&#23436;&#21892;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#24120;&#19981;&#30693;&#36947;&#32473;&#23450;&#30340;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#22312;&#39640;&#27010;&#29575;&#19979;&#26159;&#21542;&#20250;&#23398;&#20064;&#21040;&#19968;&#20010;&#23433;&#20840;&#20248;&#21270;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#36825;&#24847;&#21619;&#30528;&#22870;&#21169;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#24517;&#39035;&#32463;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#36825;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24456;&#38590;&#39044;&#27979;&#20854;&#22833;&#25928;&#27169;&#24335;&#12290;&#20854;&#20013;&#19968;&#20010;&#38459;&#30861;&#33719;&#24471;&#26356;&#22909;&#29702;&#35770;&#20445;&#35777;&#30340;&#38556;&#30861;&#26159;&#32570;&#20047;&#36739;&#22909;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#22870;&#21169;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to solve a task using reinforcement learning, it is necessary to first formalise the goal of that task as a reward function. However, for many real-world tasks, it is very difficult to manually specify a reward function that never incentivises undesirable behaviour. As a result, it is increasingly popular to use reward learning algorithms, which attempt to learn a reward function from data. However, the theoretical foundations of reward learning are not yet well-developed. In particular, it is typically not known when a given reward learning algorithm with high probability will learn a reward function that is safe to optimise. This means that reward learning algorithms generally must be evaluated empirically, which is expensive, and that their failure modes are difficult to predict in advance. One of the roadblocks to deriving better theoretical guarantees is the lack of good methods for quantifying the difference between reward functions. In this paper we provide a solution t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03572</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#21487;&#35777;&#25928;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Learning in Partially Observable Contextual Bandit. (arXiv:2308.03572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#20165;&#26377;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#26377;&#38480;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#21482;&#26377;&#37096;&#20998;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#26469;&#35782;&#21035;&#25110;&#37096;&#20998;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26410;&#30693;&#20998;&#24067;&#30340;&#21407;&#22987;&#21151;&#33021;&#32422;&#26463;&#31163;&#25955;&#21270;&#20026;&#32447;&#24615;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#39034;&#24207;&#35299;&#32447;&#24615;&#35268;&#21010;&#26469;&#37319;&#26679;&#20860;&#23481;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#20272;&#35745;&#35823;&#24046;&#24471;&#21040;&#22240;&#26524;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20026;&#36866;&#24403;&#30340;&#37319;&#26679;&#20998;&#24067;&#25552;&#20379;&#20102;&#29702;&#24819;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#32422;&#26463;&#24212;&#29992;&#20110;&#25913;&#36827;&#32463;&#20856;&#30340;&#36718;&#30424;&#36172;&#31639;&#27861;&#65292;&#24182;&#20197;&#34892;&#21160;&#38598;&#21644;&#20989;&#25968;&#31354;&#38388;&#35268;&#27169;&#20026;&#21442;&#32771;&#25913;&#21464;&#20102;&#36951;&#25022;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#19968;&#33324;&#24773;&#22659;&#20998;&#24067;&#30340;&#20989;&#25968;&#36924;&#36817;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harpa&#30340;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#30456;&#20301;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22320;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.07572</link><description>&lt;p&gt;
Harpa: &#39640;&#36895;&#29575;&#19979;&#30340;&#30456;&#20301;&#20851;&#32852;&#19982;&#36208;&#26102;&#31070;&#32463;&#22330;
&lt;/p&gt;
&lt;p&gt;
Harpa: High-Rate Phase Association with Travel Time Neural Fields. (arXiv:2307.07572v1 [physics.geo-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Harpa&#30340;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#23454;&#29616;&#30456;&#20301;&#20851;&#32852;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22320;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20301;&#20851;&#32852;&#26159;&#26681;&#25454;&#20854;&#36215;&#28304;&#22320;&#38663;&#20998;&#32452;&#22320;&#38663;&#27874;&#21040;&#36798;&#30340;&#20219;&#21153;&#12290;&#23427;&#26159;&#22320;&#38663;&#25968;&#25454;&#22788;&#29702;&#27969;&#31243;&#20013;&#30340;&#22522;&#26412;&#20219;&#21153;&#65292;&#20294;&#23545;&#20110;&#36739;&#23567;&#12289;&#39640;&#36895;&#29575;&#30340;&#22320;&#38663;&#20107;&#20214;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#36825;&#20123;&#20107;&#20214;&#25658;&#24102;&#26377;&#20851;&#22320;&#38663;&#21160;&#21147;&#23398;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#23588;&#20854;&#26159;&#22312;&#24120;&#24120;&#20551;&#23450;&#19981;&#20934;&#30830;&#30340;&#27874;&#36895;&#27169;&#22411;&#19979;&#12290;&#22240;&#27492;&#65292;&#22823;&#22810;&#25968;&#20851;&#32852;&#26041;&#27861;&#37117;&#19987;&#27880;&#20110;&#21457;&#29983;&#29575;&#36739;&#20302;&#19988;&#23481;&#26131;&#20851;&#32852;&#30340;&#36739;&#22823;&#20107;&#20214;&#65292;&#23613;&#31649;&#24494;&#22320;&#38663;&#27963;&#21160;&#25552;&#20379;&#20102;&#20117;&#19979;&#24377;&#24615;&#20171;&#36136;&#23646;&#24615;&#30340;&#23453;&#36149;&#25551;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#27874;&#36895;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#21487;&#20197;&#20197;&#27604;&#20197;&#21069;&#25253;&#21578;&#30340;&#26356;&#39640;&#30340;&#36895;&#29575;&#36827;&#34892;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Harpa&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#36895;&#29575;&#22320;&#38663;&#30456;&#20301;&#20851;&#32852;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#22330;&#26500;&#24314;&#27874;&#36895;&#21644;&#30456;&#20851;&#36208;&#26102;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#39318;&#20808;&#35299;&#20915;&#32852;&#21512;&#26102;&#31354;&#28304;&#23450;&#20301;&#21644;&#27874;&#36895;&#24674;&#22797;&#38382;&#39064;&#65292;&#28982;&#21518;&#36827;&#34892;&#30456;&#20301;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
Phase association groups seismic wave arrivals according to their originating earthquakes. It is a fundamental task in a seismic data processing pipeline, but challenging to perform for smaller, high-rate seismic events which carry fundamental information about earthquake dynamics, especially with a commonly assumed inaccurate wave speed model. As a consequence, most association methods focus on larger events that occur at a lower rate and are thus easier to associate, even though microseismicity provides a valuable description of the elastic medium properties in the subsurface. In this paper, we show that association is possible at rates much higher than previously reported even when the wave speed is unknown. We propose Harpa, a high-rate seismic phase association method which leverages deep neural fields to build generative models of wave speeds and associated travel times, and first solves a joint spatio--temporal source localization and wave speed recovery problem, followed by ass
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#38899;&#35270;&#21516;&#27493;&#25439;&#22833;&#21644;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21644;&#22836;&#37096;&#21160;&#21147;&#23398;&#30340;&#21516;&#27493;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24403;&#21069;&#29366;&#24577;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.03270</link><description>&lt;p&gt;
&#35821;&#38899;&#21644;&#21160;&#21147;&#23398;&#21516;&#27493;&#30340;&#20840;&#38754;&#22810;&#23610;&#24230;&#26041;&#27861;&#22312;&#34394;&#25311;&#35828;&#35805;&#22836;&#29983;&#25104;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Multi-scale Approach for Speech and Dynamics Synchrony in Talking Head Generation. (arXiv:2307.03270v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03270
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#22810;&#23610;&#24230;&#38899;&#35270;&#21516;&#27493;&#25439;&#22833;&#21644;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#35821;&#38899;&#21644;&#22836;&#37096;&#21160;&#21147;&#23398;&#30340;&#21516;&#27493;&#29983;&#25104;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#24403;&#21069;&#29366;&#24577;&#19979;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20351;&#29992;&#35821;&#38899;&#36755;&#20837;&#20449;&#21495;&#23545;&#38745;&#24577;&#38754;&#37096;&#22270;&#20687;&#36827;&#34892;&#21160;&#30011;&#21270;&#26159;&#19968;&#20010;&#27963;&#36291;&#30340;&#30740;&#31350;&#35838;&#39064;&#65292;&#24182;&#19988;&#36817;&#26399;&#21462;&#24471;&#20102;&#37325;&#35201;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#24456;&#22823;&#19968;&#37096;&#20998;&#24037;&#20316;&#37117;&#38598;&#20013;&#22312;&#22068;&#21767;&#21516;&#27493;&#21644;&#28210;&#26579;&#36136;&#37327;&#19978;&#65292;&#24456;&#23569;&#20851;&#27880;&#33258;&#28982;&#22836;&#37096;&#36816;&#21160;&#30340;&#29983;&#25104;&#65292;&#26356;&#19981;&#29992;&#35828;&#22836;&#37096;&#36816;&#21160;&#19982;&#35821;&#38899;&#30340;&#35270;&#21548;&#30456;&#20851;&#24615;&#20102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#38899;&#35270;&#21516;&#27493;&#25439;&#22833;&#21644;&#22810;&#23610;&#24230;&#33258;&#22238;&#24402;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#35821;&#38899;&#19982;&#22836;&#37096;&#21644;&#22068;&#21767;&#21160;&#21147;&#23398;&#20043;&#38388;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#30456;&#20851;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#36755;&#20837;&#37329;&#23383;&#22612;&#19978;&#35757;&#32451;&#20102;&#19968;&#22534;&#21516;&#27493;&#27169;&#22411;&#65292;&#24182;&#23558;&#36825;&#20123;&#27169;&#22411;&#29992;&#20316;&#22810;&#23610;&#24230;&#29983;&#25104;&#32593;&#32476;&#20013;&#30340;&#25351;&#23548;&#65292;&#20197;&#20135;&#29983;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#38899;&#39057;&#23545;&#40784;&#36816;&#21160;&#23637;&#24320;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#22312;&#38754;&#37096;&#26631;&#24535;&#22495;&#20013;&#25805;&#20316;&#65292;&#36825;&#26159;&#19968;&#31181;&#26631;&#20934;&#30340;&#20302;&#32500;&#22836;&#37096;&#34920;&#31034;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22836;&#37096;&#36816;&#21160;&#21160;&#21147;&#23398;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animating still face images with deep generative models using a speech input signal is an active research topic and has seen important recent progress. However, much of the effort has been put into lip syncing and rendering quality while the generation of natural head motion, let alone the audio-visual correlation between head motion and speech, has often been neglected. In this work, we propose a multi-scale audio-visual synchrony loss and a multi-scale autoregressive GAN to better handle short and long-term correlation between speech and the dynamics of the head and lips. In particular, we train a stack of syncer models on multimodal input pyramids and use these models as guidance in a multi-scale generator network to produce audio-aligned motion unfolding over diverse time scales. Our generator operates in the facial landmark domain, which is a standard low-dimensional head representation. The experiments show significant improvements over the state of the art in head motion dynamic
&lt;/p&gt;</description></item><item><title>NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.16869</link><description>&lt;p&gt;
NeuralFuse: &#23398;&#20064;&#25913;&#21892;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
NeuralFuse: Learning to Improve the Accuracy of Access-Limited Neural Network Inference in Low-Voltage Regimes. (arXiv:2306.16869v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16869
&lt;/p&gt;
&lt;p&gt;
NeuralFuse&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#20302;&#30005;&#21387;&#29615;&#22659;&#19979;&#26377;&#38480;&#35775;&#38382;&#31070;&#32463;&#32593;&#32476;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#26080;&#22788;&#19981;&#22312;&#65292;&#20294;&#20854;&#33021;&#37327;&#28040;&#32791;&#20173;&#28982;&#26159;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#38382;&#39064;&#12290;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#26159;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36807;&#24230;&#38477;&#20302;&#20379;&#30005;&#30005;&#21387;&#21487;&#33021;&#20250;&#23548;&#33268;&#20934;&#30830;&#24615;&#38477;&#20302;&#65292;&#22240;&#20026;&#27169;&#22411;&#21442;&#25968;&#23384;&#20648;&#22312;&#38745;&#24577;&#38543;&#26426;&#23384;&#20648;&#22120;(SRAM)&#20013;&#65292;&#32780;SRAM&#20013;&#20250;&#21457;&#29983;&#38543;&#26426;&#20301;&#32763;&#36716;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;NeuralFuse&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#38468;&#21152;&#27169;&#22359;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#36716;&#25442;&#26469;&#29983;&#25104;&#25239;&#35823;&#24046;&#30340;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#22312;&#20302;&#30005;&#21387;&#29615;&#22659;&#20013;&#35299;&#20915;&#20934;&#30830;&#24615;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;NeuralFuse&#22312;&#26631;&#31216;&#30005;&#21387;&#21644;&#20302;&#30005;&#21387;&#24773;&#20917;&#19979;&#37117;&#33021;&#20445;&#25252;DNN&#30340;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;NeuralFuse&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#26377;&#38480;&#35775;&#38382;&#30340;DNN&#65292;&#20363;&#22914;&#19981;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#25110;&#20113;&#31471;API&#30340;&#36828;&#31243;&#35775;&#38382;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;1%&#30340;&#20301;&#38169;&#35823;&#29575;&#19979;&#65292;NeuralFuse&#21487;&#20197;&#23558;SRAM&#20869;&#23384;&#35775;&#38382;&#33021;&#37327;&#38477;&#20302;&#39640;&#36798;24%&#65292;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have become ubiquitous in machine learning, but their energy consumption remains a notable issue. Lowering the supply voltage is an effective strategy for reducing energy consumption. However, aggressively scaling down the supply voltage can lead to accuracy degradation due to random bit flips in static random access memory (SRAM) where model parameters are stored. To address this challenge, we introduce NeuralFuse, a novel add-on module that addresses the accuracy-energy tradeoff in low-voltage regimes by learning input transformations to generate error-resistant data representations. NeuralFuse protects DNN accuracy in both nominal and low-voltage scenarios. Moreover, NeuralFuse is easy to implement and can be readily applied to DNNs with limited access, such as non-configurable hardware or remote access to cloud-based APIs. Experimental results demonstrate that, at a 1% bit error rate, NeuralFuse can reduce SRAM memory access energy by up to 24% while imp
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#38745;&#24687;&#24577;&#21407;&#22987;&#33041;&#30005;&#20449;&#21495;&#20013;&#26816;&#27979;&#24930;&#24615;&#30140;&#30171;&#65292;&#35813;&#31639;&#27861;&#22312;&#31867;&#21035;&#21487;&#20998;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.15194</link><description>&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#20174;&#38745;&#24687;&#24577;&#21407;&#22987;&#33041;&#30005;&#20449;&#21495;&#20013;&#26816;&#27979;&#24930;&#24615;&#30140;&#30171;
&lt;/p&gt;
&lt;p&gt;
Chronic pain detection from resting-state raw EEG signals using improved feature selection. (arXiv:2306.15194v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15194
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#38745;&#24687;&#24577;&#21407;&#22987;&#33041;&#30005;&#20449;&#21495;&#20013;&#26816;&#27979;&#24930;&#24615;&#30140;&#30171;&#65292;&#35813;&#31639;&#27861;&#22312;&#31867;&#21035;&#21487;&#20998;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#24182;&#19988;&#22312;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#39640;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#38745;&#24687;&#24577;&#21407;&#22987;&#33041;&#30005;&#25968;&#25454;&#20013;&#26816;&#27979;&#24930;&#24615;&#30140;&#30171;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;-&#25913;&#36827;&#30340;&#39034;&#24207;&#28014;&#21160;&#21069;&#21521;&#36873;&#25321;&#65288;mSFFS&#65289;&#12290;&#25913;&#36827;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#26696;&#34429;&#28982;&#36739;&#20026;&#31616;&#27905;&#65292;&#20294;&#26174;&#31034;&#20986;&#26356;&#22909;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#36825;&#30001;Bhattacharyya&#36317;&#31163;&#24230;&#37327;&#21644;&#26356;&#22909;&#30340;&#21487;&#35270;&#21270;&#32467;&#26524;&#25152;&#31034;&#12290;&#23427;&#20063;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#26041;&#27861;&#29983;&#25104;&#30340;&#36873;&#25321;&#65292;&#23558;&#27979;&#35797;&#20934;&#30830;&#29575;&#25552;&#39640;&#21040;97.5&#65285;&#65292;&#24182;&#22312;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#24930;&#24615;&#30140;&#30171;&#30340;&#22806;&#37096;&#25968;&#25454;&#38598;&#19978;&#20135;&#29983;81.4&#65285;&#30340;&#27979;&#35797;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an automatic approach that works on resting-state raw EEG data for chronic pain detection. A new feature selection algorithm - modified Sequential Floating Forward Selection (mSFFS) - is proposed. The improved feature selection scheme is rather compact but displays better class separability as indicated by the Bhattacharyya distance measures and better visualization results. It also outperforms selections generated by other benchmark methods, boosting the test accuracy to 97.5% and yielding a test accuracy of 81.4% on an external dataset that contains different types of chronic pain
&lt;/p&gt;</description></item><item><title>AdaStop&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#24207;&#21015;&#27979;&#35797;&#30340;&#26032;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27604;&#36739;&#22810;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#23454;&#39564;&#32467;&#26524;&#21487;&#22797;&#21046;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10882</link><description>&lt;p&gt;
AdaStop&#65306;&#29992;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#27604;&#36739;&#30340;&#39640;&#25928;&#21487;&#38752;&#24207;&#21015;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
AdaStop: sequential testing for efficient and reliable comparisons of Deep RL Agents. (arXiv:2306.10882v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10882
&lt;/p&gt;
&lt;p&gt;
AdaStop&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#24207;&#21015;&#27979;&#35797;&#30340;&#26032;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#27604;&#36739;&#22810;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#35299;&#20915;&#23454;&#39564;&#32467;&#26524;&#21487;&#22797;&#21046;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#23454;&#39564;&#32467;&#26524;&#30340;&#21487;&#22797;&#29616;&#24615;&#21463;&#21040;&#36136;&#30097;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#21487;&#22797;&#29616;&#24615;&#21361;&#26426;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#21487;&#38752;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27604;&#36739;&#22810;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#30001;&#20110;&#19968;&#20010;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#19968;&#27425;&#25191;&#34892;&#24615;&#33021;&#26159;&#38543;&#26426;&#30340;&#65292;&#25152;&#20197;&#38656;&#35201;&#36827;&#34892;&#29420;&#31435;&#30340;&#22810;&#27425;&#25191;&#34892;&#26469;&#31934;&#30830;&#35780;&#20272;&#23427;&#12290;&#24403;&#27604;&#36739;&#22810;&#20010;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26102;&#65292;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#38656;&#35201;&#36827;&#34892;&#22810;&#23569;&#27425;&#25191;&#34892;&#65292;&#24182;&#19988;&#22914;&#20309;&#30830;&#20445;&#36825;&#26679;&#27604;&#36739;&#30340;&#32467;&#26524;&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#38752;&#30340;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#20351;&#29992;&#23569;&#20110;5&#20010;&#29420;&#31435;&#25191;&#34892;&#26469;&#27604;&#36739;&#31639;&#27861;&#65306;&#25105;&#20204;&#35748;&#20026;&#36825;&#36890;&#24120;&#26159;&#19981;&#22815;&#30340;&#12290;&#32780;&#19988;&#65292;&#24403;&#21516;&#26102;&#27604;&#36739;&#20960;&#20010;&#31639;&#27861;&#26102;&#65292;&#27599;&#20010;&#27604;&#36739;&#30340;&#35823;&#24046;&#37117;&#20250;&#32047;&#31215;&#65292;&#24517;&#39035;&#37319;&#29992;&#22810;&#37325;&#27979;&#35797;&#31243;&#24207;&#26469;&#32771;&#34385;&#36825;&#20123;&#35823;&#24046;&#65292;&#20197;&#32500;&#25345;&#20302;&#35823;&#24046;&#20445;&#35777;&#12290;&#20026;&#20102;&#20197;&#32479;&#35745;&#23398;&#19978;&#30340;&#21487;&#38752;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AdaStop&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#32452;&#24207;&#21015;&#27979;&#35797;&#30340;&#26032;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reproducibility of many experimental results in Deep Reinforcement Learning (RL) is under question. To solve this reproducibility crisis, we propose a theoretically sound methodology to compare multiple Deep RL algorithms. The performance of one execution of a Deep RL algorithm is random so that independent executions are needed to assess it precisely. When comparing several RL algorithms, a major question is how many executions must be made and how can we assure that the results of such a comparison is theoretically sound. Researchers in Deep RL often use less than 5 independent executions to compare algorithms: we claim that this is not enough in general. Moreover, when comparing several algorithms at once, the error of each comparison accumulates and must be taken into account with a multiple tests procedure to preserve low error guarantees. To address this problem in a statistically sound way, we introduce AdaStop, a new statistical test based on multiple group sequential tests
&lt;/p&gt;</description></item><item><title>SimVP&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#23436;&#20840;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#19988;&#27809;&#26377;&#24490;&#29615;&#32467;&#26500;&#65292;&#24182;&#19988;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;SimVP&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.12509</link><description>&lt;p&gt;
SimVP: &#36739;&#20026;&#31616;&#21333;&#21364;&#24378;&#22823;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning. (arXiv:2211.12509v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12509
&lt;/p&gt;
&lt;p&gt;
SimVP&#26159;&#19968;&#31181;&#31616;&#21333;&#19988;&#24378;&#22823;&#30340;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#65292;&#23436;&#20840;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#19988;&#27809;&#26377;&#24490;&#29615;&#32467;&#26500;&#65292;&#24182;&#19988;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20026;&#35757;&#32451;&#30446;&#26631;&#12290;SimVP&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#22343;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26102;&#31354;&#39044;&#27979;&#23398;&#20064;&#22312;&#36741;&#21161;&#36755;&#20837;&#12289;&#22797;&#26434;&#31070;&#32463;&#32467;&#26500;&#21644;&#31934;&#32454;&#21270;&#35757;&#32451;&#31574;&#30053;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20027;&#27969;&#26041;&#27861;&#30340;&#31995;&#32479;&#22797;&#26434;&#24615;&#19981;&#26029;&#22686;&#21152;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#23427;&#20204;&#30340;&#20415;&#25463;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;SimVP&#65292;&#19968;&#31181;&#23436;&#20840;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#19988;&#27809;&#26377;&#24490;&#29615;&#32467;&#26500;&#12289;&#20197;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#20026;&#35757;&#32451;&#30446;&#26631;&#30340;&#31616;&#21333;&#26102;&#31354;&#39044;&#27979;&#22522;&#20934;&#27169;&#22411;&#12290;&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25216;&#24039;&#21644;&#31574;&#30053;&#65292;SimVP&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#21462;&#24471;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#20174;SimVP&#20013;&#25552;&#21462;&#20102;&#20855;&#26377;&#38376;&#24335;&#26102;&#31354;&#27880;&#24847;&#21147;&#32763;&#35793;&#22120;&#30340;&#21464;&#20307;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SimVP&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#24378;&#22823;&#30340;&#27867;&#21270;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290; &#32780;&#19988;&#35757;&#32451;&#25104;&#26412;&#26174;&#33879;&#38477;&#20302;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed remarkable advances in spatiotemporal predictive learning, incorporating auxiliary inputs, elaborate neural architectures, and sophisticated training strategies. Although impressive, the system complexity of mainstream methods is increasing as well, which may hinder the convenient applications. This paper proposes SimVP, a simple spatiotemporal predictive baseline model that is completely built upon convolutional networks without recurrent architectures and trained by common mean squared error loss in an end-to-end fashion. Without introducing any extra tricks and strategies, SimVP can achieve superior performance on various benchmark datasets. To further improve the performance, we derive variants with the gated spatiotemporal attention translator from SimVP that can achieve better performance. We demonstrate that SimVP has strong generalization and extensibility on real-world datasets through extensive experiments. The significant reduction in training cos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Bregman&#36817;&#31471;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21457;&#29616;&#20854;&#19982;&#23616;&#37096;&#20960;&#20309;&#12289;&#27491;&#21017;&#24615;&#20197;&#21450;&#38160;&#24230;&#31561;&#22240;&#32032;&#30456;&#20851;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#30028;&#35299;&#22312;&#38646;&#21644;&#38750;&#38646;Legendre&#25351;&#25968;&#30340;&#26041;&#27861;&#20043;&#38388;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#21069;&#32773;&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#32780;&#21518;&#32773;&#20197;&#27425;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#12290;&#22312;&#32447;&#24615;&#32422;&#26463;&#38382;&#39064;&#20013;&#65292;&#20855;&#26377;&#29109;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#22312;&#23574;&#38160;&#26041;&#21521;&#19978;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20256;&#32479;&#25910;&#25947;&#36895;&#24230;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;</title><link>http://arxiv.org/abs/2211.08043</link><description>&lt;p&gt;
Bregman&#36817;&#31471;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65306;&#23616;&#37096;&#20960;&#20309; vs &#27491;&#21017;&#24615; vs &#38160;&#24230;
&lt;/p&gt;
&lt;p&gt;
The rate of convergence of Bregman proximal methods: Local geometry vs. regularity vs. sharpness. (arXiv:2211.08043v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Bregman&#36817;&#31471;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21457;&#29616;&#20854;&#19982;&#23616;&#37096;&#20960;&#20309;&#12289;&#27491;&#21017;&#24615;&#20197;&#21450;&#38160;&#24230;&#31561;&#22240;&#32032;&#30456;&#20851;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21457;&#29616;&#36793;&#30028;&#35299;&#22312;&#38646;&#21644;&#38750;&#38646;Legendre&#25351;&#25968;&#30340;&#26041;&#27861;&#20043;&#38388;&#26377;&#26126;&#26174;&#30340;&#24046;&#24322;&#65292;&#21069;&#32773;&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#32780;&#21518;&#32773;&#20197;&#27425;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#12290;&#22312;&#32447;&#24615;&#32422;&#26463;&#38382;&#39064;&#20013;&#65292;&#20855;&#26377;&#29109;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#22312;&#23574;&#38160;&#26041;&#21521;&#19978;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20256;&#32479;&#25910;&#25947;&#36895;&#24230;&#26377;&#26174;&#33879;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#38236;&#20687;&#19979;&#38477;&#21040;&#38236;&#20687;-&#36817;&#31471;&#21450;&#20854;&#20048;&#35266;&#21464;&#31181;&#30340;Bregman&#36817;&#31471;&#26041;&#27861;&#30340;&#26368;&#21518;&#36845;&#20195;&#25910;&#25947;&#36895;&#24230;&#65292;&#20316;&#20026;&#19968;&#31181;&#20381;&#36182;&#20110;&#23450;&#20041;&#26041;&#27861;&#30340;&#36817;&#31471;&#26144;&#23556;&#25152;&#23548;&#33268;&#30340;&#23616;&#37096;&#20960;&#20309;&#30340;&#20989;&#25968;&#12290;&#20026;&#20102;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#20851;&#27880;&#32422;&#26463;&#30340;&#38750;&#21333;&#35843;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#23616;&#37096;&#35299;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#32473;&#23450;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#21462;&#20915;&#20110;&#20854;&#30456;&#20851;&#30340;Legendre&#25351;&#25968;&#65292;&#36825;&#20010;&#27010;&#24565;&#24230;&#37327;&#20102;&#25509;&#36817;&#35299;&#30340;Bregman&#20989;&#25968;&#65288;&#27431;&#20960;&#37324;&#24471;&#12289;&#29109;&#25110;&#20854;&#20182;&#20989;&#25968;&#65289;&#30340;&#22686;&#38271;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36793;&#30028;&#35299;&#22312;&#20855;&#26377;&#38646;&#21644;&#38750;&#38646;Legendre&#25351;&#25968;&#30340;&#26041;&#27861;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#30340;&#20998;&#31163;&#24773;&#20917;&#65306;&#21069;&#32773;&#20197;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#65292;&#32780;&#21518;&#32773;&#19968;&#33324;&#20197;&#27425;&#32447;&#24615;&#36895;&#24230;&#25910;&#25947;&#12290;&#22312;&#32447;&#24615;&#32422;&#26463;&#38382;&#39064;&#20013;&#65292;&#24403;&#26041;&#27861;&#20855;&#26377;&#29109;&#27491;&#21017;&#21270;&#26102;&#65292;&#36825;&#31181;&#20108;&#20998;&#27861;&#22312;&#23574;&#38160;&#26041;&#21521;&#19978;&#23454;&#29616;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20256;&#32479;&#25910;&#25947;&#36895;&#24230;&#30340;&#24046;&#21035;&#26356;&#21152;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the last-iterate convergence rate of Bregman proximal methods from mirror descent to mirror-prox and its optimistic variants - as a function of the local geometry induced by the prox-mapping defining the method. For generality, we focus on local solutions of constrained, non-monotone variational inequalities, and we show that the convergence rate of a given method depends sharply on its associated Legendre exponent, a notion that measures the growth rate of the underlying Bregman function (Euclidean, entropic, or other) near a solution. In particular, we show that boundary solutions exhibit a stark separation of regimes between methods with a zero and non-zero Legendre exponent: the former converge at a linear rate, while the latter converge, in general, sublinearly. This dichotomy becomes even more pronounced in linearly constrained problems where methods with entropic regularization achieve a linear convergence rate along sharp directions, compared to convergence in a fi
&lt;/p&gt;</description></item></channel></rss>