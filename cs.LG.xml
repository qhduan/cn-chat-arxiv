<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Log-NCDEs&#26159;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;NCDEs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Log-ODE&#26041;&#27861;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#36817;&#20284;CDE&#30340;&#35299;&#65292;&#24182;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.18512</link><description>&lt;p&gt;
Log&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65306;&#26446;&#25324;&#21495;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Log Neural Controlled Differential Equations: The Lie Brackets Make a Difference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18512
&lt;/p&gt;
&lt;p&gt;
Log-NCDEs&#26159;&#19968;&#31181;&#26032;&#39062;&#32780;&#26377;&#25928;&#30340;&#35757;&#32451;NCDEs&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Log-ODE&#26041;&#27861;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#36817;&#20284;CDE&#30340;&#35299;&#65292;&#24182;&#22312;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#25511;&#24494;&#20998;&#26041;&#31243;&#65288;CDE&#65289;&#30340;&#30690;&#37327;&#22330;&#25551;&#36848;&#20102;&#25511;&#21046;&#36335;&#24452;&#19982;&#35299;&#36335;&#24452;&#28436;&#21270;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#31070;&#32463;CDE&#65288;NCDE&#65289;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#35270;&#20026;&#23545;&#25511;&#21046;&#36335;&#24452;&#30340;&#35266;&#27979;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;CDE&#30340;&#30690;&#37327;&#22330;&#36827;&#34892;&#21442;&#25968;&#21270;&#65292;&#24182;&#23558;&#35299;&#36335;&#24452;&#20316;&#20026;&#25345;&#32493;&#28436;&#21270;&#30340;&#38544;&#34255;&#29366;&#24577;&#12290;&#30001;&#20110;&#20854;&#26500;&#36896;&#20351;&#20854;&#33021;&#22815;&#25269;&#25239;&#19981;&#35268;&#21017;&#37319;&#26679;&#29575;&#65292;NCDE&#26159;&#24314;&#27169;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24378;&#22823;&#26041;&#27861;&#12290;&#22312;&#31070;&#32463;&#31895;&#31961;&#24494;&#20998;&#26041;&#31243;&#65288;NRDE&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Log-NCDE&#65292;&#36825;&#26159;&#19968;&#31181;&#35757;&#32451;NCDE&#30340;&#26032;&#39062;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;Log-NCDE&#30340;&#26680;&#24515;&#32452;&#20214;&#26159;Log-ODE&#26041;&#27861;&#65292;&#36825;&#26159;&#20174;&#31895;&#31961;&#36335;&#24452;&#30740;&#31350;&#20013;&#30340;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;CDE&#35299;&#30340;&#24037;&#20855;&#12290;&#22312;&#19968;&#31995;&#21015;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#19978;&#65292;&#23637;&#31034;&#20102;Log-NCDE&#27604;NCDE&#65292;NRDE&#21644;&#20004;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;S5&#21644;&#32447;&#24615;&#36882;&#24402;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#24179;&#22343;&#27979;&#35797;&#38598;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18512v1 Announce Type: new  Abstract: The vector field of a controlled differential equation (CDE) describes the relationship between a control path and the evolution of a solution path. Neural CDEs (NCDEs) treat time series data as observations from a control path, parameterise a CDE's vector field using a neural network, and use the solution path as a continuously evolving hidden state. As their formulation makes them robust to irregular sampling rates, NCDEs are a powerful approach for modelling real-world data. Building on neural rough differential equations (NRDEs), we introduce Log-NCDEs, a novel and effective method for training NCDEs. The core component of Log-NCDEs is the Log-ODE method, a tool from the study of rough paths for approximating a CDE's solution. On a range of multivariate time series classification benchmarks, Log-NCDEs are shown to achieve a higher average test set accuracy than NCDEs, NRDEs, and two state-of-the-art models, S5 and the linear recurren
&lt;/p&gt;</description></item><item><title>&#23558;&#22870;&#21169;&#20998;&#35299;&#20026;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#21644;&#24120;&#35782;&#22870;&#21169;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#24120;&#35782;&#22870;&#21169;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#35757;&#32451;&#20195;&#29702;&#21518;&#65292;&#24182;&#19981;&#20250;&#23398;&#21040;&#26377;&#29992;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.11367</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#36870;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#24120;&#35782;&#22870;&#21169;
&lt;/p&gt;
&lt;p&gt;
Multi Task Inverse Reinforcement Learning for Common Sense Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11367
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22870;&#21169;&#20998;&#35299;&#20026;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#21644;&#24120;&#35782;&#22870;&#21169;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#24120;&#35782;&#22870;&#21169;&#65307;&#30740;&#31350;&#21457;&#29616;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#35757;&#32451;&#20195;&#29702;&#21518;&#65292;&#24182;&#19981;&#20250;&#23398;&#21040;&#26377;&#29992;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#65292;&#19968;&#20010;&#25361;&#25112;&#22312;&#20110;&#20026;agent&#25552;&#20379;&#36275;&#22815;&#35814;&#32454;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#22870;&#21169;&#19982;&#26399;&#26395;&#34892;&#20026;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#21487;&#33021;&#23548;&#33268;&#24847;&#22806;&#32467;&#26524;&#65292;&#22914;&#8220;&#22870;&#21169;&#31713;&#25913;&#8221;&#65292;agent&#36890;&#36807;&#24847;&#22806;&#34892;&#20026;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#22870;&#21169;&#20998;&#35299;&#20026;&#20004;&#20010;&#26126;&#30830;&#37096;&#20998;&#65306;&#19968;&#20010;&#31616;&#21333;&#30340;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#65292;&#27010;&#36848;&#20102;&#24403;&#21069;&#20219;&#21153;&#30340;&#32454;&#33410;&#65307;&#20197;&#21450;&#19968;&#20010;&#26410;&#30693;&#30340;&#24120;&#35782;&#22870;&#21169;&#65292;&#25351;&#31034;agent&#22312;&#29615;&#22659;&#20013;&#30340;&#39044;&#26399;&#34892;&#20026;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064;&#36825;&#31181;&#24120;&#35782;&#22870;&#21169;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#65292;&#21363;&#20351;&#36870;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#35757;&#32451;&#20102;&#19968;&#20010;&#20195;&#29702;&#65292;&#20063;&#19981;&#20250;&#23398;&#21040;&#19968;&#20010;&#26377;&#29992;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20351;&#29992;&#23398;&#21040;&#30340;&#22870;&#21169;&#35757;&#32451;&#26032;&#20195;&#29702;&#19981;&#20250;&#24433;&#21709;&#26399;&#26395;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11367v1 Announce Type: new  Abstract: One of the challenges in applying reinforcement learning in a complex real-world environment lies in providing the agent with a sufficiently detailed reward function. Any misalignment between the reward and the desired behavior can result in unwanted outcomes. This may lead to issues like "reward hacking" where the agent maximizes rewards by unintended behavior. In this work, we propose to disentangle the reward into two distinct parts. A simple task-specific reward, outlining the particulars of the task at hand, and an unknown common-sense reward, indicating the expected behavior of the agent within the environment. We then explore how this common-sense reward can be learned from expert demonstrations. We first show that inverse reinforcement learning, even when it succeeds in training an agent, does not learn a useful reward function. That is, training a new agent with the learned reward does not impair the desired behaviors. We then d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;ResNets&#23548;&#20986;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#25351;&#20986;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#26159;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2305.07715</link><description>&lt;p&gt;
&#36890;&#36807;&#27531;&#24046;&#32553;&#25918;&#23454;&#29616;ResNets&#30340;&#20449;&#21495;&#26368;&#20248;&#20256;&#36882;
&lt;/p&gt;
&lt;p&gt;
Optimal signal propagation in ResNets through residual scaling. (arXiv:2305.07715v1 [cond-mat.dis-nn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;ResNets&#23548;&#20986;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#25351;&#20986;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#26159;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Residual&#32593;&#32476;&#65288;ResNets&#65289;&#22312;&#22823;&#28145;&#24230;&#19978;&#27604;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#33021;&#21147;&#21644;&#24615;&#33021;&#12290;&#24341;&#20837;&#36339;&#36807;&#36830;&#25509;&#21487;&#20197;&#20419;&#36827;&#20449;&#21495;&#21521;&#26356;&#28145;&#23618;&#30340;&#20256;&#36882;&#12290;&#27492;&#22806;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#21457;&#29616;&#20026;&#27531;&#24046;&#20998;&#25903;&#28155;&#21152;&#32553;&#25918;&#21442;&#25968;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#23613;&#31649;&#20182;&#20204;&#32463;&#39564;&#24615;&#22320;&#30830;&#23450;&#20102;&#36825;&#31181;&#32553;&#25918;&#21442;&#25968;&#29305;&#21035;&#26377;&#21033;&#30340;&#21462;&#20540;&#33539;&#22260;&#65292;&#20294;&#20854;&#30456;&#20851;&#30340;&#24615;&#33021;&#25552;&#21319;&#21450;&#20854;&#22312;&#32593;&#32476;&#36229;&#21442;&#25968;&#19978;&#30340;&#26222;&#36866;&#24615;&#20173;&#38656;&#35201;&#36827;&#19968;&#27493;&#29702;&#35299;&#12290;&#23545;&#20110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FFNets&#65289;&#65292;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#22312;&#20449;&#21495;&#20256;&#25773;&#21644;&#36229;&#21442;&#25968;&#35843;&#33410;&#26041;&#38754;&#33719;&#24471;&#20102;&#37325;&#35201;&#27934;&#35265;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20026;ResNets&#23548;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#30340;&#26377;&#38480;&#23610;&#23544;&#29702;&#35770;&#65292;&#20197;&#30740;&#31350;&#20449;&#21495;&#20256;&#25773;&#21450;&#20854;&#23545;&#27531;&#24046;&#20998;&#25903;&#32553;&#25918;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#23548;&#20986;&#21709;&#24212;&#20989;&#25968;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#36825;&#26159;&#34913;&#37327;&#32593;&#32476;&#23545;&#36755;&#20837;&#25935;&#24863;&#24615;&#30340;&#19968;&#31181;&#25351;&#26631;&#65292;&#24182;&#34920;&#26126;&#23545;&#20110;&#28145;&#23618;&#32593;&#32476;&#26550;&#26500;&#65292;&#32553;&#25918;&#21442;&#25968;&#22312;&#20248;&#21270;&#20449;&#21495;&#20256;&#25773;&#21644;&#30830;&#20445;&#26377;&#25928;&#21033;&#29992;&#32593;&#32476;&#28145;&#24230;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Residual networks (ResNets) have significantly better trainability and thus performance than feed-forward networks at large depth. Introducing skip connections facilitates signal propagation to deeper layers. In addition, previous works found that adding a scaling parameter for the residual branch further improves generalization performance. While they empirically identified a particularly beneficial range of values for this scaling parameter, the associated performance improvement and its universality across network hyperparameters yet need to be understood. For feed-forward networks (FFNets), finite-size theories have led to important insights with regard to signal propagation and hyperparameter tuning. We here derive a systematic finite-size theory for ResNets to study signal propagation and its dependence on the scaling for the residual branch. We derive analytical expressions for the response function, a measure for the network's sensitivity to inputs, and show that for deep netwo
&lt;/p&gt;</description></item></channel></rss>