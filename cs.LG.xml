<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;</title><link>https://arxiv.org/abs/2402.16726</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35299;&#37322;&#29702;&#35299;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Interpreting Grokked Transformers in Complex Modular Arithmetic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#35266;&#23519;&#20102;Transformer&#20869;&#37096;&#30005;&#36335;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#20943;&#27861;&#22312;Transformer&#19978;&#36896;&#25104;&#20102;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#20056;&#27861;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65292;&#22810;&#39033;&#24335;&#21472;&#21152;&#20102;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#24182;&#19981;&#28165;&#26224;&#65292;Grokking&#29978;&#33267;&#21487;&#20197;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#36731;&#26494;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Grokking&#19968;&#30452;&#26159;&#35299;&#24320;&#24310;&#36831;&#27867;&#21270;&#20043;&#35868;&#30340;&#31215;&#26497;&#25506;&#32034;&#12290;&#22312;&#24050;&#35299;&#23494;&#27169;&#22411;&#20013;&#35782;&#21035;&#21487;&#35299;&#37322;&#30340;&#31639;&#27861;&#26159;&#29702;&#35299;&#20854;&#26426;&#21046;&#30340;&#26263;&#31034;&#24615;&#32447;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#38500;&#20102;&#26368;&#31616;&#21333;&#21644;&#24191;&#20026;&#30740;&#31350;&#30340;&#27169;&#22359;&#21270;&#21152;&#27861;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#36870;&#21521;&#24037;&#31243;&#35266;&#23519;&#20102;&#36890;&#36807;Grokking&#22312;&#22797;&#26434;&#27169;&#22359;&#21270;&#31639;&#26415;&#20013;&#23398;&#21040;&#30340;&#20869;&#37096;&#30005;&#36335;&#65292;&#31361;&#20986;&#26174;&#31034;&#20102;&#23427;&#20204;&#21160;&#21147;&#23398;&#19978;&#30340;&#37325;&#22823;&#24046;&#24322;&#65306;&#20943;&#27861;&#23545;Transformer&#20135;&#29983;&#24378;&#28872;&#30340;&#19981;&#23545;&#31216;&#24615;&#65307;&#20056;&#27861;&#22312;&#20613;&#31435;&#21494;&#22495;&#30340;&#25152;&#26377;&#39057;&#29575;&#19978;&#38656;&#35201;&#20313;&#24358;&#20559;&#32622;&#20998;&#37327;&#65307;&#22810;&#39033;&#24335;&#36890;&#24120;&#23548;&#33268;&#22522;&#26412;&#31639;&#26415;&#27169;&#24335;&#30340;&#21472;&#21152;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#28165;&#26224;&#30340;&#27169;&#24335;&#24182;&#19981;&#26174;&#29616;&#65307;&#21363;&#20351;&#22312;&#20855;&#26377;&#22522;&#26412;&#23545;&#31216;&#21644;&#20132;&#26367;&#34920;&#36798;&#24335;&#30340;&#39640;&#27425;&#20844;&#24335;&#20013;&#65292;Grokking&#20063;&#24456;&#23481;&#26131;&#21457;&#29983;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27169;&#22359;&#21270;&#31639;&#26415;&#30340;&#26032;&#39062;&#36827;&#23637;&#24230;&#37327;&#65307;&#20613;&#31435;&#21494;&#39057;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16726v2 Announce Type: replace-cross  Abstract: Grokking has been actively explored to reveal the mystery of delayed generalization. Identifying interpretable algorithms inside the grokked models is a suggestive hint to understanding its mechanism. In this work, beyond the simplest and well-studied modular addition, we observe the internal circuits learned through grokking in complex modular arithmetic via interpretable reverse engineering, which highlights the significant difference in their dynamics: subtraction poses a strong asymmetry on Transformer; multiplication requires cosine-biased components at all the frequencies in a Fourier domain; polynomials often result in the superposition of the patterns from elementary arithmetic, but clear patterns do not emerge in challenging cases; grokking can easily occur even in higher-degree formulas with basic symmetric and alternating expressions. We also introduce the novel progress measure for modular arithmetic; Fourier Freque
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05961</link><description>&lt;p&gt;
&#22522;&#22240;&#24341;&#23548;GFlowNets&#65306;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#26041;&#38754;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Genetic-guided GFlowNets: Advancing in Practical Molecular Optimization Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#38598;&#25104;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#21644;&#35757;&#32451;&#31574;&#30053;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26368;&#20339;&#24471;&#20998;15.185&#65292;&#21516;&#26102;&#22312;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;GFlowNet&#21464;&#20307;&#65292;&#21363;&#22522;&#22240;&#24341;&#23548;GFlowNet (Genetic GFN)&#65292;&#23427;&#23558;&#36845;&#20195;&#36951;&#20256;&#25628;&#32034;&#38598;&#25104;&#21040;GFlowNet&#20013;&#12290;&#36951;&#20256;&#25628;&#32034;&#26377;&#25928;&#22320;&#24341;&#23548;GFlowNet&#36827;&#20837;&#39640;&#22238;&#25253;&#21306;&#22495;&#65292;&#35299;&#20915;&#20102;&#20840;&#23616;&#36807;&#24230;&#25506;&#32034;&#23548;&#33268;&#30340;&#35757;&#32451;&#25928;&#29575;&#20302;&#19979;&#21644;&#25506;&#32034;&#26377;&#38480;&#21306;&#22495;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#35757;&#32451;&#31574;&#30053;&#65292;&#22914;&#22522;&#20110;&#25490;&#21517;&#30340;&#37325;&#25918;&#35757;&#32451;&#21644;&#26080;&#30417;&#30563;&#26368;&#22823;&#20284;&#28982;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#22522;&#22240;&#24341;&#23548;GFlowNet&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#22312;&#23454;&#38469;&#20998;&#23376;&#20248;&#21270; (PMO) &#39046;&#22495;&#30340;&#23448;&#26041;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#31034;&#20102;16.213&#30340;&#26368;&#26032;&#24471;&#20998;&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#27979;&#35797;&#20013;&#25253;&#21578;&#30340;&#26368;&#20339;&#24471;&#20998;15.185&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;23&#20010;&#20219;&#21153;&#20013;&#30340;14&#20010;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#25152;&#26377;&#23545;&#27604;&#26041;&#27861;&#65292;&#21253;&#25324;&#24378;&#21270;&#23398;&#20064;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#29983;&#25104;&#27169;&#22411;&#65292;GFlowNets&#21644;&#36951;&#20256;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel variant of GFlowNet, genetic-guided GFlowNet (Genetic GFN), which integrates an iterative genetic search into GFlowNet. Genetic search effectively guides the GFlowNet to high-rewarded regions, addressing global over-exploration that results in training inefficiency and exploring limited regions. In addition, training strategies, such as rank-based replay training and unsupervised maximum likelihood pre-training, are further introduced to improve the sample efficiency of Genetic GFN. The proposed method shows a state-of-the-art score of 16.213, significantly outperforming the reported best score in the benchmark of 15.185, in practical molecular optimization (PMO), which is an official benchmark for sample-efficient molecular optimization. Remarkably, ours exceeds all baselines, including reinforcement learning, Bayesian optimization, generative models, GFlowNets, and genetic algorithms, in 14 out of 23 tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#25239;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#30340;&#20805;&#20998;&#32479;&#35745;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21487;&#20197;&#38450;&#24481;&#32780;&#19981;&#24809;&#32602;&#20934;&#30830;&#24615;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#38450;&#24481;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.05674</link><description>&lt;p&gt;
&#39640;&#32500;&#27169;&#22411;&#30340;&#23545;&#25239;&#35757;&#32451;&#65306;&#20960;&#20309;&#21644;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
A High Dimensional Model for Adversarial Training: Geometry and Trade-Offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05674
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#24182;&#32473;&#20986;&#20102;&#23545;&#25239;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#30340;&#20805;&#20998;&#32479;&#35745;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#23384;&#22312;&#21487;&#20197;&#38450;&#24481;&#32780;&#19981;&#24809;&#32602;&#20934;&#30830;&#24615;&#30340;&#26041;&#21521;&#65292;&#25581;&#31034;&#20102;&#38450;&#24481;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#21363;&#32500;&#24230;$d$&#21644;&#25968;&#25454;&#28857;&#25968;$n$&#19982;&#22266;&#23450;&#27604;&#20363;$\alpha = n / d$&#21457;&#25955;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#36793;&#38469;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#20013;&#30340;&#23545;&#25239;&#35757;&#32451;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#22788;&#29702;&#30340;&#25968;&#23398;&#27169;&#22411;&#65292;&#21487;&#20197;&#30740;&#31350;&#25968;&#25454;&#21644;&#23545;&#25239;&#25915;&#20987;&#32773;&#20960;&#20309;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#21516;&#26102;&#25429;&#25417;&#21040;&#23545;&#25239;&#40065;&#26834;&#24615;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#26680;&#24515;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#36129;&#29486;&#26159;&#22312;&#36890;&#29992;&#30340;&#20984;&#19988;&#38750;&#36882;&#22686;&#25439;&#22833;&#20989;&#25968;&#19979;&#65292;&#23545;&#20110;&#23545;&#25239;&#24615;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#22120;&#30340;&#20805;&#20998;&#32479;&#35745;&#30340;&#31934;&#30830;&#28176;&#36817;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20351;&#25105;&#20204;&#33021;&#22815;&#31934;&#30830;&#22320;&#21051;&#30011;&#25968;&#25454;&#20013;&#19982;&#26356;&#39640;&#30340;&#27867;&#21270;/&#40065;&#26834;&#24615;&#26435;&#34913;&#30456;&#20851;&#30340;&#26041;&#21521;&#65292;&#30001;&#19968;&#20010;&#40065;&#26834;&#24615;&#24230;&#37327;&#21644;&#19968;&#20010;&#26377;&#29992;&#24615;&#24230;&#37327;&#23450;&#20041;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#23384;&#22312;&#19968;&#20123;&#26041;&#21521;&#65292;&#21487;&#20197;&#36827;&#34892;&#38450;&#24481;&#32780;&#19981;&#24809;&#32602;&#20934;&#30830;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38450;&#24481;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates adversarial training in the context of margin-based linear classifiers in the high-dimensional regime where the dimension $d$ and the number of data points $n$ diverge with a fixed ratio $\alpha = n / d$. We introduce a tractable mathematical model where the interplay between the data and adversarial attacker geometries can be studied, while capturing the core phenomenology observed in the adversarial robustness literature. Our main theoretical contribution is an exact asymptotic description of the sufficient statistics for the adversarial empirical risk minimiser, under generic convex and non-increasing losses. Our result allow us to precisely characterise which directions in the data are associated with a higher generalisation/robustness trade-off, as defined by a robustness and a usefulness metric. In particular, we unveil the existence of directions which can be defended without penalising accuracy. Finally, we show the advantage of defending non-robust featu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#32467;&#26500;&#65292;&#26412;&#35770;&#25991;&#20197;&#30495;&#23454;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#20026;&#22522;&#30784;&#65292;&#25171;&#30772;&#20102;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#32473;&#20986;&#20102;&#26080;&#32500;&#24230;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.05576</link><description>&lt;p&gt;
&#25968;&#23383;&#35745;&#31639;&#26426;&#25171;&#30772;&#32500;&#24230;&#35781;&#21650;&#65306;&#36890;&#36807;&#26377;&#38480;&#20960;&#20309;&#30340;&#33258;&#36866;&#24212;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Digital Computers Break the Curse of Dimensionality: Adaptive Bounds via Finite Geometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#31163;&#25955;&#32467;&#26500;&#65292;&#26412;&#35770;&#25991;&#20197;&#30495;&#23454;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#20026;&#22522;&#30784;&#65292;&#25171;&#30772;&#20102;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#65292;&#24182;&#32473;&#20986;&#20102;&#26080;&#32500;&#24230;&#29575;&#30340;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#26159;&#24314;&#31435;&#22312;&#29702;&#24819;&#24773;&#20917;&#19979;&#30340;&#21069;&#25552;&#19979;&#65292;&#21363;&#25152;&#26377;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#31354;&#38388;&#37117;&#26159;&#26080;&#31351;&#30340;&#65292;&#20363;&#22914;$\mathbb{R}^d$&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26377;&#38480;&#30340;&#26426;&#22120;&#31934;&#24230;&#12289;&#33293;&#20837;&#21644;&#26377;&#38480;&#30340;&#23384;&#20648;&#31354;&#38388;&#31561;&#25968;&#23383;&#35745;&#31639;&#26426;&#30340;&#38480;&#21046;&#65292;&#23454;&#38469;&#24773;&#20917;&#19979;&#36825;&#20010;&#26680;&#24515;&#20551;&#35774;&#24448;&#24448;&#34987;&#36829;&#32972;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;&#25968;&#23383;&#35745;&#31639;&#26426;&#22312;$\mathbb{R}^d$&#19978;&#25805;&#20316;&#30340;&#26159;&#26377;&#38480;&#30340;&#32593;&#26684;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#20123;&#31163;&#25955;&#32467;&#26500;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23454;&#38469;&#35745;&#31639;&#26426;&#19978;&#23454;&#29616;&#27169;&#22411;&#26102;&#65292;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#34987;&#31995;&#32479;&#22320;&#25171;&#30772;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38024;&#23545;&#22312;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#19978;&#23454;&#29616;&#30340;&#26680;&#20989;&#25968;&#21644;&#28145;&#24230;ReLU MLP&#22238;&#24402;&#22120;&#33719;&#24471;&#20102;&#26032;&#30340;&#26080;&#32500;&#24230;&#29575;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#24212;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#28176;&#36827;&#27979;&#24230;&#38598;&#20013;&#24615;&#32467;&#26524;&#65292;&#35813;&#32467;&#26524;&#32473;&#20986;&#20102;&#27010;&#29575;&#27979;&#24230;&#21644;&#20854;&#22312;$N$&#20010;&#29420;&#31435;&#21516;&#20998;&#24067;&#26679;&#26412;&#19978;&#30340;&#32463;&#39564;&#29256;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#20026;$1$-Wasserstein&#36317;&#31163;&#30340;&#38598;&#20013;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many of the foundations of machine learning rely on the idealized premise that all input and output spaces are infinite, e.g.~$\mathbb{R}^d$. This core assumption is systematically violated in practice due to digital computing limitations from finite machine precision, rounding, and limited RAM. In short, digital computers operate on finite grids in $\mathbb{R}^d$. By exploiting these discrete structures, we show the curse of dimensionality in statistical learning is systematically broken when models are implemented on real computers. Consequentially, we obtain new generalization bounds with dimension-free rates for kernel and deep ReLU MLP regressors, which are implemented on real-world machines.   Our results are derived using a new non-asymptotic concentration of measure result between a probability measure over any finite metric space and its empirical version associated with $N$ i.i.d. samples when measured in the $1$-Wasserstein distance. Unlike standard concentration of measure 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23398;&#20064;&#31639;&#23376;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#30446;&#26631;&#31639;&#23376;&#30340;&#35268;&#21017;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19978;&#30028;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#23545;&#20110;&#38750;&#32447;&#24615;&#31639;&#23376;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21450;&#32447;&#24615;&#36817;&#20284;&#25910;&#25947;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04691</link><description>&lt;p&gt;
&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Learning Operators with Stochastic Gradient Descent in General Hilbert Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#23398;&#20064;&#31639;&#23376;&#65292;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#30446;&#26631;&#31639;&#23376;&#30340;&#35268;&#21017;&#26465;&#20214;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#19978;&#30028;&#65292;&#21516;&#26102;&#23637;&#31034;&#20102;&#23545;&#20110;&#38750;&#32447;&#24615;&#31639;&#23376;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21450;&#32447;&#24615;&#36817;&#20284;&#25910;&#25947;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#22312;&#19968;&#33324;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#23398;&#20064;&#31639;&#23376;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#30446;&#26631;&#31639;&#23376;&#30340;&#24369;&#21644;&#24378;&#35268;&#21017;&#26465;&#20214;&#65292;&#20197;&#25551;&#36848;&#20854;&#20869;&#22312;&#32467;&#26500;&#21644;&#22797;&#26434;&#24615;&#12290;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;SGD&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#30340;&#19978;&#30028;&#65292;&#24182;&#36827;&#34892;&#20102;&#26497;&#23567;&#20540;&#19979;&#30028;&#20998;&#26512;&#65292;&#36827;&#19968;&#27493;&#35828;&#26126;&#25105;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#21644;&#35268;&#21017;&#26465;&#20214;&#23450;&#37327;&#22320;&#21051;&#30011;&#20102;&#20351;&#29992;SGD&#31639;&#27861;&#35299;&#20915;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#30340;&#21487;&#34892;&#24615;&#12290;&#20540;&#24471;&#24378;&#35843;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#20998;&#26512;&#23545;&#20110;&#38750;&#32447;&#24615;&#31639;&#23376;&#23398;&#20064;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;SGD&#20272;&#35745;&#22120;&#23558;&#25910;&#25947;&#20110;&#38750;&#32447;&#24615;&#30446;&#26631;&#31639;&#23376;&#30340;&#26368;&#20339;&#32447;&#24615;&#36817;&#20284;&#12290;&#27492;&#22806;&#65292;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#24212;&#29992;&#20110;&#22522;&#20110;&#30690;&#37327;&#20540;&#21644;&#23454;&#20540;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#31639;&#23376;&#23398;&#20064;&#38382;&#39064;&#65292;&#20135;&#29983;&#20102;&#26032;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#20174;&#32780;&#23436;&#21892;&#20102;&#29616;&#26377;&#25991;&#29486;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates leveraging stochastic gradient descent (SGD) to learn operators between general Hilbert spaces. We propose weak and strong regularity conditions for the target operator to depict its intrinsic structure and complexity. Under these conditions, we establish upper bounds for convergence rates of the SGD algorithm and conduct a minimax lower bound analysis, further illustrating that our convergence analysis and regularity conditions quantitatively characterize the tractability of solving operator learning problems using the SGD algorithm. It is crucial to highlight that our convergence analysis is still valid for nonlinear operator learning. We show that the SGD estimator will converge to the best linear approximation of the nonlinear target operator. Moreover, applying our analysis to operator learning problems based on vector-valued and real-valued reproducing kernel Hilbert spaces yields new convergence results, thereby refining the conclusions of existing litera
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10690</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#21644;MAE&#65306;&#24341;&#20837;EAUC&#26469;&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#23545;&#23454;&#20307;&#30340;&#23454;&#20540;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#22522;&#30784;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39044;&#27979;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#35780;&#20998;&#65289;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#20063;&#26377;&#35768;&#22810;&#28508;&#21147;&#20294;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#33647;&#29702;&#23398;&#20013;&#36817;&#20284;&#30830;&#23450;&#24739;&#32773;&#30340;&#36866;&#24403;&#21058;&#37327;&#65289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20010;&#20307;&#23454;&#20307;&#35266;&#23519;&#20540;&#20998;&#24067;&#30340;&#38750;&#22343;&#21248;&#24615;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#20013;&#30340;&#20005;&#37325;&#20559;&#35265;&#39044;&#27979;&#65292;&#20559;&#21521;&#20110;&#23454;&#20307;&#30340;&#35266;&#23519;&#36807;&#21435;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#21478;&#31867;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27604;&#38543;&#26426;&#39044;&#27979;&#26356;&#24046;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#21478;&#31867;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#21478;&#31867;-&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;EAUC&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#34917;&#20805;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#37327;&#21270;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#30740;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#12289;&#36130;&#21153;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#22312;LLM&#30340;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#37117;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#32454;&#33268;&#30340;&#36164;&#28304;&#25928;&#29575;&#25216;&#26415;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.00625</link><description>&lt;p&gt;
&#36229;&#36234;&#25928;&#29575;&#65306;&#36164;&#28304;&#39640;&#25928;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Beyond Efficiency: A Systematic Survey of Resource-Efficient Large Language Models. (arXiv:2401.00625v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#30740;&#31995;&#32479;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36164;&#28304;&#25928;&#29575;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#20248;&#21270;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#12289;&#36130;&#21153;&#21644;&#32593;&#32476;&#36164;&#28304;&#30340;&#20248;&#21270;&#65292;&#22312;LLM&#30340;&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#37117;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#32454;&#33268;&#30340;&#36164;&#28304;&#25928;&#29575;&#25216;&#26415;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39046;&#22495;&#30340;&#36805;&#29467;&#21457;&#23637;&#20195;&#34920;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#19968;&#20010;&#37325;&#35201;&#36827;&#27493;&#65292;&#36825;&#20123;&#27169;&#22411;&#65292;&#22914;OpenAI&#30340;ChatGPT&#65292;&#24102;&#26469;&#20102;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#21644;&#36130;&#21153;&#36164;&#28304;&#39640;&#28040;&#32791;&#31561;&#37325;&#22823;&#25361;&#25112;&#65292;&#23588;&#20854;&#26159;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#12290;&#26412;&#35843;&#30740;&#26088;&#22312;&#31995;&#32479;&#22320;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#23457;&#26597;&#20102;&#19968;&#31995;&#21015;&#26088;&#22312;&#22686;&#24378;LLMs&#36164;&#28304;&#25928;&#29575;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#26681;&#25454;&#20248;&#21270;&#37325;&#28857;&#23545;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#65306;&#35745;&#31639;&#12289;&#20869;&#23384;&#12289;&#33021;&#37327;&#12289;&#36130;&#21153;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;LLM&#29983;&#21629;&#21608;&#26399;&#30340;&#21508;&#20010;&#38454;&#27573;&#65288;&#21253;&#25324;&#26550;&#26500;&#35774;&#35745;&#12289;&#39044;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#31995;&#32479;&#35774;&#35745;&#65289;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#35813;&#35843;&#30740;&#36890;&#36807;&#29305;&#23450;&#36164;&#28304;&#31867;&#22411;&#24341;&#20837;&#20102;&#32454;&#33268;&#30340;&#36164;&#28304;&#25928;&#29575;&#25216;&#26415;&#20998;&#31867;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;&#36164;&#28304;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#21644;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
The burgeoning field of Large Language Models (LLMs), exemplified by sophisticated models like OpenAI's ChatGPT, represents a significant advancement in artificial intelligence. These models, however, bring forth substantial challenges in the high consumption of computational, memory, energy, and financial resources, especially in environments with limited resource capabilities. This survey aims to systematically address these challenges by reviewing a broad spectrum of techniques designed to enhance the resource efficiency of LLMs. We categorize methods based on their optimization focus: computational, memory, energy, financial, and network resources and their applicability across various stages of an LLM's lifecycle, including architecture design, pretraining, finetuning, and system design. Additionally, the survey introduces a nuanced categorization of resource efficiency techniques by their specific resource types, which uncovers the intricate relationships and mappings between var
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;DKFs&#65289;&#65292;&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#19968;&#31867;&#38750;&#39532;&#23572;&#21487;&#22827;&#21644;&#26465;&#20214;&#39640;&#26031;&#20449;&#21495;&#36807;&#31243;&#30340;&#26465;&#20214;&#20998;&#24067;&#24459;&#65292;&#20174;&#32780;&#20855;&#26377;&#22312;&#25968;&#23398;&#37329;&#34701;&#39046;&#22495;&#20013;&#20256;&#32479;&#27169;&#22411;&#22522;&#30784;&#19978;&#30340;&#28388;&#27874;&#38382;&#39064;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19603</link><description>&lt;p&gt;
&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21487;&#20197;&#36827;&#34892;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Deep Kalman Filters Can Filter. (arXiv:2310.19603v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;DKFs&#65289;&#65292;&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#19968;&#31867;&#38750;&#39532;&#23572;&#21487;&#22827;&#21644;&#26465;&#20214;&#39640;&#26031;&#20449;&#21495;&#36807;&#31243;&#30340;&#26465;&#20214;&#20998;&#24067;&#24459;&#65292;&#20174;&#32780;&#20855;&#26377;&#22312;&#25968;&#23398;&#37329;&#34701;&#39046;&#22495;&#20013;&#20256;&#32479;&#27169;&#22411;&#22522;&#30784;&#19978;&#30340;&#28388;&#27874;&#38382;&#39064;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;DKFs&#65289;&#26159;&#19968;&#31867;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#39640;&#26031;&#27010;&#29575;&#27979;&#24230;&#12290;&#34429;&#28982;DKFs&#21463;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#21551;&#21457;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#19982;&#38543;&#26426;&#28388;&#27874;&#38382;&#39064;&#30340;&#20855;&#20307;&#29702;&#35770;&#20851;&#32852;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20256;&#32479;&#27169;&#22411;&#22522;&#30784;&#19978;&#30340;&#28388;&#27874;&#38382;&#39064;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#20538;&#21048;&#21644;&#26399;&#26435;&#23450;&#20215;&#27169;&#22411;&#26657;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;DKFs&#65292;&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#19968;&#31867;&#38750;&#39532;&#23572;&#21487;&#22827;&#21644;&#26465;&#20214;&#39640;&#26031;&#20449;&#21495;&#36807;&#31243;&#30340;&#26465;&#20214;&#20998;&#24067;&#24459;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#25968;&#23398;&#22522;&#30784;&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36817;&#20284;&#32467;&#26524;&#22312;&#36335;&#24452;&#30340;&#36275;&#22815;&#35268;&#21017;&#30340;&#32039;&#33268;&#23376;&#38598;&#19978;&#19968;&#33268;&#25104;&#31435;&#65292;&#20854;&#20013;&#36817;&#20284;&#35823;&#24046;&#30001;&#22312;&#32473;&#23450;&#32039;&#33268;&#36335;&#24452;&#38598;&#19978;&#22343;&#19968;&#22320;&#35745;&#31639;&#30340;&#26368;&#22351;&#24773;&#20917;2-Wasserstein&#36317;&#31163;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Kalman filters (DKFs) are a class of neural network models that generate Gaussian probability measures from sequential data. Though DKFs are inspired by the Kalman filter, they lack concrete theoretical ties to the stochastic filtering problem, thus limiting their applicability to areas where traditional model-based filters have been used, e.g.\ model calibration for bond and option prices in mathematical finance. We address this issue in the mathematical foundations of deep learning by exhibiting a class of continuous-time DKFs which can approximately implement the conditional law of a broad class of non-Markovian and conditionally Gaussian signal processes given noisy continuous-times measurements. Our approximation results hold uniformly over sufficiently regular compact subsets of paths, where the approximation error is quantified by the worst-case 2-Wasserstein distance computed uniformly over the given compact set of paths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.12595</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Causal Similarity-Based Hierarchical Bayesian Models. (arXiv:2310.12595v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#23545;&#26032;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23545;&#30001;&#30456;&#20851;&#20219;&#21153;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27867;&#21270;&#30340;&#38382;&#39064;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#33021;&#22312;&#22240;&#26524;&#26426;&#21046;&#19978;&#23384;&#22312;&#24046;&#24322;&#12290;&#20363;&#22914;&#65292;&#22797;&#26434;&#30142;&#30149;&#30340;&#35266;&#23519;&#24615;&#21307;&#23398;&#25968;&#25454;&#22312;&#19981;&#21516;&#24739;&#32773;&#38388;&#20855;&#26377;&#30142;&#30149;&#22240;&#26524;&#26426;&#21046;&#30340;&#24322;&#36136;&#24615;&#65292;&#36825;&#32473;&#38656;&#35201;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#20043;&#22806;&#30340;&#26032;&#24739;&#32773;&#36827;&#34892;&#27867;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#24120;&#29992;&#30340;&#22788;&#29702;&#24322;&#36136;&#24615;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#21253;&#25324;&#20026;&#25972;&#20010;&#25968;&#25454;&#38598;&#23398;&#20064;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#20026;&#27599;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#23398;&#20064;&#26412;&#22320;&#27169;&#22411;&#65292;&#25110;&#32773;&#21033;&#29992;&#20998;&#23618;&#12289;&#20803;&#23398;&#20064;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#20174;&#27719;&#38598;&#30340;&#22810;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#27867;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#22240;&#26524;&#30456;&#20284;&#24615;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#20174;&#20855;&#26377;&#30456;&#20284;&#22240;&#26524;&#26426;&#21046;&#30340;&#35757;&#32451;&#20219;&#21153;&#20013;&#27719;&#38598;&#25968;&#25454;&#26469;&#25552;&#39640;&#23545;&#26032;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#24212;&#29992;&#36825;&#31181;&#36890;&#29992;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
The key challenge underlying machine learning is generalisation to new data. This work studies generalisation for datasets consisting of related tasks that may differ in causal mechanisms. For example, observational medical data for complex diseases suffers from heterogeneity in causal mechanisms of disease across patients, creating challenges for machine learning algorithms that need to generalise to new patients outside of the training dataset. Common approaches for learning supervised models with heterogeneous datasets include learning a global model for the entire dataset, learning local models for each tasks' data, or utilising hierarchical, meta-learning and multi-task learning approaches to learn how to generalise from data pooled across multiple tasks. In this paper we propose causal similarity-based hierarchical Bayesian models to improve generalisation to new tasks by learning how to pool data from training tasks with similar causal mechanisms. We apply this general modelling
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#36890;&#36807;&#36127;&#37319;&#26679;&#35825;&#23548;&#20102;&#27491;&#36793;&#21644;&#36127;&#36793;&#30340;&#27491;&#21521;&#20256;&#36882;&#65292;&#20197;&#26356;&#21152;&#28789;&#27963;&#32780;&#31283;&#23450;&#22320;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.09516</link><description>&lt;p&gt;
&#36890;&#36807;&#36127;&#37319;&#26679;&#35825;&#23548;&#30340;GNN&#23618;&#36827;&#34892;&#39640;&#25928;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Efficient Link Prediction via GNN Layers Induced by Negative Sampling. (arXiv:2310.09516v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09516
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#36890;&#36807;&#36127;&#37319;&#26679;&#35825;&#23548;&#20102;&#27491;&#36793;&#21644;&#36127;&#36793;&#30340;&#27491;&#21521;&#20256;&#36882;&#65292;&#20197;&#26356;&#21152;&#28789;&#27963;&#32780;&#31283;&#23450;&#22320;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#21487;&#20197;&#22823;&#33268;&#20998;&#20026;&#20004;&#22823;&#31867;&#12290;&#31532;&#19968;&#31867;&#26159;&#22522;&#20110;&#33410;&#28857;&#30340;&#32467;&#26500;&#65292;&#20026;&#27599;&#20010;&#33410;&#28857;&#39044;&#20808;&#35745;&#31639;&#20010;&#20307;&#23884;&#20837;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;&#35299;&#30721;&#22120;&#36827;&#34892;&#32452;&#21512;&#20197;&#36827;&#34892;&#39044;&#27979;&#12290;&#23613;&#31649;&#22312;&#25512;&#29702;&#26102;&#38750;&#24120;&#39640;&#25928;&#65288;&#22240;&#20026;&#33410;&#28857;&#23884;&#20837;&#21482;&#35745;&#31639;&#19968;&#27425;&#24182;&#21453;&#22797;&#37325;&#29992;&#65289;&#65292;&#20294;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#65292;&#23548;&#33268;&#26080;&#27861;&#21306;&#20998;&#23545;&#20505;&#36873;&#36793;&#26377;&#36129;&#29486;&#30340;&#21516;&#26500;&#33410;&#28857;&#65292;&#20174;&#32780;&#24433;&#21709;&#20934;&#30830;&#24615;&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#31532;&#20108;&#31867;&#26041;&#27861;&#21017;&#20381;&#36182;&#20110;&#24418;&#25104;&#38024;&#23545;&#27599;&#20010;&#36793;&#30340;&#23376;&#22270;&#23884;&#20837;&#65292;&#20197;&#20016;&#23500;&#20004;&#20004;&#20851;&#31995;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#28040;&#38500;&#21516;&#26500;&#33410;&#28857;&#65292;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#20195;&#20215;&#26159;&#22686;&#21152;&#20102;&#27169;&#22411;&#22797;&#26434;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#26435;&#34913;&#36825;&#20010;&#21462;&#33293;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;GNN&#26550;&#26500;&#65292;&#20854;&#20013;&#30340;&#27491;&#21521;&#20256;&#36882;&#26126;&#30830;&#20381;&#36182;&#20110;&#27491;&#36793;&#65288;&#36890;&#24120;&#24773;&#20917;&#19979;&#65289;&#21644;&#36127;&#36793;&#65288;&#25105;&#20204;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#65289;&#65292;&#20197;&#25552;&#20379;&#26356;&#28789;&#27963;&#20294;&#20173;&#31283;&#23450;&#30340;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) for link prediction can loosely be divided into two broad categories. First, \emph{node-wise} architectures pre-compute individual embeddings for each node that are later combined by a simple decoder to make predictions. While extremely efficient at inference time (since node embeddings are only computed once and repeatedly reused), model expressiveness is limited such that isomorphic nodes contributing to candidate edges may not be distinguishable, compromising accuracy. In contrast, \emph{edge-wise} methods rely on the formation of edge-specific subgraph embeddings to enrich the representation of pair-wise relationships, disambiguating isomorphic nodes to improve accuracy, but with the cost of increased model complexity. To better navigate this trade-off, we propose a novel GNN architecture whereby the \emph{forward pass} explicitly depends on \emph{both} positive (as is typical) and negative (unique to our approach) edges to inform more flexible, yet sti
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#65288;MEDL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#35299;&#20915;&#25968;&#25454;&#38598;&#31751;&#38388;&#20851;&#32852;&#21644;&#19981;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65292;&#26469;&#25552;&#39640;&#23545;&#31751;&#20998;&#24067;&#25968;&#25454;&#30340;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03146</link><description>&lt;p&gt;
&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#22312;&#31751;&#65288;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#65289;&#25968;&#25454;&#19978;&#25913;&#21892;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Fairness-enhancing mixed effects deep learning improves fairness on in- and out-of-distribution clustered (non-iid) data. (arXiv:2310.03146v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03146
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#20844;&#24179;&#24615;&#30340;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#65288;MEDL&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#21516;&#26102;&#35299;&#20915;&#25968;&#25454;&#38598;&#31751;&#38388;&#20851;&#32852;&#21644;&#19981;&#20844;&#24179;&#24615;&#30340;&#38382;&#39064;&#65292;&#26469;&#25552;&#39640;&#23545;&#31751;&#20998;&#24067;&#25968;&#25454;&#30340;&#20844;&#24179;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#22312;&#20004;&#20010;&#26680;&#24515;&#38382;&#39064;&#19978;&#23384;&#22312;&#22256;&#25200;&#12290;&#39318;&#20808;&#65292;&#23427;&#20551;&#35774;&#35757;&#32451;&#26679;&#26412;&#26159;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#65292;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#23558;&#26679;&#26412;&#25353;&#20849;&#20139;&#30340;&#27979;&#37327;&#20540;&#36827;&#34892;&#20998;&#32452;&#65288;&#20363;&#22914;&#65292;&#30740;&#31350;&#21442;&#19982;&#32773;&#25110;&#32454;&#32990;&#65289;&#65292;&#36829;&#21453;&#20102;&#36825;&#19968;&#20551;&#35774;&#12290;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#21487;&#33021;&#26174;&#31034;&#20986;&#24615;&#33021;&#19979;&#38477;&#12289;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#21644;&#35299;&#37322;&#24615;&#38382;&#39064;&#65292;&#24182;&#20276;&#38543;&#30528;&#31751;&#28151;&#28102;&#24341;&#36215;&#30340;&#31532;&#19968;&#22411;&#21644;&#31532;&#20108;&#22411;&#38169;&#35823;&#12290;&#20854;&#27425;&#65292;&#27169;&#22411;&#36890;&#24120;&#34987;&#35757;&#32451;&#20197;&#23454;&#29616;&#25972;&#20307;&#20934;&#30830;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#34987;&#20302;&#20272;&#30340;&#32676;&#20307;&#65292;&#22312;&#36151;&#27454;&#25209;&#20934;&#25110;&#30830;&#23450;&#20581;&#24247;&#20445;&#38505;&#36153;&#29575;&#31561;&#20851;&#38190;&#39046;&#22495;&#24341;&#20837;&#20559;&#35265;&#65292;&#36825;&#20123;&#20559;&#35265;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#20010;&#20154;&#30340;&#29983;&#27963;&#36136;&#37327;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#25928;&#24212;&#28145;&#24230;&#23398;&#20064;&#65288;MEDL&#65289;&#26694;&#26550;&#12290;MEDL&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#20869;&#23481;&#20998;&#21035;&#37327;&#21270;&#31751;&#19981;&#21464;&#30340;&#22266;&#23450;&#25928;&#24212;&#21644;&#31751;&#29305;&#23450;&#30340;&#38543;&#26426;&#25928;&#24212;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65306;1&#65289;&#19968;&#20010;&#31751;&#23545;&#25163;&#65292;&#40723;&#21169;&#31751;&#38388;&#24046;&#24322;&#30340;&#26368;&#23567;&#21270;&#65307;
&lt;/p&gt;
&lt;p&gt;
Traditional deep learning (DL) suffers from two core problems. Firstly, it assumes training samples are independent and identically distributed. However, numerous real-world datasets group samples by shared measurements (e.g., study participants or cells), violating this assumption. In these scenarios, DL can show compromised performance, limited generalization, and interpretability issues, coupled with cluster confounding causing Type 1 and 2 errors. Secondly, models are typically trained for overall accuracy, often neglecting underrepresented groups and introducing biases in crucial areas like loan approvals or determining health insurance rates, such biases can significantly impact one's quality of life. To address both of these challenges simultaneously, we present a mixed effects deep learning (MEDL) framework. MEDL separately quantifies cluster-invariant fixed effects (FE) and cluster-specific random effects (RE) through the introduction of: 1) a cluster adversary which encourage
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#35268;&#33539;&#22240;&#32032;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22240;&#23376;&#29305;&#24449;&#23481;&#37327;&#34429;&#28982;&#31616;&#21333;&#39640;&#25928;&#65292;&#20294;&#23384;&#22312;&#19981;&#33391;&#20559;&#24046;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#35268;&#33539;&#21270;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#28040;&#38500;&#20559;&#24046;&#65292;&#24182;&#22312;&#22270;&#20687;&#12289;&#36317;&#31163;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36136;&#37327;&#12289;&#40065;&#26834;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2308.15461</link><description>&lt;p&gt;
&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#35268;&#33539;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Canonical Factors for Hybrid Neural Fields. (arXiv:2308.15461v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15461
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#28151;&#21512;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#30340;&#35268;&#33539;&#22240;&#32032;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#22240;&#23376;&#29305;&#24449;&#23481;&#37327;&#34429;&#28982;&#31616;&#21333;&#39640;&#25928;&#65292;&#20294;&#23384;&#22312;&#19981;&#33391;&#20559;&#24046;&#12290;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#35268;&#33539;&#21270;&#21464;&#25442;&#65292;&#35813;&#26041;&#27861;&#25104;&#21151;&#28040;&#38500;&#20559;&#24046;&#65292;&#24182;&#22312;&#22270;&#20687;&#12289;&#36317;&#31163;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36136;&#37327;&#12289;&#40065;&#26834;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#29305;&#24449;&#23481;&#37327;&#25552;&#20379;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#26356;&#32039;&#20945;&#12289;&#39640;&#25928;&#21644;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#65292;&#20294;&#20063;&#24341;&#20837;&#20102;&#19981;&#19968;&#23450;&#26377;&#30410;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#20559;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#65288;1&#65289;&#23545;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#23545;&#40784;&#36724;&#20449;&#21495;&#30340;&#19981;&#33391;&#20559;&#24046;&#36827;&#34892;&#20102;&#34920;&#24449; - &#23427;&#20204;&#21487;&#20197;&#23548;&#33268;&#36752;&#23556;&#22330;&#37325;&#24314;&#30340;&#24046;&#24322;&#39640;&#36798;2 PSNR - &#24182;&#65288;2&#65289;&#25506;&#32034;&#20102;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#35268;&#33539;&#21270;&#21464;&#25442;&#26469;&#25552;&#39640;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#28040;&#38500;&#36825;&#20123;&#20559;&#24046;&#12290;&#22312;&#19968;&#20010;&#20108;&#32500;&#27169;&#22411;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21516;&#26102;&#23398;&#20064;&#36825;&#20123;&#21464;&#25442;&#20197;&#21450;&#22330;&#26223;&#22806;&#35266;&#21487;&#20197;&#20197;&#22823;&#22823;&#25552;&#39640;&#30340;&#25928;&#29575;&#25104;&#21151;&#12290;&#25105;&#20204;&#20351;&#29992;&#22270;&#20687;&#12289;&#26377;&#31526;&#21495;&#36317;&#31163;&#21644;&#36752;&#23556;&#22330;&#37325;&#24314;&#20219;&#21153;&#39564;&#35777;&#20102;&#25152;&#24471;&#21040;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36136;&#37327;&#12289;&#40065;&#26834;&#24615;&#12289;&#32039;&#20945;&#24615;&#21644;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;TILTED&#21487;&#20197;&#23454;&#29616;&#19982;&#22522;&#32447;&#30456;&#24403;&#30340;&#33021;&#21147;&#65292;&#32780;&#22522;&#32447;&#26159;2&#20493;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factored feature volumes offer a simple way to build more compact, efficient, and intepretable neural fields, but also introduce biases that are not necessarily beneficial for real-world data. In this work, we (1) characterize the undesirable biases that these architectures have for axis-aligned signals -- they can lead to radiance field reconstruction differences of as high as 2 PSNR -- and (2) explore how learning a set of canonicalizing transformations can improve representations by removing these biases. We prove in a two-dimensional model problem that simultaneously learning these transformations together with scene appearance succeeds with drastically improved efficiency. We validate the resulting architectures, which we call TILTED, using image, signed distance, and radiance field reconstruction tasks, where we observe improvements across quality, robustness, compactness, and runtime. Results demonstrate that TILTED can enable capabilities comparable to baselines that are 2x lar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#24515;&#34880;&#31649;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#20316;&#20026;&#32479;&#35745;&#25512;&#29702;&#36827;&#34892;&#35299;&#20915;&#65292;&#22312;&#20307;&#22806;&#36827;&#34892;&#20102;&#20116;&#20010;&#29983;&#29289;&#26631;&#35760;&#29289;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#27169;&#25311;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.13918</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#29702;&#29992;&#20110;&#24515;&#34880;&#31649;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Simulation-based Inference for Cardiovascular Models. (arXiv:2307.13918v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#24515;&#34880;&#31649;&#27169;&#22411;&#30340;&#36870;&#38382;&#39064;&#20316;&#20026;&#32479;&#35745;&#25512;&#29702;&#36827;&#34892;&#35299;&#20915;&#65292;&#22312;&#20307;&#22806;&#36827;&#34892;&#20102;&#20116;&#20010;&#29983;&#29289;&#26631;&#35760;&#29289;&#30340;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#27169;&#25311;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#21313;&#24180;&#20013;&#65292;&#34880;&#27969;&#21160;&#21147;&#23398;&#27169;&#25311;&#22120;&#19981;&#26029;&#21457;&#23637;&#65292;&#24050;&#25104;&#20026;&#30740;&#31350;&#20307;&#22806;&#24515;&#34880;&#31649;&#31995;&#32479;&#30340;&#39318;&#36873;&#24037;&#20855;&#12290;&#34429;&#28982;&#36825;&#26679;&#30340;&#24037;&#20855;&#36890;&#24120;&#29992;&#20110;&#20174;&#29983;&#29702;&#21442;&#25968;&#27169;&#25311;&#20840;&#36523;&#34880;&#27969;&#21160;&#21147;&#23398;&#65292;&#20294;&#35299;&#20915;&#23558;&#27874;&#24418;&#26144;&#23556;&#22238;&#21512;&#29702;&#30340;&#29983;&#29702;&#21442;&#25968;&#30340;&#36870;&#38382;&#39064;&#20173;&#28982;&#26377;&#24456;&#22823;&#30340;&#28508;&#21147;&#21644;&#25361;&#25112;&#12290;&#21463;&#27169;&#25311;&#25512;&#29702;&#65288;SBI&#65289;&#30340;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#36870;&#38382;&#39064;&#20316;&#20026;&#32479;&#35745;&#25512;&#29702;&#26469;&#22788;&#29702;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;SBI&#20026;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#25552;&#20379;&#20102;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20010;&#20307;&#27979;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#22810;&#32500;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#27604;&#20960;&#31181;&#27979;&#37327;&#27169;&#24577;&#26469;&#23637;&#31034;&#36825;&#31181;&#33021;&#21147;&#65292;&#36827;&#34892;&#20102;&#20116;&#20010;&#20020;&#24202;&#24863;&#20852;&#36259;&#30340;&#29983;&#29289;&#26631;&#24535;&#29289;&#30340;&#20307;&#22806;&#19981;&#30830;&#23450;&#24615;&#20998;&#26512;&#12290;&#38500;&#20102;&#30830;&#35748;&#24050;&#30693;&#20107;&#23454;&#65292;&#27604;&#22914;&#20272;&#35745;&#24515;&#29575;&#30340;&#21487;&#34892;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#31361;&#20986;&#20102;&#8230;
&lt;/p&gt;
&lt;p&gt;
Over the past decades, hemodynamics simulators have steadily evolved and have become tools of choice for studying cardiovascular systems in-silico. While such tools are routinely used to simulate whole-body hemodynamics from physiological parameters, solving the corresponding inverse problem of mapping waveforms back to plausible physiological parameters remains both promising and challenging. Motivated by advances in simulation-based inference (SBI), we cast this inverse problem as statistical inference. In contrast to alternative approaches, SBI provides \textit{posterior distributions} for the parameters of interest, providing a \textit{multi-dimensional} representation of uncertainty for \textit{individual} measurements. We showcase this ability by performing an in-silico uncertainty analysis of five biomarkers of clinical interest comparing several measurement modalities. Beyond the corroboration of known facts, such as the feasibility of estimating heart rate, our study highlight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2307.10246</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#65306;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#65288;&#32508;&#36848;&#65289;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey). (arXiv:2307.10246v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#33041;&#23545;&#40784;&#30340;&#30740;&#31350;&#65292;&#37325;&#28857;&#22312;&#20110;&#33041;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20123;&#27169;&#22411;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#22914;&#20309;&#34920;&#31034;&#19981;&#21516;&#30340;&#20449;&#24687;&#27169;&#24335;&#65311;&#25105;&#20204;&#33021;&#21542;&#35774;&#35745;&#20986;&#19968;&#20010;&#21487;&#20197;&#33258;&#21160;&#29702;&#35299;&#29992;&#25143;&#24605;&#32771;&#20869;&#23481;&#30340;&#31995;&#32479;&#65311;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#30740;&#31350;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#31561;&#22823;&#33041;&#35760;&#24405;&#26469;&#22238;&#31572;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#65292;&#31070;&#32463;&#31185;&#23398;&#30028;&#20026;&#34987;&#21160;&#38405;&#35835;/&#21548;&#35273;/&#35266;&#30475;&#27010;&#24565;&#35789;&#27719;&#12289;&#21465;&#36848;&#12289;&#22270;&#29255;&#21644;&#30005;&#24433;&#30456;&#20851;&#30340;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;&#36807;&#21435;&#20108;&#21313;&#24180;&#20013;&#65292;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#32534;&#30721;&#21644;&#35299;&#30721;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#22522;&#30784;&#30740;&#31350;&#20013;&#30340;&#39069;&#22806;&#24037;&#20855;&#65292;&#22312;&#35748;&#30693;&#31185;&#23398;&#21644;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#32534;&#30721;&#27169;&#22411;&#26088;&#22312;&#33258;&#21160;&#22320;&#29983;&#25104;fMRI&#22823;&#33041;&#34920;&#24449;&#65292;&#32473;&#23450;&#19968;&#20010;&#21050;&#28608;&#12290;&#23427;&#20204;&#22312;&#35780;&#20272;&#21644;&#35786;&#26029;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#20197;&#21450;&#35774;&#35745;&#22823;&#33041;&#25439;&#20260;&#27835;&#30103;&#26041;&#27861;&#26041;&#38754;&#26377;&#30528;&#22810;&#31181;&#23454;&#38469;&#24212;&#29992;&#12290;&#35299;&#30721;&#27169;&#22411;&#35299;&#20915;&#20102;&#26681;&#25454;fMRI&#37325;&#26500;&#21050;&#28608;&#30340;&#36870;&#38382;&#39064;&#12290;&#23427;&#20204;&#23545;&#20110;&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#22788;&#29702;&#20449;&#24687;&#20197;&#21450;&#35774;&#35745;&#33041;&#26426;&#25509;&#21475;&#30340;&#21457;&#23637;&#37117;&#26377;&#30528;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
How does the brain represent different modes of information? Can we design a system that automatically understands what the user is thinking? Such questions can be answered by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic research in cognitive science and neuroscience. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LFH&#30340;&#36229;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21160;&#24577;&#26500;&#24314;&#36229;&#36793;&#24182;&#21033;&#29992;&#24322;&#36136;&#23646;&#24615;&#36827;&#34892;&#23884;&#20837;&#26356;&#26032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.03411</link><description>&lt;p&gt;
&#20174;&#24322;&#36136;&#24615;&#20013;&#23398;&#20064;&#65306;&#29992;&#20110;&#36229;&#22270;&#30340;&#21160;&#24577;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning from Heterogeneity: A Dynamic Learning Framework for Hypergraphs. (arXiv:2307.03411v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LFH&#30340;&#36229;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21160;&#24577;&#26500;&#24314;&#36229;&#36793;&#24182;&#21033;&#29992;&#24322;&#36136;&#23646;&#24615;&#36827;&#34892;&#23884;&#20837;&#26356;&#26032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#22312;&#24314;&#27169;&#22797;&#26434;&#22270;&#32467;&#26500;&#25968;&#25454;&#26041;&#38754;&#30340;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#32780;&#22312;&#36817;&#24180;&#26469;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#22312;&#25152;&#26377;&#22270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#36229;&#22270;&#23398;&#20064;&#26159;&#19968;&#31181;&#22312;&#35757;&#32451;&#22270;&#30340;&#23884;&#20837;&#31354;&#38388;&#26102;&#25506;&#32034;&#38544;&#21547;&#30340;&#39640;&#38454;&#20851;&#32852;&#30340;&#25216;&#26415;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LFH&#30340;&#36229;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#22270;&#30340;&#24322;&#36136;&#23646;&#24615;&#36827;&#34892;&#21160;&#24577;&#36229;&#36793;&#26500;&#24314;&#21644;&#20851;&#27880;&#24615;&#23884;&#20837;&#26356;&#26032;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#36890;&#36807;&#21033;&#29992;&#26174;&#24335;&#30340;&#22270;&#32467;&#26500;&#20449;&#24687;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#29305;&#24449;&#21521;&#37327;&#12290;&#28982;&#21518;&#36890;&#36807;&#38544;&#24335;&#36229;&#36793;&#30340;&#21160;&#24577;&#20998;&#32452;&#26469;&#26500;&#24314;&#36229;&#22270;&#65292;&#24182;&#36827;&#34892;&#31867;&#22411;&#29305;&#23450;&#30340;&#36229;&#22270;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural network (GNN) has gained increasing popularity in recent years owing to its capability and flexibility in modeling complex graph structure data. Among all graph learning methods, hypergraph learning is a technique for exploring the implicit higher-order correlations when training the embedding space of the graph. In this paper, we propose a hypergraph learning framework named LFH that is capable of dynamic hyperedge construction and attentive embedding update utilizing the heterogeneity attributes of the graph. Specifically, in our framework, the high-quality features are first generated by the pairwise fusion strategy that utilizes explicit graph structure information when generating initial node embedding. Afterwards, a hypergraph is constructed through the dynamic grouping of implicit hyperedges, followed by the type-specific hypergraph learning process. To evaluate the effectiveness of our proposed framework, we conduct comprehensive experiments on several popular data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#23545;&#25239;&#25903;&#25345;&#23545;&#40784;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#29305;&#24449;&#34920;&#31034;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#31216;&#25903;&#25345;&#20998;&#27495;&#65292;&#38024;&#23545;&#26631;&#31614;&#20998;&#24067;&#20559;&#31227;&#30340;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.18458</link><description>&lt;p&gt;
&#24102;&#26631;&#31614;&#20559;&#31227;&#30340;&#22495;&#36866;&#24212;&#20013;&#30340;&#26465;&#20214;&#25903;&#25345;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Conditional Support Alignment for Domain Adaptation with Label Shift. (arXiv:2305.18458v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#23545;&#25239;&#25903;&#25345;&#23545;&#40784;&#26041;&#27861;&#65292;&#20197;&#26368;&#23567;&#21270;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#29305;&#24449;&#34920;&#31034;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#31216;&#25903;&#25345;&#20998;&#27495;&#65292;&#38024;&#23545;&#26631;&#31614;&#20998;&#24067;&#20559;&#31227;&#30340;&#22495;&#36866;&#24212;&#38382;&#39064;&#65292;&#30456;&#23545;&#29616;&#26377;&#30340;&#26041;&#27861;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#36866;&#24212; (UDA) &#26159;&#25351;&#19968;&#31181;&#22495;&#36866;&#24212;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#27169;&#22411;&#22522;&#20110;&#28304;&#22495;&#19978;&#30340;&#26631;&#35760;&#26679;&#26412;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#39046;&#22495;&#20869;&#29616;&#26377;&#30340;&#20027;&#27969;&#26041;&#27861;&#20381;&#36182;&#20110;&#32463;&#20856;&#30340;&#21327;&#21464;&#37327;&#20559;&#31227;&#20551;&#35774;&#26469;&#23398;&#20064;&#22495;&#19981;&#21464;&#29305;&#24449;&#34920;&#31034;&#65292;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26631;&#31614;&#20998;&#24067;&#20559;&#31227;&#19979;&#65292;&#24615;&#33021;&#20122;&#20248;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26465;&#20214;&#23545;&#25239;&#25903;&#25345;&#23545;&#40784; (CASA)&#65292;&#20854;&#26088;&#22312;&#26368;&#23567;&#21270;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#29305;&#24449;&#34920;&#31034;&#20998;&#24067;&#20043;&#38388;&#30340;&#26465;&#20214;&#23545;&#31216;&#25903;&#25345;&#20998;&#27495;&#65292;&#20197;&#20415;&#26356;&#22909;&#22320;&#34920;&#31034;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#29702;&#35770;&#30446;&#26631;&#39118;&#38505;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;&#35843;&#25972;&#26465;&#20214;&#29305;&#24449;&#20998;&#24067;&#25903;&#25345;&#19982;&#29616;&#26377;&#30340;UDA&#35774;&#32622;&#20013;&#30340;&#36793;&#32536;&#25903;&#25345;&#23545;&#40784;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#23545;&#20110;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24471;&#21040;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) refers to a domain adaptation framework in which a learning model is trained based on the labeled samples on the source domain and unlabelled ones in the target domain. The dominant existing methods in the field that rely on the classical covariate shift assumption to learn domain-invariant feature representation have yielded suboptimal performance under the label distribution shift between source and target domains. In this paper, we propose a novel conditional adversarial support alignment (CASA) whose aim is to minimize the conditional symmetric support divergence between the source's and target domain's feature representation distributions, aiming at a more helpful representation for the classification task. We also introduce a novel theoretical target risk bound, which justifies the merits of aligning the supports of conditional feature distributions compared to the existing marginal support alignment approach in the UDA settings. We then provi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17323</link><description>&lt;p&gt;
&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#30340;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Some Primal-Dual Theory for Subgradient Methods for Strongly Convex Optimization. (arXiv:2305.17323v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#20984;&#20248;&#21270;&#30340;&#27425;&#26799;&#24230;&#27861;&#21407;&#22987;&#23545;&#20598;&#29702;&#35770;&#65292;&#21487;&#20197;&#23454;&#29616;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#21516;&#26102;&#21487;&#20197;&#36866;&#29992;&#20110;&#21508;&#31181;&#27493;&#38271;&#30340;&#36873;&#25321;&#21644;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#20445;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#24378;&#20984;&#20294;&#28508;&#22312;&#38750;&#20809;&#28369;&#38750;Lipschitz&#20248;&#21270;&#30340;&#65288;&#38543;&#26426;&#65289;&#27425;&#26799;&#24230;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#31561;&#20215;&#23545;&#20598;&#25551;&#36848;&#65288;&#31867;&#20284;&#20110;&#23545;&#20598;&#24179;&#22343;&#65289;&#26469;&#25551;&#36848;&#32463;&#20856;&#30340;&#27425;&#26799;&#24230;&#27861;&#65292;&#36817;&#31471;&#27425;&#26799;&#24230;&#27861;&#21644;&#20999;&#25442;&#27425;&#26799;&#24230;&#27861;&#12290;&#36825;&#20123;&#31561;&#20215;&#24615;&#33021;&#22815;&#20197; $O(1/T)$ &#30340;&#36895;&#24230;&#25910;&#25947;&#65292;&#21516;&#26102;&#33021;&#22815;&#22312;&#24378;&#20984;&#20248;&#21270;&#38382;&#39064;&#19978;&#20998;&#21035;&#36824;&#25552;&#20379;&#20102;&#32463;&#20856;&#21407;&#22987;&#38388;&#38553;&#21644;&#21069;&#20154;&#26410;&#26366;&#20998;&#26512;&#30340;&#23545;&#20598;&#38388;&#38553;&#20445;&#35777;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20026;&#36825;&#20123;&#32463;&#20856;&#26041;&#27861;&#25552;&#20379;&#20102;&#31616;&#21333;&#30340;&#12289;&#26368;&#20339;&#30340;&#20572;&#27490;&#20934;&#21017;&#21644;&#20248;&#21270;&#35777;&#26126;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36866;&#29992;&#20110;&#36817;&#20046;&#25152;&#26377;&#30340;&#27493;&#38271;&#36873;&#25321;&#21644;&#19968;&#31995;&#21015;&#30340;&#38750;Lipschitz&#30149;&#24577;&#38382;&#39064;&#65292;&#23545;&#20110;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#27425;&#26799;&#24230;&#27861;&#30340;&#26089;&#26399;&#36845;&#20195;&#21487;&#33021;&#20250;&#20986;&#29616;&#25351;&#25968;&#32423;&#30340;&#21457;&#25955;&#65292;&#32780;&#20043;&#21069;&#30340;&#30740;&#31350;&#27809;&#26377;&#22788;&#29702;&#36807;&#36825;&#31181;&#38382;&#39064;&#12290;&#21363;&#20351;&#22312;&#36825;&#31181;&#19981;&#33391;&#25805;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20173;&#28982;&#30830;&#20445;&#21644; bounds &#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider (stochastic) subgradient methods for strongly convex but potentially nonsmooth non-Lipschitz optimization. We provide new equivalent dual descriptions (in the style of dual averaging) for the classic subgradient method, the proximal subgradient method, and the switching subgradient method. These equivalences enable $O(1/T)$ convergence guarantees in terms of both their classic primal gap and a not previously analyzed dual gap for strongly convex optimization. Consequently, our theory provides these classic methods with simple, optimal stopping criteria and optimality certificates at no added computational cost. Our results apply under nearly any stepsize selection and for a range of non-Lipschitz ill-conditioned problems where the early iterations of the subgradient method may diverge exponentially quickly (a phenomenon which, to the best of our knowledge, no prior works address). Even in the presence of such undesirable behaviors, our theory still ensures and bounds eventu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.09010</link><description>&lt;p&gt;
CF-VAE&#65306;&#22522;&#20110;VAE&#21644;&#22240;&#26524;&#27969;&#30340;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CF-VAE: Causal Disentangled Representation Learning with VAE and Causal Flows. (arXiv:2304.09010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#27969;&#20197;&#36827;&#34892;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#26088;&#22312;&#23398;&#20064;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#20854;&#20013;&#27599;&#20010;&#32500;&#24230;&#23545;&#24212;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#22240;&#32032;&#12290;&#30001;&#20110;&#29983;&#25104;&#22240;&#32032;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#22240;&#26524;&#20851;&#31995;&#65292;&#22240;&#26524;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#20197;&#23558;&#22240;&#26524;&#32467;&#26500;&#20449;&#24687;&#24341;&#20837;&#27169;&#22411;&#20013;&#30340;&#27969;&#65292;&#31216;&#20026;&#22240;&#26524;&#27969;&#12290;&#22522;&#20110;&#24191;&#27867;&#29992;&#20110;&#20998;&#31163;&#34920;&#31034;&#23398;&#20064;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;CF-VAE&#65292;&#21033;&#29992;&#22240;&#26524;&#27969;&#22686;&#24378;&#20102;VAE&#32534;&#30721;&#22120;&#30340;&#20998;&#31163;&#33021;&#21147;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#24341;&#20837;&#22522;&#20934;&#22240;&#32032;&#30340;&#30417;&#30563;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#20998;&#31163;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CF-VAE&#21487;&#20197;&#23454;&#29616;&#22240;&#26524;&#20998;&#31163;&#24182;&#36827;&#34892;&#24178;&#39044;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning disentangled representations is important in representation learning, aiming to learn a low dimensional representation of data where each dimension corresponds to one underlying generative factor. Due to the possibility of causal relationships between generative factors, causal disentangled representation learning has received widespread attention. In this paper, we first propose a new flows that can incorporate causal structure information into the model, called causal flows. Based on the variational autoencoders(VAE) commonly used in disentangled representation learning, we design a new model, CF-VAE, which enhances the disentanglement ability of the VAE encoder by utilizing the causal flows. By further introducing the supervision of ground-truth factors, we demonstrate the disentanglement identifiability of our model. Experimental results on both synthetic and real datasets show that CF-VAE can achieve causal disentanglement and perform intervention experiments. Moreover, C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#35299;&#20915;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25514;&#26045;&#19979;&#23454;&#29616;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#21033;&#28070;&#65292;&#24182;&#20026;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.02022</link><description>&lt;p&gt;
&#22522;&#20110;MNL&#36873;&#25321;&#27169;&#22411;&#30340;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Joint Assortment-Inventory Optimization under MNL Choices. (arXiv:2304.02022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#35299;&#20915;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25514;&#26045;&#19979;&#23454;&#29616;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#21033;&#28070;&#65292;&#24182;&#20026;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#39038;&#23458;&#30340;&#36873;&#25321;&#34892;&#20026;&#37117;&#36981;&#24490;Multinomial Logit&#65288;MNL&#65289;&#36873;&#25321;&#27169;&#22411;&#65292;&#21560;&#24341;&#21147;&#21442;&#25968;&#26159;&#20808;&#39564;&#26410;&#30693;&#30340;&#12290;&#38646;&#21806;&#21830;&#36827;&#34892;&#21608;&#26399;&#24615;&#32452;&#21512;&#21644;&#24211;&#23384;&#20915;&#31574;&#65292;&#20197;&#21160;&#24577;&#22320;&#20174;&#23454;&#29616;&#30340;&#38656;&#27714;&#20013;&#23398;&#20064;&#21560;&#24341;&#21147;&#21442;&#25968;&#65292;&#21516;&#26102;&#22312;&#26102;&#38388;&#19978;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#24635;&#21033;&#28070;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24179;&#34913;&#32452;&#21512;&#21644;&#24211;&#23384;&#22312;&#32447;&#20915;&#31574;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24314;&#31435;&#22312;&#19968;&#20010;&#26032;&#30340;MNL&#21560;&#24341;&#21147;&#21442;&#25968;&#20272;&#35745;&#22120;&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#26576;&#20123;&#24050;&#30693;&#21644;&#26410;&#30693;&#21442;&#25968;&#26469;&#28608;&#21169;&#25506;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#38745;&#24577;&#21333;&#21608;&#26399;&#32452;&#21512;&#24211;&#23384;&#35268;&#21010;&#38382;&#39064;&#30340;&#20248;&#21270;oracle&#22522;&#30784;&#20043;&#19978;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#65292;&#20197;&#21450;&#20851;&#20110;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study an online joint assortment-inventory optimization problem, in which we assume that the choice behavior of each customer follows the Multinomial Logit (MNL) choice model, and the attraction parameters are unknown a priori. The retailer makes periodic assortment and inventory decisions to dynamically learn from the realized demands about the attraction parameters while maximizing the expected total profit over time. In this paper, we propose a novel algorithm that can effectively balance the exploration and exploitation in the online decision-making of assortment and inventory. Our algorithm builds on a new estimator for the MNL attraction parameters, a novel approach to incentivize exploration by adaptively tuning certain known and unknown parameters, and an optimization oracle to static single-cycle assortment-inventory planning problems with given parameters. We establish a regret upper bound for our algorithm and a lower bound for the online joint assortment-inventory optimi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19979;&#30028;&#20248;&#21270;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#35299;&#37322;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#37327;&#21270;&#36825;&#20123;&#35201;&#27714;&#12290;&#36890;&#36807;&#20248;&#21270;&#19979;&#30028;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2212.07056</link><description>&lt;p&gt;
&#20851;&#20110;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#30340;&#27010;&#29575;&#65306;&#19968;&#31181;&#19979;&#30028;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On the Probability of Necessity and Sufficiency of Explaining Graph Neural Networks: A Lower Bound Optimization Approach. (arXiv:2212.07056v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07056
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#19979;&#30028;&#20248;&#21270;&#26469;&#35299;&#37322;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#35299;&#37322;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#37327;&#21270;&#36825;&#20123;&#35201;&#27714;&#12290;&#36890;&#36807;&#20248;&#21270;&#19979;&#30028;&#65292;&#21487;&#20197;&#33719;&#24471;&#26368;&#24517;&#35201;&#19988;&#20805;&#20998;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#21508;&#31181;GNN&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#23427;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#35299;&#37322;&#24212;&#35813;&#21516;&#26102;&#20855;&#26377;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GNN&#35299;&#37322;&#26041;&#27861;&#20165;&#20851;&#27880;&#20854;&#20013;&#20043;&#19968;&#65292;&#24517;&#35201;&#24615;&#25110;&#20805;&#20998;&#24615;&#65292;&#25110;&#32773;&#26159;&#20004;&#32773;&#20043;&#38388;&#30340;&#21551;&#21457;&#24335;&#26435;&#34913;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#30340;&#27010;&#29575;&#65288;PNS&#65289;&#26377;&#28508;&#21147;&#30830;&#23450;&#26368;&#24517;&#35201;&#21644;&#20805;&#20998;&#30340;&#35299;&#37322;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25968;&#23398;&#19978;&#37327;&#21270;&#35299;&#37322;&#30340;&#24517;&#35201;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#21333;&#35843;&#24615;&#21644;&#21453;&#20107;&#23454;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#33719;&#24471;PNS&#30340;&#22256;&#38590;&#38480;&#21046;&#20102;&#20854;&#24191;&#27867;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;PNS&#30340;&#19981;&#21487;&#35782;&#21035;&#24615;&#65292;&#25105;&#20204;&#27714;&#21161;&#20110;PNS&#30340;&#19968;&#20010;&#19979;&#30028;&#65292;&#21487;&#20197;&#36890;&#36807;&#21453;&#20107;&#23454;&#20272;&#35745;&#36827;&#34892;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;GNN&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#35299;&#37322;&#30340;&#26694;&#26550;&#65288;NSEG&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#19979;&#30028;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explainability of Graph Neural Networks (GNNs) is critical to various GNN applications, yet it remains a significant challenge. A convincing explanation should be both necessary and sufficient simultaneously. However, existing GNN explaining approaches focus on only one of the two aspects, necessity or sufficiency, or a heuristic trade-off between the two. Theoretically, the Probability of Necessity and Sufficiency (PNS) holds the potential to identify the most necessary and sufficient explanation since it can mathematically quantify the necessity and sufficiency of an explanation. Nevertheless, the difficulty of obtaining PNS due to non-monotonicity and the challenge of counterfactual estimation limit its wide use. To address the non-identifiability of PNS, we resort to a lower bound of PNS that can be optimized via counterfactual estimation, and propose a framework of Necessary and Sufficient Explanation for GNN (NSEG) via optimizing that lower bound. Specifically, we depict the 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#22270;&#27880;&#24847;&#24490;&#29615;&#32593;&#32476;&#30340;&#27963;&#21160;&#24863;&#30693;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#22270;&#21644;&#20351;&#29992;&#23618;&#27425;&#22270;&#27880;&#24847;&#27169;&#22359;&#26469;&#25429;&#25417;&#26102;&#38388;-&#27963;&#21160;-&#20301;&#32622;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21382;&#21490;&#22686;&#24378;&#32622;&#20449;&#26631;&#31614;&#65292;&#29992;&#20110;&#32858;&#28966;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#20307;&#32423;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.07765</link><description>&lt;p&gt;
&#20855;&#26377;&#23618;&#27425;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#32593;&#32476;&#30340;&#27963;&#21160;&#24863;&#30693;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Activity-aware Human Mobility Prediction with Hierarchical Graph Attention Recurrent Network. (arXiv:2210.07765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07765
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#22270;&#27880;&#24847;&#24490;&#29615;&#32593;&#32476;&#30340;&#27963;&#21160;&#24863;&#30693;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#22270;&#21644;&#20351;&#29992;&#23618;&#27425;&#22270;&#27880;&#24847;&#27169;&#22359;&#26469;&#25429;&#25417;&#26102;&#38388;-&#27963;&#21160;-&#20301;&#32622;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21382;&#21490;&#22686;&#24378;&#32622;&#20449;&#26631;&#31614;&#65292;&#29992;&#20110;&#32858;&#28966;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#20307;&#32423;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#12289;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#21644;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23545;&#34892;&#20026;&#20449;&#24687;&#30340;&#32771;&#34385;&#65292;&#36825;&#26159;&#25512;&#29702;&#20154;&#31867;&#20559;&#22909;&#21644;&#20363;&#34892;&#27963;&#21160;&#30340;&#20851;&#38190;&#65292;&#25110;&#32773;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#26102;&#38388;&#12289;&#27963;&#21160;&#21644;&#20301;&#32622;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#22270;&#27880;&#24847;&#24490;&#29615;&#32593;&#32476;&#65288;HGARN&#65289;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#25152;&#26377;&#29992;&#25143;&#30340;&#21382;&#21490;&#31227;&#21160;&#35760;&#24405;&#26500;&#24314;&#20102;&#19968;&#20010;&#23618;&#27425;&#22270;&#65292;&#24182;&#20351;&#29992;&#23618;&#27425;&#22270;&#27880;&#24847;&#27169;&#22359;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;-&#27963;&#21160;-&#20301;&#32622;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;HGARN&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#20016;&#23500;&#30340;&#20154;&#31867;&#20986;&#34892;&#35821;&#20041;&#30340;&#34920;&#31034;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21382;&#21490;&#22686;&#24378;&#32622;&#20449;&#65288;MAHEC&#65289;&#26631;&#31614;&#65292;&#20197;&#20415;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#32858;&#28966;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#20307;&#32423;&#20559;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26102;&#38388;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Human mobility prediction is a fundamental task essential for various applications, including urban planning, location-based services and intelligent transportation systems. Existing methods often ignore activity information crucial for reasoning human preferences and routines, or adopt a simplified representation of the dependencies between time, activities and locations. To address these issues, we present Hierarchical Graph Attention Recurrent Network (HGARN) for human mobility prediction. Specifically, we construct a hierarchical graph based on all users' history mobility records and employ a Hierarchical Graph Attention Module to capture complex time-activity-location dependencies. This way, HGARN can learn representations with rich human travel semantics to model user preferences at the global level. We also propose a model-agnostic history-enhanced confidence (MAHEC) label to focus our model on each user's individual-level preferences. Finally, we introduce a Temporal Module, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#32463;&#20856;&#30340;UCB&#31639;&#27861;&#21644;KL-UCB&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#36798;&#21040;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#36234;&#22810;&#65292;&#21518;&#24724;&#20540;&#36234;&#22909;&#12290;</title><link>http://arxiv.org/abs/2111.10933</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#21487;&#20197;&#36229;&#36234;&#38598;&#20013;&#24335;&#19978;&#32622;&#20449;&#30028;&#38480;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Multi-Armed Bandits Can Outperform Centralized Upper Confidence Bound Algorithms. (arXiv:2111.10933v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#31639;&#27861;&#65292;&#22522;&#20110;&#32463;&#20856;&#30340;UCB&#31639;&#27861;&#21644;KL-UCB&#31639;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#33021;&#36798;&#21040;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#36234;&#22810;&#65292;&#21518;&#24724;&#20540;&#36234;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#20551;&#35774;N&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20182;&#20204;&#38754;&#23545;&#30528;&#19968;&#32452;&#20849;&#21516;&#30340;M&#20010;&#33218;&#24182;&#20849;&#20139;&#30456;&#21516;&#30340;&#33218;&#22870;&#21169;&#20998;&#24067;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#20174;&#37051;&#23621;&#22788;&#25509;&#25910;&#20449;&#24687;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#37051;&#23621;&#20851;&#31995;&#30001;&#19968;&#20010;&#26080;&#21521;&#22270;&#25551;&#36848;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#23436;&#20840;&#20998;&#24067;&#24335;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#20998;&#21035;&#22522;&#20110;&#32463;&#20856;&#30340;&#19978;&#32622;&#20449;&#30028;&#38480;&#65288;UCB&#65289;&#31639;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;KL-UCB&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#20998;&#24067;&#24335;&#31639;&#27861;&#20351;&#32593;&#32476;&#20013;&#30340;&#27599;&#20010;&#26234;&#33021;&#20307;&#33021;&#22815;&#23454;&#29616;&#27604;&#20854;&#21333;&#19968;&#26234;&#33021;&#20307;&#30456;&#24212;&#31639;&#27861;&#26356;&#22909;&#30340;&#23545;&#25968;&#28176;&#36817;&#21518;&#24724;&#65292;&#21069;&#25552;&#26159;&#26234;&#33021;&#20307;&#33267;&#23569;&#26377;&#19968;&#20010;&#37051;&#23621;&#65292;&#32780;&#19988;&#26234;&#33021;&#20307;&#26377;&#36234;&#22810;&#30340;&#37051;&#23621;&#65292;&#21518;&#24724;&#20540;&#20250;&#36234;&#22909;&#65292;&#36825;&#24847;&#21619;&#30528;&#25972;&#20307;&#30340;&#21644;&#22823;&#20110;&#20854;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies a decentralized multi-armed bandit problem in a multi-agent network. The problem is simultaneously solved by N agents assuming they face a common set of M arms and share the same arms' reward distributions. Each agent can receive information only from its neighbors, where the neighbor relationships among the agents are described by an undirected graph. Two fully decentralized multi-armed bandit algorithms are proposed, respectively based on the classic upper confidence bound (UCB) algorithm and the state-of-the-art KL-UCB algorithm. The proposed decentralized algorithms permit each agent in the network to achieve a better logarithmic asymptotic regret than their single-agent counterparts, provided that the agent has at least one neighbor, and the more neighbors an agent has, the better regret it will have, meaning that the sum is more than its component parts.
&lt;/p&gt;</description></item></channel></rss>