<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00025</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
On the Challenges and Opportunities in Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00025
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#36817;&#24180;&#26469;&#22686;&#38271;&#36805;&#36895;&#32780;&#31283;&#23450;&#12290;&#38543;&#30528;&#28023;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#30340;&#36827;&#27493;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#23637;&#29616;&#20986;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#25991;&#26412;&#20197;&#21450;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#35270;&#39057;&#21644;&#20998;&#23376;&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#22823;&#35268;&#27169;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#33509;&#24178;&#22522;&#26412;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#30340;&#20851;&#38190;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35782;&#21035;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#25506;&#32034;&#26377;&#30410;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#35775;&#38382;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00025v1 Announce Type: cross  Abstract: The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI so
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#31070;&#32463;&#27169;&#22411;&#20013;&#24341;&#20837;&#19981;&#21516;iable&#21644;&#23436;&#20840;&#21367;&#31215;&#30340;&#21069;&#31471;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36339;&#36291;&#36830;&#25509;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26799;&#24230;&#25915;&#20987;&#30340;&#26174;&#33879;&#38887;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#26377;&#25928;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.17018</link><description>&lt;p&gt;
&#36890;&#36807;&#23436;&#20840;&#21367;&#31215;&#21644;&#21487;&#24494;&#30340;&#21069;&#31471;&#19982;&#36339;&#36291;&#36830;&#25509;&#23545;&#26799;&#24230;&#25915;&#20987;&#34920;&#29616;&#20986;&#26174;&#33879;&#38887;&#24615;&#30340;&#32784;&#20154;&#23547;&#21619;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Curious Case of Remarkable Resilience to Gradient Attacks via Fully Convolutional and Differentiable Front End with a Skip Connection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17018
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#31070;&#32463;&#27169;&#22411;&#20013;&#24341;&#20837;&#19981;&#21516;iable&#21644;&#23436;&#20840;&#21367;&#31215;&#30340;&#21069;&#31471;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#36339;&#36291;&#36830;&#25509;&#65292;&#25104;&#21151;&#23454;&#29616;&#23545;&#26799;&#24230;&#25915;&#20987;&#30340;&#26174;&#33879;&#38887;&#24615;&#65292;&#24182;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#26377;&#25928;&#23545;&#25239;&#40657;&#30418;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#27979;&#35797;&#20102;&#36890;&#36807;&#22312;&#19968;&#20010;&#20923;&#32467;&#30340;&#20998;&#31867;&#22120;&#20043;&#21069;&#22686;&#21152;&#19968;&#20010;&#21487;&#24494;&#19988;&#23436;&#20840;&#21367;&#31215;&#30340;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#30340;&#21069;&#31471;&#22686;&#24378;&#31070;&#32463;&#27169;&#22411;&#12290;&#36890;&#36807;&#20351;&#29992;&#36739;&#23567;&#30340;&#23398;&#20064;&#29575;&#36827;&#34892;&#22823;&#32422;&#19968;&#20010;epoch&#30340;&#35757;&#32451;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20123;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20445;&#25345;&#39592;&#24178;&#20998;&#31867;&#22120;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#23545;&#21253;&#25324;AutoAttack&#36719;&#20214;&#21253;&#20013;&#30340;APGD&#21644;FAB-T&#25915;&#20987;&#22312;&#20869;&#30340;&#26799;&#24230;&#25915;&#20987;&#20855;&#26377;&#24322;&#24120;&#30340;&#25269;&#25239;&#21147;&#65292;&#36825;&#24402;&#22240;&#20110;&#26799;&#24230;&#25513;&#30422;&#12290;&#26799;&#24230;&#25513;&#30422;&#29616;&#35937;&#24182;&#19981;&#26032;&#40092;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#27809;&#26377;&#26799;&#24230;&#30772;&#22351;&#37096;&#20998;&#65288;&#22914;JPEG&#21387;&#32553;&#25110;&#39044;&#35745;&#23548;&#33268;&#26799;&#24230;&#20943;&#23567;&#30340;&#37096;&#20998;&#65289;&#30340;&#23436;&#20840;&#21487;&#24494;&#27169;&#22411;&#26469;&#35828;&#65292;&#25513;&#30422;&#30340;&#31243;&#24230;&#30456;&#24403;&#26174;&#33879;&#12290;&#23613;&#31649;&#40657;&#30418;&#25915;&#20987;&#23545;&#26799;&#24230;&#25513;&#30422;&#21487;&#33021;&#37096;&#20998;&#26377;&#25928;&#65292;&#20294;&#36890;&#36807;&#23558;&#27169;&#22411;&#32452;&#21512;&#25104;&#38543;&#26426;&#38598;&#21512;&#65292;&#21487;&#20197;&#36731;&#26494;&#20987;&#36133;&#23427;&#20204;&#12290;&#25105;&#20204;&#20272;&#35745;&#36825;&#26679;&#30340;&#38598;&#21512;&#22312;CIFAR10&#21644;CIF&#31561;&#19978;&#23454;&#29616;&#20102;&#20960;&#20046;SOTA&#32423;&#21035;&#30340;AutoAttack&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17018v1 Announce Type: cross  Abstract: We tested front-end enhanced neural models where a frozen classifier was prepended by a differentiable and fully convolutional model with a skip connection. By training them using a small learning rate for about one epoch, we obtained models that retained the accuracy of the backbone classifier while being unusually resistant to gradient attacks including APGD and FAB-T attacks from the AutoAttack package, which we attributed to gradient masking. The gradient masking phenomenon is not new, but the degree of masking was quite remarkable for fully differentiable models that did not have gradient-shattering components such as JPEG compression or components that are expected to cause diminishing gradients.   Though black box attacks can be partially effective against gradient masking, they are easily defeated by combining models into randomized ensembles. We estimate that such ensembles achieve near-SOTA AutoAttack accuracy on CIFAR10, CIF
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;TNTRules&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#38388;&#38553;&#12290;</title><link>http://arxiv.org/abs/2401.13334</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Explainable Bayesian Optimization. (arXiv:2401.13334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;TNTRules&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#65292;&#22635;&#34917;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24037;&#19994;&#39046;&#22495;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#21442;&#25968;&#35843;&#20248;&#30340;&#25511;&#21046;&#31995;&#32479;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36817;&#20284;&#35823;&#24046;&#21644;&#31616;&#21270;&#30446;&#26631;&#65292;BO&#30340;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#20559;&#31163;&#20154;&#31867;&#19987;&#23478;&#30340;&#30495;&#23454;&#30446;&#26631;&#65292;&#38656;&#35201;&#21518;&#32493;&#35843;&#25972;&#12290;BO&#30340;&#40657;&#30418;&#29305;&#24615;&#38480;&#21046;&#20102;&#21327;&#20316;&#35843;&#20248;&#36807;&#31243;&#65292;&#22240;&#20026;&#19987;&#23478;&#19981;&#20449;&#20219;BO&#30340;&#24314;&#35758;&#12290;&#30446;&#21069;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#26080;&#27861;&#35299;&#20915;&#27492;&#38388;&#38553;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#38388;&#38553;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TNTRules&#65288;TUNE-NOTUNE&#35268;&#21017;&#65289;&#65292;&#19968;&#31181;&#20107;&#21518;&#22522;&#20110;&#35268;&#21017;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20248;&#21270;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#35299;&#37322;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#20248;&#21270;&#38382;&#39064;&#21644;&#23454;&#38469;&#36229;&#21442;&#25968;&#20248;&#21270;&#20219;&#21153;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;TNTRules&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#35299;&#37322;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#12290;&#36825;&#39033;&#24037;&#20316;&#23545;BO&#21644;XAI&#30340;&#20132;&#21449;&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#65292;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In industry, Bayesian optimization (BO) is widely applied in the human-AI collaborative parameter tuning of cyber-physical systems. However, BO's solutions may deviate from human experts' actual goal due to approximation errors and simplified objectives, requiring subsequent tuning. The black-box nature of BO limits the collaborative tuning process because the expert does not trust the BO recommendations. Current explainable AI (XAI) methods are not tailored for optimization and thus fall short of addressing this gap. To bridge this gap, we propose TNTRules (TUNE-NOTUNE Rules), a post-hoc, rule-based explainability method that produces high quality explanations through multiobjective optimization. Our evaluation of benchmark optimization problems and real-world hyperparameter optimization tasks demonstrates TNTRules' superiority over state-of-the-art XAI methods in generating high quality explanations. This work contributes to the intersection of BO and XAI, providing interpretable opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00369</link><description>&lt;p&gt;
&#25552;&#28860;&#24402;&#32435;&#20559;&#24046;&#65306;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression. (arXiv:2310.00369v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;Vision Transformers (ViTs) &#25552;&#20379;&#20102;&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20013;&#23454;&#29616;&#32479;&#19968;&#20449;&#24687;&#22788;&#29702;&#30340;&#35825;&#20154;&#21069;&#26223;&#12290;&#20294;&#26159;&#30001;&#20110;ViTs&#32570;&#20047;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#20351;&#23427;&#20204;&#30340;&#24212;&#29992;&#23454;&#38469;&#21487;&#34892;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#36731;&#37327;&#32423;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#12290;&#20197;&#21069;&#30340;&#31995;&#32479;&#20165;&#20381;&#38752;&#22522;&#20110;&#21367;&#31215;&#30340;&#25945;&#23398;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#20542;&#21521;&#30340;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#65288;&#20363;&#22914;&#21367;&#31215;&#21644;&#38750;&#32447;&#24615;&#21367;&#31215;&#65289;&#21516;&#26102;&#29992;&#20110;&#25351;&#23548;&#23398;&#29983;Transformer&#12290;&#30001;&#20110;&#36825;&#20123;&#29420;&#29305;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#20174;&#21508;&#31181;&#23384;&#20648;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#28041;&#21450;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;logits&#65292;&#20174;&#26681;&#26412;&#19978;&#23454;&#29616;&#20102;&#38750;&#24402;&#19968;&#21270;&#30340;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalizing prospect of unified information processing across visual and textual domains. But due to the lack of inherent inductive biases in ViTs, they require enormous amount of data for training. To make their applications practical, we introduce an innovative ensemble-based distillation approach distilling inductive bias from complementary lightweight teacher models. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to instruct the student transformer jointly. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework also involves precomputing and storing logits in advance, essentially the unnormalize
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#36890;&#36807;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#24212;&#29992;&#22240;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#37325;&#24314;&#20302;&#31209;&#30697;&#38453;&#12290;&#29305;&#21035;&#22320;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#24494;&#20998;&#26041;&#31243;&#65292;&#31216;&#20026;&#8220;&#25200;&#21160;&#26799;&#24230;&#27969;&#8221;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25200;&#21160;&#34987;&#38480;&#21046;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#26102;&#65292;&#25200;&#21160;&#26799;&#24230;&#27969;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#30446;&#26631;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2309.01796</link><description>&lt;p&gt;
&#29992;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#38750;&#23545;&#31216;&#30697;&#38453;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Asymmetric matrix sensing by gradient descent with small random initialization. (arXiv:2309.01796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#30697;&#38453;&#24863;&#30693;&#38382;&#39064;&#65292;&#36890;&#36807;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#24212;&#29992;&#22240;&#24335;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#37325;&#24314;&#20302;&#31209;&#30697;&#38453;&#12290;&#29305;&#21035;&#22320;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#24494;&#20998;&#26041;&#31243;&#65292;&#31216;&#20026;&#8220;&#25200;&#21160;&#26799;&#24230;&#27969;&#8221;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#25200;&#21160;&#34987;&#38480;&#21046;&#22312;&#19968;&#23450;&#33539;&#22260;&#20869;&#26102;&#65292;&#25200;&#21160;&#26799;&#24230;&#27969;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#30446;&#26631;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#30697;&#38453;&#24863;&#30693;&#65292;&#21363;&#20174;&#23569;&#37327;&#32447;&#24615;&#27979;&#37327;&#20013;&#37325;&#24314;&#20302;&#31209;&#30697;&#38453;&#30340;&#38382;&#39064;&#12290;&#23427;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36807;&#21442;&#25968;&#21270;&#22238;&#24402;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#22240;&#24335;&#20998;&#35299;&#30340;&#26799;&#24230;&#19979;&#38477;&#35299;&#20915;&#65292;&#24403;&#20174;&#19968;&#20010;&#23567;&#30340;&#38543;&#26426;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#22240;&#24335;&#26799;&#24230;&#19979;&#38477;&#36827;&#34892;&#30697;&#38453;&#24863;&#30693;&#65292;&#20316;&#20026;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#20984;&#38382;&#39064;&#30340;&#20856;&#22411;&#27169;&#22411;&#65292;&#21487;&#20197;&#23558;&#22797;&#26434;&#29616;&#35937;&#35299;&#24320;&#24182;&#35814;&#32454;&#30740;&#31350;&#12290;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#30740;&#31350;&#38750;&#23545;&#31216;&#30697;&#38453;&#24863;&#30693;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#20363;&#22914;&#38750;&#23545;&#31216;&#30697;&#38453;&#22240;&#24335;&#20998;&#35299;&#21644;&#23545;&#31216;&#21322;&#27491;&#23450;&#30697;&#38453;&#24863;&#30693;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#36129;&#29486;&#26159;&#24341;&#20837;&#20102;&#19968;&#20010;&#36830;&#32493;&#24494;&#20998;&#26041;&#31243;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#25200;&#21160;&#26799;&#24230;&#27969;&#8221;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#25200;&#21160;&#34987;&#38480;&#21046;&#22312;&#36275;&#22815;&#33539;&#22260;&#20869;&#26102;&#65292;&#25200;&#21160;&#26799;&#24230;&#27969;&#33021;&#22815;&#24555;&#36895;&#25910;&#25947;&#21040;&#30495;&#23454;&#30340;&#30446;&#26631;&#30697;&#38453;&#12290;&#26799;&#24230;&#19979;&#38477;&#23545;&#30697;&#38453;&#30340;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
We study matrix sensing, which is the problem of reconstructing a low-rank matrix from a few linear measurements. It can be formulated as an overparameterized regression problem, which can be solved by factorized gradient descent when starting from a small random initialization.  Linear neural networks, and in particular matrix sensing by factorized gradient descent, serve as prototypical models of non-convex problems in modern machine learning, where complex phenomena can be disentangled and studied in detail. Much research has been devoted to studying special cases of asymmetric matrix sensing, such as asymmetric matrix factorization and symmetric positive semi-definite matrix sensing.  Our key contribution is introducing a continuous differential equation that we call the $\textit{perturbed gradient flow}$. We prove that the perturbed gradient flow converges quickly to the true target matrix whenever the perturbation is sufficiently bounded. The dynamics of gradient descent for matr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23398;&#20064;&#26377;&#38480;&#33258;&#21160;&#26426;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#21644;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14111</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#20248;&#21270;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Anomaly Detection via Discrete Optimization. (arXiv:2303.14111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23398;&#20064;&#26377;&#38480;&#33258;&#21160;&#26426;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#21644;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20363;&#22914;&#32593;&#32476;&#23433;&#20840;&#12289;&#25191;&#27861;&#12289;&#21307;&#23398;&#21644;&#27450;&#35784;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#24448;&#24448;&#38590;&#20197;&#29702;&#35299;&#65292;&#36825;&#36890;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#32473;&#23450;&#30340;&#26410;&#26631;&#35760;&#24207;&#21015;&#22810;&#37325;&#38598;&#20013;&#23398;&#20064;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426; &#65288;DFA&#65289;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;&#35745;&#31639;&#38590;&#39064;&#65292;&#24182;&#22522;&#20110;&#32422;&#26463;&#20248;&#21270;&#24320;&#21457;&#20102;&#20004;&#20010;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20248;&#21270;&#38382;&#39064;&#24341;&#20837;&#20102;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#25105;&#20204;&#30340;DFA&#30340;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21407;&#22411;&#23454;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is essential in many application domains, such as cyber security, law enforcement, medicine, and fraud protection. However, the decision-making of current deep learning approaches is notoriously hard to understand, which often limits their practical applicability. To overcome this limitation, we propose a framework for learning inherently interpretable anomaly detectors from sequential data. More specifically, we consider the task of learning a deterministic finite automaton (DFA) from a given multi-set of unlabeled sequences. We show that this problem is computationally hard and develop two learning algorithms based on constraint optimization. Moreover, we introduce novel regularization schemes for our optimization problems that improve the overall interpretability of our DFAs. Using a prototype implementation, we demonstrate that our approach shows promising results in terms of accuracy and F1 score.
&lt;/p&gt;</description></item></channel></rss>