<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedShift&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#36801;&#31227;&#32858;&#21512;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01070</link><description>&lt;p&gt;
FedShift: &#36890;&#36807;&#26435;&#37325;&#36801;&#31227;&#32858;&#21512;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#30340;&#21452;&#37325;&#24322;&#36136;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FedShift: Tackling Dual Heterogeneity Problem of Federated Learning via Weight Shift Aggregation
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedShift&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26435;&#37325;&#36801;&#31227;&#32858;&#21512;&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24182;&#27880;&#37325;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#26377;&#21147;&#26041;&#27861;&#12290;FL&#20013;&#23384;&#22312;&#30340;&#31995;&#32479;&#24322;&#36136;&#24615;&#21644;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#28304;&#20110;&#23458;&#25143;&#31471;&#30828;&#20214;&#12289;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#22810;&#26679;&#24615;&#12290;&#36825;&#31181;&#22810;&#26679;&#24615;&#21487;&#33021;&#20250;&#20005;&#37325;&#24433;&#21709;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#36890;&#20449;&#25928;&#29575;&#25110;&#31283;&#23450;&#25910;&#25947;&#31639;&#27861;&#26469;&#35299;&#20915;&#31995;&#32479;&#25110;&#32479;&#35745;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#20294;&#21333;&#29420;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#24448;&#24448;&#20250;&#23548;&#33268;&#22949;&#21327;&#65292;&#22240;&#20026;&#24322;&#36136;&#24615;&#38382;&#39064;&#26410;&#24471;&#21040;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FedShift&#30340;&#26032;&#31639;&#27861;&#65292;&#26088;&#22312;&#22312;&#21452;&#37325;&#24322;&#36136;&#24615;&#22330;&#26223;&#20013;&#25552;&#39640;&#35757;&#32451;&#36895;&#24230;&#21644;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#37327;&#21270;&#25913;&#21892;&#23458;&#25143;&#21442;&#19982;&#24230;&#65292;&#24182;&#36890;&#36807;&#24212;&#29992;&#36801;&#31227;&#25216;&#26415;&#26469;&#32531;&#35299;&#37327;&#21270;&#36890;&#24120;&#23548;&#33268;&#30340;&#24615;&#33021;&#19981;&#33391;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) offers a compelling method for training machine learning models with a focus on preserving data privacy. The presence of system heterogeneity and statistical heterogeneity, recognized challenges in FL, arises from the diversity of client hardware, network, and dataset distribution. This diversity can critically affect the training pace and the performance of models. While many studies address either system or statistical heterogeneity by introducing communication-efficient or stable convergence algorithms, addressing these challenges in isolation often leads to compromises due to unaddressed heterogeneity. In response, this paper introduces FedShift, a novel algorithm designed to enhance both the training speed and the models' accuracy in a dual heterogeneity scenario. Our solution can improve client engagement through quantization and mitigate the adverse effects on performance typically associated with quantization by employing a shifting technique. This techn
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#29256;&#26412;&#30340;&#20851;&#31995;GNNs&#65292;&#36890;&#36807;&#22312;$t$&#20026;&#26080;&#31351;&#22823;&#26102;&#20165;&#20351;&#29992;&#20108;&#27425;&#31354;&#38388;&#30340;&#23884;&#20837;&#26469;&#36817;&#20284;$3$-GNNs&#65292;&#23545;&#20110;&#36739;&#20302;&#30340;$t$&#20540;&#65292;&#36890;&#36807;&#20132;&#25442;&#36739;&#23569;&#30340;&#28040;&#24687;&#23454;&#29616;&#24369;&#30340;&#36817;&#20284;&#65292;&#21516;&#26102;&#36890;&#24120;&#20135;&#29983;&#20102;&#20960;&#20010;&#35268;&#21010;&#39046;&#22495;&#20013;&#25152;&#38656;&#30340;$C_3$&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.11734</link><description>&lt;p&gt;
&#23398;&#20064;&#21476;&#20856;&#35268;&#21010;&#39046;&#22495;&#30340;&#36890;&#29992;&#31574;&#30053;&#65306;&#36229;&#36234;$C_2$
&lt;/p&gt;
&lt;p&gt;
Learning General Policies for Classical Planning Domains: Getting Beyond C$_2$
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#21270;&#29256;&#26412;&#30340;&#20851;&#31995;GNNs&#65292;&#36890;&#36807;&#22312;$t$&#20026;&#26080;&#31351;&#22823;&#26102;&#20165;&#20351;&#29992;&#20108;&#27425;&#31354;&#38388;&#30340;&#23884;&#20837;&#26469;&#36817;&#20284;$3$-GNNs&#65292;&#23545;&#20110;&#36739;&#20302;&#30340;$t$&#20540;&#65292;&#36890;&#36807;&#20132;&#25442;&#36739;&#23569;&#30340;&#28040;&#24687;&#23454;&#29616;&#24369;&#30340;&#36817;&#20284;&#65292;&#21516;&#26102;&#36890;&#24120;&#20135;&#29983;&#20102;&#20960;&#20010;&#35268;&#21010;&#39046;&#22495;&#20013;&#25152;&#38656;&#30340;$C_3$&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;GNN&#30340;&#26041;&#27861;&#29992;&#20110;&#23398;&#20064;&#36328;&#35268;&#21010;&#39046;&#22495;&#30340;&#36890;&#29992;&#31574;&#30053;&#21463;&#21040;$C_2$&#34920;&#36798;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#21363;&#19968;&#38454;&#36923;&#36753;&#21482;&#33021;&#21253;&#21547;&#20004;&#20010;&#21464;&#37327;&#21644;&#35745;&#25968;&#12290;&#36825;&#31181;&#38480;&#21046;&#21487;&#20197;&#36890;&#36807;&#36716;&#21521;$k$-GNNs&#65292;&#20854;&#20013;$k=3$&#65292;&#20854;&#20013;&#29289;&#20307;&#23884;&#20837;&#34987;&#19977;&#20803;&#32452;&#23884;&#20837;&#25152;&#26367;&#25442;&#65292;&#26469;&#20811;&#26381;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;$3$-GNNs&#20855;&#26377;$C_3$&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#19981;&#21516;&#20110;&#21463;&#38480;&#20110;$C_2$&#30340;$1$-&#21644;$2$-GNNs&#65292;&#23427;&#20204;&#38656;&#35201;&#22235;&#27425;&#26102;&#38388;&#36827;&#34892;&#28040;&#24687;&#20132;&#25442;&#21644;&#19977;&#27425;&#31354;&#38388;&#36827;&#34892;&#23884;&#20837;&#65292;&#20351;&#23427;&#20204;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#29256;&#26412;&#30340;&#20851;&#31995;GNNs&#12290;&#24403;$t$&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;R-GNN[$t$]&#20165;&#20351;&#29992;&#20108;&#27425;&#31354;&#38388;&#30340;&#23884;&#20837;&#26469;&#36817;&#20284;$3$-GNNs&#12290;&#23545;&#20110;&#36739;&#20302;&#30340;$t$&#20540;&#65292;&#20363;&#22914;$t=1$&#21644;$t=2$&#65292;R-GNN[$t$]&#36890;&#36807;&#20132;&#25442;&#36739;&#23569;&#30340;&#28040;&#24687;&#23454;&#29616;&#20102;&#26356;&#24369;&#30340;&#36817;&#20284;&#65292;&#20294;&#26377;&#36259;&#30340;&#26159;&#65292;&#36890;&#24120;&#20135;&#29983;&#20102;&#22312;&#20960;&#20010;&#35268;&#21010;&#39046;&#22495;&#20013;&#25152;&#38656;&#30340;$C_3$&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#26032;&#30340;R-GNN[$t$] ar
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11734v1 Announce Type: new  Abstract: GNN-based approaches for learning general policies across planning domains are limited by the expressive power of $C_2$, namely; first-order logic with two variables and counting. This limitation can be overcomed by transitioning to $k$-GNNs, for $k=3$, wherein object embeddings are substituted with triplet embeddings. Yet, while $3$-GNNs have the expressive power of $C_3$, unlike $1$- and $2$-GNNs that are confined to $C_2$, they require quartic time for message exchange and cubic space for embeddings, rendering them impractical. In this work, we introduce a parameterized version of relational GNNs. When $t$ is infinity, R-GNN[$t$] approximates $3$-GNNs using only quadratic space for embeddings. For lower values of $t$, such as $t=1$ and $t=2$, R-GNN[$t$] achieves a weaker approximation by exchanging fewer messages, yet interestingly, often yield the $C_3$ features required in several planning domains. Furthermore, the new R-GNN[$t$] ar
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;LLMs&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.15173</link><description>&lt;p&gt;
&#26080;&#30171;&#20154;&#24037;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20108;&#38454;&#24494;&#35843;&#65306;&#19968;&#31181;&#22522;&#20110;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15173
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#20197;&#22686;&#24378;LLMs&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32972;&#21521;&#20256;&#25773;&#36807;&#31243;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#24120;&#38656;&#35201;&#26114;&#36149;&#30340;GPU&#20869;&#23384;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36716;&#21521;&#20351;&#29992;&#38646;&#38454;&#20248;&#21270;&#22120;&#36827;&#34892;&#24494;&#35843;&#65292;&#36890;&#36807;&#20004;&#27425;&#21069;&#21521;&#20256;&#36882;&#26174;&#33879;&#33410;&#30465;&#20869;&#23384;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20248;&#21270;&#22120;&#21463;&#19981;&#21516;&#32500;&#24230;&#20043;&#38388;&#21442;&#25968;&#26354;&#29575;&#30340;&#24322;&#36136;&#24615;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HiZOO&#65292;&#19968;&#31181;&#23545;&#35282;Hessian&#20449;&#24687;&#30340;&#38646;&#38454;&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#21033;&#29992;&#23545;&#35282;Hessian&#22686;&#24378;&#38646;&#38454;&#20248;&#21270;&#22120;&#36827;&#34892;LLMs&#24494;&#35843;&#30340;&#24037;&#20316;&#12290;HiZOO&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#20869;&#23384;&#25104;&#26412;&#65292;&#24182;&#19988;&#27599;&#27493;&#21482;&#22686;&#21152;&#20102;&#19968;&#20010;&#21069;&#21521;&#20256;&#36882;&#12290;&#23545;&#21508;&#31181;&#27169;&#22411;&#65288;350M&#12316;66B&#21442;&#25968;&#65289;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;HiZOO&#25552;&#39640;&#20102;&#27169;&#22411;&#25910;&#25947;&#36895;&#24230;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35757;&#32451;&#27493;&#39588;&#65292;&#24182;&#26377;&#25928;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21487;&#35270;&#21270;&#20102;HiZOO&#22312;&#27979;&#35797;&#20989;&#25968;&#19978;&#30340;&#20248;&#21270;&#36712;&#36857;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15173v1 Announce Type: new  Abstract: Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What's more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, il
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#23454;&#20363;&#30340;&#24555;&#36895;&#39640;&#25928;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#30340;&#24066;&#22330;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31639;&#27861;&#65288;FastGreedy&#21644;FastPostponedGreedy&#65289;&#12290;&#35813;&#31639;&#27861;&#22312;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22312;&#32447;&#21152;&#26435;&#21305;&#37197;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#24555;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2305.08353</link><description>&lt;p&gt;
&#24555;&#36895;&#39640;&#25928;&#30340;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#23454;&#20363;&#30340;&#21305;&#37197;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast and Efficient Matching Algorithm with Deadline Instances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.08353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#23454;&#20363;&#30340;&#24555;&#36895;&#39640;&#25928;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#30340;&#24066;&#22330;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20248;&#21270;&#31639;&#27861;&#65288;FastGreedy&#21644;FastPostponedGreedy&#65289;&#12290;&#35813;&#31639;&#27861;&#22312;&#22788;&#29702;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22312;&#32447;&#21152;&#26435;&#21305;&#37197;&#38382;&#39064;&#26102;&#20855;&#26377;&#36739;&#24555;&#30340;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#22312;&#32447;&#21152;&#26435;&#21305;&#37197;&#38382;&#39064;&#30001;&#20110;&#20854;&#20247;&#22810;&#24212;&#29992;&#32780;&#25104;&#20026;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#23613;&#31649;&#22312;&#36825;&#20010;&#39046;&#22495;&#24050;&#32463;&#20570;&#20102;&#24456;&#22810;&#21162;&#21147;&#65292;&#20294;&#29616;&#26377;&#30340;&#31639;&#27861;&#35201;&#20040;&#36895;&#24230;&#22826;&#24930;&#65292;&#35201;&#20040;&#27809;&#26377;&#32771;&#34385;&#21040;&#25130;&#27490;&#26399;&#38480;&#65288;&#33410;&#28857;&#21487;&#20197;&#21305;&#37197;&#30340;&#26368;&#38271;&#26102;&#38388;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#20010;&#24102;&#26377;&#25130;&#27490;&#26399;&#38480;&#30340;&#24066;&#22330;&#27169;&#22411;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20248;&#21270;&#31639;&#27861;&#65288;FastGreedy&#21644;FastPostponedGreedy&#65289;&#65292;&#24182;&#32473;&#20986;&#20102;&#20851;&#20110;&#31639;&#27861;&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;&#27491;&#30830;&#24615;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#22312;FastGreedy&#31639;&#27861;&#20013;&#65292;&#25105;&#20204;&#24050;&#32463;&#30693;&#36947;&#19968;&#20010;&#33410;&#28857;&#26159;&#20080;&#23478;&#36824;&#26159;&#21334;&#23478;&#12290;&#20294;&#22312;FastPostponedGreedy&#31639;&#27861;&#20013;&#65292;&#19968;&#24320;&#22987;&#25105;&#20204;&#19981;&#30693;&#36947;&#27599;&#20010;&#33410;&#28857;&#30340;&#29366;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25512;&#24191;&#20102;&#19968;&#20010;&#33609;&#22270;&#30697;&#38453;&#65292;&#20197;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;&#21407;&#22987;&#31639;&#27861;&#21644;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#35774; &#949; &#8712;&#65288;0,0.1&#65289;&#34920;&#31034;&#27599;&#26465;&#36793;&#30340;&#30495;&#23454;&#26435;&#37325;&#30340;&#30456;&#23545;&#35823;&#24046;&#12290;&#21407;&#22987;&#30340;Greedy&#21644;Po&#31639;&#27861;&#30340;&#31454;&#20105;&#27604;&#29575;&#26159;&#22810;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
The online weighted matching problem is a fundamental problem in machine learning due to its numerous applications. Despite many efforts in this area, existing algorithms are either too slow or don't take $\mathrm{deadline}$ (the longest time a node can be matched) into account. In this paper, we introduce a market model with $\mathrm{deadline}$ first. Next, we present our two optimized algorithms (\textsc{FastGreedy} and \textsc{FastPostponedGreedy}) and offer theoretical proof of the time complexity and correctness of our algorithms. In \textsc{FastGreedy} algorithm, we have already known if a node is a buyer or a seller. But in \textsc{FastPostponedGreedy} algorithm, the status of each node is unknown at first. Then, we generalize a sketching matrix to run the original and our algorithms on both real data sets and synthetic data sets. Let $\epsilon \in (0,0.1)$ denote the relative error of the real weight of each edge. The competitive ratio of original \textsc{Greedy} and \textsc{Po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2011.08388</link><description>&lt;p&gt;
&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21487;&#35299;&#37322;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65292;&#24182;&#21033;&#29992;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation based Interpretable Image Emotion Recognition using Facial Expression Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.08388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20986;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#24182;&#23558;&#20854;&#36866;&#24212;&#20026;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#19981;&#36275;&#30340;&#25361;&#25112;&#12290;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#20851;&#38190;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39046;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#29992;&#20110;&#35782;&#21035;&#21253;&#21547;&#38754;&#37096;&#21644;&#38750;&#38754;&#37096;&#29289;&#20307;&#20197;&#21450;&#38750;&#20154;&#31867;&#32452;&#20214;&#30340;&#36890;&#29992;&#22270;&#20687;&#20013;&#30340;&#24773;&#32490;&#12290;&#23427;&#35299;&#20915;&#20102;&#22270;&#20687;&#24773;&#32490;&#35782;&#21035;&#65288;IER&#65289;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#33391;&#22909;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#38754;&#37096;&#24773;&#32490;&#35782;&#21035;&#65288;FER&#65289;&#31995;&#32479;&#65292;&#23558;&#32473;&#23450;&#30340;&#38754;&#37096;&#22270;&#20687;&#20998;&#31867;&#20026;&#31163;&#25955;&#24773;&#32490;&#31867;&#21035;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#20687;&#35782;&#21035;&#31995;&#32479;&#65292;&#23558;&#25552;&#20986;&#30340;FER&#31995;&#32479;&#36866;&#24212;&#20110;&#21033;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#35782;&#21035;&#22270;&#20687;&#25152;&#20256;&#36798;&#30340;&#24773;&#32490;&#12290;&#23427;&#23558;&#36890;&#29992;&#22270;&#20687;&#20998;&#31867;&#20026;&#8220;&#24555;&#20048;&#8221;&#65292;&#8220;&#24754;&#20260;&#8221;&#65292;&#8220;&#20167;&#24680;&#8221;&#21644;&#8220;&#24868;&#24594;&#8221;&#31867;&#21035;&#12290;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#31216;&#20026;&#20998;&#32780;&#27835;&#20043;&#30340;Shap&#65288;DnCShap&#65289;&#65292;&#29992;&#20110;&#35299;&#37322;&#24773;&#32490;&#35782;&#21035;&#20013;&#39640;&#24230;&#30456;&#20851;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
A domain adaptation technique has been proposed in this paper to identify the emotions in generic images containing facial &amp; non-facial objects and non-human components. It addresses the challenge of the insufficient availability of pre-trained models and well-annotated datasets for image emotion recognition (IER). It starts with proposing a facial emotion recognition (FER) system and then moves on to adapting it for image emotion recognition. First, a deep-learning-based FER system has been proposed that classifies a given facial image into discrete emotion classes. Further, an image recognition system has been proposed that adapts the proposed FER system to recognize the emotions portrayed by images using domain adaptation. It classifies the generic images into 'happy,' 'sad,' 'hate,' and 'anger' classes. A novel interpretability approach, Divide and Conquer based Shap (DnCShap), has also been proposed to interpret the highly relevant visual features for emotion recognition. The prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.10747</link><description>&lt;p&gt;
&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;:&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multimodal Sentiment Analysis with Missing Modality: A Knowledge-Transfer Approach. (arXiv:2401.10747v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#36801;&#31227;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32570;&#22833;&#27169;&#24577;&#19979;&#36827;&#34892;&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#12290;&#36890;&#36807;&#32763;&#35793;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20869;&#23481;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#65292;&#24182;&#21033;&#29992;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#24773;&#24863;&#39044;&#27979;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#19982;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24773;&#24863;&#20998;&#26512;&#26088;&#22312;&#36890;&#36807;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#22768;&#38899;&#32447;&#32034;&#26469;&#35782;&#21035;&#20010;&#20307;&#34920;&#36798;&#30340;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#22823;&#22810;&#20551;&#35774;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#36807;&#31243;&#20013;&#25152;&#26377;&#27169;&#24577;&#37117;&#26159;&#21487;&#29992;&#30340;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#32570;&#22833;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#36801;&#31227;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#36827;&#34892;&#32763;&#35793;&#65292;&#20197;&#37325;&#26500;&#32570;&#22833;&#30340;&#38899;&#39057;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#36328;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#20445;&#30041;&#37325;&#26500;&#21644;&#35266;&#23519;&#21040;&#30340;&#27169;&#24577;&#30340;&#26368;&#22823;&#20449;&#24687;&#65292;&#29992;&#20110;&#24773;&#24863;&#39044;&#27979;&#12290;&#22312;&#19977;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#30456;&#23545;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#26174;&#33879;&#25913;&#36827;&#65292;&#24182;&#23454;&#29616;&#20102;&#19982;&#20855;&#26377;&#23436;&#25972;&#22810;&#27169;&#24577;&#30417;&#30563;&#30340;&#20808;&#21069;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal sentiment analysis aims to identify the emotions expressed by individuals through visual, language, and acoustic cues. However, most of the existing research efforts assume that all modalities are available during both training and testing, making their algorithms susceptible to the missing modality scenario. In this paper, we propose a novel knowledge-transfer network to translate between different modalities to reconstruct the missing audio modalities. Moreover, we develop a cross-modality attention mechanism to retain the maximal information of the reconstructed and observed modalities for sentiment prediction. Extensive experiments on three publicly available datasets demonstrate significant improvements over baselines and achieve comparable results to the previous methods with complete multi-modality supervision.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#21644;&#32858;&#31867;&#31639;&#27861;&#30340;&#20248;&#21270;Mapper&#22270;&#35206;&#30422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#35206;&#30422;&#36873;&#25321;&#29983;&#25104;&#20102;&#20445;&#30041;&#25968;&#25454;&#38598;&#26412;&#36136;&#30340;Mapper&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.06634</link><description>&lt;p&gt;
$G$-Mapper&#65306;&#23398;&#20064;Mapper&#26500;&#36896;&#20013;&#30340;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
$G$-Mapper: Learning a Cover in the Mapper Construction. (arXiv:2309.06634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#21644;&#32858;&#31867;&#31639;&#27861;&#30340;&#20248;&#21270;Mapper&#22270;&#35206;&#30422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#35206;&#30422;&#36873;&#25321;&#29983;&#25104;&#20102;&#20445;&#30041;&#25968;&#25454;&#38598;&#26412;&#36136;&#30340;Mapper&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mapper&#31639;&#27861;&#26159;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#20013;&#19968;&#31181;&#21453;&#26144;&#32473;&#23450;&#25968;&#25454;&#38598;&#32467;&#26500;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;Mapper&#31639;&#27861;&#38656;&#35201;&#35843;&#25972;&#22810;&#20010;&#21442;&#25968;&#20197;&#29983;&#25104;&#19968;&#20010;"&#22909;&#30475;&#30340;"Mapper&#22270;&#12290;&#35813;&#35770;&#25991;&#20851;&#27880;&#20110;&#36873;&#25321;&#35206;&#30422;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26681;&#25454;&#27491;&#24577;&#24615;&#30340;&#32479;&#35745;&#26816;&#39564;&#21453;&#22797;&#20998;&#21106;&#35206;&#30422;&#26469;&#20248;&#21270;Mapper&#22270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;$G$-means&#32858;&#31867;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#36827;&#34892;Anderson-Darling&#26816;&#39564;&#26469;&#23547;&#25214;$k$-means&#20013;&#26368;&#20339;&#30340;&#31751;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#21106;&#36807;&#31243;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#26681;&#25454;&#32473;&#23450;&#25968;&#25454;&#30340;&#20998;&#24067;&#31934;&#24515;&#36873;&#25321;&#35206;&#30422;&#12290;&#23545;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#35206;&#30422;&#20351;Mapper&#22270;&#20445;&#30041;&#20102;&#25968;&#25454;&#38598;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2307.11127</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#23454;&#29616;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#19979;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Control Methods by Density Matching under Implicit Endogeneitiy. (arXiv:2307.11127v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65288;SCMs&#65289;&#24050;&#25104;&#20026;&#27604;&#36739;&#26696;&#20363;&#30740;&#31350;&#20013;&#22240;&#26524;&#25512;&#26029;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;SCMs&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#35266;&#27979;&#32467;&#26524;&#30340;&#21152;&#26435;&#21644;&#26469;&#20272;&#35745;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#21512;&#25104;&#23545;&#29031;&#65288;SC&#65289;&#30340;&#20934;&#30830;&#24615;&#23545;&#20110;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;SC&#26435;&#37325;&#30340;&#20272;&#35745;&#25104;&#20026;&#20102;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#29616;&#26377;&#30340;SCMs&#23384;&#22312;&#19968;&#20010;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#65292;&#21363;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#19982;&#21453;&#20107;&#23454;&#32467;&#26524;&#27169;&#22411;&#20013;&#30340;&#35823;&#24046;&#39033;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#20250;&#23545;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#20135;&#29983;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#21305;&#37197;&#30340;&#26032;&#22411;SCM&#65292;&#20551;&#35774;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#21487;&#20197;&#29992;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#36817;&#20284;&#65288;&#21363;&#28151;&#21512;&#27169;&#22411;&#65289;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matchi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#25506;&#31350;&#20102;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#21363;&#20351;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#31572;&#26696;&#20998;&#24067;&#20063;&#21487;&#33021;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#26469;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.08733</link><description>&lt;p&gt;
&#20154;&#31867;&#21644;&#26426;&#22120;&#26377;&#30456;&#21516;&#30340;&#30524;&#30555;&#21527;&#65311;&#22522;&#20110;&#22270;&#20687;&#20998;&#31867;&#30340;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do humans and machines have the same eyes? Human-machine perceptual differences on image classification. (arXiv:2304.08733v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08733
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#36890;&#36807;&#22270;&#20687;&#20998;&#31867;&#25506;&#31350;&#20102;&#20154;&#26426;&#24863;&#30693;&#24046;&#24322;&#65292;&#21457;&#29616;&#21363;&#20351;&#20934;&#30830;&#29575;&#30456;&#20284;&#65292;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#31572;&#26696;&#20998;&#24067;&#20063;&#21487;&#33021;&#19981;&#21516;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#26469;&#25552;&#39640;&#20219;&#21153;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#33391;&#22909;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#36890;&#24120;&#36890;&#36807;&#27169;&#20223;&#20174;&#35757;&#32451;&#26631;&#31614;&#20013;&#23398;&#21040;&#30340;&#20154;&#31867;&#34892;&#20026;&#26469;&#35299;&#20915;&#35270;&#35273;&#20219;&#21153;&#12290;&#36817;&#26399;&#35270;&#35273;&#30740;&#31350;&#30340;&#22823;&#37096;&#20998;&#21162;&#21147;&#38598;&#20013;&#22312;&#20351;&#29992;&#26631;&#20934;&#21270;&#22522;&#20934;&#26469;&#27979;&#37327;&#27169;&#22411;&#20219;&#21153;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20102;&#35299;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#24863;&#30693;&#24046;&#24322;&#26041;&#38754;&#30340;&#24037;&#20316;&#36824;&#24456;&#26377;&#38480;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#39318;&#20808;&#37327;&#21270;&#24182;&#20998;&#26512;&#20102;&#20004;&#31181;&#26469;&#28304;&#38169;&#35823;&#30340;&#32479;&#35745;&#20998;&#24067;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#38590;&#24230;&#32423;&#21035;&#23545;&#20219;&#21153;&#36827;&#34892;&#25490;&#24207;&#65292;&#25506;&#35752;&#20154;&#31867;&#19982;&#26426;&#22120;&#19987;&#19994;&#30693;&#35782;&#30340;&#24046;&#24322;&#12290;&#21363;&#20351;&#20154;&#31867;&#21644;&#26426;&#22120;&#30340;&#25972;&#20307;&#20934;&#30830;&#24615;&#30456;&#20284;&#65292;&#31572;&#26696;&#30340;&#20998;&#24067;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#19981;&#21516;&#12290;&#21033;&#29992;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#24863;&#30693;&#24046;&#24322;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#20102;&#19968;&#31181;&#21518;&#26399;&#20154;&#26426;&#21512;&#20316;&#65292;&#20854;&#34920;&#29616;&#27604;&#21333;&#29420;&#30340;&#20154;&#25110;&#26426;&#22120;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Trained computer vision models are assumed to solve vision tasks by imitating human behavior learned from training labels. Most efforts in recent vision research focus on measuring the model task performance using standardized benchmarks. Limited work has been done to understand the perceptual difference between humans and machines. To fill this gap, our study first quantifies and analyzes the statistical distributions of mistakes from the two sources. We then explore human vs. machine expertise after ranking tasks by difficulty levels. Even when humans and machines have similar overall accuracies, the distribution of answers may vary. Leveraging the perceptual difference between humans and machines, we empirically demonstrate a post-hoc human-machine collaboration that outperforms humans or machines alone.
&lt;/p&gt;</description></item><item><title>repliclust &#26159;&#19968;&#20010; Python &#21253;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#25552;&#20379;&#20102;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14301</link><description>&lt;p&gt;
repliclust&#65306;&#32858;&#31867;&#20998;&#26512;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
repliclust: Synthetic Data for Cluster Analysis. (arXiv:2303.14301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14301
&lt;/p&gt;
&lt;p&gt;
repliclust &#26159;&#19968;&#20010; Python &#21253;&#65292;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#25552;&#20379;&#20102;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; repliclust&#65288;&#26469;&#33258;&#20110; repli-cate &#21644; clust-er&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#32858;&#31867;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340; Python &#21253;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#38598;&#30340;&#21407;&#22411;&#65292;&#21363;&#39640;&#32423;&#20960;&#20309;&#25551;&#36848;&#65292;&#29992;&#25143;&#21487;&#20197;&#20174;&#20013;&#21019;&#24314;&#35768;&#22810;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20855;&#26377;&#25152;&#38656;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#25105;&#20204;&#36719;&#20214;&#30340;&#26550;&#26500;&#26159;&#27169;&#22359;&#21270;&#21644;&#38754;&#21521;&#23545;&#35937;&#30340;&#65292;&#23558;&#25968;&#25454;&#29983;&#25104;&#20998;&#35299;&#25104;&#25918;&#32622;&#38598;&#32676;&#20013;&#24515;&#30340;&#31639;&#27861;&#12289;&#37319;&#26679;&#38598;&#32676;&#24418;&#29366;&#30340;&#31639;&#27861;&#12289;&#36873;&#25321;&#27599;&#20010;&#38598;&#32676;&#30340;&#25968;&#25454;&#28857;&#25968;&#37327;&#30340;&#31639;&#27861;&#20197;&#21450;&#20026;&#38598;&#32676;&#20998;&#37197;&#27010;&#29575;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;repliclust.org &#39033;&#30446;&#32593;&#39029;&#25552;&#20379;&#20102;&#31616;&#26126;&#30340;&#29992;&#25143;&#25351;&#21335;&#21644;&#20840;&#38754;&#30340;&#25991;&#26723;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present repliclust (from repli-cate and clust-er), a Python package for generating synthetic data sets with clusters. Our approach is based on data set archetypes, high-level geometric descriptions from which the user can create many different data sets, each possessing the desired geometric characteristics. The architecture of our software is modular and object-oriented, decomposing data generation into algorithms for placing cluster centers, sampling cluster shapes, selecting the number of data points for each cluster, and assigning probability distributions to clusters. The project webpage, repliclust.org, provides a concise user guide and thorough documentation.
&lt;/p&gt;</description></item><item><title>&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;(GBO)&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#31890;&#29699;&#35745;&#31639;&#26469;&#25552;&#39640;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#26041;&#38754;&#23427;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2303.12807</link><description>&lt;p&gt;
&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Granular-ball Optimization Algorithm. (arXiv:2303.12807v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12807
&lt;/p&gt;
&lt;p&gt;
&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;(GBO)&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#24341;&#20837;&#31890;&#29699;&#35745;&#31639;&#26469;&#25552;&#39640;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#26041;&#38754;&#23427;&#27604;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#26234;&#33021;&#20248;&#21270;&#31639;&#27861;&#37117;&#26159;&#22522;&#20110;&#26368;&#23567;&#31890;&#24230;&#21363;&#28857;&#30340;&#35774;&#35745;&#65292;&#23548;&#33268;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#36739;&#24369;&#19988;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#20248;&#21270;&#31639;&#27861;&#65292;&#21363;&#31890;&#29699;&#20248;&#21270;&#31639;&#27861;(GBO)&#65292;&#36890;&#36807;&#24341;&#20837;&#31890;&#29699;&#35745;&#31639;&#26469;&#23454;&#29616;&#12290;GBO&#20351;&#29992;&#22810;&#20010;&#31890;&#29699;&#26469;&#35206;&#30422;&#35299;&#31354;&#38388;&#65292;&#20351;&#29992;&#35768;&#22810;&#32454;&#23567;&#30340;&#32454;&#31890;&#24230;&#31890;&#29699;&#26469;&#25551;&#36848;&#37325;&#35201;&#37096;&#20998;&#65292;&#20351;&#29992;&#23569;&#37327;&#30340;&#22823;&#31895;&#31890;&#24230;&#31890;&#29699;&#26469;&#25551;&#36848;&#19981;&#37325;&#35201;&#30340;&#37096;&#20998;&#65292;&#31934;&#32454;&#30340;&#22810;&#31890;&#24230;&#25968;&#25454;&#25551;&#36848;&#33021;&#21147;&#25552;&#39640;&#20102;&#20840;&#23616;&#25628;&#32034;&#33021;&#21147;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;&#38024;&#23545;&#20108;&#21313;&#20010;&#22522;&#20934;&#20989;&#25968;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#30456;&#27604;&#65292;GBO&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#26356;&#25509;&#36817;&#26368;&#20248;&#35299;&#65292;&#27809;&#26377;&#36229;&#21442;&#25968;&#65292;&#35774;&#35745;&#26356;&#31616;&#21333;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existing intelligent optimization algorithms are designed based on the finest granularity, i.e., a point. This leads to weak global search ability and inefficiency. To address this problem, we proposed a novel multi-granularity optimization algorithm, namely granular-ball optimization algorithm (GBO), by introducing granular-ball computing. GBO uses many granular-balls to cover the solution space. Quite a lot of small and fine-grained granular-balls are used to depict the important parts, and a little number of large and coarse-grained granular-balls are used to depict the inessential parts. Fine multi-granularity data description ability results in a higher global search capability and faster convergence speed. In comparison with the most popular and state-of-the-art algorithms, the experiments on twenty benchmark functions demonstrate its better performance. The faster speed, higher approximation ability of optimal solution, no hyper-parameters, and simpler design of GBO make it 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31561;&#24335;&#32422;&#26463;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#33609;&#22270;&#30340;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#65288;StoSQP&#65289;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#19988;&#20801;&#35768;&#33258;&#36866;&#24212;&#36873;&#25321;&#38543;&#26426;&#27493;&#38271;&#21644;&#20351;&#29992;&#39640;&#25928;&#38543;&#26426;&#36845;&#20195;&#27714;&#35299;&#22120;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2205.13687</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#33609;&#22270;&#30340;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#23545;&#32422;&#26463;&#30340;&#38543;&#26426;&#20248;&#21270;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Statistical Inference of Constrained Stochastic Optimization via Sketched Sequential Quadratic Programming. (arXiv:2205.13687v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13687
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31561;&#24335;&#32422;&#26463;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;&#33609;&#22270;&#30340;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#65288;StoSQP&#65289;&#36827;&#34892;&#27714;&#35299;&#65292;&#24182;&#19988;&#20801;&#35768;&#33258;&#36866;&#24212;&#36873;&#25321;&#38543;&#26426;&#27493;&#38271;&#21644;&#20351;&#29992;&#39640;&#25928;&#38543;&#26426;&#36845;&#20195;&#27714;&#35299;&#22120;&#26469;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23545;&#31561;&#24335;&#32422;&#26463;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#20248;&#21270;&#38382;&#39064;&#36827;&#34892;&#32479;&#35745;&#25512;&#26029;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#22312;&#32447;&#38543;&#26426;&#39034;&#24207;&#20108;&#27425;&#35268;&#21010;&#65288;StoSQP&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21487;&#20197;&#23558;&#20854;&#35270;&#20026;&#23558;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65288;&#21363;KKT&#26465;&#20214;&#65289;&#12290;&#21463;&#26368;&#36817;&#25968;&#20540;&#20108;&#38454;&#26041;&#27861;&#35774;&#35745;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20801;&#35768;StoSQP&#33258;&#36866;&#24212;&#22320;&#36873;&#25321;&#20219;&#24847;&#38543;&#26426;&#27493;&#38271;$ \bar {\ alpha} _t $&#65292;&#21482;&#35201;$ \ beta _t \ leq \ bar {\ alpha} _t \ leq \ beta _t + \ chi _t $&#65292;&#20854;&#20013; $ \ beta_t $ &#21644; $ \ chi_t = o(\beta_t) $ &#26159;&#26576;&#20123;&#25511;&#21046;&#24207;&#21015;&#12290;&#20026;&#20102;&#38477;&#20302;&#20108;&#38454;&#26041;&#27861;&#30340;&#20027;&#35201;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#36824;&#20801;&#35768;StoSQP&#36890;&#36807;&#20351;&#29992;&#33609;&#22270;&#25216;&#26415;&#30340;&#39640;&#25928;&#38543;&#26426;&#36845;&#20195;&#27714;&#35299;&#22120;&#26469;&#19981;&#31934;&#30830;&#22320;&#35299;&#20915;&#20108;&#27425;&#35268;&#21010;&#38382;&#39064;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#35201;&#27714;&#36924;&#36817;&#35823;&#24046;&#38543;&#30528;&#36845;&#20195;&#30340;&#36827;&#34892;&#32780;&#20943;&#23567;&#12290;&#23545;&#20110;&#24320;&#21457;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#65288;i&#65289;&#19979;&#65292;&#23427;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26368;&#22810;&#20026;$ O(1 / \ ep&#65289;$&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider statistical inference of equality-constrained stochastic nonlinear optimization problems. We develop a fully online stochastic sequential quadratic programming (StoSQP) method to solve the problems, which can be regarded as applying Newton's method to the first-order optimality conditions (i.e., the KKT conditions). Motivated by recent designs of numerical second-order methods, we allow StoSQP to adaptively select any random stepsize $\bar{\alpha}_t$, as long as $\beta_t\leq \bar{\alpha}_t \leq \beta_t+\chi_t$, for some control sequences $\beta_t$ and $\chi_t=o(\beta_t)$. To reduce the dominant computational cost of second-order methods, we additionally allow StoSQP to inexactly solve quadratic programs via efficient randomized iterative solvers that utilize sketching techniques. Notably, we do not require the approximation error to diminish as iteration proceeds. For the developed method, we show that under mild assumptions (i) computationally, it can take at most $O(1/\ep
&lt;/p&gt;</description></item></channel></rss>