<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#22270;&#20449;&#24687;&#21033;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18393</link><description>&lt;p&gt;
&#22522;&#20110;&#24352;&#37327;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#22810;&#35270;&#35282;&#32858;&#31867;&#22270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Tensor-based Graph Learning with Consistency and Specificity for Multi-view Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#30456;&#20284;&#24615;&#27979;&#37327;&#21644;&#22270;&#20449;&#24687;&#21033;&#29992;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#22810;&#35270;&#35282;&#32858;&#31867;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#29616;&#26377;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#28041;&#21450;&#22522;&#20110;&#27010;&#29575;&#37051;&#23621;&#26500;&#24314;&#33258;&#36866;&#24212;&#37051;&#23621;&#22270;&#65292;&#28982;&#21518;&#23398;&#20064;&#19968;&#33268;&#24615;&#22270;&#36827;&#34892;&#32858;&#31867;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#20004;&#20010;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#27431;&#27663;&#36317;&#31163;&#26469;&#34913;&#37327;&#30456;&#20284;&#24615;&#65292;&#36825;&#22312;&#35768;&#22810;&#30495;&#23454;&#22330;&#26223;&#20013;&#25429;&#25417;&#25968;&#25454;&#28857;&#38388;&#30340;&#20869;&#22312;&#32467;&#26500;&#26102;&#35777;&#26126;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#20854;&#27425;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20165;&#20851;&#27880;&#19968;&#33268;&#24615;&#22270;&#65292;&#24573;&#30053;&#20102;&#29305;&#23450;&#35270;&#22270;&#30340;&#22270;&#20449;&#24687;&#12290;&#38024;&#23545;&#19978;&#36848;&#32570;&#28857;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#24352;&#37327;&#30340;&#22270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21516;&#26102;&#32771;&#34385;&#20102;&#22810;&#35270;&#22270;&#32858;&#31867;&#30340;&#19968;&#33268;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#22312;&#26031;&#33922;&#24343;&#23572;&#27969;&#24418;&#19978;&#35745;&#31639;&#30456;&#20284;&#36317;&#31163;&#20197;&#20445;&#30041;&#20869;&#22312;str
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18393v1 Announce Type: new  Abstract: Graph learning is widely recognized as a crucial technique in multi-view clustering. Existing graph learning methods typically involve constructing an adaptive neighbor graph based on probabilistic neighbors and then learning a consensus graph to for clustering, however, they are confronted with two limitations. Firstly, they often rely on Euclidean distance to measure similarity when constructing the adaptive neighbor graph, which proves inadequate in capturing the intrinsic structure among data points in many real-world scenarios. Secondly, most of these methods focus solely on consensus graph, ignoring view-specific graph information. In response to the aforementioned drawbacks, we in this paper propose a novel tensor-based graph learning framework that simultaneously considers consistency and specificity for multi-view clustering. Specifically, we calculate the similarity distance on the Stiefel manifold to preserve the intrinsic str
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.17377</link><description>&lt;p&gt;
&#20855;&#26377;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#30340;&#33258;&#30699;&#27491;&#25193;&#25955;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Self-Rectifying Diffusion Sampling with Perturbed-Attention Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17377
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#30340;&#26032;&#22411;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#25193;&#25955; U-Net &#20013;&#26367;&#25442;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#26469;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#20174;&#32780;&#22312;&#26080;&#26465;&#20214;&#21644;&#26377;&#26465;&#20214;&#35774;&#32622;&#19979;&#25913;&#21892;&#25193;&#25955;&#26679;&#26412;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#65292;&#20294;&#20854;&#36136;&#37327;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#27604;&#22914;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CG&#65289;&#21644;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#65288;CFG&#65289;&#12290;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#25110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#22914;&#22270;&#20687;&#24674;&#22797;&#20013;&#26080;&#27861;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25277;&#26679;&#24341;&#23548;&#25216;&#26415;&#65292;&#31216;&#20026;&#25200;&#21160;&#27880;&#24847;&#21147;&#24341;&#23548;&#65288;PAG&#65289;&#65292;&#23427;&#25913;&#36827;&#20102;&#25193;&#25955;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#19981;&#31649;&#26159;&#22312;&#26080;&#26465;&#20214;&#36824;&#26159;&#26377;&#26465;&#20214;&#30340;&#35774;&#32622;&#20013;&#65292;&#37117;&#33021;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#25110;&#25972;&#21512;&#22806;&#37096;&#27169;&#22359;&#12290;PAG &#26088;&#22312;&#36890;&#36807;&#25972;&#20010;&#21435;&#22122;&#36807;&#31243;&#36880;&#27493;&#22686;&#24378;&#26679;&#26412;&#30340;&#32467;&#26500;&#12290;&#23427;&#28041;&#21450;&#36890;&#36807;&#29992;&#24658;&#31561;&#30697;&#38453;&#26367;&#25442;&#25193;&#25955; U-Net &#20013;&#36873;&#25321;&#30340;&#33258;&#27880;&#24847;&#21147;&#26144;&#23556;&#29983;&#25104;&#32467;&#26500;&#38477;&#32423;&#30340;&#20013;&#38388;&#26679;&#26412;&#65292;&#32771;&#34385;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17377v1 Announce Type: cross  Abstract: Recent studies have demonstrated that diffusion models are capable of generating high-quality samples, but their quality heavily depends on sampling guidance techniques, such as classifier guidance (CG) and classifier-free guidance (CFG). These techniques are often not applicable in unconditional generation or in various downstream tasks such as image restoration. In this paper, we propose a novel sampling guidance, called Perturbed-Attention Guidance (PAG), which improves diffusion sample quality across both unconditional and conditional settings, achieving this without requiring additional training or the integration of external modules. PAG is designed to progressively enhance the structure of samples throughout the denoising process. It involves generating intermediate samples with degraded structure by substituting selected self-attention maps in diffusion U-Net with an identity matrix, by considering the self-attention mechanisms
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#26234;&#33021;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#21516;&#20998;&#35010;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#35010;&#20026;Grid Station&#65288;GS&#65289;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;SP&#65289;&#37096;&#20998;&#65292;&#23454;&#29616;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#36127;&#36733;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.01438</link><description>&lt;p&gt;
&#38754;&#21521;&#26234;&#33021;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#21516;&#20998;&#35010;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Collaborative Split Learning Framework for Smart Grid Load Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01438
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#26234;&#33021;&#30005;&#32593;&#36127;&#33655;&#39044;&#27979;&#30340;&#38544;&#31169;&#20445;&#25252;&#21327;&#21516;&#20998;&#35010;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#35010;&#20026;Grid Station&#65288;GS&#65289;&#21644;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;SP&#65289;&#37096;&#20998;&#65292;&#23454;&#29616;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#36127;&#36733;&#39044;&#27979;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#36127;&#33655;&#39044;&#27979;&#23545;&#33021;&#28304;&#31649;&#29702;&#12289;&#22522;&#30784;&#35774;&#26045;&#35268;&#21010;&#21644;&#20379;&#38656;&#24179;&#34913;&#33267;&#20851;&#37325;&#35201;&#12290;&#26234;&#33021;&#30005;&#34920;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#23548;&#33268;&#20102;&#20256;&#24863;&#22120;&#25968;&#25454;&#39537;&#21160;&#30340;&#36127;&#33655;&#39044;&#27979;&#38656;&#27714;&#12290;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#20801;&#35768;&#20351;&#29992;&#26469;&#33258;&#22810;&#20010;&#26234;&#33021;&#30005;&#34920;&#30340;&#25968;&#25454;&#35757;&#32451;&#21333;&#20010;&#20840;&#23616;&#27169;&#22411;&#65292;&#36825;&#38656;&#35201;&#23558;&#25968;&#25454;&#20256;&#36755;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#24341;&#21457;&#20102;&#23545;&#32593;&#32476;&#35201;&#27714;&#12289;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#35010;&#23398;&#20064;&#30340;&#36127;&#33655;&#39044;&#27979;&#26694;&#26550;&#65292;&#20197;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20998;&#20026;&#20004;&#20010;&#37096;&#20998;&#65292;&#19968;&#20010;&#29992;&#20110;&#27599;&#20010;Grid Station&#65288;GS&#65289;&#65292;&#36127;&#36131;&#19968;&#20010;&#25972;&#20010;&#31038;&#21306;&#30340;&#26234;&#33021;&#30005;&#34920;&#65307;&#21478;&#19968;&#20010;&#29992;&#20110;&#26381;&#21153;&#25552;&#20379;&#21830;&#65288;SP&#65289;&#12290;&#23458;&#25143;&#26234;&#33021;&#30005;&#34920;&#19981;&#20849;&#20139;&#20854;&#25968;&#25454;&#65292;&#32780;&#26159;&#20351;&#29992;&#21508;&#33258;&#30340;GS&#27169;&#22411;&#25286;&#20998;&#36827;&#34892;&#21069;&#21521;&#20256;&#36882;&#65292;&#21482;&#23558;&#20854;&#28608;&#27963;&#19982;GS&#20849;&#20139;&#12290;&#22312;&#36825;&#19968;&#26694;&#26550;&#19979;&#65292;&#27599;&#20010;GS&#36127;&#36131;&#20026;&#20854;&#21508;&#33258;&#30340;&#31038;&#21306;&#35757;&#32451;&#20010;&#24615;&#21270;&#27169;&#22411;&#20998;&#35010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01438v1 Announce Type: new  Abstract: Accurate load forecasting is crucial for energy management, infrastructure planning, and demand-supply balancing. Smart meter data availability has led to the demand for sensor-based load forecasting. Conventional ML allows training a single global model using data from multiple smart meters requiring data transfer to a central server, raising concerns for network requirements, privacy, and security. We propose a split learning-based framework for load forecasting to alleviate this issue. We split a deep neural network model into two parts, one for each Grid Station (GS) responsible for an entire neighbourhood's smart meters and the other for the Service Provider (SP). Instead of sharing their data, client smart meters use their respective GSs' model split for forward pass and only share their activations with the GS. Under this framework, each GS is responsible for training a personalized model split for their respective neighbourhoods,
&lt;/p&gt;</description></item><item><title>&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#21464;&#38761;&#24615;&#21147;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;CAD&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#33410;&#30465;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#25552;&#39640;&#20915;&#31574;&#25928;&#29575;&#65292;&#24182;&#21019;&#36896;&#20986;&#20855;&#26377;&#21019;&#26032;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.17695</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Geometric Deep Learning for Computer-Aided Design: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17695
&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#39046;&#22495;&#20855;&#26377;&#21464;&#38761;&#24615;&#21147;&#37327;&#65292;&#21487;&#20197;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;CAD&#35774;&#35745;&#24072;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#33410;&#30465;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#25552;&#39640;&#20915;&#31574;&#25928;&#29575;&#65292;&#24182;&#21019;&#36896;&#20986;&#20855;&#26377;&#21019;&#26032;&#24615;&#21644;&#23454;&#29992;&#24615;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;CAD&#65289;&#39046;&#22495;&#30340;&#19968;&#32929;&#21464;&#38761;&#21147;&#37327;&#65292;&#24182;&#26377;&#21487;&#33021;&#24443;&#24213;&#25913;&#21464;&#35774;&#35745;&#24072;&#21644;&#24037;&#31243;&#24072;&#22788;&#29702;&#21644;&#22686;&#24378;&#35774;&#35745;&#36807;&#31243;&#30340;&#26041;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;CAD&#35774;&#35745;&#24072;&#21487;&#20197;&#20248;&#21270;&#20182;&#20204;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#33410;&#30465;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#20570;&#20986;&#26356;&#20026;&#26126;&#26234;&#30340;&#20915;&#31574;&#65292;&#21019;&#36896;&#26082;&#21019;&#26032;&#21448;&#23454;&#29992;&#30340;&#35774;&#35745;&#12290;&#22788;&#29702;&#20197;&#20960;&#20309;&#25968;&#25454;&#34920;&#31034;&#30340;CAD&#35774;&#35745;&#24182;&#20998;&#26512;&#20854;&#32534;&#30721;&#29305;&#24449;&#30340;&#33021;&#21147;&#20351;&#24471;&#33021;&#22815;&#35782;&#21035;&#19981;&#21516;CAD&#27169;&#22411;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#65292;&#25552;&#20986;&#26367;&#20195;&#35774;&#35745;&#21644;&#22686;&#24378;&#26041;&#26696;&#65292;&#29978;&#33267;&#29983;&#25104;&#26032;&#30340;&#35774;&#35745;&#26367;&#20195;&#26041;&#26696;&#12290;&#36825;&#20221;&#35843;&#26597;&#20840;&#38754;&#20171;&#32461;&#20102;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#20013;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#31867;&#21035;&#65292;&#21253;&#25324;&#30456;&#20284;&#24615;&#20998;&#26512;&#21644;&#26816;&#32034;&#12289;2D&#21644;3D CAD&#27169;&#22411;&#21512;&#25104;&#65292;&#20197;&#21450;CAD&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17695v1 Announce Type: cross  Abstract: Geometric Deep Learning techniques have become a transformative force in the field of Computer-Aided Design (CAD), and have the potential to revolutionize how designers and engineers approach and enhance the design process. By harnessing the power of machine learning-based methods, CAD designers can optimize their workflows, save time and effort while making better informed decisions, and create designs that are both innovative and practical. The ability to process the CAD designs represented by geometric data and to analyze their encoded features enables the identification of similarities among diverse CAD models, the proposition of alternative designs and enhancements, and even the generation of novel design alternatives. This survey offers a comprehensive overview of learning-based methods in computer-aided design across various categories, including similarity analysis and retrieval, 2D and 3D CAD model synthesis, and CAD generatio
&lt;/p&gt;</description></item><item><title>VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.13126</link><description>&lt;p&gt;
VGMShield&#65306;&#32531;&#35299;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;
&lt;/p&gt;
&lt;p&gt;
VGMShield: Mitigating Misuse of Video Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13126
&lt;/p&gt;
&lt;p&gt;
VGMShield&#25552;&#20986;&#20102;&#19977;&#39033;&#31616;&#21333;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#36890;&#36807;&#26816;&#27979;&#34394;&#20551;&#35270;&#39057;&#12289;&#28335;&#28304;&#38382;&#39064;&#21644;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#31354;&#38388;-&#26102;&#38388;&#21160;&#24577;&#27169;&#22411;&#65292;&#38450;&#33539;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#35823;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#20154;&#20204;&#21487;&#20197;&#26041;&#20415;&#22320;&#21033;&#29992;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#31526;&#21512;&#20854;&#29305;&#23450;&#38656;&#27714;&#30340;&#35270;&#39057;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#20063;&#36234;&#26469;&#36234;&#25285;&#24515;&#36825;&#20123;&#25216;&#26415;&#34987;&#29992;&#20110;&#21019;&#20316;&#21644;&#20256;&#25773;&#34394;&#20551;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VGMShield&#65306;&#19968;&#22871;&#21253;&#21547;&#19977;&#39033;&#30452;&#25509;&#20294;&#24320;&#21019;&#24615;&#30340;&#25514;&#26045;&#65292;&#29992;&#20110;&#38450;&#33539;&#34394;&#20551;&#35270;&#39057;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#8220;&#34394;&#20551;&#35270;&#39057;&#26816;&#27979;&#8221;&#24320;&#22987;&#65292;&#23581;&#35797;&#29702;&#35299;&#29983;&#25104;&#30340;&#35270;&#39057;&#20013;&#26159;&#21542;&#23384;&#22312;&#29420;&#29305;&#24615;&#65292;&#20197;&#21450;&#25105;&#20204;&#26159;&#21542;&#33021;&#22815;&#21306;&#20998;&#23427;&#20204;&#19982;&#30495;&#23454;&#35270;&#39057;&#30340;&#19981;&#21516;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#8220;&#28335;&#28304;&#8221;&#38382;&#39064;&#65292;&#21363;&#23558;&#19968;&#27573;&#34394;&#20551;&#35270;&#39057;&#36861;&#28335;&#22238;&#29983;&#25104;&#23427;&#30340;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#20851;&#27880;&#8220;&#26102;&#31354;&#21160;&#24577;&#8221;&#30340;&#27169;&#22411;&#20316;&#20026;&#39592;&#24178;&#65292;&#20197;&#35782;&#21035;&#35270;&#39057;&#20013;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#23545;&#19971;&#20010;&#26368;&#20808;&#36827;&#30340;&#24320;&#28304;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13126v1 Announce Type: cross  Abstract: With the rapid advancement in video generation, people can conveniently utilize video generation models to create videos tailored to their specific desires. Nevertheless, there are also growing concerns about their potential misuse in creating and disseminating false information.   In this work, we introduce VGMShield: a set of three straightforward but pioneering mitigations through the lifecycle of fake video generation. We start from \textit{fake video detection} trying to understand whether there is uniqueness in generated videos and whether we can differentiate them from real videos; then, we investigate the \textit{tracing} problem, which maps a fake video back to a model that generates it. Towards these, we propose to leverage pre-trained models that focus on {\it spatial-temporal dynamics} as the backbone to identify inconsistencies in videos. Through experiments on seven state-of-the-art open-source models, we demonstrate that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25216;&#26415;&#25945;&#31243;&#65292;&#37325;&#28857;&#35762;&#35299;&#20102;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20844;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#37319;&#26679;&#21644;&#20998;&#25968;&#21305;&#37197;&#12290;&#36866;&#21512;&#21021;&#23398;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#65292;&#24182;&#19988;&#20174;&#19994;&#20154;&#21592;&#22312;&#35774;&#35745;&#26032;&#27169;&#22411;&#25110;&#31639;&#27861;&#26102;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2402.07487</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25216;&#26415;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Score-based Diffusion Models via Stochastic Differential Equations -- a Technical Tutorial
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07487
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#25216;&#26415;&#25945;&#31243;&#65292;&#37325;&#28857;&#35762;&#35299;&#20102;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#36827;&#34892;&#20844;&#24335;&#21270;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#37319;&#26679;&#21644;&#20998;&#25968;&#21305;&#37197;&#12290;&#36866;&#21512;&#21021;&#23398;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#65292;&#24182;&#19988;&#20174;&#19994;&#20154;&#21592;&#22312;&#35774;&#35745;&#26032;&#27169;&#22411;&#25110;&#31639;&#27861;&#26102;&#20063;&#21487;&#33021;&#20250;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#20851;&#20110;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#38416;&#37322;&#24615;&#25991;&#31456;&#65292;&#37325;&#28857;&#20171;&#32461;&#20102;&#36890;&#36807;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;(SDE)&#36827;&#34892;&#20844;&#24335;&#21270;&#30340;&#26041;&#27861;&#12290;&#22312;&#28201;&#21644;&#30340;&#20171;&#32461;&#20043;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25193;&#25955;&#24314;&#27169;&#30340;&#20004;&#20010;&#20851;&#38190;&#28857;--&#37319;&#26679;&#21644;&#20998;&#25968;&#21305;&#37197;&#65292;&#20854;&#20013;&#21253;&#25324;SDE/ODE&#37319;&#26679;&#65292;&#20998;&#25968;&#21305;&#37197;&#25928;&#29575;&#65292;&#19968;&#33268;&#24615;&#27169;&#22411;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31616;&#30701;&#30340;&#35777;&#26126;&#26469;&#35828;&#26126;&#25152;&#36848;&#32467;&#26524;&#30340;&#20027;&#35201;&#24605;&#24819;&#12290;&#26412;&#25991;&#20027;&#35201;&#26159;&#20026;&#20102;&#21521;&#21021;&#23398;&#32773;&#20171;&#32461;&#36825;&#20010;&#39046;&#22495;&#65292;&#21516;&#26102;&#20174;&#19994;&#20154;&#21592;&#22312;&#35774;&#35745;&#26032;&#27169;&#22411;&#25110;&#31639;&#27861;&#26102;&#20063;&#21487;&#33021;&#20250;&#21457;&#29616;&#19968;&#20123;&#20998;&#26512;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This is an expository article on the score-based diffusion models, with a particular focus on the formulation via stochastic differential equations (SDE). After a gentle introduction, we discuss the two pillars in the diffusion modeling -- sampling and score matching, which encompass the SDE/ODE sampling, score matching efficiency, the consistency model, and reinforcement learning. Short proofs are given to illustrate the main idea of the stated results. The article is primarily for introducing the beginners to the field, and practitioners may also find some analysis useful in designing new models or algorithms.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.15800</link><description>&lt;p&gt;
&#20351;&#29992;SHAP&#21644;LIME&#36827;&#34892;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#29305;&#24449;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Provably Stable Feature Rankings with SHAP and LIME. (arXiv:2401.15800v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26222;&#36941;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20998;&#36755;&#20837;&#21464;&#37327;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;SHAP&#21644;LIME&#65292;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#32780;&#20855;&#26377;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#20511;&#37492;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#27491;&#30830;&#25490;&#21517;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;RankSHAP&#20445;&#35777;$K$&#20010;&#26368;&#39640;Shapley&#20540;&#20855;&#26377;&#36229;&#36807;$1-\alpha$&#30340;&#27491;&#30830;&#25490;&#24207;&#27010;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#20026;LIME&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#30830;&#20445;&#20197;&#27491;&#30830;&#39034;&#24207;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attributions are ubiquitous tools for understanding the predictions of machine learning models. However, popular methods for scoring input variables such as SHAP and LIME suffer from high instability due to random sampling. Leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley values have the proper ordering with probability exceeding $1-\alpha$. Empirical results demonstrate its validity and impressive computational efficiency. We also build on previous work to yield similar results for LIME, ensuring the most important features are selected in the right order.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#26657;&#20934;&#21644;&#36951;&#25022;&#22312;&#35780;&#20272;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#31561;&#20215;&#24615;&#65292;&#23558;&#35780;&#20272;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39044;&#27979;&#32773;&#12289;&#19968;&#20010;&#36172;&#24466;&#21644;&#33258;&#28982;&#20043;&#38388;&#30340;&#21338;&#24328;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#35780;&#20272;&#19982;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2401.14483</link><description>&lt;p&gt;
&#39044;&#27979;&#30340;&#22235;&#20010;&#26041;&#38754;&#65306;&#26657;&#20934;&#12289;&#39044;&#27979;&#24615;&#12289;&#38543;&#26426;&#24615;&#21644;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Four Facets of Forecast Felicity: Calibration, Predictiveness, Randomness and Regret. (arXiv:2401.14483v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#26657;&#20934;&#21644;&#36951;&#25022;&#22312;&#35780;&#20272;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#31561;&#20215;&#24615;&#65292;&#23558;&#35780;&#20272;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39044;&#27979;&#32773;&#12289;&#19968;&#20010;&#36172;&#24466;&#21644;&#33258;&#28982;&#20043;&#38388;&#30340;&#21338;&#24328;&#65292;&#24182;&#23558;&#39044;&#27979;&#30340;&#35780;&#20272;&#19982;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#20851;&#20110;&#39044;&#27979;&#30340;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#21482;&#26377;&#32463;&#36807;&#35780;&#20272;&#21518;&#25165;&#20855;&#26377;&#20854;&#26377;&#29992;&#24615;&#12290;&#26426;&#22120;&#23398;&#20064;&#20256;&#32479;&#19978;&#20851;&#27880;&#25439;&#22833;&#31867;&#22411;&#21450;&#20854;&#30456;&#24212;&#30340;&#36951;&#25022;&#12290;&#30446;&#21069;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#37325;&#26032;&#23545;&#26657;&#20934;&#20135;&#29983;&#20102;&#20852;&#36259;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26657;&#20934;&#21644;&#36951;&#25022;&#22312;&#35780;&#20272;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#31561;&#20215;&#24615;&#12290;&#25105;&#20204;&#23558;&#35780;&#20272;&#38382;&#39064;&#26500;&#24314;&#20026;&#19968;&#20010;&#39044;&#27979;&#32773;&#12289;&#19968;&#20010;&#36172;&#24466;&#21644;&#33258;&#28982;&#20043;&#38388;&#30340;&#21338;&#24328;&#12290;&#36890;&#36807;&#23545;&#36172;&#24466;&#21644;&#39044;&#27979;&#32773;&#26045;&#21152;&#30452;&#35266;&#30340;&#38480;&#21046;&#65292;&#26657;&#20934;&#21644;&#36951;&#25022;&#33258;&#28982;&#22320;&#25104;&#20026;&#20102;&#36825;&#20010;&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#21338;&#24328;&#23558;&#39044;&#27979;&#30340;&#35780;&#20272;&#19982;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#32852;&#31995;&#36215;&#26469;&#12290;&#30456;&#23545;&#20110;&#39044;&#27979;&#32780;&#35328;&#65292;&#32467;&#26524;&#30340;&#38543;&#26426;&#24615;&#31561;&#21516;&#20110;&#20851;&#20110;&#32467;&#26524;&#30340;&#22909;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#31216;&#36825;&#20004;&#20010;&#26041;&#38754;&#20026;&#26657;&#20934;&#21644;&#36951;&#25022;&#12289;&#39044;&#27979;&#24615;&#21644;&#38543;&#26426;&#24615;&#65292;&#21363;&#39044;&#27979;&#30340;&#22235;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is about forecasting. Forecasts, however, obtain their usefulness only through their evaluation. Machine learning has traditionally focused on types of losses and their corresponding regret. Currently, the machine learning community regained interest in calibration. In this work, we show the conceptual equivalence of calibration and regret in evaluating forecasts. We frame the evaluation problem as a game between a forecaster, a gambler and nature. Putting intuitive restrictions on gambler and forecaster, calibration and regret naturally fall out of the framework. In addition, this game links evaluation of forecasts to randomness of outcomes. Random outcomes with respect to forecasts are equivalent to good forecasts with respect to outcomes. We call those dual aspects, calibration and regret, predictiveness and randomness, the four facets of forecast felicity.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.09596</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#21147;Transformer&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Efficient generative adversarial networks using linear additive-attention Transformers. (arXiv:2401.09596v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09596
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LadaGAN&#30340;&#39640;&#25928;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#20351;&#29992;&#20102;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#65292;&#36890;&#36807;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#35299;&#20915;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#31561;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#36817;&#24180;&#26469;&#24471;&#21040;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#25104;&#21151;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#35745;&#31639;&#22797;&#26434;&#30340;&#26550;&#26500;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#30740;&#31350;&#23454;&#39564;&#23460;&#21644;&#36164;&#28304;&#20805;&#36275;&#30340;&#20844;&#21496;&#20013;&#30340;&#37319;&#29992;&#21644;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26497;&#22823;&#22320;&#22686;&#21152;&#20102;&#35757;&#32451;&#12289;&#24494;&#35843;&#21644;&#25512;&#29702;&#30340;&#30899;&#36275;&#36857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LadaGAN&#65292;&#36825;&#26159;&#19968;&#20010;&#39640;&#25928;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#23427;&#24314;&#31435;&#22312;&#19968;&#31181;&#21517;&#20026;Ladaformer&#30340;&#26032;&#22411;Transformer&#22359;&#19978;&#12290;&#35813;&#22359;&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#19968;&#20010;&#32447;&#24615;&#21152;&#27861;&#27880;&#24847;&#26426;&#21046;&#65292;&#23427;&#27599;&#20010;&#22836;&#37096;&#35745;&#31639;&#19968;&#20010;&#27880;&#24847;&#21521;&#37327;&#65292;&#32780;&#19981;&#26159;&#20108;&#27425;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#22312;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20013;&#37117;&#37319;&#29992;&#20102;Ladaformer&#65292;&#36825;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#20811;&#26381;&#20102;Transformer GAN&#32463;&#24120;&#20986;&#29616;&#30340;&#35757;&#32451;&#19981;&#31283;&#23450;&#24615;&#12290;LadaGAN&#19968;&#30452;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;GANs&#12290;
&lt;/p&gt;
&lt;p&gt;
Although the capacity of deep generative models for image generation, such as Diffusion Models (DMs) and Generative Adversarial Networks (GANs), has dramatically improved in recent years, much of their success can be attributed to computationally expensive architectures. This has limited their adoption and use to research laboratories and companies with large resources, while significantly raising the carbon footprint for training, fine-tuning, and inference. In this work, we present LadaGAN, an efficient generative adversarial network that is built upon a novel Transformer block named Ladaformer. The main component of this block is a linear additive-attention mechanism that computes a single attention vector per head instead of the quadratic dot-product attention. We employ Ladaformer in both the generator and discriminator, which reduces the computational complexity and overcomes the training instabilities often associated with Transformer GANs. LadaGAN consistently outperforms exist
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;DKFs&#65289;&#65292;&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#19968;&#31867;&#38750;&#39532;&#23572;&#21487;&#22827;&#21644;&#26465;&#20214;&#39640;&#26031;&#20449;&#21495;&#36807;&#31243;&#30340;&#26465;&#20214;&#20998;&#24067;&#24459;&#65292;&#20174;&#32780;&#20855;&#26377;&#22312;&#25968;&#23398;&#37329;&#34701;&#39046;&#22495;&#20013;&#20256;&#32479;&#27169;&#22411;&#22522;&#30784;&#19978;&#30340;&#28388;&#27874;&#38382;&#39064;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19603</link><description>&lt;p&gt;
&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21487;&#20197;&#36827;&#34892;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Deep Kalman Filters Can Filter. (arXiv:2310.19603v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;DKFs&#65289;&#65292;&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#19968;&#31867;&#38750;&#39532;&#23572;&#21487;&#22827;&#21644;&#26465;&#20214;&#39640;&#26031;&#20449;&#21495;&#36807;&#31243;&#30340;&#26465;&#20214;&#20998;&#24067;&#24459;&#65292;&#20174;&#32780;&#20855;&#26377;&#22312;&#25968;&#23398;&#37329;&#34701;&#39046;&#22495;&#20013;&#20256;&#32479;&#27169;&#22411;&#22522;&#30784;&#19978;&#30340;&#28388;&#27874;&#38382;&#39064;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;DKFs&#65289;&#26159;&#19968;&#31867;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#39640;&#26031;&#27010;&#29575;&#27979;&#24230;&#12290;&#34429;&#28982;DKFs&#21463;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#21551;&#21457;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#19982;&#38543;&#26426;&#28388;&#27874;&#38382;&#39064;&#30340;&#20855;&#20307;&#29702;&#35770;&#20851;&#32852;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20256;&#32479;&#27169;&#22411;&#22522;&#30784;&#19978;&#30340;&#28388;&#27874;&#38382;&#39064;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#20538;&#21048;&#21644;&#26399;&#26435;&#23450;&#20215;&#27169;&#22411;&#26657;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;DKFs&#65292;&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#19968;&#31867;&#38750;&#39532;&#23572;&#21487;&#22827;&#21644;&#26465;&#20214;&#39640;&#26031;&#20449;&#21495;&#36807;&#31243;&#30340;&#26465;&#20214;&#20998;&#24067;&#24459;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#25968;&#23398;&#22522;&#30784;&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36817;&#20284;&#32467;&#26524;&#22312;&#36335;&#24452;&#30340;&#36275;&#22815;&#35268;&#21017;&#30340;&#32039;&#33268;&#23376;&#38598;&#19978;&#19968;&#33268;&#25104;&#31435;&#65292;&#20854;&#20013;&#36817;&#20284;&#35823;&#24046;&#30001;&#22312;&#32473;&#23450;&#32039;&#33268;&#36335;&#24452;&#38598;&#19978;&#22343;&#19968;&#22320;&#35745;&#31639;&#30340;&#26368;&#22351;&#24773;&#20917;2-Wasserstein&#36317;&#31163;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Kalman filters (DKFs) are a class of neural network models that generate Gaussian probability measures from sequential data. Though DKFs are inspired by the Kalman filter, they lack concrete theoretical ties to the stochastic filtering problem, thus limiting their applicability to areas where traditional model-based filters have been used, e.g.\ model calibration for bond and option prices in mathematical finance. We address this issue in the mathematical foundations of deep learning by exhibiting a class of continuous-time DKFs which can approximately implement the conditional law of a broad class of non-Markovian and conditionally Gaussian signal processes given noisy continuous-times measurements. Our approximation results hold uniformly over sufficiently regular compact subsets of paths, where the approximation error is quantified by the worst-case 2-Wasserstein distance computed uniformly over the given compact set of paths.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21270;&#26080;&#27169;&#22411;&#31639;&#27861;RandQL&#65292;&#29992;&#20110;&#20943;&#23567;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36951;&#25022;&#12290;RandQL&#36890;&#36807;&#23398;&#20064;&#29575;&#38543;&#26426;&#21270;&#23454;&#29616;&#20048;&#35266;&#25506;&#32034;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.18186</link><description>&lt;p&gt;
&#26080;&#27169;&#22411;&#21518;&#39564;&#37319;&#26679;&#30340;&#27169;&#22411;&#33258;&#30001;&#38543;&#26426;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Model-free Posterior Sampling via Learning Rate Randomization. (arXiv:2310.18186v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38543;&#26426;&#21270;&#26080;&#27169;&#22411;&#31639;&#27861;RandQL&#65292;&#29992;&#20110;&#20943;&#23567;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#36951;&#25022;&#12290;RandQL&#36890;&#36807;&#23398;&#20064;&#29575;&#38543;&#26426;&#21270;&#23454;&#29616;&#20048;&#35266;&#25506;&#32034;&#65292;&#24182;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21270;&#26080;&#27169;&#22411;&#31639;&#27861;&#65292;Randomized Q-learning&#65288;&#31616;&#31216;RandQL&#65289;&#65292;&#29992;&#20110;&#20943;&#23567;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#30340;&#36951;&#25022;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;RandQL&#26159;&#31532;&#19968;&#20010;&#21487;&#34892;&#30340;&#27169;&#22411;&#33258;&#30001;&#21518;&#39564;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;RandQL&#22312;&#34920;&#26684;&#21644;&#38750;&#34920;&#26684;&#24230;&#37327;&#31354;&#38388;&#35774;&#32622;&#19979;&#30340;&#24615;&#33021;&#12290;&#22312;&#34920;&#26684;MDPs&#20013;&#65292;RandQL&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#30340;&#39034;&#24207;&#20026;$\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})$&#65292;&#20854;&#20013;$H$&#26159;&#35745;&#21010;&#30340;&#26102;&#38388;&#38271;&#24230;&#65292;$S$&#26159;&#29366;&#24577;&#25968;&#65292;$A$&#26159;&#21160;&#20316;&#25968;&#65292;$T$&#26159;&#22238;&#21512;&#25968;&#12290;&#23545;&#20110;&#24230;&#37327;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#65292;RandQL&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#30340;&#39034;&#24207;&#20026;$\widetilde{\mathcal{O}}(H^{5/2} T^{(d_z+1)/(d_z+2)})$&#65292;&#20854;&#20013;$d_z$&#34920;&#31034;&#32553;&#25918;&#32500;&#24230;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;RandQL&#23454;&#29616;&#20102;&#20048;&#35266;&#25506;&#32034;&#65292;&#32780;&#19981;&#20351;&#29992;&#22870;&#21169;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#23398;&#20064;&#29575;&#38543;&#26426;&#21270;&#30340;&#26032;&#24605;&#24819;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;RandQL&#22312;&#22522;&#32447;&#25506;&#32034;&#19978;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce Randomized Q-learning (RandQL), a novel randomized model-free algorithm for regret minimization in episodic Markov Decision Processes (MDPs). To the best of our knowledge, RandQL is the first tractable model-free posterior sampling-based algorithm. We analyze the performance of RandQL in both tabular and non-tabular metric space settings. In tabular MDPs, RandQL achieves a regret bound of order $\widetilde{\mathcal{O}}(\sqrt{H^{5}SAT})$, where $H$ is the planning horizon, $S$ is the number of states, $A$ is the number of actions, and $T$ is the number of episodes. For a metric state-action space, RandQL enjoys a regret bound of order $\widetilde{\mathcal{O}}(H^{5/2} T^{(d_z+1)/(d_z+2)})$, where $d_z$ denotes the zooming dimension. Notably, RandQL achieves optimistic exploration without using bonuses, relying instead on a novel idea of learning rate randomization. Our empirical study shows that RandQL outperforms existing approaches on baseline exploration en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;</title><link>http://arxiv.org/abs/2310.13103</link><description>&lt;p&gt;
AVTENet: &#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AVTENet: Audio-Visual Transformer-based Ensemble Network Exploiting Multiple Experts for Video Deepfake Detection. (arXiv:2310.13103v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AVTENet&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#22522;&#20110;&#38899;&#39057;-&#35270;&#35273;Transformer&#30340;&#22810;&#19987;&#23478;&#38598;&#25104;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#20013;&#32771;&#34385;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#24191;&#27867;&#20998;&#20139;&#30340;&#20266;&#36896;&#20869;&#23481;&#26159;&#19968;&#20010;&#37325;&#22823;&#31038;&#20250;&#38382;&#39064;&#65292;&#35201;&#27714;&#21152;&#24378;&#30417;&#31649;&#24182;&#32473;&#30740;&#31350;&#31038;&#21306;&#24102;&#26469;&#26032;&#30340;&#25361;&#25112;&#12290;&#36817;&#24180;&#26469;&#65292;&#36229;&#30495;&#23454;&#30340;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#30340;&#26222;&#21450;&#24341;&#36215;&#20102;&#23545;&#38899;&#39057;&#21644;&#35270;&#35273;&#20266;&#36896;&#23041;&#32961;&#30340;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#20851;&#20110;&#26816;&#27979;AI&#29983;&#25104;&#30340;&#20266;&#36896;&#35270;&#39057;&#30340;&#20808;&#21069;&#24037;&#20316;&#21482;&#21033;&#29992;&#20102;&#35270;&#35273;&#27169;&#24577;&#25110;&#38899;&#39057;&#27169;&#24577;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#26377;&#19968;&#20123;&#26041;&#27861;&#21033;&#29992;&#38899;&#39057;&#21644;&#35270;&#35273;&#27169;&#24577;&#26469;&#26816;&#27979;&#20266;&#36896;&#35270;&#39057;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#22312;&#28041;&#21450;&#22768;&#23398;&#21644;&#35270;&#35273;&#25805;&#20316;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#20266;&#36896;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#22522;&#20110;CNN&#65292;&#24182;&#19988;&#26816;&#27979;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#21463;&#21040;Transformer&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#26368;&#26032;&#25104;&#21151;&#21551;&#21457;&#65292;&#20026;&#20102;&#35299;&#20915;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#22768;&#23398;&#25805;&#20316;&#30340;&#38899;&#39057;-&#35270;&#35273;Transformer&#38598;&#25104;&#32593;&#32476;&#65288;AVTENet&#65289;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forged content shared widely on social media platforms is a major social problem that requires increased regulation and poses new challenges to the research community. The recent proliferation of hyper-realistic deepfake videos has drawn attention to the threat of audio and visual forgeries. Most previous work on detecting AI-generated fake videos only utilizes visual modality or audio modality. While there are some methods in the literature that exploit audio and visual modalities to detect forged videos, they have not been comprehensively evaluated on multi-modal datasets of deepfake videos involving acoustic and visual manipulations. Moreover, these existing methods are mostly based on CNN and suffer from low detection accuracy. Inspired by the recent success of Transformer in various fields, to address the challenges posed by deepfake technology, in this paper, we propose an Audio-Visual Transformer-based Ensemble Network (AVTENet) framework that considers both acoustic manipulatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38543;&#26426;Metropolis-Hastings&#31639;&#27861;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#20462;&#27491;&#39033;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#35745;&#31639;&#25104;&#26412;&#19978;&#30340;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#38750;&#21442;&#25968;&#22238;&#24402;&#24773;&#26223;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#30340;&#25968;&#20540;&#23454;&#20363;&#26469;&#35777;&#26126;&#20102;&#20854;&#22312;&#37319;&#26679;&#21644;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.09335</link><description>&lt;p&gt;
&#38024;&#23545;&#38543;&#26426;Metropolis-Hastings&#31639;&#27861;&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical guarantees for stochastic Metropolis-Hastings. (arXiv:2310.09335v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09335
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#38543;&#26426;Metropolis-Hastings&#31639;&#27861;&#30340;&#32479;&#35745;&#20445;&#35777;&#12290;&#36890;&#36807;&#24341;&#20837;&#31616;&#21333;&#30340;&#20462;&#27491;&#39033;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#36991;&#20813;&#35745;&#31639;&#25104;&#26412;&#19978;&#30340;&#25439;&#22833;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#38750;&#21442;&#25968;&#22238;&#24402;&#24773;&#26223;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#30340;&#25968;&#20540;&#23454;&#20363;&#26469;&#35777;&#26126;&#20102;&#20854;&#22312;&#37319;&#26679;&#21644;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metropolis-Hastings&#27493;&#39588;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#12290;&#36890;&#36807;&#23545;&#25209;&#27425;&#35745;&#31639;&#25509;&#21463;&#27010;&#29575;&#65292;&#38543;&#26426;Metropolis-Hastings&#27493;&#39588;&#33410;&#30465;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#20294;&#38477;&#20302;&#20102;&#26377;&#25928;&#26679;&#26412;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#31616;&#21333;&#30340;&#20462;&#27491;&#39033;&#21487;&#20197;&#36991;&#20813;&#36825;&#20010;&#38556;&#30861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#26524;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#35774;&#32622;&#20013;&#24212;&#29992;&#25913;&#36827;&#30340;&#38543;&#26426;Metropolis-Hastings&#26041;&#27861;&#20174;Gibbs&#21518;&#39564;&#20998;&#24067;&#20013;&#37319;&#26679;&#65292;&#21017;&#38142;&#30340;&#32467;&#26524;&#31283;&#24577;&#20998;&#24067;&#30340;&#32479;&#35745;&#23646;&#24615;&#12290;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;PAC-Bayes&#39044;&#35328;&#19981;&#31561;&#24335;&#65292;&#23427;&#25552;&#20379;&#20102;&#26368;&#20248;&#30340;&#25910;&#32553;&#36895;&#29575;&#65292;&#24182;&#20998;&#26512;&#20102;&#32467;&#26524;&#21487;&#20449;&#21306;&#38388;&#30340;&#30452;&#24452;&#21644;&#39640;&#32622;&#20449;&#27010;&#29575;&#12290;&#36890;&#36807;&#22312;&#39640;&#32500;&#21442;&#25968;&#31354;&#38388;&#20013;&#30340;&#25968;&#20540;&#23454;&#20363;&#65292;&#25105;&#20204;&#35828;&#26126;&#20102;&#38543;&#26426;Metropolis-Hastings&#31639;&#27861;&#30340;&#21487;&#20449;&#21306;&#38388;&#21644;&#25910;&#32553;&#36895;&#29575;&#30830;&#23454;&#34920;&#29616;&#20986;&#31867;&#20284;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Metropolis-Hastings step is widely used for gradient-based Markov chain Monte Carlo methods in uncertainty quantification. By calculating acceptance probabilities on batches, a stochastic Metropolis-Hastings step saves computational costs, but reduces the effective sample size. We show that this obstacle can be avoided by a simple correction term. We study statistical properties of the resulting stationary distribution of the chain if the corrected stochastic Metropolis-Hastings approach is applied to sample from a Gibbs posterior distribution in a nonparametric regression setting. Focusing on deep neural network regression, we prove a PAC-Bayes oracle inequality which yields optimal contraction rates and we analyze the diameter and show high coverage probability of the resulting credible sets. With a numerical example in a high-dimensional parameter space, we illustrate that credible sets and contraction rates of the stochastic Metropolis-Hastings algorithm indeed behave similar to 
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#26469;&#35745;&#31639;&#33539;&#25968;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#33539;&#22260;&#12290;&#24212;&#29992;&#20110;&#19981;&#21516;&#23618;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2309.06577</link><description>&lt;p&gt;
&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Finite Initialization for Tensorized Neural Networks. (arXiv:2309.06577v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06577
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26377;&#38480;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#26041;&#27861;&#65292;&#36991;&#20813;&#20102;&#21442;&#25968;&#29190;&#28856;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#26469;&#35745;&#31639;&#33539;&#25968;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#33539;&#22260;&#12290;&#24212;&#29992;&#20110;&#19981;&#21516;&#23618;&#30340;&#23454;&#39564;&#34920;&#26126;&#20854;&#24615;&#33021;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21021;&#22987;&#21270;&#24352;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#65292;&#20197;&#36991;&#20813;&#21442;&#25968;&#29190;&#28856;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#33410;&#28857;&#30340;&#23618;&#65292;&#20854;&#20013;&#25152;&#26377;&#25110;&#22823;&#22810;&#25968;&#33410;&#28857;&#19982;&#36755;&#20837;&#25110;&#36755;&#20986;&#26377;&#36830;&#25509;&#12290;&#35813;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#20351;&#29992;&#35813;&#23618;&#30340;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#30340;&#36845;&#20195;&#37096;&#20998;&#24418;&#24335;&#65292;&#20351;&#20854;&#20855;&#26377;&#26377;&#38480;&#30340;&#33539;&#22260;&#12290;&#36825;&#20010;&#33539;&#25968;&#30340;&#35745;&#31639;&#26159;&#39640;&#25928;&#30340;&#65292;&#23545;&#20110;&#22823;&#22810;&#25968;&#24773;&#20917;&#37117;&#21487;&#20197;&#23436;&#20840;&#25110;&#37096;&#20998;&#35745;&#31639;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#23618;&#65292;&#24182;&#26816;&#26597;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;Python&#20989;&#25968;&#65292;&#22312;i3BQuantum&#23384;&#20648;&#24211;&#30340;Jupyter Notebook&#20013;&#21487;&#20197;&#36816;&#34892;&#23427;&#65306;https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
&lt;/p&gt;
&lt;p&gt;
We present a novel method for initializing layers of tensorized neural networks in a way that avoids the explosion of the parameters of the matrix it emulates. The method is intended for layers with a high number of nodes in which there is a connection to the input or output of all or most of the nodes. The core of this method is the use of the Frobenius norm of this layer in an iterative partial form, so that it has to be finite and within a certain range. This norm is efficient to compute, fully or partially for most cases of interest. We apply the method to different layers and check its performance. We create a Python function to run it on an arbitrary layer, available in a Jupyter Notebook in the i3BQuantum repository: https://github.com/i3BQuantumTeam/Q4Real/blob/e07c827651ef16bcf74590ab965ea3985143f891/Quantum-Inspired%20Variational%20Methods/Normalization_process.ipynb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Tukey&#28145;&#24230;&#30340;&#38543;&#26426;&#36817;&#20284;&#36136;&#37327;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#32500;&#24230;&#36739;&#39640;&#19988;&#25968;&#25454;&#20174;&#23545;&#25968;&#20985;&#38598;&#30340;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#36817;&#20284;&#26368;&#22823;&#28145;&#24230;&#21644;&#25509;&#36817;&#38646;&#30340;&#28145;&#24230;&#65292;&#32780;&#23545;&#20110;&#20013;&#38388;&#28145;&#24230;&#30340;&#28857;&#65292;&#20219;&#20309;&#22909;&#30340;&#36817;&#20284;&#37117;&#38656;&#35201;&#25351;&#25968;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.05657</link><description>&lt;p&gt;
&#20851;&#20110;Tukey&#28145;&#24230;&#30340;&#38543;&#26426;&#36817;&#20284;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
On the quality of randomized approximations of Tukey's depth. (arXiv:2309.05657v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Tukey&#28145;&#24230;&#30340;&#38543;&#26426;&#36817;&#20284;&#36136;&#37327;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#32500;&#24230;&#36739;&#39640;&#19988;&#25968;&#25454;&#20174;&#23545;&#25968;&#20985;&#38598;&#30340;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#38543;&#26426;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#36817;&#20284;&#26368;&#22823;&#28145;&#24230;&#21644;&#25509;&#36817;&#38646;&#30340;&#28145;&#24230;&#65292;&#32780;&#23545;&#20110;&#20013;&#38388;&#28145;&#24230;&#30340;&#28857;&#65292;&#20219;&#20309;&#22909;&#30340;&#36817;&#20284;&#37117;&#38656;&#35201;&#25351;&#25968;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Tukey&#28145;&#24230;&#65288;&#25110;&#21322;&#31354;&#38388;&#28145;&#24230;&#65289;&#26159;&#29992;&#20110;&#22810;&#20803;&#25968;&#25454;&#20013;&#24515;&#24230;&#37327;&#30340;&#24191;&#27867;&#24212;&#29992;&#30340;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#24230;&#19979;&#65292;Tukey&#28145;&#24230;&#30340;&#31934;&#30830;&#35745;&#31639;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;Tukey&#28145;&#24230;&#30340;&#38543;&#26426;&#36817;&#20284;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#26679;&#30340;&#38543;&#26426;&#31639;&#27861;&#20309;&#26102;&#33021;&#22815;&#36820;&#22238;&#19968;&#20010;&#33391;&#22909;&#30340;Tukey&#28145;&#24230;&#36817;&#20284;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#20174;&#23545;&#25968;&#20985;&#38519;&#22343;&#21248;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#22914;&#26524;&#35201;&#27714;&#31639;&#27861;&#22312;&#32500;&#24230;&#19978;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#36816;&#34892;&#65292;&#38543;&#26426;&#31639;&#27861;&#21487;&#20197;&#27491;&#30830;&#22320;&#36817;&#20284;&#26368;&#22823;&#28145;&#24230;1/2&#21644;&#25509;&#36817;&#38646;&#30340;&#28145;&#24230;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23545;&#20110;&#20219;&#20309;&#20013;&#38388;&#28145;&#24230;&#30340;&#28857;&#65292;&#20219;&#20309;&#22909;&#30340;&#36817;&#20284;&#37117;&#38656;&#35201;&#25351;&#25968;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tukey's depth (or halfspace depth) is a widely used measure of centrality for multivariate data. However, exact computation of Tukey's depth is known to be a hard problem in high dimensions. As a remedy, randomized approximations of Tukey's depth have been proposed. In this paper we explore when such randomized algorithms return a good approximation of Tukey's depth. We study the case when the data are sampled from a log-concave isotropic distribution. We prove that, if one requires that the algorithm runs in polynomial time in the dimension, the randomized algorithm correctly approximates the maximal depth $1/2$ and depths close to zero. On the other hand, for any point of intermediate depth, any good approximation requires exponential complexity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2306.09519</link><description>&lt;p&gt;
&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Relation-Aware Network with Attention-Based Loss for Few-Shot Knowledge Graph Completion. (arXiv:2306.09519v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;RANA&#26694;&#26550;&#65292;&#21033;&#29992;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#65292;&#21516;&#26102;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#33719;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23567;&#26679;&#26412;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#26088;&#22312;&#21033;&#29992;&#23569;&#37327;&#21442;&#32771;&#23454;&#20307;&#23545;&#39044;&#27979;&#20851;&#31995;&#30340;&#26410;&#35265;&#20107;&#23454;&#12290;&#29616;&#26377;&#26041;&#27861;&#38543;&#26426;&#36873;&#25321;&#19968;&#20010;&#36127;&#37319;&#26679;&#26469;&#26368;&#23567;&#21270;&#22522;&#20110;&#36793;&#30028;&#30340;&#25490;&#21517;&#25439;&#22833;&#65292;&#20294;&#36825;&#23481;&#26131;&#23548;&#33268;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23454;&#20307;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#24212;&#35813;&#20855;&#26377;&#19981;&#21516;&#30340;&#34920;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20851;&#31995;&#24863;&#30693;&#32593;&#32476;&#22522;&#20110;&#27880;&#24847;&#21147;&#25439;&#22833;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#26377;&#31574;&#30053;&#22320;&#36873;&#25321;&#30456;&#20851;&#36127;&#26679;&#26412;&#21644;&#35774;&#35745;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#26356;&#22909;&#22320;&#21033;&#29992;&#20016;&#23500;&#30340;&#36127;&#26679;&#26412;&#24182;&#32531;&#35299;&#38646;&#25439;&#22833;&#38382;&#39064;&#12290;&#30452;&#35273;&#19978;&#65292;&#19982;&#27491;&#26679;&#26412;&#26356;&#30456;&#20284;&#30340;&#36127;&#26679;&#26412;&#23558;&#23545;&#27169;&#22411;&#36129;&#29486;&#26356;&#22823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21160;&#24577;&#30340;&#20851;&#31995;&#24863;&#30693;&#23454;&#20307;&#32534;&#30721;&#26469;&#25429;&#25417;&#19981;&#21516;&#20851;&#31995;&#19979;&#23454;&#20307;&#30340;&#19981;&#21516;&#34920;&#31034;&#12290;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;RANA&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot knowledge graph completion (FKGC) task aims to predict unseen facts of a relation with few-shot reference entity pairs. Current approaches randomly select one negative sample for each reference entity pair to minimize a margin-based ranking loss, which easily leads to a zero-loss problem if the negative sample is far away from the positive sample and then out of the margin. Moreover, the entity should have a different representation under a different context. To tackle these issues, we propose a novel Relation-Aware Network with Attention-Based Loss (RANA) framework. Specifically, to better utilize the plentiful negative samples and alleviate the zero-loss issue, we strategically select relevant negative samples and design an attention-based loss function to further differentiate the importance of each negative sample. The intuition is that negative samples more similar to positive samples will contribute more to the model. Further, we design a dynamic relation-aware entity en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;Wasserstein&#32858;&#31867;&#65292;&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#26426;&#26500;&#30340;&#22797;&#26434;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#32570;&#22833;&#20540;&#21644;&#22522;&#20110;&#29305;&#23450;&#29305;&#24449;&#35782;&#21035;&#32858;&#31867;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#30417;&#31649;&#32773;&#30340;&#30417;&#31649;&#24037;&#20316;&#65292;&#24182;&#22312;&#20854;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.03565</link><description>&lt;p&gt;
&#37329;&#34701;&#26426;&#26500;&#30340;&#20960;&#20309;&#24418;&#24577;--&#37329;&#34701;&#25968;&#25454;&#30340;Wasserstein&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
The geometry of financial institutions -- Wasserstein clustering of financial data. (arXiv:2305.03565v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03565
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;Wasserstein&#32858;&#31867;&#65292;&#29992;&#20110;&#22788;&#29702;&#37329;&#34701;&#26426;&#26500;&#30340;&#22797;&#26434;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#32570;&#22833;&#20540;&#21644;&#22522;&#20110;&#29305;&#23450;&#29305;&#24449;&#35782;&#21035;&#32858;&#31867;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#35813;&#31639;&#27861;&#21487;&#29992;&#20110;&#30417;&#31649;&#32773;&#30340;&#30417;&#31649;&#24037;&#20316;&#65292;&#24182;&#22312;&#20854;&#39046;&#22495;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#26029;&#22686;&#21152;&#30340;&#21508;&#31181;&#26377;&#36259;&#23545;&#35937;&#30340;&#32454;&#33410;&#21644;&#22823;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20351;&#24471;&#26377;&#24517;&#35201;&#24320;&#21457;&#23558;&#36825;&#20123;&#20449;&#24687;&#21387;&#32553;&#25104;&#20195;&#34920;&#24615;&#21644;&#21487;&#29702;&#35299;&#30340;&#22320;&#22270;&#30340;&#26041;&#27861;&#12290;&#37329;&#34701;&#30417;&#31649;&#26159;&#19968;&#20010;&#23637;&#31034;&#36825;&#31181;&#38656;&#27714;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#30417;&#31649;&#26426;&#26500;&#38656;&#35201;&#20174;&#37329;&#34701;&#26426;&#26500;&#33719;&#21462;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#65292;&#26377;&#26102;&#26159;&#39640;&#24230;&#32454;&#31890;&#24230;&#30340;&#65292;&#20197;&#30417;&#30563;&#21644;&#35780;&#20272;&#20182;&#20204;&#30340;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#21644;&#20998;&#26512;&#36825;&#26679;&#30340;&#25968;&#25454;&#21487;&#33021;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#22788;&#29702;&#32570;&#22833;&#20540;&#21644;&#22522;&#20110;&#29305;&#23450;&#29305;&#24449;&#35782;&#21035;&#32858;&#31867;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#27010;&#29575;&#20998;&#24067;&#30340;Lloyd&#31639;&#27861;&#21464;&#20307;&#65292;&#24182;&#20351;&#29992;&#24191;&#20041;Wasserstein&#37325;&#24515;&#26500;&#24314;&#34920;&#31034;&#19981;&#21516;&#23545;&#35937;&#19978;&#30340;&#32473;&#23450;&#25968;&#25454;&#30340;&#24230;&#37327;&#31354;&#38388;&#65292;&#20174;&#32780;&#24212;&#23545;&#37329;&#34701;&#30417;&#31649;&#32972;&#26223;&#19979;&#30417;&#31649;&#32773;&#38754;&#20020;&#30340;&#20855;&#20307;&#25361;&#25112;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#31181;&#26041;&#27861;&#22312;&#37329;&#34701;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of granular and big data on various objects of interest has made it necessary to develop methods for condensing this information into a representative and intelligible map. Financial regulation is a field that exemplifies this need, as regulators require diverse and often highly granular data from financial institutions to monitor and assess their activities. However, processing and analyzing such data can be a daunting task, especially given the challenges of dealing with missing values and identifying clusters based on specific features.  To address these challenges, we propose a variant of Lloyd's algorithm that applies to probability distributions and uses generalized Wasserstein barycenters to construct a metric space which represents given data on various objects in condensed form. By applying our method to the financial regulation context, we demonstrate its usefulness in dealing with the specific challenges faced by regulators in this domain. We beli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#29992;&#20110;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20445;&#23432;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#31616;&#21270;&#21518;&#30340;&#32593;&#32476;&#39564;&#35777;&#19982;&#21407;&#32593;&#32476;&#39564;&#35777;&#27966;&#29983;&#31561;&#20215;&#12290;&#31616;&#21270;&#21518;&#21487;&#23558;&#32593;&#32476;&#20943;&#23569;&#21040;&#23567;&#20110;5&#65285;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#30456;&#24212;&#30340;&#39564;&#35777;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.01932</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Specification-Driven Neural Network Reduction for Scalable Formal Verification. (arXiv:2305.01932v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35268;&#33539;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#29992;&#20110;&#22823;&#35268;&#27169;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;&#20445;&#23432;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#31616;&#21270;&#21518;&#30340;&#32593;&#32476;&#39564;&#35777;&#19982;&#21407;&#32593;&#32476;&#39564;&#35777;&#27966;&#29983;&#31561;&#20215;&#12290;&#31616;&#21270;&#21518;&#21487;&#23558;&#32593;&#32476;&#20943;&#23569;&#21040;&#23567;&#20110;5&#65285;&#30340;&#31070;&#32463;&#20803;&#25968;&#37327;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#30456;&#24212;&#30340;&#39564;&#35777;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#20013;&#37096;&#32626;&#20043;&#21069;&#65292;&#24418;&#24335;&#39564;&#35777;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#24335;&#39564;&#35777;&#26041;&#27861;&#36824;&#26080;&#27861;&#22788;&#29702;&#28041;&#21450;&#22823;&#37327;&#31070;&#32463;&#20803;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65306;&#20445;&#23432;&#30340;&#31070;&#32463;&#32593;&#32476;&#31616;&#21270;&#26041;&#27861;&#65292;&#30830;&#20445;&#31616;&#21270;&#21518;&#30340;&#32593;&#32476;&#39564;&#35777;&#27966;&#29983;&#20986;&#21407;&#32593;&#32476;&#30340;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21516;&#26102;&#26500;&#36896;&#31616;&#21270;&#32593;&#32476;&#65292;&#39564;&#35777;&#21407;&#22987;&#32593;&#32476;&#21450;&#20854;&#35268;&#33539;&#12290;&#31616;&#21270;&#23558;&#25152;&#26377;&#36755;&#20986;&#30456;&#20284;&#30340;&#38750;&#32447;&#24615;&#23618;&#31070;&#32463;&#20803;&#21512;&#24182;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#20219;&#20309;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#22914;ReLU&#65292;sigmoid&#21644;tanh&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23558;&#32593;&#32476;&#20943;&#23569;&#21040;&#23567;&#20110;&#31070;&#32463;&#20803;&#25968;&#30340;5&#65285;&#65292;&#22240;&#27492;&#21487;&#20197;&#23558;&#39564;&#35777;&#26102;&#38388;&#30456;&#20284;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Formal verification of neural networks is essential before their deployment in safety-critical settings. However, existing methods for formally verifying neural networks are not yet scalable enough to handle practical problems that involve a large number of neurons. In this work, we propose a novel approach to address this challenge: A conservative neural network reduction approach that ensures that the verification of the reduced network implies the verification of the original network. Our approach constructs the reduction on-the-fly, while simultaneously verifying the original network and its specifications. The reduction merges all neurons of a nonlinear layer with similar outputs and is applicable to neural networks with any type of activation function such as ReLU, sigmoid, and tanh. Our evaluation shows that our approach can reduce a network to less than 5% of the number of neurons and thus to a similar degree the verification time is reduced.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2303.17765</link><description>&lt;p&gt;
&#23398;&#20064;&#30456;&#20284;&#30340;&#32447;&#24615;&#34920;&#31034;&#65306;&#36866;&#24212;&#24615;&#12289;&#26497;&#23567;&#21270;&#12289;&#20197;&#21450;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#28982;&#32780;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#27424;&#32570;&#12290;&#26412;&#25991;&#26088;&#22312;&#29702;&#35299;&#20174;&#20855;&#26377;&#30456;&#20284;&#20294;&#24182;&#38750;&#23436;&#20840;&#30456;&#21516;&#30340;&#32447;&#24615;&#34920;&#31034;&#30340;&#20219;&#21153;&#20013;&#23398;&#20064;&#65292;&#21516;&#26102;&#22788;&#29702;&#24322;&#24120;&#20540;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65292;&#36866;&#24212;&#30456;&#20284;&#24615;&#32467;&#26500;&#24182;&#23545;&#24322;&#24120;&#20540;&#20219;&#21153;&#20855;&#26377;&#31283;&#20581;&#24615;&#65292;&#36866;&#29992;&#20110;&#34920;&#31034;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#35774;&#32622;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#21333;&#20219;&#21153;&#25110;&#20165;&#30446;&#26631;&#23398;&#20064;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representation multi-task learning (MTL) and transfer learning (TL) have achieved tremendous success in practice. However, the theoretical understanding of these methods is still lacking. Most existing theoretical works focus on cases where all tasks share the same representation, and claim that MTL and TL almost always improve performance. However, as the number of tasks grow, assuming all tasks share the same representation is unrealistic. Also, this does not always match empirical findings, which suggest that a shared representation may not necessarily improve single-task or target-only learning performance. In this paper, we aim to understand how to learn from tasks with \textit{similar but not exactly the same} linear representations, while dealing with outlier tasks. We propose two algorithms that are \textit{adaptive} to the similarity structure and \textit{robust} to outlier tasks under both MTL and TL settings. Our algorithms outperform single-task or target-only learning when
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20351;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#23454;&#26102;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.01201</link><description>&lt;p&gt;
&#23884;&#20837;&#24335;&#31995;&#32479;&#23454;&#26102;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Uncertainty in Real-Time Semantic Segmentation on Embedded Systems. (arXiv:2301.01201v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30340;&#39044;&#27979;&#26041;&#27861;&#65292;&#33021;&#22815;&#23454;&#26102;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20351;&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#23454;&#26102;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#21644;&#20154;&#26426;&#20132;&#20114;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#38656;&#35201;&#23454;&#26102;&#39044;&#27979;&#33021;&#21147;&#12290;&#23454;&#26102;&#24212;&#29992;&#30340;&#25361;&#25112;&#34987;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25152;&#21152;&#21095;&#12290;&#34429;&#28982;&#36825;&#20123;&#24179;&#21488;&#19978;&#23454;&#26102;&#26041;&#27861;&#30340;&#24320;&#21457;&#24471;&#21040;&#20102;&#22686;&#21152;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#36275;&#22815;&#22320;&#32771;&#34385;&#21040;&#23384;&#22312;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#28145;&#23618;&#29305;&#24449;&#25552;&#21462;&#19982;&#36125;&#21494;&#26031;&#22238;&#24402;&#21644;&#21160;&#37327;&#20256;&#25773;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#39044;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#22312;&#23884;&#20837;&#24335;&#30828;&#20214;&#19978;&#23454;&#26102;&#20135;&#29983;&#26377;&#24847;&#20041;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application for semantic segmentation models in areas such as autonomous vehicles and human computer interaction require real-time predictive capabilities. The challenges of addressing real-time application is amplified by the need to operate on resource constrained hardware. Whilst development of real-time methods for these platforms has increased, these models are unable to sufficiently reason about uncertainty present. This paper addresses this by combining deep feature extraction from pre-trained models with Bayesian regression and moment propagation for uncertainty aware predictions. We demonstrate how the proposed method can yield meaningful uncertainty on embedded hardware in real-time whilst maintaining predictive performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24577;&#24341;&#23548;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#21644;&#22238;&#25253;&#65292;&#20197;&#21450;&#19982;&#26631;&#20934;&#20540;&#20989;&#25968;&#19981;&#21516;&#30340;&#20540;&#20998;&#24067;&#32467;&#26500;&#29305;&#24449;&#26469;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#31181;&#22312;&#32447;&#31639;&#27861;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2208.13125</link><description>&lt;p&gt;
&#36830;&#32493;&#25511;&#21046;&#30340;&#27491;&#24120;&#24341;&#23548;&#20998;&#24067;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Normality-Guided Distributional Reinforcement Learning for Continuous Control. (arXiv:2208.13125v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.13125
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#24577;&#24341;&#23548;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#21644;&#22238;&#25253;&#65292;&#20197;&#21450;&#19982;&#26631;&#20934;&#20540;&#20989;&#25968;&#19981;&#21516;&#30340;&#20540;&#20998;&#24067;&#32467;&#26500;&#29305;&#24449;&#26469;&#26356;&#26032;&#31574;&#30053;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#20004;&#31181;&#22312;&#32447;&#31639;&#27861;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#23398;&#20064;&#19968;&#20010;&#39044;&#27979;&#22238;&#25253;&#30340;&#22343;&#20540;&#27169;&#22411;&#65292;&#25110;&#20215;&#20540;&#20989;&#25968;&#65292;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(DRL)&#36890;&#36807;&#24314;&#27169;&#20540;&#20998;&#24067;&#32780;&#19981;&#20165;&#20165;&#26159;&#22343;&#20540;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#20010;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20540;&#20998;&#24067;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#30340;&#20540;&#20998;&#24067;&#19982;&#27491;&#24577;&#20998;&#24067;&#38750;&#24120;&#25509;&#36817;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21033;&#29992;&#36825;&#20010;&#24615;&#36136;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20174;&#26041;&#24046;&#32593;&#32476;&#39044;&#27979;&#30340;&#26041;&#24046;&#65292;&#20197;&#21450;&#22238;&#25253;&#65292;&#26469;&#20998;&#26512;&#35745;&#31639;&#20195;&#34920;&#25105;&#20204;&#20998;&#24067;&#24335;&#20540;&#20989;&#25968;&#30340;&#27491;&#24577;&#20998;&#24067;&#30340;&#30446;&#26631;&#20998;&#20301;&#26639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#20998;&#24067;&#30340;&#32467;&#26500;&#29305;&#24449;&#30340;&#27491;&#30830;&#24615;&#26469;&#34913;&#37327;&#30340;&#31574;&#30053;&#26356;&#26032;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24449;&#22312;&#26631;&#20934;&#30340;&#20540;&#20989;&#25968;&#20013;&#19981;&#23384;&#22312;&#12290;&#25105;&#20204;&#27010;&#36848;&#30340;&#26041;&#27861;&#19982;&#35768;&#22810;DRL&#32467;&#26500;&#20860;&#23481;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#20195;&#34920;&#24615;&#30340;&#22312;&#32447;&#31639;&#27861;&#65292;PPO&#21644;TRPO&#65292;&#20316;&#20026;&#27979;&#35797;&#24179;&#21488;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32479;&#35745;&#19978;&#20135;&#29983;&#20102;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a predictive model of the mean return, or value function, plays a critical role in many reinforcement learning algorithms. Distributional reinforcement learning (DRL) has been shown to improve performance by modeling the value distribution, not just the mean. We study the value distribution in several continuous control tasks and find that the learned value distribution is empirical quite close to normal. We design a method that exploits this property, employ variances predicted from a variance network, along with returns, to analytically compute target quantile bars representing a normal for our distributional value function. In addition, we propose a policy update strategy based on the correctness as measured by structural characteristics of the value distribution not present in the standard value function. The approach we outline is compatible with many DRL structures. We use two representative on-policy algorithms, PPO and TRPO, as testbeds. Our method yields statistically
&lt;/p&gt;</description></item></channel></rss>