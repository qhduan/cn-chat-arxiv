<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;</title><link>https://arxiv.org/abs/2403.07300</link><description>&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#25511;&#21046;&#39044;&#35757;&#32451;LLMs&#36827;&#34892;&#24191;&#20041;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Taming Pre-trained LLMs for Generalised Time Series Forecasting via Cross-modal Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#21644;LLMs&#23545;&#40784;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#38745;&#24577;&#21644;&#21160;&#24577;&#30693;&#35782;&#65292;&#20805;&#20998;&#37322;&#25918;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26368;&#36817;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24555;&#36895;&#22686;&#38271;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#26377;&#38480;&#30340;&#26102;&#38388;&#25968;&#25454;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28608;&#22686;&#65292;&#19968;&#20123;&#24037;&#20316;&#23581;&#35797;&#23558;LLMs&#24341;&#20837;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30452;&#25509;&#23558;&#26102;&#38388;&#24207;&#21015;&#20316;&#20026;LLMs&#30340;&#36755;&#20837;&#65292;&#24573;&#30053;&#20102;&#26102;&#38388;&#21644;&#25991;&#26412;&#25968;&#25454;&#20043;&#38388;&#22266;&#26377;&#30340;&#27169;&#24577;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21644;&#26102;&#38388;&#24207;&#21015;&#23545;&#40784;&#26694;&#26550;&#65292;&#31216;&#20026;LLaTA&#65292;&#20197;&#20805;&#20998;&#21457;&#25381;LLMs&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25361;&#25112;&#20013;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#36328;&#27169;&#24577;&#30693;&#35782;&#33976;&#39311;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#36755;&#20837;&#26080;&#20851;&#38745;&#24577;&#30693;&#35782;&#21644;&#36755;&#20837;&#30456;&#20851;&#21160;&#24577;&#30693;&#35782;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#20026;&#39044;&#27979;&#27169;&#22411;&#36171;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07300v1 Announce Type: cross  Abstract: Multivariate time series forecasting has recently gained great success with the rapid growth of deep learning models. However, existing approaches usually train models from scratch using limited temporal data, preventing their generalization. Recently, with the surge of the Large Language Models (LLMs), several works have attempted to introduce LLMs into time series forecasting. Despite promising results, these methods directly take time series as the input to LLMs, ignoring the inherent modality gap between temporal and text data. In this work, we propose a novel Large Language Models and time series alignment framework, dubbed LLaTA, to fully unleash the potentials of LLMs in the time series forecasting challenge. Based on cross-modal knowledge distillation, the proposed method exploits both input-agnostic static knowledge and input-dependent dynamic knowledge in pre-trained LLMs. In this way, it empowers the forecasting model with f
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.02437</link><description>&lt;p&gt;
SoK: &#32852;&#37030;&#21453;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
SoK: Challenges and Opportunities in Federated Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02437
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24341;&#20837;&#20102;&#26032;&#30340;&#38544;&#31169;&#35201;&#27714;&#65292;&#20419;&#20351;&#30740;&#31350;&#24320;&#22987;&#20851;&#27880;&#36866;&#29992;&#20110;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20110;2017&#24180;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20419;&#36827;&#20102;&#19981;&#20449;&#20219;&#26041;&#20043;&#38388;&#30340;&#21512;&#20316;&#23398;&#20064;&#65292;&#26080;&#38656;&#21508;&#26041;&#26126;&#30830;&#20849;&#20139;&#20854;&#25968;&#25454;&#12290;&#36825;&#20801;&#35768;&#22312;&#23562;&#37325;GDPR&#21644;CPRA&#31561;&#38544;&#31169;&#35268;&#23450;&#30340;&#21516;&#26102;&#65292;&#22312;&#29992;&#25143;&#25968;&#25454;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26032;&#20852;&#30340;&#38544;&#31169;&#35201;&#27714;&#21487;&#33021;&#35201;&#27714;&#27169;&#22411;&#25152;&#26377;&#32773;&#33021;&#22815;&#8220;&#36951;&#24536;&#8221;&#19968;&#20123;&#24050;&#23398;&#20064;&#30340;&#25968;&#25454;&#65292;&#20363;&#22914;&#24403;&#25968;&#25454;&#25152;&#26377;&#32773;&#25110;&#25191;&#27861;&#26426;&#26500;&#35201;&#27714;&#26102;&#12290;&#36825;&#20652;&#29983;&#20102;&#19968;&#20010;&#21517;&#20026;&#8220;&#26426;&#22120;&#21453;&#23398;&#20064;&#8221;&#30340;&#27963;&#36291;&#30740;&#31350;&#39046;&#22495;&#12290;&#22312;FL&#30340;&#32972;&#26223;&#19979;&#65292;&#35768;&#22810;&#20026;&#38598;&#20013;&#24335;&#29615;&#22659;&#24320;&#21457;&#30340;&#21453;&#23398;&#20064;&#25216;&#26415;&#24182;&#19981;&#23481;&#26131;&#24212;&#29992;&#65281;&#36825;&#26159;&#30001;&#20110;FL&#20013;&#38598;&#20013;&#24335;&#21644;&#20998;&#24067;&#24335;&#23398;&#20064;&#20043;&#38388;&#30340;&#29420;&#29305;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#20114;&#21160;&#24615;&#12289;&#38543;&#26426;&#24615;&#12289;&#24322;&#26500;&#24615;&#21644;&#26377;&#38480;&#21487;&#35775;&#38382;&#24615;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#26368;&#36817;&#30340;&#19968;&#31995;&#21015;&#30740;&#31350;&#24037;&#20316;&#32858;&#28966;&#20110;&#24320;&#21457;&#36866;&#29992;&#20110;FL&#30340;&#21453;&#23398;&#20064;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02437v1 Announce Type: cross  Abstract: Federated learning (FL), introduced in 2017, facilitates collaborative learning between non-trusting parties with no need for the parties to explicitly share their data among themselves. This allows training models on user data while respecting privacy regulations such as GDPR and CPRA. However, emerging privacy requirements may mandate model owners to be able to \emph{forget} some learned data, e.g., when requested by data owners or law enforcement. This has given birth to an active field of research called \emph{machine unlearning}. In the context of FL, many techniques developed for unlearning in centralized settings are not trivially applicable! This is due to the unique differences between centralized and distributed learning, in particular, interactivity, stochasticity, heterogeneity, and limited accessibility in FL. In response, a recent line of work has focused on developing unlearning mechanisms tailored to FL.   This SoK pape
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.10206</link><description>&lt;p&gt;
&#24322;&#26500;&#22270;&#19978;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#29305;&#23450;&#20219;&#21153;&#22270;&#23376;&#25277;&#26679;
&lt;/p&gt;
&lt;p&gt;
Ising on the Graph: Task-specific Graph Subsampling via the Ising Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10206
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20234;&#36763;&#27169;&#22411;&#30340;&#22270;&#23376;&#25277;&#26679;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#22312;&#22270;&#32467;&#26500;&#19978;&#36827;&#34892;&#20943;&#23567;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#26469;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#22312;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#31561;&#24212;&#29992;&#20013;&#24471;&#21040;&#23637;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22270;&#30340;&#22823;&#23567;&#21516;&#26102;&#20445;&#25345;&#20854;&#25972;&#20307;&#32467;&#26500;&#26159;&#19968;&#20010;&#20855;&#26377;&#35768;&#22810;&#24212;&#29992;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#36890;&#24120;&#65292;&#20943;&#23567;&#22270;&#30340;&#26041;&#27861;&#35201;&#20040;&#21024;&#38500;&#36793;&#32536;&#65288;&#31232;&#30095;&#21270;&#65289;&#65292;&#35201;&#20040;&#21512;&#24182;&#33410;&#28857;&#65288;&#31895;&#21270;&#65289;&#65292;&#32780;&#27809;&#26377;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22312;&#33410;&#28857;&#25110;&#36793;&#19978;&#23450;&#20041;&#30340;&#20234;&#36763;&#27169;&#22411;&#23545;&#22270;&#32467;&#26500;&#36827;&#34892;&#23376;&#25277;&#26679;&#30340;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20234;&#36763;&#27169;&#22411;&#30340;&#22806;&#37096;&#30913;&#22330;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#20219;&#21153;&#29305;&#23450;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#31471;&#21040;&#31471;&#22320;&#23398;&#20064;&#22914;&#20309;&#20026;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#20943;&#23567;&#22270;&#30340;&#22823;&#23567;&#12290;&#25152;&#20351;&#29992;&#30340;&#20219;&#21153;&#25439;&#22833;&#20989;&#25968;&#29978;&#33267;&#19981;&#38656;&#35201;&#21487;&#24494;&#20998;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#24212;&#29992;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#65306;&#22270;&#20687;&#20998;&#21106;&#12289;&#19977;&#32500;&#24418;&#29366;&#31232;&#30095;&#21270;&#21644;&#31232;&#30095;&#36924;&#36817;&#30697;&#38453;&#27714;&#36870;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10206v1 Announce Type: cross  Abstract: Reducing a graph while preserving its overall structure is an important problem with many applications. Typically, the reduction approaches either remove edges (sparsification) or merge nodes (coarsening) in an unsupervised way with no specific downstream task in mind. In this paper, we present an approach for subsampling graph structures using an Ising model defined on either the nodes or edges and learning the external magnetic field of the Ising model using a graph neural network. Our approach is task-specific as it can learn how to reduce a graph for a specific downstream task in an end-to-end fashion. The utilized loss function of the task does not even have to be differentiable. We showcase the versatility of our approach on three distinct applications: image segmentation, 3D shape sparsification, and sparse approximate matrix inverse determination.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2402.04051</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Analysis of Linear Mode Connectivity via Permutation-Based Weight Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04051
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25490;&#21015;&#30340;&#26435;&#37325;&#21305;&#37197;&#20998;&#26512;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65292;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#20102;&#36890;&#36807;&#26435;&#37325;&#21305;&#37197;&#25214;&#21040;&#30340;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#26435;&#37325;&#30697;&#38453;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Ainsworth&#31561;&#20154;&#23637;&#31034;&#20102;&#20351;&#29992;&#26435;&#37325;&#21305;&#37197;&#65288;WM&#65289;&#26469;&#26368;&#23567;&#21270;&#25490;&#21015;&#25628;&#32034;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;$L_2$&#36317;&#31163;&#26377;&#25928;&#22320;&#35782;&#21035;&#28385;&#36275;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#24615;&#65288;LMC&#65289;&#30340;&#25490;&#21015;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#65292;&#22312;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#31181;&#23376;&#30340;&#29420;&#31435;&#35757;&#32451;&#27169;&#22411;&#20043;&#38388;&#30340;&#32447;&#24615;&#36335;&#24452;&#19978;&#30340;&#25439;&#22833;&#20445;&#25345;&#20960;&#20046;&#24658;&#23450;&#12290;&#26412;&#25991;&#36890;&#36807;WM&#25552;&#20379;&#20102;LMC&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#36825;&#23545;&#20110;&#29702;&#35299;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#26377;&#25928;&#24615;&#21450;&#20854;&#22312;&#27169;&#22411;&#21512;&#24182;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23454;&#39564;&#21644;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#24182;&#19981;&#26174;&#30528;&#20943;&#23569;&#20004;&#20010;&#27169;&#22411;&#20043;&#38388;&#30340;$L_2$&#36317;&#31163;&#65292;&#32780;LMC&#30340;&#20986;&#29616;&#24182;&#19981;&#20165;&#20165;&#26159;&#30001;&#20110;WM&#26412;&#36523;&#30340;&#36317;&#31163;&#20943;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#65292;&#34920;&#26126;&#25490;&#21015;&#21487;&#20197;&#25913;&#21464;&#27599;&#23618;&#26435;&#37325;&#30697;&#38453;&#30340;&#22855;&#24322;&#21521;&#37327;&#30340;&#26041;&#21521;&#65292;&#20294;&#19981;&#33021;&#25913;&#21464;&#22855;&#24322;&#20540;&#12290;&#36825;&#19968;&#21457;&#29616;&#34920;&#26126;&#65292;WM&#25214;&#21040;&#30340;&#25490;&#21015;&#20027;&#35201;&#25913;&#21464;&#20102;&#26435;&#37325;&#30697;&#38453;&#30340;&#26041;&#21521;&#65292;&#32780;&#19981;&#26159;&#22855;&#24322;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Ainsworth et al. showed that using weight matching (WM) to minimize the $L_2$ distance in a permutation search of model parameters effectively identifies permutations that satisfy linear mode connectivity (LMC), in which the loss along a linear path between two independently trained models with different seeds remains nearly constant. This paper provides a theoretical analysis of LMC using WM, which is crucial for understanding stochastic gradient descent's effectiveness and its application in areas like model merging. We first experimentally and theoretically show that permutations found by WM do not significantly reduce the $L_2$ distance between two models and the occurrence of LMC is not merely due to distance reduction by WM in itself. We then provide theoretical insights showing that permutations can change the directions of the singular vectors, but not the singular values, of the weight matrices in each layer. This finding shows that permutations found by WM mainly al
&lt;/p&gt;</description></item><item><title>TinyFormer&#26159;&#19968;&#20010;&#20855;&#26377;SuperNAS&#12289;SparseNAS&#21644;SparseEngine&#32452;&#25104;&#30340;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#22312;MCUs&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#36164;&#28304;&#39640;&#25928;&#30340;transformer&#27169;&#22411;&#12290;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25552;&#20986;&#20102;SparseEngine&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;MCUs&#19978;&#25191;&#34892;&#31232;&#30095;&#27169;&#22411;&#30340;transformer&#25512;&#29702;&#30340;&#37096;&#32626;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2311.01759</link><description>&lt;p&gt;
TinyFormer: &#39640;&#25928;&#30340;Transformer&#35774;&#35745;&#21644;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#30340;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
TinyFormer: Efficient Transformer Design and Deployment on Tiny Devices. (arXiv:2311.01759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01759
&lt;/p&gt;
&lt;p&gt;
TinyFormer&#26159;&#19968;&#20010;&#20855;&#26377;SuperNAS&#12289;SparseNAS&#21644;SparseEngine&#32452;&#25104;&#30340;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#22312;MCUs&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#36164;&#28304;&#39640;&#25928;&#30340;transformer&#27169;&#22411;&#12290;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#25552;&#20986;&#20102;SparseEngine&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;MCUs&#19978;&#25191;&#34892;&#31232;&#30095;&#27169;&#22411;&#30340;transformer&#25512;&#29702;&#30340;&#37096;&#32626;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#23884;&#20837;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#65292;&#20197;&#24494;&#25511;&#21046;&#22120;&#21333;&#20803;&#65288;MCUs&#65289;&#20026;&#20195;&#34920;&#30340;&#23567;&#22411;&#35774;&#22791;&#19978;&#24320;&#21457;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20005;&#37325;&#30340;&#30828;&#20214;&#36164;&#28304;&#38480;&#21046;&#65292;&#22914;&#20309;&#39640;&#25928;&#22320;&#35774;&#35745;&#21644;&#37096;&#32626;&#26368;&#26032;&#30340;&#20808;&#36827;&#27169;&#22411;&#65288;&#22914;transformer&#65289;&#22312;&#23567;&#22411;&#35774;&#22791;&#19978;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TinyFormer&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#22312;MCUs&#19978;&#24320;&#21457;&#21644;&#37096;&#32626;&#36164;&#28304;&#39640;&#25928;&#30340;transformer&#30340;&#26694;&#26550;&#12290;TinyFormer&#20027;&#35201;&#30001;SuperNAS&#12289;SparseNAS&#21644;SparseEngine&#32452;&#25104;&#12290;&#20854;&#20013;&#65292;SuperNAS&#26088;&#22312;&#20174;&#24191;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#23547;&#25214;&#36866;&#24403;&#30340;&#36229;&#32593;&#32476;&#12290;SparseNAS&#35780;&#20272;&#26368;&#20339;&#30340;&#31232;&#30095;&#21333;&#36335;&#24452;&#27169;&#22411;&#65292;&#21253;&#25324;&#20174;&#24050;&#35782;&#21035;&#30340;&#36229;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;transformer&#26550;&#26500;&#12290;&#26368;&#21518;&#65292;SparseEngine&#23558;&#25628;&#32034;&#21040;&#30340;&#31232;&#30095;&#27169;&#22411;&#39640;&#25928;&#22320;&#37096;&#32626;&#21040;MCUs&#19978;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SparseEngine&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;MCUs&#19978;&#25191;&#34892;&#31232;&#30095;&#27169;&#22411;&#30340;transformer&#25512;&#29702;&#30340;&#37096;&#32626;&#26694;&#26550;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;TinyFormer&#22312;&#20445;&#25345;&#25512;&#29702;&#31934;&#24230;&#30340;&#21516;&#26102;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;transformer&#27169;&#22411;&#65292;&#20943;&#23569;&#20102;&#22823;&#32422;78&#65285;&#30340;&#25512;&#29702;&#35745;&#31639;&#37327;&#21644;53&#65285;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Developing deep learning models on tiny devices (e.g. Microcontroller units, MCUs) has attracted much attention in various embedded IoT applications. However, it is challenging to efficiently design and deploy recent advanced models (e.g. transformers) on tiny devices due to their severe hardware resource constraints. In this work, we propose TinyFormer, a framework specifically designed to develop and deploy resource-efficient transformers on MCUs. TinyFormer mainly consists of SuperNAS, SparseNAS and SparseEngine. Separately, SuperNAS aims to search for an appropriate supernet from a vast search space. SparseNAS evaluates the best sparse single-path model including transformer architecture from the identified supernet. Finally, SparseEngine efficiently deploys the searched sparse models onto MCUs. To the best of our knowledge, SparseEngine is the first deployment framework capable of performing inference of sparse models with transformer on MCUs. Evaluation results on the CIFAR-10 da
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#25972;&#21512;&#38750;&#32447;&#24615;&#26641;&#31361;&#32467;&#26500;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#24615;&#33021;&#65292;&#21516;&#26102;&#25511;&#21046;&#20449;&#21495;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#26410;&#26469;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.11950</link><description>&lt;p&gt;
&#32531;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36890;&#20449;&#25104;&#26412;&#65306;&#26641;&#31361;&#38750;&#32447;&#24615;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mitigating Communication Costs in Neural Networks: The Role of Dendritic Nonlinearity. (arXiv:2306.11950v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#25972;&#21512;&#38750;&#32447;&#24615;&#26641;&#31361;&#32467;&#26500;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#27169;&#22411;&#30340;&#23481;&#37327;&#21644;&#24615;&#33021;&#65292;&#21516;&#26102;&#25511;&#21046;&#20449;&#21495;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#23545;&#20110;&#26410;&#26469;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;&#28145;&#21051;&#22320;&#24433;&#21709;&#20102;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;ANN&#20013;&#20351;&#29992;&#30340;&#31070;&#32463;&#20803;&#19982;&#20854;&#29983;&#29289;&#27169;&#22411;&#23384;&#22312;&#26126;&#26174;&#20559;&#24046;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#23569;&#21253;&#21547;&#23616;&#37096;&#38750;&#32447;&#24615;&#30340;&#22797;&#26434;&#26641;&#31361;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#26679;&#30340;&#24046;&#24322;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#28857;&#31070;&#32463;&#20803;&#21487;&#20197;&#22312;&#25191;&#34892;&#35745;&#31639;&#20219;&#21153;&#26041;&#38754;&#22312;&#21151;&#33021;&#19978;&#26367;&#20195;&#26641;&#31361;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#26641;&#31361;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#26641;&#31361;&#32467;&#26500;&#38750;&#32447;&#24615;&#23545;&#31070;&#32463;&#32593;&#32476;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#25972;&#21512;&#26641;&#31361;&#32467;&#26500;&#21487;&#20197;&#22312;&#20445;&#25345;&#20449;&#21495;&#36890;&#20449;&#25104;&#26412;&#26377;&#25928;&#25233;&#21046;&#30340;&#21516;&#26102;&#65292;&#26174;&#33879;&#22686;&#24378;&#27169;&#22411;&#23481;&#37327;&#21644;&#24615;&#33021;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35265;&#35299;&#65292;&#23545;&#26410;&#26469;&#31070;&#32463;&#32593;&#32476;&#30340;&#21457;&#23637;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Our comprehension of biological neuronal networks has profoundly influenced the evolution of artificial neural networks (ANNs). However, the neurons employed in ANNs exhibit remarkable deviations from their biological analogs, mainly due to the absence of complex dendritic trees encompassing local nonlinearity. Despite such disparities, previous investigations have demonstrated that point neurons can functionally substitute dendritic neurons in executing computational tasks. In this study, we scrutinized the importance of nonlinear dendrites within neural networks. By employing machine-learning methodologies, we assessed the impact of dendritic structure nonlinearity on neural network performance. Our findings reveal that integrating dendritic structures can substantially enhance model capacity and performance while keeping signal communication costs effectively restrained. This investigation offers pivotal insights that hold considerable implications for the development of future neur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.11908</link><description>&lt;p&gt;
&#22522;&#20110;&#23450;&#28857;&#26641;&#30340;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Accelerating Generalized Random Forests with Fixed-Point Trees. (arXiv:2306.11908v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#24314;&#31435;&#22312;&#20256;&#32479;&#38543;&#26426;&#26862;&#26519;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#33258;&#36866;&#24212;&#26680;&#21152;&#26435;&#31639;&#27861;&#26469;&#26500;&#24314;&#20272;&#31639;&#22120;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26641;&#29983;&#38271;&#36807;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#22522;&#20110;&#23450;&#28857;&#36845;&#20195;&#36817;&#20284;&#34920;&#31034;&#26799;&#24230;&#36817;&#20284;&#65292;&#23454;&#29616;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#20026;&#27492;&#24320;&#21457;&#20102;&#28176;&#36817;&#29702;&#35770;&#12290;&#36825;&#26377;&#25928;&#22320;&#33410;&#30465;&#20102;&#26102;&#38388;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#37327;&#30340;&#32500;&#24230;&#36866;&#20013;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized random forests arXiv:1610.01271 build upon the well-established success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization, and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#30528;&#30524;&#20110;&#20854;&#20013;&#28041;&#21450;&#30340;&#20613;&#37324;&#21494;&#39057;&#29575;&#19982;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20123;&#39057;&#29575;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.15203</link><description>&lt;p&gt;
&#36890;&#36807;&#20869;&#22312;&#32500;&#24230;&#23558;&#38544;&#24615;&#20559;&#35265;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#30456;&#20851;&#32852;
&lt;/p&gt;
&lt;p&gt;
Relating Implicit Bias and Adversarial Attacks through Intrinsic Dimension. (arXiv:2305.15203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24615;&#20559;&#24046;&#65292;&#30528;&#30524;&#20110;&#20854;&#20013;&#28041;&#21450;&#30340;&#20613;&#37324;&#21494;&#39057;&#29575;&#19982;&#22270;&#20687;&#20998;&#31867;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21457;&#29616;&#36825;&#20123;&#39057;&#29575;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20247;&#25152;&#21608;&#30693;&#23427;&#20204;&#26131;&#21463;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#25915;&#20987;&#26159;&#38024;&#23545;&#27169;&#22411;&#30340;&#36755;&#20837;&#25968;&#25454;&#36827;&#34892;&#30340;&#23567;&#24178;&#25200;&#65292;&#26088;&#22312;&#27450;&#39575;&#27169;&#22411;&#12290;&#33258;&#28982;&#32780;&#28982;&#30340;&#38382;&#39064;&#26159;&#65292;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#35774;&#32622;&#25110;&#23646;&#24615;&#19982;&#25915;&#20987;&#30340;&#24615;&#36136;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#32852;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20851;&#27880;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24615;&#20559;&#24046;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#20854;&#22266;&#26377;&#20542;&#21521;&#20110;&#25903;&#25345;&#29305;&#23450;&#27169;&#24335;&#25110;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38544;&#24615;&#20559;&#24046;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#20854;&#20013;&#21253;&#25324;&#36827;&#34892;&#20934;&#30830;&#22270;&#20687;&#20998;&#31867;&#25152;&#38656;&#30340;&#22522;&#26412;&#20613;&#37324;&#21494;&#39057;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#27979;&#35797;&#20197;&#35780;&#20272;&#36825;&#20123;&#39057;&#29575;&#19982;&#25104;&#21151;&#25915;&#20987;&#25152;&#38656;&#30340;&#39057;&#29575;&#20043;&#38388;&#30340;&#32479;&#35745;&#20851;&#31995;&#12290;&#20026;&#20102;&#28145;&#20837;&#25506;&#35752;&#36825;&#31181;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25581;&#31034;&#22352;&#26631;&#38598;&#20043;&#38388;&#30340;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#65292;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#22352;&#26631;&#38598;&#23601;&#26159;&#21069;&#36848;&#30340;&#20613;&#37324;&#21494;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their impressive performance in classification, neural networks are known to be vulnerable to adversarial attacks. These attacks are small perturbations of the input data designed to fool the model. Naturally, a question arises regarding the potential connection between the architecture, settings, or properties of the model and the nature of the attack. In this work, we aim to shed light on this problem by focusing on the implicit bias of the neural network, which refers to its inherent inclination to favor specific patterns or outcomes. Specifically, we investigate one aspect of the implicit bias, which involves the essential Fourier frequencies required for accurate image classification. We conduct tests to assess the statistical relationship between these frequencies and those necessary for a successful attack. To delve into this relationship, we propose a new method that can uncover non-linear correlations between sets of coordinates, which, in our case, are the aforementio
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21517;&#20026;SidAE&#65292;&#23427;&#32467;&#21512;&#20102;&#23402;&#29983;&#26550;&#26500;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02549</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#23402;&#29983;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Siamese Autoencoders. (arXiv:2304.02549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#21517;&#20026;SidAE&#65292;&#23427;&#32467;&#21512;&#20102;&#23402;&#29983;&#26550;&#26500;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#30340;&#20248;&#28857;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25552;&#21462;&#36755;&#20837;&#25968;&#25454;&#30340;&#29305;&#24449;&#65292;&#20197;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#30417;&#30563;&#30340;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#36825;&#24448;&#24448;&#26159;&#26114;&#36149;&#19988;&#38590;&#20197;&#33719;&#24471;&#30340;&#12290;&#30456;&#21453;&#65292;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#20943;&#23569;&#20102;&#23454;&#29616;&#30456;&#21516;&#25110;&#26356;&#39640;&#19979;&#28216;&#24615;&#33021;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#12290;&#30446;&#26631;&#26159;&#22312;&#33258;&#30417;&#30563;&#20219;&#21153;&#19978;&#39044;&#20808;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20415;&#32593;&#32476;&#33021;&#22815;&#20174;&#21407;&#22987;&#36755;&#20837;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#23558;&#36825;&#20123;&#29305;&#24449;&#29992;&#20316;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#22270;&#20687;&#20998;&#31867;&#65289;&#20013;&#30340;&#36755;&#20837;&#12290;&#22312;&#20808;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#33258;&#32534;&#30721;&#22120;&#21644;&#23402;&#29983;&#32593;&#32476;&#65288;&#22914;SimSiam&#65289;&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#36825;&#20123;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#23558;&#29305;&#24449;&#30340;&#29305;&#24615;&#65288;&#20363;&#22914;&#65292;&#32454;&#33410;&#32423;&#21035;&#65289;&#19982;&#32473;&#23450;&#30340;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#21305;&#37197;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#23402;&#29983;&#26550;&#26500;&#21644;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#20248;&#21183;&#30340;&#26032;&#33258;&#30417;&#30563;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#21517;&#20026;SidAE&#65288;&#23402;&#29983;&#21435;&#22122;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#19978;&#32988;&#36807;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#26368;&#26032;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fully supervised models often require large amounts of labeled training data, which tends to be costly and hard to acquire. In contrast, self-supervised representation learning reduces the amount of labeled data needed for achieving the same or even higher downstream performance. The goal is to pre-train deep neural networks on a self-supervised task such that afterwards the networks are able to extract meaningful features from raw input data. These features are then used as inputs in downstream tasks, such as image classification. Previously, autoencoders and Siamese networks such as SimSiam have been successfully employed in those tasks. Yet, challenges remain, such as matching characteristics of the features (e.g., level of detail) to the given task and data set. In this paper, we present a new self-supervised method that combines the benefits of Siamese architectures and denoising autoencoders. We show that our model, called SidAE (Siamese denoising autoencoder), outperforms two se
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#33539;&#25968;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#24809;&#32602;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.01353</link><description>&lt;p&gt;
&#23545;&#27491;&#21017;&#21270;&#20013;&#30340;&#20559;&#24046;&#36827;&#34892;&#24809;&#32602;&#23558;&#20351;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Penalising the biases in norm regularisation enforces sparsity. (arXiv:2303.01353v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#25511;&#21046;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#33539;&#25968;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#23545;&#31070;&#32463;&#32593;&#32476;&#20013;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#24809;&#32602;&#21487;&#20197;&#23454;&#29616;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#36890;&#36807;&#25511;&#21046;&#21442;&#25968;&#30340;&#33539;&#25968;&#24448;&#24448;&#21487;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#33539;&#25968;&#21644;&#25152;&#24471;&#20272;&#35745;&#37327;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#29702;&#35770;&#19978;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#26412;&#25991;&#38024;&#23545;&#20855;&#26377;&#21333;&#19968;&#38544;&#34255;&#23618;&#21644;&#19968;&#32500;&#25968;&#25454;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23637;&#31034;&#20102;&#34920;&#31034;&#20989;&#25968;&#25152;&#38656;&#30340;&#21442;&#25968;&#33539;&#25968;&#30001;&#20854;&#20108;&#38454;&#23548;&#25968;&#30340;&#24635;&#21464;&#24046;&#21152;&#26435;&#24471;&#21040;&#65292;&#20854;&#20013;&#25152;&#21152;&#26435;&#30340;&#22240;&#23376;&#20026;$\sqrt{1+x^2}$&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#19981;&#23545;&#20559;&#24046;&#39033;&#30340;&#33539;&#25968;&#36827;&#34892;&#27491;&#21017;&#21270;&#26102;&#65292;&#36825;&#20010;&#21152;&#26435;&#22240;&#23376;&#20250;&#28040;&#22833;&#12290;&#36825;&#20010;&#39069;&#22806;&#30340;&#21152;&#26435;&#22240;&#23376;&#30340;&#23384;&#22312;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#34987;&#35777;&#26126;&#21487;&#20197;&#24378;&#21046;&#23454;&#29616;&#26368;&#23567;&#33539;&#25968;&#20869;&#25554;&#22120;&#30340;&#21807;&#19968;&#24615;&#21644;&#31232;&#30095;&#24615;&#65288;&#22312;&#25296;&#28857;&#25968;&#37327;&#19978;&#65289;&#12290;&#30456;&#21453;&#65292;&#30465;&#30053;&#20559;&#24046;&#30340;&#33539;&#25968;&#21017;&#20250;&#23548;&#33268;&#38750;&#31232;&#30095;&#35299;&#12290;&#22240;&#27492;&#65292;&#22312;&#27491;&#21017;&#21270;&#20013;&#23545;&#20559;&#24046;&#39033;&#36827;&#34892;&#24809;&#32602;&#65292;&#26080;&#35770;&#26159;&#26174;&#24335;&#36824;&#26159;&#38544;&#24335;&#22320;&#65292;&#37117;&#20250;&#23548;&#33268;&#31232;&#30095;&#20272;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Controlling the parameters' norm often yields good generalisation when training neural networks. Beyond simple intuitions, the relation between regularising parameters' norm and obtained estimators remains theoretically misunderstood. For one hidden ReLU layer networks with unidimensional data, this work shows the parameters' norm required to represent a function is given by the total variation of its second derivative, weighted by a $\sqrt{1+x^2}$ factor. Notably, this weighting factor disappears when the norm of bias terms is not regularised. The presence of this additional weighting factor is of utmost significance as it is shown to enforce the uniqueness and sparsity (in the number of kinks) of the minimal norm interpolator. Conversely, omitting the bias' norm allows for non-sparse solutions. Penalising the bias terms in the regularisation, either explicitly or implicitly, thus leads to sparse estimators.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24615;&#24378;&#12289;&#36866;&#29992;&#20110;&#22797;&#26434;&#27010;&#29575;&#20132;&#36890;&#39044;&#27979;&#30340;&#21160;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#26102;&#21464;&#20998;&#24067;&#20197;&#26356;&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#24773;&#20917;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.06653</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#23436;&#25972;&#21327;&#26041;&#24046;&#21160;&#24577;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#27010;&#29575;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Dynamic Mixture Model with Full Covariance for Probabilistic Traffic Forecasting. (arXiv:2212.06653v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06653
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#24615;&#24378;&#12289;&#36866;&#29992;&#20110;&#22797;&#26434;&#27010;&#29575;&#20132;&#36890;&#39044;&#27979;&#30340;&#21160;&#24577;&#28151;&#21512;&#27169;&#22411;&#65292;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#26102;&#21464;&#20998;&#24067;&#20197;&#26356;&#20934;&#30830;&#39044;&#27979;&#20132;&#36890;&#24773;&#20917;&#65292;&#20855;&#26377;&#39640;&#25928;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22810;&#20803;&#22810;&#27493;&#39588;&#20132;&#36890;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35774;&#32622;&#20013;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#25110;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#65292;&#31616;&#21333;&#22320;&#20551;&#35774;&#35823;&#24046;&#36981;&#24490;&#29420;&#31435;&#19988;&#21508;&#21521;&#21516;&#24615;&#30340;&#39640;&#26031;&#25110;&#25289;&#26222;&#25289;&#26031;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#20132;&#36890;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#36825;&#26679;&#30340;&#20551;&#35774;&#24448;&#24448;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22240;&#20026;&#26102;&#31354;&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#38750;&#24120;&#22797;&#26434;&#65292;&#24182;&#19988;&#22312;&#26102;&#38388;&#19978;&#23384;&#22312;&#24378;&#28872;&#30340;&#21516;&#26102;&#30456;&#20851;&#24615;&#65292;&#28041;&#21450;&#20256;&#24863;&#22120;&#21644;&#39044;&#27979;&#26102;&#38388;&#36328;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#30697;&#38453;&#21464;&#37327;&#35823;&#24046;&#36807;&#31243;&#30340;&#26102;&#21464;&#20998;&#24067;&#24314;&#27169;&#20026;&#38646;&#22343;&#20540;&#39640;&#26031;&#20998;&#24067;&#30340;&#21160;&#24577;&#28151;&#21512;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#39640;&#25928;&#24615;&#12289;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#30697;&#38453;&#27491;&#24577;&#20998;&#24067;&#21442;&#25968;&#21270;&#27599;&#20010;&#28151;&#21512;&#25104;&#20998;&#65292;&#24182;&#20801;&#35768;&#28151;&#21512;&#26435;&#37325;&#38543;&#26102;&#38388;&#21464;&#21270;&#21644;&#21487;&#39044;&#27979;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Deep learning-based multivariate and multistep-ahead traffic forecasting models are typically trained with the mean squared error (MSE) or mean absolute error (MAE) as the loss function in a sequence-to-sequence setting, simply assuming that the errors follow an independent and isotropic Gaussian or Laplacian distributions. However, such assumptions are often unrealistic for real-world traffic forecasting tasks, where the probabilistic distribution of spatiotemporal forecasting is very complex with strong concurrent correlations across both sensors and forecasting horizons in a time-varying manner. In this paper, we model the time-varying distribution for the matrix-variate error process as a dynamic mixture of zero-mean Gaussian distributions. To achieve efficiency, flexibility, and scalability, we parameterize each mixture component using a matrix normal distribution and allow the mixture weight to change and be predictable over time. The proposed method can be seamlessly integrated 
&lt;/p&gt;</description></item></channel></rss>