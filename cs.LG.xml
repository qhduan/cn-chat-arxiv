<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#21644;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#30340;&#20803;&#21021;&#22987;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#30340;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02289</link><description>&lt;p&gt;
&#34892;&#26143;&#25506;&#27979;&#30340;&#32852;&#37030;&#22810;&#26234;&#33021;&#20307;&#24314;&#22270;
&lt;/p&gt;
&lt;p&gt;
Federated Multi-Agent Mapping for Planetary Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02289
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#21644;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#30340;&#20803;&#21021;&#22987;&#21270;&#65292;&#23454;&#29616;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#30340;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#26426;&#22120;&#20154;&#25506;&#27979;&#20013;&#65292;&#31649;&#29702;&#21644;&#26377;&#25928;&#21033;&#29992;&#21160;&#24577;&#29615;&#22659;&#20135;&#29983;&#30340;&#22823;&#37327;&#24322;&#26500;&#25968;&#25454;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#20998;&#24067;&#24335;&#26144;&#23556;&#26041;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#21327;&#20316;&#23398;&#20064;&#20013;&#21435;&#20013;&#24515;&#21270;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;FL&#20351;&#22810;&#20010;&#26234;&#33021;&#20307;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#32852;&#21512;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#26080;&#38656;&#38598;&#20013;&#21270;&#25110;&#20849;&#20139;&#21407;&#22987;&#25968;&#25454;&#65292;&#20811;&#26381;&#20102;&#24102;&#23485;&#21644;&#23384;&#20648;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#38544;&#24335;&#31070;&#32463;&#26144;&#23556;&#65292;&#23558;&#22320;&#22270;&#34920;&#31034;&#20026;&#30001;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#36830;&#32493;&#20989;&#25968;&#65292;&#20197;&#20415;&#23454;&#29616;&#32039;&#20945;&#21644;&#36866;&#24212;&#24615;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#22312;&#22320;&#29699;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20803;&#21021;&#22987;&#21270;&#26469;&#22686;&#24378;&#36825;&#19968;&#26041;&#27861;&#65292;&#39044;&#35757;&#32451;&#32593;&#32476;&#20197;&#24555;&#36895;&#23398;&#20064;&#26032;&#30340;&#22320;&#22270;&#32467;&#26500;&#12290;&#36825;&#31181;&#32452;&#21512;&#22312;&#35832;&#22914;&#28779;&#26143;&#22320;&#24418;&#21644;&#20912;&#24029;&#31561;&#19981;&#21516;&#39046;&#22495;&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#36825;&#19968;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02289v1 Announce Type: cross  Abstract: In multi-agent robotic exploration, managing and effectively utilizing the vast, heterogeneous data generated from dynamic environments poses a significant challenge. Federated learning (FL) is a promising approach for distributed mapping, addressing the challenges of decentralized data in collaborative learning. FL enables joint model training across multiple agents without requiring the centralization or sharing of raw data, overcoming bandwidth and storage constraints. Our approach leverages implicit neural mapping, representing maps as continuous functions learned by neural networks, for compact and adaptable representations. We further enhance this approach with meta-initialization on Earth datasets, pre-training the network to quickly learn new map structures. This combination demonstrates strong generalization to diverse domains like Martian terrain and glaciers. We rigorously evaluate this approach, demonstrating its effectiven
&lt;/p&gt;</description></item><item><title>MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.02694</link><description>&lt;p&gt;
&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38544;&#31169;&#24863;&#30693;&#35821;&#20041;&#32531;&#23384;
&lt;/p&gt;
&lt;p&gt;
Privacy-Aware Semantic Cache for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02694
&lt;/p&gt;
&lt;p&gt;
MeanCache&#26159;&#19968;&#31181;&#38754;&#21521;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#65292;&#20174;&#32780;&#20943;&#23569;&#26597;&#35810;&#25104;&#26412;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#36127;&#36733;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#12289;Google Bard&#12289;Claude&#21644;Llama 2&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#25628;&#32034;&#24341;&#25806;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#36896;&#25104;&#20102;&#24322;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MeanCache&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#30340;&#35821;&#20041;&#32531;&#23384;&#65292;&#23427;&#33021;&#22815;&#35782;&#21035;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#26597;&#35810;&#20197;&#30830;&#23450;&#32531;&#23384;&#21629;&#20013;&#25110;&#26410;&#21629;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02694v1 Announce Type: cross  Abstract: Large Language Models (LLMs) like ChatGPT, Google Bard, Claude, and Llama 2 have revolutionized natural language processing and search engine dynamics. However, these models incur exceptionally high computational costs. For instance, GPT-3 consists of 175 billion parameters and inference on these models also demands billions of floating-point operations. Caching is a natural solution to reduce LLM inference costs on repeated queries. However, existing caching methods are incapable of finding semantic similarities among LLM queries, leading to unacceptable false hit-and-miss rates.   This paper introduces MeanCache, a semantic cache for LLMs that identifies semantically similar queries to determine cache hit or miss. Using MeanCache, the response to a user's semantically similar query can be retrieved from a local cache rather than re-querying the LLM, thus reducing costs, service provider load, and environmental impact. MeanCache lever
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#24179;&#34913;&#21484;&#22238;&#21644;&#20869;&#23384;&#28040;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.18668</link><description>&lt;p&gt;
&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#24179;&#34913;&#20102;&#21484;&#22238;-&#21534;&#21520;&#37327;&#30340;&#25240;&#34935;
&lt;/p&gt;
&lt;p&gt;
Simple linear attention language models balance the recall-throughput tradeoff
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18668
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#27880;&#24847;&#21147;&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#65292;&#21487;&#20197;&#24179;&#34913;&#21484;&#22238;&#21644;&#20869;&#23384;&#28040;&#32791;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#25797;&#38271;&#21484;&#22238;&#65292;&#21363;&#22312;&#19978;&#19979;&#25991;&#20013;&#24050;&#32463;&#30475;&#21040;&#30340;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#27169;&#22411;&#30340;&#25928;&#29575;&#21463;&#21040;KV-cache&#30340;&#20869;&#23384;&#28040;&#32791;&#30340;&#29942;&#39048;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26159;&#21542;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#65288;&#20363;&#22914;&#36890;&#36807;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#65289;&#32780;&#19981;&#24433;&#21709;&#21484;&#22238;&#12290;&#36890;&#36807;&#23558;&#23454;&#39564;&#21644;&#29702;&#35770;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#26550;&#26500;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#27169;&#22411;&#29366;&#24577;&#22823;&#23567;&#21644;&#21484;&#22238;&#33021;&#21147;&#20043;&#38388;&#30340;&#19968;&#20010;&#20851;&#38190;&#26435;&#34913;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#27880;&#24847;&#21147;&#30340;&#39640;&#25928;&#26367;&#20195;&#26041;&#27861;&#65288;&#20363;&#22914;H3&#12289;Mamba&#12289;RWKV&#65289;&#20445;&#25345;&#22266;&#23450;&#22823;&#23567;&#30340;&#24490;&#29615;&#29366;&#24577;&#65292;&#20294;&#22312;&#21484;&#22238;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;BASED&#65292;&#36825;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#32447;&#24615;&#21644;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#30340;&#31616;&#21333;&#26550;&#26500;&#12290;&#36890;&#36807;&#25913;&#21464;BASED&#31383;&#21475;&#22823;&#23567;&#21644;&#32447;&#24615;&#27880;&#24847;&#21147;&#29305;&#24449;&#32500;&#24230;&#65292;&#25105;&#20204;&#21487;&#20197;&#35843;&#25972;&#29366;&#24577;&#22823;&#23567;&#65292;&#24182;&#36941;&#21382;&#21484;&#22238;-&#20869;&#23384;&#25240;&#34935;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18668v1 Announce Type: new  Abstract: Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff cu
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#19982;&#36339;&#34920;&#35774;&#35745;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#36339;&#34920;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#25628;&#32034;&#26597;&#35810;&#26102;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.10457</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#22411;&#36339;&#34920;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented Skip Lists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10457
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#19982;&#36339;&#34920;&#35774;&#35745;&#25972;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#22686;&#24378;&#22411;&#36339;&#34920;&#65292;&#33021;&#22815;&#23454;&#29616;&#26368;&#20248;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#65292;&#22312;&#22788;&#29702;&#25628;&#32034;&#26597;&#35810;&#26102;&#20855;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#26426;&#22120;&#23398;&#20064;&#24314;&#35758;&#25972;&#21512;&#21040;&#36339;&#34920;&#35774;&#35745;&#20013;&#65292;&#20197;&#25913;&#36827;&#20256;&#32479;&#25968;&#25454;&#32467;&#26500;&#35774;&#35745;&#12290;&#36890;&#36807;&#35775;&#38382;&#21487;&#33021;&#26377;&#35823;&#30340;&#39044;&#27979;&#20998;&#25968;&#39057;&#29575;&#30340;&#39044;&#27979;&#20540;&#30340;&#39044;&#35328;&#31070;&#35861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#36339;&#34920;&#65292;&#21487;&#20197;&#35777;&#26126;&#25552;&#20379;&#26368;&#20339;&#30340;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#65292;&#20960;&#20046;&#26377;&#20108;&#20493;&#30340;&#20248;&#21183;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#30340;&#23398;&#20064;&#22686;&#24378;&#22411;&#36339;&#34920;&#20173;&#28982;&#26159;&#26368;&#20339;&#30340;&#65292;&#21363;&#20351;&#31070;&#35861;&#21482;&#22312;&#24120;&#25968;&#22240;&#23376;&#20869;&#20934;&#30830;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#25628;&#32034;&#26597;&#35810;&#36981;&#24490;&#26222;&#36941;&#23384;&#22312;&#30340;Zipfian&#20998;&#24067;&#65292;&#37027;&#20040;&#25105;&#20204;&#30340;&#36339;&#34920;&#23545;&#20110;&#19968;&#20010;&#39033;&#30446;&#30340;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#20165;&#20026;&#19968;&#20010;&#24120;&#25968;&#65292;&#19982;&#39033;&#30446;&#24635;&#25968;n&#26080;&#20851;&#65292;&#21363;O(1)&#65292;&#32780;&#20256;&#32479;&#30340;&#36339;&#34920;&#30340;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#20026;O(log n)&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#26500;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23637;&#31034;&#25105;&#20204;&#30340;&#25968;&#25454;&#32467;&#26500;&#23454;&#29616;&#20102;&#19968;&#20010;&#26399;&#26395;&#25628;&#32034;&#26102;&#38388;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10457v1 Announce Type: cross  Abstract: We study the integration of machine learning advice into the design of skip lists to improve upon traditional data structure design. Given access to a possibly erroneous oracle that outputs estimated fractional frequencies for search queries on a set of items, we construct a skip list that provably provides the optimal expected search time, within nearly a factor of two. In fact, our learning-augmented skip list is still optimal up to a constant factor, even if the oracle is only accurate within a constant factor. We show that if the search queries follow the ubiquitous Zipfian distribution, then the expected search time for an item by our skip list is only a constant, independent of the total number $n$ of items, i.e., $\mathcal{O}(1)$, whereas a traditional skip list will have an expected search time of $\mathcal{O}(\log n)$. We also demonstrate robustness by showing that our data structure achieves an expected search time that is wi
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06287</link><description>&lt;p&gt;
AI&#65292;&#19982;&#20154;&#30456;&#36935;&#65306;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#39640;&#39118;&#38505;&#20219;&#21153;&#21644;&#20915;&#31574;&#12290;&#36825;&#31181;&#26085;&#30410;&#22686;&#38271;&#30340;&#23384;&#22312;&#24847;&#21619;&#30528;&#20154;&#31867;&#29616;&#22312;&#19981;&#26029;&#19982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#36827;&#34892;&#20114;&#21160;&#65292;&#27599;&#22825;&#36827;&#34892;&#27169;&#22411;&#30340;&#22521;&#35757;&#21644;&#20351;&#29992;&#12290;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#32771;&#34385;&#20154;&#19982;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20132;&#20114;&#65292;&#20294;&#20854;&#20998;&#31867;&#31232;&#30095;&#19988;&#30446;&#26631;&#21508;&#24322;&#12290;&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#24403;&#21069;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04836</link><description>&lt;p&gt;
&#20851;&#20110;&#19981;&#21464;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#23436;&#22791;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Completeness of Invariant Geometric Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04836
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38598;&#20013;&#20110;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#23436;&#22791;&#30340;&#35774;&#35745;GeoNGNN&#65292;&#24182;&#21033;&#29992;&#20854;&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21464;&#27169;&#22411;&#26159;&#19968;&#31867;&#37325;&#35201;&#30340;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20016;&#23500;&#30340;&#20960;&#20309;&#29305;&#24449;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#20960;&#20309;&#34920;&#31034;&#12290;&#36825;&#20123;&#27169;&#22411;&#20197;&#20854;&#31616;&#21333;&#24615;&#12289;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#32780;&#38395;&#21517;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#20173;&#28982;&#19981;&#28165;&#26970;&#65292;&#38480;&#21046;&#20102;&#23545;&#36825;&#31181;&#27169;&#22411;&#28508;&#21147;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38598;&#20013;&#35752;&#35770;&#19981;&#21464;&#27169;&#22411;&#30340;&#29702;&#35770;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#20005;&#26684;&#38480;&#21046;&#20102;&#26368;&#32463;&#20856;&#30340;&#19981;&#21464;&#27169;&#22411;Vanilla DisGNN&#65288;&#32467;&#21512;&#36317;&#31163;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#23558;&#20854;&#19981;&#21487;&#35782;&#21035;&#30340;&#24773;&#20917;&#20165;&#38480;&#20110;&#39640;&#24230;&#23545;&#31216;&#30340;&#20960;&#20309;&#22270;&#24418;&#12290;&#20026;&#20102;&#25171;&#30772;&#36825;&#20123;&#29305;&#27530;&#24773;&#20917;&#30340;&#23545;&#31216;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#23436;&#22791;&#30340;&#19981;&#21464;&#35774;&#35745;&#65292;&#21363;&#23884;&#22871;Vanilla DisGNN&#30340;GeoNGNN&#12290;&#21033;&#29992;GeoNGNN&#20316;&#20026;&#29702;&#35770;&#24037;&#20855;&#65292;&#25105;&#20204;&#39318;&#27425;&#35777;&#26126;&#20102;E(3)-&#23436;&#22791;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Invariant models, one important class of geometric deep learning models, are capable of generating meaningful geometric representations by leveraging informative geometric features. These models are characterized by their simplicity, good experimental results and computational efficiency. However, their theoretical expressive power still remains unclear, restricting a deeper understanding of the potential of such models. In this work, we concentrate on characterizing the theoretical expressiveness of invariant models. We first rigorously bound the expressiveness of the most classical invariant model, Vanilla DisGNN (message passing neural networks incorporating distance), restricting its unidentifiable cases to be only those highly symmetric geometric graphs. To break these corner cases' symmetry, we introduce a simple yet E(3)-complete invariant design by nesting Vanilla DisGNN, named GeoNGNN. Leveraging GeoNGNN as a theoretical tool, we for the first time prove the E(3)-completeness 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;RSGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20462;&#25913;&#27969;&#65288;RSMF&#65289;&#30340;&#25193;&#25955;&#36924;&#36817;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;RSGD&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.03467</link><description>&lt;p&gt;
&#22522;&#20110;&#38543;&#26426;&#20462;&#25913;&#27969;&#30340;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic Modified Flows for Riemannian Stochastic Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03467
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;RSGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#20462;&#25913;&#27969;&#65288;RSMF&#65289;&#30340;&#25193;&#25955;&#36924;&#36817;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#23545;RSGD&#30340;&#36817;&#20284;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#40654;&#26364;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;RSGD&#65289;&#25910;&#25947;&#36895;&#24230;&#32473;&#20986;&#20102;&#23450;&#37327;&#20272;&#35745;&#65292;&#24182;&#23558;&#20854;&#19982;&#40654;&#26364;&#26799;&#24230;&#27969;&#21644;&#25193;&#25955;&#36807;&#31243;&#8212;&#8212;&#40654;&#26364;&#38543;&#26426;&#20462;&#25913;&#27969;&#65288;RSMF&#65289;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#21033;&#29992;&#38543;&#26426;&#24494;&#20998;&#20960;&#20309;&#24037;&#20855;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#23567;&#23398;&#20064;&#29575;&#33539;&#22260;&#20869;&#65292;RSGD&#21487;&#20197;&#36817;&#20284;&#20026;&#30001;&#26080;&#31351;&#32500;&#32500;&#32435;&#36807;&#31243;&#39537;&#21160;&#30340;RSMF&#30340;&#35299;&#12290;RSMF&#32771;&#34385;&#21040;&#20102;RSGD&#30340;&#38543;&#26426;&#27874;&#21160;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#19982;&#30830;&#23450;&#24615;&#40654;&#26364;&#26799;&#24230;&#27969;&#30340;&#36924;&#36817;&#39034;&#24207;&#12290;RSGD&#20351;&#29992;&#20102;&#37325;&#20256;&#36882;&#26144;&#23556;&#30340;&#27010;&#24565;&#65292;&#21363;&#23545;&#25351;&#25968;&#26144;&#23556;&#30340;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#36817;&#20284;&#65292;&#25105;&#20204;&#23545;&#25193;&#25955;&#36924;&#36817;&#30340;&#24369;&#35823;&#24046;&#36827;&#34892;&#20102;&#23450;&#37327;&#30028;&#23450;&#65292;&#22312;&#37325;&#20256;&#36882;&#26144;&#23556;&#12289;&#27969;&#24418;&#20960;&#20309;&#21644;&#26799;&#24230;&#30340;&#38543;&#26426;&#20272;&#35745;&#30340;&#20551;&#35774;&#19979;&#35777;&#26126;&#20102;&#36825;&#20123;&#30028;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give quantitative estimates for the rate of convergence of Riemannian stochastic gradient descent (RSGD) to Riemannian gradient flow and to a diffusion process, the so-called Riemannian stochastic modified flow (RSMF). Using tools from stochastic differential geometry we show that, in the small learning rate regime, RSGD can be approximated by the solution to the RSMF driven by an infinite-dimensional Wiener process. The RSMF accounts for the random fluctuations of RSGD and, thereby, increases the order of approximation compared to the deterministic Riemannian gradient flow. The RSGD is build using the concept of a retraction map, that is, a cost efficient approximation of the exponential map, and we prove quantitative bounds for the weak error of the diffusion approximation under assumptions on the retraction map, the geometry of the manifold, and the random estimators of the gradient.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#21453;&#21521;&#24037;&#31243;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#20381;&#36182;&#20869;&#37096;&#29305;&#24449;&#22270;&#26469;&#26816;&#27979;&#21644;&#21453;&#21521;&#24037;&#31243;&#21518;&#38376;&#65292;&#24182;&#35782;&#21035;&#20854;&#30446;&#26631;&#31867;&#21035;&#65292;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.02034</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#38024;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#21453;&#21521;&#24037;&#31243;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Post-Training Reverse-Engineering Defense Against Backdoors in Deep Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02034
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#21518;&#38376;&#25915;&#20987;&#30340;&#36890;&#29992;&#21518;&#35757;&#32451;&#21453;&#21521;&#24037;&#31243;&#38450;&#24481;&#26041;&#27861;&#65292;&#36890;&#36807;&#20381;&#36182;&#20869;&#37096;&#29305;&#24449;&#22270;&#26469;&#26816;&#27979;&#21644;&#21453;&#21521;&#24037;&#31243;&#21518;&#38376;&#65292;&#24182;&#35782;&#21035;&#20854;&#30446;&#26631;&#31867;&#21035;&#65292;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#26041;&#27861;&#12290;&#36890;&#29992;&#26041;&#27861;&#26088;&#22312;&#21487;&#38752;&#22320;&#26816;&#27979;&#21644;/&#25110;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#65292;&#32780;&#21453;&#21521;&#24037;&#31243;&#26041;&#27861;&#36890;&#24120;&#26126;&#30830;&#20551;&#35774;&#20854;&#20013;&#19968;&#31181;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#34987;&#38450;&#23432;&#30340;DNN&#30340;&#20869;&#37096;&#29305;&#24449;&#22270;&#26469;&#26816;&#27979;&#21644;&#21453;&#21521;&#24037;&#31243;&#21518;&#38376;&#65292;&#24182;&#35782;&#21035;&#20854;&#30446;&#26631;&#31867;&#21035;&#65307;&#23427;&#21487;&#20197;&#22312;&#21518;&#35757;&#32451;&#26102;&#25805;&#20316;&#65288;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#38598;&#65289;&#65307;&#23545;&#20110;&#19981;&#21516;&#30340;&#23884;&#20837;&#26426;&#21046;&#65288;&#21363;&#36890;&#29992;&#30340;&#65289;&#38750;&#24120;&#26377;&#25928;&#65307;&#24182;&#19988;&#20855;&#26377;&#20302;&#35745;&#31639;&#24320;&#38144;&#65292;&#22240;&#27492;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;CIFAR-10&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#19981;&#21516;&#25915;&#20987;&#36827;&#34892;&#20102;&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
A variety of defenses have been proposed against backdoors attacks on deep neural network (DNN) classifiers. Universal methods seek to reliably detect and/or mitigate backdoors irrespective of the incorporation mechanism used by the attacker, while reverse-engineering methods often explicitly assume one. In this paper, we describe a new detector that: relies on internal feature map of the defended DNN to detect and reverse-engineer the backdoor and identify its target class; can operate post-training (without access to the training dataset); is highly effective for various incorporation mechanisms (i.e., is universal); and which has low computational overhead and so is scalable. Our detection approach is evaluated for different attacks on a benchmark CIFAR-10 image classifier.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10690</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#21644;MAE&#65306;&#24341;&#20837;EAUC&#26469;&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#23545;&#23454;&#20307;&#30340;&#23454;&#20540;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#22522;&#30784;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39044;&#27979;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#35780;&#20998;&#65289;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#20063;&#26377;&#35768;&#22810;&#28508;&#21147;&#20294;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#33647;&#29702;&#23398;&#20013;&#36817;&#20284;&#30830;&#23450;&#24739;&#32773;&#30340;&#36866;&#24403;&#21058;&#37327;&#65289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20010;&#20307;&#23454;&#20307;&#35266;&#23519;&#20540;&#20998;&#24067;&#30340;&#38750;&#22343;&#21248;&#24615;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#20013;&#30340;&#20005;&#37325;&#20559;&#35265;&#39044;&#27979;&#65292;&#20559;&#21521;&#20110;&#23454;&#20307;&#30340;&#35266;&#23519;&#36807;&#21435;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#21478;&#31867;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27604;&#38543;&#26426;&#39044;&#27979;&#26356;&#24046;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#21478;&#31867;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#21478;&#31867;-&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;EAUC&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#34917;&#20805;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#37327;&#21270;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10510</link><description>&lt;p&gt;
&#22825;&#20316;&#20043;&#21512;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;
&lt;/p&gt;
&lt;p&gt;
A match made in consistency heaven: when large language models meet evolutionary algorithms. (arXiv:2401.10510v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10510
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#36827;&#21270;&#31639;&#27861;&#30340;&#32467;&#21512;&#20855;&#26377;&#24378;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#31561;&#22810;&#20010;&#26680;&#24515;&#29305;&#24449;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#26412;&#36335;&#32447;&#21644;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#21019;&#36896;&#24615;&#30340;&#33258;&#28982;&#25991;&#26412;&#26041;&#38754;&#20855;&#26377;&#24378;&#22823;&#30340;&#33021;&#21147;&#12290;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#21487;&#20197;&#21457;&#29616;&#22797;&#26434;&#23454;&#38469;&#38382;&#39064;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#25991;&#26412;&#24207;&#21015;&#29983;&#25104;&#21644;&#36827;&#21270;&#30340;&#20849;&#21516;&#29305;&#28857;&#21644;&#26041;&#21521;&#24615;&#65292;&#38416;&#36848;&#20102;LLMs&#19982;EAs&#20043;&#38388;&#30340;&#24378;&#22823;&#19968;&#33268;&#24615;&#65292;&#21253;&#25324;&#22810;&#20010;&#19968;&#23545;&#19968;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#26631;&#35760;&#23884;&#20837;&#21644;&#22522;&#22240;&#22411;-&#34920;&#29616;&#22411;&#26144;&#23556;&#12289;&#20301;&#32622;&#32534;&#30721;&#21644;&#36866;&#24212;&#24615;&#22609;&#36896;&#12289;&#20301;&#32622;&#23884;&#20837;&#21644;&#36873;&#25321;&#12289;&#27880;&#24847;&#21147;&#21644;&#20132;&#21449;&#12289;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#21644;&#31361;&#21464;&#12289;&#27169;&#22411;&#35757;&#32451;&#21644;&#21442;&#25968;&#26356;&#26032;&#20197;&#21450;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#22312;&#36825;&#31181;&#19968;&#33268;&#24615;&#35270;&#35282;&#19979;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#32806;&#21512;&#30740;&#31350;&#65292;&#21253;&#25324;&#36827;&#21270;&#24494;&#35843;&#21644;LLM&#22686;&#24378;&#22411;EAs&#12290;&#20511;&#21161;&#36825;&#20123;&#27934;&#35265;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#26469;&#22312;LLMs&#21644;EAs&#32806;&#21512;&#26041;&#38754;&#30340;&#22522;&#26412;&#30740;&#31350;&#36335;&#32447;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large language models (LLMs) have powerful capabilities for generating creative natural text. Evolutionary algorithms (EAs) can discover diverse solutions to complex real-world problems. Motivated by the common collective and directionality of text sequence generation and evolution, this paper illustrates the strong consistency of LLMs and EAs, which includes multiple one-to-one key characteristics: token embedding and genotype-phenotype mapping, position encoding and fitness shaping, position embedding and selection, attention and crossover, feed-forward neural network and mutation, model training and parameter update, and multi-task learning and multi-objective optimization. Based on this consistency perspective, existing coupling studies are analyzed, including evolutionary fine-tuning and LLM-enhanced EAs. Leveraging these insights, we outline a fundamental roadmap for future research in coupling LLMs and EAs, while highlighting key challenges along the way. The consist
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05535</link><description>&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving the Accuracy and Interpretability of Random Forests via Forest Pruning. (arXiv:2401.05535v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05535
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#21487;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25509;&#36817;&#20960;&#21313;&#24180;&#30340;&#21457;&#23637;&#20043;&#21518;&#65292;&#38543;&#26426;&#26862;&#26519;&#20173;&#28982;&#22312;&#21508;&#31181;&#23398;&#20064;&#38382;&#39064;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#36825;&#26041;&#38754;&#36229;&#36234;&#20102;&#20915;&#31574;&#26641;&#29978;&#33267;&#31070;&#32463;&#32593;&#32476;&#31561;&#26367;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#38543;&#26426;&#26862;&#26519;&#22312;&#35299;&#37322;&#24615;&#26041;&#38754;&#24448;&#24448;&#27604;&#20915;&#31574;&#26641;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20107;&#21518;&#26041;&#27861;&#65292;&#26088;&#22312;&#20860;&#39038;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#20915;&#31574;&#26641;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26862;&#26519;&#20462;&#21098;&#26041;&#27861;&#65292;&#20197;&#22312;&#32473;&#23450;&#30340;&#38543;&#26426;&#26862;&#26519;&#20869;&#25214;&#21040;&#26368;&#20339;&#23376;&#26862;&#26519;&#65292;&#28982;&#21518;&#22312;&#36866;&#29992;&#30340;&#24773;&#20917;&#19979;&#23558;&#36873;&#23450;&#30340;&#26641;&#21512;&#24182;&#20026;&#19968;&#26869;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#31181;&#26041;&#27861;&#20381;&#36182;&#20110;&#32422;&#26463;&#31351;&#20030;&#25628;&#32034;&#65292;&#32780;&#31532;&#20108;&#31181;&#26041;&#27861;&#22522;&#20110;LASSO&#26041;&#27861;&#30340;&#25913;&#36827;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#26223;&#19979;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#20013;&#33267;&#23569;&#26377;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#38543;&#26426;&#26862;&#26519;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decades after their inception, random forests continue to provide state-of-the-art accuracy in a variety of learning problems, outperforming in this respect alternative machine learning algorithms such as decision trees or even neural networks. However, being an ensemble method, the one aspect where random forests tend to severely underperform decision trees is interpretability. In the present work, we propose a post-hoc approach that aims to have the best of both worlds: the accuracy of random forests and the interpretability of decision trees. To this end, we present two forest-pruning methods to find an optimal sub-forest within a given random forest, and then, when applicable, combine the selected trees into one. Our first method relies on constrained exhaustive search, while our second method is based on an adaptation of the LASSO methodology. Extensive experiments over synthetic and real world datasets show that, in the majority of scenarios, at least one of the two methods propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#31283;&#23450;&#24615;&#30340;&#20004;&#31181;&#31867;&#22411;&#65306;&#22402;&#30452;&#31283;&#23450;&#24615;&#21644;&#27700;&#24179;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#30340;&#31616;&#21333;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#32780;&#31283;&#23450;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2310.17332</link><description>&lt;p&gt;
&#20851;&#20110;&#39044;&#27979;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Forecast Stability. (arXiv:2310.17332v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17332
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39044;&#27979;&#31283;&#23450;&#24615;&#30340;&#20004;&#31181;&#31867;&#22411;&#65306;&#22402;&#30452;&#31283;&#23450;&#24615;&#21644;&#27700;&#24179;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#30340;&#31616;&#21333;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#31181;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#32780;&#31283;&#23450;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36890;&#24120;&#19981;&#26159;&#22312;&#30495;&#31354;&#20013;&#20135;&#29983;&#30340;&#65292;&#32780;&#26159;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#29983;&#25104;&#30340;&#65292;&#39044;&#27979;&#26159;&#23450;&#26399;&#29983;&#25104;&#30340;&#65292;&#24182;&#19988;&#24444;&#27492;&#20043;&#38388;&#20114;&#30456;&#24433;&#21709;&#12290;&#23545;&#20110;&#20915;&#31574;&#26469;&#35828;&#65292;&#39044;&#27979;&#19981;&#20250;&#20219;&#24847;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#26159;&#31283;&#23450;&#30340;&#21487;&#33021;&#24456;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#39044;&#27979;&#25991;&#29486;&#20013;&#65292;&#36825;&#20010;&#39046;&#22495;&#21482;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#25105;&#20204;&#31216;&#20043;&#20026;&#22402;&#30452;&#31283;&#23450;&#24615;&#21644;&#27700;&#24179;&#31283;&#23450;&#24615;&#30340;&#39044;&#27979;&#31283;&#23450;&#24615;&#31867;&#22411;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#24037;&#20316;&#21482;&#36866;&#29992;&#20110;&#26576;&#20123;&#22522;&#30784;&#27169;&#22411;&#65292;&#23558;&#36825;&#20123;&#26694;&#26550;&#25193;&#23637;&#25104;&#19982;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#20860;&#23481;&#24182;&#19981;&#23481;&#26131;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26694;&#26550;&#21482;&#33021;&#20351;&#39044;&#27979;&#22402;&#30452;&#31283;&#23450;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31616;&#21333;&#32447;&#24615;&#25554;&#20540;&#30340;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#22402;&#30452;&#21644;&#27700;&#24179;&#31283;&#23450;&#21270;&#20219;&#20309;&#22522;&#30784;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#20135;&#29983;&#20934;&#30830;&#32780;&#31283;&#23450;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasts are typically not produced in a vacuum but in a business context, where forecasts are generated on a regular basis and interact with each other. For decisions, it may be important that forecasts do not change arbitrarily, and are stable in some sense. However, this area has received only limited attention in the forecasting literature. In this paper, we explore two types of forecast stability that we call vertical stability and horizontal stability. The existing works in the literature are only applicable to certain base models and extending these frameworks to be compatible with any base model is not straightforward. Furthermore, these frameworks can only stabilise the forecasts vertically. To fill this gap, we propose a simple linear-interpolation-based approach that is applicable to stabilise the forecasts provided by any base model vertically and horizontally. The approach can produce both accurate and stable forecasts. Using N-BEATS, Pooled Regression and LightGBM as the
&lt;/p&gt;</description></item><item><title>CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.08944</link><description>&lt;p&gt;
CAMELL&#65306;&#22522;&#20110;&#32622;&#20449;&#24230;&#30340;&#39640;&#25928;&#33258;&#30417;&#30563;&#20027;&#21160;&#23398;&#20064;&#19982;&#26631;&#31614;&#39564;&#35777;&#33719;&#21462;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CAMELL: Confidence-based Acquisition Model for Efficient Self-supervised Active Learning with Label Validation. (arXiv:2310.08944v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08944
&lt;/p&gt;
&lt;p&gt;
CAMELL&#26159;&#19968;&#20010;&#36866;&#29992;&#20110;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20165;&#38656;&#19987;&#23478;&#26631;&#27880;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#12289;&#33258;&#30417;&#30563;&#21644;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#26469;&#35299;&#20915;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#23545;&#22823;&#35268;&#27169;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#21463;&#22823;&#35268;&#27169;&#19988;&#31934;&#30830;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#20381;&#36182;&#38480;&#21046;&#65292;&#30417;&#30563;&#31070;&#32463;&#26041;&#27861;&#21463;&#21040;&#38459;&#30861;&#12290;&#26631;&#27880;&#36136;&#37327;&#38543;&#30528;&#20174;&#19987;&#23478;&#26631;&#27880;&#21521;&#20247;&#21253;&#26631;&#27880;&#30340;&#36716;&#21464;&#32780;&#36880;&#28176;&#24694;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CAMELL&#65288;Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#24207;&#21015;&#22810;&#36755;&#20986;&#38382;&#39064;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#27744;&#21270;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;CAMELL&#20855;&#26377;&#19977;&#20010;&#26680;&#24515;&#29305;&#28857;&#65306;(1)&#20165;&#35201;&#27714;&#19987;&#23478;&#26631;&#27880;&#25152;&#36873;&#24207;&#21015;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;(2)&#20026;&#20854;&#20313;&#24207;&#21015;&#25552;&#20379;&#33258;&#30417;&#30563;&#65292;(3)&#37319;&#29992;&#26631;&#31614;&#39564;&#35777;&#26426;&#21046;&#65292;&#38450;&#27490;&#38169;&#35823;&#26631;&#31614;&#27745;&#26579;&#25968;&#25454;&#38598;&#24182;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#24207;&#21015;&#20219;&#21153;&#20013;&#23545;CAMELL&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#29305;&#21035;&#24378;&#35843;&#23545;&#35805;&#20449;&#24565;&#36319;&#36394;&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#38480;&#21046;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised neural approaches are hindered by their dependence on large, meticulously annotated datasets, a requirement that is particularly cumbersome for sequential tasks. The quality of annotations tends to deteriorate with the transition from expert-based to crowd-sourced labelling. To address these challenges, we present \textbf{CAMELL} (Confidence-based Acquisition Model for Efficient self-supervised active Learning with Label validation), a pool-based active learning framework tailored for sequential multi-output problems. CAMELL possesses three core features: (1) it requires expert annotators to label only a fraction of a chosen sequence, (2) it facilitates self-supervision for the remainder of the sequence, and (3) it employs a label validation mechanism to prevent erroneous labels from contaminating the dataset and harming model performance. We evaluate CAMELL on sequential tasks, with a special emphasis on dialogue belief tracking, a task plagued by the constraints of limited
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.05239</link><description>&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS).
&lt;/p&gt;
&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS). (arXiv:2308.05239v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05239
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#36719;&#20214;&#12289;&#31995;&#32479;&#21644;&#20225;&#19994;&#30340;&#26550;&#26500;&#26694;&#26550;&#12290;&#23427;&#20204;&#35782;&#21035;&#20102;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#24182;&#23450;&#20041;&#20102;&#26550;&#26500;&#30340;&#35266;&#28857;&#21644;&#35270;&#22270;&#65292;&#20197;&#26694;&#26550;&#21644;&#35299;&#20915;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20851;&#27880;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#26550;&#26500;&#26694;&#26550;&#20013;&#65292;&#23578;&#26410;&#21253;&#25324;&#19982;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#25968;&#25454;&#24037;&#31243;&#24072;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26410;&#33021;&#35299;&#20915;&#21709;&#24212;&#25968;&#25454;&#31185;&#23398;&#31038;&#21306;&#20851;&#27880;&#30340;&#26550;&#26500;&#35270;&#28857;&#21644;&#35270;&#22270;&#12290;&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#36866;&#29992;&#20110;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#30340;&#26550;&#26500;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#26222;&#36941;&#23384;&#22312;&#19988;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;CPS&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#32452;&#36866;&#24212;CPS&#39640;&#25928;&#24320;&#21457;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#20248;&#28857;&#26631;&#20934;&#65292;&#21363;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;CPS&#30340;&#26631;&#20934;&#65292;
&lt;/p&gt;
&lt;p&gt;
Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#36879;&#26126;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25490;&#21517;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>http://arxiv.org/abs/2308.01196</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#36879;&#26126;&#30340;&#25512;&#33616;&#31995;&#32479;: &#29992;&#20110;&#35299;&#37322;&#24615;&#30340;&#36125;&#21494;&#26031;&#22270;&#20687;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Sustainable Transparency in Recommender Systems: Bayesian Ranking of Images for Explainability. (arXiv:2308.01196v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#23454;&#29616;&#25512;&#33616;&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#36879;&#26126;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36125;&#21494;&#26031;&#25490;&#21517;&#22270;&#20687;&#36827;&#34892;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#20197;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#29616;&#20195;&#19990;&#30028;&#20013;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#24120;&#25351;&#23548;&#29992;&#25143;&#25214;&#21040;&#30456;&#20851;&#30340;&#20869;&#23481;&#25110;&#20135;&#21697;&#65292;&#24182;&#23545;&#29992;&#25143;&#21644;&#20844;&#27665;&#30340;&#20915;&#31574;&#20135;&#29983;&#37325;&#22823;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65307;&#20010;&#24615;&#21270;&#35299;&#37322;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#25512;&#33616;&#25552;&#20379;&#29702;&#30001;&#12290;&#22312;&#29983;&#25104;&#20010;&#24615;&#21270;&#35299;&#37322;&#30340;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#20351;&#29992;&#29992;&#25143;&#21019;&#24314;&#30340;&#35270;&#35273;&#20869;&#23481;&#26159;&#19968;&#20010;&#29305;&#21035;&#26377;&#28508;&#21147;&#30340;&#36873;&#39033;&#65292;&#26377;&#28508;&#21147;&#26368;&#22823;&#21270;&#36879;&#26126;&#24230;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#35299;&#37322;&#25512;&#33616;&#26102;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#21487;&#25345;&#32493;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#23548;&#33268;&#30340;&#30899;&#25490;&#25918;&#37327;&#19982;&#23427;&#20204;&#34987;&#25972;&#21512;&#21040;&#25512;&#33616;&#31995;&#32479;&#20013;&#30456;&#24403;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#20351;&#29992;&#30340;&#26367;&#20195;&#23398;&#20064;&#30446;&#26631;&#19982;&#25490;&#21517;&#26368;&#26377;&#25928;&#30340;&#30446;&#26631;&#19981;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender Systems have become crucial in the modern world, commonly guiding users towards relevant content or products, and having a large influence over the decisions of users and citizens. However, ensuring transparency and user trust in these systems remains a challenge; personalized explanations have emerged as a solution, offering justifications for recommendations. Among the existing approaches for generating personalized explanations, using visual content created by the users is one particularly promising option, showing a potential to maximize transparency and user trust. Existing models for explaining recommendations in this context face limitations: sustainability has been a critical concern, as they often require substantial computational resources, leading to significant carbon emissions comparable to the Recommender Systems where they would be integrated. Moreover, most models employ surrogate learning goals that do not align with the objective of ranking the most effect
&lt;/p&gt;</description></item><item><title>MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.01697</link><description>&lt;p&gt;
MutateNN&#65306;&#29992;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#31361;&#21464;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators. (arXiv:2306.01697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01697
&lt;/p&gt;
&lt;p&gt;
MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#24182;&#25512;&#21160;&#25216;&#26415;&#21457;&#23637;&#30340;&#26032;&#26426;&#36935;&#24212;&#36816;&#32780;&#29983;&#12290;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#29305;&#21035;&#26159;&#34987;&#20998;&#37197;&#20102;&#24863;&#30693;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#24182;&#23548;&#33268;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#36164;&#28304;&#38656;&#27714;&#20063;&#26377;&#25152;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#20248;&#21270;&#21644;&#30828;&#20214;&#21152;&#36895;&#24050;&#25104;&#20026;&#20851;&#38190;&#25216;&#26415;&#65292;&#20294;&#26377;&#25928;&#25972;&#21512;&#36825;&#20123;&#27010;&#24565;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35753;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#22312;&#19981;&#21516;&#30828;&#20214;&#21152;&#36895;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MutateNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#27492;&#30446;&#30340;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#21151;&#33021;&#65292;&#25105;&#20204;&#23545;7&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;21&#31181;&#21464;&#24322;&#12290;&#25105;&#20204;&#22312;4&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#20102;&#25105;&#20204;&#30340;&#21464;&#24322;&#20307;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#34892;&#20026;&#65292;&#24182;&#35780;&#20272;&#20102;MutateNN&#22312;&#26816;&#27979;&#20986;&#19981;&#27491;&#30830;&#25110;&#19981;&#31934;&#30830;&#30340;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the research advancement of Artificial Intelligence in the last years, there are new opportunities to mitigate real-world problems and advance technologically. Image recognition models in particular, are assigned with perception tasks to mitigate complex real-world challenges and lead to new solutions. Furthermore, the computational complexity and demand for resources of such models has also increased. To mitigate this, model optimization and hardware acceleration has come into play, but effectively integrating such concepts is a challenging and error-prone process.  In order to allow developers and researchers to explore the robustness of deep learning image recognition models deployed on different hardware acceleration devices, we propose MutateNN, a tool that provides mutation testing and analysis capabilities for that purpose. To showcase its capabilities, we utilized 21 mutations for 7 widely-known pre-trained deep neural network models. We deployed our mutants on 4 different
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2303.17251</link><description>&lt;p&gt;
&#25581;&#24320;&#23545;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#35823;&#35299;
&lt;/p&gt;
&lt;p&gt;
Demystifying Misconceptions in Social Bots Research. (arXiv:2303.17251v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25581;&#31034;&#20102;&#20851;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#26222;&#36941;&#35823;&#35299;&#65292;&#24378;&#35843;&#38656;&#35201;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#26426;&#22120;&#20154;&#31185;&#23398;&#23547;&#27714;&#35299;&#20915;&#32593;&#32476;&#34394;&#20551;&#20449;&#24687;&#26368;&#21463;&#20105;&#35758;&#30340;&#24418;&#24335;&#20043;&#19968;&#30340;&#30693;&#35782;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#21463;&#21040;&#26222;&#36941;&#30340;&#20559;&#35265;&#12289;&#22840;&#22823;&#30340;&#32467;&#26524;&#21644;&#35823;&#35299;&#30340;&#22256;&#25200;&#65292;&#36825;&#20123;&#37117;&#20026;&#27495;&#20041;&#12289;&#19981;&#20999;&#23454;&#38469;&#30340;&#26399;&#26395;&#21644;&#30475;&#20284;&#26080;&#27861;&#35843;&#21644;&#30340;&#21457;&#29616;&#25171;&#19979;&#20102;&#22522;&#30784;&#12290;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#23545;&#20110;&#30830;&#20445;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#37325;&#30003;&#31185;&#23398;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#20462;&#35746;&#20102;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#20013;&#30340;&#19968;&#20123;&#26368;&#26032;&#32467;&#26524;&#65292;&#24378;&#35843;&#21644;&#32416;&#27491;&#20102;&#20107;&#23454;&#38169;&#35823;&#20197;&#21450;&#26041;&#27861;&#35770;&#21644;&#27010;&#24565;&#38382;&#39064;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#24320;&#20102;&#26222;&#36941;&#30340;&#35823;&#35299;&#65292;&#35299;&#20915;&#20102;&#26377;&#20851;&#22914;&#20309;&#35752;&#35770;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#20197;&#20005;&#35880;&#12289;&#20844;&#27491;&#21644;&#36127;&#36131;&#20219;&#30340;&#26041;&#24335;&#35752;&#35770;&#34394;&#20551;&#20449;&#24687;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#30830;&#23450;&#24182;&#39539;&#26021;&#31038;&#20132;&#26426;&#22120;&#20154;&#30740;&#31350;&#30340;&#25903;&#25345;&#32773;&#21644;&#21453;&#23545;&#32773;&#24120;&#29992;&#30340;&#35884;&#35823;&#35770;&#35777;&#65292;&#25903;&#25345;&#36825;&#31181;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The science of social bots seeks knowledge and solutions to one of the most debated forms of online misinformation. Yet, social bots research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution we revise some recent results in social bots research, highlighting and correcting factual errors as well as methodological and conceptual issues. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss misinformation research in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#33258;&#36866;&#24212;&#36827;&#21270;&#29305;&#24449;&#36873;&#25321;&#19982;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#31649;&#29702;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#30830;&#23450;&#20102;&#36866;&#24403;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#27700;&#24179;&#65292;&#21487;&#20197;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.11949</link><description>&lt;p&gt;
&#19968;&#31181;&#27169;&#31946;&#33258;&#36866;&#24212;&#36827;&#21270;&#29305;&#24449;&#36873;&#25321;&#19982;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A fuzzy adaptive evolutionary-based feature selection and machine learning framework for single and multi-objective body fat prediction. (arXiv:2303.11949v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#31946;&#33258;&#36866;&#24212;&#36827;&#21270;&#29305;&#24449;&#36873;&#25321;&#19982;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#31649;&#29702;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#25104;&#26412;&#30340;&#21516;&#26102;&#30830;&#23450;&#20102;&#36866;&#24403;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#27700;&#24179;&#65292;&#21487;&#20197;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#36523;&#20307;&#33026;&#32938;&#21487;&#20197;&#20026;&#21307;&#23398;&#20174;&#19994;&#32773;&#21644;&#29992;&#25143;&#25552;&#20379;&#39044;&#38450;&#21644;&#35786;&#26029;&#24515;&#33039;&#30142;&#30149;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#28151;&#21512;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36890;&#36807;&#36873;&#25321;&#30456;&#20851;&#30340;&#36523;&#20307;&#27979;&#37327;&#20540;&#21644;&#25429;&#25417;&#27169;&#22411;&#20013;&#25152;&#36873;&#29305;&#24449;&#20043;&#38388;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#25552;&#20379;&#20102;&#27604;&#31616;&#21333;&#30340;&#22238;&#24402;&#20998;&#26512;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#12290;&#24403;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#24314;&#27169;&#26041;&#27861;&#23558;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;&#38382;&#39064;&#24314;&#27169;&#20026;&#32452;&#21512;&#30340;&#21333;&#30446;&#26631;&#21644;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24448;&#24448;&#20250;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#12290;&#24403;&#22810;&#20010;&#29305;&#24449;&#23376;&#38598;&#20135;&#29983;&#31867;&#20284;&#25110;&#25509;&#36817;&#30340;&#39044;&#27979;&#26102;&#65292;&#36991;&#20813;&#23616;&#37096;&#26368;&#20248;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#36827;&#21270;&#29305;&#24449;&#36873;&#25321;&#24050;&#34987;&#29992;&#20110;&#35299;&#20915;&#20960;&#20010;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#19968;&#20010;&#27169;&#31946;&#38598;&#29702;&#35770;&#30830;&#23450;&#36866;&#24403;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#27700;&#24179;&#65292;&#21516;&#26102;&#31649;&#29702;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#37319;&#29992;&#21152;&#26435;&#21644;&#36523;&#20307;&#33026;&#32938;&#39044;&#27979;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Predicting body fat can provide medical practitioners and users with essential information for preventing and diagnosing heart diseases. Hybrid machine learning models offer better performance than simple regression analysis methods by selecting relevant body measurements and capturing complex nonlinear relationships among selected features in modelling body fat prediction problems. There are, however, some disadvantages to them. Current machine learning. Modelling body fat prediction as a combinatorial single- and multi-objective optimisation problem often gets stuck in local optima. When multiple feature subsets produce similar or close predictions, avoiding local optima becomes more complex. Evolutionary feature selection has been used to solve several machine-learning-based optimisation problems. A fuzzy set theory determines appropriate levels of exploration and exploitation while managing parameterisation and computational costs. A weighted-sum body fat prediction approach was ex
&lt;/p&gt;</description></item></channel></rss>