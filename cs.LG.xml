<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>https://arxiv.org/abs/2403.14539</link><description>&lt;p&gt;
Object-Centric Domain Randomization&#29992;&#20110;&#37326;&#22806;3D&#24418;&#29366;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35270;&#22270;3D&#24418;&#29366;&#22312;&#37326;&#22806;&#30340;&#37325;&#24314;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#26469;&#33258;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&lt;3D&#24418;&#29366;&#65292;2D&#22270;&#20687;&gt;-&#37197;&#23545;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#21463;&#22495;&#38543;&#26426;&#21270;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ObjectDR&#65292;&#36890;&#36807;&#23545;&#23545;&#35937;&#22806;&#35266;&#21644;&#32972;&#26223;&#30340;&#35270;&#35273;&#21464;&#21270;&#36827;&#34892;&#38543;&#26426;&#20223;&#30495;&#65292;&#21512;&#25104;&#36825;&#31181;&#37197;&#23545;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;ControlNet&#65289;&#29983;&#25104;&#31526;&#21512;&#31354;&#38388;&#26465;&#20214;&#65288;&#20363;&#22914;2.5D&#33609;&#22270;&#65289;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#26465;&#20214;&#21487;&#20197;&#36890;&#36807;&#20174;&#23545;&#35937;&#38598;&#21512;&#65288;&#20363;&#22914;Objaverse-XL&#65289;&#30340;&#28210;&#26579;&#36807;&#31243;&#33719;&#24471;3D&#24418;&#29366;&#12290;&#20026;&#20102;&#27169;&#25311;&#22810;&#26679;&#21270;&#30340;&#21464;&#21270;&#21516;&#26102;&#20445;&#30041;&#23884;&#20837;&#31354;&#38388;&#26465;&#20214;&#20013;&#30340;&#23545;&#35937;&#36718;&#24275;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21033;&#29992;&#21021;&#22987;&#23545;&#35937;&#25351;&#23548;&#30340;&#35299;&#32806;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14539v1 Announce Type: cross  Abstract: One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of &lt;3D shape, 2D image&gt;-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#36817;&#20284;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#65292;&#25104;&#21151;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#34920;&#29616;&#20986;&#33394;.</title><link>https://arxiv.org/abs/2403.04764</link><description>&lt;p&gt;
&#23558;Thompson&#25277;&#26679;&#36951;&#25022;&#19982;Sigma&#27604;&#29575;&#65288;TS-RSR&#65289;&#26368;&#23567;&#21270;&#65306;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#32463;&#36807;&#35777;&#26126;&#30340;&#39640;&#25928;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimizing the Thompson Sampling Regret-to-Sigma Ratio (TS-RSR): a provably efficient algorithm for batch Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#36817;&#20284;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#65292;&#25104;&#21151;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#30340;&#21160;&#20316;&#36873;&#25321;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#34920;&#29616;&#20986;&#33394;.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25209;&#37327;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#65292;&#20854;&#20013;&#25277;&#26679;&#36890;&#36807;&#26368;&#23567;&#21270;Thompson&#25277;&#26679;&#26041;&#27861;&#30340;&#36951;&#25022;&#19982;&#19981;&#30830;&#23450;&#24615;&#27604;&#29575;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#33021;&#22815;&#21327;&#35843;&#27599;&#20010;&#25209;&#27425;&#20013;&#36873;&#25321;&#30340;&#21160;&#20316;&#65292;&#20197;&#26368;&#23567;&#21270;&#28857;&#20043;&#38388;&#30340;&#20887;&#20313;&#65292;&#21516;&#26102;&#20851;&#27880;&#20855;&#26377;&#39640;&#39044;&#27979;&#22343;&#20540;&#25110;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#28857;&#12290;&#25105;&#20204;&#23545;&#31639;&#27861;&#30340;&#36951;&#25022;&#25552;&#20379;&#20102;&#39640;&#27010;&#29575;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#26368;&#21518;&#65292;&#20174;&#25968;&#23383;&#19978;&#30475;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;&#38750;&#20984;&#27979;&#35797;&#20989;&#25968;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;&#24179;&#22343;&#20540;&#19978;&#27604;&#20960;&#20010;&#31454;&#20105;&#23545;&#25163;&#30340;&#22522;&#20934;&#25209;&#37327;BO&#31639;&#27861;&#34920;&#29616;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04764v1 Announce Type: new  Abstract: This paper presents a new approach for batch Bayesian Optimization (BO), where the sampling takes place by minimizing a Thompson Sampling approximation of a regret to uncertainty ratio. Our objective is able to coordinate the actions chosen in each batch in a way that minimizes redundancy between points whilst focusing on points with high predictive means or high uncertainty. We provide high-probability theoretical guarantees on the regret of our algorithm. Finally, numerically, we demonstrate that our method attains state-of-the-art performance on a range of nonconvex test functions, where it outperforms several competitive benchmark batch BO algorithms by an order of magnitude on average.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#20016;&#23500;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#65292;&#26377;&#26395;&#20811;&#26381;&#29616;&#23384;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.16887</link><description>&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#28508;&#21147;&#12289;&#26041;&#27861;&#35770;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Complex Network: Potential, Methodology and Application
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16887
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#19982;&#20016;&#23500;&#30495;&#23454;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#65292;&#26377;&#26395;&#20811;&#26381;&#29616;&#23384;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#23384;&#22312;&#20110;&#21508;&#31181;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#65292;&#20174;&#33258;&#28982;&#29615;&#22659;&#21040;&#20154;&#31867;&#31038;&#20250;&#12290;&#36825;&#20123;&#32593;&#32476;&#30340;&#26412;&#36136;&#22312;&#20110;&#23427;&#20204;&#33021;&#22815;&#20174;&#24494;&#35266;&#28151;&#20081;-&#20854;&#20013;&#32593;&#32476;&#25299;&#25169;&#21644;&#33410;&#28857;&#21160;&#24577;&#20132;&#32455;-&#36716;&#21464;&#21644;&#28436;&#21270;&#20026;&#20855;&#26377;&#29305;&#23450;&#38598;&#20307;&#34892;&#20026;&#30340;&#23439;&#35266;&#31209;&#24207;&#12290;&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#37324;&#65292;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#26174;&#33879;&#22686;&#24378;&#20102;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#28508;&#22312;&#26426;&#21046;&#12289;&#32467;&#26500;&#21644;&#21160;&#24577;&#30340;&#29702;&#35299;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#22312;&#25506;&#32034;&#26356;&#21152;&#30495;&#23454;&#31995;&#32479;&#21644;&#25552;&#21319;&#23454;&#38469;&#24212;&#29992;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#25361;&#25112;&#12290;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#20197;&#21450;&#20016;&#23500;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#24320;&#21551;&#20102;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#30340;&#26032;&#26102;&#20195;&#12290;&#26412;&#35843;&#26597;&#26088;&#22312;&#31995;&#32479;&#22320;&#25506;&#35752;&#20154;&#24037;&#26234;&#33021;&#22312;&#20811;&#26381;&#22797;&#26434;&#32593;&#32476;&#31185;&#23398;&#30740;&#31350;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16887v1 Announce Type: cross  Abstract: Complex networks pervade various real-world systems, from the natural environment to human societies. The essence of these networks is in their ability to transition and evolve from microscopic disorder-where network topology and node dynamics intertwine-to a macroscopic order characterized by certain collective behaviors. Over the past two decades, complex network science has significantly enhanced our understanding of the statistical mechanics, structures, and dynamics underlying real-world networks. Despite these advancements, there remain considerable challenges in exploring more realistic systems and enhancing practical applications. The emergence of artificial intelligence (AI) technologies, coupled with the abundance of diverse real-world network data, has heralded a new era in complex network science research. This survey aims to systematically address the potential advantages of AI in overcoming the lingering challenges of com
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;</title><link>https://arxiv.org/abs/2402.14264</link><description>&lt;p&gt;
&#21452;&#31283;&#20581;&#23398;&#20064;&#22312;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#32467;&#26500;&#19981;&#21487;&#30693;&#24615;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14264
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#26368;&#26680;&#24515;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#24191;&#27867;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#20272;&#35745;&#31574;&#30053;&#65292;&#26368;&#36817;&#36824;&#32435;&#20837;&#20102;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#37319;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#32479;&#35745;&#19979;&#30028;&#32467;&#26500;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#24178;&#25200;&#20989;&#25968;&#27809;&#26377;&#32467;&#26500;&#24615;&#36136;&#20551;&#35774;&#65292;&#38500;&#20102;&#35775;&#38382;&#40657;&#30418;&#20272;&#35745;&#22120;&#20197;&#36798;&#21040;&#23567;&#35823;&#24046;&#65307;&#24403;&#21482;&#24895;&#24847;&#32771;&#34385;&#20351;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#20998;&#31867;&#31070;&#35861;&#20316;&#20026;&#40657;&#30418;&#23376;&#36807;&#31243;&#30340;&#20272;&#35745;&#31574;&#30053;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#23545;&#20110;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14264v1 Announce Type: cross  Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Avera
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10793</link><description>&lt;p&gt;
&#25513;&#30721;&#27880;&#24847;&#21147;&#26159;&#22270;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Masked Attention is All You Need for Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#21464;&#31181;&#20027;&#35201;&#29992;&#20110;&#22312;&#22270;&#19978;&#23398;&#20064;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#28789;&#27963;&#24615;&#12289;&#36895;&#24230;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;GNNs&#38656;&#35201;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#28040;&#24687;&#20256;&#36882;&#25805;&#20316;&#31526;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#27880;&#24847;&#21147;&#12290;&#22270;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#25110;&#36793;&#38598;&#65292;&#24182;&#36890;&#36807;&#25513;&#30721;&#27880;&#24847;&#26435;&#37325;&#30697;&#38453;&#26469;&#24378;&#21046;&#23427;&#20204;&#30340;&#36830;&#25509;&#65292;&#26377;&#25928;&#22320;&#20026;&#27599;&#20010;&#22270;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#29992;&#20110;&#22270;&#30340;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;55&#22810;&#20010;&#33410;&#28857;&#21644;&#22270;&#32423;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#28040;&#24687;&#20256;&#36882;&#22522;&#32447;&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10793v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2402.10686</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#21644;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65306;&#20449;&#24687;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#20013;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#36807;&#24230;&#33258;&#20449;&#26469;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20869;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#65288;LiRA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20801;&#35768;&#30740;&#31350;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#30001;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#38598;&#24341;&#36215;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#30446;&#26631;&#27169;&#22411;&#30340;&#26657;&#20934;&#27700;&#24179;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#20174;&#30446;&#26631;&#27169;&#22411;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#36880;&#28176;&#20943;&#23569;&#65306;&#32622;&#20449;&#21521;&#37327;&#65288;CV&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#36755;&#20986;&#27010;&#29575;&#21521;&#37327;&#34987;&#21457;&#24067;&#65307;&#30495;&#23454;&#26631;&#31614;&#32622;&#20449;&#24230;&#65288;TLC&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#21482;&#26377;&#27169;&#22411;&#20998;&#37197;&#32473;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#26159;&#21487;&#29992;&#30340;&#65307;&#20197;&#21450;&#20915;&#31574;&#38598;&#65288;DS&#65289;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10686v1 Announce Type: cross  Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.10504</link><description>&lt;p&gt;
&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of the quadratic Littlewood-Offord problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10504
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#32500;&#25968;&#25454;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$&#21453;&#38598;&#20013;&#29305;&#24615;&#30340;&#24433;&#21709;&#30340;&#20272;&#35745;&#65292;&#20854;&#20013;$M$&#26159;&#19968;&#20010;&#22266;&#23450;&#30340;&#65288;&#39640;&#32500;&#65289;&#30697;&#38453;&#65292;$\boldsymbol{\xi}$&#26159;&#19968;&#20010;&#20849;&#24418;Rademacher&#21521;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;$\boldsymbol{\xi}$&#33021;&#22815;&#25215;&#21463;&#22810;&#23569;&#23545;&#25239;&#24615;&#31526;&#21495;&#32763;&#36716;&#32780;&#19981;&#8220;&#33192;&#32960;&#8221;$\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$&#65292;&#20174;&#32780;&#8220;&#21435;&#38500;&#8221;&#21407;&#22987;&#20998;&#24067;&#23548;&#33268;&#26356;&#8220;&#26377;&#31890;&#24230;&#8221;&#21644;&#23545;&#25239;&#24615;&#20559;&#20506;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#19979;&#38480;&#20272;&#35745;&#65307;&#36825;&#20123;&#32467;&#26524;&#22312;&#20851;&#38190;&#21306;&#22495;&#34987;&#35777;&#26126;&#26159;&#28176;&#36817;&#32039;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10504v1 Announce Type: cross  Abstract: We study the statistical resilience of high-dimensional data. Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher vector. Specifically, we pursue the question of how many adversarial sign-flips can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$ and thus "de-smooth" the original distribution resulting in a more "grainy" and adversarially biased distribution. Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.10500</link><description>&lt;p&gt;
&#36890;&#36807;&#20027;&#21160;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#32463;&#39564;&#35777;&#30340;&#26679;&#26412;&#25928;&#29575;&#30340;RLHF
&lt;/p&gt;
&lt;p&gt;
Provably Sample Efficient RLHF via Active Preference Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10500
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Active Preference Optimization&#31639;&#27861;&#65292;&#22312;Bradley-Terry-Luce&#20559;&#22909;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;RLHF&#30340;&#26679;&#26412;&#25928;&#29575;&#25552;&#39640;&#65292;&#20248;&#21270;&#20102;&#23545;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#40784;&#30340;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#26159;&#20381;&#36182;&#39640;&#36136;&#37327;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#22312;&#23454;&#38469;RLHF&#23454;&#26045;&#20013;&#26500;&#25104;&#20102;&#26114;&#36149;&#30340;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26356;&#22909;&#21644;&#33258;&#36866;&#24212;&#30340;&#25968;&#25454;&#25910;&#38598;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;RLHF&#20197;&#19978;&#19979;&#25991;&#20559;&#22909;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#24418;&#24335;&#26694;&#23450;&#65292;&#20854;&#20013;&#25552;&#31034;&#20316;&#20026;&#19978;&#19979;&#25991;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#38543;&#26426;&#36873;&#25321;&#25552;&#31034;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#30340;&#22825;&#30495;&#26041;&#24335;&#23548;&#33268;&#19968;&#20010;&#22312;&#22870;&#21169;&#26041;&#38754;&#20855;&#26377;$\Omega(1)$&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\textit{Active Preference Optimization}$&#65288;$\texttt{APO}$&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#31215;&#26497;&#36873;&#25321;&#25552;&#31034;&#20197;&#25910;&#38598;&#20559;&#22909;&#25968;&#25454;&#12290;&#22312;Bradley-Terry-Luce&#65288;BTL&#65289;&#20559;&#22909;&#27169;&#22411;&#19979;&#65292;\texttt{APO}&#23454;&#29616;&#20102;&#26679;&#26412;&#25928;&#29575;&#65292;&#32780;&#19981;&#20250;&#22949;&#21327;&#20110;polic
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10500v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is pivotal in aligning Large Language Models (LLMs) with human preferences. While these aligned generative models have demonstrated impressive capabilities across various tasks, the dependence on high-quality human preference data poses a costly bottleneck in practical implementation of RLHF. Hence better and adaptive strategies for data collection is needed. To this end, we frame RLHF as a contextual preference bandit problem with prompts as contexts and show that the naive way of collecting preference data by choosing prompts uniformly at random leads to a policy that suffers an $\Omega(1)$ suboptimality gap in rewards. Then we propose $\textit{Active Preference Optimization}$ ($\texttt{APO}$), an algorithm that actively selects prompts to collect preference data. Under the Bradley-Terry-Luce (BTL) preference model, \texttt{APO} achieves sample efficiency without compromising on polic
&lt;/p&gt;</description></item><item><title>EasyFS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#24377;&#24615;&#25193;&#23637;&#21644;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#24314;&#27169;&#65292;&#24182;&#21457;&#29616;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26032;&#30340;&#20887;&#20313;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#20887;&#20313;&#29305;&#24449;&#30340;&#39640;&#25928;&#36807;&#28388;&#12290;</title><link>https://arxiv.org/abs/2402.05954</link><description>&lt;p&gt;
EasyFS:&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#30340;&#24377;&#24615;&#21464;&#25442;&#23454;&#29616;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyFS: an Efficient Model-free Feature Selection Framework via Elastic Transformation of Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05954
&lt;/p&gt;
&lt;p&gt;
EasyFS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#24377;&#24615;&#25193;&#23637;&#21644;&#21387;&#32553;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#20043;&#38388;&#30456;&#20114;&#20851;&#31995;&#30340;&#24314;&#27169;&#65292;&#24182;&#21457;&#29616;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#26032;&#30340;&#20887;&#20313;&#24230;&#24230;&#37327;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#20887;&#20313;&#29305;&#24449;&#30340;&#39640;&#25928;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#23558;&#27599;&#20010;&#29305;&#24449;&#29420;&#31435;&#22788;&#29702;&#65292;&#24573;&#35270;&#20102;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#36825;&#23548;&#33268;&#20854;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#65292;&#19982;&#27169;&#22411;&#24863;&#30693;&#26041;&#27861;&#30456;&#27604;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#29305;&#24449;&#36827;&#34892;&#24377;&#24615;&#25193;&#23637;&#21644;&#21387;&#32553;&#30340;&#39640;&#25928;&#26080;&#27169;&#22411;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#8212;&#8212;EasyFS&#65292;&#20197;&#23454;&#29616;&#27604;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#24863;&#30693;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#22791;&#29616;&#26377;&#26080;&#27169;&#22411;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#28789;&#27963;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EasyFS&#37319;&#29992;&#38543;&#26426;&#38750;&#32447;&#24615;&#25237;&#24433;&#32593;&#32476;&#25193;&#23637;&#29305;&#24449;&#31354;&#38388;&#65292;&#23454;&#29616;&#21407;&#22987;&#29305;&#24449;&#30340;&#38750;&#32447;&#24615;&#32452;&#21512;&#65292;&#20197;&#24314;&#27169;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#24182;&#21457;&#29616;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32534;&#30721;&#29575;&#21464;&#21270;&#30340;&#26032;&#22411;&#20887;&#20313;&#24230;&#24230;&#37327;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#36807;&#28388;&#20887;&#20313;&#29305;&#24449;&#12290;&#22312;21&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
Traditional model-free feature selection methods treat each feature independently while disregarding the interrelationships among features, which leads to relatively poor performance compared with the model-aware methods. To address this challenge, we propose an efficient model-free feature selection framework via elastic expansion and compression of the features, namely EasyFS, to achieve better performance than state-of-the-art model-aware methods while sharing the characters of efficiency and flexibility with the existing model-free methods. In particular, EasyFS expands the feature space by using the random non-linear projection network to achieve the non-linear combinations of the original features, so as to model the interrelationships among the features and discover most correlated features. Meanwhile, a novel redundancy measurement based on the change of coding rate is proposed for efficient filtering of redundant features. Comprehensive experiments on 21 different datasets sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04062</link><description>&lt;p&gt;
&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction with Relational Hypergraphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#36866;&#29992;&#20110;&#20851;&#31995;&#36229;&#22270;&#30340;&#38142;&#25509;&#39044;&#27979;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#26694;&#26550;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#24050;&#32463;&#36827;&#34892;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#65292;&#23548;&#33268;&#20102;&#20855;&#26377;&#25104;&#21151;&#24212;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#20016;&#23500;&#26223;&#35266;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#25104;&#21151;&#36716;&#31227;&#21040;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20851;&#31995;&#36229;&#36793;&#30340;&#23384;&#22312;&#20351;&#24471;&#38142;&#25509;&#39044;&#27979;&#25104;&#20026;&#22312;&#19981;&#21516;&#36873;&#25321;&#30340;k&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#20219;&#21153;&#65292;&#36825;&#27604;&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#35201;&#22256;&#38590;&#24471;&#22810;&#65292;&#22240;&#20026;&#27599;&#20010;&#20851;&#31995;&#37117;&#26159;&#20108;&#36827;&#21046;&#30340;&#65288;k=2&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#20351;&#29992;&#20851;&#31995;&#36229;&#22270;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#30456;&#24212;&#30340;&#20851;&#31995;Weisfeiler-Leman&#31639;&#27861;&#20197;&#21450;&#19968;&#20123;&#33258;&#28982;&#36923;&#36753;&#24418;&#24335;&#23545;&#29983;&#25104;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#36827;&#34892;&#20102;&#24443;&#24213;&#20998;&#26512;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#25552;&#20986;&#30340;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#22312;&#21508;&#31181;&#20851;&#31995;&#36229;&#22270;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction with knowledge graphs has been thoroughly studied in graph machine learning, leading to a rich landscape of graph neural network architectures with successful applications. Nonetheless, it remains challenging to transfer the success of these architectures to link prediction with relational hypergraphs. The presence of relational hyperedges makes link prediction a task between $k$ nodes for varying choices of $k$, which is substantially harder than link prediction with knowledge graphs, where every relation is binary ($k=2$). In this paper, we propose two frameworks for link prediction with relational hypergraphs and conduct a thorough analysis of the expressive power of the resulting model architectures via corresponding relational Weisfeiler-Leman algorithms, and also via some natural logical formalisms. Through extensive empirical analysis, we validate the power of the proposed model architectures on various relational hypergraph benchmarks. The resulting model archit
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;&#26041;&#27861;RONIN&#12290;&#36890;&#36807;&#23558;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#20462;&#22797;&#26367;&#25442;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#24471;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;OOD&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#23545;&#35937;&#30456;&#24046;&#36739;&#36828;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;RONIN&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.03292</link><description>&lt;p&gt;
&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Object-Level OOD Detection with Context-Aware Inpainting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#19978;&#19979;&#25991;&#24863;&#30693;&#20462;&#22797;&#30340;&#38646;&#26679;&#26412;&#29289;&#20307;&#32423;OOD&#26816;&#27979;&#26041;&#27861;RONIN&#12290;&#36890;&#36807;&#23558;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#36827;&#34892;&#20462;&#22797;&#26367;&#25442;&#65292;&#24182;&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#24471;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;OOD&#24773;&#20917;&#19979;&#19982;&#21407;&#22987;&#23545;&#35937;&#30456;&#24046;&#36739;&#36828;&#65292;&#20174;&#32780;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;RONIN&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36234;&#26469;&#36234;&#22810;&#22320;&#20316;&#20026;&#40657;&#30418;&#20113;&#26381;&#21153;&#25110;&#39044;&#35757;&#32451;&#27169;&#22411;&#25552;&#20379;&#65292;&#26080;&#27861;&#35775;&#38382;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#23601;&#24341;&#21457;&#20102;&#38646;&#26679;&#26412;&#31163;&#32676;&#25968;&#25454;&#65288;OOD&#65289;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#26816;&#27979;&#19981;&#23646;&#20110;&#20998;&#31867;&#22120;&#26631;&#31614;&#38598;&#20294;&#34987;&#38169;&#35823;&#22320;&#24402;&#31867;&#20026;&#20837;&#22495;&#65288;ID&#65289;&#23545;&#35937;&#30340;OOD&#23545;&#35937;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;RONIN&#20351;&#29992;&#29616;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#29992;&#20462;&#22797;&#26367;&#25442;&#25481;&#26816;&#27979;&#21040;&#30340;&#23545;&#35937;&#12290;RONIN&#20351;&#29992;&#39044;&#27979;&#30340;ID&#26631;&#31614;&#26469;&#26465;&#20214;&#21270;&#20462;&#22797;&#36807;&#31243;&#65292;&#20351;&#36755;&#20837;&#23545;&#35937;&#25509;&#36817;&#20837;&#22495;&#22495;&#12290;&#32467;&#26524;&#26159;&#65292;&#37325;&#26500;&#30340;&#23545;&#35937;&#22312;ID&#24773;&#20917;&#19979;&#38750;&#24120;&#25509;&#36817;&#21407;&#22987;&#23545;&#35937;&#65292;&#22312;OOD&#24773;&#20917;&#19979;&#21017;&#30456;&#24046;&#36739;&#36828;&#65292;&#20351;&#24471;RONIN&#33021;&#22815;&#26377;&#25928;&#21306;&#20998;ID&#21644;OOD&#26679;&#26412;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;RONIN&#22312;&#38646;&#26679;&#26412;&#21644;&#38750;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#30456;&#23545;&#20110;&#20808;&#21069;&#26041;&#27861;&#65292;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are increasingly provided as black-box cloud services or pre-trained models, without access to their training data. This motivates the problem of zero-shot out-of-distribution (OOD) detection. Concretely, we aim to detect OOD objects that do not belong to the classifier's label set but are erroneously classified as in-distribution (ID) objects. Our approach, RONIN, uses an off-the-shelf diffusion model to replace detected objects with inpainting. RONIN conditions the inpainting process with the predicted ID label, drawing the input object closer to the in-distribution domain. As a result, the reconstructed object is very close to the original in the ID cases and far in the OOD cases, allowing RONIN to effectively distinguish ID and OOD samples. Throughout extensive experiments, we demonstrate that RONIN achieves competitive results compared to previous approaches across several datasets, both in zero-shot and non-zero-shot settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#20989;&#25968;&#32447;&#24615;&#22238;&#24402;&#19979;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;RKHS&#36317;&#31163;&#34913;&#37327;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#22788;&#29702;&#36801;&#31227;&#65292;&#19968;&#31181;&#38656;&#35201;&#24050;&#30693;&#27491;&#28304;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#32858;&#21512;&#25216;&#26415;&#23454;&#29616;&#26080;&#28304;&#20449;&#24687;&#30340;&#31283;&#20581;&#20256;&#36755;&#12290;&#21516;&#26102;&#24314;&#31435;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#19978;&#30028;&#12290;</title><link>https://arxiv.org/abs/2206.04277</link><description>&lt;p&gt;
&#20851;&#20110;&#20989;&#25968;&#32447;&#24615;&#27169;&#22411;&#20551;&#35774;&#36801;&#31227;&#23398;&#20064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Hypothesis Transfer Learning of Functional Linear Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.04277
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#20989;&#25968;&#32447;&#24615;&#22238;&#24402;&#19979;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;RKHS&#36317;&#31163;&#34913;&#37327;&#20219;&#21153;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#26469;&#22788;&#29702;&#36801;&#31227;&#65292;&#19968;&#31181;&#38656;&#35201;&#24050;&#30693;&#27491;&#28304;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#32858;&#21512;&#25216;&#26415;&#23454;&#29616;&#26080;&#28304;&#20449;&#24687;&#30340;&#31283;&#20581;&#20256;&#36755;&#12290;&#21516;&#26102;&#24314;&#31435;&#20102;&#23398;&#20064;&#38382;&#39064;&#30340;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#26694;&#26550;&#19979;&#30340;&#20989;&#25968;&#32447;&#24615;&#22238;&#24402;&#65288;FLR&#65289;&#30340;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#65292;&#35266;&#23519;&#21040;&#29616;&#26377;&#39640;&#32500;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;TL&#25216;&#26415;&#19982;&#22522;&#20110;&#25130;&#26029;&#30340;FLR&#26041;&#27861;&#19981;&#20860;&#23481;&#65292;&#22240;&#20026;&#20989;&#25968;&#25968;&#25454;&#22312;&#26412;&#36136;&#19978;&#26159;&#26080;&#38480;&#32500;&#30340;&#65292;&#24182;&#30001;&#24179;&#28369;&#30340;&#22522;&#30784;&#36807;&#31243;&#29983;&#25104;&#12290;&#25105;&#20204;&#20351;&#29992;RKHS&#36317;&#31163;&#26469;&#34913;&#37327;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20801;&#35768;&#20256;&#36755;&#30340;&#20449;&#24687;&#31867;&#22411;&#19982;&#25152;&#26045;&#21152;&#30340;RKHS&#30340;&#23646;&#24615;&#30456;&#20851;&#32852;&#12290;&#22522;&#20110;&#20551;&#35774;&#20559;&#31227;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#31639;&#27861;&#65306;&#19968;&#31181;&#22312;&#24050;&#30693;&#27491;&#28304;&#26102;&#36827;&#34892;&#20256;&#36755;&#65292;&#21478;&#19968;&#31181;&#21033;&#29992;&#32858;&#21512;&#25216;&#26415;&#23454;&#29616;&#26080;&#38656;&#20808;&#39564;&#20449;&#24687;&#30340;&#31283;&#20581;&#20256;&#36755;&#12290;&#25105;&#20204;&#20026;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#24314;&#31435;&#20102;&#19979;&#30028;&#65292;&#24182;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20139;&#26377;&#21305;&#37197;&#30340;&#28176;&#36817;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.04277v4 Announce Type: replace-cross  Abstract: We study the transfer learning (TL) for the functional linear regression (FLR) under the Reproducing Kernel Hilbert Space (RKHS) framework, observing the TL techniques in existing high-dimensional linear regression is not compatible with the truncation-based FLR methods as functional data are intrinsically infinite-dimensional and generated by smooth underlying processes. We measure the similarity across tasks using RKHS distance, allowing the type of information being transferred tied to the properties of the imposed RKHS. Building on the hypothesis offset transfer learning paradigm, two algorithms are proposed: one conducts the transfer when positive sources are known, while the other leverages aggregation techniques to achieve robust transfer without prior information about the sources. We establish lower bounds for this learning problem and show the proposed algorithms enjoy a matching asymptotic upper bound. These analyses
&lt;/p&gt;</description></item><item><title>SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00793</link><description>&lt;p&gt;
SecFormer&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00793
&lt;/p&gt;
&lt;p&gt;
SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#20113;&#24179;&#21488;&#19978;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#20379;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#38544;&#31169;&#38382;&#39064;&#26085;&#30410;&#21152;&#21095;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25237;&#36164;&#35745;&#21010;&#21644;&#38134;&#34892;&#36134;&#25143;&#31561;&#25935;&#24863;&#25968;&#25454;&#12290;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMPC&#65289;&#34987;&#35270;&#20026;&#20445;&#25252;&#25512;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#38544;&#31169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SMPC&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20943;&#36895;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;Transformer&#26550;&#26500;&#20013;&#30340;&#20247;&#22810;&#38750;&#32447;&#24615;&#25805;&#20316;&#19981;&#36866;&#21512;SMPC&#65292;&#24182;&#19988;&#38590;&#20197;&#26377;&#25928;&#35268;&#36991;&#25110;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;SecFormer&#65292;&#20197;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#26045;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#25105;&#20204;&#25104;&#21151;&#28040;&#38500;&#20102;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
&lt;/p&gt;</description></item><item><title>RSAM&#26159;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;Sharpness-Aware Minimization (SAM)&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#65292;&#24341;&#20837;&#20102;&#27969;&#24418;&#19978;sharpness&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17215</link><description>&lt;p&gt;
RSAM&#65306;&#20351;&#29992;Riemannian Sharpness-aware Minimization&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RSAM: Learning on manifolds with Riemannian Sharpness-aware Minimization. (arXiv:2309.17215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17215
&lt;/p&gt;
&lt;p&gt;
RSAM&#26159;&#19968;&#31181;&#22312;&#27969;&#24418;&#19978;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;Sharpness-Aware Minimization (SAM)&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#65292;&#24341;&#20837;&#20102;&#27969;&#24418;&#19978;sharpness&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;&#20854;&#19982;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#12290;&#36890;&#36807;&#35813;&#31639;&#27861;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#20102;&#35299;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#26377;&#26395;&#25552;&#21319;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#20043;&#21069;&#23558;&#20960;&#20309;&#21407;&#29702;&#24212;&#29992;&#20110;&#20248;&#21270;&#30340;&#30740;&#31350;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#21463;&#38480;&#21046;&#20248;&#21270;&#38382;&#39064;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20107;&#23454;&#19978;&#65292;&#26412;&#25991;&#26088;&#22312;&#23558;Sharpness-Aware Minimization (SAM)&#20248;&#21270;&#22120;&#25512;&#24191;&#21040;Riemannian&#27969;&#24418;&#12290;&#20026;&#20102;&#25903;&#25345;&#27969;&#24418;&#19978;&#30340;&#8220;sharpness&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#27969;&#24418;&#19978;&#30340;sharpness&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23450;&#20041;&#12290;&#20026;&#20102;&#35777;&#26126;&#36825;&#20010;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#20998;&#26512;&#26469;&#25551;&#36848;&#27969;&#24418;sharpness&#19982;&#27867;&#21270;&#33021;&#21147;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21576;&#29616;&#20102;&#19968;&#20010;&#26356;&#32039;&#23494;&#30340;&#27867;&#21270;&#32570;&#21475;&#19978;&#38480;&#65292;&#36825;&#26159;&#20043;&#21069;&#26410;&#30693;&#30340;&#32467;&#26524;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;Riemannian Sharpness-Aware Minimization (RSAM)&#12290;&#20026;&#20102;&#23637;&#31034;RSAM&#22312;&#25552;&#21319;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#19978;&#35780;&#20272;&#21644;&#23545;&#27604;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, understanding the geometry of the loss landscape shows promise in enhancing a model's generalization ability. In this work, we draw upon prior works that apply geometric principles to optimization and present a novel approach to improve robustness and generalization ability for constrained optimization problems. Indeed, this paper aims to generalize the Sharpness-Aware Minimization (SAM) optimizer to Riemannian manifolds. In doing so, we first extend the concept of sharpness and introduce a novel notion of sharpness on manifolds. To support this notion of sharpness, we present a theoretical analysis characterizing generalization capabilities with respect to manifold sharpness, which demonstrates a tighter bound on the generalization gap, a result not known before. Motivated by this analysis, we introduce our algorithm, Riemannian Sharpness-Aware Minimization (RSAM). To demonstrate RSAM's ability to enhance generalization ability, we evaluate and contrast our algorithm on a br
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20462;&#25913;&#22240;&#26524;&#26862;&#26519;&#26041;&#27861;&#65292;&#20197;&#30456;&#23545;&#39118;&#38505;&#20026;&#30446;&#26631;&#65292;&#20174;&#32780;&#25429;&#25417;&#21040;&#27835;&#30103;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#28508;&#22312;&#26469;&#28304;&#12290;</title><link>http://arxiv.org/abs/2309.15793</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#26862;&#26519;&#38024;&#23545;&#30456;&#23545;&#39118;&#38505;&#24322;&#36136;&#24615;&#36827;&#34892;&#30446;&#26631;&#21270;
&lt;/p&gt;
&lt;p&gt;
Targeting Relative Risk Heterogeneity with Causal Forests. (arXiv:2309.15793v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20462;&#25913;&#22240;&#26524;&#26862;&#26519;&#26041;&#27861;&#65292;&#20197;&#30456;&#23545;&#39118;&#38505;&#20026;&#30446;&#26631;&#65292;&#20174;&#32780;&#25429;&#25417;&#21040;&#27835;&#30103;&#25928;&#24212;&#24322;&#36136;&#24615;&#30340;&#28508;&#22312;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20020;&#24202;&#35797;&#39564;&#20998;&#26512;&#20013;&#65292;&#27835;&#30103;&#25928;&#24212;&#24322;&#36136;&#24615;&#65288;TEH&#65289;&#21363;&#31181;&#32676;&#20013;&#19981;&#21516;&#20122;&#32676;&#30340;&#27835;&#30103;&#25928;&#24212;&#30340;&#21464;&#24322;&#24615;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22240;&#26524;&#26862;&#26519;&#65288;Wager&#21644;Athey&#65292;2018&#65289;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#38750;&#24120;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#20294;&#20687;&#35768;&#22810;&#20854;&#20182;&#21457;&#29616;TEH&#30340;&#26041;&#27861;&#19968;&#26679;&#65292;&#23427;&#29992;&#20110;&#20998;&#31163;&#20122;&#32676;&#30340;&#26631;&#20934;&#20391;&#37325;&#20110;&#32477;&#23545;&#39118;&#38505;&#30340;&#24046;&#24322;&#12290;&#36825;&#21487;&#33021;&#20250;&#21066;&#24369;&#32479;&#35745;&#21151;&#25928;&#65292;&#25513;&#30422;&#20102;&#30456;&#23545;&#39118;&#38505;&#20013;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#32780;&#30456;&#23545;&#39118;&#38505;&#36890;&#24120;&#26159;&#20020;&#24202;&#20851;&#27880;&#30340;&#26356;&#21512;&#36866;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#23454;&#29616;&#20102;&#19968;&#31181;&#20462;&#25913;&#22240;&#26524;&#26862;&#26519;&#20197;&#38024;&#23545;&#30456;&#23545;&#39118;&#38505;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20110;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#65288;GLM&#65289;&#27604;&#36739;&#30340;&#26032;&#39062;&#33410;&#28857;&#20998;&#21106;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#34920;&#26126;&#30456;&#23545;&#39118;&#38505;&#30340;&#22240;&#26524;&#26862;&#26519;&#21487;&#20197;&#25429;&#25417;&#21040;&#20854;&#20182;&#26410;&#35266;&#23519;&#21040;&#30340;&#24322;&#36136;&#24615;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Treatment effect heterogeneity (TEH), or variability in treatment effect for different subgroups within a population, is of significant interest in clinical trial analysis. Causal forests (Wager and Athey, 2018) is a highly popular method for this problem, but like many other methods for detecting TEH, its criterion for separating subgroups focuses on differences in absolute risk. This can dilute statistical power by masking nuance in the relative risk, which is often a more appropriate quantity of clinical interest. In this work, we propose and implement a methodology for modifying causal forests to target relative risk using a novel node-splitting procedure based on generalized linear model (GLM) comparison. We present results on simulated and real-world data that suggest relative risk causal forests can capture otherwise unobserved sources of heterogeneity.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;X&#23556;&#32447;CT&#20013;&#20943;&#23569;&#25195;&#25551;&#35282;&#24230;&#30340;&#25968;&#37327;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#65292;&#20174;&#32780;&#36866;&#29992;&#20110;&#22312;&#32447;&#36136;&#37327;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2307.06343</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;X&#23556;&#32447;CT&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning. (arXiv:2307.06343v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#39034;&#24207;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;X&#23556;&#32447;CT&#20013;&#20943;&#23569;&#25195;&#25551;&#35282;&#24230;&#30340;&#25968;&#37327;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#65292;&#20174;&#32780;&#36866;&#29992;&#20110;&#22312;&#32447;&#36136;&#37327;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;X&#23556;&#32447;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#20013;&#65292;&#38656;&#20174;&#22810;&#20010;&#35282;&#24230;&#33719;&#21462;&#25237;&#24433;&#65292;&#24182;&#29992;&#20110;&#19977;&#32500;&#37325;&#24314;&#12290;&#20026;&#20102;&#20351;CT&#36866;&#29992;&#20110;&#22312;&#32447;&#36136;&#37327;&#25511;&#21046;&#65292;&#38656;&#35201;&#20943;&#23569;&#35282;&#24230;&#25968;&#30446;&#21516;&#26102;&#20445;&#25345;&#37325;&#24314;&#36136;&#37327;&#12290;&#31232;&#30095;&#35282;&#24230;&#26029;&#23618;&#25195;&#25551;&#26159;&#20174;&#26377;&#38480;&#25968;&#25454;&#33719;&#21462;&#19977;&#32500;&#37325;&#24314;&#30340;&#24120;&#29992;&#26041;&#27861;&#12290;&#20026;&#20102;&#20248;&#21270;&#20854;&#24615;&#33021;&#65292;&#21487;&#20197;&#25353;&#24207;&#36866;&#24212;&#25195;&#25551;&#35282;&#24230;&#65292;&#36873;&#25321;&#27599;&#20010;&#25195;&#25551;&#23545;&#35937;&#26368;&#26377;&#20449;&#24687;&#37327;&#30340;&#35282;&#24230;&#12290;&#25968;&#23398;&#19978;&#65292;&#36825;&#23545;&#24212;&#20110;&#35299;&#20915;&#19968;&#20010;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65288;OED&#65289;&#38382;&#39064;&#12290;OED&#38382;&#39064;&#26159;&#39640;&#32500;&#12289;&#38750;&#20984;&#12289;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#26080;&#27861;&#22312;&#32447;&#35299;&#20915;&#65292;&#21363;&#26080;&#27861;&#22312;&#25195;&#25551;&#36807;&#31243;&#20013;&#35299;&#20915;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23558;OED&#38382;&#39064;&#22312;&#36125;&#21494;&#26031;&#26694;&#26550;&#20013;&#24314;&#27169;&#20026;&#19968;&#20010;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#27714;&#35299;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22823;&#37327;&#31163;&#32447;&#35757;&#32451;&#23398;&#20064;&#39640;&#25928;&#30340;&#38750;&#36138;&#23146;&#31574;&#30053;&#26469;&#35299;&#20915;&#32473;&#23450;&#31867;&#21035;&#30340;OED&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In X-ray Computed Tomography (CT), projections from many angles are acquired and used for 3D reconstruction. To make CT suitable for in-line quality control, reducing the number of angles while maintaining reconstruction quality is necessary. Sparse-angle tomography is a popular approach for obtaining 3D reconstructions from limited data. To optimize its performance, one can adapt scan angles sequentially to select the most informative angles for each scanned object. Mathematically, this corresponds to solving and optimal experimental design (OED) problem. OED problems are high-dimensional, non-convex, bi-level optimization problems that cannot be solved online, i.e., during the scan. To address these challenges, we pose the OED problem as a partially observable Markov decision process in a Bayesian framework, and solve it through deep reinforcement learning. The approach learns efficient non-greedy policies to solve a given class of OED problems through extensive offline training rath
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2304.03907</link><description>&lt;p&gt;
&#22522;&#20110;&#26377;&#38480;&#32500;&#35889;&#21160;&#24577;&#23884;&#20837;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Stochastic Nonlinear Control via Finite-dimensional Spectral Dynamic Embedding. (arXiv:2304.03907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#31639;&#27861;&#65288;SDEC&#65289;&#29992;&#20110;&#35299;&#20915;&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26368;&#20248;&#25511;&#21046;&#19968;&#30452;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#12290;Ren&#31561;&#20154;&#24341;&#20837;&#20102;&#35889;&#21160;&#24577;&#23884;&#20837;&#26469;&#24320;&#21457;&#25511;&#21046;&#26410;&#30693;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#26080;&#31351;&#32500;&#29305;&#24449;&#26469;&#32447;&#24615;&#34920;&#31034;&#29366;&#24577;&#20540;&#20989;&#25968;&#65292;&#24182;&#21033;&#29992;&#26377;&#38480;&#32500;&#30340;&#25130;&#26029;&#36924;&#36817;&#36827;&#34892;&#23454;&#38469;&#23454;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#24050;&#30693;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25511;&#21046;&#20013;&#30340;&#26377;&#38480;&#32500;&#36924;&#36817;&#24615;&#36136;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#34892;&#30340;&#38543;&#26426;&#38750;&#32447;&#24615;&#25511;&#21046;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#26377;&#38480;&#32500;&#29305;&#24449;&#36924;&#36817;&#30340;&#38750;&#32447;&#24615;&#21160;&#24577;&#35889;&#23884;&#20837;&#25511;&#21046;&#65288;SDEC&#65289;&#65292;&#24182;&#36827;&#34892;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#34920;&#24449;&#30001;&#26377;&#38480;&#32500;&#25130;&#26029;&#24341;&#36215;&#30340;&#36924;&#36817;&#35823;&#24046;&#21644;&#30001;&#26377;&#38480;&#26679;&#26412;&#36924;&#36817;&#24341;&#36215;&#30340;&#32479;&#35745;&#35823;&#24046;&#65292;&#21516;&#26102;&#36827;&#34892;&#25919;&#31574;&#35780;&#20272;&#21644;&#25919;&#31574;&#20248;&#21270;&#30340;&#23454;&#39564;&#27979;&#35797;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimal control is notoriously difficult for stochastic nonlinear systems. Ren et al. introduced Spectral Dynamics Embedding for developing reinforcement learning methods for controlling an unknown system. It uses an infinite-dimensional feature to linearly represent the state-value function and exploits finite-dimensional truncation approximation for practical implementation. However, the finite-dimensional approximation properties in control have not been investigated even when the model is known. In this paper, we provide a tractable stochastic nonlinear control algorithm that exploits the nonlinear dynamics upon the finite-dimensional feature approximation, Spectral Dynamics Embedding Control (SDEC), with an in-depth theoretical analysis to characterize the approximation error induced by the finite-dimension truncation and statistical error induced by finite-sample approximation in both policy evaluation and policy optimization. We also empirically test the algorithm and compare th
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#24182;&#20272;&#35745;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2302.08854</link><description>&lt;p&gt;
&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Post-Episodic Reinforcement Learning Inference. (arXiv:2302.08854v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08854
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21518;&#26399;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#25512;&#26029;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#24182;&#20272;&#35745;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#24773;&#33410;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#25910;&#38598;&#30340;&#25968;&#25454;&#36827;&#34892;&#20272;&#35745;&#21644;&#25512;&#26029;&#65307;&#21363;&#22312;&#27599;&#20010;&#26102;&#26399;&#65288;&#20063;&#31216;&#20026;&#24773;&#33410;&#65289;&#20197;&#39034;&#24207;&#26041;&#24335;&#19982;&#21333;&#20010;&#21463;&#35797;&#21333;&#20803;&#22810;&#27425;&#20132;&#20114;&#30340;&#33258;&#36866;&#24212;&#35797;&#39564;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#25910;&#38598;&#25968;&#25454;&#21518;&#33021;&#22815;&#35780;&#20272;&#21453;&#20107;&#23454;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#65292;&#24182;&#20272;&#35745;&#32467;&#26500;&#21442;&#25968;&#65292;&#22914;&#21160;&#24577;&#22788;&#29702;&#25928;&#24212;&#65292;&#36825;&#21487;&#20197;&#29992;&#20110;&#20449;&#29992;&#20998;&#37197;&#65288;&#20363;&#22914;&#65292;&#31532;&#19968;&#20010;&#26102;&#26399;&#30340;&#34892;&#21160;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#65289;&#12290;&#36825;&#20123;&#24863;&#20852;&#36259;&#30340;&#21442;&#25968;&#21487;&#20197;&#26500;&#25104;&#30697;&#26041;&#31243;&#30340;&#35299;&#65292;&#20294;&#19981;&#26159;&#24635;&#20307;&#25439;&#22833;&#20989;&#25968;&#30340;&#26368;&#23567;&#21270;&#22120;&#65292;&#22312;&#38745;&#24577;&#25968;&#25454;&#24773;&#20917;&#19979;&#23548;&#33268;&#20102;$Z$-&#20272;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#20272;&#35745;&#37327;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#25910;&#38598;&#30340;&#24773;&#20917;&#19979;&#19981;&#33021;&#28176;&#36817;&#27491;&#24577;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#21152;&#26435;&#30340;$Z$-&#20272;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#31934;&#24515;&#35774;&#35745;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#26469;&#31283;&#23450;&#24773;&#33410;&#21464;&#21270;&#30340;&#20272;&#35745;&#26041;&#24046;&#65292;&#36825;&#26159;&#30001;&#38750;...
&lt;/p&gt;
&lt;p&gt;
We consider estimation and inference with data collected from episodic reinforcement learning (RL) algorithms; i.e. adaptive experimentation algorithms that at each period (aka episode) interact multiple times in a sequential manner with a single treated unit. Our goal is to be able to evaluate counterfactual adaptive policies after data collection and to estimate structural parameters such as dynamic treatment effects, which can be used for credit assignment (e.g. what was the effect of the first period action on the final outcome). Such parameters of interest can be framed as solutions to moment equations, but not minimizers of a population loss function, leading to $Z$-estimation approaches in the case of static data. However, such estimators fail to be asymptotically normal in the case of adaptive data collection. We propose a re-weighted $Z$-estimation approach with carefully designed adaptive weights to stabilize the episode-varying estimation variance, which results from the non
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20855;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#25191;&#34892;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2210.16402</link><description>&lt;p&gt;
GradSkip&#65306;&#20855;&#26377;&#26356;&#22909;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#36890;&#20449;&#21152;&#36895;&#23616;&#37096;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GradSkip: Communication-Accelerated Local Gradient Methods with Better Computational Complexity. (arXiv:2210.16402v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#20855;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#25191;&#34892;&#36739;&#23569;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#65292;&#36825;&#19968;&#26041;&#27861;&#21487;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#20998;&#24067;&#24335;&#20248;&#21270;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20801;&#35768;&#23458;&#25143;&#31471;&#22312;&#36890;&#20449;&#20043;&#21069;&#25191;&#34892;&#22810;&#20010;&#26412;&#22320;&#26799;&#24230;&#31867;&#22411;&#30340;&#35757;&#32451;&#27493;&#39588;&#26469;&#20943;&#36731;&#39640;&#36890;&#20449;&#25104;&#26412;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#24050;&#32463;&#30740;&#31350;&#20102;&#32422;&#21313;&#24180;&#65292;&#20294;&#26412;&#22320;&#35757;&#32451;&#30340;&#21152;&#36895;&#24615;&#36136;&#22312;&#29702;&#35770;&#19978;&#36824;&#26410;&#24471;&#21040;&#23436;&#20840;&#35299;&#37322;&#12290;&#26368;&#36817;&#65292;Mishchenko&#31561;&#20154;(2022 International Conference on Machine Learning)&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#35777;&#26126;&#20102;&#24403;&#26412;&#22320;&#35757;&#32451;&#24471;&#21040;&#27491;&#30830;&#25191;&#34892;&#26102;&#65292;&#20250;&#23548;&#33268;&#21487;&#35777;&#26126;&#30340;&#36890;&#20449;&#21152;&#36895;&#65292;&#22312;&#24378;&#20984;&#21306;&#22495;&#20869;&#36825;&#19968;&#28857;&#25104;&#31435;&#65292;&#32780;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#25968;&#25454;&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#26041;&#27861;ProxSkip&#35201;&#27714;&#25152;&#26377;&#23458;&#25143;&#31471;&#22312;&#27599;&#27425;&#36890;&#20449;&#36718;&#20013;&#25191;&#34892;&#30456;&#21516;&#25968;&#37327;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#12290;&#28789;&#24863;&#26469;&#33258;&#24120;&#35782;&#30340;&#30452;&#35273;&#65292;&#25105;&#20204;&#36890;&#36807;&#29468;&#27979;&#35748;&#20026;&#25317;&#26377;&#8220;&#27425;&#35201;&#8221;&#25968;&#25454;&#30340;&#23458;&#25143;&#31471;&#24212;&#35813;&#33021;&#22815;&#29992;&#36739;&#23569;&#30340;&#26412;&#22320;&#35757;&#32451;&#27493;&#39588;&#23601;&#33021;&#23436;&#25104;&#65292;&#32780;&#19981;&#24433;&#21709;&#25972;&#20307;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
We study a class of distributed optimization algorithms that aim to alleviate high communication costs by allowing the clients to perform multiple local gradient-type training steps prior to communication. While methods of this type have been studied for about a decade, the empirically observed acceleration properties of local training eluded all attempts at theoretical understanding. In a recent breakthrough, Mishchenko et al. (ICML 2022) proved that local training, when properly executed, leads to provable communication acceleration, and this holds in the strongly convex regime without relying on any data similarity assumptions. However, their method ProxSkip requires all clients to take the same number of local training steps in each communication round. Inspired by a common sense intuition, we start our investigation by conjecturing that clients with ``less important'' data should be able to get away with fewer local training steps without this impacting the overall communication c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#24207;&#21015;&#23454;&#39564;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;&#23545;&#21453;&#20107;&#23454;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#24314;&#31435;&#20102;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2202.06891</link><description>&lt;p&gt;
&#24207;&#21015;&#23454;&#39564;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Counterfactual inference for sequential experiments. (arXiv:2202.06891v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24207;&#21015;&#23454;&#39564;&#30340;&#21453;&#20107;&#23454;&#25512;&#26029;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#65292;&#20351;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;&#23545;&#21453;&#20107;&#23454;&#22343;&#20540;&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#24314;&#31435;&#20102;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#38024;&#23545;&#36830;&#32493;&#35774;&#35745;&#23454;&#39564;&#36827;&#34892;&#30340;&#20107;&#21518;&#32479;&#35745;&#25512;&#26029;&#65292;&#22312;&#27492;&#23454;&#39564;&#20013;&#65292;&#22810;&#20010;&#21333;&#20301;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#20998;&#37197;&#27835;&#30103;&#65292;&#24182;&#20351;&#29992;&#38543;&#26102;&#38388;&#32780;&#36866;&#24212;&#30340;&#27835;&#30103;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#22312;&#23545;&#36866;&#24212;&#24615;&#27835;&#30103;&#31574;&#30053;&#20570;&#20986;&#26368;&#23569;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#26368;&#23567;&#21487;&#33021;&#35268;&#27169;&#30340;&#21453;&#20107;&#23454;&#22343;&#20540;&#25552;&#20379;&#25512;&#26029;&#20445;&#35777;&#65292;&#21363;&#22312;&#27599;&#20010;&#21333;&#20301;&#21644;&#27599;&#20010;&#26102;&#38388;&#19979;&#65292;&#38024;&#23545;&#19981;&#21516;&#27835;&#30103;&#30340;&#24179;&#22343;&#32467;&#26524;&#12290;&#22312;&#27809;&#26377;&#23545;&#21453;&#20107;&#23454;&#22343;&#20540;&#36827;&#34892;&#20219;&#20309;&#32467;&#26500;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#26410;&#30693;&#21464;&#37327;&#27604;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#28857;&#36824;&#22810;&#12290;&#20026;&#20102;&#21462;&#24471;&#36827;&#23637;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#29992;&#20110;&#21453;&#20107;&#23454;&#22343;&#20540;&#19978;&#65292;&#35813;&#27169;&#22411;&#20316;&#20026;&#38750;&#21442;&#25968;&#24418;&#24335;&#30340;&#38750;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#21644;&#20197;&#21069;&#24037;&#20316;&#20013;&#32771;&#34385;&#30340;&#21452;&#32447;&#24615;&#28508;&#22312;&#22240;&#23376;&#27169;&#22411;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#20351;&#29992;&#38750;&#21442;&#25968;&#26041;&#27861;&#36827;&#34892;&#20272;&#35745;&#65292;&#21363;&#26368;&#36817;&#37051;&#30340;&#21464;&#20307;&#65292;&#24182;&#20026;&#27599;&#20010;&#21333;&#20301;&#21644;&#27599;&#20010;&#26102;&#38388;&#30340;&#21453;&#20107;&#23454;&#22343;&#20540;&#24314;&#31435;&#20102;&#38750;&#28176;&#36827;&#39640;&#27010;&#29575;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider after-study statistical inference for sequentially designed experiments wherein multiple units are assigned treatments for multiple time points using treatment policies that adapt over time. Our goal is to provide inference guarantees for the counterfactual mean at the smallest possible scale -- mean outcome under different treatments for each unit and each time -- with minimal assumptions on the adaptive treatment policy. Without any structural assumptions on the counterfactual means, this challenging task is infeasible due to more unknowns than observed data points. To make progress, we introduce a latent factor model over the counterfactual means that serves as a non-parametric generalization of the non-linear mixed effects model and the bilinear latent factor model considered in prior works. For estimation, we use a non-parametric method, namely a variant of nearest neighbors, and establish a non-asymptotic high probability error bound for the counterfactual mean for ea
&lt;/p&gt;</description></item></channel></rss>