<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.06925</link><description>&lt;p&gt;
&#29992;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling Latent Selection with Structural Causal Models. (arXiv:2401.06925v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#24110;&#21161;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#65292;&#21253;&#25324;&#22788;&#29702;&#36873;&#25321;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#20559;&#20506;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#65292;&#22914;&#26524;&#19981;&#27491;&#30830;&#22788;&#29702;&#21487;&#33021;&#23548;&#33268;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23545;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCMs&#65289;&#36827;&#34892;&#26465;&#20214;&#25805;&#20316;&#30340;&#26041;&#27861;&#65292;&#20197;&#20174;&#22240;&#26524;&#30340;&#35282;&#24230;&#23545;&#28508;&#22312;&#36873;&#25321;&#36827;&#34892;&#24314;&#27169;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26465;&#20214;&#25805;&#20316;&#23558;&#20855;&#26377;&#26126;&#30830;&#28508;&#22312;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#36716;&#25442;&#20026;&#27809;&#26377;&#27492;&#31867;&#36873;&#25321;&#26426;&#21046;&#30340;SCM&#65292;&#36825;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32534;&#30721;&#20102;&#26681;&#25454;&#21407;&#22987;SCM&#36873;&#25321;&#30340;&#20122;&#24635;&#20307;&#30340;&#22240;&#26524;&#35821;&#20041;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#35813;&#26465;&#20214;&#25805;&#20316;&#20445;&#25345;SCMs&#30340;&#31616;&#27905;&#24615;&#65292;&#26080;&#29615;&#24615;&#21644;&#32447;&#24615;&#24615;&#65292;&#24182;&#19982;&#36793;&#38469;&#21270;&#25805;&#20316;&#30456;&#31526;&#21512;&#12290;&#30001;&#20110;&#36825;&#20123;&#29305;&#24615;&#19982;&#36793;&#38469;&#21270;&#21644;&#24178;&#39044;&#32467;&#21512;&#36215;&#26469;&#65292;&#26465;&#20214;&#25805;&#20316;&#20026;&#22312;&#28508;&#22312;&#32454;&#33410;&#24050;&#32463;&#21435;&#38500;&#30340;&#22240;&#26524;&#27169;&#22411;&#20013;&#36827;&#34892;&#22240;&#26524;&#25512;&#29702;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#36890;&#36807;&#20363;&#23376;&#28436;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#25512;&#26029;&#30340;&#32463;&#20856;&#32467;&#26524;&#25512;&#24191;&#20197;&#21253;&#25324;&#36873;&#25321;&#20559;&#20506;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection b
&lt;/p&gt;</description></item><item><title>FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2306.06031</link><description>&lt;p&gt;
FinGPT&#65306;&#24320;&#28304;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FinGPT: Open-Source Financial Large Language Models. (arXiv:2306.06031v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06031
&lt;/p&gt;
&lt;p&gt;
FinGPT&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#37329;&#34701;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#37329;&#34701;LLMs&#65292;&#20854;&#37325;&#35201;&#24615;&#22312;&#20110;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#20010;&#39046;&#22495;&#38761;&#26032;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#28508;&#21147;&#65292;&#24341;&#36215;&#20102;&#37329;&#34701;&#39046;&#22495;&#30340;&#27987;&#21402;&#20852;&#36259;&#12290;&#33719;&#24471;&#39640;&#36136;&#37327;&#30340;&#37329;&#34701;&#25968;&#25454;&#26159;&#37329;&#34701;LLMs&#65288;FinLLMs&#65289;&#30340;&#31532;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#37329;&#34701;&#39046;&#22495;&#30340;&#24320;&#28304;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;FinGPT&#12290;&#19982;&#19987;&#26377;&#27169;&#22411;&#19981;&#21516;&#65292;FinGPT&#37319;&#29992;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#21487;&#35775;&#38382;&#21644;&#36879;&#26126;&#30340;&#36164;&#28304;&#26469;&#24320;&#21457;&#20182;&#20204;&#30340;&#37329;&#34701;LLMs&#12290;&#25105;&#20204;&#24378;&#35843;&#33258;&#21160;&#25968;&#25454;&#31579;&#36873;&#31649;&#36947;&#21644;&#36731;&#37327;&#32423;&#20302;&#31209;&#36866;&#24212;&#25216;&#26415;&#22312;&#24314;&#31435;FinGPT&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20960;&#20010;&#28508;&#22312;&#30340;&#24212;&#29992;&#20316;&#20026;&#29992;&#25143;&#30340;&#22522;&#30784;&#65292;&#22914;&#26426;&#22120;&#39038;&#38382;&#12289;&#31639;&#27861;&#20132;&#26131;&#21644;&#35770; &#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have shown the potential of revolutionizing natural language processing tasks in diverse domains, sparking great interest in finance. Accessing high-quality financial data is the first challenge for financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken advantage of their unique data accumulation, such privileged access calls for an open-source alternative to democratize Internet-scale financial data.  In this paper, we present an open-source large language model, FinGPT, for the finance sector. Unlike proprietary models, FinGPT takes a data-centric approach, providing researchers and practitioners with accessible and transparent resources to develop their FinLLMs. We highlight the importance of an automatic data curation pipeline and the lightweight low-rank adaptation technique in building FinGPT. Furthermore, we showcase several potential applications as stepping stones for users, such as robo-advising, algorithmic trading, and l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;</title><link>http://arxiv.org/abs/2306.03303</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#21151;&#33021;&#24615;&#36755;&#20837;&#26144;&#23556;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#23450;&#20041;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#65292;&#20854;&#20540;&#20063;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#30340;&#36755;&#20986;&#31354;&#38388;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21152;&#24615;&#26063;&#20316;&#20026;&#38544;&#34255;&#23618;&#26144;&#23556;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#24212;&#29992;&#20110;&#27599;&#20010;&#38544;&#34255;&#23618;&#12290;&#20381;&#38752;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#30340;Stone-Weierstrass&#23450;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#65292;&#36229;&#36234;&#20102;&#24120;&#35268;&#32039;&#38598;&#36924;&#36817;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#36890;&#36807;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65288;&#38750;&#20808;&#35265;&#20043;&#26126;&#30340;&#65289;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#12290;&#20316;&#20026;&#24102;&#26435;Stone-Weierstrass&#23450;&#29702;&#30340;&#36827;&#19968;&#27493;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#24341;&#20837;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#31614;&#21517;&#20869;&#26680;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26159;&#26576;&#20123;&#39640;&#26031;&#36807;&#31243;&#30340;Cameron-Martin&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#24230;&#37327;&#26041;&#27861;&#30340;&#32508;&#36848;&#21644;&#25351;&#21335;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#22312;&#23545;&#24037;&#31243;&#24212;&#29992;&#30340;&#35201;&#27714;&#19978;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#65292;&#22240;&#27492;&#26412;&#25991;&#32534;&#36753;&#20102;&#19968;&#32452;&#20840;&#38754;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#30340;&#32570;&#28857;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#24037;&#31243;&#35774;&#35745;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22914;&#20309;&#24212;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22312;&#25429;&#25417;&#35774;&#35745;&#30340;&#37325;&#35201;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2302.02913</link><description>&lt;p&gt;
&#36229;&#36234;&#32479;&#35745;&#30456;&#20284;&#24615;&#65306;&#37325;&#26032;&#24605;&#32771;&#26426;&#22120;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Statistical Similarity: Rethinking Metrics for Deep Generative Models in Engineering Design. (arXiv:2302.02913v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31687;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#24230;&#37327;&#26041;&#27861;&#30340;&#32508;&#36848;&#21644;&#25351;&#21335;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#22312;&#23545;&#24037;&#31243;&#24212;&#29992;&#30340;&#35201;&#27714;&#19978;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#65292;&#22240;&#27492;&#26412;&#25991;&#32534;&#36753;&#20102;&#19968;&#32452;&#20840;&#38754;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#30340;&#32570;&#28857;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#24037;&#31243;&#35774;&#35745;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22914;&#20309;&#24212;&#29992;&#20110;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#22312;&#25429;&#25417;&#35774;&#35745;&#30340;&#37325;&#35201;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#65292;&#25193;&#25955;&#27169;&#22411;&#21644;Transformer&#31561;&#65292;&#22312;&#22270;&#20687;&#21644;&#35821;&#38899;&#21512;&#25104;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#33647;&#29289;&#24320;&#21457;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#24037;&#31243;&#35774;&#35745;&#38382;&#39064;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#26102;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#24456;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#20256;&#32479;&#30340;&#22522;&#20110;&#20284;&#28982;&#24615;&#30340;&#32479;&#35745;&#24230;&#37327;&#26041;&#27861;&#21487;&#33021;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#24037;&#31243;&#24212;&#29992;&#30340;&#35201;&#27714;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31687;&#28145;&#24230;&#23398;&#20064;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#30340;&#24230;&#37327;&#25351;&#21335;&#21644;&#32508;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#8220;&#32463;&#20856;&#8221;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#36825;&#20123;&#26631;&#20934;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#29702;&#35770;&#21644;&#20856;&#22411;&#30340;&#35745;&#31639;&#26426;&#24212;&#29992;&#65292;&#28982;&#21518;&#20351;&#29992;&#26696;&#20363;&#30740;&#31350;&#65292;&#24378;&#35843;&#20102;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#20026;&#20309;&#24456;&#23569;&#33021;&#22815;&#36716;&#21270;&#20026;&#35774;&#35745;&#38382;&#39064;&#20294;&#21448;&#22240;&#32570;&#20047;&#30830;&#31435;&#30340;&#26367;&#20195;&#36873;&#25321;&#32780;&#32463;&#24120;&#20351;&#29992;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32534;&#36753;&#20102;&#19968;&#32452;&#20840;&#38754;&#30340;&#26032;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#35299;&#20915;&#20256;&#32479;&#24230;&#37327;&#26631;&#20934;&#30340;&#32570;&#28857;&#65292;&#24182;&#26356;&#22909;&#22320;&#19982;&#24037;&#31243;&#35774;&#35745;&#30340;&#38656;&#27714;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#24230;&#37327;&#26631;&#20934;&#26469;&#35780;&#20272;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#22312;&#24037;&#31243;&#35774;&#35745;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#20986;&#30340;&#24230;&#37327;&#26041;&#27861;&#22312;&#25429;&#25417;&#35774;&#35745;&#30340;&#37325;&#35201;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#65292;&#22240;&#27492;&#22312;&#24037;&#31243;&#35774;&#35745;&#24773;&#22659;&#20013;&#20026;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#26356;&#20934;&#30830;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models, such as Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), Diffusion Models, and Transformers, have shown great promise in a variety of applications, including image and speech synthesis, natural language processing, and drug discovery. However, when applied to engineering design problems, evaluating the performance of these models can be challenging, as traditional statistical metrics based on likelihood may not fully capture the requirements of engineering applications. This paper doubles as a review and a practical guide to evaluation metrics for deep generative models (DGMs) in engineering design. We first summarize well-accepted `classic' evaluation metrics for deep generative models grounded in machine learning theory and typical computer science applications. Using case studies, we then highlight why these metrics seldom translate well to design problems but see frequent use due to the lack of established alternatives. Next, we curat
&lt;/p&gt;</description></item></channel></rss>