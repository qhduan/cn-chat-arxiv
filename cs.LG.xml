<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#38598;&#21512;&#30340;&#24555;&#36895;&#20056;&#27861;&#26041;&#27861;&#65292;&#22312;&#29305;&#23450;&#20301;&#25968;&#33539;&#22260;&#20869;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;&#35813;&#26041;&#27861;&#25226;&#25968;&#23383;&#34920;&#31034;&#20026;&#25972;&#25968;&#32034;&#24341;&#21015;&#34920;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2311.09922</link><description>&lt;p&gt;
&#36890;&#36807;&#37319;&#29992;&#25972;&#25968;&#21015;&#34920;&#20316;&#20026;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#30340;&#38598;&#21512;&#26469;&#23454;&#29616;&#24555;&#36895;&#20056;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fast multiplication by two's complement addition of numbers represented as a set of polynomial radix 2 indexes, stored as an integer list for massively parallel computation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#38598;&#21512;&#30340;&#24555;&#36895;&#20056;&#27861;&#26041;&#27861;&#65292;&#22312;&#29305;&#23450;&#20301;&#25968;&#33539;&#22260;&#20869;&#27604;&#20256;&#32479;&#26041;&#27861;&#26356;&#24555;&#12290;&#35813;&#26041;&#27861;&#25226;&#25968;&#23383;&#34920;&#31034;&#20026;&#25972;&#25968;&#32034;&#24341;&#21015;&#34920;&#65292;&#24182;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#28436;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#29992;&#25972;&#25968;&#21015;&#34920;&#34920;&#31034;&#30340;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25351;&#25968;&#38598;&#21512;&#30340;&#20056;&#27861;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#37319;&#29992;python&#20195;&#30721;&#23454;&#29616;&#20102;&#19968;&#32452;&#31639;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#26576;&#19968;&#20301;&#25968;&#33539;&#22260;&#20869;&#27604;&#25968;&#35770;&#21464;&#25442;(NTT)&#21644;&#21345;&#25289;&#33576;&#24052;(Karatsuba)&#20056;&#27861;&#26356;&#24555;&#12290;&#25105;&#20204;&#36824;&#23454;&#29616;&#20102;&#29992;python&#20195;&#30721;&#36827;&#34892;&#27604;&#36739;&#65292;&#19982;&#22810;&#39033;&#24335;&#22522;&#25968;2&#25972;&#25968;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#25972;&#25968;&#25110;&#23454;&#25968;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#25972;&#25968;&#32034;&#24341;&#21015;&#34920;&#65292;&#34920;&#31034;&#20108;&#36827;&#21046;&#20013;&#30340;&#26377;&#38480;&#32423;&#25968;&#12290;&#35813;&#25968;&#23383;&#30340;&#25972;&#25968;&#32034;&#24341;&#26377;&#38480;&#32423;&#25968;&#21487;&#20197;&#23384;&#20648;&#21644;&#20998;&#24067;&#22312;&#22810;&#20010;CPU / GPU&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21152;&#27861;&#21644;&#20056;&#27861;&#36816;&#31639;&#21487;&#20197;&#24212;&#29992;&#20110;&#20316;&#20026;&#32034;&#24341;&#25972;&#25968;&#34920;&#31034;&#30340;&#20004;&#20010;&#34917;&#30721;&#21152;&#27861;&#65292;&#24182;&#21487;&#20197;&#23436;&#20840;&#20998;&#24067;&#22312;&#32473;&#23450;&#30340;CPU / GPU&#26550;&#26500;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23436;&#20840;&#30340;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We demonstrate a multiplication method based on numbers represented as set of polynomial radix 2 indices stored as an integer list. The 'polynomial integer index multiplication' method is a set of algorithms implemented in python code. We demonstrate the method to be faster than both the Number Theoretic Transform (NTT) and Karatsuba for multiplication within a certain bit range. Also implemented in python code for comparison purposes with the polynomial radix 2 integer method. We demonstrate that it is possible to express any integer or real number as a list of integer indices, representing a finite series in base two. The finite series of integer index representation of a number can then be stored and distributed across multiple CPUs / GPUs. We show that operations of addition and multiplication can be applied as two's complement additions operating on the index integer representations and can be fully distributed across a given CPU / GPU architecture. We demonstrate fully distribute
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20984;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#20849;&#36717;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#37327;&#24230;&#37327;&#21644;&#27493;&#38271;&#35268;&#21017;&#20248;&#21270;&#20102;&#36817;&#31471;&#29275;&#39039;&#31639;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;Prox-N-SCORE&#21644;Prox-GGN-SCORE&#31639;&#27861;&#65292;&#21518;&#32773;&#36890;&#36807;&#37325;&#35201;&#36817;&#20284;&#31243;&#24207;&#26174;&#33879;&#20943;&#23569;&#20102;&#36870;Hessian&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2309.01781</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#20984;&#32452;&#21512;&#20248;&#21270;&#30340;&#33258;&#20849;&#36717;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Self-concordant Smoothing for Large-Scale Convex Composite Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01781
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#20984;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#20849;&#36717;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#37327;&#24230;&#37327;&#21644;&#27493;&#38271;&#35268;&#21017;&#20248;&#21270;&#20102;&#36817;&#31471;&#29275;&#39039;&#31639;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#20102;&#38750;&#20809;&#28369;&#20989;&#25968;&#30340;&#32467;&#26500;&#65292;&#25552;&#20986;&#20102;Prox-N-SCORE&#21644;Prox-GGN-SCORE&#31639;&#27861;&#65292;&#21518;&#32773;&#36890;&#36807;&#37325;&#35201;&#36817;&#20284;&#31243;&#24207;&#26174;&#33879;&#20943;&#23569;&#20102;&#36870;Hessian&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#20849;&#36717;&#24179;&#28369;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20004;&#20010;&#20984;&#20989;&#25968;&#30340;&#21644;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#20809;&#28369;&#30340;&#65292;&#21478;&#19968;&#20010;&#21487;&#33021;&#26159;&#38750;&#20809;&#28369;&#30340;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#20142;&#28857;&#22312;&#20110;&#25152;&#24471;&#38382;&#39064;&#32467;&#26500;&#30340;&#33258;&#28982;&#29305;&#24615;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21464;&#37327;&#24230;&#37327;&#36873;&#25321;&#26041;&#27861;&#21644;&#19968;&#20010;&#29305;&#21035;&#36866;&#29992;&#20110;&#36817;&#31471;&#29275;&#39039;&#31867;&#22411;&#31639;&#27861;&#30340;&#27493;&#38271;&#36873;&#25321;&#35268;&#21017;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#39640;&#25928;&#22788;&#29702;&#20102;&#38750;&#20809;&#28369;&#20989;&#25968;&#25512;&#21160;&#30340;&#20855;&#20307;&#32467;&#26500;&#65292;&#22914;$\ell_1$&#27491;&#21017;&#21270;&#21644;&#20998;&#32452;Lasso&#24809;&#32602;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65306;Prox-N-SCORE&#65292;&#19968;&#31181;&#36817;&#31471;&#29275;&#39039;&#31639;&#27861;&#65292;&#21644;Prox-GGN-SCORE&#65292;&#19968;&#31181;&#36817;&#31471;&#24191;&#20041;&#39640;&#26031;-&#29275;&#39039;&#31639;&#27861;&#12290;Prox-GGN-SCORE&#31639;&#27861;&#31361;&#20986;&#20102;&#19968;&#31181;&#37325;&#35201;&#30340;&#36817;&#20284;&#31243;&#24207;&#65292;&#26377;&#21161;&#20110;&#26174;&#33879;&#20943;&#23569;&#36870;Hessian&#30456;&#20851;&#30340;&#22823;&#37096;&#20998;&#35745;&#31639;&#24320;&#38144;&#12290;&#36825;&#31181;&#36817;&#20284;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01781v2 Announce Type: replace-cross  Abstract: We introduce a notion of self-concordant smoothing for minimizing the sum of two convex functions, one of which is smooth and the other may be nonsmooth. The key highlight of our approach is in a natural property of the resulting problem's structure which provides us with a variable-metric selection method and a step-length selection rule particularly suitable for proximal Newton-type algorithms. In addition, we efficiently handle specific structures promoted by the nonsmooth function, such as $\ell_1$-regularization and group-lasso penalties. We prove the convergence of two resulting algorithms: Prox-N-SCORE, a proximal Newton algorithm and Prox-GGN-SCORE, a proximal generalized Gauss-Newton algorithm. The Prox-GGN-SCORE algorithm highlights an important approximation procedure which helps to significantly reduce most of the computational overhead associated with the inverse Hessian. This approximation is essentially useful fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.09213</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#21512;&#25104;&#26410;&#35265;&#36807;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Unseen Image Synthesis with Diffusion Models. (arXiv:2310.09213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#21512;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#65292;&#24182;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29983;&#25104;&#39046;&#22495;&#30340;&#36235;&#21183;&#26159;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#35268;&#27169;&#21644;&#22686;&#21152;&#35757;&#32451;&#25968;&#25454;&#26469;&#23454;&#29616;&#36890;&#29992;&#39046;&#22495;&#34920;&#31034;&#65292;&#32780;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#36873;&#25321;&#30456;&#21453;&#30340;&#26041;&#21521;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#21644;&#20923;&#32467;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPMs&#65289;&#22312;&#21333;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28508;&#22312;&#37319;&#26679;&#21644;&#20960;&#20309;&#20248;&#21270;&#26469;&#21512;&#25104;&#26410;&#35265;&#36807;&#30340;&#39046;&#22495;&#22270;&#20687;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#21363;&#20351;&#26159;&#20165;&#22312;&#21333;&#39046;&#22495;&#22270;&#20687;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;DDPMs&#24050;&#32463;&#20855;&#22791;&#20102;&#36275;&#22815;&#30340;&#34920;&#31034;&#33021;&#21147;&#65292;&#21487;&#20197;&#36890;&#36807;&#21453;&#36716;&#28508;&#22312;&#32534;&#30721;&#65292;&#24182;&#32463;&#36807;&#21452;&#21521;&#30830;&#23450;&#24615;&#25193;&#25955;&#21644;&#21435;&#22122;&#36712;&#36857;&#37325;&#26500;&#20219;&#24847;&#22270;&#20687;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#30740;&#31350;&#26410;&#35265;&#36807;&#22270;&#20687;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#27839;&#21435;&#22122;&#38142;&#30340;OOD&#26679;&#26412;&#30340;&#32479;&#35745;&#21644;&#20960;&#20309;&#34892;&#20026;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#37117;&#34920;&#26126;&#65292;&#21453;&#36716;&#30340;OOD&#26679;&#26412;&#20063;&#24314;&#31435;&#20102;&#39640;&#26031;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
While the current trend in the generative field is scaling up towards larger models and more training data for generalized domain representations, we go the opposite direction in this work by synthesizing unseen domain images without additional training. We do so via latent sampling and geometric optimization using pre-trained and frozen Denoising Diffusion Probabilistic Models (DDPMs) on single-domain datasets. Our key observation is that DDPMs pre-trained even just on single-domain images are already equipped with sufficient representation abilities to reconstruct arbitrary images from the inverted latent encoding following bi-directional deterministic diffusion and denoising trajectories. This motivates us to investigate the statistical and geometric behaviors of the Out-Of-Distribution (OOD) samples from unseen image domains in the latent spaces along the denoising chain. Notably, we theoretically and empirically show that the inverted OOD samples also establish Gaussians that are 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2309.16858</link><description>&lt;p&gt;
Transductive Learning&#30340;&#23574;&#38160;&#27867;&#21270;&#65306;&#19968;&#31181;Transductive Local Rademacher Complexity&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sharp Generalization of Transductive Learning: A Transductive Local Rademacher Complexity Approach. (arXiv:2309.16858v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16858
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#21033;&#29992;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#26500;&#24314;&#20102;TLRC&#65292;&#24182;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24037;&#20855;&#65292;Transductive Local Rademacher Complexity (TLRC)&#65292;&#29992;&#20110;&#20998;&#26512;transductive learning&#26041;&#27861;&#30340;&#27867;&#21270;&#24615;&#33021;&#24182;&#25512;&#21160;&#26032;&#30340;transductive learning&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23558;&#20256;&#32479;&#30340;local rademacher complexity (LRC)&#30340;&#24605;&#24819;&#25193;&#23637;&#21040;&#20102;transductive&#35774;&#32622;&#20013;&#65292;&#30456;&#23545;&#20110;&#20856;&#22411;&#30340;LRC&#26041;&#27861;&#22312;&#24402;&#32435;&#35774;&#32622;&#20013;&#30340;&#20998;&#26512;&#26377;&#20102;&#30456;&#24403;&#22823;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Rademacher complex&#30340;&#23616;&#37096;&#21270;&#24037;&#20855;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;transductive learning&#38382;&#39064;&#65292;&#24182;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#24471;&#21040;&#20102;&#23574;&#38160;&#30340;&#30028;&#38480;&#12290;&#19982;LRC&#30340;&#21457;&#23637;&#31867;&#20284;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#29420;&#31435;&#21464;&#37327;&#30340;&#26041;&#24046;&#20449;&#24687;&#24320;&#22987;&#26500;&#24314;TLRC&#65292;&#23558;transductive learning&#27169;&#22411;&#30340;&#39044;&#27979;&#20989;&#25968;&#31867;&#20998;&#20026;&#22810;&#20010;&#37096;&#20998;&#65292;&#27599;&#20010;&#37096;&#20998;&#30340;Rademacher complexity&#19978;&#30028;&#30001;&#19968;&#20010;&#23376;&#26681;&#20989;&#25968;&#32473;&#20986;&#65292;&#24182;&#38480;&#21046;&#20102;&#27599;&#20010;&#37096;&#20998;&#20013;&#25152;&#26377;&#20989;&#25968;&#30340;&#26041;&#24046;&#12290;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;...
&lt;/p&gt;
&lt;p&gt;
We introduce a new tool, Transductive Local Rademacher Complexity (TLRC), to analyze the generalization performance of transductive learning methods and motivate new transductive learning algorithms. Our work extends the idea of the popular Local Rademacher Complexity (LRC) to the transductive setting with considerable changes compared to the analysis of typical LRC methods in the inductive setting. We present a localized version of Rademacher complexity based tool wihch can be applied to various transductive learning problems and gain sharp bounds under proper conditions. Similar to the development of LRC, we build TLRC by starting from a sharp concentration inequality for independent variables with variance information. The prediction function class of a transductive learning model is then divided into pieces with a sub-root function being the upper bound for the Rademacher complexity of each piece, and the variance of all the functions in each piece is limited. A carefully designed 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2205.12186</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#22686;&#24378;&#25345;&#32493;&#23398;&#20064;: &#23545;&#25239;&#36127;&#34920;&#31034;&#28418;&#31227;
&lt;/p&gt;
&lt;p&gt;
Enhancing Continual Learning with Global Prototypes: Counteracting Negative Representation Drift. (arXiv:2205.12186v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12186
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#21407;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#19979;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#65292;&#20197;&#32531;&#35299;&#36127;&#38754;&#34920;&#31034;&#28418;&#31227;&#38382;&#39064;&#65292;&#24182;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20854;&#20013;&#25968;&#25454;&#20998;&#24067;&#20174;&#19968;&#20010;&#20219;&#21153;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#35757;&#32451;&#26032;&#20219;&#21153;&#25968;&#25454;&#26102;&#65292;&#26087;&#20219;&#21153;&#30340;&#25968;&#25454;&#34920;&#31034;&#21487;&#33021;&#20250;&#28418;&#31227;&#12290;&#19968;&#20123;&#36127;&#38754;&#30340;&#34920;&#31034;&#28418;&#31227;&#21487;&#33021;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#22240;&#20026;&#20250;&#23548;&#33268;&#20174;&#26412;&#22320;&#23398;&#20064;&#30340;&#31867;&#21035;&#21407;&#22411;&#21644;&#25968;&#25454;&#34920;&#31034;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#36739;&#24046;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#34920;&#31034;&#28418;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21407;&#22411;&#25351;&#23548;&#23398;&#20064;&#65292;&#29992;&#33258;&#30417;&#30563;&#20449;&#24687;&#30340;&#27491;&#21017;&#21270;&#26469;&#23398;&#20064;&#25968;&#25454;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23545;&#20110;NLP&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#20219;&#21153;&#20197;&#23631;&#34109;&#35821;&#35328;&#24314;&#27169;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24335;&#21270;&#65292;&#24182;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30456;&#37051;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#20986;&#20855;&#26377;&#36739;&#23569;&#34920;&#31034;&#28418;&#31227;&#30340;&#30456;&#24403;&#19968;&#33268;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#19981;&#37325;&#26032;&#37319;&#26679;&#36807;&#21435;&#20219;&#21153;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#20943;&#23569;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) aims to learn a sequence of tasks over time, with data distributions shifting from one task to another. When training on new task data, data representations from old tasks may drift. Some negative representation drift can result in catastrophic forgetting, by causing the locally learned class prototypes and data representations to correlate poorly across tasks. To mitigate such representation drift, we propose a method that finds global prototypes to guide the learning, and learns data representations with the regularization of the self-supervised information. Specifically, for NLP tasks, we formulate each task in a masked language modeling style, and learn the task via a neighbor attention mechanism over a pre-trained language model. Experimental results show that our proposed method can learn fairly consistent representations with less representation drift, and significantly reduce catastrophic forgetting in CL without resampling data from past tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#21457;&#31958;&#23615;&#30149;&#25104;&#20154;&#24739;&#32773;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ADA&#39118;&#38505;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.02900</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#36827;&#34892;&#26032;&#21457;&#31958;&#23615;&#30149;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
New-Onset Diabetes Assessment Using Artificial Intelligence-Enhanced Electrocardiography. (arXiv:2205.02900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#26032;&#21457;&#31958;&#23615;&#30149;&#25104;&#20154;&#24739;&#32773;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ADA&#39118;&#38505;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#29305;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#35786;&#26029;&#30340;&#31958;&#23615;&#30149;&#22312;&#24739;&#32773;&#20013;&#21344;21.4&#65285;&#65292;&#30001;&#20110;&#31579;&#26597;&#29575;&#30340;&#38480;&#21046;&#65292;&#31958;&#23615;&#30149;&#21487;&#33021;&#28508;&#20239;&#26080;&#30151;&#29366;&#32780;&#26410;&#34987;&#26816;&#27979;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22686;&#24378;&#30340;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#26469;&#30830;&#23450;&#26032;&#21457;&#31958;&#23615;&#30149;&#30340;&#25104;&#20154;&#24739;&#32773;&#12290; &#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;12&#23548;&#32852;&#24515;&#30005;&#22270;&#21644;&#21487;&#29992;&#30340;&#20154;&#21475;&#32479;&#35745;&#23398;&#25968;&#25454;&#26469;&#20272;&#35745;HbA1c&#12290; &#25105;&#20204;&#22238;&#39038;&#24615;&#22320;&#25910;&#38598;&#20102;&#19968;&#32452;&#21253;&#21547;&#26377;&#37197;&#23545;&#30340;ECG&#21644;HbA1c&#25968;&#25454;&#30340;&#30149;&#20154;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;ADA&#39118;&#38505;&#26816;&#27979;&#65292;&#22522;&#20110;ECG&#30340;&#35780;&#20272;&#25928;&#26524;&#26356;&#22909;&#12290;AI&#22686;&#24378;&#30340;ECG&#35780;&#20272;&#30340;&#20934;&#30830;&#24615;&#36798;&#21040;81&#65285;&#65292;&#28789;&#25935;&#24230;&#20026;80&#65285;&#65292;&#29305;&#24322;&#24615;&#20026;82&#65285;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#30340;ECG&#21487;&#20197;&#25104;&#20026;&#26032;&#21457;&#31958;&#23615;&#30149;&#25104;&#20154;&#24739;&#32773;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#32479;&#31579;&#26597;&#26041;&#27861;&#26377;&#38480;&#30340;&#20154;&#32676;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Undiagnosed diabetes is present in 21.4% of adults with diabetes. Diabetes can remain asymptomatic and undetected due to limitations in screening rates. To address this issue, questionnaires, such as the American Diabetes Association (ADA) Risk test, have been recommended for use by physicians and the public. Based on evidence that blood glucose concentration can affect cardiac electrophysiology, we hypothesized that an artificial intelligence (AI)-enhanced electrocardiogram (ECG) could identify adults with new-onset diabetes. We trained a neural network to estimate HbA1c using a 12-lead ECG and readily available demographics. We retrospectively assembled a dataset comprised of patients with paired ECG and HbA1c data. The population of patients who receive both an ECG and HbA1c may a biased sample of the complete outpatient population, so we adjusted the importance placed on each patient to generate a more representative pseudo-population. We found ECG-based assessment outperforms the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/1808.08316</link><description>&lt;p&gt;
&#19968;&#31181;&#19977;&#20803;&#31070;&#32463;&#27169;&#22411;&#29992;&#20110;&#21160;&#24577;&#23454;&#20307;&#30456;&#20851;&#24615;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
A Trio Neural Model for Dynamic Entity Relatedness Ranking. (arXiv:1808.08316v4 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/1808.08316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#23398;&#20064;&#21040;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#65292;&#33021;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#23454;&#20307;&#30456;&#20851;&#24615;&#26159;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#24212;&#29992;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#22312;&#38745;&#24577;&#35774;&#32622;&#21644;&#38750;&#30417;&#30563;&#26041;&#24335;&#19979;&#30740;&#31350;&#23454;&#20307;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#23454;&#20307;&#24448;&#24448;&#28041;&#21450;&#35768;&#22810;&#19981;&#21516;&#30340;&#20851;&#31995;&#65292;&#22240;&#27492;&#23454;&#20307;&#20851;&#31995;&#38543;&#26102;&#38388;&#21464;&#24471;&#38750;&#24120;&#21160;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#21160;&#24577;&#35780;&#20272;&#23454;&#20307;&#30456;&#20851;&#24615;&#65292;&#21033;&#29992;&#38598;&#20307;&#27880;&#24847;&#21147;&#20316;&#20026;&#30417;&#30563;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#22312;&#32852;&#21512;&#26694;&#26550;&#20013;&#23398;&#20064;&#20016;&#23500;&#32780;&#19981;&#21516;&#30340;&#23454;&#20307;&#34920;&#31034;&#12290;&#36890;&#36807;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#31454;&#20105;&#22522;&#32447;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring entity relatedness is a fundamental task for many natural language processing and information retrieval applications. Prior work often studies entity relatedness in static settings and an unsupervised manner. However, entities in real-world are often involved in many different relationships, consequently entity-relations are very dynamic over time. In this work, we propose a neural networkbased approach for dynamic entity relatedness, leveraging the collective attention as supervision. Our model is capable of learning rich and different entity representations in a joint framework. Through extensive experiments on large-scale datasets, we demonstrate that our method achieves better results than competitive baselines.
&lt;/p&gt;</description></item></channel></rss>