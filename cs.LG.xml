<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.17064</link><description>&lt;p&gt;
&#22312;T2I&#27169;&#22411;&#20013;&#36890;&#36807;&#35782;&#21035;&#35821;&#20041;&#26041;&#21521;&#23454;&#29616;&#36830;&#32493;&#12289;&#20027;&#39064;&#29305;&#23450;&#30340;&#23646;&#24615;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Continuous, Subject-Specific Attribute Control in T2I Models by Identifying Semantic Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17064
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35782;&#21035;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#30340;&#35821;&#20041;&#26041;&#21521;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23545;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#30340;&#38480;&#21046;&#65288;&#20363;&#22914;&#8220;&#20154;&#8221;&#21644;&#8220;&#32769;&#24180;&#20154;&#8221;&#20043;&#38388;&#19981;&#23384;&#22312;&#36830;&#32493;&#30340;&#20013;&#38388;&#25551;&#36848;&#30340;&#38598;&#21512;&#65289;&#65292;&#23454;&#29616;&#23545;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#25511;&#21046;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#23613;&#31649;&#24341;&#20837;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#25110;&#29983;&#25104;&#36807;&#31243;&#20197;&#23454;&#29616;&#36825;&#31181;&#25511;&#21046;&#65292;&#20294;&#19981;&#38656;&#35201;&#22266;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#21551;&#29992;&#20840;&#23616;&#32454;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#25110;&#20165;&#38480;&#20110;&#29305;&#23450;&#20027;&#39064;&#30340;&#31895;&#31890;&#24230;&#23646;&#24615;&#34920;&#36798;&#25511;&#21046;&#65292;&#32780;&#19981;&#33021;&#21516;&#26102;&#20860;&#39038;&#20004;&#32773;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#29992;&#30340;&#22522;&#20110;&#26631;&#35760;&#32423;&#21035;&#30340;CLIP&#25991;&#26412;&#23884;&#20837;&#20013;&#23384;&#22312;&#21487;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#39640;&#32423;&#23646;&#24615;&#30340;&#32454;&#31890;&#24230;&#20027;&#39064;&#29305;&#23450;&#25511;&#21046;&#30340;&#26041;&#21521;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17064v1 Announce Type: cross  Abstract: In recent years, advances in text-to-image (T2I) diffusion models have substantially elevated the quality of their generated images. However, achieving fine-grained control over attributes remains a challenge due to the limitations of natural language prompts (such as no continuous set of intermediate descriptions existing between ``person'' and ``old person''). Even though many methods were introduced that augment the model or generation process to enable such control, methods that do not require a fixed reference image are limited to either enabling global fine-grained attribute expression control or coarse attribute expression control localized to specific subjects, not both simultaneously. We show that there exist directions in the commonly used token-level CLIP text embeddings that enable fine-grained subject-specific control of high-level attributes in text-to-image models. Based on this observation, we introduce one efficient op
&lt;/p&gt;</description></item><item><title>&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08215</link><description>&lt;p&gt;
LIX&#65306;&#23558;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08215
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#34701;&#21512;&#32593;&#32476;&#22312;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#32570;&#20047;&#31354;&#38388;&#20960;&#20309;&#25968;&#25454;&#26102;&#65292;&#21452;&#32534;&#30721;&#22120;&#21464;&#24471;&#26080;&#25928;&#12290;&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#26159;&#19968;&#20010;&#23454;&#29992;&#20294;&#19981;&#22826;&#25506;&#32034;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#24182;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Learning to Infuse "X" (LIX) &#26694;&#26550;&#65292;&#22312;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#38754;&#36827;&#34892;&#20102;&#26032;&#39062;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#35777;&#26126;&#65292;&#24378;&#35843;&#22312;&#35299;&#32806;&#30693;&#35782;&#33976;&#39311;&#20013;&#20351;&#29992;&#21333;&#19968;&#22266;&#23450;&#26435;&#37325;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;logit&#26234;&#33021;&#21160;&#24577;&#26435;&#37325;&#25511;&#21046;&#22120;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#26032;&#26657;&#20934;&#30340;&#29305;&#24449;&#33976;&#39311;&#31639;&#27861;&#65292;&#21253;&#25324;&#20004;&#31181;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08215v1 Announce Type: cross  Abstract: Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to knowledge distillation approaches to address this problem. We introduce the Learning to Infuse "X" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#21457;&#29616;&#22312;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#22312;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#20445;&#25345;&#36739;&#23567;&#35268;&#33539;&#65292;&#24182;&#19988;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#26041;&#21521;&#36817;&#20284;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.08121</link><description>&lt;p&gt;
&#26089;&#26399;&#26041;&#21521;&#24615;&#25910;&#25947;&#22312;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#23567;&#21021;&#22987;&#21270;&#26102;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#21457;&#29616;&#22312;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#22312;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#20445;&#25345;&#36739;&#23567;&#35268;&#33539;&#65292;&#24182;&#19988;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#26041;&#21521;&#36817;&#20284;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#20123;&#32593;&#32476;&#20174;&#23567;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#26412;&#25991;&#32771;&#34385;&#21040;&#20855;&#26377;&#23616;&#37096;Lipschitz&#26799;&#24230;&#21644;&#38454;&#25968;&#20005;&#26684;&#22823;&#20110;&#20004;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25991;&#31456;&#35777;&#26126;&#20102;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#65292;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20445;&#25345;&#35268;&#33539;&#36739;&#23567;&#65292;&#24182;&#19988;&#22312;Karush-Kuhn-Tucker (KKT)&#28857;&#22788;&#36817;&#20284;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;&#26041;&#21521;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#24179;&#26041;&#25439;&#22833;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#19978;&#36827;&#34892;&#21487;&#20998;&#31163;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36824;&#23637;&#31034;&#20102;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#26576;&#20123;&#38797;&#28857;&#38468;&#36817;&#26799;&#24230;&#27969;&#21160;&#21160;&#24577;&#30340;&#31867;&#20284;&#26041;&#21521;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08121v1 Announce Type: new  Abstract: This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations. The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points of the neural correlation function introduced in [1]. Additionally, for square loss and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2403.03551</link><description>&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#20026;&#39640;&#26031;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#65292;&#29992;&#20110;&#22270;&#20687;&#22686;&#24378;&#30340;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#65292;&#30001;&#20110;&#20854;&#22522;&#20110;&#30005;&#31163;&#36752;&#23556;&#65292;&#22240;&#27492;&#24076;&#26395;&#23613;&#37327;&#20943;&#23569;&#36752;&#23556;&#21058;&#37327;&#12290;&#28982;&#32780;&#65292;&#38477;&#20302;&#36752;&#23556;&#21058;&#37327;&#20250;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#65292;&#20174;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#25968;&#25454;&#37325;&#24314;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20540;&#24471;&#36827;&#34892;&#30740;&#31350;&#12290;&#26681;&#25454;LoDoPaB-CT&#22522;&#20934;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#28041;&#21450;UNet&#22411;&#26550;&#26500;&#30340;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25490;&#21517;&#31532;&#19968;&#30340;&#26041;&#27861;ItNet&#20351;&#29992;&#21253;&#25324;&#28388;&#27874;&#21453;&#25237;&#24433;&#65288;FBP&#65289;&#12289;&#22312;CT&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;UNet&#21644;&#36845;&#20195;&#32454;&#21270;&#27493;&#39588;&#30340;&#19977;&#38454;&#27573;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#31532;&#19968;&#38454;&#27573;&#20063;&#20351;&#29992;&#20102;FBP&#65292;&#32780;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29305;&#28857;&#26159;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#26159;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03551v1 Announce Type: cross  Abstract: Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose. However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures. Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step. In this paper, we propose a less complex two-stage method. The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage. The crucial point of our approach is that the neural network is pretrained
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Brenier&#30340;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#31070;&#32463;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#28508;&#22312;&#20989;&#25968;$u$&#65292;&#20174;&#26368;&#26032;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;</title><link>https://arxiv.org/abs/2403.03071</link><description>&lt;p&gt;
&#35770;Brenier&#30340;&#26497;&#20998;&#35299;&#30340;&#31070;&#32463;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
On a Neural Implementation of Brenier's Polar Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03071
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Brenier&#30340;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#31070;&#32463;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#28508;&#22312;&#20989;&#25968;$u$&#65292;&#20174;&#26368;&#26032;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1991&#24180;&#65292;Brenier&#35777;&#26126;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#23558;$QR$&#20998;&#35299;&#65288;&#20998;&#20026;&#21322;&#27491;&#23450;&#30697;&#38453;$\times$&#37193;&#30697;&#38453;&#65289;&#25512;&#24191;&#21040;&#20219;&#24847;&#30690;&#37327;&#22330;$F:\mathbb{R}^d\rightarrow \mathbb{R}^d$&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#20219;&#24847;&#22330;$F$&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#20984;&#20989;&#25968;$u$&#30340;&#26799;&#24230;&#19982;&#20445;&#27979;&#24230;&#26144;&#23556;$M$&#30340;&#22797;&#21512;&#65292;&#21363;$F=\nabla u \circ M$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#20855;&#26377;&#28145;&#36828;&#29702;&#35770;&#24847;&#20041;&#30340;&#32467;&#26524;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#33021;&#30340;&#24212;&#29992;&#12290;&#35813;&#23450;&#29702;&#19982;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#29702;&#35770;&#23494;&#20999;&#30456;&#20851;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#28508;&#22312;&#20989;&#25968;$u$&#21442;&#25968;&#21270;&#20026;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#12290;&#26144;&#23556;$M$&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;$u^*$&#65292;&#21363;$u$&#30340;&#20984;&#20849;&#36717;&#65292;&#36880;&#28857;&#35745;&#31639;&#24471;&#21040;&#65292;&#21363;$M=\nabla u^* \circ F$&#65292;&#25110;&#32773;&#20316;&#20026;&#36741;&#21161;&#32593;&#32476;&#23398;&#20064;&#24471;&#21040;&#12290;&#22240;&#20026;$M$&#22312;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03071v1 Announce Type: cross  Abstract: In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\times$ unitary -- to any vector field $F:\mathbb{R}^d\rightarrow \mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\nabla u \circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\nabla u^* \circ F$, or learned as an auxiliary network. Because $M$ is, in gene
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;</title><link>https://arxiv.org/abs/2402.10686</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#21644;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65306;&#20449;&#24687;&#35770;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Uncertainty, Calibration, and Membership Inference Attacks: An Information-Theoretic Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10686
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20449;&#24687;&#35770;&#26694;&#26550;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#23545;&#19981;&#30830;&#23450;&#24615;&#12289;&#26657;&#20934;&#27700;&#24179;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#20013;&#38544;&#21547;&#30340;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#65288;MIA&#65289;&#20013;&#65292;&#25915;&#20987;&#32773;&#21033;&#29992;&#20856;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#36807;&#24230;&#33258;&#20449;&#26469;&#30830;&#23450;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20869;&#20998;&#26512;&#20102;&#26368;&#20808;&#36827;&#30340;&#20284;&#28982;&#27604;&#25915;&#20987;&#65288;LiRA&#65289;&#30340;&#24615;&#33021;&#65292;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#20801;&#35768;&#30740;&#31350;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#30001;&#26377;&#38480;&#35757;&#32451;&#25968;&#25454;&#38598;&#24341;&#36215;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20197;&#21450;&#30446;&#26631;&#27169;&#22411;&#30340;&#26657;&#20934;&#27700;&#24179;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#35774;&#32622;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#20174;&#30446;&#26631;&#27169;&#22411;&#25509;&#25910;&#21040;&#30340;&#20449;&#24687;&#36880;&#28176;&#20943;&#23569;&#65306;&#32622;&#20449;&#21521;&#37327;&#65288;CV&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#36755;&#20986;&#27010;&#29575;&#21521;&#37327;&#34987;&#21457;&#24067;&#65307;&#30495;&#23454;&#26631;&#31614;&#32622;&#20449;&#24230;&#65288;TLC&#65289;&#25259;&#38706;&#65292;&#20854;&#20013;&#21482;&#26377;&#27169;&#22411;&#20998;&#37197;&#32473;&#30495;&#23454;&#26631;&#31614;&#30340;&#27010;&#29575;&#26159;&#21487;&#29992;&#30340;&#65307;&#20197;&#21450;&#20915;&#31574;&#38598;&#65288;DS&#65289;&#25259;&#38706;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10686v1 Announce Type: cross  Abstract: In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the state-of-the-art likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;ODE&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#22122;&#22768;&#35266;&#23519;&#21644;&#21021;&#22987;&#26465;&#20214;&#20197;&#21450;&#21521;&#37327;&#22330;&#36827;&#34892;&#26144;&#23556;&#65292;&#24471;&#21040;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#30340;&#32467;&#26524;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#12289;&#21305;&#37197;&#21644;&#32452;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07594</link><description>&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Foundational Inference Models for Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;ODE&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#22122;&#22768;&#35266;&#23519;&#21644;&#21021;&#22987;&#26465;&#20214;&#20197;&#21450;&#21521;&#37327;&#22330;&#36827;&#34892;&#26144;&#23556;&#65292;&#24471;&#21040;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#30340;&#32467;&#26524;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#12289;&#21305;&#37197;&#21644;&#32452;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#26500;&#25104;&#20102;&#20316;&#20026;&#33258;&#28982;&#21644;&#31038;&#20250;&#29616;&#35937;&#27169;&#22411;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#20986;&#26368;&#20339;&#25551;&#36848;&#32473;&#23450;&#29616;&#35937;&#30340;&#19968;&#32452;&#22122;&#22768;&#35266;&#23519;&#30340;ODE&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#24448;&#24448;&#20063;&#38750;&#24120;&#19987;&#19994;&#21270;&#21644;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30417;&#30563;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;ODE&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#21021;&#22987;&#26465;&#20214;&#31354;&#38388;&#21644;&#23450;&#20041;&#23427;&#20204;&#30340;&#21521;&#37327;&#22330;&#31354;&#38388;&#30340;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#65292;&#29983;&#25104;&#22823;&#22411;&#19968;&#32500;ODE&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23398;&#20064;&#23558;&#36825;&#20123;&#26041;&#31243;&#30340;&#35299;&#30340;&#22122;&#22768;&#35266;&#23519;&#19982;&#20854;&#30456;&#24212;&#30340;&#21021;&#22987;&#26465;&#20214;&#21644;&#21521;&#37327;&#22330;&#20043;&#38388;&#30340;&#31070;&#32463;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#27169;&#22411;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#65288;i&#65289;&#27839;&#26102;&#38388;&#32500;&#22797;&#21046;&#21644;&#21305;&#37197;&#20197;&#22686;&#21152;&#20998;&#36776;&#29575;&#65307;&#65288;ii&#65289;&#22797;&#21046;&#21644;&#32452;&#21512;&#20197;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LoGoNet&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;LoGoNet&#36890;&#36807;&#37319;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.06190</link><description>&lt;p&gt;
Masked LoGoNet&#65306;&#29992;&#20110;&#21307;&#23398;&#39046;&#22495;&#30340;&#24555;&#36895;&#20934;&#30830;3D&#22270;&#20687;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Masked LoGoNet: Fast and Accurate 3D Image Analysis for Medical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;LoGoNet&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#26469;&#24212;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#25361;&#25112;&#12290;LoGoNet&#36890;&#36807;&#37319;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#32452;&#21512;&#25216;&#26415;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#22270;&#20687;&#26041;&#27861;&#22312;&#21307;&#23398;&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#39640;&#25104;&#26412;&#21644;&#26377;&#38480;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#37096;&#32626;&#26102;&#36890;&#24120;&#29992;&#20110;&#27599;&#22825;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#65292;&#32473;&#21307;&#30103;&#35774;&#26045;&#24102;&#26469;&#39640;&#32500;&#25252;&#25104;&#26412;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;LoGoNet&#65292;&#37319;&#29992;&#23450;&#21046;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;LoGoNet&#22312;U&#24418;&#26550;&#26500;&#20869;&#25972;&#21512;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#65292;&#21033;&#29992;&#22823;&#20869;&#26680;&#27880;&#24847;&#21147;&#65288;LKA&#65289;&#21644;&#21452;&#37325;&#32534;&#30721;&#31574;&#30053;&#65292;&#28789;&#27963;&#22320;&#25429;&#25417;&#38271;&#12289;&#30701;&#36317;&#31163;&#29305;&#24449;&#30456;&#20851;&#24615;&#12290;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#22686;&#21152;&#32593;&#32476;&#23481;&#37327;&#20197;&#22686;&#24378;&#29305;&#24449;&#25552;&#21462;&#30340;&#26041;&#24335;&#24418;&#25104;&#23545;&#27604;&#12290;&#25105;&#20204;&#27169;&#22411;&#20013;&#36825;&#20123;&#26032;&#25216;&#26415;&#30340;&#32452;&#21512;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#29305;&#21035;&#26377;&#30410;&#65292;&#32771;&#34385;&#21040;&#20854;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Standard modern machine-learning-based imaging methods have faced challenges in medical applications due to the high cost of dataset construction and, thereby, the limited labeled training data available. Additionally, upon deployment, these methods are usually used to process a large volume of data on a daily basis, imposing a high maintenance cost on medical facilities. In this paper, we introduce a new neural network architecture, termed LoGoNet, with a tailored self-supervised learning (SSL) method to mitigate such challenges. LoGoNet integrates a novel feature extractor within a U-shaped architecture, leveraging Large Kernel Attention (LKA) and a dual encoding strategy to capture both long-range and short-range feature dependencies adeptly. This is in contrast to existing methods that rely on increasing network capacity to enhance feature extraction. This combination of novel techniques in our model is especially beneficial in medical image segmentation, given the difficulty of le
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;BOWLL&#65292;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#26631;&#20934;&#27169;&#22411;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#65292;&#21152;&#36895;&#20102;&#36825;&#20010;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#25506;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.04814</link><description>&lt;p&gt;
BOWLL&#65306;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
BOWLL: A Deceptively Simple Open World Lifelong Learner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04814
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;BOWLL&#65292;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#26631;&#20934;&#27169;&#22411;&#29992;&#20110;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#65292;&#21152;&#36895;&#20102;&#36825;&#20010;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#23545;&#39044;&#20808;&#30830;&#23450;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#26631;&#37327;&#24615;&#33021;&#25968;&#23383;&#30340;&#25913;&#36827;&#20284;&#20046;&#28145;&#28145;&#26893;&#26681;&#20110;&#20854;&#20013;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#24456;&#23569;&#31934;&#24515;&#31574;&#21010;&#65292;&#24212;&#29992;&#20063;&#24456;&#23569;&#20165;&#38480;&#20110;&#22312;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#23454;&#38469;&#30340;&#31995;&#32479;&#26469;&#35782;&#21035;&#26032;&#27010;&#24565;&#65292;&#36991;&#20813;&#20027;&#21160;&#21253;&#25324;&#26080;&#20449;&#24687;&#30340;&#25968;&#25454;&#65292;&#24182;&#22312;&#20854;&#29983;&#21629;&#21608;&#26399;&#20869;&#20445;&#30041;&#20808;&#21069;&#33719;&#21462;&#30340;&#30693;&#35782;&#12290;&#23613;&#31649;&#36825;&#20123;&#20851;&#38190;&#35201;&#32032;&#22312;&#20010;&#20307;&#19978;&#24050;&#32463;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#30740;&#31350;&#65292;&#20294;&#23545;&#23427;&#20204;&#30340;&#32467;&#21512;&#65292;&#21363;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#65292;&#21482;&#26159;&#26368;&#36817;&#30340;&#36235;&#21183;&#12290;&#20026;&#20102;&#21152;&#36895;&#36825;&#20010;&#22810;&#26041;&#38754;&#39046;&#22495;&#30340;&#25506;&#32034;&#65292;&#25105;&#20204;&#24341;&#20837;&#20854;&#39318;&#20010;&#23436;&#25972;&#19988;&#26497;&#24230;&#38656;&#35201;&#30340;&#22522;&#20934;&#12290;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25209;&#37327;&#24402;&#19968;&#21270;&#30340;&#26222;&#36941;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#30475;&#20284;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#37325;&#26032;&#21033;&#29992;&#26631;&#20934;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#19990;&#30028;&#32456;&#36523;&#23398;&#20064;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#24378;&#35843;&#20026;&#20160;&#20040;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#35813;&#25104;&#20026;&#26410;&#26469;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The quest to improve scalar performance numbers on predetermined benchmarks seems to be deeply engraved in deep learning. However, the real world is seldom carefully curated and applications are seldom limited to excelling on test sets. A practical system is generally required to recognize novel concepts, refrain from actively including uninformative data, and retain previously acquired knowledge throughout its lifetime. Despite these key elements being rigorously researched individually, the study of their conjunction, open world lifelong learning, is only a recent trend. To accelerate this multifaceted field's exploration, we introduce its first monolithic and much-needed baseline. Leveraging the ubiquitous use of batch normalization across deep neural networks, we propose a deceptively simple yet highly effective way to repurpose standard models for open world lifelong learning. Through extensive empirical evaluation, we highlight why our approach should serve as a future standard f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20855;&#26377;&#20559;&#24577;&#26799;&#24230;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;Adagrad&#21644;RMSProp&#31639;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#19982;&#26080;&#20559;&#24773;&#20917;&#30456;&#20284;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25910;&#25947;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#38477;&#20302;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.02857</link><description>&lt;p&gt;
&#20559;&#24577;&#33258;&#36866;&#24212;&#38543;&#26426;&#36924;&#36817;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20855;&#26377;&#20559;&#24577;&#26799;&#24230;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38750;&#28176;&#36827;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;Adagrad&#21644;RMSProp&#31639;&#27861;&#22312;&#25910;&#25947;&#36895;&#24230;&#19978;&#19982;&#26080;&#20559;&#24773;&#20917;&#30456;&#20284;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25910;&#25947;&#32467;&#26524;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#38477;&#20302;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#27493;&#38271;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#29616;&#22312;&#24191;&#27867;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#22823;&#22810;&#25968;&#29702;&#35770;&#32467;&#26524;&#20551;&#35774;&#21487;&#20197;&#33719;&#24471;&#26080;&#20559;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#28982;&#32780;&#22312;&#19968;&#20123;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#20351;&#29992;&#20102;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#65292;&#21364;&#26080;&#27861;&#28385;&#36275;&#36825;&#19968;&#20551;&#35774;&#12290;&#26412;&#25991;&#23545;&#20855;&#26377;&#20559;&#24577;&#26799;&#24230;&#21644;&#33258;&#36866;&#24212;&#27493;&#38271;&#30340;SGD&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#38750;&#28176;&#36827;&#24615;&#20998;&#26512;&#65292;&#38024;&#23545;&#20984;&#21644;&#38750;&#20984;&#24179;&#28369;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21253;&#25324;&#26102;&#21464;&#20559;&#24046;&#65292;&#24182;&#24378;&#35843;&#25511;&#21046;&#20559;&#24046;&#21644;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26799;&#24230;&#20272;&#35745;&#30340;&#37325;&#35201;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#20559;&#24577;&#26799;&#24230;&#30340;Adagrad&#21644;RMSProp&#31639;&#27861;&#23545;&#20110;&#38750;&#20984;&#24179;&#28369;&#20989;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#19982;&#25991;&#29486;&#20013;&#26080;&#20559;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#30456;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25910;&#25947;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36866;&#24403;&#30340;&#26041;&#27861;&#38477;&#20302;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for training deep neural networks. Most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for convex and non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In particular, we establish that Adagrad and RMSProp with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23569;&#27425;&#35843;&#29992;&#30340;&#26041;&#27861;&#31363;&#21462;&#40657;&#30418;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#35775;&#38382;&#38480;&#21046;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2310.00096</link><description>&lt;p&gt;
&#36817;&#20284;&#40657;&#30418;&#27169;&#22411;&#31363;&#21462;&#30340;&#23569;&#27425;&#35843;&#29992;&#26041;&#27861;&#65306;&#27963;&#36291;&#33258;&#36866;&#24212;&#30693;&#35782;&#33976;&#39311;&#21644;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Towards Few-Call Model Stealing via Active Self-Paced Knowledge Distillation and Diffusion-Based Image Generation. (arXiv:2310.00096v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#23569;&#27425;&#35843;&#29992;&#30340;&#26041;&#27861;&#31363;&#21462;&#40657;&#30418;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#31361;&#30772;&#20102;&#35775;&#38382;&#38480;&#21046;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20363;&#65292;&#21363;&#22312;&#27809;&#26377;&#35775;&#38382;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#12289;&#26550;&#26500;&#21644;&#27169;&#22411;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#22797;&#21046;&#40657;&#30418;&#20998;&#31867;&#27169;&#22411;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25512;&#29702;API&#20351;&#29992;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21482;&#33021;&#35266;&#23519;&#21040;&#19968;&#20123;&#22270;&#20687;&#26679;&#26412;&#20316;&#20026;&#36755;&#20837;&#20256;&#36882;&#32473;&#27169;&#22411;&#26102;&#30340;&#65288;&#36719;&#24615;&#25110;&#30828;&#24615;&#65289;&#26631;&#31614;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32771;&#34385;&#21040;&#38480;&#21046;&#27169;&#22411;&#35843;&#29992;&#27425;&#25968;&#30340;&#39069;&#22806;&#32422;&#26463;&#65292;&#20027;&#35201;&#20851;&#27880;&#20110;&#23569;&#27425;&#35843;&#29992;&#30340;&#27169;&#22411;&#31363;&#21462;&#12290;&#20026;&#20102;&#22312;&#24212;&#29992;&#38480;&#21046;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#27169;&#22411;&#25552;&#21462;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#26694;&#26550;&#12290;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#65292;&#25105;&#20204;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#19988;&#22810;&#26679;&#21270;&#30340;&#22270;&#20687;&#21019;&#24314;&#20102;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#20195;&#29702;&#25968;&#25454;&#38598;&#65289;&#12290;&#32473;&#23450;&#20801;&#35768;&#30340;&#26368;&#22823;API&#35843;&#29992;&#27425;&#25968;&#65292;&#25105;&#20204;&#20256;&#36882;&#30456;&#24212;&#25968;&#37327;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models showcased strong capabilities in image synthesis, being used in many computer vision tasks with great success. To this end, we propose to explore a new use case, namely to copy black-box classification models without having access to the original training data, the architecture, and the weights of the model, \ie~the model is only exposed through an inference API. More specifically, we can only observe the (soft or hard) labels for some image samples passed as input to the model. Furthermore, we consider an additional constraint limiting the number of model calls, mostly focusing our research on few-call model stealing. In order to solve the model extraction task given the applied restrictions, we propose the following framework. As training data, we create a synthetic data set (called proxy data set) by leveraging the ability of diffusion models to generate realistic and diverse images. Given a maximum number of allowed API calls, we pass the respective number of sampl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#26679;&#25237;&#24433;&#38598;&#21512;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#38598;&#21512;&#24046;&#24322;&#24230;&#37327;&#30340;&#31639;&#27861;&#65292;&#20197;&#20419;&#36827;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2306.07124</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26679;&#25237;&#24433;&#38598;&#21512;
&lt;/p&gt;
&lt;p&gt;
Diverse Projection Ensembles for Distributional Reinforcement Learning. (arXiv:2306.07124v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#26679;&#25237;&#24433;&#38598;&#21512;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#38598;&#21512;&#24046;&#24322;&#24230;&#37327;&#30340;&#31639;&#27861;&#65292;&#20197;&#20419;&#36827;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#19981;&#21516;&#65292;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26088;&#22312;&#23398;&#20064;&#22238;&#25253;&#30340;&#20998;&#24067;&#32780;&#19981;&#26159;&#20854;&#26399;&#26395;&#20540;&#12290;&#30001;&#20110;&#22238;&#25253;&#20998;&#24067;&#30340;&#24615;&#36136;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#25110;&#36807;&#20110;&#22797;&#26434;&#65292;&#22240;&#27492;&#36890;&#24120;&#37319;&#29992;&#23558;&#26410;&#32422;&#26463;&#30340;&#20998;&#24067;&#25237;&#24433;&#21040;&#21487;&#34920;&#31034;&#30340;&#21442;&#25968;&#20998;&#24067;&#38598;&#21512;&#20013;&#30340;&#26041;&#27861;&#36827;&#34892;&#36924;&#36817;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24403;&#23558;&#36825;&#31181;&#25237;&#24433;&#27493;&#39588;&#19982;&#31070;&#32463;&#32593;&#32476;&#21644;&#26799;&#24230;&#19979;&#38477;&#30456;&#32467;&#21512;&#26102;&#65292;&#36825;&#31181;&#25237;&#24433;&#27493;&#39588;&#20250;&#20135;&#29983;&#24378;&#28872;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20174;&#32780;&#28145;&#21051;&#24433;&#21709;&#23398;&#20064;&#27169;&#22411;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#20026;&#20102;&#36890;&#36807;&#22810;&#26679;&#24615;&#20419;&#36827;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#38598;&#21512;&#20013;&#22810;&#20010;&#19981;&#21516;&#30340;&#25237;&#24433;&#21644;&#34920;&#31034;&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#31181;&#25237;&#24433;&#38598;&#21512;&#30340;&#29702;&#35770;&#29305;&#24615;&#65292;&#24182;&#25512;&#23548;&#20986;&#19968;&#31181;&#20351;&#29992;&#38598;&#21512;&#24046;&#24322;&#24230;&#37327;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In contrast to classical reinforcement learning, distributional reinforcement learning algorithms aim to learn the distribution of returns rather than their expected value. Since the nature of the return distribution is generally unknown a priori or arbitrarily complex, a common approach finds approximations within a set of representable, parametric distributions. Typically, this involves a projection of the unconstrained distribution onto the set of simplified distributions. We argue that this projection step entails a strong inductive bias when coupled with neural networks and gradient descent, thereby profoundly impacting the generalization behavior of learned models. In order to facilitate reliable uncertainty estimation through diversity, this work studies the combination of several different projections and representations in a distributional ensemble. We establish theoretical properties of such projection ensembles and derive an algorithm that uses ensemble disagreement, measure
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#34920;&#24449;&#30340;Vecchia&#39640;&#26031;&#36807;&#31243;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#19982;DNN&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#19968;&#31181;&#19981;&#20165;&#33021;&#22815;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19988;&#33021;&#22815;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#30340;&#28145;&#24230;Vecchia&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.17063</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#34920;&#24449;&#19978;&#30340;Vecchia&#39640;&#26031;&#36807;&#31243;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Vecchia Gaussian Process Ensembles on Internal Representations of Deep Neural Networks. (arXiv:2305.17063v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17063
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#34920;&#24449;&#30340;Vecchia&#39640;&#26031;&#36807;&#31243;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;&#19982;DNN&#30456;&#32467;&#21512;&#65292;&#29983;&#25104;&#19968;&#31181;&#19981;&#20165;&#33021;&#22815;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#32780;&#19988;&#33021;&#22815;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#30340;&#28145;&#24230;Vecchia&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22238;&#24402;&#20219;&#21153;&#65292;&#26631;&#20934;&#39640;&#26031;&#36807;&#31243;(GPs)&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#32780;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25797;&#38271;&#34920;&#24449;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#21327;&#21516;&#32452;&#21512;&#36215;&#26469;&#65292;&#24418;&#25104;&#19968;&#20010;&#22522;&#20110;DNN&#30340;&#38544;&#34255;&#23618;&#36755;&#20986;&#26500;&#24314;&#30340;GP&#38598;&#21512;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#37051;&#26465;&#20214;&#29420;&#31435;&#30340;Vecchia&#36817;&#20284;&#23454;&#29616;&#20102;GP&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#29983;&#25104;&#30340;&#28145;&#24230;Vecchia&#38598;&#21512;&#19981;&#20165;&#36171;&#20104;DNN&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#36824;&#21487;&#20197;&#25552;&#20379;&#26356;&#20934;&#30830;&#21644;&#26356;&#31283;&#20581;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25928;&#29992;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#20197;&#20102;&#35299;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
For regression tasks, standard Gaussian processes (GPs) provide natural uncertainty quantification, while deep neural networks (DNNs) excel at representation learning. We propose to synergistically combine these two approaches in a hybrid method consisting of an ensemble of GPs built on the output of hidden layers of a DNN. GP scalability is achieved via Vecchia approximations that exploit nearest-neighbor conditional independence. The resulting deep Vecchia ensemble not only imbues the DNN with uncertainty quantification but can also provide more accurate and robust predictions. We demonstrate the utility of our model on several datasets and carry out experiments to understand the inner workings of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2305.10947</link><description>&lt;p&gt;
&#20851;&#20110;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#36777;&#25252;
&lt;/p&gt;
&lt;p&gt;
In Defense of Pure 16-bit Floating-Point Neural Networks. (arXiv:2305.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#30340;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#65292;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#65292;&#24182;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#30340;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#25152;&#38656;&#30340;&#20301;&#25968;&#26159;&#38750;&#24120;&#21487;&#21462;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21152;&#24555;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#26102;&#38388;&#65292;&#21516;&#26102;&#20943;&#23569;&#20869;&#23384;&#28040;&#32791;&#12290;&#22240;&#27492;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20197;&#24320;&#21457;&#21033;&#29992;&#26356;&#20302;&#31934;&#24230;&#35745;&#31639;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#27604;&#22914;&#28151;&#21512;&#31934;&#24230;&#35757;&#32451;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#30446;&#21069;&#19981;&#23384;&#22312;&#32431;16&#20301;&#28014;&#28857;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25581;&#31034;&#20102;&#32431;16&#20301;&#28014;&#28857;&#31070;&#32463;&#32593;&#32476;&#34987;&#24573;&#35270;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20379;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#25506;&#35752;&#36896;&#25104;16&#20301;&#21644;32&#20301;&#27169;&#22411;&#30340;&#24046;&#24322;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#28014;&#28857;&#35823;&#24046;&#21644;&#23481;&#24525;&#24230;&#30340;&#27010;&#24565;&#65292;&#20174;&#32780;&#21487;&#20197;&#23450;&#37327;&#35299;&#37322;16&#20301;&#27169;&#22411;&#19982;&#20854;32&#20301;&#23545;&#24212;&#29289;&#20043;&#38388;&#23494;&#20999;&#36924;&#36817;&#32467;&#26524;&#30340;&#26465;&#20214;&#12290;&#36825;&#31181;&#29702;&#35770;&#25506;&#32034;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reducing the number of bits needed to encode the weights and activations of neural networks is highly desirable as it speeds up their training and inference time while reducing memory consumption. For these reasons, research in this area has attracted significant attention toward developing neural networks that leverage lower-precision computing, such as mixed-precision training. Interestingly, none of the existing approaches has investigated pure 16-bit floating-point settings. In this paper, we shed light on the overlooked efficiency of pure 16-bit floating-point neural networks. As such, we provide a comprehensive theoretical analysis to investigate the factors contributing to the differences observed between 16-bit and 32-bit models. We formalize the concepts of floating-point error and tolerance, enabling us to quantitatively explain the conditions under which a 16-bit model can closely approximate the results of its 32-bit counterpart. This theoretical exploration offers perspect
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20844;&#21496;&#30408;&#21033;&#65292;&#20294;&#21516;&#26679;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#32780;&#20256;&#32479;&#22521;&#35757;&#30340;&#32929;&#24066;&#20998;&#26512;&#24072;&#21644;&#32463;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22521;&#35757;&#30340;&#20998;&#26512;&#24072;&#30456;&#27604;&#20250;&#20135;&#29983;&#36739;&#23569;&#30340;&#36807;&#24230;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2303.16158</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20934;&#30830;&#39044;&#27979;&#36130;&#25253;&#65292;&#20294;&#21516;&#26679;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Behavioral Machine Learning? Computer Predictions of Corporate Earnings also Overreact. (arXiv:2303.16158v1 [q-fin.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16158
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#21457;&#29616;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#20844;&#21496;&#30408;&#21033;&#65292;&#20294;&#21516;&#26679;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;&#30340;&#38382;&#39064;&#65292;&#32780;&#20256;&#32479;&#22521;&#35757;&#30340;&#32929;&#24066;&#20998;&#26512;&#24072;&#21644;&#32463;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22521;&#35757;&#30340;&#20998;&#26512;&#24072;&#30456;&#27604;&#20250;&#20135;&#29983;&#36739;&#23569;&#30340;&#36807;&#24230;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#35777;&#25454;&#34920;&#26126;&#65292;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#27604;&#20154;&#31867;&#26356;&#20026;&#20934;&#30830;&#12290;&#20294;&#26159;&#65292;&#25991;&#29486;&#24182;&#26410;&#27979;&#35797;&#31639;&#27861;&#39044;&#27979;&#26159;&#21542;&#26356;&#20026;&#29702;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20010;&#31639;&#27861;&#65288;&#21253;&#25324;&#32447;&#24615;&#22238;&#24402;&#21644;&#19968;&#31181;&#21517;&#20026;Gradient Boosted Regression Trees&#30340;&#27969;&#34892;&#31639;&#27861;&#65289;&#23545;&#20110;&#20844;&#21496;&#30408;&#21033;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;GBRT&#24179;&#22343;&#32988;&#36807;&#32447;&#24615;&#22238;&#24402;&#21644;&#20154;&#31867;&#32929;&#24066;&#20998;&#26512;&#24072;&#65292;&#20294;&#20173;&#23384;&#22312;&#36807;&#24230;&#21453;&#24212;&#19988;&#26080;&#27861;&#28385;&#36275;&#29702;&#24615;&#39044;&#26399;&#26631;&#20934;&#12290;&#36890;&#36807;&#38477;&#20302;&#23398;&#20064;&#29575;&#65292;&#21487;&#26368;&#23567;&#31243;&#24230;&#19978;&#20943;&#23569;&#36807;&#24230;&#21453;&#24212;&#31243;&#24230;&#65292;&#20294;&#36825;&#20250;&#29306;&#29298;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22521;&#35757;&#36807;&#30340;&#32929;&#24066;&#20998;&#26512;&#24072;&#27604;&#20256;&#32479;&#35757;&#32451;&#30340;&#20998;&#26512;&#24072;&#20135;&#29983;&#30340;&#36807;&#24230;&#21453;&#24212;&#36739;&#23569;&#12290;&#27492;&#22806;&#65292;&#32929;&#24066;&#20998;&#26512;&#24072;&#30340;&#39044;&#27979;&#21453;&#26144;&#20986;&#26426;&#22120;&#31639;&#27861;&#27809;&#26377;&#25429;&#25417;&#21040;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is considerable evidence that machine learning algorithms have better predictive abilities than humans in various financial settings. But, the literature has not tested whether these algorithmic predictions are more rational than human predictions. We study the predictions of corporate earnings from several algorithms, notably linear regressions and a popular algorithm called Gradient Boosted Regression Trees (GBRT). On average, GBRT outperformed both linear regressions and human stock analysts, but it still overreacted to news and did not satisfy rational expectation as normally defined. By reducing the learning rate, the magnitude of overreaction can be minimized, but it comes with the cost of poorer out-of-sample prediction accuracy. Human stock analysts who have been trained in machine learning methods overreact less than traditionally trained analysts. Additionally, stock analyst predictions reflect information not otherwise available to machine algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.02731</link><description>&lt;p&gt;
&#23548;&#33322;&#30340;&#20013;&#23618;&#34920;&#31034;&#8212;&#8212;&#34394;&#25311;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Virtual Guidance as a Mid-level Representation for Navigation. (arXiv:2303.02731v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02731
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#21472;&#21152;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#30340;&#35270;&#35273;&#25351;&#24341;&#65292;&#20197;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#20449;&#24687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#34394;&#25311;&#23548;&#33322;&#22312;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#31561;&#22810;&#20010;&#25351;&#26631;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#20027;&#23548;&#33322;&#30340;&#32972;&#26223;&#19979;&#65292;&#26377;&#25928;&#22320;&#20256;&#36798;&#25277;&#35937;&#30340;&#23548;&#33322;&#25351;&#24341;&#32473;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26234;&#33021;&#20307;&#23384;&#22312;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#24403;&#23548;&#33322;&#20449;&#24687;&#26159;&#22810;&#27169;&#24577;&#30340;&#26102;&#20505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#34394;&#25311;&#23548;&#33322;&#8221;&#30340;&#26032;&#25216;&#26415;&#65292;&#26088;&#22312;&#20197;&#35270;&#35273;&#26041;&#24335;&#21576;&#29616;&#38750;&#35270;&#35273;&#25351;&#20196;&#20449;&#21495;&#12290;&#36825;&#20123;&#35270;&#35273;&#25351;&#24341;&#20197;&#24425;&#33394;&#36335;&#24452;&#25110;&#29699;&#30340;&#24418;&#24335;&#21472;&#21152;&#22312;&#26234;&#33021;&#20307;&#30340;&#30456;&#26426;&#35270;&#22270;&#19978;&#65292;&#20316;&#20026;&#26131;&#20110;&#29702;&#35299;&#30340;&#23548;&#33322;&#25351;&#20196;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#23454;&#39564;&#26469;&#35780;&#20272;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#34394;&#25311;&#23548;&#33322;&#22312;&#22810;&#39033;&#25351;&#26631;&#19978;&#20248;&#20110;&#22522;&#32447;&#28151;&#21512;&#26041;&#27861;&#65292;&#21253;&#25324;&#36981;&#24490;&#35745;&#21010;&#36335;&#24452;&#21644;&#36991;&#24320;&#38556;&#30861;&#29289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#34394;&#25311;&#23548;&#33322;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;&#23558;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#30340;&#25351;&#20196;&#36716;&#25442;&#20026;&#29992;&#20110;&#30495;&#23454;&#29615;&#22659;&#23454;&#39564;&#30340;&#30452;&#35266;&#35270;&#35273;&#26684;&#24335;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#39564;&#35777;&#20102;&#34394;&#25311;&#23548;&#33322;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the context of autonomous navigation, effectively conveying abstract navigational cues to agents in dynamic environments poses challenges, particularly when the navigation information is multimodal. To address this issue, the paper introduces a novel technique termed "Virtual Guidance," which is designed to visually represent non-visual instructional signals. These visual cues, rendered as colored paths or spheres, are overlaid onto the agent's camera view, serving as easily comprehensible navigational instructions. We evaluate our proposed method through experiments in both simulated and real-world settings. In the simulated environments, our virtual guidance outperforms baseline hybrid approaches in several metrics, including adherence to planned routes and obstacle avoidance. Furthermore, we extend the concept of virtual guidance to transform text-prompt-based instructions into a visually intuitive format for real-world experiments. Our results validate the adaptability of virtua
&lt;/p&gt;</description></item></channel></rss>