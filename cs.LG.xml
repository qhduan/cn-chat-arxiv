<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20316;&#20026;&#39069;&#22806;&#25104;&#20687;&#25216;&#26415;&#26469;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#32467;&#21512;&#30340;&#26041;&#27861;&#25104;&#21151;&#21306;&#20998;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#21644;&#38750;&#39118;&#38505;&#24739;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.18873</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#32593;&#33180;OCT&#25104;&#20687;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Predicting risk of cardiovascular disease using retinal OCT imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18873
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20316;&#20026;&#39069;&#22806;&#25104;&#20687;&#25216;&#26415;&#26469;&#39044;&#27979;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#21644;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#32467;&#21512;&#30340;&#26041;&#27861;&#25104;&#21151;&#21306;&#20998;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#39118;&#38505;&#21644;&#38750;&#39118;&#38505;&#24739;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#20316;&#20026;&#19968;&#31181;&#39069;&#22806;&#25104;&#20687;&#25216;&#26415;&#26469;&#39044;&#27979;&#26410;&#26469;&#24515;&#34880;&#31649;&#30142;&#30149;&#65288;CVD&#65289;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#21033;&#29992;&#22522;&#20110;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#33258;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#23398;&#20064;&#20102;&#39640;&#32500;3D OCT&#22270;&#20687;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#24182;&#25429;&#25417;&#20102;OCT&#22270;&#20687;&#20013;&#19981;&#21516;&#35270;&#32593;&#33180;&#23618;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#38543;&#21518;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#28508;&#22312;&#29305;&#24449;&#21644;&#21442;&#19982;&#32773;&#30340;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20197;&#21450;&#20020;&#24202;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#20998;&#31867;&#22120;&#65292;&#20197;&#21306;&#20998;&#22788;&#20110;CVD&#20107;&#20214;&#39118;&#38505;&#65288;&#24515;&#26775;&#25110;&#20013;&#39118;&#65289;&#21644;&#38750;CVD&#30149;&#20363;&#30340;&#24739;&#32773;&#12290;&#25105;&#20204;&#30340;&#39044;&#27979;&#27169;&#22411;&#22522;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35757;&#32451;&#65292;&#35780;&#20272;&#20854;&#33021;&#21147;&#26469;&#27491;&#30830;&#35782;&#21035;&#22312;&#22270;&#20687;&#33719;&#21462;&#21518;&#30340;5&#24180;&#20869;&#21487;&#33021;&#24739;&#26377;CVD&#20107;&#20214;&#65288;&#24515;&#26775;&#25110;&#20013;&#39118;&#65289;&#30340;&#20010;&#20307;&#12290;&#25105;&#20204;&#30340;&#33258;&#30417;&#30563;VAE&#29305;&#24449;&#36873;&#25321;&#21644;&#22810;&#27169;&#24577;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#21306;&#20998;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18873v1 Announce Type: cross  Abstract: We investigated the potential of optical coherence tomography (OCT) as an additional imaging technique to predict future cardiovascular disease (CVD). We utilised a self-supervised deep learning approach based on Variational Autoencoders (VAE) to learn low-dimensional representations of high-dimensional 3D OCT images and to capture distinct characteristics of different retinal layers within the OCT image. A Random Forest (RF) classifier was subsequently trained using the learned latent features and participant demographic and clinical data, to differentiate between patients at risk of CVD events (MI or stroke) and non-CVD cases. Our predictive model, trained on multimodal data, was assessed based on its ability to correctly identify individuals likely to suffer from a CVD event(MI or stroke), within a 5-year interval after image acquisition. Our self-supervised VAE feature selection and multimodal Random Forest classifier differentiate
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20061;&#31181;&#38598;&#25104;&#23398;&#20064;&#22120;&#24182;&#21033;&#29992;&#26032;&#39062;&#29305;&#24449;&#24037;&#31243;&#31574;&#30053;&#65292;&#32467;&#21512;&#22810;&#31181;&#20998;&#20301;&#25968;&#22238;&#24402;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#31354;&#38388;&#25554;&#20540;&#20013;&#38598;&#25104;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;</title><link>https://arxiv.org/abs/2403.10567</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#20013;&#30340;&#21355;&#26143;&#38477;&#27700;&#31354;&#38388;&#25554;&#20540;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation in spatial interpolation of satellite precipitation with ensemble learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10567
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20061;&#31181;&#38598;&#25104;&#23398;&#20064;&#22120;&#24182;&#21033;&#29992;&#26032;&#39062;&#29305;&#24449;&#24037;&#31243;&#31574;&#30053;&#65292;&#32467;&#21512;&#22810;&#31181;&#20998;&#20301;&#25968;&#22238;&#24402;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#31354;&#38388;&#25554;&#20540;&#20013;&#38598;&#25104;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10567v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#27010;&#29575;&#20998;&#24067;&#24418;&#24335;&#30340;&#39044;&#27979;&#23545;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20998;&#20301;&#25968;&#22238;&#24402;&#22312;&#31354;&#38388;&#25554;&#20540;&#35774;&#32622;&#20013;&#33021;&#22815;&#21512;&#24182;&#36965;&#24863;&#21644;&#38632;&#37327;&#25968;&#25454;&#65292;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#65292;&#20998;&#20301;&#25968;&#22238;&#24402;&#31639;&#27861;&#30340;&#38598;&#25104;&#23398;&#20064;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20061;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#38598;&#25104;&#23398;&#20064;&#22120;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#38477;&#27700;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#24037;&#31243;&#31574;&#30053;&#65292;&#23558;&#39044;&#27979;&#22240;&#23376;&#20943;&#23569;&#20026;&#30456;&#20851;&#20301;&#32622;&#30340;&#21152;&#26435;&#36317;&#31163;&#21355;&#26143;&#38477;&#27700;&#65292;&#32467;&#21512;&#20301;&#32622;&#39640;&#31243;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#23398;&#20064;&#22120;&#21253;&#25324;&#20845;&#31181;&#22534;&#21472;&#26041;&#27861;&#21644;&#19977;&#31181;&#31616;&#21333;&#26041;&#27861;&#65288;&#22343;&#20540;&#12289;&#20013;&#20301;&#25968;&#12289;&#26368;&#20339;&#32452;&#21512;&#22120;&#65289;&#65292;&#32467;&#21512;&#20102;&#20845;&#31181;&#20010;&#20307;&#31639;&#27861;&#65306;&#20998;&#20301;&#25968;&#22238;&#24402;(QR)&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;(QRF)&#12289;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;(GRF)&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;(GBM)&#12289;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;(LightGBM)&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10567v1 Announce Type: new  Abstract: Predictions in the form of probability distributions are crucial for decision-making. Quantile regression enables this within spatial interpolation settings for merging remote sensing and gauge precipitation data. However, ensemble learning of quantile regression algorithms remains unexplored in this context. Here, we address this gap by introducing nine quantile-based ensemble learners and applying them to large precipitation datasets. We employed a novel feature engineering strategy, reducing predictors to distance-weighted satellite precipitation at relevant locations, combined with location elevation. Our ensemble learners include six stacking and three simple methods (mean, median, best combiner), combining six individual algorithms: quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.10089</link><description>&lt;p&gt;
&#29992;&#20110;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Approximation and bounding techniques for the Fisher-Rao distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#38381;&#21512;&#24418;&#24335;1D&#23376;&#27169;&#22411;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#20197;&#21450;&#21462;&#20915;&#20110;&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#30340;&#20960;&#31181;&#36890;&#29992;&#36817;&#20284;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#20445;&#35777;&#36817;&#20284;&#35823;&#24046;&#20219;&#24847;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#27169;&#22411;&#30340;&#20004;&#20010;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;Fisher-Rao&#36317;&#31163;&#34987;&#23450;&#20041;&#20026;Fisher&#20449;&#24687;&#24230;&#37327;&#35825;&#23548;&#30340;Riemannian&#27979;&#22320;&#36317;&#31163;&#12290;&#20026;&#20102;&#20197;&#38381;&#21512;&#24418;&#24335;&#35745;&#31639;Fisher-Rao&#36317;&#31163;&#65292;&#25105;&#20204;&#38656;&#35201;&#65288;1&#65289;&#25512;&#23548;&#20986;Fisher-Rao&#27979;&#22320;&#32447;&#30340;&#20844;&#24335;&#65292;&#20197;&#21450;&#65288;2&#65289;&#27839;&#30528;&#36825;&#20123;&#27979;&#22320;&#32447;&#31215;&#20998;Fisher&#38271;&#24230;&#20803;&#32032;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#25968;&#20540;&#19978;&#31283;&#20581;&#30340;Fisher-Rao&#36317;&#31163;&#30340;&#36817;&#20284;&#21644;&#30028;&#23450;&#25216;&#26415;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#22522;&#20110;&#23376;&#27169;&#22411;&#30340;&#38381;&#21512;&#24418;&#24335;1D Fisher-Rao&#36317;&#31163;&#25253;&#21578;&#20102;Fisher-Rao&#36317;&#31163;&#30340;&#36890;&#29992;&#19978;&#30028;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20960;&#31181;&#36890;&#29992;&#30340;&#36817;&#20284;&#26041;&#26696;&#65292;&#21462;&#20915;&#20110;Fisher-Rao&#27979;&#22320;&#32447;&#25110;&#39044;&#27979;&#27979;&#22320;&#32447;&#26159;&#21542;&#33021;&#20197;&#38381;&#21512;&#24418;&#24335;&#33719;&#24471;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#25552;&#20379;Fisher-Rao&#39044;&#27979;&#27979;&#22320;&#32447;&#21644;&#20005;&#26684;&#30340;&#19979;&#30028;&#21644;&#19978;&#30028;&#26102;&#36817;&#20284;&#20135;&#29983;&#20219;&#24847;&#23567;&#30340;&#38468;&#21152;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10089v1 Announce Type: cross  Abstract: The Fisher-Rao distance between two probability distributions of a statistical model is defined as the Riemannian geodesic distance induced by the Fisher information metric. In order to calculate the Fisher-Rao distance in closed-form, we need (1) to elicit a formula for the Fisher-Rao geodesics, and (2) to integrate the Fisher length element along those geodesics. We consider several numerically robust approximation and bounding techniques for the Fisher-Rao distances: First, we report generic upper bounds on Fisher-Rao distances based on closed-form 1D Fisher-Rao distances of submodels. Second, we describe several generic approximation schemes depending on whether the Fisher-Rao geodesics or pregeodesics are available in closed-form or not. In particular, we obtain a generic method to guarantee an arbitrarily small additive error on the approximation provided that Fisher-Rao pregeodesics and tight lower and upper bounds are available
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.05300</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#32479;&#19968;&#22810;&#26679;&#24615;&#65306;&#25913;&#36827;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unity by Diversity: Improved Representation Learning in Multimodal VAEs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05300
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36719;&#32422;&#26463;&#21462;&#20195;&#30828;&#32422;&#26463;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#25913;&#21892;&#20102;&#22810;&#27169;&#24577;VAEs&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#22312;&#25968;&#25454;&#20998;&#26512;&#30340;&#35768;&#22810;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#22914;&#34920;&#31034;&#23398;&#20064;&#12289;&#26377;&#26465;&#20214;&#29983;&#25104;&#21644;&#22635;&#34917;&#12290;&#30446;&#21069;&#30340;&#26550;&#26500;&#35201;&#20040;&#36328;&#27169;&#24577;&#20849;&#20139;&#32534;&#30721;&#22120;&#36755;&#20986;&#12289;&#35299;&#30721;&#22120;&#36755;&#20837;&#65292;&#35201;&#20040;&#20004;&#32773;&#37117;&#35201;&#23398;&#20064;&#20849;&#20139;&#34920;&#31034;&#12290;&#36825;&#26679;&#30340;&#26550;&#26500;&#23545;&#27169;&#22411;&#26045;&#21152;&#20102;&#20005;&#26684;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#29992;&#36719;&#32422;&#26463;&#21462;&#20195;&#36825;&#20123;&#30828;&#32422;&#26463;&#21487;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19987;&#23478;&#28151;&#21512;&#20808;&#39564;&#65292;&#36719;&#24615;&#22320;&#24341;&#23548;&#27599;&#20010;&#27169;&#24577;&#30340;&#28508;&#22312;&#34920;&#31034;&#26397;&#30528;&#20849;&#20139;&#30340;&#21518;&#39564;&#12290;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20248;&#31168;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#20801;&#35768;&#27599;&#20010;&#32534;&#30721;&#20445;&#30041;&#26469;&#33258;&#20854;&#26410;&#21387;&#32553;&#21407;&#22987;&#29305;&#24449;&#26356;&#22909;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29616;&#23454;&#19990;&#30028;&#31070;&#32463;&#31185;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#21644;&#22635;&#34917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05300v1 Announce Type: cross  Abstract: Variational Autoencoders for multimodal data hold promise for many tasks in data analysis, such as representation learning, conditional generation, and imputation. Current architectures either share the encoder output, decoder input, or both across modalities to learn a shared representation. Such architectures impose hard constraints on the model. In this work, we show that a better latent representation can be obtained by replacing these hard constraints with a soft constraint. We propose a new mixture-of-experts prior, softly guiding each modality's latent representation towards a shared aggregate posterior. This approach results in a superior latent representation and allows each encoding to preserve information from its uncompressed original features better. In extensive experiments on multiple benchmark datasets and a challenging real-world neuroscience data set, we show improved learned latent representations and imputation of m
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14746</link><description>&lt;p&gt;
&#25193;&#23637;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14746
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#21363;&#22823;&#37096;&#20998;&#21442;&#25968;&#20026;&#38646;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21363;&#37027;&#20123;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#35757;&#32451;&#25439;&#22833;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20272;&#35745;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#20013;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#19978;&#19979;&#30028;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#65306;(1)&#35201;&#22312;&#35757;&#32451;&#35821;&#26009;&#20013;&#34920;&#31034;&#30340;&#25216;&#33021;&#25968;&#37327;&#32763;&#20493;&#65292;&#38656;&#35201;&#23558;&#35821;&#26009;&#35268;&#27169;&#22823;&#32422;&#25193;&#23637;&#19977;&#21040;&#20116;&#20493;&#65292;(2)&#23545;&#20110;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;$N$&#21644;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;$D$&#28385;&#36275;$N \sim D^{0.58}$&#30340;&#20851;&#31995;&#65292;(3)&#22914;&#26524;&#19968;&#20010;LLM&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#65292;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#20986;&#26032;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
&lt;/p&gt;</description></item><item><title>FIDLAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#26368;&#20248;&#27946;&#27700;&#31649;&#29702;&#65292;&#20934;&#30830;&#36827;&#34892;&#27700;&#21069;&#37322;&#25918;&#12290;</title><link>https://arxiv.org/abs/2402.13371</link><description>&lt;p&gt;
FIDLAR: &#27700;&#28798;&#20943;&#28798;&#30340;&#39044;&#27979;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
FIDLAR: Forecast-Informed Deep Learning Architecture for Flood Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13371
&lt;/p&gt;
&lt;p&gt;
FIDLAR&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#27979;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21644;&#26368;&#20248;&#27946;&#27700;&#31649;&#29702;&#65292;&#20934;&#30830;&#36827;&#34892;&#27700;&#21069;&#37322;&#25918;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27839;&#28023;&#27827;&#27969;&#31995;&#32479;&#20013;&#65292;&#39057;&#32321;&#21457;&#29983;&#30340;&#27946;&#27700;&#24448;&#24448;&#22312;&#22823;&#39118;&#26292;&#25110;&#28385;&#28526;&#26102;&#21457;&#29983;&#65292;&#23545;&#29983;&#21629;&#21644;&#36130;&#20135;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#22312;&#26530;&#32445;&#32467;&#26500;(&#22914;&#27700;&#22365;&#12289;&#38392;&#38376;&#12289;&#27893;&#31449;&#21644;&#27700;&#24211;)&#22312;&#26497;&#31471;&#22825;&#27668;&#20107;&#20214;&#21069;&#36827;&#34892;&#25112;&#30053;&#24615;&#22320;&#37322;&#25918;&#27700;&#65292;&#36825;&#20123;&#27946;&#27700;&#21487;&#20197;&#24471;&#21040;&#20943;&#36731;&#29978;&#33267;&#39044;&#38450;&#12290;&#24403;&#22320;&#27700;&#21033;&#31649;&#29702;&#26426;&#26500;&#36890;&#24120;&#20351;&#29992;&#30340;&#26631;&#20934;&#26041;&#27861;&#26159;&#8220;&#22522;&#20110;&#35268;&#21017;&#8221;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21382;&#21490;&#21644;&#32463;&#36807;&#26102;&#38388;&#39564;&#35777;&#30340;&#20154;&#31867;&#32463;&#39564;&#25351;&#23450;&#39044;&#20808;&#37322;&#25918;&#27700;&#65292;&#20294;&#24448;&#24448;&#23548;&#33268;&#36807;&#37327;&#25110;&#19981;&#36275;&#30340;&#27700;&#37327;&#37322;&#25918;&#12290;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;(MPC)&#26159;&#19968;&#20010;&#26367;&#20195;&#26041;&#27861;&#65292;&#26159;&#22522;&#20110;&#29289;&#29702;&#30340;&#39044;&#27979;&#27169;&#22411;&#65292;&#23613;&#31649;&#38656;&#35201;&#36827;&#34892;&#35745;&#31639;&#23494;&#38598;&#22411;&#35745;&#31639;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FIDLAR&#30340;&#39044;&#27979;&#23548;&#21521;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#27700;&#21069;&#37322;&#25918;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#26368;&#20248;&#30340;&#27946;&#27700;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13371v1 Announce Type: new  Abstract: In coastal river systems, frequent floods, often occurring during major storms or king tides, pose a severe threat to lives and property. However, these floods can be mitigated or even prevented by strategically releasing water before extreme weather events with hydraulic structures such as dams, gates, pumps, and reservoirs. A standard approach used by local water management agencies is the "rule-based" method, which specifies predetermined pre-releases of water based on historical and time-tested human experience, but which tends to result in excess or inadequate water release. The model predictive control (MPC), a physics-based model for prediction, is an alternative approach, albeit involving computationally intensive calculations. In this paper, we propose a Forecast Informed Deep Learning Architecture, FIDLAR, to achieve rapid and optimal flood management with precise water pre-releases. FIDLAR seamlessly integrates two neural netw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.11140</link><description>&lt;p&gt;
&#24605;&#32500;&#30340;&#25552;&#21319;&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#35797;&#38169;&#38382;&#39064;&#35299;&#20915;
&lt;/p&gt;
&lt;p&gt;
Boosting of Thoughts: Trial-and-Error Problem Solving with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11140
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#22810;&#20010;&#24605;&#32500;&#26641;&#65292;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#30340;&#25512;&#29702;&#24615;&#33021;&#20851;&#38190;&#21462;&#20915;&#20110;&#24605;&#32500;&#38142;&#25552;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#22312;&#25552;&#31034;&#20013;&#25552;&#20379;&#19968;&#20123;&#24605;&#32500;&#38142;&#31034;&#33539;&#20316;&#20026;&#31034;&#20363;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#65288;&#20363;&#22914;Thought Tree&#65289;&#25351;&#20986;&#20102;&#22312;&#22797;&#26434;&#38382;&#39064;&#35299;&#20915;&#30340;&#25512;&#29702;&#27493;&#39588;&#36873;&#25321;&#20013;&#65292;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Boosting of Thoughts&#65288;BoT&#65289;&#30340;&#33258;&#21160;&#25552;&#31034;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#33258;&#25105;&#35780;&#20272;&#35768;&#22810;&#24605;&#32500;&#26641;&#26469;&#33719;&#24471;&#19968;&#31995;&#21015;&#35797;&#38169;&#25512;&#29702;&#32463;&#39564;&#65292;&#36825;&#23558;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#30340;&#25552;&#31034;&#12290;BoT&#20174;&#19968;&#20010;&#31616;&#21333;&#25552;&#31034;&#24320;&#22987;&#65292;&#26080;&#38656;&#31034;&#20363;&#65292;&#36845;&#20195;&#22320;&#25506;&#32034;&#21644;&#35780;&#20272;&#22823;&#37327;&#30340;&#25512;&#29702;&#27493;&#39588;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#21033;&#29992;LLM&#33719;&#24471;&#30340;&#38169;&#35823;&#20998;&#26512;&#26469;&#26126;&#30830;&#20462;&#25913;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11140v1 Announce Type: new  Abstract: The reasoning performance of Large Language Models (LLMs) on a wide range of problems critically relies on chain-of-thought prompting, which involves providing a few chain of thought demonstrations as exemplars in prompts. Recent work, e.g., Tree of Thoughts, has pointed out the importance of exploration and self-evaluation in reasoning step selection for complex problem solving. In this paper, we present Boosting of Thoughts (BoT), an automated prompting framework for problem solving with LLMs by iteratively exploring and self-evaluating many trees of thoughts in order to acquire an ensemble of trial-and-error reasoning experiences, which will serve as a new form of prompting to solve the complex problem. Starting from a simple prompt without requiring examples, BoT iteratively explores and evaluates a large collection of reasoning steps, and more importantly, uses error analysis obtained from the LLM on them to explicitly revise prompt
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POTNet&#30340;&#29983;&#25104;&#24314;&#27169;&#32593;&#32476;&#65292;&#22522;&#20110;&#36793;&#32536;&#24809;&#32602;&#30340;Wasserstein&#25439;&#22833;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21516;&#26102;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#29305;&#24449;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2402.10456</link><description>&lt;p&gt;
&#36890;&#36807;&#24809;&#32602;&#26368;&#20248;&#36755;&#36816;&#32593;&#32476;&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Generative Modeling for Tabular Data via Penalized Optimal Transport Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10456
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;POTNet&#30340;&#29983;&#25104;&#24314;&#27169;&#32593;&#32476;&#65292;&#22522;&#20110;&#36793;&#32536;&#24809;&#32602;&#30340;Wasserstein&#25439;&#22833;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21516;&#26102;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#29305;&#24449;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#23398;&#20064;&#34920;&#26684;&#25968;&#25454;&#20013;&#34892;&#30340;&#27010;&#29575;&#20998;&#24067;&#24182;&#29983;&#25104;&#30495;&#23454;&#30340;&#21512;&#25104;&#26679;&#26412;&#30340;&#20219;&#21153;&#26082;&#20851;&#38190;&#21448;&#38750;&#24179;&#20961;&#12290;Wasserstein&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(WGAN)&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#35299;&#20915;&#20102;&#20854;&#21069;&#36523;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#20013;&#23384;&#22312;&#28151;&#21512;&#25968;&#25454;&#31867;&#22411;&#21644;&#22810;&#27169;&#24577;&#24615;&#65292;&#29983;&#25104;&#22120;&#21644;&#37492;&#21035;&#22120;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#20197;&#21450;Wasserstein&#36317;&#31163;&#22312;&#39640;&#32500;&#24230;&#20013;&#30340;&#22266;&#26377;&#19981;&#31283;&#23450;&#24615;&#65292;WGAN&#36890;&#24120;&#26080;&#27861;&#29983;&#25104;&#39640;&#20445;&#30495;&#26679;&#26412;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;POTNet&#65288;&#24809;&#32602;&#26368;&#20248;&#36755;&#36816;&#32593;&#32476;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#12289;&#24378;&#22823;&#19988;&#21487;&#35299;&#37322;&#30340;&#36793;&#38469;&#24809;&#32602;Wasserstein&#65288;MPW&#65289;&#25439;&#22833;&#30340;&#29983;&#25104;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;POTNet&#33021;&#22815;&#26377;&#25928;&#22320;&#24314;&#27169;&#21253;&#21547;&#20998;&#31867;&#21644;&#36830;&#32493;&#29305;&#24449;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10456v1 Announce Type: cross  Abstract: The task of precisely learning the probability distribution of rows within tabular data and producing authentic synthetic samples is both crucial and non-trivial. Wasserstein generative adversarial network (WGAN) marks a notable improvement in generative modeling, addressing the challenges faced by its predecessor, generative adversarial network. However, due to the mixed data types and multimodalities prevalent in tabular data, the delicate equilibrium between the generator and discriminator, as well as the inherent instability of Wasserstein distance in high dimensions, WGAN often fails to produce high-fidelity samples. To this end, we propose POTNet (Penalized Optimal Transport Network), a generative deep neural network based on a novel, robust, and interpretable marginally-penalized Wasserstein (MPW) loss. POTNet can effectively model tabular data containing both categorical and continuous features. Moreover, it offers the flexibil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#36229;&#36807;$\mathcal{O}(\sqrt{T})$&#36951;&#25022;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;$\mathcal{O}(T^{1/3})$&#30340;&#36951;&#25022;&#12290;</title><link>https://arxiv.org/abs/2402.07108</link><description>&lt;p&gt;
&#35299;&#32806;&#23398;&#20064;&#21644;&#20915;&#31574;&#65306;&#29992;&#19968;&#38454;&#26041;&#27861;&#31361;&#30772;&#22312;&#32447;&#36164;&#28304;&#20998;&#37197;&#20013;&#30340;$\mathcal{O}(\sqrt{T})$&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Decoupling Learning and Decision-Making: Breaking the $\mathcal{O}(\sqrt{T})$ Barrier in Online Resource Allocation with First-Order Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#36229;&#36807;$\mathcal{O}(\sqrt{T})$&#36951;&#25022;&#30340;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;$\mathcal{O}(T^{1/3})$&#30340;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#22312;&#25910;&#30410;&#31649;&#29702;&#21644;&#36164;&#28304;&#20998;&#37197;&#20043;&#38388;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24320;&#21457;&#26377;&#25928;&#30340;&#19968;&#38454;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#12290;&#23613;&#31649;&#19968;&#38454;&#26041;&#27861;&#22312;&#23454;&#35777;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21482;&#33021;&#23454;&#29616;$\mathcal{O}(\sqrt{T})$&#30340;&#36951;&#25022;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#32447;&#24615;&#35268;&#21010;(LP)&#30340;&#22312;&#32447;&#31639;&#27861;&#25152;&#20445;&#35777;&#30340;$\mathcal{O}(\log T)$&#30028;&#38480;&#30456;&#27604;&#26159;&#27425;&#20248;&#30340;&#12290;&#26412;&#25991;&#30830;&#23450;&#20102;&#20851;&#20110;&#22312;&#32447;&#32447;&#24615;&#35268;&#21010;&#30340;&#20960;&#20010;&#37325;&#35201;&#20107;&#23454;&#65292;&#25581;&#31034;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#32447;&#31639;&#27861;&#23454;&#29616;&#36229;&#36807;$\mathcal{O}(\sqrt{T})$&#36951;&#25022;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#23398;&#20064;&#19982;&#20915;&#31574;&#20998;&#31163;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#39318;&#27425;&#23637;&#31034;&#20102;&#19968;&#38454;&#26041;&#27861;&#22312;&#36825;&#20010;&#26032;&#26694;&#26550;&#19979;&#21487;&#20197;&#36798;&#21040;$\mathcal{O}(T^{1/3})$&#30340;&#36951;&#25022;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online linear programming plays an important role in both revenue management and resource allocation, and recent research has focused on developing efficient first-order online learning algorithms. Despite the empirical success of first-order methods, they typically achieve a regret no better than $\mathcal{O}(\sqrt{T})$, which is suboptimal compared to the $\mathcal{O}(\log T)$ bound guaranteed by the state-of-the-art linear programming (LP)-based online algorithms. This paper establishes several important facts about online linear programming, which unveils the challenge for first-order-method-based online algorithms to achieve beyond $\mathcal{O}(\sqrt{T})$ regret. To address the challenge, we introduce a new algorithmic framework that decouples learning from decision-making. More importantly, for the first time, we show that first-order methods can attain regret $\mathcal{O}(T^{1/3})$ with this new framework. Lastly, we conduct numerical experiments to validate our theoretical find
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.00592</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Partial-Label Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;Dempster-Shafer&#29702;&#35770;&#23454;&#29616;&#23545;&#27169;&#31946;&#26631;&#35760;&#30340;&#25968;&#25454;&#30340;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36935;&#21040;&#26631;&#35760;&#27169;&#31946;&#30340;&#25968;&#25454;&#65292;&#21363;&#19981;&#21516;&#30340;&#26631;&#27880;&#32773;&#20026;&#30456;&#21516;&#26679;&#26412;&#20998;&#37197;&#20102;&#20914;&#31361;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#20801;&#35768;&#22312;&#36825;&#31181;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20998;&#31867;&#22120;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#24050;&#32463;&#20855;&#26377;&#33391;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#21463;&#21040;&#38169;&#35823;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#20855;&#26377;&#33391;&#22909;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#20102;Dempster-Shafer&#29702;&#35770;&#12290;&#23545;&#20154;&#24037;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#33391;&#22909;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20855;&#26377;&#39118;&#38505;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world applications, one often encounters ambiguously labeled data, where different annotators assign conflicting class labels. Partial-label learning allows training classifiers in this weakly supervised setting. While state-of-the-art methods already feature good predictive performance, they often suffer from miscalibrated uncertainty estimates. However, having well-calibrated uncertainty estimates is important, especially in safety-critical domains like medicine and autonomous driving. In this article, we propose a novel nearest-neighbor-based partial-label-learning algorithm that leverages Dempster-Shafer theory. Extensive experiments on artificial and real-world datasets show that the proposed method provides a well-calibrated uncertainty estimate and achieves competitive prediction performance. Additionally, we prove that our algorithm is risk-consistent.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#32852;&#21512;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2401.00546</link><description>&lt;p&gt;
AllSpark: &#19968;&#20010;&#20855;&#26377;&#21313;&#19977;&#31181;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
AllSpark: A Multimodal Spatio-Temporal General Intelligence Model with Thirteen Modalities
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00546
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#38598;&#25104;&#20102;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#65292;&#26088;&#22312;&#35299;&#20915;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#32852;&#21512;&#35299;&#37322;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#30001;&#20110;&#21508;&#31181;&#26102;&#31354;&#27169;&#24577;&#25968;&#25454;&#20043;&#38388;&#32467;&#26500;&#21644;&#35821;&#20041;&#30340;&#39640;&#24230;&#24322;&#36136;&#24615;&#65292;&#22810;&#27169;&#24577;&#26102;&#31354;&#25968;&#25454;&#30340;&#32852;&#21512;&#35299;&#37322;&#19968;&#30452;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#20957;&#32858;&#21147;&#21644;&#33258;&#27835;&#24615;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#32780;&#38543;&#30528;&#27169;&#24577;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#24179;&#34913;&#34920;&#29616;&#20986;&#36880;&#28176;&#38750;&#32447;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#35821;&#35328;&#20316;&#20026;&#21442;&#32771;&#26694;&#26550;&#65288;LaRF&#65289;&#65292;&#36825;&#26159;&#26500;&#24314;&#22810;&#27169;&#24577;&#32479;&#19968;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#21462;&#24471;&#20957;&#32858;&#21147;&#21644;&#33258;&#27835;&#24615;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;AllSpark&#30340;&#22810;&#27169;&#24577;&#26102;&#31354;&#26234;&#33021;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#21313;&#19977;&#31181;&#19981;&#21516;&#30340;&#27169;&#24577;&#38598;&#25104;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#21253;&#25324;1D&#65288;&#25991;&#26412;&#65292;&#20195;&#30721;&#65289;&#65292;2D&#65288;RGB&#65292;&#32418;&#22806;&#32447;&#65292;SAR&#65292;&#22810;&#20809;&#35889;&#65292;&#39640;&#20809;&#35889;&#65292;&#34920;&#26684;&#65292;&#22270;&#34920;&#65292;&#36712;&#36857;&#65292;&#26012;&#35282;&#25668;&#24433;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.00546v2 Announce Type: replace  Abstract: For a long time, due to the high heterogeneity in structure and semantics among various spatiotemporal modal data, the joint interpretation of multimodal spatiotemporal data has been an extremely challenging problem. The primary challenge resides in striking a trade-off between the cohesion and autonomy of diverse modalities, and this trade-off exhibits a progressively nonlinear nature as the number of modalities expands. We introduce the Language as Reference Framework (LaRF), a fundamental principle for constructing a multimodal unified model, aiming to strike a trade-off between the cohesion and autonomy among different modalities. We propose a multimodal spatiotemporal general artificial intelligence model, called AllSpark. Our model integrates thirteen different modalities into a unified framework, including 1D (text, code), 2D (RGB, infrared, SAR, multispectral, hyperspectral, tables, graphs, trajectory, oblique photography), a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPOD&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#39046;&#22495;&#25968;&#25454;&#26469;&#25913;&#21892;&#25152;&#38656;&#39046;&#22495;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#35823;&#20449;&#24687;&#26816;&#27979;&#65292;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.16496</link><description>&lt;p&gt;
DPOD&#65306;&#38754;&#21521;&#22810;&#27169;&#24577;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;
DPOD: Domain-Specific Prompt Tuning for Multimodal Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPOD&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#36328;&#39046;&#22495;&#25968;&#25454;&#26469;&#25913;&#21892;&#25152;&#38656;&#39046;&#22495;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#35823;&#20449;&#24687;&#26816;&#27979;&#65292;&#35299;&#20915;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#20551;&#26032;&#38395;&#21033;&#29992;&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#22270;&#20687;&#20256;&#25773;&#24050;&#32463;&#21464;&#24471;&#26222;&#36941;&#65292;&#26159;&#20449;&#24687;&#36807;&#36733;&#26102;&#20195;&#19968;&#20010;&#30456;&#20851;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#33073;&#31163;&#19978;&#19979;&#25991;&#30340;&#34394;&#20551;&#26032;&#38395;&#21487;&#33021;&#28041;&#21450;&#19981;&#21516;&#39046;&#22495;&#65292;&#22914;&#25919;&#27835;&#12289;&#20307;&#32946;&#12289;&#23089;&#20048;&#31561;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#19981;&#21516;&#39046;&#22495;&#26032;&#38395;&#25991;&#31456;&#23384;&#22312;&#30528;&#25968;&#25454;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#65292;&#37096;&#20998;&#39046;&#22495;&#25968;&#25454;&#20016;&#23500;&#65292;&#32780;&#20854;&#20182;&#39046;&#22495;&#25968;&#25454;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#24320;&#21457;&#20986;&#33021;&#22815;&#36866;&#24212;&#19981;&#21516;&#25968;&#25454;&#37327;&#35774;&#32622;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36328;&#39046;&#22495;&#25968;&#25454;&#26159;&#21542;&#26377;&#21161;&#20110;&#25913;&#21892;&#25152;&#38656;&#39046;&#22495;&#30340;&#33073;&#31163;&#19978;&#19979;&#25991;&#35823;&#20449;&#24687;&#26816;&#27979;&#65288;&#22312;&#27492;&#31216;&#20026;&#22810;&#27169;&#24577;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#65289;&#30340;&#26041;&#27861;&#20197;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DPOD&#65288;&#20351;&#29992;&#36328;&#39046;&#22495;&#25968;&#25454;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#25552;&#31034;&#35843;&#33410;&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16496v2 Announce Type: replace  Abstract: The spread of fake news using out-of-context images has become widespread and is a relevant problem in this era of information overload. Such out-of-context fake news may arise across different domains like politics, sports, entertainment, etc. In practical scenarios, an inherent problem of imbalance exists among news articles from such widely varying domains, resulting in a few domains with abundant data, while the rest containing very limited data. Under such circumstances, it is imperative to develop methods which can work in such varying amounts of data setting. In this work, we explore whether out-of-domain data can help to improve out-of-context misinformation detection (termed here as multi-modal fake news detection) of a desired domain, to address this challenging problem. Towards this goal, we propose a novel framework termed DPOD (Domain-specific Prompt-tuning using Out-of-Domain data). First, to compute generalizable featu
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#36951;&#24536;&#65288;FU&#65289;&#26159;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21457;&#23637;FU&#26041;&#27861;&#26102;&#38656;&#35201;&#24179;&#34913;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#31454;&#20105;&#24615;&#35201;&#27714;&#65292;&#20197;&#32500;&#25345;FL&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#21487;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.19218</link><description>&lt;p&gt;
&#32852;&#37030;&#36951;&#24536;&#30340;&#32508;&#36848;&#65306;&#20998;&#31867;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey of Federated Unlearning: A Taxonomy, Challenges and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.19218
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#36951;&#24536;&#65288;FU&#65289;&#26159;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#30340;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#21457;&#23637;FU&#26041;&#27861;&#26102;&#38656;&#35201;&#24179;&#34913;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#30340;&#31454;&#20105;&#24615;&#35201;&#27714;&#65292;&#20197;&#32500;&#25345;FL&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#21487;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38544;&#31169;&#20445;&#25252;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21457;&#23637;&#65292;&#23545;&#23454;&#29616;&#34987;&#36951;&#24536;&#26435;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22823;&#12290;&#30001;&#20110;FL&#30340;&#20998;&#25955;&#24615;&#36136;&#65292;&#23454;&#26045;&#36873;&#25321;&#24615;&#36951;&#24536;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#20652;&#29983;&#20102;&#19968;&#20010;&#26032;&#30340;&#39046;&#22495;&#65292;&#21363;&#32852;&#37030;&#36951;&#24536;&#65288;FU&#65289;&#12290;FU&#20316;&#20026;&#35299;&#20915;&#25968;&#25454;&#38544;&#31169;&#38656;&#27714;&#30340;&#25112;&#30053;&#35299;&#20915;&#26041;&#26696;&#65292;&#21253;&#25324;&#23454;&#26045;&#8220;&#34987;&#36951;&#24536;&#26435;&#8221;&#12290;&#24320;&#21457;FU&#26041;&#27861;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#12289;&#25928;&#29992;&#21644;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#22240;&#20026;&#36825;&#20123;&#22240;&#32032;&#24448;&#24448;&#20855;&#26377;&#31454;&#20105;&#24615;&#35201;&#27714;&#12290;&#22312;&#20445;&#25345;FL&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#29992;&#24615;&#30340;&#21516;&#26102;&#65292;&#23454;&#29616;&#36825;&#20123;&#26041;&#38754;&#30340;&#26368;&#20339;&#24179;&#34913;&#23545;&#20110;&#36981;&#23432;&#38544;&#31169;&#21644;&#23433;&#20840;&#26631;&#20934;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#32508;&#36848;&#23545;&#29616;&#26377;&#30340;FU&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#21253;&#25324;&#23545;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#30340;&#35814;&#32454;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evolution of privacy-preserving Federated Learning (FL) has led to an increasing demand for implementing the right to be forgotten. The implementation of selective forgetting is particularly challenging in FL due to its decentralized nature. This complexity has given rise to a new field, Federated Unlearning (FU). FU emerges as a strategic solution to address the increasing need for data privacy, including the implementation of the `right to be forgotten'. The primary challenge in developing FU approaches lies in balancing the trade-offs in privacy, security, utility, and efficiency, as these elements often have competing requirements. Achieving an optimal equilibrium among these facets is crucial for maintaining the effectiveness and usability of FL systems while adhering to privacy and security standards. This survey provides a comprehensive analysis of existing FU methods, incorporating a detailed review of the various evaluation metrics. Furthermore, we unify these diverse meth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04482</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25345;&#32493;&#23398;&#20064;&#26032;&#35789;
&lt;/p&gt;
&lt;p&gt;
Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;&#20856;&#22411;&#30340;&#38169;&#35823;&#21253;&#25324;&#32553;&#20889;&#35789;&#12289;&#21629;&#21517;&#23454;&#20307;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#19987;&#29992;&#35789;&#65292;&#36825;&#20123;&#35789;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#25968;&#25454;&#21487;&#29992;&#26469;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#35782;&#21035;&#36825;&#20123;&#35789;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#24102;&#26377;&#23545;&#24212;&#24187;&#28783;&#29255;&#30340;&#35762;&#24231;&#24405;&#38899;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;ASR&#27169;&#22411;&#26469;&#23558;&#27169;&#22411;&#20559;&#21521;&#20110;&#20174;&#24187;&#28783;&#29255;&#20013;&#35299;&#30721;&#26032;&#35789;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35762;&#24231;&#36827;&#34892;&#25512;&#29702;&#65292;&#23558;&#21253;&#21547;&#26816;&#27979;&#21040;&#30340;&#26032;&#35789;&#30340;&#35805;&#35821;&#25910;&#38598;&#21040;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#20013;&#12290;&#25509;&#30528;&#65292;&#23545;&#36825;&#20010;&#38598;&#21512;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#30697;&#38453;&#26435;&#37325;&#12290;&#25972;&#20010;&#36807;&#31243;&#23545;&#22810;&#20010;&#35762;&#24231;&#36827;&#34892;&#36845;&#20195;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#33719;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65288;&#36229;&#36807;80%&#30340;&#21484;&#22238;&#29575;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LogEI&#20316;&#20026;&#19968;&#31867;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#30340;EI&#20989;&#25968;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.20708</link><description>&lt;p&gt;
&#23545;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26399;&#26395;&#25913;&#36827;&#30340;&#24847;&#22806;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
Unexpected Improvements to Expected Improvement for Bayesian Optimization. (arXiv:2310.20708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20708
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LogEI&#20316;&#20026;&#19968;&#31867;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#20855;&#26377;&#19982;&#20256;&#32479;&#30340;EI&#20989;&#25968;&#30456;&#21516;&#25110;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26399;&#26395;&#25913;&#36827;&#65288;EI&#65289;&#21487;&#20197;&#35828;&#26159;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26368;&#27969;&#34892;&#30340;&#33719;&#24471;&#20989;&#25968;&#65292;&#24182;&#19988;&#24050;&#32463;&#22312;&#24456;&#22810;&#25104;&#21151;&#30340;&#24212;&#29992;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;EI&#30340;&#24615;&#33021;&#24448;&#24448;&#34987;&#19968;&#20123;&#26032;&#26041;&#27861;&#36229;&#36234;&#12290;&#23588;&#20854;&#26159;&#65292;EI&#21450;&#20854;&#21464;&#31181;&#22312;&#24182;&#34892;&#21644;&#22810;&#30446;&#26631;&#35774;&#32622;&#20013;&#24456;&#38590;&#36827;&#34892;&#20248;&#21270;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#33719;&#24471;&#20540;&#22312;&#35768;&#22810;&#21306;&#22495;&#20013;&#25968;&#20540;&#19978;&#21464;&#20026;&#38646;&#12290;&#24403;&#35266;&#27979;&#27425;&#25968;&#22686;&#21152;&#12289;&#25628;&#32034;&#31354;&#38388;&#30340;&#32500;&#24230;&#22686;&#21152;&#25110;&#32422;&#26463;&#26465;&#20214;&#30340;&#25968;&#37327;&#22686;&#21152;&#26102;&#65292;&#36825;&#31181;&#22256;&#38590;&#36890;&#24120;&#20250;&#22686;&#21152;&#65292;&#23548;&#33268;&#24615;&#33021;&#22312;&#25991;&#29486;&#20013;&#19981;&#19968;&#33268;&#19988;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#20122;&#20248;&#21270;&#12290;&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LogEI&#65292;&#36825;&#26159;&#19968;&#31867;&#26032;&#30340;&#37319;&#26679;&#20989;&#25968;&#12290;&#19982;&#26631;&#20934;EI&#30456;&#27604;&#65292;&#36825;&#20123;LogEI&#20989;&#25968;&#30340;&#25104;&#21592;&#35201;&#20040;&#20855;&#26377;&#30456;&#21516;&#30340;&#26368;&#20248;&#35299;&#65292;&#35201;&#20040;&#20855;&#26377;&#36817;&#20284;&#30456;&#31561;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#25968;&#20540;&#19978;&#26356;&#23481;&#26131;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25968;&#20540;&#30149;&#24577;&#22312;&#8220;&#32463;&#20856;&#8221;&#20998;&#26512;EI&#12289;&#26399;&#26395;&#36229;&#20307;&#31215;&#25913;&#36827;&#65288;EHVI&#65289;&#20197;&#21450;&#23427;&#20204;&#30340;...
&lt;/p&gt;
&lt;p&gt;
Expected Improvement (EI) is arguably the most popular acquisition function in Bayesian optimization and has found countless successful applications, but its performance is often exceeded by that of more recent methods. Notably, EI and its variants, including for the parallel and multi-objective settings, are challenging to optimize because their acquisition values vanish numerically in many regions. This difficulty generally increases as the number of observations, dimensionality of the search space, or the number of constraints grow, resulting in performance that is inconsistent across the literature and most often sub-optimal. Herein, we propose LogEI, a new family of acquisition functions whose members either have identical or approximately equal optima as their canonical counterparts, but are substantially easier to optimize numerically. We demonstrate that numerical pathologies manifest themselves in "classic" analytic EI, Expected Hypervolume Improvement (EHVI), as well as their
&lt;/p&gt;</description></item><item><title>GUPNet++&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#27010;&#29575;&#26041;&#24335;&#24314;&#27169;&#20960;&#20309;&#25237;&#24433;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#28145;&#24230;&#39044;&#27979;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.15624</link><description>&lt;p&gt;
GUPNet++&#65306;&#29992;&#20110;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
GUPNet++: Geometry Uncertainty Propagation Network for Monocular 3D Object Detection. (arXiv:2310.15624v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15624
&lt;/p&gt;
&lt;p&gt;
GUPNet++&#26159;&#19968;&#31181;&#36890;&#36807;&#20197;&#27010;&#29575;&#26041;&#24335;&#24314;&#27169;&#20960;&#20309;&#25237;&#24433;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;&#65292;&#21487;&#20197;&#25552;&#39640;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#30340;&#28145;&#24230;&#39044;&#27979;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#22312;&#21333;&#30446;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23427;&#21487;&#20197;&#36890;&#36807;&#29289;&#20307;&#30340;&#29289;&#29702;&#23610;&#23544;&#19982;&#22270;&#20687;&#24179;&#38754;&#20013;&#30340;&#20108;&#32500;&#25237;&#24433;&#20043;&#38388;&#30340;&#36879;&#35270;&#25237;&#24433;&#26469;&#20272;&#35745;&#29289;&#20307;&#30340;&#28145;&#24230;&#65292;&#36825;&#21487;&#20197;&#23558;&#25968;&#23398;&#20808;&#39564;&#24341;&#20837;&#28145;&#24230;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25237;&#24433;&#36807;&#31243;&#20063;&#20250;&#24341;&#20837;&#35823;&#24046;&#25918;&#22823;&#65292;&#20272;&#35745;&#39640;&#24230;&#30340;&#35823;&#24046;&#20250;&#34987;&#25918;&#22823;&#24182;&#21453;&#26144;&#21040;&#25237;&#24433;&#30340;&#28145;&#24230;&#20013;&#12290;&#36825;&#23548;&#33268;&#28145;&#24230;&#25512;&#26029;&#19981;&#21487;&#38752;&#65292;&#24182;&#19988;&#24433;&#21709;&#35757;&#32451;&#30340;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#32593;&#32476;(GUPNet++)&#65292;&#36890;&#36807;&#20197;&#27010;&#29575;&#26041;&#24335;&#24314;&#27169;&#20960;&#20309;&#25237;&#24433;&#12290;&#36825;&#30830;&#20445;&#20102;&#28145;&#24230;&#39044;&#27979;&#26159;&#26377;&#30028;&#30340;&#65292;&#24182;&#19982;&#21512;&#29702;&#30340;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#32852;&#12290;&#24341;&#20837;&#36825;&#31181;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#30340;&#24847;&#20041;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;(1)&#12290;&#23427;&#27169;&#25311;&#20102;&#20960;&#20309;&#25237;&#24433;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#20851;&#31995;&#65292;&#25552;&#39640;&#20102;&#31471;&#21040;&#31471;&#27169;&#22411;&#23398;&#20064;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Geometry plays a significant role in monocular 3D object detection. It can be used to estimate object depth by using the perspective projection between object's physical size and 2D projection in the image plane, which can introduce mathematical priors into deep models. However, this projection process also introduces error amplification, where the error of the estimated height is amplified and reflected into the projected depth. It leads to unreliable depth inferences and also impairs training stability. To tackle this problem, we propose a novel Geometry Uncertainty Propagation Network (GUPNet++) by modeling geometry projection in a probabilistic manner. This ensures depth predictions are well-bounded and associated with a reasonable uncertainty. The significance of introducing such geometric uncertainty is two-fold: (1). It models the uncertainty propagation relationship of the geometry projection during training, improving the stability and efficiency of the end-to-end model learni
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#30340;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.04195</link><description>&lt;p&gt;
&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#32531;&#35299;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Mitigating Architecture Overfitting in Dataset Distillation. (arXiv:2309.04195v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#25968;&#25454;&#38598;&#33976;&#39311;&#20013;&#30340;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#26469;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#26041;&#27861;&#22312;&#20351;&#29992;&#26497;&#23569;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#26159;&#26550;&#26500;&#36807;&#24230;&#25311;&#21512;&#65306;&#30001;&#29305;&#23450;&#32593;&#32476;&#26550;&#26500;&#65288;&#21363;&#35757;&#32451;&#32593;&#32476;&#65289;&#21512;&#25104;&#30340;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#22312;&#20854;&#20182;&#32593;&#32476;&#26550;&#26500;&#65288;&#21363;&#27979;&#35797;&#32593;&#32476;&#65289;&#35757;&#32451;&#26102;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26550;&#26500;&#35774;&#35745;&#21644;&#35757;&#32451;&#26041;&#26696;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20849;&#21516;&#25552;&#39640;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#22312;&#33976;&#39311;&#35757;&#32451;&#25968;&#25454;&#19978;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#33976;&#39311;&#25968;&#25454;&#28041;&#21450;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#23481;&#37327;&#26356;&#22823;&#30340;&#32593;&#32476;&#23545;&#33976;&#39311;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26102;&#23454;&#29616;&#20102;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation methods have demonstrated remarkable performance for neural networks trained with very limited training data. However, a significant challenge arises in the form of architecture overfitting: the distilled training data synthesized by a specific network architecture (i.e., training network) generates poor performance when trained by other network architectures (i.e., test networks). This paper addresses this issue and proposes a series of approaches in both architecture designs and training schemes which can be adopted together to boost the generalization performance across different network architectures on the distilled training data. We conduct extensive experiments to demonstrate the effectiveness and generality of our methods. Particularly, across various scenarios involving different sizes of distilled data, our approaches achieve comparable or superior performance to existing methods when training on the distilled data using networks with larger capacities.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05185</link><description>&lt;p&gt;
&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Penalty-based Bilevel Gradient Descent Method. (arXiv:2302.05185v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a penalty-based bilevel gradient descent algorithm to solve the constrained bilevel problem without lower-level strong convexity, and experiments show its efficiency.
&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#20248;&#21270;&#22312;&#36229;&#21442;&#25968;&#20248;&#21270;&#12289;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#39046;&#22495;&#26377;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#26159;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#38590;&#20197;&#35299;&#20915;&#12290;&#26368;&#36817;&#30340;&#21487;&#25193;&#23637;&#21452;&#23618;&#31639;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#19979;&#23618;&#30446;&#26631;&#20989;&#25968;&#26159;&#24378;&#20984;&#25110;&#26080;&#32422;&#26463;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24809;&#32602;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#23618;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#19968;&#23450;&#26465;&#20214;&#19979;&#65292;&#24809;&#32602;&#37325;&#26500;&#21487;&#20197;&#24674;&#22797;&#21407;&#22987;&#21452;&#23618;&#38382;&#39064;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24809;&#32602;&#30340;&#21452;&#23618;&#26799;&#24230;&#19979;&#38477;&#65288;PBGD&#65289;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#19979;&#23618;&#38750;&#24378;&#20984;&#32422;&#26463;&#21452;&#23618;&#38382;&#39064;&#19978;&#30340;&#26377;&#38480;&#26102;&#38388;&#25910;&#25947;&#24615;&#12290;&#23454;&#39564;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;PBGD&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization enjoys a wide range of applications in hyper-parameter optimization, meta-learning and reinforcement learning. However, bilevel optimization problems are difficult to solve. Recent progress on scalable bilevel algorithms mainly focuses on bilevel optimization problems where the lower-level objective is either strongly convex or unconstrained. In this work, we tackle the bilevel problem through the lens of the penalty method. We show that under certain conditions, the penalty reformulation recovers the solutions of the original bilevel problem. Further, we propose the penalty-based bilevel gradient descent (PBGD) algorithm and establish its finite-time convergence for the constrained bilevel problem without lower-level strong convexity. Experiments showcase the efficiency of the proposed PBGD algorithm.
&lt;/p&gt;</description></item></channel></rss>