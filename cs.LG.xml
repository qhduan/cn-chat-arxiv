<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#24341;&#20837;&#20102;PPA-Game&#27169;&#22411;&#26469;&#34920;&#24449;&#31867;&#20284;YouTube&#21644;TikTok&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#31454;&#20105;&#21160;&#24577;&#65292;&#20998;&#26512;&#26174;&#31034;&#32431;&#32435;&#20160;&#22343;&#34913;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#24120;&#35265;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#27599;&#20010;&#20195;&#29702;&#32773;&#30340;&#32047;&#31215;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2403.15524</link><description>&lt;p&gt;
PPA-Game&#65306;&#34920;&#24449;&#21644;&#23398;&#20064;&#22312;&#32447;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#30340;&#31454;&#20105;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15524
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;PPA-Game&#27169;&#22411;&#26469;&#34920;&#24449;&#31867;&#20284;YouTube&#21644;TikTok&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#31454;&#20105;&#21160;&#24577;&#65292;&#20998;&#26512;&#26174;&#31034;&#32431;&#32435;&#20160;&#22343;&#34913;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#24120;&#35265;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#27599;&#20010;&#20195;&#29702;&#32773;&#30340;&#32047;&#31215;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27604;&#20363;&#24615;&#25910;&#30410;&#20998;&#37197;&#28216;&#25103;&#65288;PPA-Game&#65289;&#26469;&#27169;&#25311;&#20195;&#29702;&#32773;&#22914;&#20309;&#31454;&#20105;&#21487;&#20998;&#37197;&#36164;&#28304;&#21644;&#28040;&#36153;&#32773;&#30340;&#27880;&#24847;&#21147;&#65292;&#31867;&#20284;&#20110;YouTube&#21644;TikTok&#31561;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#12290;&#26681;&#25454;&#24322;&#36136;&#26435;&#37325;&#20026;&#20195;&#29702;&#32773;&#20998;&#37197;&#25910;&#30410;&#65292;&#21453;&#26144;&#20102;&#21019;&#20316;&#32773;&#20043;&#38388;&#20869;&#23481;&#36136;&#37327;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#32431;&#32435;&#20160;&#22343;&#34913;&#65288;PNE&#65289;&#24182;&#19981;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#37117;&#26377;&#20445;&#35777;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#20250;&#35266;&#23519;&#21040;&#65292;&#20854;&#32570;&#20047;&#24773;&#20917;&#26159;&#32597;&#35265;&#30340;&#12290;&#38500;&#20102;&#20998;&#26512;&#38745;&#24577;&#25910;&#30410;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20195;&#29702;&#32773;&#20851;&#20110;&#36164;&#28304;&#25910;&#30410;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#23558;&#22810;&#29609;&#23478;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#22312;$T$&#36718;&#20013;&#20419;&#36827;&#27599;&#20010;&#20195;&#29702;&#32773;&#32047;&#31215;&#25910;&#30410;&#30340;&#26368;&#22823;&#21270;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20219;&#20309;&#20195;&#29702;&#32773;&#30340;&#36951;&#25022;&#22312;&#20219;&#20309;$\eta &gt; 0$&#19979;&#37117;&#21463;&#21040;$O(\log^{1 + \eta} T)$&#30340;&#38480;&#21046;&#12290;&#32463;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15524v1 Announce Type: cross  Abstract: We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how agents, akin to content creators on platforms like YouTube and TikTok, compete for divisible resources and consumers' attention. Payoffs are allocated to agents based on heterogeneous weights, reflecting the diversity in content quality among creators. Our analysis reveals that although a pure Nash equilibrium (PNE) is not guaranteed in every scenario, it is commonly observed, with its absence being rare in our simulations. Beyond analyzing static payoffs, we further discuss the agents' online learning about resource payoffs by integrating a multi-player multi-armed bandit framework. We propose an online algorithm facilitating each agent's maximization of cumulative payoffs over $T$ rounds. Theoretically, we establish that the regret of any agent is bounded by $O(\log^{1 + \eta} T)$ for any $\eta &gt; 0$. Empirical results further validate the effectiveness of ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2403.00841</link><description>&lt;p&gt;
&#31454;&#20105;&#28216;&#25103;&#30340;&#31163;&#32447;&#34394;&#26500;&#33258;&#25105;&#23545;&#24328;
&lt;/p&gt;
&lt;p&gt;
Offline Fictitious Self-Play for Competitive Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#65292;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22240;&#20854;&#22312;&#20197;&#21069;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#25913;&#36827;&#31574;&#30053;&#32780;&#19981;&#38656;&#35201;&#22312;&#32447;&#20132;&#20114;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#23613;&#31649;&#22312;&#21333;&#19968;&#26234;&#33021;&#20307;&#35774;&#32622;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;RL&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#31454;&#20105;&#28216;&#25103;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Off-FSP&#65292;&#36825;&#26159;&#31454;&#20105;&#28216;&#25103;&#30340;&#31532;&#19968;&#20010;&#23454;&#29992;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;RL&#31639;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#35843;&#25972;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#26435;&#37325;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#27169;&#25311;&#19982;&#21508;&#31181;&#23545;&#25163;&#30340;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00841v1 Announce Type: cross  Abstract: Offline Reinforcement Learning (RL) has received significant interest due to its ability to improve policies in previously collected datasets without online interactions. Despite its success in the single-agent setting, offline multi-agent RL remains a challenge, especially in competitive games. Firstly, unaware of the game structure, it is impossible to interact with the opponents and conduct a major learning paradigm, self-play, for competitive games. Secondly, real-world datasets cannot cover all the state and action space in the game, resulting in barriers to identifying Nash equilibrium (NE). To address these issues, this paper introduces Off-FSP, the first practical model-free offline RL algorithm for competitive games. We start by simulating interactions with various opponents by adjusting the weights of the fixed dataset with importance sampling. This technique allows us to learn best responses to different opponents and employ
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24341;&#20837;&#20048;&#35266;&#26356;&#26032;&#65292;&#20197;&#32531;&#35299;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#26469;&#37325;&#22609;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#23545;&#28508;&#22312;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#20302;&#22238;&#25253;&#20010;&#21035;&#21160;&#20316;&#30340;&#20048;&#35266;&#24577;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.01953</link><description>&lt;p&gt;
&#20048;&#35266;&#30340;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#22312;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Optimistic Multi-Agent Policy Gradient for Cooperative Tasks. (arXiv:2311.01953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#12289;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#22312;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#20013;&#24341;&#20837;&#20048;&#35266;&#26356;&#26032;&#65292;&#20197;&#32531;&#35299;&#21512;&#20316;&#20219;&#21153;&#20013;&#30340;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#26469;&#37325;&#22609;&#20248;&#21183;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20445;&#25345;&#23545;&#28508;&#22312;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#20302;&#22238;&#25253;&#20010;&#21035;&#21160;&#20316;&#30340;&#20048;&#35266;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#30001;&#20110;&#36807;&#25311;&#21512;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#27425;&#20248;&#34892;&#20026;&#65292;&#23548;&#33268;&#26234;&#33021;&#20307;&#25910;&#25947;&#21040;&#27425;&#20248;&#32852;&#21512;&#31574;&#30053;&#65292;&#20986;&#29616;&#20102;&#30456;&#23545;&#36807;&#24230;&#27010;&#25324;&#65288;RO&#65289;&#38382;&#39064;&#12290;&#26089;&#26399;&#30740;&#31350;&#34920;&#26126;&#65292;&#20048;&#35266;&#20027;&#20041;&#21487;&#20197;&#32531;&#35299;&#20351;&#29992;&#34920;&#26684;&#21270;Q&#23398;&#20064;&#26102;&#30340;RO&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22797;&#26434;&#20219;&#21153;&#26469;&#35828;&#65292;&#21033;&#29992;&#20989;&#25968;&#36924;&#36817;&#20048;&#35266;&#20027;&#20041;&#21487;&#33021;&#21152;&#21095;&#36807;&#20272;&#35745;&#65292;&#20174;&#32780;&#22833;&#36133;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26368;&#36817;&#30340;&#28145;&#24230;&#22810;&#26234;&#33021;&#20307;&#31574;&#30053;&#26799;&#24230;&#65288;MAPG&#65289;&#26041;&#27861;&#22312;&#35768;&#22810;&#22797;&#26434;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#20005;&#37325;&#30340;RO&#24773;&#20917;&#19979;&#21487;&#33021;&#22833;&#36133;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#32780;&#31616;&#21333;&#30340;&#26694;&#26550;&#65292;&#20197;&#22312;MAPG&#26041;&#27861;&#20013;&#23454;&#29616;&#20048;&#35266;&#26356;&#26032;&#24182;&#32531;&#35299;RO&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#27844;&#28431;&#21270;&#32447;&#24615;&#25972;&#27969;&#20989;&#25968;&#65292;&#20854;&#20013;&#19968;&#20010;&#36229;&#21442;&#25968;&#36873;&#25321;&#20048;&#35266;&#31243;&#24230;&#20197;&#22312;&#26356;&#26032;&#31574;&#30053;&#26102;&#37325;&#26032;&#22609;&#36896;&#20248;&#21183;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#21487;&#33021;&#30001;&#20854;&#20182;&#20195;&#29702;&#24341;&#36215;&#30340;&#22238;&#25253;&#36739;&#20302;&#30340;&#20010;&#21035;&#21160;&#20316;&#20445;&#25345;&#20048;&#35266;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
\textit{Relative overgeneralization} (RO) occurs in cooperative multi-agent learning tasks when agents converge towards a suboptimal joint policy due to overfitting to suboptimal behavior of other agents. In early work, optimism has been shown to mitigate the \textit{RO} problem when using tabular Q-learning. However, with function approximation optimism can amplify overestimation and thus fail on complex tasks. On the other hand, recent deep multi-agent policy gradient (MAPG) methods have succeeded in many complex tasks but may fail with severe \textit{RO}. We propose a general, yet simple, framework to enable optimistic updates in MAPG methods and alleviate the RO problem. Specifically, we employ a \textit{Leaky ReLU} function where a single hyperparameter selects the degree of optimism to reshape the advantages when updating the policy. Intuitively, our method remains optimistic toward individual actions with lower returns which are potentially caused by other agents' sub-optimal be
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#65292;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.14782</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-shot Task Preference Addressing Enabled by Imprecise Bayesian Continual Learning. (arXiv:2305.14782v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14782
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38646;&#26679;&#26412;&#20219;&#21153;&#20559;&#22909;&#30340;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#30340;&#30693;&#35782;&#24211;&#65292;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;&#20110;&#36890;&#29992;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#32487;&#32493;&#23398;&#20064;&#20063;&#20855;&#26377;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#29305;&#24615;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#21516;&#20219;&#21153;&#30340;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#20026;&#20102;&#20248;&#21270;&#24403;&#21069;&#20219;&#21153;&#20998;&#24067;&#65292;&#21487;&#33021;&#38656;&#35201;&#22312;&#19968;&#20123;&#20219;&#21153;&#19978;&#29306;&#29298;&#24615;&#33021;&#20197;&#25552;&#39640;&#20854;&#20182;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#36825;&#24847;&#21619;&#30528;&#23384;&#22312;&#22810;&#20010;&#27169;&#22411;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#19981;&#21516;&#30340;&#26102;&#38388;&#37117;&#26159;&#26368;&#20248;&#30340;&#65292;&#27599;&#20010;&#27169;&#22411;&#37117;&#33021;&#22815;&#35299;&#20915;&#19981;&#21516;&#30340;&#20219;&#21153;-&#24615;&#33021;&#26435;&#34913;&#38382;&#39064;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#35752;&#35770;&#22914;&#20309;&#35757;&#32451;&#29305;&#23450;&#30340;&#27169;&#22411;&#20197;&#28385;&#36275;&#20132;&#26131;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31639;&#27861;&#38656;&#35201;&#39069;&#22806;&#30340;&#37319;&#26679;&#24320;&#38144;-&#22312;&#23384;&#22312;&#22810;&#20010;&#65292;&#21487;&#33021;&#26159;&#26080;&#38480;&#25968;&#37327;&#30340;&#20559;&#22909;&#26102;&#20250;&#20135;&#29983;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19981;&#31934;&#30830;&#36125;&#21494;&#26031;&#32487;&#32493;&#23398;&#20064;&#65288;IBCL&#65289;&#12290;&#19968;&#26086;&#26377;&#26032;&#20219;&#21153;&#65292;IBCL&#20250;&#65288;1&#65289;&#26356;&#26032;&#19968;&#20010;&#20197;&#27169;&#22411;&#21442;&#25968;&#20998;&#24067;&#20984;&#22771;&#24418;&#24335;&#23384;&#22312;&#30340;&#30693;&#35782;&#24211;&#65292;&#65288;2&#65289;&#24182;&#20351;&#29992;&#38646;&#26679;&#26412;&#33719;&#21462;&#29305;&#23450;&#27169;&#22411;&#20197;&#28385;&#36275;&#19981;&#21516;&#30340;&#20559;&#22909;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;IBCL&#19981;&#38656;&#35201;&#20219;&#20309;&#39069;&#22806;&#30340;&#25968;&#25454;&#23601;&#33021;&#20026;&#19968;&#20010;&#29305;&#23450;&#30340;&#20219;&#21153;&#20559;&#22909;&#29983;&#25104;&#26032;&#30340;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;&#20855;&#26377;&#22823;&#37327;&#20219;&#21153;&#20559;&#22909;&#30340;&#24773;&#20917;&#19979;&#26356;&#21152;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Like generic multi-task learning, continual learning has the nature of multi-objective optimization, and therefore faces a trade-off between the performance of different tasks. That is, to optimize for the current task distribution, it may need to compromise performance on some tasks to improve on others. This means there exist multiple models that are each optimal at different times, each addressing a distinct task-performance trade-off. Researchers have discussed how to train particular models to address specific preferences on these trade-offs. However, existing algorithms require additional sample overheads -- a large burden when there are multiple, possibly infinitely many, preferences. As a response, we propose Imprecise Bayesian Continual Learning (IBCL). Upon a new task, IBCL (1) updates a knowledge base in the form of a convex hull of model parameter distributions and (2) obtains particular models to address preferences with zero-shot. That is, IBCL does not require any additi
&lt;/p&gt;</description></item><item><title>HyFL&#26159;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#21644;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20445;&#35777;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#38544;&#31169;&#23433;&#20840;&#65292;&#26377;&#21161;&#20110;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;</title><link>http://arxiv.org/abs/2302.09904</link><description>&lt;p&gt;
HyFL:&#19968;&#31181;&#29992;&#20110;&#31169;&#26377;&#32852;&#21512;&#23398;&#20064;&#30340;&#28151;&#21512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
HyFL: A Hybrid Framework For Private Federated Learning. (arXiv:2302.09904v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09904
&lt;/p&gt;
&lt;p&gt;
HyFL&#26159;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#23427;&#32467;&#21512;&#20102;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#21644;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#20445;&#35777;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#38544;&#31169;&#23433;&#20840;&#65292;&#26377;&#21161;&#20110;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#20445;&#30041;&#35757;&#32451;&#25968;&#25454;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;FL&#20013;&#30340;&#28431;&#27934;&#65292;&#21253;&#25324;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#26356;&#26032;&#29978;&#33267;&#25972;&#20010;&#20840;&#23616;&#27169;&#22411;&#27844;&#28431;&#25935;&#24863;&#20449;&#24687;&#12290;&#34429;&#28982;&#20851;&#27880;&#28857;&#24050;&#25918;&#22312;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#19978;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#35299;&#20915;&#20102;&#20840;&#23616;&#27169;&#22411;&#38544;&#31169;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#23458;&#25143;&#31471;&#26412;&#22320;&#35757;&#32451;&#20026;&#24694;&#24847;&#23458;&#25143;&#31471;&#21551;&#21160;&#24378;&#22823;&#30340;&#27169;&#22411;&#27745;&#26579;&#25915;&#20987;&#24320;&#36767;&#20102;&#36884;&#24452;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#24037;&#20316;&#25552;&#20379;&#20840;&#38754;&#35299;&#20915;&#25152;&#26377;&#36825;&#20123;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HyFL&#65292;&#36825;&#26159;&#19968;&#31181;&#28151;&#21512;&#26694;&#26550;&#65292;&#21487;&#23454;&#29616;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#38544;&#31169;&#65292;&#24182;&#20419;&#36827;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;HyFL&#30340;&#22522;&#30784;&#26159;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#25216;&#26415;&#21644;&#20998;&#23618;&#32852;&#21512;&#23398;&#20064;&#30340;&#29420;&#29305;&#32452;&#21512;&#12290;&#22312;HyFL&#30340;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#22312;&#22810;&#20010;&#25277;&#35937;&#23618;&#27425;&#19978;&#36827;&#34892;&#23433;&#20840;&#32858;&#21512;&#65292;&#20197;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#25552;&#20379;&#38544;&#31169;&#20445;&#25252;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HyFL&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#33391;&#22909;&#24615;&#33021;&#65292;&#21516;&#26102;&#30830;&#20445;&#23458;&#25143;&#31471;&#25968;&#25454;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24378;&#22823;&#38544;&#31169;&#21644;&#23433;&#20840;&#20445;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) has emerged as an efficient approach for large-scale distributed machine learning, ensuring data privacy by keeping training data on client devices. However, recent research has highlighted vulnerabilities in FL, including the potential disclosure of sensitive information through individual model updates and even the aggregated global model. While much attention has been given to clients' data privacy, limited research has addressed the issue of global model privacy. Furthermore, local training at the client's side has opened avenues for malicious clients to launch powerful model poisoning attacks. Unfortunately, no existing work has provided a comprehensive solution that tackles all these issues. Therefore, we introduce HyFL, a hybrid framework that enables data and global model privacy while facilitating large-scale deployments. The foundation of HyFL is a unique combination of secure multi-party computation (MPC) techniques with hierarchical federated learnin
&lt;/p&gt;</description></item></channel></rss>