<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.05406</link><description>&lt;p&gt;
&#29616;&#22312;&#25152;&#26377;&#20154;&#37117;&#20462;&#21098;&#65306;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#36890;&#36807;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#65292;&#24182;&#19988;&#36895;&#24230;&#26159;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30340;&#20004;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#38750;&#19987;&#19994;&#20174;&#19994;&#32773;&#21644;&#26368;&#23500;&#26377;&#36164;&#28304;&#30340;&#26426;&#26500;&#20043;&#38388;&#30340;&#30828;&#20214;&#24046;&#36317;&#65292;&#23610;&#23544;&#19981;&#26029;&#22686;&#38271;&#30340;LLM&#21464;&#24471;&#36234;&#26469;&#36234;&#38590;&#20197;&#20351;&#29992;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#21387;&#32553;LLM&#65292;&#20197;&#20351;&#20854;&#36164;&#28304;&#28040;&#32791;&#21487;&#31649;&#29702;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26412;&#36523;&#24448;&#24448;&#32791;&#36153;&#36164;&#28304;&#65292;&#20351;&#20854;&#30446;&#26631;&#29992;&#25143;&#32676;&#26080;&#27861;&#25509;&#35302;&#21040;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20165;&#20351;&#29992;&#21069;&#21521;&#20256;&#36882;&#30340;LLM&#32467;&#26500;&#21270;&#20462;&#21098;&#38382;&#39064;&#12290;&#25105;&#20204;&#24076;&#26395;&#35753;&#20174;&#19994;&#32773;&#33021;&#22815;&#20462;&#21098;&#27169;&#22411;&#65292;&#20351;&#20854;&#35268;&#27169;&#22823;&#21040;&#30828;&#20214;&#20165;&#26377;&#36275;&#22815;&#30340;&#20869;&#23384;&#26469;&#36816;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;Bonsai&#65292;&#36825;&#26159;&#19968;&#31181;&#26080;&#26799;&#24230;&#12289;&#25200;&#21160;&#20462;&#21098;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#23567;&#12289;&#24555;&#21644;&#20934;&#30830;&#30340;&#20462;&#21098;&#27169;&#22411;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;Bonsai&#29983;&#25104;&#30340;&#20462;&#21098;&#27169;&#22411;&#65288;i&#65289;&#20248;&#20110;&#26356;&#26114;&#36149;&#30340;&#26799;&#24230;-based&#32467;&#26500;&#21270;&#20462;&#21098;&#26041;&#27861;&#29983;&#25104;&#30340;&#27169;&#22411;&#65292;&#24182;&#19988;&#65288;ii&#65289;&#19982;&#21322;&#32467;&#26500;&#21270;&#20462;&#21098;&#27169;&#22411;&#30456;&#27604;&#65292;&#36895;&#24230;&#24555;&#19968;&#20493;&#19988;&#20934;&#30830;&#24615;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models.   We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20869;&#26680;&#29305;&#24449;&#26144;&#23556;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#31639;&#27861;&#21487;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#65292;&#24182;&#22312;&#20960;&#20998;&#38047;&#20869;&#23436;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.04794</link><description>&lt;p&gt;
&#36890;&#36807;&#26174;&#24335;&#30340;&#20869;&#26680;&#29305;&#24449;&#26144;&#23556;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#22810;&#35270;&#35282;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Scalable Multi-view Clustering via Explicit Kernel Features Maps
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04794
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20869;&#26680;&#29305;&#24449;&#26144;&#23556;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#12290;&#35813;&#31639;&#27861;&#21487;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#24212;&#29992;&#65292;&#24182;&#22312;&#20960;&#20998;&#38047;&#20869;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35270;&#35282;&#23398;&#20064;&#20316;&#20026;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#22810;&#35270;&#35282;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#22312;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#30340;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26694;&#26550;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31574;&#30053;&#65292;&#21033;&#29992;&#20869;&#26680;&#29305;&#24449;&#26144;&#23556;&#26469;&#20943;&#36731;&#35745;&#31639;&#36127;&#25285;&#65292;&#21516;&#26102;&#20445;&#25345;&#33391;&#22909;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#31639;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#24847;&#21619;&#30528;&#23427;&#21487;&#20197;&#22312;&#26631;&#20934;&#26426;&#22120;&#19978;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20855;&#26377;&#25968;&#30334;&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20960;&#20998;&#38047;&#20869;&#23436;&#25104;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#21508;&#31181;&#35268;&#27169;&#30340;&#22522;&#20934;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#35270;&#35282;&#23376;&#31354;&#38388;&#32858;&#31867;&#26041;&#27861;&#21644;&#23646;&#24615;&#32593;&#32476;&#22810;&#35270;&#35282;&#26041;&#27861;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing awareness of multi-view learning as an important component in data science and machine learning is a consequence of the increasing prevalence of multiple views in real-world applications, especially in the context of networks. In this paper we introduce a new scalability framework for multi-view subspace clustering. An efficient optimization strategy is proposed, leveraging kernel feature maps to reduce the computational burden while maintaining good clustering performance. The scalability of the algorithm means that it can be applied to large-scale datasets, including those with millions of data points, using a standard machine, in a few minutes. We conduct extensive experiments on real-world benchmark networks of various sizes in order to evaluate the performance of our algorithm against state-of-the-art multi-view subspace clustering methods and attributed-network multi-view approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2308.11551</link><description>&lt;p&gt;
&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-event Video-Text Retrieval. (arXiv:2308.11551v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11551
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#19968;&#31181;&#29305;&#27530;&#22330;&#26223;&#65292;&#21363;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;VTR&#65289;&#26159;&#20114;&#32852;&#32593;&#19978;&#28023;&#37327;&#35270;&#39057;&#25991;&#26412;&#25968;&#25454;&#26102;&#20195;&#20013;&#19968;&#39033;&#20851;&#38190;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;&#20351;&#29992;&#21452;&#27969;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26550;&#26500;&#23398;&#20064;&#35270;&#39057;&#25991;&#26412;&#23545;&#30340;&#32852;&#21512;&#34920;&#31034;&#25104;&#20026;VTR&#20219;&#21153;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20551;&#35774;&#35270;&#39057;&#25991;&#26412;&#23545;&#24212;&#26159;&#21452;&#23556;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#24182;&#24573;&#35270;&#20102;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#35270;&#39057;&#20869;&#23481;&#36890;&#24120;&#28085;&#30422;&#22810;&#20010;&#20107;&#20214;&#65292;&#32780;&#29992;&#25143;&#26597;&#35810;&#25110;&#32593;&#39029;&#20803;&#25968;&#25454;&#31561;&#25991;&#26412;&#24448;&#24448;&#26159;&#20855;&#20307;&#30340;&#65292;&#24182;&#23545;&#24212;&#21333;&#20010;&#20107;&#20214;&#12290;&#36825;&#36896;&#25104;&#20102;&#20043;&#21069;&#30340;&#35757;&#32451;&#30446;&#26631;&#19982;&#23454;&#38469;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21487;&#33021;&#23548;&#33268;&#26089;&#26399;&#27169;&#22411;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#22810;&#20107;&#20214;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#65288;MeVTR&#65289;&#20219;&#21153;&#65292;&#38024;&#23545;&#27599;&#20010;&#35270;&#39057;&#21253;&#21547;&#22810;&#20010;&#19981;&#21516;&#20107;&#20214;&#30340;&#22330;&#26223;&#65292;&#20316;&#20026;&#20256;&#32479;&#35270;&#39057;&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#30340;&#19968;&#20010;&#21033;&#22522;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video-Text Retrieval (VTR) is a crucial multi-modal task in an era of massive video-text data on the Internet. A plethora of work characterized by using a two-stream Vision-Language model architecture that learns a joint representation of video-text pairs has become a prominent approach for the VTR task. However, these models operate under the assumption of bijective video-text correspondences and neglect a more practical scenario where video content usually encompasses multiple events, while texts like user queries or webpage metadata tend to be specific and correspond to single events. This establishes a gap between the previous training objective and real-world applications, leading to the potential performance degradation of earlier models during inference. In this study, we introduce the Multi-event Video-Text Retrieval (MeVTR) task, addressing scenarios in which each video contains multiple different events, as a niche scenario of the conventional Video-Text Retrieval Task. We pr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#37325;&#26032;&#26694;&#23450;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19922</link><description>&lt;p&gt;
&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Representation-Driven Reinforcement Learning. (arXiv:2305.19922v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#37325;&#26032;&#26694;&#23450;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#31574;&#30053;&#34920;&#31034;&#20026;&#20854;&#26399;&#26395;&#20540;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#24773;&#22659;&#25512;&#26029;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#23558;&#31574;&#30053;&#32593;&#32476;&#23884;&#20837;&#21040;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#37325;&#26032;&#26694;&#23450;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20854;&#20013;&#33391;&#22909;&#30340;&#31574;&#30053;&#34920;&#31034;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#26469;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102;&#31574;&#30053;&#34920;&#31034;&#22312;&#20915;&#23450;&#26368;&#20339;&#25506;&#32034;-&#21033;&#29992;&#31574;&#30053;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.
&lt;/p&gt;</description></item></channel></rss>