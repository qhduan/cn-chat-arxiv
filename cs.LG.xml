<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19243</link><description>&lt;p&gt;
&#29992;&#27491;&#24358;&#28608;&#27963;&#30340;&#20302;&#31209;&#30697;&#38453;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sine Activated Low-Rank Matrices for Parameter Efficient Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19243
&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#24050;&#32463;&#25104;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#22686;&#24378;&#21442;&#25968;&#25928;&#29575;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#25216;&#26415;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#31616;&#27905;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#21442;&#25968;&#20943;&#23569;&#24448;&#24448;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#21450;&#23436;&#25972;&#31209;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#19968;&#20010;&#27491;&#24358;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20445;&#30041;&#20102;&#20302;&#31209;&#26041;&#27861;&#30340;&#21442;&#25968;&#25928;&#29575;&#29305;&#24615;&#30340;&#22909;&#22788;&#65292;&#36824;&#22686;&#21152;&#20102;&#20998;&#35299;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#29616;&#26377;&#20302;&#31209;&#27169;&#22411;&#30340;&#19968;&#31181;&#36866;&#24212;&#24615;&#22686;&#24378;&#65292;&#27491;&#22914;&#20854;&#25104;&#21151;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19243v1 Announce Type: new  Abstract: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;$(\varepsilon, \Phi(\delta))$-&#23616;&#37096;&#22343;&#34913;&#65292;&#20197;&#35299;&#20915;&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#23616;&#37096;&#22343;&#34913;&#23384;&#22312;&#20294;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08171</link><description>&lt;p&gt;
&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#21487;&#22788;&#29702;&#30340;&#23616;&#37096;&#22343;&#34913;
&lt;/p&gt;
&lt;p&gt;
Tractable Local Equilibria in Non-Concave Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08171
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;$(\varepsilon, \Phi(\delta))$-&#23616;&#37096;&#22343;&#34913;&#65292;&#20197;&#35299;&#20915;&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#23616;&#37096;&#22343;&#34913;&#23384;&#22312;&#20294;&#38590;&#20197;&#22788;&#29702;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#22312;&#32447;&#26799;&#24230;&#19979;&#38477;&#21644;&#20854;&#20182;&#26080;&#24724;&#23398;&#20064;&#31243;&#24207;&#21487;&#20197;&#26377;&#25928;&#22320;&#25910;&#25947;&#21040;&#21327;&#35843;&#22343;&#34913;&#65292;&#22312;&#27599;&#20010;Agent&#30340;&#25928;&#29992;&#23545;&#20110;&#20854;&#33258;&#36523;&#31574;&#30053;&#21576;&#20985;&#24418;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#24403;&#25928;&#29992;&#26159;&#38750;&#20985;&#30340;&#26102;&#65292;&#36825;&#31181;&#24773;&#20917;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24456;&#24120;&#35265;&#65292;&#20854;&#20013;Agent&#30340;&#31574;&#30053;&#30001;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#65292;&#25110;&#32773;Agent&#30340;&#25928;&#29992;&#30001;&#31070;&#32463;&#32593;&#32476;&#35745;&#31639;&#65292;&#25110;&#20004;&#32773;&#20860;&#32780;&#26377;&#20043;&#12290;&#23454;&#38469;&#19978;&#65292;&#38750;&#20985;&#28216;&#25103;&#23384;&#22312;&#19968;&#31995;&#21015;&#21338;&#24328;&#35770;&#21644;&#20248;&#21270;&#25361;&#25112;&#65306;(i) Nash&#22343;&#34913;&#21487;&#33021;&#19981;&#23384;&#22312;&#65307;(ii) &#23616;&#37096;Nash&#22343;&#34913;&#23384;&#22312;&#20294;&#26159;&#19981;&#21487;&#22788;&#29702;&#65307;(iii) &#28151;&#21512;Nash&#12289;&#21327;&#35843;&#21644;&#31895;&#31961;&#21327;&#35843;&#22343;&#34913;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#20855;&#26377;&#26080;&#38480;&#25903;&#25345;&#65292;&#24182;&#19988;&#26159;&#19981;&#21487;&#22788;&#29702;&#30340;&#12290;&#20026;&#20102;&#36991;&#24320;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#27010;&#24565;&#65292;&#31216;&#20026;$(\varepsilon, \Phi(\delta))$-&#23616;&#37096;&#22343;&#34913;&#65292;&#35813;&#27010;&#24565;&#22312;&#38750;&#20985;&#28216;&#25103;&#20013;&#27010;&#25324;&#20102;&#23616;&#37096;Nash&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08171v1 Announce Type: cross  Abstract: While Online Gradient Descent and other no-regret learning procedures are known to efficiently converge to coarse correlated equilibrium in games where each agent's utility is concave in their own strategy, this is not the case when the utilities are non-concave, a situation that is common in machine learning applications where the agents' strategies are parameterized by deep neural networks, or the agents' utilities are computed by a neural network, or both. Indeed, non-concave games present a host of game-theoretic and optimization challenges: (i) Nash equilibria may fail to exist; (ii) local Nash equilibria exist but are intractable; and (iii) mixed Nash, correlated, and coarse correlated equilibria have infinite support in general, and are intractable. To sidestep these challenges we propose a new solution concept, termed $(\varepsilon, \Phi(\delta))$-local equilibrium, which generalizes local Nash equilibrium in non-concave games,
&lt;/p&gt;</description></item><item><title>SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.07378</link><description>&lt;p&gt;
SVD-LLM: &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21387;&#32553;&#30340;&#25130;&#26029;&#24863;&#30693;&#22855;&#24322;&#20540;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SVD-LLM: Truncation-aware Singular Value Decomposition for Large Language Model Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07378
&lt;/p&gt;
&lt;p&gt;
SVD-LLM&#26159;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#25130;&#26029;&#24863;&#30693;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#21644;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#30452;&#25509;&#26144;&#23556;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#21463;&#21040;&#20854;&#24222;&#22823;&#23610;&#23544;&#30340;&#38480;&#21046;&#65292;&#36825;&#38656;&#35201;LLM&#21387;&#32553;&#26041;&#27861;&#20197;&#23454;&#29616;&#23454;&#38469;&#37096;&#32626;&#12290;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#20026;LLM&#21387;&#32553;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38480;&#21046;&#65306;&#25130;&#26029;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#33021;&#23548;&#33268;&#26356;&#39640;&#30340;&#21387;&#32553;&#25439;&#22833;&#65292;&#24182;&#19988;&#22312;SVD&#25130;&#26029;&#21518;&#21097;&#20313;&#27169;&#22411;&#21442;&#25968;&#30340;&#26356;&#26032;&#32570;&#22833;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SVD-LLM&#65292;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;SVD&#30340;LLM&#21387;&#32553;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;SVD-LLM&#37319;&#29992;&#20102;&#19968;&#31181;&#25130;&#26029;&#24863;&#30693;&#30340;&#25968;&#25454;&#30333;&#21270;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#22855;&#24322;&#20540;&#21644;&#21387;&#32553;&#25439;&#22833;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;SVD-LLM&#37319;&#29992;&#19968;&#31181;&#36880;&#23618;&#38381;&#24335;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#31574;&#30053;&#65292;&#20197;&#24357;&#34917;SVD&#25130;&#26029;&#24341;&#36215;&#30340;&#20934;&#30830;&#24615;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;&#24635;&#20849;11&#20010;&#25968;&#25454;&#38598;&#21644;&#19971;&#20010;m&#19978;&#35780;&#20272;&#20102;SVD-LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07378v1 Announce Type: new  Abstract: The advancements in Large Language Models (LLMs) have been hindered by their substantial sizes, which necessitate LLM compression methods for practical deployment. Singular Value Decomposition (SVD) offers a promising solution for LLM compression. However, state-of-the-art SVD-based LLM compression methods have two key limitations: truncating smaller singular values may lead to higher compression loss, and the lack of update on the remaining model parameters after SVD truncation. In this work, we propose SVD-LLM, a new SVD-based LLM compression method that addresses the limitations of existing methods. SVD-LLM incorporates a truncation-aware data whitening strategy to ensure a direct mapping between singular values and compression loss. Moreover, SVD-LLM adopts a layer-wise closed-form model parameter update strategy to compensate for accuracy degradation caused by SVD truncation. We evaluate SVD-LLM on a total of 11 datasets and seven m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#35760;&#24518;&#36755;&#20837;&#20449;&#21495;&#30340;&#20851;&#32852;&#65292;&#24182;&#22522;&#20110;&#32622;&#20449;&#24230;&#20851;&#32852;&#20998;&#24067;&#24335;&#35760;&#24518;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#29305;&#24449;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21270;&#35760;&#24518;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#36866;&#21512;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;</title><link>https://arxiv.org/abs/2402.14598</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#30340;&#26080;&#29305;&#24449;&#33258;&#21160;&#36866;&#24212;&#39046;&#22495;
&lt;/p&gt;
&lt;p&gt;
Brain-inspired Distributed Memorization Learning for Efficient Feature-free Unsupervised Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14598
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#20154;&#31867;&#22823;&#33041;&#35760;&#24518;&#26426;&#21046;&#21551;&#21457;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#26426;&#21046;&#65292;&#36890;&#36807;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#35760;&#24518;&#36755;&#20837;&#20449;&#21495;&#30340;&#20851;&#32852;&#65292;&#24182;&#22522;&#20110;&#32622;&#20449;&#24230;&#20851;&#32852;&#20998;&#24067;&#24335;&#35760;&#24518;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#29305;&#24449;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21270;&#35760;&#24518;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#36866;&#21512;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#34920;&#29616;&#20986;&#26356;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26410;&#30693;&#29615;&#22659;&#32780;&#26080;&#38656;&#20351;&#29992;&#20219;&#20309;&#26799;&#24230;&#21453;&#21521;&#20256;&#25773;&#31243;&#24207;&#12290;&#21463;&#20154;&#31867;&#22823;&#33041;&#20998;&#24067;&#24335;&#35760;&#24518;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20998;&#24067;&#24335;&#35760;&#24518;&#23398;&#20064;&#26426;&#21046;&#65292;&#31216;&#20026;DML&#65292;&#20197;&#25903;&#25345;&#36716;&#31227;&#27169;&#22411;&#30340;&#24555;&#36895;&#39046;&#22495;&#36866;&#24212;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DML&#37319;&#29992;&#38543;&#26426;&#36830;&#25509;&#30340;&#31070;&#32463;&#20803;&#26469;&#35760;&#24518;&#36755;&#20837;&#20449;&#21495;&#30340;&#20851;&#32852;&#65292;&#36825;&#20123;&#20449;&#21495;&#20316;&#20026;&#20914;&#21160;&#20256;&#25773;&#65292;&#24182;&#36890;&#36807;&#20851;&#32852;&#20998;&#24067;&#24335;&#35760;&#24518;&#30340;&#32622;&#20449;&#24230;&#20570;&#20986;&#26368;&#32456;&#20915;&#31574;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;DML&#33021;&#22815;&#22522;&#20110;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#24378;&#21270;&#35760;&#24518;&#65292;&#24555;&#36895;&#36866;&#24212;&#26032;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#23545;&#28145;&#23618;&#29305;&#24449;&#36827;&#34892;&#32321;&#37325;&#30340;&#24494;&#35843;&#65292;&#36825;&#20351;&#20854;&#38750;&#24120;&#36866;&#21512;&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#12290;&#22522;&#20110;&#22235;&#20010;&#20132;&#21449;&#39046;&#22495;&#30340;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14598v1 Announce Type: cross  Abstract: Compared with gradient based artificial neural networks, biological neural networks usually show a more powerful generalization ability to quickly adapt to unknown environments without using any gradient back-propagation procedure. Inspired by the distributed memory mechanism of human brains, we propose a novel gradient-free Distributed Memorization Learning mechanism, namely DML, to support quick domain adaptation of transferred models. In particular, DML adopts randomly connected neurons to memorize the association of input signals, which are propagated as impulses, and makes the final decision by associating the distributed memories based on their confidence. More importantly, DML is able to perform reinforced memorization based on unlabeled data to quickly adapt to a new domain without heavy fine-tuning of deep features, which makes it very suitable for deploying on edge devices. Experiments based on four cross-domain real-world da
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.12265</link><description>&lt;p&gt;
&#35770;&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Byzantine-Resilience of Distillation-Based Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12265
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33976;&#39311;&#30340;&#32852;&#37030;&#23398;&#20064;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#19979;&#34920;&#29616;&#20986;&#26497;&#24378;&#30340;&#24377;&#24615;&#65292;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#38544;&#31169;&#12289;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#21644;&#36890;&#20449;&#25104;&#26412;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#31639;&#27861;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#25308;&#21344;&#24237;&#29615;&#22659;&#20013;&#30340;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#22522;&#20110;KD&#30340;FL&#31639;&#27861;&#30456;&#24403;&#20855;&#26377;&#24377;&#24615;&#65292;&#24182;&#20998;&#26512;&#20102;&#25308;&#21344;&#24237;&#23458;&#25143;&#31471;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#36807;&#31243;&#30456;&#23545;&#20110;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#12290;&#26681;&#25454;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#65292;&#24182;&#35777;&#26126;&#23427;&#20204;&#23545;&#20808;&#21069;&#30340;&#25308;&#21344;&#24237;&#24377;&#24615;&#26041;&#27861;&#26159;&#26377;&#25928;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FilterExp&#65292;&#19968;&#31181;&#26088;&#22312;&#22686;&#24378;&#25308;&#21344;&#24237;&#24377;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12265v1 Announce Type: cross  Abstract: Federated Learning (FL) algorithms using Knowledge Distillation (KD) have received increasing attention due to their favorable properties with respect to privacy, non-i.i.d. data and communication cost. These methods depart from transmitting model parameters and, instead, communicate information about a learning task by sharing predictions on a public dataset. In this work, we study the performance of such approaches in the byzantine setting, where a subset of the clients act in an adversarial manner aiming to disrupt the learning process. We show that KD-based FL algorithms are remarkably resilient and analyze how byzantine clients can influence the learning process compared to Federated Averaging. Based on these insights, we introduce two new byzantine attacks and demonstrate that they are effective against prior byzantine-resilient methods. Additionally, we propose FilterExp, a novel method designed to enhance the byzantine resilien
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.10504</link><description>&lt;p&gt;
&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#24377;&#24615;
&lt;/p&gt;
&lt;p&gt;
Resilience of the quadratic Littlewood-Offord problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10504
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20108;&#27425;Littlewood-Offord&#38382;&#39064;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#65292;&#20272;&#35745;&#20102;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#30340;&#19979;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#32500;&#25968;&#25454;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25552;&#20379;&#20102;&#20851;&#20110;&#23545;&#25239;&#24615;&#22122;&#22768;&#23545;&#20108;&#27425;Radamecher&#28151;&#27788;$\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$&#21453;&#38598;&#20013;&#29305;&#24615;&#30340;&#24433;&#21709;&#30340;&#20272;&#35745;&#65292;&#20854;&#20013;$M$&#26159;&#19968;&#20010;&#22266;&#23450;&#30340;&#65288;&#39640;&#32500;&#65289;&#30697;&#38453;&#65292;$\boldsymbol{\xi}$&#26159;&#19968;&#20010;&#20849;&#24418;Rademacher&#21521;&#37327;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;$\boldsymbol{\xi}$&#33021;&#22815;&#25215;&#21463;&#22810;&#23569;&#23545;&#25239;&#24615;&#31526;&#21495;&#32763;&#36716;&#32780;&#19981;&#8220;&#33192;&#32960;&#8221;$\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$&#65292;&#20174;&#32780;&#8220;&#21435;&#38500;&#8221;&#21407;&#22987;&#20998;&#24067;&#23548;&#33268;&#26356;&#8220;&#26377;&#31890;&#24230;&#8221;&#21644;&#23545;&#25239;&#24615;&#20559;&#20506;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;&#20108;&#27425;&#21644;&#21452;&#32447;&#24615;Rademacher&#28151;&#27788;&#30340;&#32479;&#35745;&#40065;&#26834;&#24615;&#25552;&#20379;&#20102;&#19979;&#38480;&#20272;&#35745;&#65307;&#36825;&#20123;&#32467;&#26524;&#22312;&#20851;&#38190;&#21306;&#22495;&#34987;&#35777;&#26126;&#26159;&#28176;&#36817;&#32039;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10504v1 Announce Type: cross  Abstract: We study the statistical resilience of high-dimensional data. Our results provide estimates as to the effects of adversarial noise over the anti-concentration properties of the quadratic Radamecher chaos $\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi}$, where $M$ is a fixed (high-dimensional) matrix and $\boldsymbol{\xi}$ is a conformal Rademacher vector. Specifically, we pursue the question of how many adversarial sign-flips can $\boldsymbol{\xi}$ sustain without "inflating" $\sup_{x\in \mathbb{R}} \mathbb{P} \left\{\boldsymbol{\xi}^{\mathsf{T}} M \boldsymbol{\xi} = x\right\}$ and thus "de-smooth" the original distribution resulting in a more "grainy" and adversarially biased distribution. Our results provide lower bound estimations for the statistical resilience of the quadratic and bilinear Rademacher chaos; these are shown to be asymptotically tight across key regimes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#65292;&#36890;&#36807;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06783</link><description>&lt;p&gt;
&#23398;&#20064;&#25945;&#23398;&#65306;&#25913;&#21892;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#29616;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learn to Teach: Improve Sample Efficiency in Teacher-student Learning for Sim-to-Real Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#65292;&#36890;&#36807;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21040;&#29616;&#23454;&#65288;sim-to-real&#65289;&#30340;&#36801;&#31227;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22495;&#38543;&#26426;&#21270;&#26159;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#24615;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#35266;&#27979;&#20013;&#30340;&#22122;&#22768;&#20351;&#24471;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#33539;&#24335;&#21487;&#20197;&#21152;&#36895;&#38543;&#26426;&#21270;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65292;&#25945;&#24072;&#26234;&#33021;&#20307;&#21487;&#20197;&#25351;&#23548;&#23398;&#29983;&#26234;&#33021;&#20307;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#19981;&#26159;&#26679;&#26412;&#25928;&#29575;&#30340;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#23398;&#29983;&#26234;&#33021;&#20307;&#26102;&#23436;&#20840;&#33293;&#24323;&#20102;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#28010;&#36153;&#20102;&#29615;&#22659;&#25152;&#36879;&#38706;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#30340;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#26469;&#25193;&#23637;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#19968;&#23545;&#25945;&#24072;-&#23398;&#29983;&#26234;&#33021;&#20307;&#65292;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#23545;&#20004;&#32773;&#37117;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-to-reality (sim-to-real) transfer is a fundamental problem for robot learning. Domain Randomization, which adds randomization during training, is a powerful technique that effectively addresses the sim-to-real gap. However, the noise in observations makes learning significantly harder. Recently, studies have shown that employing a teacher-student learning paradigm can accelerate training in randomized environments. Learned with privileged information, a teacher agent can instruct the student agent to operate in noisy environments. However, this approach is often not sample efficient as the experience collected by the teacher is discarded completely when training the student, wasting information revealed by the environment. In this work, we extend the teacher-student learning paradigm by proposing a sample efficient learning framework termed Learn to Teach (L2T) that recycles experience collected by the teacher agent. We observe that the dynamics of the environments for both 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05864</link><description>&lt;p&gt;
Permute-and-Flip&#65306;&#19968;&#31181;&#20855;&#26377;&#26368;&#20339;&#40065;&#26834;&#24615;&#21644;&#21487;&#21152;&#27700;&#21360;&#30340;LLMs&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05864
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#30340;&#26032;&#35299;&#30721;&#26041;&#27861;&#12290;&#23427;&#20855;&#26377;&#19982;&#26631;&#20934;&#37319;&#26679;&#35299;&#30721;&#22120;&#30456;&#20284;&#30340;&#40065;&#26834;&#24615;&#29305;&#24615;&#65292;&#20294;&#22312;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340; tradeoff &#19978;&#35777;&#26126;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#65292;&#19988;&#27704;&#36828;&#19981;&#20250;&#24046;&#20110;&#20219;&#20309;&#20854;&#20182;&#35299;&#30721;&#22120;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;Aaronson&#30340;Gumbel&#27700;&#21360;&#30340;&#21152;&#23494;&#27700;&#21360;&#26041;&#26696;&#65292;&#20294;&#26159;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#32780;&#33258;&#28982;&#37327;&#36523;&#23450;&#21046;&#12290;&#35813;&#27700;&#21360;&#26041;&#26696;&#19981;&#25913;&#21464;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#65292;&#21482;&#35201;&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#39640;&#29109;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PF&#35299;&#30721;&#22120;&#65288;&#21450;&#20854;&#24102;&#26377;&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65288;&#21450;&#20854;&#24102;&#26377;Gumbel&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#65288;&#21644;&#21487;&#26816;&#27979;&#24615;&#65289;&#65292;&#22240;&#27492;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/XuandongZhao/pf-decoding&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#24182;&#35757;&#32451;&#20986;&#22122;&#22768;&#23481;&#24525;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04398</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Learning from Time Series under Temporal Label Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#38388;&#24207;&#21015;&#19979;&#22788;&#29702;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#30452;&#25509;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#24182;&#35757;&#32451;&#20986;&#22122;&#22768;&#23481;&#24525;&#20998;&#31867;&#22120;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39034;&#24207;&#20998;&#31867;&#20219;&#21153;&#21463;&#21040;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#26631;&#31614;&#22122;&#22768;&#30340;&#24433;&#21709;&#12290;&#36825;&#31181;&#22122;&#22768;&#21487;&#33021;&#20250;&#23548;&#33268;&#26631;&#31614;&#36136;&#37327;&#38543;&#26102;&#38388;&#25913;&#21892;&#12289;&#24694;&#21270;&#25110;&#21608;&#26399;&#24615;&#21464;&#21270;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#21644;&#31995;&#32479;&#21270;&#20102;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#30340;&#27010;&#24565;&#65292;&#36825;&#26159;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#39034;&#24207;&#20998;&#31867;&#30340;&#19968;&#20010;&#26410;&#32463;&#30740;&#31350;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#22810;&#20010;&#26631;&#31614;&#36830;&#32493;&#35760;&#24405;&#65292;&#21516;&#26102;&#21463;&#21040;&#19968;&#20010;&#19982;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#20989;&#25968;&#30340;&#24178;&#25200;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#24314;&#27169;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#21450;&#29616;&#26377;&#26041;&#27861;&#30340;&#25345;&#32493;&#20302;&#25928;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#35757;&#32451;&#20986;&#23545;&#22122;&#22768;&#20855;&#26377;&#23481;&#24525;&#24615;&#30340;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#21508;&#26679;&#30340;&#26102;&#38388;&#26631;&#31614;&#22122;&#22768;&#20989;&#25968;&#19979;&#65292;&#20351;&#29992;&#30495;&#23454;&#21644;&#21512;&#25104;&#25968;&#25454;&#22312;&#24615;&#33021;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sequential classification tasks are affected by label noise that varies over time. Such noise can cause label quality to improve, worsen, or periodically change over time. We first propose and formalize temporal label noise, an unstudied problem for sequential classification of time series. In this setting, multiple labels are recorded in sequence while being corrupted by a time-dependent noise function. We first demonstrate the importance of modelling the temporal nature of the label noise function and how existing methods will consistently underperform. We then propose methods that can train noise-tolerant classifiers by estimating the temporal label noise function directly from data. We show that our methods lead to state-of-the-art performance in the presence of diverse temporal label noise functions using real and synthetic data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.02041</link><description>&lt;p&gt;
&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
$\alpha$-Divergence Loss Function for Neural Density Ratio Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#31070;&#32463;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#65292;&#36890;&#36807;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#32473;&#20986;&#20102;&#26679;&#26412;&#35201;&#27714;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22522;&#30784;&#25216;&#26415;&#23494;&#24230;&#27604;&#20272;&#35745;(DRE)&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22240;DRE&#30340;&#25439;&#22833;&#20989;&#25968;&#32780;&#20986;&#29616;&#20102;&#20248;&#21270;&#38382;&#39064;&#65306;KL&#25955;&#24230;&#38656;&#35201;&#22823;&#26679;&#26412;&#65292;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#28040;&#22833;&#65292;&#25439;&#22833;&#20989;&#25968;&#26799;&#24230;&#26377;&#20559;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#20379;&#31616;&#27905;&#23454;&#29616;&#21644;&#31283;&#23450;&#20248;&#21270;&#30340;$\alpha$-&#25955;&#24230;&#25439;&#22833;&#20989;&#25968;($\alpha$-Div)&#12290;&#27492;&#22806;&#65292;&#36824;&#32473;&#20986;&#20102;&#23545;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#25216;&#26415;&#39564;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#31283;&#23450;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;DRE&#20219;&#21153;&#30340;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;DRE&#30340;&#26679;&#26412;&#35201;&#27714;&#65292;&#20197;$L_1$&#35823;&#24046;&#30340;&#19978;&#30028;&#32852;&#31995;&#36215;&#26469;&#65292;&#35813;&#19978;&#30028;&#23558;&#39640;&#32500;&#24230;DRE&#20219;&#21153;&#20013;&#30340;&#32500;&#24230;&#35781;&#21650;&#20316;&#20026;&#19968;&#20010;&#20849;&#21516;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, neural networks have produced state-of-the-art results for density-ratio estimation (DRE), a fundamental technique in machine learning. However, existing methods bear optimization issues that arise from the loss functions of DRE: a large sample requirement of Kullback--Leibler (KL)-divergence, vanishing of train loss gradients, and biased gradients of the loss functions. Thus, an $\alpha$-divergence loss function ($\alpha$-Div) that offers concise implementation and stable optimization is proposed in this paper. Furthermore, technical justifications for the proposed loss function are presented. The stability of the proposed loss function is empirically demonstrated and the estimation accuracy of DRE tasks is investigated. Additionally, this study presents a sample requirement for DRE using the proposed loss function in terms of the upper bound of $L_1$ error, which connects a curse of dimensionality as a common problem in high-dimensional DRE tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#19979;&#30340;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#23545;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#36807;&#21442;&#25968;&#21270;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20173;&#28982;&#21463;&#30410;&#12290;</title><link>https://arxiv.org/abs/2311.17539</link><description>&lt;p&gt;
&#22312;&#36807;&#21442;&#25968;&#21270;&#19979;&#20998;&#26512;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Analyzing Sharpness-aware Minimization under Overparameterization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#36807;&#21442;&#25968;&#21270;&#26465;&#20214;&#19979;&#30340;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#26041;&#27861;&#12290;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#21457;&#29616;&#36807;&#21442;&#25968;&#21270;&#23545;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#19988;&#22312;&#36807;&#21442;&#25968;&#21270;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#65292;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#20173;&#28982;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#36807;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#65292;&#23613;&#31649;&#35757;&#32451;&#25439;&#22833;&#30456;&#21516;&#65292;&#20294;&#21487;&#20197;&#24471;&#21040;&#20855;&#26377;&#19981;&#21516;&#27867;&#21270;&#33021;&#21147;&#30340;&#26497;&#23567;&#20540;&#12290;&#26377;&#35777;&#25454;&#34920;&#26126;&#65292;&#26497;&#23567;&#20540;&#30340;&#38160;&#24230;&#19982;&#20854;&#27867;&#21270;&#35823;&#24046;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#65292;&#22240;&#27492;&#24050;&#32463;&#20570;&#20986;&#20102;&#26356;&#22810;&#21162;&#21147;&#24320;&#21457;&#19968;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#26174;&#24335;&#22320;&#25214;&#21040;&#25153;&#24179;&#26497;&#23567;&#20540;&#20316;&#20026;&#26356;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#30340;&#35299;&#12290;&#28982;&#32780;&#65292;&#33267;&#20170;&#20026;&#27490;&#65292;&#20851;&#20110;&#36807;&#21442;&#25968;&#21270;&#23545;&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#31574;&#30053;&#30340;&#24433;&#21709;&#30340;&#30740;&#31350;&#36824;&#19981;&#22810;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#22312;&#19981;&#21516;&#31243;&#24230;&#30340;&#36807;&#21442;&#25968;&#21270;&#19979;&#30340;SAM&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#35777;&#21644;&#29702;&#35770;&#32467;&#26524;&#65292;&#34920;&#26126;&#36807;&#21442;&#25968;&#21270;&#23545;SAM&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#21508;&#20010;&#39046;&#22495;&#65292;&#24182;&#34920;&#26126;&#23384;&#22312;&#19968;&#31181;&#19968;&#33268;&#30340;&#36235;&#21183;&#65292;&#21363;SAM&#22312;&#36807;&#21442;&#25968;&#21270;&#22686;&#21152;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#21463;&#30410;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#19968;&#20123;&#20196;&#20154;&#20449;&#26381;&#30340;&#26696;&#20363;&#65292;&#35828;&#26126;&#20102;&#36807;&#21442;&#25968;&#21270;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training an overparameterized neural network can yield minimizers of different generalization capabilities despite the same level of training loss. With evidence that suggests a correlation between sharpness of minima and their generalization errors, increasing efforts have been made to develop an optimization method to explicitly find flat minima as more generalizable solutions. However, this sharpness-aware minimization (SAM) strategy has not been studied much yet as to whether and how it is affected by overparameterization.   In this work, we analyze SAM under overparameterization of varying degrees and present both empirical and theoretical results that indicate a critical influence of overparameterization on SAM. Specifically, we conduct extensive numerical experiments across various domains, and show that there exists a consistent trend that SAM continues to benefit from increasing overparameterization. We also discover compelling cases where the effect of overparameterization is
&lt;/p&gt;</description></item><item><title>LMC&#26159;&#31532;&#19968;&#20010;&#24102;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#23376;&#22270;&#25277;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#65292;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2302.00924</link><description>&lt;p&gt;
LMC: &#22522;&#20110;&#23376;&#22270;&#25277;&#26679;&#30340;GNN&#24555;&#36895;&#35757;&#32451;&#19982;&#25910;&#25947;&#24615;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
LMC: Fast Training of GNNs via Subgraph Sampling with Provable Convergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00924
&lt;/p&gt;
&lt;p&gt;
LMC&#26159;&#31532;&#19968;&#20010;&#24102;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#23376;&#22270;&#25277;&#26679;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#65292;&#25552;&#39640;&#35757;&#32451;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#35757;&#32451;GNNs&#23384;&#22312;&#30528;&#20247;&#25152;&#21608;&#30693;&#30340;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#65292;&#21363;&#33410;&#28857;&#19982;&#28040;&#24687;&#20256;&#36882;&#23618;&#25968;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23376;&#22270;&#25277;&#26679;&#26041;&#27861;&#26159;&#19968;&#31867;&#26377;&#21069;&#36884;&#30340;&#23567;&#25209;&#37327;&#35757;&#32451;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#20013;&#20002;&#24323;&#23567;&#25209;&#37327;&#20043;&#22806;&#30340;&#28040;&#24687;&#26469;&#36991;&#20813;&#37051;&#23621;&#29190;&#28856;&#38382;&#39064;&#65292;&#20294;&#36825;&#20250;&#29306;&#29298;&#26799;&#24230;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24102;&#26377;&#25910;&#25947;&#24615;&#20445;&#35777;&#30340;&#23376;&#22270;&#25277;&#26679;&#26041;&#27861;&#65292;&#21363;&#23616;&#37096;&#28040;&#24687;&#34917;&#20607;&#65288;LMC&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.00924v3 Announce Type: replace  Abstract: The message passing-based graph neural networks (GNNs) have achieved great success in many real-world applications. However, training GNNs on large-scale graphs suffers from the well-known neighbor explosion problem, i.e., the exponentially increasing dependencies of nodes with the number of message passing layers. Subgraph-wise sampling methods -- a promising class of mini-batch training techniques -- discard messages outside the mini-batches in backward passes to avoid the neighbor explosion problem at the expense of gradient estimation accuracy. This poses significant challenges to their convergence analysis and convergence speeds, which seriously limits their reliable real-world applications. To address this challenge, we propose a novel subgraph-wise sampling method with a convergence guarantee, namely Local Message Compensation (LMC). To the best of our knowledge, LMC is the {\it first} subgraph-wise sampling method with provab
&lt;/p&gt;</description></item><item><title>Constraint-Generation Policy Optimization (CGPO)&#26159;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2401.12243</link><description>&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO): &#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#30340;&#38750;&#32447;&#24615;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming for Policy Optimization in Mixed Discrete-Continuous MDPs. (arXiv:2401.12243v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12243
&lt;/p&gt;
&lt;p&gt;
Constraint-Generation Policy Optimization (CGPO)&#26159;&#19968;&#31181;&#38024;&#23545;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;MDPs&#30340;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Constraint-Generation Policy Optimization (CGPO)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28151;&#21512;&#31163;&#25955;&#36830;&#32493;Markov Decision Processes (DC-MDPs)&#20013;&#20248;&#21270;&#31574;&#30053;&#21442;&#25968;&#12290;CGPO&#19981;&#20165;&#33021;&#22815;&#25552;&#20379;&#26377;&#30028;&#30340;&#31574;&#30053;&#35823;&#24046;&#20445;&#35777;&#65292;&#35206;&#30422;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#38750;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#26080;&#25968;&#21021;&#22987;&#29366;&#24577;&#33539;&#22260;&#30340;DC-MDPs&#65292;&#32780;&#19988;&#22312;&#32467;&#26463;&#26102;&#21487;&#20197;&#26126;&#30830;&#22320;&#25512;&#23548;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;CGPO&#36824;&#33021;&#22815;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#26469;&#35786;&#26029;&#31574;&#30053;&#32570;&#38519;&#65292;&#24182;&#25552;&#20379;&#26368;&#20248;&#34892;&#21160;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#32467;&#26524;&#65292;CGPO&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#30340;&#28151;&#21512;&#25972;&#25968;&#38750;&#32447;&#24615;&#20248;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#23450;&#20041;&#30340;&#34920;&#36798;&#33021;&#21147;&#31867;&#21035;&#65288;&#21363;&#20998;&#27573;(&#38750;)&#32447;&#24615;&#65289;&#20869;&#20248;&#21270;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#30340;&#32422;&#26463;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25239;&#24615;&#29983;&#25104;&#26368;&#22351;&#24773;&#20917;&#30340;&#29366;&#24577;&#36712;&#36857;&#12290;&#27492;&#22806;&#65292;&#20511;&#21161;&#29616;&#20195;&#38750;&#32447;&#24615;&#20248;&#21270;&#22120;&#65292;CGPO&#21487;&#20197;&#33719;&#24471;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Constraint-Generation Policy Optimization (CGPO) for optimizing policy parameters within compact and interpretable policy classes for mixed discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able to provide bounded policy error guarantees over an infinite range of initial states for many DC-MDPs with expressive nonlinear dynamics, but it can also provably derive optimal policies in cases where it terminates with zero error. Furthermore, CGPO can generate worst-case state trajectories to diagnose policy deficiencies and provide counterfactual explanations of optimal actions. To achieve such results, CGPO proposes a bi-level mixed-integer nonlinear optimization framework for optimizing policies within defined expressivity classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint generation methodology that adversarially generates worst-case state trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can obtain soluti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21322;&#27491;&#23450;&#35268;&#21010;&#36827;&#34892;&#20154;&#32676;&#32858;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#21407;&#22987;&#31181;&#32676;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#65292;&#36866;&#29992;&#20110;&#31181;&#32676;&#20043;&#38388;&#24046;&#24322;&#36739;&#23567;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.10927</link><description>&lt;p&gt;
&#20351;&#29992;&#21322;&#27491;&#23450;&#35268;&#21010;&#30340;&#21435;&#20559;&#21644;&#23616;&#37096;&#20998;&#26512;&#36827;&#34892;&#20154;&#32676;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Debiasing and a local analysis for population clustering using semidefinite programming. (arXiv:2401.10927v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10927
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#21322;&#27491;&#23450;&#35268;&#21010;&#36827;&#34892;&#20154;&#32676;&#32858;&#31867;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#12290;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#26681;&#25454;&#23567;&#26679;&#26412;&#25968;&#25454;&#30340;&#21407;&#22987;&#31181;&#32676;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#65292;&#36866;&#29992;&#20110;&#31181;&#32676;&#20043;&#38388;&#24046;&#24322;&#36739;&#23567;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#20174;&#28151;&#21512;&#30340;2&#20010;&#27425;&#39640;&#26031;&#20998;&#24067;&#20013;&#25277;&#21462;&#30340;&#23567;&#25968;&#25454;&#26679;&#26412;&#30340;&#20998;&#21306;&#38382;&#39064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21516;&#19968;&#20316;&#32773;&#25552;&#20986;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23558;&#25968;&#25454;&#26681;&#25454;&#20854;&#21407;&#22987;&#31181;&#32676;&#22823;&#33268;&#20998;&#20026;&#20004;&#32452;&#65292;&#32473;&#23450;&#19968;&#20010;&#23567;&#26679;&#26412;&#12290;&#26412;&#25991;&#30340;&#30740;&#31350;&#21160;&#26426;&#26159;&#23558;&#20010;&#20307;&#26681;&#25454;&#20854;&#21407;&#22987;&#31181;&#32676;&#20351;&#29992;p&#20010;&#26631;&#35760;&#36827;&#34892;&#32858;&#31867;&#65292;&#24403;&#20219;&#24847;&#20004;&#20010;&#31181;&#32676;&#20043;&#38388;&#30340;&#24046;&#24322;&#24456;&#23567;&#26102;&#12290;&#25105;&#20204;&#22522;&#20110;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#30340;&#21322;&#27491;&#23450;&#26494;&#24347;&#24418;&#24335;&#26500;&#24314;&#65292;&#35813;&#35268;&#21010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#22312;&#19968;&#20010;&#22270;&#19978;&#25214;&#21040;&#26368;&#22823;&#21106;&#65292;&#20854;&#20013;&#21106;&#20013;&#30340;&#36793;&#26435;&#37325;&#34920;&#31034;&#22522;&#20110;&#23427;&#20204;&#30340;p&#20010;&#29305;&#24449;&#30340;&#20004;&#20010;&#33410;&#28857;&#20043;&#38388;&#30340;&#19981;&#30456;&#20284;&#24230;&#24471;&#20998;&#12290;&#25105;&#20204;&#29992;&#916;^2:=p&#947;&#26469;&#34920;&#31034;&#20004;&#20010;&#20013;&#24515;&#65288;&#22343;&#20540;&#21521;&#37327;&#65289;&#20043;&#38388;&#30340;&#8467;_2^2&#36317;&#31163;&#65292;&#21363;&#956;^(1), &#956;^(2)&#8712;&#8477;^p&#12290;&#30446;&#26631;&#26159;&#22312;&#20132;&#25442;&#31934;&#24230;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#25552;&#20379;&#20840;&#38754;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of partitioning a small data sample of size $n$ drawn from a mixture of $2$ sub-gaussian distributions. In particular, we analyze computational efficient algorithms proposed by the same author, to partition data into two groups approximately according to their population of origin given a small sample. This work is motivated by the application of clustering individuals according to their population of origin using $p$ markers, when the divergence between any two of the populations is small. We build upon the semidefinite relaxation of an integer quadratic program that is formulated essentially as finding the maximum cut on a graph, where edge weights in the cut represent dissimilarity scores between two nodes based on their $p$ features. Here we use $\Delta^2 :=p \gamma$ to denote the $\ell_2^2$ distance between two centers (mean vectors), namely, $\mu^{(1)}$, $\mu^{(2)}$ $\in$ $\mathbb{R}^p$. The goal is to allow a full range of tradeoffs between
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#24180;&#40836;&#24230;&#37327;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39118;&#38505;&#29366;&#24577;&#27010;&#24565;&#21644;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2401.10265</link><description>&lt;p&gt;
&#26368;&#20339;&#26356;&#26032;&#26102;&#38388;&#65306;&#22522;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#24180;&#40836;&#24230;&#37327;&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
The Best Time for an Update: Risk-Sensitive Minimization of Age-Based Metrics. (arXiv:2401.10265v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10265
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#39118;&#38505;&#25935;&#24863;&#30340;&#24180;&#40836;&#24230;&#37327;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39118;&#38505;&#29366;&#24577;&#27010;&#24565;&#21644;&#39118;&#38505;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#20004;&#31181;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#34892;&#30340;&#37327;&#21270;&#20256;&#36755;&#25968;&#25454;&#36136;&#37327;&#30340;&#26041;&#27861;&#21253;&#25324;&#20449;&#24687;&#24180;&#40836;&#65288;Age of Information&#65292;AoI&#65289;&#65292;&#26597;&#35810;&#20449;&#24687;&#24180;&#40836;&#65288;Query Age of Information&#65292;QAoI&#65289;&#21644;&#19981;&#27491;&#30830;&#20449;&#24687;&#24180;&#40836;&#65288;Age of Incorrect Information&#65292;AoII&#65289;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#28857;&#23545;&#28857;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#20351;&#29992;&#36825;&#20123;&#24230;&#37327;&#65292;&#21457;&#36865;&#26041;&#30417;&#35270;&#19968;&#20010;&#36827;&#31243;&#24182;&#21521;&#25509;&#25910;&#26041;&#21457;&#36865;&#29366;&#24577;&#26356;&#26032;&#12290;&#25105;&#20204;&#30340;&#25361;&#25112;&#26159;&#20915;&#23450;&#26356;&#26032;&#30340;&#26368;&#20339;&#26102;&#38388;&#65292;&#24179;&#34913;&#20256;&#36755;&#33021;&#37327;&#21644;&#25509;&#25910;&#26041;&#30340;&#24180;&#40836;&#24230;&#37327;&#12290;&#30001;&#20110;&#39640;&#24180;&#40836;&#24230;&#37327;&#20540;&#24341;&#36215;&#30340;&#19981;&#31283;&#23450;&#31995;&#32479;&#29366;&#24577;&#31561;&#38382;&#39064;&#30340;&#22266;&#26377;&#39118;&#38505;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#30340;&#39118;&#38505;&#29366;&#24577;&#30340;&#27010;&#24565;&#26469;&#34920;&#31034;&#24180;&#40836;&#24230;&#37327;&#39640;&#30340;&#29366;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20010;&#26032;&#30340;&#39118;&#38505;&#29366;&#24577;&#27010;&#24565;&#26469;&#37327;&#21270;&#21644;&#26368;&#23567;&#21270;&#39640;&#24180;&#40836;&#24230;&#37327;&#30340;&#39118;&#38505;&#65292;&#36890;&#36807;&#30452;&#25509;&#23548;&#20986;&#39118;&#38505;&#29366;&#24577;&#30340;&#39057;&#29575;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#39118;&#38505;&#24230;&#37327;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#38024;&#23545;AoI&#65292;QAoI&#21644;AoII&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#12290;&#31532;&#19968;&#31181;&#31574;&#30053;&#20351;&#29992;&#31995;&#32479;&#30693;&#35782;&#65292;
&lt;/p&gt;
&lt;p&gt;
Popular methods to quantify transmitted data quality are the Age of Information (AoI), the Query Age of Information (QAoI), and the Age of Incorrect Information (AoII). We consider these metrics in a point-to-point wireless communication system, where the transmitter monitors a process and sends status updates to a receiver. The challenge is to decide on the best time for an update, balancing the transmission energy and the age-based metric at the receiver. Due to the inherent risk of high age-based metric values causing complications such as unstable system states, we introduce the new concept of risky states to denote states with high age-based metric. We use this new notion of risky states to quantify and minimize this risk of experiencing high age-based metrics by directly deriving the frequency of risky states as a novel risk-metric. Building on this foundation, we introduce two risk-sensitive strategies for AoI, QAoI and AoII. The first strategy uses system knowledge, i.e., chann
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29109;&#21305;&#37197;&#26694;&#26550;&#30340;&#26032;&#30340;&#21487;&#22788;&#29702;&#30340;&#25512;&#26029;&#26041;&#26696;&#65292;&#21487;&#20197;&#23884;&#20837;&#21040;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#20013;&#65292;&#23545;&#20110;&#25551;&#36848;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#36807;&#31243;&#30340;Markov&#36339;&#36291;&#36807;&#31243;&#30340;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#31867;&#36817;&#20284;&#20998;&#24067;&#30340;&#38381;&#24335;&#32467;&#26524;&#20197;&#21450;&#24212;&#29992;&#20110;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#30340;&#19968;&#33324;&#31867;&#21035;&#26469;&#21152;&#20197;&#35770;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36817;&#20284;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31243;&#24207;&#23548;&#20986;&#20102;&#28508;&#22312;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#22312;&#21508;&#31181;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#31034;&#20363;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35813;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.15604</link><description>&lt;p&gt;
Entropic Matching&#29992;&#20110;Markov&#36339;&#36291;&#36807;&#31243;&#30340;&#26399;&#26395;&#20256;&#25773;&#30340;&#29109;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Entropic Matching for Expectation Propagation of Markov Jump Processes. (arXiv:2309.15604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29109;&#21305;&#37197;&#26694;&#26550;&#30340;&#26032;&#30340;&#21487;&#22788;&#29702;&#30340;&#25512;&#26029;&#26041;&#26696;&#65292;&#21487;&#20197;&#23884;&#20837;&#21040;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#20013;&#65292;&#23545;&#20110;&#25551;&#36848;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#36807;&#31243;&#30340;Markov&#36339;&#36291;&#36807;&#31243;&#30340;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#19968;&#31867;&#36817;&#20284;&#20998;&#24067;&#30340;&#38381;&#24335;&#32467;&#26524;&#20197;&#21450;&#24212;&#29992;&#20110;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#30340;&#19968;&#33324;&#31867;&#21035;&#26469;&#21152;&#20197;&#35770;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36817;&#20284;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31243;&#24207;&#23548;&#20986;&#20102;&#28508;&#22312;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#24182;&#22312;&#21508;&#31181;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#31034;&#20363;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#35813;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#28508;&#22312;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#36807;&#31243;&#30340;&#32479;&#35745;&#25512;&#26029;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#30001;Markov&#36339;&#36291;&#36807;&#31243;&#25551;&#36848;&#30340;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#36807;&#31243;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#22788;&#29702;&#30340;&#25512;&#26029;&#26041;&#26696;&#65292;&#22522;&#20110;&#29109;&#21305;&#37197;&#26694;&#26550;&#65292;&#21487;&#20197;&#23884;&#20837;&#21040;&#20247;&#25152;&#21608;&#30693;&#30340;&#26399;&#26395;&#20256;&#25773;&#31639;&#27861;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#20026;&#19968;&#31867;&#31616;&#21333;&#30340;&#36817;&#20284;&#20998;&#24067;&#25552;&#20379;&#38381;&#24335;&#32467;&#26524;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#30340;&#19968;&#33324;&#31867;&#21035;&#65292;&#35813;&#31867;&#21035;&#26159;&#31995;&#32479;&#29983;&#29289;&#23398;&#24314;&#27169;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#26469;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#36817;&#20284;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#31243;&#24207;&#23548;&#20986;&#20102;&#28508;&#22312;&#21442;&#25968;&#30340;&#28857;&#20272;&#35745;&#30340;&#38381;&#24335;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21508;&#31181;&#21270;&#23398;&#21453;&#24212;&#32593;&#32476;&#31034;&#20363;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#38543;&#26426;&#30340;Lotka-Voltera&#31034;&#20363;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#30340;&#23616;&#38480;&#24615;&#21644;&#26410;&#26469;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of statistical inference for latent continuous-time stochastic processes, which is often intractable, particularly for discrete state space processes described by Markov jump processes. To overcome this issue, we propose a new tractable inference scheme based on an entropic matching framework that can be embedded into the well-known expectation propagation algorithm. We demonstrate the effectiveness of our method by providing closed-form results for a simple family of approximate distributions and apply it to the general class of chemical reaction networks, which are a crucial tool for modeling in systems biology. Moreover, we derive closed form expressions for point estimation of the underlying parameters using an approximate expectation maximization procedure. We evaluate the performance of our method on various chemical reaction network instantiations, including a stochastic Lotka-Voltera example, and discuss its limitations and potential for future 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#21644;&#32447;&#24615;&#25237;&#24433;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.07261</link><description>&lt;p&gt;
&#20855;&#26377;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21516;&#26102;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Simultaneous inference for generalized linear models with unmeasured confounders. (arXiv:2309.07261v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07261
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#21644;&#32447;&#24615;&#25237;&#24433;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#24341;&#36215;&#30340;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#22240;&#32452;&#30740;&#31350;&#20013;&#65292;&#24120;&#24120;&#36827;&#34892;&#25104;&#21315;&#19978;&#19975;&#20010;&#21516;&#26102;&#20551;&#35774;&#26816;&#39564;&#65292;&#20197;&#30830;&#23450;&#24046;&#24322;&#34920;&#36798;&#30340;&#22522;&#22240;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#26410;&#27979;&#28151;&#28102;&#22240;&#32032;&#65292;&#35768;&#22810;&#26631;&#20934;&#32479;&#35745;&#26041;&#27861;&#21487;&#33021;&#23384;&#22312;&#20005;&#37325;&#30340;&#20559;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23384;&#22312;&#28151;&#28102;&#25928;&#24212;&#26102;&#30340;&#22810;&#20803;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#20551;&#35774;&#26816;&#39564;&#38382;&#39064;&#12290;&#22312;&#20219;&#24847;&#28151;&#28102;&#26426;&#21046;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#32479;&#35745;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#21033;&#29992;&#27491;&#20132;&#32467;&#26500;&#24182;&#23558;&#32447;&#24615;&#25237;&#24433;&#25972;&#21512;&#21040;&#19977;&#20010;&#20851;&#38190;&#38454;&#27573;&#20013;&#12290;&#39318;&#20808;&#65292;&#21033;&#29992;&#22810;&#20803;&#21709;&#24212;&#21464;&#37327;&#20998;&#31163;&#36793;&#38469;&#21644;&#19981;&#30456;&#20851;&#30340;&#28151;&#28102;&#25928;&#24212;&#65292;&#24674;&#22797;&#28151;&#28102;&#31995;&#25968;&#30340;&#21015;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;&#21033;&#29992;$\ell_1$&#27491;&#21017;&#21270;&#36827;&#34892;&#31232;&#30095;&#24615;&#20272;&#35745;&#65292;&#24182;&#24378;&#21152;&#27491;&#20132;&#24615;&#38480;&#21046;&#20110;&#28151;&#28102;&#31995;&#25968;&#65292;&#32852;&#21512;&#20272;&#35745;&#28508;&#22312;&#22240;&#23376;&#21644;&#20027;&#35201;&#25928;&#24212;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#25237;&#24433;&#21644;&#21152;&#26435;&#20559;&#24046;&#26657;&#27491;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tens of thousands of simultaneous hypothesis tests are routinely performed in genomic studies to identify differentially expressed genes. However, due to unmeasured confounders, many standard statistical approaches may be substantially biased. This paper investigates the large-scale hypothesis testing problem for multivariate generalized linear models in the presence of confounding effects. Under arbitrary confounding mechanisms, we propose a unified statistical estimation and inference framework that harnesses orthogonal structures and integrates linear projections into three key stages. It first leverages multivariate responses to separate marginal and uncorrelated confounding effects, recovering the confounding coefficients' column space. Subsequently, latent factors and primary effects are jointly estimated, utilizing $\ell_1$-regularization for sparsity while imposing orthogonality onto confounding coefficients. Finally, we incorporate projected and weighted bias-correction steps 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19968;&#20301;&#21387;&#32553;&#24863;&#30693;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#20256;&#36755;&#19968;&#20301;&#20449;&#24687;&#24182;&#20943;&#23569;&#36890;&#20449;&#22238;&#21512;&#30340;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#31232;&#30095;&#32422;&#26463;&#30340;&#20849;&#20139;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2308.16671</link><description>&lt;p&gt;
&#22522;&#20110;&#19968;&#20301;&#21387;&#32553;&#24863;&#30693;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Decentralized Federated Learning via One-Bit Compressive Sensing. (arXiv:2308.16671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#19968;&#20301;&#21387;&#32553;&#24863;&#30693;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#20256;&#36755;&#19968;&#20301;&#20449;&#24687;&#24182;&#20943;&#23569;&#36890;&#20449;&#22238;&#21512;&#30340;&#25968;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#20855;&#26377;&#31232;&#30095;&#32422;&#26463;&#30340;&#20849;&#20139;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#22240;&#20854;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#32780;&#21464;&#24471;&#27969;&#34892;&#12290;&#19982;&#38598;&#20013;&#24335;&#29256;&#26412;&#30456;&#27604;&#65292;&#22312;DFL&#20013;&#22312;&#22823;&#37327;&#33410;&#28857;&#20043;&#38388;&#35757;&#32451;&#20849;&#20139;&#27169;&#22411;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#27809;&#26377;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#21327;&#35843;&#35757;&#32451;&#36807;&#31243;&#12290;&#23588;&#20854;&#26159;&#24403;&#20998;&#24067;&#24335;&#33410;&#28857;&#22312;&#36890;&#20449;&#25110;&#35745;&#31639;&#36164;&#28304;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#26102;&#65292;DFL&#30340;&#35757;&#32451;&#23558;&#21464;&#24471;&#38750;&#24120;&#20302;&#25928;&#21644;&#19981;&#31283;&#23450;&#12290;&#37492;&#20110;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#22522;&#20110;&#19981;&#31934;&#30830;&#20132;&#26367;&#26041;&#21521;&#26041;&#27861;&#65288;iADM&#65289;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#20855;&#26377;&#31232;&#30095;&#32422;&#26463;&#30340;&#20849;&#20139;&#27169;&#22411;&#12290;&#35813;&#32422;&#26463;&#20351;&#25105;&#20204;&#33021;&#22815;&#21033;&#29992;&#19968;&#20301;&#21387;&#32553;&#24863;&#30693;&#65288;1BCS&#65289;&#65292;&#20801;&#35768;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#20256;&#36755;&#19968;&#20301;&#20449;&#24687;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#36890;&#20449;&#20165;&#22312;&#26576;&#20123;&#27493;&#39588;&#20013;&#21457;&#29983;&#65292;&#20943;&#23569;&#20102;&#36890;&#20449;&#22238;&#21512;&#30340;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#35813;&#31639;&#27861;&#23637;&#29616;&#20102;&#39640;&#25928;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decentralized federated learning (DFL) has gained popularity due to its practicality across various applications. Compared to the centralized version, training a shared model among a large number of nodes in DFL is more challenging, as there is no central server to coordinate the training process. Especially when distributed nodes suffer from limitations in communication or computational resources, DFL will experience extremely inefficient and unstable training. Motivated by these challenges, in this paper, we develop a novel algorithm based on the framework of the inexact alternating direction method (iADM). On one hand, our goal is to train a shared model with a sparsity constraint. This constraint enables us to leverage one-bit compressive sensing (1BCS), allowing transmission of one-bit information among neighbour nodes. On the other hand, communication between neighbour nodes occurs only at certain steps, reducing the number of communication rounds. Therefore, the algorithm exhibi
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#20026;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;</title><link>http://arxiv.org/abs/2308.12794</link><description>&lt;p&gt;
&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#22522;&#20934;&#65306;&#29992;&#20110;&#23398;&#20064;&#21644;&#38750;&#23398;&#20064;&#26041;&#27861;&#30340;&#29615;&#22659;&#21644;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Job Shop Scheduling Benchmark: Environments and Instances for Learning and Non-learning Methods. (arXiv:2308.12794v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12794
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#20026;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#25552;&#20379;&#20102;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#22810;&#31181;&#29615;&#22659;&#21644;&#23454;&#20363;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;GitHub&#20179;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#24191;&#27867;&#30340;&#26426;&#22120;&#35843;&#24230;&#38382;&#39064;&#30340;&#32508;&#21512;&#22522;&#20934;&#65292;&#21253;&#25324;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#65288;JSP&#65289;&#65292;&#27969;&#27700;&#36710;&#38388;&#35843;&#24230;&#65288;FSP&#65289;&#65292;&#28789;&#27963;&#24037;&#20316;&#36710;&#38388;&#35843;&#24230;&#65288;FJSP&#65289;&#65292;&#20855;&#26377;&#35013;&#37197;&#32422;&#26463;&#30340;FJSP&#65288;FAJSP&#65289;&#65292;&#20855;&#26377;&#24207;&#21015;&#20381;&#36182;&#35774;&#32622;&#26102;&#38388;&#30340;FJSP&#65288;FJSP-SDST&#65289;&#21644;&#22312;&#32447;FJSP&#65288;&#22312;&#32447;&#20316;&#19994;&#21040;&#36798;&#65289;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#20026;&#23545;&#26426;&#22120;&#35843;&#24230;&#25361;&#25112;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#20174;&#19994;&#32773;&#21644;&#29233;&#22909;&#32773;&#25552;&#20379;&#19968;&#20010;&#38598;&#20013;&#30340;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an open-source GitHub repository containing comprehensive benchmarks for a wide range of machine scheduling problems, including Job Shop Scheduling (JSP), Flow Shop Scheduling (FSP), Flexible Job Shop Scheduling (FJSP), FJSP with Assembly constraints (FAJSP), FJSP with Sequence-Dependent Setup Times (FJSP-SDST), and the online FJSP (with online job arrivals). Our primary goal is to provide a centralized hub for researchers, practitioners, and enthusiasts interested in tackling machine scheduling challenges.
&lt;/p&gt;</description></item><item><title>MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.01697</link><description>&lt;p&gt;
MutateNN&#65306;&#29992;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#31361;&#21464;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
MutateNN: Mutation Testing of Image Recognition Models Deployed on Hardware Accelerators. (arXiv:2306.01697v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01697
&lt;/p&gt;
&lt;p&gt;
MutateNN&#26159;&#19968;&#31181;&#29992;&#20110;&#25506;&#32034;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#65292;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#65292;&#19988;&#26377;&#25928;&#24615;&#24050;&#22312;&#22810;&#31181;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20013;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#24182;&#25512;&#21160;&#25216;&#26415;&#21457;&#23637;&#30340;&#26032;&#26426;&#36935;&#24212;&#36816;&#32780;&#29983;&#12290;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#29305;&#21035;&#26159;&#34987;&#20998;&#37197;&#20102;&#24863;&#30693;&#20219;&#21153;&#65292;&#20197;&#35299;&#20915;&#22797;&#26434;&#30340;&#29616;&#23454;&#19990;&#30028;&#25361;&#25112;&#24182;&#23548;&#33268;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#36825;&#31867;&#27169;&#22411;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#36164;&#28304;&#38656;&#27714;&#20063;&#26377;&#25152;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27169;&#22411;&#20248;&#21270;&#21644;&#30828;&#20214;&#21152;&#36895;&#24050;&#25104;&#20026;&#20851;&#38190;&#25216;&#26415;&#65292;&#20294;&#26377;&#25928;&#25972;&#21512;&#36825;&#20123;&#27010;&#24565;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35753;&#24320;&#21457;&#20154;&#21592;&#21644;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#25506;&#32034;&#22312;&#19981;&#21516;&#30828;&#20214;&#21152;&#36895;&#35774;&#22791;&#19978;&#37096;&#32626;&#30340;&#28145;&#24230;&#23398;&#20064;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MutateNN&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;&#27492;&#30446;&#30340;&#25552;&#20379;&#31361;&#21464;&#27979;&#35797;&#21644;&#20998;&#26512;&#33021;&#21147;&#30340;&#24037;&#20855;&#12290;&#20026;&#20102;&#23637;&#31034;&#20854;&#21151;&#33021;&#65292;&#25105;&#20204;&#23545;7&#20010;&#24191;&#20026;&#20154;&#30693;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36827;&#34892;&#20102;21&#31181;&#21464;&#24322;&#12290;&#25105;&#20204;&#22312;4&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#20102;&#25105;&#20204;&#30340;&#21464;&#24322;&#20307;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#34892;&#20026;&#65292;&#24182;&#35780;&#20272;&#20102;MutateNN&#22312;&#26816;&#27979;&#20986;&#19981;&#27491;&#30830;&#25110;&#19981;&#31934;&#30830;&#30340;&#27169;&#22411;&#34892;&#20026;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the research advancement of Artificial Intelligence in the last years, there are new opportunities to mitigate real-world problems and advance technologically. Image recognition models in particular, are assigned with perception tasks to mitigate complex real-world challenges and lead to new solutions. Furthermore, the computational complexity and demand for resources of such models has also increased. To mitigate this, model optimization and hardware acceleration has come into play, but effectively integrating such concepts is a challenging and error-prone process.  In order to allow developers and researchers to explore the robustness of deep learning image recognition models deployed on different hardware acceleration devices, we propose MutateNN, a tool that provides mutation testing and analysis capabilities for that purpose. To showcase its capabilities, we utilized 21 mutations for 7 widely-known pre-trained deep neural network models. We deployed our mutants on 4 different
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.17473</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27010;&#36848;&#19982;&#27604;&#36739;&#20998;&#26512;&#65306;CNN&#12289;RNN&#12289;LSTM&#12289;GRU&#12290;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Overview and Comparative Analysis on Deep Learning Models: CNN, RNN, LSTM, GRU. (arXiv:2305.17473v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#27010;&#25324;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#31867;&#22411;&#21644;&#24212;&#29992;&#65292;&#27604;&#36739;&#20998;&#26512;&#20102;&#21508;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#26377;&#21161;&#20110;&#36873;&#25321;&#21644;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24378;&#22823;&#23376;&#38598;&#65292;&#29305;&#21035;&#22312;&#22788;&#29702;&#38750;&#32467;&#26500;&#21270;&#21644;&#22823;&#22411;&#25968;&#25454;&#38598;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;ML&#26041;&#27861;&#12290;&#20854;&#24433;&#21709;&#36328;&#36234;&#21508;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#35821;&#38899;&#35782;&#21035;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#39044;&#27979;&#20998;&#26512;&#31561;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#21644;&#21160;&#24577;&#24615;&#32473;&#35774;&#35745;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#24320;&#21457;&#20986;&#20102;&#20960;&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#35299;&#20915;&#19981;&#21516;&#30340;&#38382;&#39064;&#21644;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35843;&#26597;&#65292;&#21253;&#25324;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12289;&#29983;&#25104;&#27169;&#22411;&#12289;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#21644;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#32771;&#23519;&#20102;&#27599;&#20010;&#27169;&#22411;&#30340;&#32467;&#26500;&#12289;&#24212;&#29992;&#12289;&#22909;&#22788;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19977;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) has emerged as a powerful subset of machine learning (ML) and artificial intelligence (AI), outperforming traditional ML methods, especially in handling unstructured and large datasets. Its impact spans across various domains, including speech recognition, healthcare, autonomous vehicles, cybersecurity, predictive analytics, and more. However, the complexity and dynamic nature of real-world problems present challenges in designing effective deep learning models. Consequently, several deep learning models have been developed to address different problems and applications. In this article, we conduct a comprehensive survey of various deep learning models, including Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), Generative Models, Deep Reinforcement Learning (DRL), and Deep Transfer Learning. We examine the structure, applications, benefits, and limitations of each model. Furthermore, we perform an analysis using three publicly available dataset
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;RViDeformer&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;&#21450;&#20854;&#37197;&#22871;&#25968;&#25454;&#38598;ReCRVD&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#20302;ISO&#35774;&#32622;&#37325;&#26032;&#25429;&#25417;&#29616;&#26377;&#35270;&#39057;&#20197;&#26500;&#24314;&#22122;&#22768;-&#28165;&#26224;&#23545;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#38750;&#26412;&#22320;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.00767</link><description>&lt;p&gt;
RViDeformer&#65306;&#20855;&#26377;&#26356;&#22823;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
RViDeformer: Efficient Raw Video Denoising Transformer with a Larger Benchmark Dataset. (arXiv:2305.00767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;RViDeformer&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;&#21450;&#20854;&#37197;&#22871;&#25968;&#25454;&#38598;ReCRVD&#65292;&#20854;&#20013;&#21033;&#29992;&#39640;&#20302;ISO&#35774;&#32622;&#37325;&#26032;&#25429;&#25417;&#29616;&#26377;&#35270;&#39057;&#20197;&#26500;&#24314;&#22122;&#22768;-&#28165;&#26224;&#23545;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#38750;&#26412;&#22320;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30001;&#20110;&#19982;&#25104;&#20687;&#36807;&#31243;&#30340;&#19968;&#33268;&#24615;&#21644;&#21407;&#22987;&#39046;&#22495;&#20013;&#25104;&#29087;&#30340;&#22122;&#22768;&#24314;&#27169;&#65292;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#38459;&#30861;&#20102;&#21435;&#22122;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#23545;&#20110;&#21463;&#25511;&#30340;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#26469;&#35828;&#65292;&#27809;&#26377;&#20855;&#26377;&#30495;&#23454;&#36816;&#21160;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#22240;&#20026;&#20026;&#30495;&#23454;&#21160;&#24577;&#22330;&#26223;&#25429;&#25417;&#22122;&#22768;&#21644;&#28165;&#26224;&#24103;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#37325;&#26032;&#25429;&#25417;&#20197;&#39640;&#20302;ISO&#35774;&#32622;&#26174;&#31034;&#30340;&#29616;&#26377;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#20197;&#26500;&#24314;&#22122;&#22768;-&#28165;&#26224;&#37197;&#23545;&#24103;&#12290;&#36825;&#26679;&#65292;&#25105;&#20204;&#26500;&#24314;&#19968;&#20010;&#35270;&#39057;&#21435;&#22122;&#25968;&#25454;&#38598;&#65288;&#21629;&#21517;&#20026;ReCRVD&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;120&#32452;&#22122;&#22768;-&#28165;&#26224;&#35270;&#39057;&#65292;&#20854;ISO&#20540;&#20174;1600&#21040;25600&#19981;&#31561;&#12290;&#20854;&#27425;&#65292;&#34429;&#28982;&#38750;&#26412;&#22320;&#26102;&#31354;&#20851;&#27880;&#23545;&#20110;&#21435;&#22122;&#26377;&#30410;&#65292;&#20294;&#23427;&#36890;&#24120;&#23548;&#33268;&#27785;&#37325;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#22987;&#35270;&#39057;&#21435;&#22122;&#21464;&#25442;&#22120;&#32593;&#32476;&#65288;RViDeformer&#65289;&#65292;&#23427;&#25506;&#32034;&#20102;&#30701;&#36317;&#31163;&#21644;&#38271;&#36317;&#31163;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31354;&#38388;&#21464;&#25442;&#22120;&#65292;&#29992;&#20110;&#22788;&#29702;&#26412;&#22320;&#35270;&#35273;&#29305;&#24449;&#20197;&#20943;&#23569;&#31354;&#38388;&#20887;&#20313;&#24182;&#21152;&#36895;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#38271;&#31243;&#22122;&#22768;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#21464;&#25442;&#22120;&#32593;&#32476;&#65292;&#21516;&#26102;&#27169;&#22411;&#21270;&#38750;&#26412;&#22320;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, raw video denoising has garnered increased attention due to the consistency with the imaging process and well-studied noise modeling in the raw domain. However, two problems still hinder the denoising performance. Firstly, there is no large dataset with realistic motions for supervised raw video denoising, as capturing noisy and clean frames for real dynamic scenes is difficult. To address this, we propose recapturing existing high-resolution videos displayed on a 4K screen with high-low ISO settings to construct noisy-clean paired frames. In this way, we construct a video denoising dataset (named as ReCRVD) with 120 groups of noisy-clean videos, whose ISO values ranging from 1600 to 25600. Secondly, while non-local temporal-spatial attention is beneficial for denoising, it often leads to heavy computation costs. We propose an efficient raw video denoising transformer network (RViDeformer) that explores both short and long-distance correlations. Specifically, we propos
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Nystr\"om $M$-Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65292;&#38024;&#23545;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#20108;&#27425;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#36827;&#34892;&#20102;&#35299;&#20915;&#65292;&#24182;&#20860;&#39038;&#20102;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#25512;&#24191;&#24773;&#20917;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2302.09930</link><description>&lt;p&gt;
Nystr\"om $M$-Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Nystr\"om $M$-Hilbert-Schmidt Independence Criterion. (arXiv:2302.09930v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09930
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;Nystr\"om $M$-Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65292;&#38024;&#23545;&#22823;&#35268;&#27169;&#24212;&#29992;&#30340;&#20108;&#27425;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#36827;&#34892;&#20102;&#35299;&#20915;&#65292;&#24182;&#20860;&#39038;&#20102;&#22810;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#25512;&#24191;&#24773;&#20917;&#21644;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#25216;&#26415;&#26159;&#25968;&#25454;&#31185;&#23398;&#20013;&#26368;&#21463;&#27426;&#36814;&#21644;&#24378;&#22823;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#26680;&#30340;&#24191;&#27867;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24615;&#21253;&#25324;&#65306;(i) &#23427;&#20204;&#38024;&#23545;&#30340;&#39046;&#22495;&#25968;&#37327;&#22810;&#65292;(ii) &#19982;&#26680;&#30456;&#20851;&#30340;&#20989;&#25968;&#31867;&#20855;&#26377;Hilbert&#32467;&#26500;&#65292;&#20415;&#20110;&#32479;&#35745;&#20998;&#26512;&#65292;&#20197;&#21450;(iii) &#23427;&#20204;&#33021;&#22815;&#20197;&#19981;&#20002;&#22833;&#20449;&#24687;&#30340;&#26041;&#24335;&#34920;&#31034;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#20123;&#29305;&#24615;&#23548;&#33268;&#20102;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;(HSIC)&#30340;&#24040;&#22823;&#25104;&#21151;&#65292;&#35813;&#20934;&#21017;&#33021;&#22815;&#22312;&#28201;&#21644;&#26465;&#20214;&#19979;&#25429;&#25417;&#38543;&#26426;&#21464;&#37327;&#30340;&#32852;&#21512;&#29420;&#31435;&#24615;&#65292;&#24182;&#20801;&#35768;&#20855;&#26377;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24615;&#30340;&#38381;&#24335;&#20272;&#35745;&#22120;(&#30456;&#23545;&#20110;&#26679;&#26412;&#22823;&#23567;)&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#30340;&#20108;&#27425;&#35745;&#31639;&#29942;&#39048;&#38382;&#39064;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#20010;HSIC&#36817;&#20284;&#20272;&#35745;&#22120;&#65292;&#28982;&#32780;&#36825;&#20123;&#20272;&#35745;&#22120;&#38480;&#21046;&#20110;$M=2$&#20010;&#38543;&#26426;&#21464;&#37327;&#65292;&#19981;&#33021;&#33258;&#28982;&#22320;&#25512;&#24191;&#21040;$M \geq 2$&#30340;&#24773;&#20917;&#65292;&#24182;&#19988;&#32570;&#20047;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;Nystr\"om $M$-Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Kernel techniques are among the most popular and powerful approaches of data science. Among the key features that make kernels ubiquitous are (i) the number of domains they have been designed for, (ii) the Hilbert structure of the function class associated to kernels facilitating their statistical analysis, and (iii) their ability to represent probability distributions without loss of information. These properties give rise to the immense success of Hilbert-Schmidt independence criterion (HSIC) which is able to capture joint independence of random variables under mild conditions, and permits closed-form estimators with quadratic computational complexity (w.r.t. the sample size). In order to alleviate the quadratic computational bottleneck in large-scale applications, multiple HSIC approximations have been proposed, however these estimators are restricted to $M=2$ random variables, do not extend naturally to the $M\ge 2$ case, and lack theoretical guarantees. In this work, we propose an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#36848;&#20102;&#24352;&#37327;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#24182;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#32467;&#21512;&#65306;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#65292;&#25506;&#35752;&#20102;TNN&#22312;&#32593;&#32476;&#21387;&#32553;&#12289;&#20449;&#24687;&#34701;&#21512;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#38754;&#30340;&#20248;&#32570;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2302.09019</link><description>&lt;p&gt;
&#35780;&#36848;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Tensor Networks Meet Neural Networks: A Survey and Future Perspectives. (arXiv:2302.09019v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#36848;&#20102;&#24352;&#37327;&#32593;&#32476;&#21644;&#31070;&#32463;&#32593;&#32476;&#24182;&#20171;&#32461;&#20102;&#23427;&#20204;&#30340;&#32467;&#21512;&#65306;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;(TNN)&#65292;&#25506;&#35752;&#20102;TNN&#22312;&#32593;&#32476;&#21387;&#32553;&#12289;&#20449;&#24687;&#34701;&#21512;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#26041;&#38754;&#30340;&#20248;&#32570;&#28857;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24352;&#37327;&#32593;&#32476;(TN)&#21644;&#31070;&#32463;&#32593;&#32476;(NN)&#26159;&#20004;&#31181;&#22522;&#26412;&#30340;&#25968;&#25454;&#24314;&#27169;&#26041;&#27861;&#12290;TN&#26088;&#22312;&#36890;&#36807;&#23558;&#25351;&#25968;&#32500;&#24230;&#36716;&#21270;&#20026;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#26469;&#35299;&#20915;&#22788;&#29702;&#22823;&#35268;&#27169;&#24352;&#37327;&#30340;&#32500;&#24230;&#28798;&#38590;&#65292;&#24182;&#21560;&#24341;&#20102;&#29289;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;NN&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#26426;&#22120;&#20154;&#30740;&#31350;&#31561;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23613;&#31649;&#36825;&#20004;&#31181;&#32593;&#32476;&#28304;&#20110;&#19981;&#21516;&#30340;&#35266;&#23519;&#65292;&#20294;&#23427;&#20204;&#36890;&#36807;TNN (&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;)&#27010;&#24565;&#30340;&#32467;&#21512;&#26377;&#30528;&#20869;&#22312;&#30340;&#32852;&#31995;&#65292;&#20849;&#21516;&#25903;&#25745;&#30528;&#22810;&#32447;&#24615;&#32467;&#26500;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TNN&#30340;&#19977;&#20010;&#26041;&#38754;&#65306;&#32593;&#32476;&#21387;&#32553;&#12289;&#20449;&#24687;&#34701;&#21512;&#21644;&#37327;&#23376;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#65292;&#24182;&#38416;&#36848;&#20102;TNN&#19982;&#20854;&#20182;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#24046;&#24322;&#20197;&#21450;TNN&#30340;&#20248;&#28857;&#12289;&#23616;&#38480;&#24615;&#21450;&#20854;&#23545;&#20110;&#19981;&#21516;&#24212;&#29992;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;TNN&#30340;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor networks (TNs) and neural networks (NNs) are two fundamental data modeling approaches. TNs were introduced to solve the curse of dimensionality in large-scale tensors by converting an exponential number of dimensions to polynomial complexity. As a result, they have attracted significant attention in the fields of quantum physics and machine learning. Meanwhile, NNs have displayed exceptional performance in various applications, e.g., computer vision, natural language processing, and robotics research. Interestingly, although these two types of networks originate from different observations, they are inherently linked through the common multilinearity structure underlying both TNs and NNs, thereby motivating a significant number of intellectual developments regarding combinations of TNs and NNs. In this paper, we refer to these combinations as tensorial neural networks (TNNs), and present an introduction to TNNs in three primary aspects: network compression, information fusion, a
&lt;/p&gt;</description></item><item><title>&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.02173</link><description>&lt;p&gt;
&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning based Time Series Analysis with Frequency Transformation. (arXiv:2302.02173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02173
&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#22312;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#26412;&#25991;&#31995;&#32479;&#22238;&#39038;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;FT&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20248;&#21183;&#12289;&#38480;&#21046;&#20197;&#21450;&#20027;&#35201;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39057;&#29575;&#21464;&#25442;&#65288;FT&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#32435;&#20837;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#39057;&#29575;&#21464;&#25442;&#30340;&#20248;&#21183;&#65292;&#22914;&#39640;&#25928;&#24615;&#21644;&#20840;&#23616;&#35270;&#35282;&#65292;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#34987;&#36805;&#36895;&#25506;&#32034;&#21644;&#21033;&#29992;&#65292;&#23637;&#31034;&#20102;&#39057;&#29575;&#21464;&#25442;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#22312;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#21463;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#30740;&#31350;&#65292;&#20294;&#30446;&#21069;&#36824;&#32570;&#20047;&#23545;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#30340;&#31995;&#32479;&#22238;&#39038;&#21644;&#28145;&#20837;&#20998;&#26512;&#12290;&#30446;&#21069;&#36824;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#39057;&#29575;&#21464;&#25442;&#21487;&#20197;&#25552;&#21319;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25928;&#26524;&#65292;&#20197;&#21450;&#23427;&#22312;&#35813;&#39046;&#22495;&#30340;&#38480;&#21046;&#26159;&#20160;&#20040;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20221;&#20840;&#38754;&#30340;&#32508;&#36848;&#65292;&#31995;&#32479;&#35843;&#26597;&#21644;&#24635;&#32467;&#20102;&#22522;&#20110;&#39057;&#29575;&#21464;&#25442;&#30340;&#28145;&#24230;&#23398;&#20064;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20027;&#35201;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, frequency transformation (FT) has been increasingly incorporated into deep learning models to significantly enhance state-of-the-art accuracy and efficiency in time series analysis. The advantages of FT, such as high efficiency and a global view, have been rapidly explored and exploited in various time series tasks and applications, demonstrating the promising potential of FT as a new deep learning paradigm for time series analysis. Despite the growing attention and the proliferation of research in this emerging field, there is currently a lack of a systematic review and in-depth analysis of deep learning-based time series models with FT. It is also unclear why FT can enhance time series analysis and what its limitations in the field are. To address these gaps, we present a comprehensive review that systematically investigates and summarizes the recent research advancements in deep learning-based time series analysis with FT. Specifically, we explore the primary approaches us
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#20113;&#26381;&#21153;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;&#20110;&#20113;API&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#20102;&#25915;&#20987;&#26102;&#38388;&#30340;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2210.16371</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#20113;&#26381;&#21153;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Distributed Black-box Attack against Image Classification Cloud Services. (arXiv:2210.16371v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.16371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#20113;&#26381;&#21153;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#30452;&#25509;&#24212;&#29992;&#20110;&#20113;API&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#65292;&#36991;&#20813;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#30340;&#38169;&#35823;&#65292;&#24182;&#21033;&#29992;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#20102;&#25915;&#20987;&#26102;&#38388;&#30340;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40657;&#30418;&#23545;&#25239;&#25915;&#20987;&#21487;&#20197;&#27450;&#39575;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#22312;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#32467;&#26500;&#21644;&#26435;&#37325;&#30340;&#24773;&#20917;&#19979;&#23545;&#22270;&#20687;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25253;&#21578;&#20102;&#36229;&#36807;95%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#26597;&#35810;&#27425;&#25968;&#23569;&#20110;1000&#27425;&#12290;&#28982;&#21518;&#20135;&#29983;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#40657;&#30418;&#25915;&#20987;&#26159;&#21542;&#24050;&#32463;&#25104;&#20026;&#20381;&#36182;&#20113;API&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#30495;&#27491;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#20043;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#39640;&#25104;&#21151;&#29575;&#21644;&#20943;&#23569;&#26597;&#35810;&#27425;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#40657;&#30418;&#25915;&#20987;&#20113;API&#32780;&#35328;&#65292;&#25915;&#20987;&#25152;&#38656;&#30340;&#26102;&#38388;&#20063;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#12290;&#26412;&#25991;&#23558;&#40657;&#30418;&#25915;&#20987;&#30452;&#25509;&#24212;&#29992;&#20110;&#20113;API&#65292;&#32780;&#19981;&#26159;&#26412;&#22320;&#27169;&#22411;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20043;&#21069;&#30740;&#31350;&#20013;&#22312;&#22270;&#20687;&#32534;&#30721;&#21644;&#39044;&#22788;&#29702;&#20043;&#21069;&#24212;&#29992;&#25200;&#21160;&#36896;&#25104;&#30340;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#36127;&#36733;&#24179;&#34913;&#23454;&#29616;&#20102;&#20998;&#24067;&#24335;&#40657;&#30418;&#25915;&#20987;&#65292;&#21487;&#20197;&#23558;&#25915;&#20987;&#26102;&#38388;&#32553;&#30701;&#32422;&#20116;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Black-box adversarial attacks can fool image classifiers into misclassifying images without requiring access to model structure and weights. Recent studies have reported attack success rates of over 95% with less than 1,000 queries. The question then arises of whether black-box attacks have become a real threat against IoT devices that rely on cloud APIs to achieve image classification. To shed some light on this, note that prior research has primarily focused on increasing the success rate and reducing the number of queries. However, another crucial factor for black-box attacks against cloud APIs is the time required to perform the attack. This paper applies black-box attacks directly to cloud APIs rather than to local models, thereby avoiding mistakes made in prior research that applied the perturbation before image encoding and pre-processing. Further, we exploit load balancing to enable distributed black-box attacks that can reduce the attack time by a factor of about five for both
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2202.09134</link><description>&lt;p&gt;
&#22312;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#24335;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation in the Underparameterized and Overparameterized Regimes. (arXiv:2202.09134v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09134
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#20999;&#37327;&#21270;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#32467;&#26524;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#20010;&#20855;&#20307;&#27169;&#22411;&#12290;&#32467;&#26524;&#35777;&#23454;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#30340;&#19968;&#20123;&#35266;&#23519;&#65292;&#20294;&#20063;&#24471;&#20986;&#20102;&#24847;&#22806;&#30340;&#21457;&#29616;&#65306;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#32780;&#19981;&#26159;&#20943;&#23569;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#27604;&#22914;&#32463;&#39564;&#39044;&#27979;&#39118;&#38505;&#12290;&#23427;&#21487;&#20197;&#20805;&#24403;&#27491;&#21017;&#21270;&#22120;&#65292;&#20294;&#22312;&#26576;&#20123;&#39640;&#32500;&#38382;&#39064;&#20013;&#21364;&#26080;&#27861;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#25913;&#21464;&#32463;&#39564;&#39118;&#38505;&#30340;&#21452;&#37325;&#19979;&#38477;&#23792;&#20540;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20998;&#26512;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#34987;&#36171;&#20104;&#30340;&#20960;&#20010;&#23646;&#24615;&#35201;&#20040;&#26159;&#30495;&#30340;&#65292;&#35201;&#20040;&#26159;&#20551;&#30340;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#30340;&#32452;&#21512;-&#29305;&#21035;&#26159;&#25968;&#25454;&#20998;&#24067;&#65292;&#20272;&#35745;&#22120;&#30340;&#23646;&#24615;&#20197;&#21450;&#26679;&#26412;&#22823;&#23567;&#65292;&#22686;&#24378;&#25968;&#37327;&#21644;&#32500;&#25968;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#24037;&#20855;&#26159;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide results that exactly quantify how data augmentation affects the variance and limiting distribution of estimates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. Our main theoretical tool is a limit theorem for functions of randomly transformed, high-dimensional random vectors. The proof draws on 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#31616;&#21333;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;&#12290;&#36890;&#36807;&#24341;&#20837;&#24207;&#36143;&#26680;&#23884;&#20837;&#25216;&#26415;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22240;&#26524;&#20272;&#35745;&#30340;&#31616;&#21270;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#21644;&#26222;&#36866;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.03950</link><description>&lt;p&gt;
&#24207;&#36143;&#26680;&#23884;&#20837;&#29992;&#20110;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sequential Kernel Embedding for Mediated and Time-Varying Dose Response Curves. (arXiv:2111.03950v4 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.03950
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#31616;&#21333;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20272;&#35745;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;&#12290;&#36890;&#36807;&#24341;&#20837;&#24207;&#36143;&#26680;&#23884;&#20837;&#25216;&#26415;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#22240;&#26524;&#20272;&#35745;&#30340;&#31616;&#21270;&#12290;&#36890;&#36807;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#20272;&#35745;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#26680;&#23725;&#22238;&#24402;&#30340;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;&#30340;&#31616;&#21333;&#38750;&#21442;&#25968;&#20272;&#35745;&#22120;&#12290;&#36890;&#36807;&#23884;&#20837;Pearl&#30340;&#20171;&#23548;&#20844;&#24335;&#21644;Robins&#30340;g&#20844;&#24335;&#19982;&#26680;&#20989;&#25968;&#65292;&#25105;&#20204;&#20801;&#35768;&#22788;&#29702;&#12289;&#20171;&#23548;&#32773;&#21644;&#21327;&#21464;&#37327;&#22312;&#19968;&#33324;&#31354;&#38388;&#20013;&#36830;&#32493;&#21464;&#21270;&#65292;&#20063;&#20801;&#35768;&#38750;&#32447;&#24615;&#30340;&#22788;&#29702;-&#28151;&#28102;&#22240;&#32032;&#21453;&#39304;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#19968;&#31181;&#31216;&#20026;&#24207;&#36143;&#26680;&#23884;&#20837;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25216;&#26415;&#65292;&#25105;&#20204;&#20351;&#29992;&#23427;&#26469;&#26500;&#24314;&#22797;&#26434;&#22240;&#26524;&#20272;&#35745;&#30340;&#31616;&#21333;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#30340;&#20272;&#35745;&#22120;&#20445;&#30041;&#20102;&#32463;&#20856;&#35782;&#21035;&#30340;&#26222;&#36866;&#24615;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#38750;&#28176;&#36827;&#22343;&#21248;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#35768;&#22810;&#21327;&#21464;&#37327;&#30340;&#38750;&#32447;&#24615;&#27169;&#25311;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20272;&#35745;&#20102;&#32654;&#22269;&#32844;&#19994;&#35757;&#32451;&#22242;&#30340;&#20171;&#23548;&#21644;&#26102;&#21464;&#21058;&#37327;&#21709;&#24212;&#26354;&#32447;&#65292;&#24182;&#28165;&#27905;&#21487;&#33021;&#25104;&#20026;&#26410;&#26469;&#24037;&#20316;&#22522;&#20934;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#20171;&#23548;&#21644;&#26102;&#21464;&#22788;&#29702;&#25928;&#24212;&#20197;&#21450;&#21453;&#20107;&#23454;&#20998;&#24067;&#65292;&#39564;&#35777;&#20102;&#21322;&#21442;&#25968;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose simple nonparametric estimators for mediated and time-varying dose response curves based on kernel ridge regression. By embedding Pearl's mediation formula and Robins' g-formula with kernels, we allow treatments, mediators, and covariates to be continuous in general spaces, and also allow for nonlinear treatment-confounder feedback. Our key innovation is a reproducing kernel Hilbert space technique called sequential kernel embedding, which we use to construct simple estimators for complex causal estimands. Our estimators preserve the generality of classic identification while also achieving nonasymptotic uniform rates. In nonlinear simulations with many covariates, we demonstrate strong performance. We estimate mediated and time-varying dose response curves of the US Job Corps, and clean data that may serve as a benchmark in future work. We extend our results to mediated and time-varying treatment effects and counterfactual distributions, verifying semiparametric efficiency 
&lt;/p&gt;</description></item></channel></rss>