<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#65292;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#12289;&#36229;&#24102;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.14695</link><description>&lt;p&gt;
&#38024;&#23545;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#38142;&#24335;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Chain-structured neural architecture search for financial time series forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14695
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#39046;&#22495;&#65292;&#27604;&#36739;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#12289;&#36229;&#24102;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38142;&#24335;&#25628;&#32034;&#31354;&#38388;&#19978;&#27604;&#36739;&#20102;&#19977;&#31181;&#27969;&#34892;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31574;&#30053;&#65306;&#36125;&#21494;&#26031;&#20248;&#21270;&#12289;&#36229;&#24102;&#26041;&#27861;&#21644;&#24378;&#21270;&#23398;&#20064;&#65292;&#36825;&#22312;&#37329;&#34701;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32972;&#26223;&#19979;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14695v1 Announce Type: cross  Abstract: We compare three popular neural architecture search strategies on chain-structured search spaces: Bayesian optimization, the hyperband method, and reinforcement learning in the context of financial time series forecasting.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.12712</link><description>&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#21464;&#24418;&#35299;&#20915;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Source Scale Bias via Image Warping for Domain Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12712
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#22788;&#29702;&#65292;&#20197;&#21450;&#38024;&#23545;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#26377;&#25928;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#35782;&#21035;&#20013;&#65292;&#30001;&#20110;&#30495;&#23454;&#22330;&#26223;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#21644;&#22270;&#20687;&#22823;&#23567;&#20998;&#24067;&#30340;&#19981;&#24179;&#34913;&#65292;&#23610;&#24230;&#20559;&#24046;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#21253;&#25324;&#27880;&#20837;&#23610;&#24230;&#19981;&#21464;&#24615;&#20808;&#39564;&#12289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#23610;&#24230;&#36827;&#34892;&#36807;&#37319;&#26679;&#65292;&#25110;&#32773;&#22312;&#25512;&#26029;&#26102;&#35843;&#25972;&#23610;&#24230;&#12290;&#34429;&#28982;&#36825;&#20123;&#31574;&#30053;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20943;&#36731;&#20102;&#23610;&#24230;&#20559;&#24046;&#65292;&#20294;&#23427;&#20204;&#22312;&#36328;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#26102;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#20250;&#22686;&#21152;&#35757;&#32451;&#36807;&#31243;&#30340;&#35745;&#31639;&#36127;&#36733;&#21644;&#25512;&#26029;&#36807;&#31243;&#30340;&#24310;&#36831;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#27880;&#24847;&#21147;&#22788;&#29702;&#8212;&#8212;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23601;&#22320;&#25197;&#26354;&#22270;&#20687;&#26469;&#23545;&#31361;&#20986;&#30340;&#23545;&#35937;&#21306;&#22495;&#36827;&#34892;&#36807;&#37319;&#26679;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#25913;&#21464;&#28304;&#23610;&#24230;&#20998;&#24067;&#21487;&#20197;&#25913;&#21892;&#20027;&#24178;&#29305;&#24449;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38754;&#21521;&#23545;&#35937;&#21306;&#22495;&#37319;&#26679;&#30340;&#23454;&#20363;&#32423;&#21464;&#24418;&#24341;&#23548;&#65292;&#20197;&#20943;&#36731;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28304;&#23610;&#24230;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#23545;&#22320;&#29702;&#12289;&#20809;&#29031;&#21644;&#22825;&#27668;&#26465;&#20214;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12712v1 Announce Type: cross  Abstract: In visual recognition, scale bias is a key challenge due to the imbalance of object and image size distribution inherent in real scene datasets. Conventional solutions involve injecting scale invariance priors, oversampling the dataset at different scales during training, or adjusting scale at inference. While these strategies mitigate scale bias to some extent, their ability to adapt across diverse datasets is limited. Besides, they increase computational load during training and latency during inference. In this work, we use adaptive attentional processing -- oversampling salient object regions by warping images in-place during training. Discovering that shifting the source scale distribution improves backbone features, we developed a instance-level warping guidance aimed at object region sampling to mitigate source scale bias in domain adaptation. Our approach improves adaptation across geographies, lighting and weather conditions, 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10182</link><description>&lt;p&gt;
&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10182
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#31609;&#23398;(OR)&#20013;&#65292;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#39046;&#22495;&#30340;&#20986;&#33394;&#24615;&#33021;&#20351;&#20854;&#22312;OR&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;OOD&#25968;&#25454;&#26102;&#65292;NNs&#24448;&#24448;&#20250;&#20570;&#20986;&#33258;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20026;&#33258;&#20449;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#24403;&#36755;&#20986;&#24212;(&#19981;&#24212;)&#34987;&#20449;&#20219;&#26102;&#36827;&#34892;&#36890;&#20449;&#12290;&#22240;&#27492;&#65292;&#22312;OR&#39046;&#22495;&#20013;&#65292;NNs&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#22810;&#20010;&#29420;&#31435;NNs&#32452;&#25104;&#30340;&#28145;&#24230;&#38598;&#21512;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#36824;&#33021;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#30001;&#20110;&#36739;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22522;&#30784;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;NN&#38598;&#25104;&#65292;&#21363;sna
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10182v1 Announce Type: new  Abstract: In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the sna
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#20851;&#27880;&#25552;&#39640;&#21484;&#22238;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.09548</link><description>&lt;p&gt;
&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#37325;&#28857;&#20943;&#23569;&#20551;&#38452;&#24615;&#21644;&#20351;&#29992; SHAP &#36827;&#34892;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Breast Cancer Classification Using Gradient Boosting Algorithms Focusing on Reducing the False Negative and SHAP for Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#31639;&#27861;&#23545;&#20083;&#33146;&#30284;&#36827;&#34892;&#20998;&#31867;&#65292;&#20851;&#27880;&#25552;&#39640;&#21484;&#22238;&#29575;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26816;&#27979;&#21644;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30284;&#30151;&#26159;&#19990;&#30028;&#19978;&#22842;&#36208;&#26368;&#22810;&#22899;&#24615;&#29983;&#21629;&#30340;&#30142;&#30149;&#20043;&#19968;&#65292;&#20854;&#20013;&#20083;&#33146;&#30284;&#21344;&#25454;&#20102;&#30284;&#30151;&#30149;&#20363;&#21644;&#27515;&#20129;&#20154;&#25968;&#26368;&#39640;&#30340;&#20301;&#32622;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#26089;&#26399;&#26816;&#27979;&#21487;&#20197;&#39044;&#38450;&#20083;&#33146;&#30284;&#65292;&#20174;&#32780;&#36827;&#34892;&#26089;&#26399;&#27835;&#30103;&#12290;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22312;&#30284;&#30151;&#39044;&#27979;&#20013;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#20294;&#26377;&#26102;&#20165;&#20381;&#38752;&#20934;&#30830;&#24615;&#21487;&#33021;&#24182;&#38750;&#22987;&#32456;&#21487;&#38752;&#12290;&#26412;&#30740;&#31350;&#23545;&#20351;&#29992;&#25552;&#21319;&#25216;&#26415;&#22522;&#20110;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#20083;&#33146;&#30284;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35843;&#26597;&#24615;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#21484;&#22238;&#29575;&#25351;&#26631;&#12290;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24050;&#34987;&#35777;&#26126;&#26159;&#26816;&#27979;&#21307;&#23398;&#30142;&#30149;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#21033;&#29992;&#21152;&#24030;&#22823;&#23398;&#23572;&#28286;&#20998;&#26657; (UCI) &#25968;&#25454;&#38598;&#23545;&#35757;&#32451;&#21644;&#27979;&#35797;&#27169;&#22411;&#20998;&#31867;&#22120;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#21253;&#21547;&#21508;&#33258;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09548v1 Announce Type: new  Abstract: Cancer is one of the diseases that kill the most women in the world, with breast cancer being responsible for the highest number of cancer cases and consequently deaths. However, it can be prevented by early detection and, consequently, early treatment. Any development for detection or perdition this kind of cancer is important for a better healthy life. Many studies focus on a model with high accuracy in cancer prediction, but sometimes accuracy alone may not always be a reliable metric. This study implies an investigative approach to studying the performance of different machine learning algorithms based on boosting to predict breast cancer focusing on the recall metric. Boosting machine learning algorithms has been proven to be an effective tool for detecting medical diseases. The dataset of the University of California, Irvine (UCI) repository has been utilized to train and test the model classifier that contains their attributes. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L-BFGS-B&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22312;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#35782;&#21035;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#30456;&#27604;&#20256;&#32479;&#32447;&#24615;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26524;&#12289;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#20351;&#29992;&#30340;&#36890;&#29992;&#24615;&#20197;&#21450;&#25968;&#20540;&#31283;&#23450;&#24615;&#26041;&#38754;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.03827</link><description>&lt;p&gt;
&#22312;L-BFGS-B&#31639;&#27861;&#30340;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#36827;&#34892;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Linear and nonlinear system identification under $\ell_1$- and group-Lasso regularization via L-BFGS-B
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03827
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L-BFGS-B&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#22312;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#35782;&#21035;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31995;&#32479;&#65292;&#30456;&#27604;&#20256;&#32479;&#32447;&#24615;&#23376;&#31354;&#38388;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26524;&#12289;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#20351;&#29992;&#30340;&#36890;&#29992;&#24615;&#20197;&#21450;&#25968;&#20540;&#31283;&#23450;&#24615;&#26041;&#38754;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L-BFGS-B&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#21487;&#33021;&#22312;$\ell_1$&#21644;group-Lasso&#27491;&#21017;&#21270;&#19979;&#30340;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#31163;&#25955;&#26102;&#38388;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#38024;&#23545;&#32447;&#24615;&#27169;&#22411;&#30340;&#35782;&#21035;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#32463;&#20856;&#32447;&#24615;&#23376;&#31354;&#38388;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#36890;&#24120;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#22312;&#25439;&#22833;&#21644;&#27491;&#21017;&#21270;&#39033;&#30340;&#20351;&#29992;&#26041;&#38754;&#26356;&#21152;&#36890;&#29992;&#65292;&#20063;&#22312;&#25968;&#20540;&#19978;&#26356;&#21152;&#31283;&#23450;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#20016;&#23500;&#20102;&#29616;&#26377;&#30340;&#32447;&#24615;&#31995;&#32479;&#35782;&#21035;&#24037;&#20855;&#38598;&#65292;&#36824;&#21487;&#20197;&#24212;&#29992;&#20110;&#35782;&#21035;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#38750;&#24120;&#24191;&#27867;&#30340;&#21442;&#25968;&#21270;&#38750;&#32447;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#39564;&#25968;&#25454;&#38598;&#19978;&#28436;&#31034;&#20102;&#35813;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#35299;&#20915;Weigand&#31561;&#20154;&#65288;2022&#24180;&#65289;&#25552;&#20986;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#19994;&#26426;&#22120;&#20154;&#22522;&#20934;&#30340;&#38750;&#32447;&#24615;&#22810;&#36755;&#20837;/&#22810;&#36755;&#20986;&#31995;&#32479;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03827v1 Announce Type: cross  Abstract: In this paper, we propose an approach for identifying linear and nonlinear discrete-time state-space models, possibly under $\ell_1$- and group-Lasso regularization, based on the L-BFGS-B algorithm. For the identification of linear models, we show that, compared to classical linear subspace methods, the approach often provides better results, is much more general in terms of the loss and regularization terms used, and is also more stable from a numerical point of view. The proposed method not only enriches the existing set of linear system identification tools but can be also applied to identifying a very broad class of parametric nonlinear state-space models, including recurrent neural networks. We illustrate the approach on synthetic and experimental datasets and apply it to solve the challenging industrial robot benchmark for nonlinear multi-input/multi-output system identification proposed by Weigand et al. (2022). A Python impleme
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;</title><link>https://arxiv.org/abs/2402.17826</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Prediction-Powered Ranking of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17826
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#65292;&#21487;&#20197;&#34913;&#37327;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#27979;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#26681;&#25454;&#20854;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#19968;&#33268;&#24615;&#27700;&#24179;&#36827;&#34892;&#25490;&#21517;--&#22914;&#26524;&#19968;&#20010;&#27169;&#22411;&#30340;&#36755;&#20986;&#26356;&#21463;&#20154;&#31867;&#20559;&#22909;&#65292;&#37027;&#20040;&#23427;&#23601;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#22909;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#35745;&#26694;&#26550;&#26469;&#24357;&#21512;&#20154;&#31867;&#19982;&#27169;&#22411;&#20559;&#22909;&#20043;&#38388;&#21487;&#33021;&#24341;&#20837;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17826v1 Announce Type: cross  Abstract: Large language models are often ranked according to their level of alignment with human preferences -- a model is better than other models if its outputs are more frequently preferred by humans. One of the most popular ways to elicit human preferences utilizes pairwise comparisons between the outputs provided by different models to the same inputs. However, since gathering pairwise comparisons by humans is costly and time-consuming, it has become a very common practice to gather pairwise comparisons by a strong large language model -- a model strongly aligned with human preferences. Surprisingly, practitioners cannot currently measure the uncertainty that any mismatch between human and model preferences may introduce in the constructed rankings. In this work, we develop a statistical framework to bridge this gap. Given a small set of pairwise comparisons by humans and a large set of pairwise comparisons by a model, our framework provid
&lt;/p&gt;</description></item><item><title>CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.17363</link><description>&lt;p&gt;
CGGM&#65306;&#19968;&#31181;&#20855;&#26377;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#30340;&#26465;&#20214;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CGGM: A conditional graph generation model with adaptive sparsity for node anomaly detection in IoT networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17363
&lt;/p&gt;
&lt;p&gt;
CGGM&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#65292;&#35299;&#20915;&#20102;&#29289;&#32852;&#32593;&#32593;&#32476;&#20013;&#33410;&#28857;&#24322;&#24120;&#26816;&#27979;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#26816;&#27979;&#29289;&#32852;&#32593;&#20013;&#33410;&#28857;&#30340;&#24322;&#24120;&#34892;&#20026;&#12290;&#29983;&#25104;&#27169;&#22411;&#36890;&#24120;&#29992;&#20110;&#35299;&#20915;&#21160;&#24577;&#22270;&#20013;&#33410;&#28857;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30340;&#32422;&#26463;&#21253;&#25324;&#37051;&#25509;&#20851;&#31995;&#30340;&#21333;&#35843;&#24615;&#65292;&#20026;&#33410;&#28857;&#26500;&#24314;&#22810;&#32500;&#29305;&#24449;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#32570;&#20047;&#31471;&#21040;&#31471;&#29983;&#25104;&#22810;&#31867;&#33410;&#28857;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CGGM&#30340;&#26032;&#39062;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29983;&#25104;&#23569;&#25968;&#31867;&#21035;&#20013;&#26356;&#22810;&#33410;&#28857;&#12290;&#36890;&#36807;&#33258;&#36866;&#24212;&#31232;&#30095;&#24615;&#29983;&#25104;&#37051;&#25509;&#30697;&#38453;&#30340;&#26426;&#21046;&#22686;&#24378;&#20102;&#20854;&#32467;&#26500;&#30340;&#28789;&#27963;&#24615;&#12290;&#29305;&#24449;&#29983;&#25104;&#27169;&#22359;&#21517;&#20026;&#22810;&#32500;&#29305;&#24449;&#29983;&#25104;&#22120;&#65288;MFG&#65289;&#65292;&#21487;&#29983;&#25104;&#21253;&#25324;&#25299;&#25169;&#20449;&#24687;&#22312;&#20869;&#30340;&#33410;&#28857;&#29305;&#24449;&#12290;&#26631;&#31614;&#34987;&#36716;&#25442;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#29992;&#20316;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17363v1 Announce Type: cross  Abstract: Dynamic graphs are extensively employed for detecting anomalous behavior in nodes within the Internet of Things (IoT). Generative models are often used to address the issue of imbalanced node categories in dynamic graphs. Nevertheless, the constraints it faces include the monotonicity of adjacency relationships, the difficulty in constructing multi-dimensional features for nodes, and the lack of a method for end-to-end generation of multiple categories of nodes. This paper presents a novel graph generation model, called CGGM, designed specifically to generate a larger number of nodes belonging to the minority class. The mechanism for generating an adjacency matrix, through adaptive sparsity, enhances flexibility in its structure. The feature generation module, called multidimensional features generator (MFG) to generate node features along with topological information. Labels are transformed into embedding vectors, serving as condition
&lt;/p&gt;</description></item><item><title>DynaMITE-RL &#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#30340;&#28508;&#22312;&#20449;&#24687;&#12289;&#20250;&#35805;&#25513;&#30721;&#21644;&#20808;&#39564;&#28508;&#22312;&#26465;&#20214;&#31561;&#20851;&#38190;&#20462;&#25913;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26356;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25512;&#29702;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.15957</link><description>&lt;p&gt;
DynaMITE-RL: &#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#29992;&#20110;&#25913;&#36827;&#26102;&#38388;&#20803;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DynaMITE-RL: A Dynamic Model for Improved Temporal Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15957
&lt;/p&gt;
&lt;p&gt;
DynaMITE-RL &#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#27169;&#22411;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#33268;&#24615;&#30340;&#28508;&#22312;&#20449;&#24687;&#12289;&#20250;&#35805;&#25513;&#30721;&#21644;&#20808;&#39564;&#28508;&#22312;&#26465;&#20214;&#31561;&#20851;&#38190;&#20462;&#25913;&#65292;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#22522;&#32447;&#26356;&#20248;&#24322;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25512;&#29702;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;DynaMITE-RL&#65292;&#36825;&#26159;&#19968;&#31181;&#20803;&#24378;&#21270;&#23398;&#20064;(meta-RL)&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#28508;&#22312;&#29366;&#24577;&#20197;&#19981;&#21516;&#36895;&#29575;&#28436;&#21464;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#36817;&#20284;&#25512;&#29702;&#12290;&#25105;&#20204;&#24314;&#27169;&#21095;&#38598;&#20250;&#35805; - &#21095;&#38598;&#30340;&#37096;&#20998;&#65292;&#22312;&#36825;&#20123;&#37096;&#20998;&#20013;&#65292;&#28508;&#22312;&#29366;&#24577;&#26159;&#22266;&#23450;&#30340; - &#24182;&#23545;&#29616;&#26377;&#30340;meta-RL&#26041;&#27861;&#25552;&#20986;&#20102;&#19977;&#20010;&#20851;&#38190;&#20462;&#25913;&#65306;&#21095;&#38598;&#20869;&#37096;&#28508;&#22312;&#20449;&#24687;&#30340;&#19968;&#33268;&#24615;&#65292;&#20250;&#35805;&#25513;&#30721;&#21644;&#20808;&#39564;&#28508;&#22312;&#26465;&#20214;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#39046;&#22495;&#23637;&#31034;&#20102;&#36825;&#20123;&#20462;&#25913;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#31163;&#25955;Gridworld&#29615;&#22659;&#21040;&#36830;&#32493;&#25511;&#21046;&#21644;&#27169;&#25311;&#26426;&#22120;&#20154;&#36741;&#21161;&#20219;&#21153;&#65292;&#35777;&#26126;&#20102;DynaMITE-RL&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#25512;&#29702;&#32467;&#26524;&#26041;&#38754;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15957v1 Announce Type: new  Abstract: We introduce DynaMITE-RL, a meta-reinforcement learning (meta-RL) approach to approximate inference in environments where the latent state evolves at varying rates. We model episode sessions - parts of the episode where the latent state is fixed - and propose three key modifications to existing meta-RL methods: consistency of latent information within sessions, session masking, and prior latent conditioning. We demonstrate the importance of these modifications in various domains, ranging from discrete Gridworld environments to continuous-control and simulated robot assistive tasks, demonstrating that DynaMITE-RL significantly outperforms state-of-the-art baselines in sample efficiency and inference returns.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14400</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Modeling 3D Infant Kinetics Using Adaptive Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14400
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#65292;&#21033;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#23545;3D&#23156;&#20799;&#21160;&#21147;&#23398;&#36827;&#34892;&#24314;&#27169;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#35780;&#20272;&#26041;&#27861;&#23545;&#20110;&#26089;&#26399;&#21457;&#29616;&#21487;&#33021;&#38656;&#35201;&#21450;&#26102;&#24178;&#39044;&#30340;&#21307;&#23398;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#21457;&#30340;&#36816;&#21160;&#27963;&#21160;&#65292;&#21363;&#8220;&#21160;&#21147;&#23398;&#8221;&#65292;&#34987;&#35777;&#26126;&#21487;&#25552;&#20379;&#19968;&#20010;&#24378;&#26377;&#21147;&#30340;&#39044;&#27979;&#26410;&#26469;&#31070;&#32463;&#21457;&#32946;&#30340;&#26367;&#20195;&#24615;&#27979;&#37327;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#35780;&#20272;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#23450;&#24615;&#21644;&#20027;&#35266;&#30340;&#65292;&#20391;&#37325;&#20110;&#23545;&#36890;&#36807;&#35270;&#35273;&#35782;&#21035;&#30340;&#29305;&#23450;&#24180;&#40836;&#25163;&#21183;&#30340;&#25551;&#36848;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#26681;&#25454;&#25968;&#25454;&#39537;&#21160;&#35780;&#20272;&#20010;&#20307;&#21160;&#20316;&#27169;&#24335;&#26469;&#39044;&#27979;&#23156;&#20799;&#31070;&#32463;&#21457;&#32946;&#25104;&#29087;&#12290;&#25105;&#20204;&#21033;&#29992;&#22788;&#29702;&#36807;&#30340;3D&#23156;&#20799;&#35270;&#39057;&#24405;&#20687;&#36827;&#34892;&#23039;&#21183;&#20272;&#35745;&#65292;&#25552;&#21462;&#35299;&#21078;&#26631;&#24535;&#29289;&#30340;&#26102;&#31354;&#31995;&#21015;&#65292;&#24182;&#24212;&#29992;&#33258;&#36866;&#24212;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#23454;&#38469;&#24180;&#40836;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#23545;&#20110;&#22522;&#20110;&#25163;&#21160;&#35774;&#35745;&#29305;&#24449;&#30340;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#22522;&#32447;&#21462;&#24471;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14400v1 Announce Type: cross  Abstract: Reliable methods for the neurodevelopmental assessment of infants are essential for early detection of medical issues that may need prompt interventions. Spontaneous motor activity, or `kinetics', is shown to provide a powerful surrogate measure of upcoming neurodevelopment. However, its assessment is by and large qualitative and subjective, focusing on visually identified, age-specific gestures. Here, we follow an alternative approach, predicting infants' neurodevelopmental maturation based on data-driven evaluation of individual motor patterns. We utilize 3D video recordings of infants processed with pose-estimation to extract spatio-temporal series of anatomical landmarks, and apply adaptive graph convolutional networks to predict the actual age. We show that our data-driven approach achieves improvement over traditional machine learning baselines based on manually engineered features.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2402.04376</link><description>&lt;p&gt;
&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#30340;&#25193;&#23637;&#35268;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for learning with real and surrogate data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#26367;&#20195;&#25968;&#25454;&#19982;&#30495;&#23454;&#25968;&#25454;&#25972;&#21512;&#20197;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#21457;&#29616;&#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#27979;&#35797;&#35823;&#24046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#28151;&#21512;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25910;&#38598;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#25104;&#26412;&#26114;&#36149;&#25110;&#19981;&#20999;&#23454;&#38469;&#30340;&#33539;&#22260;&#20869;, &#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#30456;&#21453;&#22320;, &#21487;&#20197;&#23558;&#26469;&#33258;&#30446;&#26631;&#20998;&#24067;&#30340;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19982;&#26469;&#33258;&#20844;&#20849;&#25968;&#25454;&#38598;&#12289;&#19981;&#21516;&#24773;&#20917;&#19979;&#25910;&#38598;&#30340;&#25968;&#25454;&#25110;&#30001;&#29983;&#25104;&#27169;&#22411;&#21512;&#25104;&#30340;&#25968;&#25454;&#30456;&#32467;&#21512;, &#20316;&#20026;&#26367;&#20195;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#26696;&#26469;&#23558;&#26367;&#20195;&#25968;&#25454;&#25972;&#21512;&#21040;&#35757;&#32451;&#20013;, &#24182;&#20351;&#29992;&#29702;&#35770;&#27169;&#22411;&#21644;&#23454;&#35777;&#30740;&#31350;&#25506;&#32034;&#20854;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65306;(i) &#25972;&#21512;&#26367;&#20195;&#25968;&#25454;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#21407;&#22987;&#20998;&#24067;&#30340;&#27979;&#35797;&#35823;&#24046;&#65307;(ii) &#20026;&#20102;&#33719;&#24471;&#36825;&#31181;&#25928;&#30410;, &#20351;&#29992;&#26368;&#20248;&#21152;&#26435;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38750;&#24120;&#20851;&#38190;&#65307;(iii) &#22312;&#28151;&#21512;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#21644;&#26367;&#20195;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#27979;&#35797;&#35823;&#24046;&#21487;&#20197;&#24456;&#22909;&#22320;&#29992;&#19968;&#20010;&#25193;&#23637;&#35268;&#24459;&#26469;&#25551;&#36848;&#12290;&#36825;&#21487;&#20197;&#29992;&#26469;&#39044;&#27979;&#26368;&#20248;&#21152;&#26435;&#21644;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collecting large quantities of high-quality data is often prohibitively expensive or impractical, and a crucial bottleneck in machine learning. One may instead augment a small set of $n$ data points from the target distribution with data from more accessible sources like public datasets, data collected under different circumstances, or synthesized by generative models. Blurring distinctions, we refer to such data as `surrogate data'.   We define a simple scheme for integrating surrogate data into training and use both theoretical models and empirical studies to explore its behavior. Our main findings are: $(i)$ Integrating surrogate data can significantly reduce the test error on the original distribution; $(ii)$ In order to reap this benefit, it is crucial to use optimally weighted empirical risk minimization; $(iii)$ The test error of models trained on mixtures of real and surrogate data is well described by a scaling law. This can be used to predict the optimal weighting and the gai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#26469;&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.01930</link><description>&lt;p&gt;
&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;
&lt;/p&gt;
&lt;p&gt;
Reducing Optimism Bias in Incomplete Cooperative Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#26469;&#20943;&#23569;&#19981;&#23436;&#20840;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20048;&#35266;&#20559;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#20316;&#21338;&#24328;&#29702;&#35770;&#22312;&#24403;&#20195;&#20154;&#24037;&#26234;&#33021;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#35299;&#37322;&#24615;&#26426;&#22120;&#23398;&#20064;&#12289;&#36164;&#28304;&#20998;&#37197;&#21644;&#21327;&#21516;&#20915;&#31574;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#25351;&#23450;&#19968;&#20010;&#21512;&#20316;&#21338;&#24328;&#38656;&#35201;&#20026;&#25351;&#25968;&#22810;&#20010;&#32852;&#30431;&#20998;&#37197;&#20215;&#20540;&#65292;&#24182;&#19988;&#22312;&#23454;&#36341;&#20013;&#33719;&#24471;&#19968;&#20010;&#32852;&#30431;&#20215;&#20540;&#21487;&#33021;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#31616;&#21333;&#22320;&#19981;&#20844;&#24320;&#26576;&#20123;&#32852;&#30431;&#30340;&#20215;&#20540;&#20250;&#24341;&#20837;&#20851;&#20110;&#20010;&#20307;&#23545;&#38598;&#20307;&#22823;&#32852;&#30431;&#30340;&#36129;&#29486;&#30340;&#27169;&#31946;&#24615;&#12290;&#36825;&#31181;&#27169;&#31946;&#24615;&#32463;&#24120;&#23548;&#33268;&#29609;&#23478;&#25345;&#26377;&#36807;&#20110;&#20048;&#35266;&#30340;&#26399;&#26395;&#65292;&#20854;&#28304;&#20110;&#20869;&#22312;&#20559;&#35265;&#25110;&#25112;&#30053;&#32771;&#34385;&#65292;&#36827;&#32780;&#24120;&#24120;&#23548;&#33268;&#38598;&#20307;&#35201;&#27714;&#36229;&#36807;&#23454;&#38469;&#30340;&#22823;&#32852;&#30431;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#25581;&#31034;&#32852;&#30431;&#20215;&#20540;&#30340;&#39034;&#24207;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#22320;&#32553;&#23567;&#21512;&#20316;&#21338;&#24328;&#20013;&#29609;&#23478;&#26399;&#26395;&#19982;&#21487;&#23454;&#29616;&#32467;&#26524;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cooperative game theory has diverse applications in contemporary artificial intelligence, including domains like interpretable machine learning, resource allocation, and collaborative decision-making. However, specifying a cooperative game entails assigning values to exponentially many coalitions, and obtaining even a single value can be resource-intensive in practice. Yet simply leaving certain coalition values undisclosed introduces ambiguity regarding individual contributions to the collective grand coalition. This ambiguity often leads to players holding overly optimistic expectations, stemming from either inherent biases or strategic considerations, frequently resulting in collective claims exceeding the actual grand coalition value. In this paper, we present a framework aimed at optimizing the sequence for revealing coalition values, with the overarching goal of efficiently closing the gap between players' expectations and achievable outcomes in cooperative games. Our contributio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#22238;&#28378;&#30340;&#24494;&#35843;&#26041;&#27861;OLOR&#65292;&#36890;&#36807;&#32467;&#21512;&#20248;&#21270;&#22120;&#21644;&#26435;&#37325;&#22238;&#28378;&#39033;&#65292;&#35299;&#20915;&#20102;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.10962</link><description>&lt;p&gt;
&#19968;&#27493;&#23398;&#20064;&#65292;&#19968;&#27493;&#35780;&#23457;
&lt;/p&gt;
&lt;p&gt;
One Step Learning, One Step Review. (arXiv:2401.10962v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#22238;&#28378;&#30340;&#24494;&#35843;&#26041;&#27861;OLOR&#65292;&#36890;&#36807;&#32467;&#21512;&#20248;&#21270;&#22120;&#21644;&#26435;&#37325;&#22238;&#28378;&#39033;&#65292;&#35299;&#20915;&#20102;&#23436;&#20840;&#24494;&#35843;&#26041;&#27861;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#25552;&#39640;&#20102;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#35270;&#35273;&#24494;&#35843;&#24050;&#32463;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#24403;&#21069;&#20027;&#27969;&#30340;&#26041;&#27861;&#8212;&#8212;&#23436;&#20840;&#24494;&#35843;&#65292;&#23384;&#22312;&#30693;&#35782;&#36951;&#24536;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#21482;&#19987;&#27880;&#20110;&#25311;&#21512;&#19979;&#28216;&#35757;&#32451;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26435;&#37325;&#22238;&#28378;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;OLOR&#65288;&#19968;&#27493;&#23398;&#20064;&#65292;&#19968;&#27493;&#35780;&#23457;&#65289;&#12290;OLOR&#23558;&#24494;&#35843;&#19982;&#20248;&#21270;&#22120;&#30456;&#32467;&#21512;&#65292;&#23558;&#26435;&#37325;&#22238;&#28378;&#39033;&#21152;&#20837;&#21040;&#27599;&#20010;&#27493;&#39588;&#30340;&#26435;&#37325;&#26356;&#26032;&#39033;&#20013;&#12290;&#36825;&#30830;&#20445;&#20102;&#19978;&#28216;&#21644;&#19979;&#28216;&#27169;&#22411;&#30340;&#26435;&#37325;&#33539;&#22260;&#30340;&#19968;&#33268;&#24615;&#65292;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#65292;&#24182;&#22686;&#24378;&#20102;&#24494;&#35843;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36880;&#23618;&#24809;&#32602;&#26041;&#27861;&#65292;&#36890;&#36807; penalty decay &#21644;&#19981;&#21516;&#30340;&#34928;&#20943;&#29575;&#26469;&#35843;&#25972;&#23618;&#30340;&#26435;&#37325;&#22238;&#28378;&#31243;&#24230;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#12289;&#35821;&#20041;&#20998;&#21106;&#21644;&#23454;&#20363;&#20998;&#21106;&#31561;&#21508;&#31181;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual fine-tuning has garnered significant attention with the rise of pre-trained vision models. The current prevailing method, full fine-tuning, suffers from the issue of knowledge forgetting as it focuses solely on fitting the downstream training set. In this paper, we propose a novel weight rollback-based fine-tuning method called OLOR (One step Learning, One step Review). OLOR combines fine-tuning with optimizers, incorporating a weight rollback term into the weight update term at each step. This ensures consistency in the weight range of upstream and downstream models, effectively mitigating knowledge forgetting and enhancing fine-tuning performance. In addition, a layer-wise penalty is presented to employ penalty decay and the diversified decay rate to adjust the weight rollback levels of layers for adapting varying downstream tasks. Through extensive experiments on various tasks such as image classification, object detection, semantic segmentation, and instance segmentation, we
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;</title><link>http://arxiv.org/abs/2310.01225</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#29616;&#20195;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65306;&#24433;&#21709;&#12289;&#21069;&#26223;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A path-norm toolkit for modern networks: consequences, promises and challenges. (arXiv:2310.01225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36866;&#29992;&#20110;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#65292;&#21487;&#20197;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#24674;&#22797;&#25110;&#36229;&#36234;&#20102;&#24050;&#30693;&#30340;&#36335;&#24452;&#33539;&#25968;&#30028;&#38480;&#65292;&#24182;&#25361;&#25112;&#20102;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#19968;&#20123;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23436;&#20840;&#33021;&#22815;&#21253;&#25324;&#20855;&#26377;&#20559;&#24046;&#12289;&#36339;&#36291;&#36830;&#25509;&#21644;&#26368;&#22823;&#27744;&#21270;&#30340;&#36890;&#29992;DAG ReLU&#32593;&#32476;&#30340;&#36335;&#24452;&#33539;&#25968;&#24037;&#20855;&#21253;&#12290;&#36825;&#20010;&#24037;&#20855;&#21253;&#19981;&#20165;&#36866;&#29992;&#20110;&#26368;&#24191;&#27867;&#30340;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#65292;&#36824;&#21487;&#20197;&#24674;&#22797;&#25110;&#36229;&#36234;&#24050;&#30693;&#30340;&#27492;&#31867;&#33539;&#25968;&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#12290;&#36825;&#20123;&#25193;&#23637;&#30340;&#36335;&#24452;&#33539;&#25968;&#36824;&#20139;&#26377;&#36335;&#24452;&#33539;&#25968;&#30340;&#24120;&#35268;&#20248;&#28857;&#65306;&#35745;&#31639;&#31616;&#20415;&#12289;&#23545;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20855;&#26377;&#19981;&#21464;&#24615;&#65292;&#22312;&#21069;&#39304;&#32593;&#32476;&#19978;&#27604;&#25805;&#20316;&#31526;&#33539;&#25968;&#30340;&#20056;&#31215;&#65288;&#21478;&#19968;&#31181;&#24120;&#29992;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65289;&#20855;&#26377;&#26356;&#22909;&#30340;&#38160;&#24230;&#12290;&#24037;&#20855;&#21253;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#26131;&#20110;&#23454;&#26045;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#25968;&#20540;&#35780;&#20272;&#22312;ImageNet&#19978;&#23545;ResNet&#30340;&#26368;&#23574;&#38160;&#30028;&#38480;&#26469;&#25361;&#25112;&#22522;&#20110;&#36335;&#24452;&#33539;&#25968;&#30340;&#20855;&#20307;&#25215;&#35834;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces the first toolkit around path-norms that is fully able to encompass general DAG ReLU networks with biases, skip connections and max pooling. This toolkit notably allows us to establish generalization bounds for real modern neural networks that are not only the most widely applicable path-norm based ones, but also recover or beat the sharpest known bounds of this type. These extended path-norms further enjoy the usual benefits of path-norms: ease of computation, invariance under the symmetries of the network, and improved sharpness on feedforward networks compared to the product of operators' norms, another complexity measure most commonly used.  The versatility of the toolkit and its ease of implementation allow us to challenge the concrete promises of path-norm-based generalization bounds, by numerically evaluating the sharpest known bounds for ResNets on ImageNet.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#30340;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#32467;&#21512;&#26234;&#33021;&#20998;&#26512;&#21644;&#22810;&#32452;&#20214;&#30340;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26399;&#33410;&#33021;&#21644;&#20248;&#21270;&#31649;&#29702;&#12290;</title><link>http://arxiv.org/abs/2306.05567</link><description>&lt;p&gt;
&#26234;&#33021;&#20998;&#26512;&#65292;&#22312;&#29289;&#32852;&#32593;&#26694;&#26550;&#19979;&#30340;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#65306;&#22797;&#26434;&#32593;&#32476;&#21644;&#31995;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Intelligent Energy Management with IoT Framework in Smart Cities Using Intelligent Analysis: An Application of Machine Learning Methods for Complex Networks and Systems. (arXiv:2306.05567v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05567
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#30340;&#29289;&#32852;&#32593;&#26694;&#26550;&#65292;&#32467;&#21512;&#26234;&#33021;&#20998;&#26512;&#21644;&#22810;&#32452;&#20214;&#30340;&#26550;&#26500;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#26399;&#33410;&#33021;&#21644;&#20248;&#21270;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#24314;&#31569;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26080;&#32447;&#20256;&#24863;&#31995;&#32479;&#26469;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#26234;&#33021;&#22478;&#24066;&#33021;&#28304;&#31649;&#29702;&#26694;&#26550;&#65292;&#34701;&#21512;&#20102;&#22810;&#20010;&#29289;&#32852;&#32593;&#26550;&#26500;&#21644;&#26694;&#26550;&#30340;&#32452;&#20214;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#26234;&#33021;&#20998;&#26512;&#65292;&#19981;&#20165;&#25910;&#38598;&#21644;&#23384;&#20648;&#20449;&#24687;&#65292;&#32780;&#19988;&#36824;&#26159;&#20854;&#20182;&#20225;&#19994;&#24320;&#21457;&#24212;&#29992;&#30340;&#24179;&#21488;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22522;&#20110;&#26234;&#33021;&#26426;&#21046;&#30340;&#26234;&#33021;&#33021;&#28304;&#31649;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;&#33021;&#28304;&#36164;&#28304;&#30340;&#28040;&#32791;&#21644;&#38656;&#27714;&#22686;&#21152;&#23548;&#33268;&#20102;&#33410;&#33021;&#19982;&#20248;&#21270;&#31649;&#29702;&#30340;&#38656;&#27714;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart buildings are increasingly using Internet of Things (IoT)-based wireless sensing systems to reduce their energy consumption and environmental impact. As a result of their compact size and ability to sense, measure, and compute all electrical properties, Internet of Things devices have become increasingly important in our society. A major contribution of this study is the development of a comprehensive IoT-based framework for smart city energy management, incorporating multiple components of IoT architecture and framework. An IoT framework for intelligent energy management applications that employ intelligent analysis is an essential system component that collects and stores information. Additionally, it serves as a platform for the development of applications by other companies. Furthermore, we have studied intelligent energy management solutions based on intelligent mechanisms. The depletion of energy resources and the increase in energy demand have led to an increase in energy 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#23398;&#20064;&#26426;&#26041;&#27861;&#26469;&#36866;&#24212;&#22810;&#26631;&#31614;&#23398;&#20064;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12289;&#30830;&#23450;&#24615;&#20197;&#21450;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2305.05518</link><description>&lt;p&gt;
&#22810;&#26631;&#31614;&#23398;&#20064;&#30340;&#26368;&#23567;&#23398;&#20064;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
Minimal Learning Machine for Multi-Label Learning. (arXiv:2305.05518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#23398;&#20064;&#26426;&#26041;&#27861;&#26469;&#36866;&#24212;&#22810;&#26631;&#31614;&#23398;&#20064;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#23427;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12289;&#30830;&#23450;&#24615;&#20197;&#21450;&#31616;&#21333;&#30340;&#36229;&#21442;&#25968;&#36873;&#25321;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36317;&#31163;&#30340;&#30417;&#30563;&#26041;&#27861;&#8212;&#8212;&#26368;&#23567;&#23398;&#20064;&#26426;&#65292;&#36890;&#36807;&#23398;&#20064;&#36755;&#20837;&#21644;&#36755;&#20986;&#36317;&#31163;&#30697;&#38453;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20174;&#25968;&#25454;&#20013;&#26500;&#24314;&#39044;&#27979;&#27169;&#22411;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#24182;&#35780;&#20272;&#20102;&#36825;&#31181;&#25216;&#26415;&#21450;&#20854;&#26680;&#24515;&#32452;&#20214;&#8212;&#8212;&#36317;&#31163;&#26144;&#23556;&#22914;&#20309;&#36866;&#24212;&#22810;&#26631;&#31614;&#23398;&#20064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#23558;&#36317;&#31163;&#26144;&#23556;&#19982;&#36870;&#36317;&#31163;&#21152;&#26435;&#30456;&#32467;&#21512;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#26159;&#22810;&#26631;&#31614;&#23398;&#20064;&#25991;&#29486;&#20013;&#26368;&#31616;&#21333;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#23427;&#22312;&#23567;&#21040;&#20013;&#31561;&#35268;&#27169;&#30340;&#22810;&#26631;&#31614;&#23398;&#20064;&#38382;&#39064;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#38500;&#20102;&#23427;&#30340;&#31616;&#21333;&#24615;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#23436;&#20840;&#30830;&#23450;&#24615;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#25490;&#21517;&#25439;&#22833;&#32479;&#35745;&#37327;&#30340;&#26041;&#27861;&#36873;&#25321;&#20854;&#36229;&#21442;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#23553;&#38381;&#24418;&#24335;&#65292;&#22240;&#27492;&#36991;&#20813;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#20854;&#31616;&#21333;&#30340;&#32447;&#24615;&#36317;&#31163;&#26144;&#23556;&#26500;&#36896;&#26041;&#24335;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#35780;&#20272;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distance-based supervised method, the minimal learning machine, constructs a predictive model from data by learning a mapping between input and output distance matrices. In this paper, we propose methods and evaluate how this technique and its core component, the distance mapping, can be adapted to multi-label learning. The proposed approach is based on combining the distance mapping with an inverse distance weighting. Although the proposal is one of the simplest methods in the multi-label learning literature, it achieves state-of-the-art performance for small to moderate-sized multi-label learning problems. Besides its simplicity, the proposed method is fully deterministic and its hyper-parameter can be selected via ranking loss-based statistic which has a closed form, thus avoiding conventional cross-validation-based hyper-parameter tuning. In addition, due to its simple linear distance mapping-based construction, we demonstrate that the proposed method can assess predictions' uncert
&lt;/p&gt;</description></item><item><title>OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;</title><link>http://arxiv.org/abs/2304.04203</link><description>&lt;p&gt;
OpenDriver: &#19968;&#20221;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
OpenDriver: an open-road driver state detection dataset. (arXiv:2304.04203v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04203
&lt;/p&gt;
&lt;p&gt;
OpenDriver&#26159;&#19968;&#20221;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#23384;&#22312;&#38382;&#39064;&#30340;&#24320;&#25918;&#36335;&#20917;&#39550;&#39542;&#21592;&#29366;&#24577;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#20449;&#21495;&#20004;&#31181;&#27169;&#24577;&#30340;&#25968;&#25454;&#65292;&#21487;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#20195;&#31038;&#20250;&#20013;&#65292;&#36947;&#36335;&#23433;&#20840;&#20005;&#37325;&#20381;&#36182;&#20110;&#39550;&#39542;&#21592;&#30340;&#24515;&#29702;&#21644;&#29983;&#29702;&#29366;&#24577;&#12290;&#30130;&#21171;&#12289;&#26127;&#26127;&#27442;&#30561;&#21644;&#21387;&#21147;&#31561;&#36127;&#38754;&#22240;&#32032;&#20250;&#24433;&#21709;&#39550;&#39542;&#21592;&#30340;&#21453;&#24212;&#26102;&#38388;&#21644;&#20915;&#31574;&#33021;&#21147;&#65292;&#23548;&#33268;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#29575;&#22686;&#21152;&#12290;&#22312;&#20247;&#22810;&#30340;&#39550;&#39542;&#21592;&#34892;&#20026;&#30417;&#27979;&#30740;&#31350;&#20013;&#65292;&#21487;&#31359;&#25140;&#29983;&#29702;&#27979;&#37327;&#26159;&#19968;&#31181;&#23454;&#26102;&#30417;&#27979;&#39550;&#39542;&#21592;&#29366;&#24577;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#22312;&#24320;&#25918;&#36947;&#36335;&#22330;&#26223;&#19979;&#65292;&#32570;&#23569;&#39550;&#39542;&#21592;&#29983;&#29702;&#25968;&#25454;&#38598;&#65292;&#24050;&#26377;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#20449;&#21495;&#36136;&#37327;&#24046;&#12289;&#26679;&#26412;&#37327;&#23567;&#21644;&#25968;&#25454;&#25910;&#38598;&#26102;&#38388;&#30701;&#31561;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#35774;&#35745;&#24182;&#25551;&#36848;&#20102;&#19968;&#31181;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#39550;&#39542;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#39550;&#39542;&#21592;&#21463;&#25439;&#26816;&#27979;&#21644;&#29983;&#29289;&#35782;&#21035;&#25968;&#25454;&#35782;&#21035;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20004;&#31181;&#39550;&#39542;&#20449;&#21495;&#27169;&#24577;&#65306;&#20845;&#36724;&#24815;&#24615;&#20449;&#21495;&#21644;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#26159;&#22312;100&#22810;&#21517;&#39550;&#39542;&#21592;&#36981;&#24490;&#30456;&#21516;&#36335;&#32447;&#34892;&#39542;&#26102;&#35760;&#24405;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In modern society, road safety relies heavily on the psychological and physiological state of drivers. Negative factors such as fatigue, drowsiness, and stress can impair drivers' reaction time and decision making abilities, leading to an increased incidence of traffic accidents. Among the numerous studies for impaired driving detection, wearable physiological measurement is a real-time approach to monitoring a driver's state. However, currently, there are few driver physiological datasets in open road scenarios and the existing datasets suffer from issues such as poor signal quality, small sample sizes, and short data collection periods. Therefore, in this paper, a large-scale multimodal driving dataset for driver impairment detection and biometric data recognition is designed and described. The dataset contains two modalities of driving signals: six-axis inertial signals and electrocardiogram (ECG) signals, which were recorded while over one hundred drivers were following the same ro
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25805;&#20316;&#22120;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#22312;&#19968;&#31867;DeepONets&#20013;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#19981;&#20250;&#38543;&#32593;&#32476;&#23485;&#24230;&#25193;&#23637;&#32780;&#26126;&#30830;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#23637;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;Huber&#25439;&#22833;&#26469;&#33719;&#24471;&#19981;&#26126;&#30830;&#20381;&#36182;&#20110;&#32593;&#32476;&#22823;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2205.11359</link><description>&lt;p&gt;
&#38754;&#21521;&#23610;&#24230;&#26080;&#20851;&#30340;&#28145;&#24230;&#25805;&#20316;&#22120;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;
&lt;/p&gt;
&lt;p&gt;
Towards Size-Independent Generalization Bounds for Deep Operator Nets. (arXiv:2205.11359v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.11359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#25805;&#20316;&#22120;&#32593;&#32476;&#30340;&#27867;&#21270;&#30028;&#38480;&#38382;&#39064;&#65292;&#22312;&#19968;&#31867;DeepONets&#20013;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;Rademacher&#22797;&#26434;&#24230;&#30340;&#30028;&#38480;&#19981;&#20250;&#38543;&#32593;&#32476;&#23485;&#24230;&#25193;&#23637;&#32780;&#26126;&#30830;&#21464;&#21270;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#23637;&#31034;&#20102;&#22914;&#20309;&#36873;&#25321;Huber&#25439;&#22833;&#26469;&#33719;&#24471;&#19981;&#26126;&#30830;&#20381;&#36182;&#20110;&#32593;&#32476;&#22823;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#26102;&#26399;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#20998;&#26512;&#29289;&#29702;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#22312;&#36825;&#20010;&#20027;&#39064;&#20013;&#29305;&#21035;&#27963;&#36291;&#30340;&#39046;&#22495;&#26159;"&#29289;&#29702;&#20449;&#24687;&#26426;&#22120;&#23398;&#20064;"&#65292;&#23427;&#19987;&#27880;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#25968;&#20540;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25512;&#36827;&#22312;&#35757;&#32451;DeepONets&#26102;&#27979;&#37327;&#26679;&#26412;&#22806;&#35823;&#24046;&#30340;&#29702;&#35770; - &#36825;&#26159;&#35299;&#20915;PDE&#31995;&#32479;&#26368;&#36890;&#29992;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;&#39318;&#20808;&#65292;&#38024;&#23545;&#19968;&#31867;DeepONets&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;Rademacher&#22797;&#26434;&#24230;&#26377;&#19968;&#20010;&#30028;&#38480;&#65292;&#35813;&#30028;&#38480;&#19981;&#20250;&#26126;&#30830;&#22320;&#38543;&#30528;&#28041;&#21450;&#30340;&#32593;&#32476;&#23485;&#24230;&#25193;&#23637;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#32467;&#26524;&#26469;&#23637;&#31034;&#22914;&#20309;&#36873;&#25321;Huber&#25439;&#22833;&#65292;&#20351;&#24471;&#23545;&#20110;&#36825;&#20123;DeepONet&#31867;&#65292;&#33021;&#22815;&#33719;&#24471;&#19981;&#26126;&#30830;&#20381;&#36182;&#20110;&#32593;&#32476;&#22823;&#23567;&#30340;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#20219;&#20309;&#30446;&#26631;&#26159;&#30001;DeepONets&#27714;&#35299;&#30340;PDE&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times machine learning methods have made significant advances in becoming a useful tool for analyzing physical systems. A particularly active area in this theme has been "physics-informed machine learning" which focuses on using neural nets for numerically solving differential equations. In this work, we aim to advance the theory of measuring out-of-sample error while training DeepONets -- which is among the most versatile ways to solve PDE systems in one-shot.  Firstly, for a class of DeepONets, we prove a bound on their Rademacher complexity which does not explicitly scale with the width of the nets involved. Secondly, we use this to show how the Huber loss can be chosen so that for these DeepONet classes generalization error bounds can be obtained that have no explicit dependence on the size of the nets. We note that our theoretical results apply to any PDE being targeted to be solved by DeepONets.
&lt;/p&gt;</description></item></channel></rss>