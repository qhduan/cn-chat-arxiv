<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Holo-VQVAE&#26159;&#19968;&#31181;&#38024;&#23545;&#20165;&#30456;&#20301;&#20840;&#24687;&#22270;&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#35282;&#35889;&#26041;&#27861;&#26469;&#23398;&#20064;&#22270;&#20687;&#22495;&#65292;&#22312;&#20840;&#24687;&#22270;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#20998;&#24067;&#20013;&#30452;&#25509;&#29983;&#25104;&#22810;&#26679;&#21270;&#20840;&#24687;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2404.01330</link><description>&lt;p&gt;
Holo-VQVAE&#65306;&#29992;&#20110;&#20165;&#30456;&#20301;&#20840;&#24687;&#22270;&#30340;VQ-VAE
&lt;/p&gt;
&lt;p&gt;
Holo-VQVAE: VQ-VAE for phase-only holograms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01330
&lt;/p&gt;
&lt;p&gt;
Holo-VQVAE&#26159;&#19968;&#31181;&#38024;&#23545;&#20165;&#30456;&#20301;&#20840;&#24687;&#22270;&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#32467;&#26500;&#65292;&#36890;&#36807;&#38598;&#25104;&#35282;&#35889;&#26041;&#27861;&#26469;&#23398;&#20064;&#22270;&#20687;&#22495;&#65292;&#22312;&#20840;&#24687;&#22270;&#29983;&#25104;&#20013;&#23454;&#29616;&#20102;&#20174;&#22797;&#26434;&#20998;&#24067;&#20013;&#30452;&#25509;&#29983;&#25104;&#22810;&#26679;&#21270;&#20840;&#24687;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Holography stands at the forefront of visual technology innovation, offering immersive, three-dimensional visualizations through the manipulation of light wave amplitude and phase. Contemporary research in hologram generation has predominantly focused on image-to-hologram conversion, producing holograms from existing images. These approaches, while effective, inherently limit the scope of innovation and creativity in hologram generation. In response to this limitation, we present Holo-VQVAE, a novel generative framework tailored for phase-only holograms (POHs). Holo-VQVAE leverages the architecture of Vector Quantized Variational AutoEncoders, enabling it to learn the complex distributions of POHs. Furthermore, it integrates the Angular Spectrum Method into the training process, facilitating learning in the image domain. This framework allows for the generation of unseen, diverse holographic content directly from its intricately learned distributions.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01330v1 Announce Type: cross  Abstract: Holography stands at the forefront of visual technology innovation, offering immersive, three-dimensional visualizations through the manipulation of light wave amplitude and phase. Contemporary research in hologram generation has predominantly focused on image-to-hologram conversion, producing holograms from existing images. These approaches, while effective, inherently limit the scope of innovation and creativity in hologram generation. In response to this limitation, we present Holo-VQVAE, a novel generative framework tailored for phase-only holograms (POHs). Holo-VQVAE leverages the architecture of Vector Quantized Variational AutoEncoders, enabling it to learn the complex distributions of POHs. Furthermore, it integrates the Angular Spectrum Method into the training process, facilitating learning in the image domain. This framework allows for the generation of unseen, diverse holographic content directly from its intricately learne
&lt;/p&gt;</description></item><item><title>&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07404</link><description>&lt;p&gt;
&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#21644;&#20943;&#23569;&#36951;&#24536;&#65306;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#21452;&#37325;&#22909;&#22788;
&lt;/p&gt;
&lt;p&gt;
Accelerated Inference and Reduced Forgetting: The Dual Benefits of Early-Exit Networks in Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07404
&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#38477;&#20302;&#36951;&#24536;&#21644;&#22312;&#36164;&#28304;&#21033;&#29992;&#19978;&#34920;&#29616;&#20248;&#24322;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#30028; &#25688;&#35201;: &#21463;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#28304;&#39640;&#25928;&#21033;&#29992;&#38656;&#27714;&#39537;&#21160;&#65292;&#26089;&#26399;&#36864;&#20986;&#26041;&#27861;&#22791;&#21463;&#20851;&#27880;&#12290;&#36825;&#20123;&#31574;&#30053;&#36890;&#36807;&#22312;&#32593;&#32476;&#26089;&#26399;&#20570;&#20986;&#20915;&#23450;&#65292;&#23454;&#29616;&#24555;&#36895;&#39044;&#27979;&#65292;&#20174;&#32780;&#33410;&#30465;&#35745;&#31639;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#20165;&#38024;&#23545;&#38745;&#24577;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#20102;&#24320;&#21457;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#25345;&#32493;&#38750;&#38745;&#24577;&#25968;&#25454;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26089;&#26399;&#36864;&#20986;&#32593;&#32476;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#25105;&#20204;&#25913;&#32534;&#29616;&#26377;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#20197;&#36866;&#24212;&#26089;&#26399;&#36864;&#20986;&#26550;&#26500;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22312;&#25345;&#32493;&#35774;&#32622;&#20013;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#26089;&#26399;&#32593;&#32476;&#23618;&#34920;&#29616;&#20986;&#20943;&#23569;&#36951;&#24536;&#65292;&#21363;&#20351;&#20351;&#29992;&#30340;&#36164;&#28304;&#26174;&#33879;&#26356;&#23569;&#65292;&#20063;&#33021;&#32988;&#36807;&#26631;&#20934;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#26512;&#20219;&#21153;&#26368;&#36817;&#24615;&#20559;&#24046;&#23545;&#26089;&#26399;&#36864;&#20986;&#25512;&#29702;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20219;&#21153;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07404v1 Announce Type: cross  Abstract: Driven by the demand for energy-efficient employment of deep neural networks, early-exit methods have experienced a notable increase in research attention. These strategies allow for swift predictions by making decisions early in the network, thereby conserving computation time and resources. However, so far the early-exit networks have only been developed for stationary data distributions, which restricts their application in real-world scenarios with continuous non-stationary data. This study aims to explore the continual learning of the early-exit networks. We adapt existing continual learning methods to fit with early-exit architectures and investigate their behavior in the continual setting. We notice that early network layers exhibit reduced forgetting and can outperform standard networks even when using significantly fewer resources. Furthermore, we analyze the impact of task-recency bias on early-exit inference and propose Task
&lt;/p&gt;</description></item><item><title>FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;</title><link>https://arxiv.org/abs/2402.18789</link><description>&lt;p&gt;
FlexLLM&#65306;&#19968;&#31181;&#29992;&#20110;&#20849;&#21516;&#25552;&#20379;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18789
&lt;/p&gt;
&lt;p&gt;
FlexLLM&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20849;&#21516;&#25552;&#20379;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#23454;&#29616;&#20849;&#20139;GPU&#36164;&#28304;&#30340;&#39640;&#25928;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Parameter-efficient finetuning&#65288;PEFT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#20026;&#19981;&#21516;&#20219;&#21153;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#36890;&#24120;&#65292;&#26381;&#21153;&#25552;&#20379;&#21830;&#20250;&#20026;&#29992;&#25143;&#21019;&#24314;&#21333;&#29420;&#30340;&#31995;&#32479;&#65292;&#20197;&#25191;&#34892;PEFT&#27169;&#22411;&#24494;&#35843;&#21644;&#25512;&#29702;&#20219;&#21153;&#12290;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#31995;&#32479;&#26080;&#27861;&#22788;&#29702;&#21253;&#21547;&#25512;&#29702;&#21644;PEFT&#24494;&#35843;&#35831;&#27714;&#28151;&#21512;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#22240;&#27492;&#65292;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#21033;&#29992;&#19981;&#36275;&#65292;&#23548;&#33268;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FlexLLM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#22312;&#21516;&#19968;&#36845;&#20195;&#20013;&#20026;&#25512;&#29702;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#35831;&#27714;&#25552;&#20379;&#26381;&#21153;&#30340;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21033;&#29992;&#36825;&#20004;&#20010;&#20219;&#21153;&#30340;&#20114;&#34917;&#24615;&#36136;&#65292;&#24182;&#21033;&#29992;&#20849;&#20139;&#30340;GPU&#36164;&#28304;&#26469;&#20849;&#21516;&#36816;&#34892;&#23427;&#20204;&#65292;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#20849;&#21516;&#25552;&#20379;&#30340;&#26041;&#27861;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;FlexLLM&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#32423;&#24494;&#35843;&#26426;&#21046;&#65292;&#23558;&#24207;&#21015;&#30340;&#24494;&#35843;&#35745;&#31639;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#26631;&#35760;&#32423;&#35745;&#31639;&#65292;&#24182;&#20351;&#29992;&#20381;&#36182;&#24182;&#34892;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18789v1 Announce Type: cross  Abstract: Parameter-efficient finetuning (PEFT) is a widely used technique to adapt large language models for different tasks. Service providers typically create separate systems for users to perform PEFT model finetuning and inference tasks. This is because existing systems cannot handle workloads that include a mix of inference and PEFT finetuning requests. As a result, shared GPU resources are underutilized, leading to inefficiencies. To address this problem, we present FlexLLM, the first system that can serve inference and parameter-efficient finetuning requests in the same iteration. Our system leverages the complementary nature of these two tasks and utilizes shared GPU resources to run them jointly, using a method called co-serving. To achieve this, FlexLLM introduces a novel token-level finetuning mechanism, which breaks down the finetuning computation of a sequence into smaller token-level computations and uses dependent parallelization
&lt;/p&gt;</description></item><item><title>Prismatic&#26159;&#19968;&#20010;&#38598;&#25104;&#21382;&#21490;&#25968;&#25454;&#20998;&#26512;&#21644;&#19994;&#21153;&#20851;&#31995;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#22810;&#35270;&#35282;&#27010;&#24565;&#32929;&#38598;&#32676;&#20998;&#26512;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#38598;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#20016;&#23500;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#38598;&#32676;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#33268;&#30340;&#19994;&#21153;&#30456;&#20851;&#24615;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.08978</link><description>&lt;p&gt;
Prismatic:&#20132;&#20114;&#24335;&#22810;&#35270;&#35282;&#27010;&#24565;&#32929;&#38598;&#32676;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Prismatic: Interactive Multi-View Cluster Analysis of Concept Stocks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08978
&lt;/p&gt;
&lt;p&gt;
Prismatic&#26159;&#19968;&#20010;&#38598;&#25104;&#21382;&#21490;&#25968;&#25454;&#20998;&#26512;&#21644;&#19994;&#21153;&#20851;&#31995;&#30693;&#35782;&#30340;&#20132;&#20114;&#24335;&#22810;&#35270;&#35282;&#27010;&#24565;&#32929;&#38598;&#32676;&#20998;&#26512;&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#35270;&#35282;&#38598;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#20016;&#23500;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#38598;&#32676;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#33268;&#30340;&#19994;&#21153;&#30456;&#20851;&#24615;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08978v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#37329;&#34701;&#38598;&#32676;&#20998;&#26512;&#20351;&#25237;&#36164;&#32773;&#33021;&#22815;&#21457;&#29616;&#25237;&#36164;&#26367;&#20195;&#21697;&#65292;&#24182;&#36991;&#20813;&#25215;&#25285;&#36807;&#39640;&#30340;&#39118;&#38505;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20998;&#26512;&#20219;&#21153;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#65292;&#22914;&#22823;&#37327;&#30340;&#20004;&#20004;&#27604;&#36739;&#12289;&#26102;&#38388;&#36328;&#24230;&#30340;&#21160;&#24577;&#30456;&#20851;&#24615;&#20197;&#21450;&#20174;&#19994;&#21153;&#20851;&#31995;&#30693;&#35782;&#20013;&#24471;&#20986;&#25512;&#35770;&#30340;&#27169;&#31946;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Prismatic&#65292;&#19968;&#31181;&#21487;&#35270;&#21270;&#20998;&#26512;&#31995;&#32479;&#65292;&#23427;&#25972;&#21512;&#20102;&#21382;&#21490;&#24615;&#33021;&#30340;&#23450;&#37327;&#20998;&#26512;&#21644;&#19994;&#21153;&#20851;&#31995;&#30693;&#35782;&#30340;&#23450;&#24615;&#20998;&#26512;&#65292;&#20197;&#20132;&#20114;&#26041;&#24335;&#23545;&#30456;&#20851;&#19994;&#21153;&#36827;&#34892;&#38598;&#32676;&#20998;&#26512;&#12290;Prismatic&#20855;&#26377;&#19977;&#20010;&#38598;&#32676;&#29983;&#25104;&#36807;&#31243;&#65306;&#21160;&#24577;&#38598;&#32676;&#29983;&#25104;&#12289;&#22522;&#20110;&#30693;&#35782;&#30340;&#38598;&#32676;&#25506;&#32034;&#21644;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#38598;&#32676;&#39564;&#35777;&#12290;&#21033;&#29992;&#22810;&#35270;&#35282;&#38598;&#32676;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#30693;&#35782;&#39537;&#21160;&#30340;&#30456;&#20284;&#24615;&#20016;&#23500;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#38598;&#32676;&#65292;&#25552;&#20379;&#20102;&#23545;&#19994;&#21153;&#30456;&#20851;&#24615;&#30340;&#32454;&#33268;&#29702;&#35299;&#12290;&#36890;&#36807;&#33391;&#22909;&#21327;&#35843;&#30340;&#21487;&#35270;&#21270;&#35270;&#22270;&#65292;Prismatic&#20415;&#20110;&#20102;&#35299;&#20225;&#19994;&#30340;&#20851;&#32852;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08978v1 Announce Type: cross Abstract: Financial cluster analysis allows investors to discover investment alternatives and avoid undertaking excessive risks. However, this analytical task faces substantial challenges arising from many pairwise comparisons, the dynamic correlations across time spans, and the ambiguity in deriving implications from business relational knowledge. We propose Prismatic, a visual analytics system that integrates quantitative analysis of historical performance and qualitative analysis of business relational knowledge to cluster correlated businesses interactively. Prismatic features three clustering processes: dynamic cluster generation, knowledge-based cluster exploration, and correlation-based cluster validation. Utilizing a multi-view clustering approach, it enriches data-driven clusters with knowledge-driven similarity, providing a nuanced understanding of business correlations. Through well-coordinated visual views, Prismatic facilitates a com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Variance-Reduced Sketching&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#24230;&#20013;&#20272;&#35745;&#23494;&#24230;&#20989;&#25968;&#21644;&#38750;&#21442;&#25968;&#22238;&#24402;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20989;&#25968;&#27010;&#24565;&#21270;&#20026;&#30697;&#38453;&#65292;&#24182;&#37319;&#29992;&#33609;&#22270;&#25216;&#26415;&#26469;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#33021;&#21644;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.11646</link><description>&lt;p&gt;
&#36890;&#36807;&#26041;&#24046;&#38477;&#20302;&#30340;&#33609;&#22270;&#36827;&#34892;&#38750;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Estimation via Variance-Reduced Sketching. (arXiv:2401.11646v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Variance-Reduced Sketching&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#39640;&#32500;&#24230;&#20013;&#20272;&#35745;&#23494;&#24230;&#20989;&#25968;&#21644;&#38750;&#21442;&#25968;&#22238;&#24402;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20989;&#25968;&#27010;&#24565;&#21270;&#20026;&#30697;&#38453;&#65292;&#24182;&#37319;&#29992;&#33609;&#22270;&#25216;&#26415;&#26469;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#24341;&#36215;&#30340;&#26041;&#24046;&#65292;&#23637;&#31034;&#20102;&#40065;&#26834;&#24615;&#33021;&#21644;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#21442;&#25968;&#27169;&#22411;&#22312;&#21508;&#20010;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#32463;&#20856;&#30340;&#26680;&#26041;&#27861;&#22312;&#20302;&#32500;&#24773;&#20917;&#19979;&#20855;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#32479;&#35745;&#21487;&#38752;&#24615;&#65292;&#20294;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#30001;&#20110;&#32500;&#24230;&#28798;&#38590;&#21464;&#24471;&#19981;&#22815;&#36866;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Variance-Reduced Sketching&#65288;VRS&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;&#22312;&#38477;&#20302;&#32500;&#24230;&#28798;&#38590;&#30340;&#21516;&#26102;&#22312;&#39640;&#32500;&#24230;&#20013;&#20272;&#35745;&#23494;&#24230;&#20989;&#25968;&#21644;&#38750;&#21442;&#25968;&#22238;&#24402;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#22810;&#21464;&#37327;&#20989;&#25968;&#27010;&#24565;&#21270;&#20026;&#26080;&#38480;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#24182;&#20511;&#37492;&#20102;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#25991;&#29486;&#20013;&#30340;&#19968;&#31181;&#26032;&#30340;&#33609;&#22270;&#25216;&#26415;&#26469;&#38477;&#20302;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#27169;&#25311;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#24212;&#29992;&#23637;&#31034;&#20102;VRS&#30340;&#40065;&#26834;&#24615;&#33021;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#35768;&#22810;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#20013;&#65292;VRS&#30456;&#36739;&#20110;&#29616;&#26377;&#30340;&#31070;&#32463;&#32593;&#32476;&#20272;&#35745;&#22120;&#21644;&#32463;&#20856;&#30340;&#26680;&#26041;&#27861;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonparametric models are of great interest in various scientific and engineering disciplines. Classical kernel methods, while numerically robust and statistically sound in low-dimensional settings, become inadequate in higher-dimensional settings due to the curse of dimensionality. In this paper, we introduce a new framework called Variance-Reduced Sketching (VRS), specifically designed to estimate density functions and nonparametric regression functions in higher dimensions with a reduced curse of dimensionality. Our framework conceptualizes multivariable functions as infinite-size matrices, and facilitates a new sketching technique motivated by numerical linear algebra literature to reduce the variance in estimation problems. We demonstrate the robust numerical performance of VRS through a series of simulated experiments and real-world data applications. Notably, VRS shows remarkable improvement over existing neural network estimators and classical kernel methods in numerous density 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#31070;&#32463;&#31639;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#32593;&#32476;&#25552;&#20379;&#19981;&#38656;&#35201;&#25968;&#20540;&#27714;&#35299;PDE&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2401.02398</link><description>&lt;p&gt;
&#29983;&#25104;&#31070;&#32463;&#31639;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Generating synthetic data for neural operators. (arXiv:2401.02398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02398
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#31070;&#32463;&#31639;&#23376;&#30340;&#21512;&#25104;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#20026;&#35757;&#32451;&#32593;&#32476;&#25552;&#20379;&#19981;&#38656;&#35201;&#25968;&#20540;&#27714;&#35299;PDE&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25991;&#29486;&#20013;&#30340;&#35768;&#22810;&#21457;&#23637;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#33719;&#21462;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#25968;&#20540;&#35299;&#26041;&#38754;&#30340;&#28508;&#21147;&#65292;&#36825;&#36229;&#20986;&#20102;&#24403;&#21069;&#25968;&#20540;&#27714;&#35299;&#22120;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#31070;&#32463;&#31639;&#23376;&#37117;&#23384;&#22312;&#21516;&#26679;&#30340;&#38382;&#39064;&#65306;&#35757;&#32451;&#32593;&#32476;&#25152;&#38656;&#30340;&#25968;&#25454;&#20381;&#36182;&#20110;&#20256;&#32479;&#30340;&#25968;&#20540;&#27714;&#35299;&#22120;&#65292;&#22914;&#26377;&#38480;&#24046;&#20998;&#25110;&#26377;&#38480;&#20803;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#30340;&#20989;&#25968;&#35757;&#32451;&#25968;&#25454;&#65292;&#32780;&#26080;&#38656;&#25968;&#20540;&#27714;&#35299;PDE&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65306;&#25105;&#20204;&#20174;&#24050;&#30693;&#35299;&#20301;&#20110;&#30340;&#32463;&#20856;&#29702;&#35770;&#35299;&#31354;&#38388;&#65288;&#20363;&#22914;$H_0^1(\Omega)$&#65289;&#20013;&#25277;&#21462;&#22823;&#37327;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#8220;&#38543;&#26426;&#20989;&#25968;&#8221;$u_j$&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#38543;&#26426;&#35299;&#26041;&#26696;&#20195;&#20837;&#26041;&#31243;&#24182;&#33719;&#24471;&#30456;&#24212;&#30340;&#21491;&#20391;&#20989;&#25968;$f_j$&#65292;&#23558;$(f_j, u_j)_{j=1}^N$&#20316;&#20026;&#30417;&#30563;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Numerous developments in the recent literature show the promising potential of deep learning in obtaining numerical solutions to partial differential equations (PDEs) beyond the reach of current numerical solvers. However, data-driven neural operators all suffer from the same problem: the data needed to train a network depends on classical numerical solvers such as finite difference or finite element, among others. In this paper, we propose a new approach to generating synthetic functional training data that does not require solving a PDE numerically. The way we do this is simple: we draw a large number $N$ of independent and identically distributed `random functions' $u_j$ from the underlying solution space (e.g., $H_0^1(\Omega)$) in which we know the solution lies according to classical theory. We then plug each such random candidate solution into the equation and get a corresponding right-hand side function $f_j$ for the equation, and consider $(f_j, u_j)_{j=1}^N$ as supervised trai
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21452;&#37325;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861; (ADMM) &#21644;&#21322;&#24179;&#28369;&#29275;&#39039; (SSN) &#22522;&#20110;&#22686;&#24191;&#23545;&#20598;&#27861; (ALM) &#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#31639;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#26680;&#24515;&#22270;&#24418;Lasso&#27169;&#22411;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22823;&#32500;&#24230;&#30340;&#20219;&#21153;&#20013;&#33410;&#30465;&#36229;&#36807;70\%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.08852</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#23398;&#20064;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#26680;&#24515;&#22270;&#24418;Lasso&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning the hub graphical Lasso model with the structured sparsity via an efficient algorithm. (arXiv:2308.08852v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08852
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861; (ADMM) &#21644;&#21322;&#24179;&#28369;&#29275;&#39039; (SSN) &#22522;&#20110;&#22686;&#24191;&#23545;&#20598;&#27861; (ALM) &#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#31639;&#27861;&#26469;&#23398;&#20064;&#20855;&#26377;&#32467;&#26500;&#31232;&#30095;&#24615;&#30340;&#26680;&#24515;&#22270;&#24418;Lasso&#27169;&#22411;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#22312;&#22823;&#32500;&#24230;&#30340;&#20219;&#21153;&#20013;&#33410;&#30465;&#36229;&#36807;70\%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#27169;&#22411;&#22312;&#20174;&#29983;&#29289;&#20998;&#26512;&#21040;&#25512;&#33616;&#31995;&#32479;&#31561;&#20247;&#22810;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#26680;&#24515;&#33410;&#28857;&#30340;&#22270;&#24418;&#27169;&#22411;&#22312;&#25968;&#25454;&#32500;&#24230;&#36739;&#22823;&#26102;&#35745;&#31639;&#19978;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#39640;&#25928;&#20272;&#35745;&#26680;&#24515;&#22270;&#24418;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#31639;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#39318;&#20808;&#36890;&#36807;&#21452;&#37325;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861; (ADMM) &#29983;&#25104;&#19968;&#20010;&#33391;&#22909;&#30340;&#21021;&#22987;&#28857;&#65292;&#28982;&#21518;&#20351;&#29992;&#21322;&#24179;&#28369;&#29275;&#39039; (SSN) &#22522;&#20110;&#22686;&#24191;&#23545;&#20598;&#27861; (ALM) &#30340;&#26041;&#27861;&#36827;&#34892;&#28909;&#21551;&#21160;&#65292;&#20197;&#35745;&#31639;&#20986;&#33021;&#22815;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#31934;&#30830;&#21040;&#36275;&#22815;&#31243;&#24230;&#30340;&#35299;&#12290;&#24191;&#20041;&#38597;&#21487;&#27604;&#30697;&#38453;&#30340;&#31232;&#30095;&#32467;&#26500;&#30830;&#20445;&#20102;&#35813;&#31639;&#27861;&#33021;&#22815;&#38750;&#24120;&#39640;&#25928;&#22320;&#33719;&#24471;&#19968;&#20010;&#33391;&#22909;&#30340;&#35299;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#20840;&#38754;&#23454;&#39564;&#20013;&#65292;&#35813;&#31639;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;&#29305;&#21035;&#26159;&#22312;&#26576;&#20123;&#39640;&#32500;&#20219;&#21153;&#20013;&#65292;&#23427;&#21487;&#20197;&#33410;&#30465;&#36229;&#36807;70\%&#30340;&#25191;&#34892;&#26102;&#38388;&#65292;&#21516;&#26102;&#20173;&#28982;&#21487;&#20197;&#36798;&#21040;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graphical models have exhibited their performance in numerous tasks ranging from biological analysis to recommender systems. However, graphical models with hub nodes are computationally difficult to fit, particularly when the dimension of the data is large. To efficiently estimate the hub graphical models, we introduce a two-phase algorithm. The proposed algorithm first generates a good initial point via a dual alternating direction method of multipliers (ADMM), and then warm starts a semismooth Newton (SSN) based augmented Lagrangian method (ALM) to compute a solution that is accurate enough for practical tasks. The sparsity structure of the generalized Jacobian ensures that the algorithm can obtain a nice solution very efficiently. Comprehensive experiments on both synthetic data and real data show that it obviously outperforms the existing state-of-the-art algorithms. In particular, in some high dimensional tasks, it can save more than 70\% of the execution time, meanwhile still ach
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38750;&#31283;&#24577;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#26029;&#24207;&#21015;&#25968;&#25454;&#30340;&#26410;&#30693;&#26631;&#31614;&#65292;&#24182;&#36866;&#24212;&#26102;&#38388;&#28418;&#31227;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#28418;&#31227;&#24133;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.01658</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#28418;&#31227;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#24369;&#30417;&#30563;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Adaptive Method for Weak Supervision with Drifting Data. (arXiv:2306.01658v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38750;&#31283;&#24577;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24369;&#30417;&#30563;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#25512;&#26029;&#24207;&#21015;&#25968;&#25454;&#30340;&#26410;&#30693;&#26631;&#31614;&#65292;&#24182;&#36866;&#24212;&#26102;&#38388;&#28418;&#31227;&#65292;&#32780;&#26080;&#38656;&#20551;&#35774;&#28418;&#31227;&#24133;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#38750;&#31283;&#24577;&#35774;&#32622;&#20013;&#20855;&#26377;&#27491;&#24335;&#36136;&#37327;&#20445;&#35777;&#30340;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#29992;&#20110;&#23545;&#25968;&#25454;&#24207;&#21015;&#36827;&#34892;&#24369;&#30417;&#30563;&#26631;&#35760;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20351;&#29992;&#25552;&#20379;&#27599;&#20010;&#25968;&#25454;&#28857;&#27491;&#30830;&#20998;&#31867;&#30340;&#29420;&#31435;&#22024;&#26434;&#20449;&#21495;&#30340;&#24369;&#30417;&#30563;&#28304;&#26469;&#25512;&#26029;&#26410;&#30693;&#26631;&#31614;&#12290;&#36825;&#31181;&#24773;&#20917;&#21253;&#25324;&#20247;&#21253;&#21644;&#32534;&#31243;&#24335;&#24369;&#30417;&#30563;&#12290;&#25105;&#20204;&#37325;&#28857;&#30740;&#31350;&#38750;&#31283;&#24577;&#24773;&#20917;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#24369;&#30417;&#30563;&#28304;&#30340;&#31934;&#24230;&#21487;&#33021;&#20250;&#38543;&#26102;&#38388;&#28418;&#31227;&#65292;&#20363;&#22914;&#30001;&#20110;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#30001;&#20110;&#28418;&#31227;&#65292;&#26087;&#25968;&#25454;&#21487;&#33021;&#20250;&#25552;&#20379;&#35823;&#23548;&#24615;&#20449;&#24687;&#26469;&#25512;&#26029;&#24403;&#21069;&#25968;&#25454;&#28857;&#30340;&#26631;&#31614;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#20808;&#39564;&#23545;&#28418;&#31227;&#24133;&#24230;&#30340;&#20551;&#35774;&#65292;&#20197;&#20915;&#23450;&#20351;&#29992;&#22810;&#23569;&#36807;&#21435;&#30340;&#25968;&#25454;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#28418;&#31227;&#20551;&#35774;&#65292;&#32780;&#26159;&#26681;&#25454;&#36755;&#20837;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20445;&#35777;&#24369;&#30417;&#30563;&#28304;&#24403;&#21069;&#20934;&#30830;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an adaptive method with formal quality guarantees for weak supervision in a non-stationary setting. Our goal is to infer the unknown labels of a sequence of data by using weak supervision sources that provide independent noisy signals of the correct classification for each data point. This setting includes crowdsourcing and programmatic weak supervision. We focus on the non-stationary case, where the accuracy of the weak supervision sources can drift over time, e.g., because of changes in the underlying data distribution. Due to the drift, older data could provide misleading information to infer the label of the current data point. Previous work relied on a priori assumptions on the magnitude of the drift to decide how much data to use from the past. Comparatively, our algorithm does not require any assumptions on the drift, and it adapts based on the input. In particular, at each step, our algorithm guarantees an estimation of the current accuracies of the weak supervisio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#27491;&#24577;&#26144;&#23556;&#31639;&#27861;&#29992;&#20110;&#38750;&#20984;&#22797;&#21512;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#25193;&#23637;&#20102;&#22522;&#26412;Proximal&#38543;&#26426;&#26799;&#24230;&#27861;&#30340;&#26356;&#26377;&#38480;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.05828</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#24577;&#26144;&#23556;&#30340;Prox-SGD&#26041;&#27861;&#22312;KL&#19981;&#31561;&#24335;&#19979;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of a Normal Map-based Prox-SGD Method under the KL Inequality. (arXiv:2305.05828v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#27491;&#24577;&#26144;&#23556;&#31639;&#27861;&#29992;&#20110;&#38750;&#20984;&#22797;&#21512;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35777;&#26126;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#35813;&#26041;&#27861;&#25193;&#23637;&#20102;&#22522;&#26412;Proximal&#38543;&#26426;&#26799;&#24230;&#27861;&#30340;&#26356;&#26377;&#38480;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#27491;&#24577;&#26144;&#23556;&#31639;&#27861;&#65288;$\mathsf{norM}\text{-}\mathsf{SGD}$&#65289;&#29992;&#20110;&#38750;&#20984;&#22797;&#21512;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#25910;&#25947;&#24615;&#36136;&#12290;&#20351;&#29992;&#22522;&#20110;&#26102;&#38388;&#31383;&#21475;&#30340;&#31574;&#30053;&#65292;&#39318;&#20808;&#20998;&#26512;&#20102;$\mathsf{norM}\text{-}\mathsf{SGD}$&#30340;&#20840;&#23616;&#25910;&#25947;&#34892;&#20026;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#29983;&#25104;&#30340;&#36845;&#20195;&#24207;&#21015;$\{\boldsymbol{x}^k\}_k$&#30340;&#27599;&#20010;&#32047;&#31215;&#28857;&#20960;&#20046;&#30830;&#23450;&#22320;&#21644;&#26399;&#26395;&#19978;&#37117;&#23545;&#24212;&#20110;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#25152;&#24471;&#32467;&#26524;&#22312;&#26631;&#20934;&#20551;&#35774;&#19979;&#25104;&#31435;&#65292;&#24182;&#25193;&#23637;&#20102;&#22522;&#26412;Proximal&#38543;&#26426;&#26799;&#24230;&#27861;&#30340;&#26356;&#26377;&#38480;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#33879;&#21517;&#30340;Kurdyka-{\L}ojasiewicz&#65288;KL&#65289;&#20998;&#26512;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;&#36845;&#20195;&#24207;&#21015;$\{\boldsymbol{x}^k\}_k$&#25552;&#20379;&#20102;&#26032;&#30340;&#36880;&#28857;&#25910;&#25947;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#21462;&#20915;&#20110;&#22522;&#30784;KL&#25351;&#25968;$\boldsymbol{\theta}$&#21644;&#27493;&#38271;&#21160;&#24577;$\{\alpha_k\}_k$&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel stochastic normal map-based algorithm ($\mathsf{norM}\text{-}\mathsf{SGD}$) for nonconvex composite-type optimization problems and discuss its convergence properties. Using a time window-based strategy, we first analyze the global convergence behavior of $\mathsf{norM}\text{-}\mathsf{SGD}$ and it is shown that every accumulation point of the generated sequence of iterates $\{\boldsymbol{x}^k\}_k$ corresponds to a stationary point almost surely and in an expectation sense. The obtained results hold under standard assumptions and extend the more limited convergence guarantees of the basic proximal stochastic gradient method. In addition, based on the well-known Kurdyka-{\L}ojasiewicz (KL) analysis framework, we provide novel point-wise convergence results for the iterates $\{\boldsymbol{x}^k\}_k$ and derive convergence rates that depend on the underlying KL exponent $\boldsymbol{\theta}$ and the step size dynamics $\{\alpha_k\}_k$. Specifically, for the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#36718;&#20165;&#35266;&#23519;&#37096;&#20998;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#21521;&#37327;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22343;&#26041;&#35823;&#24046;&#20272;&#35745;&#30340;&#39034;&#24207;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36830;&#32493;&#28040;&#38500;&#31639;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;&#21516;&#26102;&#65292;&#23548;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26497;&#23567;&#20540;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2203.16810</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#20272;&#35745;&#24102;&#26377;&#36172;&#21338;&#21453;&#39304;&#30340;&#38543;&#26426;&#21521;&#37327;&#65306;&#20174;&#22343;&#26041;&#35823;&#24046;&#35270;&#35282;&#26469;&#30475;
&lt;/p&gt;
&lt;p&gt;
Adaptive Estimation of Random Vectors with Bandit Feedback: A mean-squared error viewpoint. (arXiv:2203.16810v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.16810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27599;&#36718;&#20165;&#35266;&#23519;&#37096;&#20998;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;&#21521;&#37327;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22343;&#26041;&#35823;&#24046;&#20272;&#35745;&#30340;&#39034;&#24207;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#36830;&#32493;&#28040;&#38500;&#31639;&#27861;&#30340;&#19968;&#31181;&#21464;&#20307;&#12290;&#21516;&#26102;&#65292;&#23548;&#20986;&#20102;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#26497;&#23567;&#20540;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#27599;&#36718;&#35266;&#23519;&#20165;&#26377;$ m &lt; K $&#20010;&#26410;&#30693;&#21327;&#26041;&#24046;&#30340;&#39640;&#26031;$ K $&#21521;&#37327;&#30340;&#38382;&#39064;&#19979;&#65292;&#36890;&#36807;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#20272;&#35745;&#39034;&#24207;&#23398;&#20064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;MSE&#20272;&#35745;&#30340;&#38598;&#20013;&#30028;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36172;&#21338;&#21453;&#39304;&#30340;&#26041;&#27861;&#37325;&#26032;&#26500;&#24314;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#28040;&#38500;&#31639;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#36824;&#23548;&#20986;&#20102;&#19968;&#20010;&#26497;&#23567;&#20540;&#19979;&#30028;&#65292;&#20197;&#20102;&#35299;&#35813;&#38382;&#39064;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#22522;&#26412;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of sequentially learning to estimate, in the mean squared error (MSE) sense, a Gaussian $K$-vector of unknown covariance by observing only $m &lt; K$ of its entries in each round. We first establish a concentration bound for MSE estimation. We then frame the estimation problem with bandit feedback, and propose a variant of the successive elimination algorithm. We also derive a minimax lower bound to understand the fundamental limit on the sample complexity of this problem.
&lt;/p&gt;</description></item></channel></rss>