<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#22312;&#20302;&#32500;&#26367;&#20195;&#31354;&#38388;&#20013;&#30340;&#20984;&#22810;&#38754;&#20307;&#39030;&#28857;&#19978;&#23884;&#20837;&#32467;&#26524;&#65292;&#24182;&#25506;&#31350;&#21333;&#32431;&#24418;&#20013;&#30340;&#19968;&#33268;&#24615;&#21306;&#22495;&#65292;&#26435;&#34913;&#20102;&#26367;&#20195;&#25439;&#22833;&#32500;&#24230;&#12289;&#38382;&#39064;&#23454;&#20363;&#25968;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.10818</link><description>&lt;p&gt;
&#22312;&#27169;&#22411;&#30340;&#20984;&#26367;&#20195;&#21697;&#30340;&#19968;&#33268;&#24615;&#21644;&#32500;&#24230;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Trading off Consistency and Dimensionality of Convex Surrogates for the Mode
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10818
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20302;&#32500;&#26367;&#20195;&#31354;&#38388;&#20013;&#30340;&#20984;&#22810;&#38754;&#20307;&#39030;&#28857;&#19978;&#23884;&#20837;&#32467;&#26524;&#65292;&#24182;&#25506;&#31350;&#21333;&#32431;&#24418;&#20013;&#30340;&#19968;&#33268;&#24615;&#21306;&#22495;&#65292;&#26435;&#34913;&#20102;&#26367;&#20195;&#25439;&#22833;&#32500;&#24230;&#12289;&#38382;&#39064;&#23454;&#20363;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#65292;&#24517;&#39035;&#23558;&#32467;&#26524;&#23884;&#20837;&#21040;&#33267;&#23569;&#26377;$n-1$&#32500;&#30340;&#23454;&#25968;&#31354;&#38388;&#20013;&#65292;&#20197;&#35774;&#35745;&#19968;&#31181;&#19968;&#33268;&#30340;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#65292;&#36825;&#20250;&#23548;&#33268;"&#27491;&#30830;"&#30340;&#20998;&#31867;&#65292;&#32780;&#19981;&#21463;&#25968;&#25454;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#22312;&#20449;&#24687;&#26816;&#32034;&#21644;&#32467;&#26500;&#21270;&#39044;&#27979;&#20219;&#21153;&#31561;&#38656;&#35201;&#22823;&#37327;n&#26102;&#65292;&#20248;&#21270;n-1&#32500;&#26367;&#20195;&#24120;&#24120;&#26159;&#26840;&#25163;&#30340;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#22914;&#20309;&#26435;&#34913;&#26367;&#20195;&#25439;&#22833;&#32500;&#24230;&#12289;&#38382;&#39064;&#23454;&#20363;&#25968;&#37327;&#20197;&#21450;&#22312;&#21333;&#32431;&#24418;&#19978;&#32422;&#26463;&#19968;&#33268;&#24615;&#21306;&#22495;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36319;&#38543;&#36807;&#21435;&#30340;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#23884;&#20837;&#36807;&#31243;&#65292;&#23558;&#32467;&#26524;&#26144;&#23556;&#21040;&#20302;&#32500;&#26367;&#20195;&#31354;&#38388;&#20013;&#30340;&#20984;&#22810;&#38754;&#20307;&#30340;&#39030;&#28857;&#19978;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#27599;&#20010;&#28857;&#36136;&#37327;&#20998;&#24067;&#21608;&#22260;&#23384;&#22312;&#21333;&#32431;&#24418;&#30340;&#20840;&#32500;&#23376;&#38598;&#65292;&#20854;&#20013;&#19968;&#33268;&#24615;&#25104;&#31435;&#65292;&#20294;&#26159;&#65292;&#23569;&#20110;n-1&#32500;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23384;&#22312;&#19968;&#20123;&#20998;&#24067;&#65292;&#23545;&#20110;&#36825;&#20123;&#20998;&#24067;&#65292;&#19968;&#31181;&#29616;&#35937;&#24615;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10818v1 Announce Type: new  Abstract: In multiclass classification over $n$ outcomes, the outcomes must be embedded into the reals with dimension at least $n-1$ in order to design a consistent surrogate loss that leads to the "correct" classification, regardless of the data distribution. For large $n$, such as in information retrieval and structured prediction tasks, optimizing a surrogate in $n-1$ dimensions is often intractable. We investigate ways to trade off surrogate loss dimension, the number of problem instances, and restricting the region of consistency in the simplex for multiclass classification. Following past work, we examine an intuitive embedding procedure that maps outcomes into the vertices of convex polytopes in a low-dimensional surrogate space. We show that full-dimensional subsets of the simplex exist around each point mass distribution for which consistency holds, but also, with less than $n-1$ dimensions, there exist distributions for which a phenomeno
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#21442;&#25968;&#25968;&#37327;&#26356;&#22810;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#23485;&#30340;&#32593;&#32476;&#65292;&#32780;&#26679;&#26412;&#28857;&#25968;&#37327;&#21644;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#26356;&#39640;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#28145;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2402.00152</link><description>&lt;p&gt;
&#26356;&#28145;&#36824;&#26159;&#26356;&#23485;: &#20174;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#35282;&#24230;&#30475;
&lt;/p&gt;
&lt;p&gt;
Deeper or Wider: A Perspective from Optimal Generalization Error with Sobolev Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#30740;&#31350;&#21457;&#29616;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21463;&#22810;&#31181;&#22240;&#32032;&#24433;&#21709;&#65292;&#21442;&#25968;&#25968;&#37327;&#26356;&#22810;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#23485;&#30340;&#32593;&#32476;&#65292;&#32780;&#26679;&#26412;&#28857;&#25968;&#37327;&#21644;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#26356;&#39640;&#20542;&#21521;&#20110;&#36873;&#25321;&#26356;&#28145;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36861;&#27714;&#65292;&#21040;&#24213;&#26159;&#26356;&#28145;&#36824;&#26159;&#26356;&#23485;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#32493;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#32034;&#20102;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;DeNNs&#65289;&#21644;&#20855;&#26377;&#26377;&#38480;&#38544;&#34255;&#23618;&#30340;&#26356;&#23485;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;WeNNs&#65289;&#22312;Sobolev&#25439;&#22833;&#30340;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#26041;&#38754;&#30340;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21487;&#20197;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21253;&#25324;&#26679;&#26412;&#28857;&#30340;&#25968;&#37327;&#65292;&#31070;&#32463;&#32593;&#32476;&#20869;&#30340;&#21442;&#25968;&#20197;&#21450;&#25439;&#22833;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26356;&#22810;&#30340;&#21442;&#25968;&#20542;&#21521;&#20110;&#36873;&#25321;WeNNs&#65292;&#32780;&#26356;&#22810;&#30340;&#26679;&#26412;&#28857;&#21644;&#26356;&#39640;&#30340;&#25439;&#22833;&#20989;&#25968;&#35268;&#21017;&#24615;&#20542;&#21521;&#20110;&#36873;&#25321;DeNNs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#29702;&#35770;&#24212;&#29992;&#20110;&#20351;&#29992;&#28145;&#24230;Ritz&#21644;&#29289;&#29702;&#24863;&#30693;&#31070;&#32463;&#32593;&#32476;&#65288;PINN&#65289;&#26041;&#27861;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constructing the architecture of a neural network is a challenging pursuit for the machine learning community, and the dilemma of whether to go deeper or wider remains a persistent question. This paper explores a comparison between deeper neural networks (DeNNs) with a flexible number of layers and wider neural networks (WeNNs) with limited hidden layers, focusing on their optimal generalization error in Sobolev losses. Analytical investigations reveal that the architecture of a neural network can be significantly influenced by various factors, including the number of sample points, parameters within the neural networks, and the regularity of the loss function. Specifically, a higher number of parameters tends to favor WeNNs, while an increased number of sample points and greater regularity in the loss function lean towards the adoption of DeNNs. We ultimately apply this theory to address partial differential equations using deep Ritz and physics-informed neural network (PINN) methods,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#26497;&#23567;&#20540;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#20123;&#38544;&#34255;&#26497;&#23567;&#20540;&#30340;&#29420;&#29305;&#35299;&#26512;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2312.16819</link><description>&lt;p&gt;
&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#26497;&#23567;&#20540;
&lt;/p&gt;
&lt;p&gt;
Hidden Minima in Two-Layer ReLU Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;ReLU&#32593;&#32476;&#20013;&#30340;&#38544;&#34255;&#26497;&#23567;&#20540;&#29616;&#35937;&#65292;&#24182;&#25552;&#20986;&#26041;&#27861;&#26469;&#30740;&#31350;&#36825;&#20123;&#38544;&#34255;&#26497;&#23567;&#20540;&#30340;&#29420;&#29305;&#35299;&#26512;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#25311;&#21512;&#20855;&#26377;$d$&#20010;&#36755;&#20837;&#12289;$k$&#20010;&#31070;&#32463;&#20803;&#20197;&#21450;&#30001;&#30446;&#26631;&#32593;&#32476;&#29983;&#25104;&#30340;&#26631;&#31614;&#30340;&#20004;&#23618;ReLU&#32593;&#32476;&#25152;&#28041;&#21450;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#26368;&#36817;&#21457;&#29616;&#20102;&#20004;&#31181;&#26080;&#31351;&#26063;&#30340;&#34394;&#20551;&#26497;&#23567;&#20540;&#65292;&#27599;&#20010;$d$&#23545;&#24212;&#19968;&#20010;&#26497;&#23567;&#20540;&#12290;&#23646;&#20110;&#31532;&#19968;&#31867;&#30340;&#26497;&#23567;&#20540;&#30340;&#25439;&#22833;&#22312;$d$&#22686;&#21152;&#26102;&#25910;&#25947;&#20110;&#38646;&#12290;&#22312;&#31532;&#20108;&#31867;&#20013;&#65292;&#25439;&#22833;&#20445;&#25345;&#36828;&#31163;&#20110;&#38646;&#12290;&#37027;&#20040;&#65292;&#22914;&#20309;&#36991;&#20813;&#23646;&#20110;&#21518;&#19968;&#31867;&#30340;&#26497;&#23567;&#20540;&#21602;&#65311;&#24184;&#36816;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;&#26497;&#23567;&#20540;&#20174;&#19981;&#20250;&#34987;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#26816;&#27979;&#21040;&#12290;&#21463;&#21040;&#27492;&#29616;&#35937;&#24615;&#36136;&#30340;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#30740;&#31350;&#38544;&#34255;&#26497;&#23567;&#20540;&#29420;&#29305;&#35299;&#26512;&#24615;&#36136;&#30340;&#26041;&#27861;&#12290;&#26681;&#25454;&#29616;&#26377;&#30340;&#20998;&#26512;&#65292;&#20004;&#31181;&#31867;&#22411;&#30340;Hessian&#35889;&#22312;$O(d^{-1/2})$&#39033;&#27169;&#24847;&#20041;&#19979;&#19968;&#33268; -- &#19981;&#22826;&#20048;&#35266;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#25439;&#22833;&#34987;&#26368;&#23567;&#21270;&#25110;&#26368;&#22823;&#21270;&#30340;&#26354;&#32447;&#36827;&#34892;&#65292;&#36890;&#24120;&#31216;&#20026;&#20999;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16819v2 Announce Type: replace  Abstract: The optimization problem associated to fitting two-layer ReLU networks having $d$~inputs, $k$~neurons, and labels generated by a target network, is considered. Two types of infinite families of spurious minima, giving one minimum per $d$, were recently found. The loss at minima belonging to the first type converges to zero as $d$ increases. In the second type, the loss remains bounded away from zero. That being so, how may one avoid minima belonging to the latter type? Fortunately, such minima are never detected by standard optimization methods. Motivated by questions concerning the nature of this phenomenon, we develop methods to study distinctive analytic properties of hidden minima.   By existing analyses, the Hessian spectrum of both types agree modulo $O(d^{-1/2})$-terms -- not promising. Thus, rather, our investigation proceeds by studying curves along which the loss is minimized or maximized, generally referred to as tangency 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.06122</link><description>&lt;p&gt;
&#29992;&#26799;&#24230;&#24377;&#23556;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Manipulating Feature Visualizations with Gradient Slingshots. (arXiv:2401.06122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#28608;&#27963;&#26368;&#22823;&#21270;&#26041;&#27861;&#22312;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#20013;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20197;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#32780;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#65292;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#35821;&#20041;&#24615;&#36136;&#20173;&#28982;&#26410;&#30693;&#12290;&#35299;&#37322;DNNs&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24120;&#29992;&#26041;&#27861;&#26159;&#28608;&#27963;&#26368;&#22823;&#21270;(AM)&#65292;&#23427;&#29983;&#25104;&#19968;&#20010;&#21512;&#25104;&#30340;&#36755;&#20837;&#20449;&#21495;&#65292;&#26368;&#22823;&#21270;&#28608;&#27963;&#32593;&#32476;&#20013;&#30340;&#29305;&#23450;&#31070;&#32463;&#20803;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23545;&#25239;&#27169;&#22411;&#25805;&#20316;&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25805;&#32437;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#32780;&#19981;&#25913;&#21464;&#27169;&#22411;&#32467;&#26500;&#25110;&#23545;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20960;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#38544;&#34255;&#29305;&#23450;&#31070;&#32463;&#20803;&#21151;&#33021;&#30340;&#33021;&#21147;&#65292;&#22312;&#27169;&#22411;&#23457;&#26680;&#36807;&#31243;&#20013;&#20351;&#29992;&#36873;&#25321;&#30340;&#30446;&#26631;&#35299;&#37322;&#23631;&#34109;&#20102;&#21407;&#22987;&#35299;&#37322;&#12290;&#20316;&#20026;&#19968;&#31181;&#34917;&#25937;&#25514;&#26045;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#36825;&#31181;&#25805;&#32437;&#30340;&#38450;&#25252;&#25514;&#26045;&#65292;&#24182;&#25552;&#20379;&#20102;&#23450;&#37327;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are capable of learning complex and versatile representations, however, the semantic nature of the learned concepts remains unknown. A common method used to explain the concepts learned by DNNs is Activation Maximization (AM), which generates a synthetic input signal that maximally activates a particular neuron in the network. In this paper, we investigate the vulnerability of this approach to adversarial model manipulations and introduce a novel method for manipulating feature visualization without altering the model architecture or significantly impacting the model's decision-making process. We evaluate the effectiveness of our method on several neural network models and demonstrate its capabilities to hide the functionality of specific neurons by masking the original explanations of neurons with chosen target explanations during model auditing. As a remedy, we propose a protective measure against such manipulations and provide quantitative evidence which 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.01172</link><description>&lt;p&gt;
&#25391;&#21160;&#20449;&#21495;&#30340;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#29992;&#20110;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
Quadratic Time-Frequency Analysis of Vibration Signals for Diagnosing Bearing Faults. (arXiv:2401.01172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#23454;&#38469;&#26465;&#20214;&#19979;&#35786;&#26029;&#24102;&#26377;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#36724;&#25215;&#25925;&#38556;&#12290;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#22320;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36724;&#25215;&#25925;&#38556;&#30340;&#35786;&#26029;&#23545;&#20110;&#38477;&#20302;&#32500;&#20462;&#25104;&#26412;&#21644;&#35774;&#22791;&#20572;&#26426;&#33267;&#20851;&#37325;&#35201;&#12290;&#36724;&#25215;&#25925;&#38556;&#26159;&#26426;&#22120;&#25391;&#21160;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#20998;&#26512;&#20854;&#20449;&#21495;&#24418;&#24577;&#21487;&#20197;&#25581;&#31034;&#20854;&#20581;&#24247;&#29366;&#20917;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38024;&#23545;&#25511;&#21046;&#29615;&#22659;&#36827;&#34892;&#20248;&#21270;&#65292;&#24573;&#30053;&#20102;&#23454;&#38469;&#26465;&#20214;&#19979;&#30340;&#26102;&#38388;&#21464;&#21270;&#30340;&#36716;&#36895;&#21644;&#25391;&#21160;&#30340;&#38750;&#24179;&#31283;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#39057;&#29575;&#20998;&#26512;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#34701;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#21464;&#21270;&#36895;&#24230;&#21644;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#19979;&#35786;&#26029;&#36724;&#25215;&#25925;&#38556;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#36724;&#25215;&#25925;&#38556;&#24341;&#36215;&#30340;&#25391;&#21160;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#38750;&#24179;&#31283;&#24615;&#19982;&#36724;&#25215;&#22266;&#26377;&#21644;&#25805;&#20316;&#21442;&#25968;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#36824;&#38416;&#36848;&#20102;&#20108;&#27425;&#26102;&#38388;&#39057;&#29575;&#20998;&#24067;&#65292;&#24182;&#39564;&#35777;&#20102;&#23427;&#20204;&#35299;&#26512;&#19982;&#19981;&#21516;&#36724;&#25215;&#25925;&#38556;&#30456;&#20851;&#30340;&#29420;&#29305;&#21160;&#24577;&#27169;&#24335;&#30340;&#26377;&#25928;&#24615;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26102;&#38388;&#39057;&#29575;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diagnosis of bearing faults is paramount to reducing maintenance costs and operational breakdowns. Bearing faults are primary contributors to machine vibrations, and analyzing their signal morphology offers insights into their health status. Unfortunately, existing approaches are optimized for controlled environments, neglecting realistic conditions such as time-varying rotational speeds and the vibration's non-stationary nature. This paper presents a fusion of time-frequency analysis and deep learning techniques to diagnose bearing faults under time-varying speeds and varying noise levels. First, we formulate the bearing fault-induced vibrations and discuss the link between their non-stationarity and the bearing's inherent and operational parameters. We also elucidate quadratic time-frequency distributions and validate their effectiveness in resolving distinctive dynamic patterns associated with different bearing faults. Based on this, we design a time-frequency convolutional neural n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2310.20350</link><description>&lt;p&gt;
&#23558;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#32467;&#21512;&#65292;&#23454;&#29616;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;
&lt;/p&gt;
&lt;p&gt;
Combining Shape Completion and Grasp Prediction for Fast and Versatile Grasping with a Multi-Fingered Hand. (arXiv:2310.20350v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20350
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#24418;&#29366;&#23436;&#25104;&#21644;&#25235;&#21462;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28789;&#27963;&#30340;&#22810;&#25351;&#25235;&#21462;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36741;&#21161;&#26426;&#22120;&#20154;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#26377;&#38480;&#25110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#29289;&#20307;&#36827;&#34892;&#25235;&#21462;&#26159;&#19968;&#39033;&#38750;&#24120;&#37325;&#35201;&#30340;&#25216;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#26222;&#36866;&#24773;&#20917;&#19979;&#65292;&#23588;&#20854;&#26159;&#22312;&#35266;&#27979;&#33021;&#21147;&#26377;&#38480;&#21644;&#21033;&#29992;&#22810;&#25351;&#25163;&#36827;&#34892;&#28789;&#27963;&#25235;&#21462;&#26102;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#24555;&#36895;&#21644;&#39640;&#20445;&#30495;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#65292;&#30001;&#22522;&#20110;&#21333;&#20010;&#28145;&#24230;&#22270;&#20687;&#30340;&#24418;&#29366;&#23436;&#25104;&#27169;&#22359;&#21644;&#22522;&#20110;&#39044;&#27979;&#30340;&#29289;&#20307;&#24418;&#29366;&#30340;&#25235;&#21462;&#39044;&#27979;&#22120;&#32452;&#25104;&#12290;&#24418;&#29366;&#23436;&#25104;&#32593;&#32476;&#22522;&#20110;VQDIF&#65292;&#22312;&#20219;&#24847;&#26597;&#35810;&#28857;&#19978;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#20540;&#12290;&#20316;&#20026;&#25235;&#21462;&#39044;&#27979;&#22120;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20004;&#38454;&#27573;&#26550;&#26500;&#65292;&#39318;&#20808;&#20351;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#25163;&#23039;&#21183;&#65292;&#28982;&#21518;&#22238;&#24402;&#27599;&#20010;&#23039;&#21183;&#30340;&#25163;&#25351;&#20851;&#33410;&#37197;&#32622;&#12290;&#20851;&#38190;&#22240;&#32032;&#26159;&#36275;&#22815;&#30340;&#25968;&#25454;&#30495;&#23454;&#24615;&#21644;&#22686;&#24378;&#65292;&#20197;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#22256;&#38590;&#24773;&#20917;&#30340;&#29305;&#27530;&#20851;&#27880;&#12290;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#24179;&#21488;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#25235;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Grasping objects with limited or no prior knowledge about them is a highly relevant skill in assistive robotics. Still, in this general setting, it has remained an open problem, especially when it comes to only partial observability and versatile grasping with multi-fingered hands. We present a novel, fast, and high fidelity deep learning pipeline consisting of a shape completion module that is based on a single depth image, and followed by a grasp predictor that is based on the predicted object shape. The shape completion network is based on VQDIF and predicts spatial occupancy values at arbitrary query points. As grasp predictor, we use our two-stage architecture that first generates hand poses using an autoregressive model and then regresses finger joint configurations per pose. Critical factors turn out to be sufficient data realism and augmentation, as well as special attention to difficult cases during training. Experiments on a physical robot platform demonstrate successful gras
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#36827;&#34892;&#26465;&#20214;&#25512;&#26029;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#25209;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#26159;&#26368;&#20248;&#30340;&#65307;&#24403;&#23454;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#38754;&#26159;&#20301;&#32622;&#19981;&#21464;&#30340;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65307;&#22312;&#20572;&#27490;&#26102;&#38388;&#12289;&#20998;&#37197;&#27010;&#29575;&#21644;&#30446;&#26631;&#21442;&#25968;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#22810;&#38754;&#20307;&#20107;&#20214;&#38598;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#35745;&#31639;&#21487;&#34892;&#19988;&#26368;&#20248;&#30340;&#26465;&#20214;&#25512;&#26029;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2309.12162</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#30340;&#26368;&#20248;&#26465;&#20214;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Optimal Conditional Inference in Adaptive Experiments. (arXiv:2309.12162v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12162
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#33258;&#36866;&#24212;&#23454;&#39564;&#20013;&#36827;&#34892;&#26465;&#20214;&#25512;&#26029;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#25209;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#26159;&#26368;&#20248;&#30340;&#65307;&#24403;&#23454;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#38754;&#26159;&#20301;&#32622;&#19981;&#21464;&#30340;&#26102;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;&#20102;&#39069;&#22806;&#30340;&#20449;&#24687;&#65307;&#22312;&#20572;&#27490;&#26102;&#38388;&#12289;&#20998;&#37197;&#27010;&#29575;&#21644;&#30446;&#26631;&#21442;&#25968;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#22810;&#38754;&#20307;&#20107;&#20214;&#38598;&#21512;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#35745;&#31639;&#21487;&#34892;&#19988;&#26368;&#20248;&#30340;&#26465;&#20214;&#25512;&#26029;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#25209;&#37327;&#36172;&#24466;&#23454;&#39564;&#65292;&#24182;&#32771;&#34385;&#20102;&#22312;&#23454;&#29616;&#20572;&#27490;&#26102;&#38388;&#12289;&#20998;&#37197;&#27010;&#29575;&#21644;&#30446;&#26631;&#21442;&#25968;&#30340;&#26465;&#20214;&#19979;&#36827;&#34892;&#25512;&#26029;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#25152;&#26377;&#36825;&#20123;&#21487;&#33021;&#37117;&#26159;&#26681;&#25454;&#23454;&#39564;&#30340;&#26368;&#21518;&#19968;&#25209;&#20449;&#24687;&#36827;&#34892;&#33258;&#36866;&#24212;&#36873;&#25321;&#30340;&#12290;&#22312;&#27809;&#26377;&#23545;&#23454;&#39564;&#36827;&#34892;&#36827;&#19968;&#27493;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20165;&#20351;&#29992;&#26368;&#21518;&#19968;&#25209;&#32467;&#26524;&#36827;&#34892;&#25512;&#26029;&#26159;&#26368;&#20248;&#30340;&#12290;&#24403;&#23454;&#39564;&#30340;&#33258;&#36866;&#24212;&#26041;&#38754;&#34987;&#35748;&#20026;&#26159;&#20301;&#32622;&#19981;&#21464;&#30340;&#65292;&#21363;&#24403;&#25105;&#20204;&#23558;&#25152;&#26377;&#25209;&#27425;-&#33218;&#30340;&#24179;&#22343;&#20540;&#37117;&#21521;&#19968;&#20010;&#24120;&#25968;&#31227;&#21160;&#26102;&#65292;&#25105;&#20204;&#35777;&#26126;&#25968;&#25454;&#20013;&#36824;&#23384;&#22312;&#39069;&#22806;&#30340;&#20449;&#24687;&#65292;&#21487;&#20197;&#36890;&#36807;&#19968;&#20010;&#39069;&#22806;&#30340;&#25209;&#27425;-&#33218;&#22343;&#20540;&#30340;&#32447;&#24615;&#20989;&#25968;&#26469;&#25429;&#25417;&#12290;&#22312;&#26356;&#20005;&#26684;&#30340;&#24773;&#20917;&#19979;&#65292;&#20572;&#27490;&#26102;&#38388;&#12289;&#20998;&#37197;&#27010;&#29575;&#21644;&#30446;&#26631;&#21442;&#25968;&#34987;&#35748;&#20026;&#20165;&#20381;&#36182;&#20110;&#25968;&#25454;&#36890;&#36807;&#19968;&#20010;&#22810;&#38754;&#20307;&#20107;&#20214;&#30340;&#38598;&#21512;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#35745;&#31639;&#21487;&#34892;&#19988;&#26368;&#20248;&#30340;&#26465;&#20214;&#25512;&#26029;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study batched bandit experiments and consider the problem of inference conditional on the realized stopping time, assignment probabilities, and target parameter, where all of these may be chosen adaptively using information up to the last batch of the experiment. Absent further restrictions on the experiment, we show that inference using only the results of the last batch is optimal. When the adaptive aspects of the experiment are known to be location-invariant, in the sense that they are unchanged when we shift all batch-arm means by a constant, we show that there is additional information in the data, captured by one additional linear function of the batch-arm means. In the more restrictive case where the stopping time, assignment probabilities, and target parameter are known to depend on the data only through a collection of polyhedral events, we derive computationally tractable and optimal conditional inference procedures.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.00377</link><description>&lt;p&gt;
&#24102;&#26377;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#30340;&#24418;&#29366;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Shape Completion with Prediction of Uncertain Regions. (arXiv:2308.00377v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00377
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#22788;&#29702;&#22312;&#32473;&#23450;&#27169;&#31946;&#29289;&#20307;&#35270;&#22270;&#26102;&#21487;&#33021;&#23384;&#22312;&#30340;&#29289;&#20307;&#37096;&#20998;&#30340;&#19981;&#30830;&#23450;&#21306;&#22495;&#39044;&#27979;&#38382;&#39064;&#12290;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#39044;&#27979;&#31354;&#38388;&#21344;&#29992;&#30340;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#25110;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#26041;&#27861;&#19982;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#28145;&#24230;&#22270;&#20687;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#23436;&#25104;&#65292;&#21363;&#20174;&#37096;&#20998;&#35266;&#27979;&#39044;&#27979;&#29289;&#20307;&#30340;&#23436;&#25972;&#20960;&#20309;&#24418;&#29366;&#65292;&#23545;&#20110;&#20960;&#20010;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#26426;&#22120;&#20154;&#25805;&#20316;&#12290;&#24403;&#22522;&#20110;&#29289;&#20307;&#24418;&#29366;&#37325;&#24314;&#36827;&#34892;&#35268;&#21010;&#25110;&#23454;&#38469;&#25235;&#21462;&#30340;&#39044;&#27979;&#26102;&#65292;&#25351;&#31034;&#20005;&#37325;&#20960;&#20309;&#19981;&#30830;&#23450;&#24615;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#29305;&#21035;&#26159;&#22312;&#32473;&#23450;&#27169;&#31946;&#30340;&#29289;&#20307;&#35270;&#22270;&#26102;&#65292;&#22312;&#25972;&#20010;&#29289;&#20307;&#37096;&#20998;&#23384;&#22312; irreducible uncertainty &#30340;&#25193;&#23637;&#21306;&#22495;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#31181;&#37325;&#35201;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#26469;&#39044;&#27979;&#36825;&#20123;&#19981;&#30830;&#23450;&#21306;&#22495;&#65292;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#20316;&#20026;&#39044;&#27979;&#23616;&#37096;&#31354;&#38388;&#21344;&#29992;&#30340;&#20219;&#20309;&#26041;&#27861;&#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#19968;&#31181;&#26159;&#36890;&#36807;&#21518;&#22788;&#29702;&#21344;&#29992;&#35780;&#20998;&#65292;&#21478;&#19968;&#31181;&#26159;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#25351;&#26631;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#27861;&#19982;&#20004;&#31181;&#24050;&#30693;&#30340;&#27010;&#29575;&#24418;&#29366;&#23436;&#25104;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#29983;&#25104;&#20102;&#19968;&#20010;&#22522;&#20110;ShapeNet&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#30495;&#23454;&#28210;&#26579;&#30340;&#29289;&#20307;&#35270;&#22270;&#28145;&#24230;&#22270;&#20687;&#21450;&#20854;&#24102;&#26377;&#22320;&#38754;&#30495;&#20540;&#26631;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape completion, i.e., predicting the complete geometry of an object from a partial observation, is highly relevant for several downstream tasks, most notably robotic manipulation. When basing planning or prediction of real grasps on object shape reconstruction, an indication of severe geometric uncertainty is indispensable. In particular, there can be an irreducible uncertainty in extended regions about the presence of entire object parts when given ambiguous object views. To treat this important case, we propose two novel methods for predicting such uncertain regions as straightforward extensions of any method for predicting local spatial occupancy, one through postprocessing occupancy scores, the other through direct prediction of an uncertainty indicator. We compare these methods together with two known approaches to probabilistic shape completion. Moreover, we generate a dataset, derived from ShapeNet, of realistically rendered depth images of object views with ground-truth annot
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2307.10634</link><description>&lt;p&gt;
&#20154;&#31867;&#22522;&#22240;&#26680;&#33527;&#37240;&#24207;&#21015;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Language Models on Nucleotide Sequences of Human Genes. (arXiv:2307.10634v1 [q-bio.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#22635;&#34917;&#20102;DNA&#24207;&#21015;&#29983;&#25104;&#27169;&#22411;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;DNA&#30456;&#20851;&#30340;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#65292;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#31867;&#20284;&#20110;GPT-3&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#12290;&#32771;&#34385;&#21040;&#22788;&#29702;&#25972;&#20010;DNA&#24207;&#21015;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#65292;&#25105;&#20204;&#20915;&#23450;&#22312;&#26356;&#23567;&#30340;&#23610;&#24230;&#19978;&#36827;&#34892;&#30740;&#31350;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#31867;&#22522;&#22240;&#30340;&#26680;&#33527;&#37240;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;DNA&#12290;&#36825;&#20010;&#20915;&#31574;&#24182;&#19981;&#25913;&#21464;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#22240;&#20026;DNA&#21644;&#22522;&#22240;&#37117;&#21487;&#20197;&#30475;&#20316;&#30001;&#22235;&#31181;&#19981;&#21516;&#30340;&#26680;&#33527;&#37240;&#32452;&#25104;&#30340;&#19968;&#32500;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models, primarily transformer-based ones, obtained colossal success in NLP. To be more precise, studies like BERT in NLU and works such as GPT-3 for NLG are very crucial. DNA sequences are very close to natural language in terms of structure, so if the DNA-related bioinformatics domain is concerned, discriminative models, like DNABert, exist. Yet, the generative side of the coin is mainly unexplored to the best of our knowledge. Consequently, we focused on developing an autoregressive generative language model like GPT-3 for DNA sequences. Because working with whole DNA sequences is challenging without substantial computational resources, we decided to carry out our study on a smaller scale, focusing on nucleotide sequences of human genes, unique parts in DNA with specific functionalities, instead of the whole DNA. This decision did not change the problem structure a lot due to the fact that both DNA and genes can be seen as 1D sequences consisting of four different nucleotide
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#26469;&#24809;&#32602;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#32467;&#26524;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#20855;&#26377;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#23545;&#35937;&#12290;</title><link>http://arxiv.org/abs/2307.03137</link><description>&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Topology-Aware Loss for Aorta and Great Vessel Segmentation in Computed Tomography Images. (arXiv:2307.03137v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#26469;&#24809;&#32602;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#20998;&#21106;&#32467;&#26524;&#19982;&#30495;&#23454;&#20540;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25913;&#21892;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38024;&#23545;&#20855;&#26377;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20351;&#29992;&#26631;&#20934;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#20998;&#21106;&#32593;&#32476;&#26102;&#65292;&#32593;&#32476;&#24182;&#27809;&#26377;&#26126;&#30830;&#34987;&#35201;&#27714;&#23398;&#20064;&#22270;&#20687;&#30340;&#20840;&#23616;&#19981;&#21464;&#24615;&#65292;&#22914;&#23545;&#35937;&#30340;&#24418;&#29366;&#21644;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#19981;&#21464;&#24615;&#32435;&#20837;&#32593;&#32476;&#35757;&#32451;&#20013;&#21487;&#33021;&#26377;&#21161;&#20110;&#25913;&#21892;&#21508;&#31181;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#24403;&#23427;&#20204;&#26159;&#38656;&#35201;&#20998;&#21106;&#30340;&#23545;&#35937;&#30340;&#22266;&#26377;&#29305;&#24615;&#26102;&#12290;&#26412;&#25991;&#20197;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#20027;&#21160;&#33033;&#21644;&#22823;&#34880;&#31649;&#30340;&#20998;&#21106;&#20026;&#20363;&#65292;&#36825;&#20123;&#34880;&#31649;&#30001;&#20110;&#20154;&#20307;&#35299;&#21078;&#23398;&#65292;&#36890;&#24120;&#22312;&#36523;&#20307;&#20013;&#20197;&#29305;&#23450;&#30340;&#20960;&#20309;&#24418;&#29366;&#20986;&#29616;&#65292;&#24182;&#22312;2D CT&#22270;&#20687;&#19978;&#20027;&#35201;&#21576;&#29616;&#20026;&#22278;&#24418;&#23545;&#35937;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#25299;&#25169;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#25345;&#20037;&#21516;&#35843;&#24809;&#32602;&#22320;&#38754;&#30495;&#23454;&#20540;&#21644;&#39044;&#27979;&#20043;&#38388;&#30340;&#25299;&#25169;&#24046;&#24322;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#19982;&#20808;&#21069;&#25552;&#20986;&#30340;&#20998;&#21106;&#32593;&#32476;&#35774;&#35745;&#19981;&#21516;&#65292;&#20808;&#21069;&#30340;&#35774;&#35745;&#26159;&#23558;&#38408;&#20540;&#28388;&#27874;&#24212;&#29992;&#20110;&#39044;&#27979;&#22270;&#20687;&#30340;&#20284;&#28982;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation networks are not explicitly imposed to learn global invariants of an image, such as the shape of an object and the geometry between multiple objects, when they are trained with a standard loss function. On the other hand, incorporating such invariants into network training may help improve performance for various segmentation tasks when they are the intrinsic characteristics of the objects to be segmented. One example is segmentation of aorta and great vessels in computed tomography (CT) images where vessels are found in a particular geometry in the body due to the human anatomy and they mostly seem as round objects on a 2D CT image. This paper addresses this issue by introducing a new topology-aware loss function that penalizes topology dissimilarities between the ground truth and prediction through persistent homology. Different from the previously suggested segmentation network designs, which apply the threshold filtration on a likelihood function of the prediction map 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04919</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#36125;&#21494;&#26031;&#31890;&#23376;&#27969;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Cross-Domain Soft Sensor Modelling via A Deep Bayesian Particle Flow Framework. (arXiv:2306.04919v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031;&#26694;&#26550;&#30340;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#26080;&#30417;&#30563;&#24314;&#27169;&#26041;&#27861;&#65292;&#33021;&#22815;&#35299;&#20915;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#25552;&#39640;&#36719;&#27979;&#37327;&#24314;&#27169;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36719;&#27979;&#37327;&#23545;&#20110;&#36890;&#36807;&#21487;&#38752;&#30340;&#29366;&#24577;&#25512;&#26029;&#23454;&#29616;&#31934;&#30830;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#26631;&#31614;&#32570;&#22833;&#12289;&#39046;&#22495;&#36866;&#24212;&#24615;&#21644;&#25968;&#25454;&#26102;&#38388;&#19968;&#33268;&#24615;&#31561;&#38382;&#39064;&#65292;&#24320;&#21457;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#36719;&#27979;&#37327;&#27169;&#22411;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#31890;&#23376;&#27969;&#36125;&#21494;&#26031; (DPFB) &#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26080;&#30446;&#26631;&#29366;&#24577;&#26631;&#31614;&#24773;&#20917;&#19979;&#36827;&#34892;&#36328;&#39046;&#22495;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#39318;&#20808;&#21046;&#23450;&#20102;&#19968;&#20010;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#65292;&#20197;&#25191;&#34892;&#28508;&#22312;&#30340;&#36328;&#39046;&#22495;&#36719;&#24863;&#30693;&#38382;&#39064;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#12290;&#22312;&#26694;&#26550;&#26680;&#24515;&#65292;&#25105;&#20204;&#32467;&#21512;&#29289;&#29702;&#23398;&#21551;&#21457;&#30340;&#31890;&#23376;&#27969;&#65292;&#36890;&#36807;&#20248;&#21270;&#39034;&#24207;&#36125;&#21494;&#26031;&#30446;&#26631;&#26469;&#25191;&#34892;&#27169;&#22411;&#25552;&#21462;&#30340;&#28508;&#22312;&#21644;&#38544;&#34255;&#29305;&#24449;&#30340;&#31934;&#30830;&#36125;&#21494;&#26031;&#26356;&#26032;&#12290;&#30001;&#27492;&#65292;&#36825;&#20123;&#36129;&#29486;&#20351;&#24471;&#35813;&#26694;&#26550;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#26377;&#26426;&#30340;&#36817;&#20284;&#21518;&#39564;&#29305;&#24449;&#34920;&#31034;&#65292;&#33021;&#22815;&#34920;&#24449;&#22797;&#26434;&#30340;&#36328;&#39046;&#22495;&#31995;&#32479;&#21160;&#21147;&#23398;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#36719;&#27979;&#37327;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-driven soft sensors are essential for achieving accurate perception through reliable state inference. However, developing representative soft sensor models is challenged by issues such as missing labels, domain adaptability, and temporal coherence in data. To address these challenges, we propose a deep Particle Flow Bayes (DPFB) framework for cross-domain soft sensor modeling in the absence of target state labels. In particular, a sequential Bayes objective is first formulated to perform the maximum likelihood estimation underlying the cross-domain soft sensing problem. At the core of the framework, we incorporate a physics-inspired particle flow that optimizes the sequential Bayes objective to perform an exact Bayes update of the model extracted latent and hidden features. As a result, these contributions enable the proposed framework to learn a cohesive approximate posterior feature representation capable of characterizing complex cross-domain system dynamics and performing effe
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32422;&#26463;&#21644;&#26080;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#24182;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#26469;&#23637;&#31034;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10932</link><description>&lt;p&gt;
&#20851;&#20110;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected Conditional Risk Measures. (arXiv:2301.10932v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32422;&#26463;&#21644;&#26080;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#24182;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#26469;&#23637;&#31034;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#25511;&#21046;&#19981;&#30830;&#23450;&#32467;&#26524;&#21644;&#30830;&#20445;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#21487;&#38752;&#24615;&#33021;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#34429;&#28982;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#20855;&#26377;&#19982;&#39118;&#38505;&#20013;&#24615;&#24773;&#20917;&#19979;&#30456;&#21516;&#30340;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#21160;&#24577;&#26102;&#38388;&#19968;&#33268;&#39118;&#38505;&#24230;&#37327;&#65292;&#31216;&#20026;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#65288;ECRM&#65289;&#65292;&#24182;&#20026;&#22522;&#20110;ECRM&#30340;&#30446;&#26631;&#20989;&#25968;&#25512;&#23548;&#20986;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#12290;&#22312;&#32422;&#26463;&#30452;&#25509;&#21442;&#25968;&#21270;&#21644;&#26080;&#32422;&#26463;softmax&#21442;&#25968;&#21270;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Risk-sensitive reinforcement learning (RL) has become a popular tool to control the risk of uncertain outcomes and ensure reliable performance in various sequential decision-making problems. While policy gradient methods have been developed for risk-sensitive RL, it remains unclear if these methods enjoy the same global convergence guarantees as in the risk-neutral case. In this paper, we consider a class of dynamic time-consistent risk measures, called Expected Conditional Risk Measures (ECRMs), and derive policy gradient updates for ECRM-based objective functions. Under both constrained direct parameterization and unconstrained softmax parameterization, we provide global convergence and iteration complexities of the corresponding risk-averse policy gradient algorithms. We further test risk-averse variants of REINFORCE and actor-critic algorithms to demonstrate the efficacy of our method and the importance of risk control.
&lt;/p&gt;</description></item></channel></rss>