<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.01685</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Methodology for Improving Accuracy of Embedded Spiking Neural Networks through Kernel Size Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01685
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;&#23884;&#20837;&#24335;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#20934;&#30830;&#24615;&#30340;&#26041;&#27861;&#23398;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30001;&#20110;&#20854;&#31232;&#30095;&#30340;&#22522;&#20110;&#33033;&#20914;&#30340;&#25805;&#20316;&#32780;&#33021;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#25552;&#20379;&#36229;&#20302;&#21151;&#32791;/&#33021;&#32791;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#38656;&#35201;&#26356;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#25165;&#33021;&#23454;&#29616;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#24212;&#29992;&#19981;&#22826;&#36866;&#21512;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#24320;&#21457;&#33021;&#22815;&#20197;&#21487;&#25509;&#21463;&#30340;&#20869;&#23384;&#21344;&#29992;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#30340;SNNs&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#22823;&#23567;&#32553;&#25918;&#25552;&#39640;SNNs&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#23398;&#12290;&#20854;&#20851;&#38190;&#27493;&#39588;&#21253;&#25324;&#35843;&#26597;&#19981;&#21516;&#26680;&#22823;&#23567;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#35774;&#35745;&#26032;&#30340;&#26680;&#22823;&#23567;&#38598;&#21512;&#65292;&#22522;&#20110;&#36873;&#23450;&#30340;&#26680;&#22823;&#23567;&#29983;&#25104;SNN&#26550;&#26500;&#65292;&#24182;&#20998;&#26512;SNN&#27169;&#22411;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;-&#20869;&#23384;&#25240;&#34935;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23398;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#23545;&#20110;CIFAR10&#26377;93.24%&#30340;&#20934;&#30830;&#24230;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01685v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) can offer ultra low power/ energy consumption for machine learning-based applications due to their sparse spike-based operations. Currently, most of the SNN architectures need a significantly larger model size to achieve higher accuracy, which is not suitable for resource-constrained embedded applications. Therefore, developing SNNs that can achieve high accuracy with acceptable memory footprint is highly needed. Toward this, we propose a novel methodology that improves the accuracy of SNNs through kernel size scaling. Its key steps include investigating the impact of different kernel sizes on the accuracy, devising new sets of kernel sizes, generating SNN architectures based on the selected kernel sizes, and analyzing the accuracy-memory trade-offs for SNN model selection. The experimental results show that our methodology achieves higher accuracy than state-of-the-art (93.24% accuracy for CIFAR10 and 70
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.20233</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Functional Bilevel Optimization for Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20233
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19981;&#20381;&#36182;&#20110;&#24378;&#20984;&#20551;&#35774;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#30340;&#19968;&#31181;&#26032;&#30340;&#20989;&#25968;&#35270;&#35282;&#65292;&#20854;&#20013;&#20869;&#37096;&#30446;&#26631;&#22312;&#20989;&#25968;&#31354;&#38388;&#19978;&#34987;&#26368;&#23567;&#21270;&#12290;&#36825;&#20123;&#31867;&#22411;&#30340;&#38382;&#39064;&#36890;&#24120;&#36890;&#36807;&#22312;&#21442;&#25968;&#35774;&#32622;&#19979;&#24320;&#21457;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#65292;&#20854;&#20013;&#20869;&#37096;&#30446;&#26631;&#23545;&#20110;&#39044;&#27979;&#20989;&#25968;&#30340;&#21442;&#25968;&#24378;&#20984;&#12290;&#20989;&#25968;&#35270;&#35282;&#19981;&#20381;&#36182;&#20110;&#27492;&#20551;&#35774;&#65292;&#29305;&#21035;&#20801;&#35768;&#20351;&#29992;&#36229;&#21442;&#25968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#20869;&#37096;&#39044;&#27979;&#20989;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#20989;&#25968;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#36866;&#21512;&#33258;&#28982;&#20989;&#25968;&#21452;&#23618;&#32467;&#26500;&#30340;&#20202;&#34920;&#22238;&#24402;&#21644;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20233v1 Announce Type: cross  Abstract: In this paper, we introduce a new functional point of view on bilevel optimization problems for machine learning, where the inner objective is minimized over a function space. These types of problems are most often solved by using methods developed in the parametric setting, where the inner objective is strongly convex with respect to the parameters of the prediction function. The functional point of view does not rely on this assumption and notably allows using over-parameterized neural networks as the inner prediction function. We propose scalable and efficient algorithms for the functional bilevel optimization problem and illustrate the benefits of our approach on instrumental regression and reinforcement learning tasks, which admit natural functional bilevel structures.
&lt;/p&gt;</description></item><item><title>Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19546</link><description>&lt;p&gt;
Croissant&#65306;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;
&lt;/p&gt;
&lt;p&gt;
Croissant: A Metadata Format for ML-Ready Datasets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19546
&lt;/p&gt;
&lt;p&gt;
Croissant&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#20851;&#38190;&#36164;&#28304;&#65292;&#20294;&#22788;&#29702;&#25968;&#25454;&#20173;&#28982;&#26159;&#19968;&#20010;&#20027;&#35201;&#30340;&#25705;&#25830;&#28857;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;Croissant&#65292;&#19968;&#31181;&#29992;&#20110;&#25968;&#25454;&#38598;&#30340;&#20803;&#25968;&#25454;&#26684;&#24335;&#65292;&#31616;&#21270;&#20102;&#25968;&#25454;&#34987;ML&#24037;&#20855;&#21644;&#26694;&#26550;&#20351;&#29992;&#30340;&#26041;&#24335;&#12290;Croissant&#20351;&#25968;&#25454;&#38598;&#26356;&#26131;&#21457;&#29616;&#12289;&#21487;&#31227;&#26893;&#21644;&#20114;&#25805;&#20316;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;ML&#25968;&#25454;&#31649;&#29702;&#21644;&#36127;&#36131;&#20219;AI&#20013;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;Croissant&#24050;&#24471;&#21040;&#20960;&#20010;&#27969;&#34892;&#25968;&#25454;&#38598;&#24211;&#30340;&#25903;&#25345;&#65292;&#28085;&#30422;&#25968;&#21313;&#19975;&#20010;&#25968;&#25454;&#38598;&#65292;&#21487;&#20197;&#21152;&#36733;&#21040;&#26368;&#27969;&#34892;&#30340;ML&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19546v1 Announce Type: cross  Abstract: Data is a critical resource for Machine Learning (ML), yet working with data remains a key friction point. This paper introduces Croissant, a metadata format for datasets that simplifies how data is used by ML tools and frameworks. Croissant makes datasets more discoverable, portable and interoperable, thereby addressing significant challenges in ML data management and responsible AI. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, ready to be loaded into the most popular ML frameworks.
&lt;/p&gt;</description></item><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.16851</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT predict article retraction based on Twitter mentions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#26377;&#38382;&#39064;&#30340;&#30740;&#31350;&#25991;&#31456;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26681;&#25454;&#34987;&#25764;&#22238;&#25991;&#31456;&#22312;Twitter&#19978;&#30340;&#25552;&#21450;&#26159;&#21542;&#33021;&#22815;&#22312;&#25991;&#31456;&#34987;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#20998;&#26512;&#20102;&#21253;&#25324;3,505&#31687;&#24050;&#25764;&#22238;&#25991;&#31456;&#21450;&#20854;&#30456;&#20851;Twitter&#25552;&#21450;&#22312;&#20869;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20351;&#29992;&#31895;&#31961;&#31934;&#30830;&#21305;&#37197;&#26041;&#27861;&#33719;&#21462;&#30340;&#20855;&#26377;&#31867;&#20284;&#29305;&#24449;&#30340;3,505&#31687;&#26410;&#25764;&#22238;&#25991;&#31456;&#12290;&#36890;&#36807;&#22235;&#31181;&#39044;&#27979;&#26041;&#27861;&#35780;&#20272;&#20102;Twitter&#25552;&#21450;&#22312;&#39044;&#27979;&#25991;&#31456;&#25764;&#22238;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25163;&#21160;&#26631;&#27880;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;ChatGPT&#12290;&#25163;&#21160;&#26631;&#27880;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30340;&#30830;&#26377;&#34987;&#25764;&#22238;&#30340;&#25991;&#31456;&#65292;&#20854;Twitter&#25552;&#21450;&#21253;&#21547;&#22312;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#30340;&#21487;&#35782;&#21035;&#35777;&#25454;&#65292;&#23613;&#31649;&#23427;&#20204;&#21482;&#21344;&#25152;&#26377;&#34987;&#25764;&#22238;&#25991;&#31456;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16851v1 Announce Type: cross  Abstract: Detecting problematic research articles timely is a vital task. This study explores whether Twitter mentions of retracted articles can signal potential problems with the articles prior to retraction, thereby playing a role in predicting future retraction of problematic articles. A dataset comprising 3,505 retracted articles and their associated Twitter mentions is analyzed, alongside 3,505 non-retracted articles with similar characteristics obtained using the Coarsened Exact Matching method. The effectiveness of Twitter mentions in predicting article retraction is evaluated by four prediction methods, including manual labelling, keyword identification, machine learning models, and ChatGPT. Manual labelling results indicate that there are indeed retracted articles with their Twitter mentions containing recognizable evidence signaling problems before retraction, although they represent only a limited share of all retracted articles with 
&lt;/p&gt;</description></item><item><title>&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#32473;&#20986;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#20998;&#37197;&#29575;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.14713</link><description>&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#19979;&#23457;&#35745;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Auditing Fairness under Unobserved Confounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14713
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#32473;&#20986;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#20998;&#37197;&#29575;&#30340;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#31995;&#32479;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#36328;&#36234;&#20154;&#21475;&#32479;&#35745;&#32447;&#23384;&#22312;&#19981;&#20844;&#24179;&#24615;&#12290;&#28982;&#32780;&#65292;&#19981;&#20844;&#24179;&#24615;&#21487;&#33021;&#38590;&#20197;&#37327;&#21270;&#65292;&#29305;&#21035;&#26159;&#22914;&#26524;&#25105;&#20204;&#23545;&#20844;&#24179;&#24615;&#30340;&#29702;&#35299;&#20381;&#36182;&#20110;&#38590;&#20197;&#34913;&#37327;&#30340;&#39118;&#38505;&#31561;&#35266;&#24565;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#37027;&#20123;&#27809;&#26377;&#20854;&#27835;&#30103;&#23601;&#20250;&#27515;&#20129;&#30340;&#20154;&#24179;&#31561;&#33719;&#24471;&#27835;&#30103;&#65289;&#12290;&#23457;&#35745;&#36825;&#31181;&#19981;&#20844;&#24179;&#24615;&#38656;&#35201;&#20934;&#30830;&#27979;&#37327;&#20010;&#20307;&#39118;&#38505;&#65292;&#32780;&#22312;&#26410;&#35266;&#27979;&#28151;&#26434;&#30340;&#29616;&#23454;&#29615;&#22659;&#20013;&#65292;&#38590;&#20197;&#20272;&#35745;&#12290;&#22312;&#36825;&#20123;&#26410;&#35266;&#27979;&#21040;&#30340;&#22240;&#32032;&#8220;&#35299;&#37322;&#8221;&#26126;&#26174;&#24046;&#24322;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21487;&#33021;&#20302;&#20272;&#25110;&#39640;&#20272;&#19981;&#20844;&#24179;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#22312;&#25918;&#23485;&#25110;&#65288;&#20196;&#20154;&#24778;&#35766;&#22320;&#65289;&#29978;&#33267;&#22312;&#25490;&#38500;&#25152;&#26377;&#30456;&#20851;&#39118;&#38505;&#22240;&#32032;&#34987;&#35266;&#27979;&#21040;&#30340;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#28982;&#21487;&#20197;&#23545;&#39640;&#39118;&#38505;&#20010;&#20307;&#30340;&#20998;&#37197;&#29575;&#32473;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#21033;&#29992;&#20102;&#22312;&#35768;&#22810;&#23454;&#38469;&#29615;&#22659;&#20013;&#65288;&#20363;&#22914;&#24341;&#20837;&#26032;&#22411;&#27835;&#30103;&#65289;&#25105;&#20204;&#25317;&#26377;&#22312;&#20219;&#20309;&#20998;&#37197;&#20043;&#21069;&#30340;&#25968;&#25454;&#30340;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14713v1 Announce Type: cross  Abstract: A fundamental problem in decision-making systems is the presence of inequity across demographic lines. However, inequity can be difficult to quantify, particularly if our notion of equity relies on hard-to-measure notions like risk (e.g., equal access to treatment for those who would die without it). Auditing such inequity requires accurate measurements of individual risk, which is difficult to estimate in the realistic setting of unobserved confounding. In the case that these unobservables "explain" an apparent disparity, we may understate or overstate inequity. In this paper, we show that one can still give informative bounds on allocation rates among high-risk individuals, even while relaxing or (surprisingly) even when eliminating the assumption that all relevant risk factors are observed. We utilize the fact that in many real-world settings (e.g., the introduction of a novel treatment) we have data from a period prior to any alloc
&lt;/p&gt;</description></item><item><title>&#38543;&#26426;&#33293;&#20837;&#25216;&#26415;&#33021;&#26377;&#25928;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;&#65292;&#30830;&#20445;&#33293;&#20837;&#21518;&#30340;&#30697;&#38453;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;</title><link>https://arxiv.org/abs/2403.12278</link><description>&lt;p&gt;
&#38543;&#26426;&#33293;&#20837;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Stochastic Rounding Implicitly Regularizes Tall-and-Thin Matrices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12278
&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#33293;&#20837;&#25216;&#26415;&#33021;&#26377;&#25928;&#38544;&#24335;&#27491;&#21017;&#21270;&#39640;&#30246;&#30697;&#38453;&#65292;&#30830;&#20445;&#33293;&#20837;&#21518;&#30340;&#30697;&#38453;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#38543;&#26426;&#33293;&#20837;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#27969;&#34892;&#65292;&#25105;&#20204;&#32771;&#34385;&#23454;&#30697;&#38453;$\mathbf{A}$&#30340;&#38543;&#26426;&#36817;&#20284;&#33293;&#20837;&#65292;&#20854;&#20013;&#34892;&#25968;&#36828;&#36828;&#22810;&#20110;&#21015;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#29702;&#35770;&#35777;&#25454;&#65292;&#24182;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35780;&#20272;&#25903;&#25345;&#65292;&#39640;&#27010;&#29575;&#19979;&#65292;&#38543;&#26426;&#33293;&#20837;&#30697;&#38453;&#30340;&#26368;&#23567;&#22855;&#24322;&#20540;&#36828;&#31163;&#38646;--&#26080;&#35770;$\mathbf{A}$&#25509;&#36817;&#22855;&#24322;&#36824;&#26159;$\mathbf{A}$&#22855;&#24322;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#38543;&#26426;&#33293;&#20837;\textit{&#38544;&#24335;&#27491;&#21017;&#21270;}&#39640;&#30246;&#30697;&#38453;$\mathbf{A}$&#65292;&#20351;&#24471;&#33293;&#20837;&#21518;&#30340;&#29256;&#26412;&#20855;&#26377;&#23436;&#25972;&#30340;&#21015;&#31209;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#20013;&#30340;&#26377;&#21147;&#32467;&#26524;&#65292;&#20197;&#21450;&#38543;&#26426;&#33293;&#20837;&#35823;&#24046;&#19981;&#38598;&#20013;&#22312;&#20302;&#32500;&#21015;&#31354;&#38388;&#30340;&#24605;&#24819;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12278v1 Announce Type: new  Abstract: Motivated by the popularity of stochastic rounding in the context of machine learning and the training of large-scale deep neural network models, we consider stochastic nearness rounding of real matrices $\mathbf{A}$ with many more rows than columns. We provide novel theoretical evidence, supported by extensive experimental evaluation that, with high probability, the smallest singular value of a stochastically rounded matrix is well bounded away from zero -- regardless of how close $\mathbf{A}$ is to being rank deficient and even if $\mathbf{A}$ is rank-deficient. In other words, stochastic rounding \textit{implicitly regularizes} tall and skinny matrices $\mathbf{A}$ so that the rounded version has full column rank. Our proofs leverage powerful results in random matrix theory, and the idea that stochastic rounding errors do not concentrate in low-dimensional column spaces.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#21457;&#29616;&#22312;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#22312;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#20445;&#25345;&#36739;&#23567;&#35268;&#33539;&#65292;&#24182;&#19988;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#26041;&#21521;&#36817;&#20284;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2403.08121</link><description>&lt;p&gt;
&#26089;&#26399;&#26041;&#21521;&#24615;&#25910;&#25947;&#22312;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#20013;&#36827;&#34892;&#23567;&#21021;&#22987;&#21270;&#26102;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08121
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#21457;&#29616;&#22312;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#22312;&#35757;&#32451;&#26089;&#26399;&#38454;&#27573;&#20445;&#25345;&#36739;&#23567;&#35268;&#33539;&#65292;&#24182;&#19988;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;KKT&#28857;&#26041;&#21521;&#36817;&#20284;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35757;&#32451;&#28145;&#24230;&#40784;&#27425;&#31070;&#32463;&#32593;&#32476;&#26102;&#26799;&#24230;&#27969;&#21160;&#21147;&#23398;&#30340;&#21160;&#24577;&#24615;&#65292;&#36825;&#20123;&#32593;&#32476;&#20174;&#23567;&#21021;&#22987;&#21270;&#24320;&#22987;&#12290;&#26412;&#25991;&#32771;&#34385;&#21040;&#20855;&#26377;&#23616;&#37096;Lipschitz&#26799;&#24230;&#21644;&#38454;&#25968;&#20005;&#26684;&#22823;&#20110;&#20004;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#25991;&#31456;&#35777;&#26126;&#20102;&#23545;&#20110;&#36275;&#22815;&#23567;&#30340;&#21021;&#22987;&#21270;&#65292;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#20445;&#25345;&#35268;&#33539;&#36739;&#23567;&#65292;&#24182;&#19988;&#22312;Karush-Kuhn-Tucker (KKT)&#28857;&#22788;&#36817;&#20284;&#27839;&#30528;&#31070;&#32463;&#30456;&#20851;&#20989;&#25968;&#30340;&#26041;&#21521;&#25910;&#25947;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#24179;&#26041;&#25439;&#22833;&#24182;&#22312;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#19978;&#36827;&#34892;&#21487;&#20998;&#31163;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#36824;&#23637;&#31034;&#20102;&#22312;&#25439;&#22833;&#20989;&#25968;&#30340;&#26576;&#20123;&#38797;&#28857;&#38468;&#36817;&#26799;&#24230;&#27969;&#21160;&#21160;&#24577;&#30340;&#31867;&#20284;&#26041;&#21521;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08121v1 Announce Type: new  Abstract: This paper studies the gradient flow dynamics that arise when training deep homogeneous neural networks, starting with small initializations. The present work considers neural networks that are assumed to have locally Lipschitz gradients and an order of homogeneity strictly greater than two. This paper demonstrates that for sufficiently small initializations, during the early stages of training, the weights of the neural network remain small in norm and approximately converge in direction along the Karush-Kuhn-Tucker (KKT) points of the neural correlation function introduced in [1]. Additionally, for square loss and under a separability assumption on the weights of neural networks, a similar directional convergence of gradient flow dynamics is shown near certain saddle points of the loss function.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07937</link><description>&lt;p&gt;
&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65306;&#29992;&#20110;&#35821;&#38899;&#35782;&#21035;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
Speech Robust Bench: A Robustness Benchmark For Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07937
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#22312;&#19981;&#21516;&#20154;&#21475;&#20122;&#32452;&#19978;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#30830;&#20445;&#23427;&#20204;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#30772;&#22351;&#19979;&#36827;&#34892;&#21487;&#38752;&#39044;&#27979;&#21464;&#24471;&#24840;&#21457;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#38899;&#40065;&#26834;&#22522;&#20934;&#65288;SRB&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;ASR&#27169;&#22411;&#23545;&#21508;&#31181;&#30772;&#22351;&#30340;&#40065;&#26834;&#24615;&#30340;&#20840;&#38754;&#22522;&#20934;&#12290;SRB&#30001;69&#20010;&#36755;&#20837;&#25200;&#21160;&#32452;&#25104;&#65292;&#26088;&#22312;&#27169;&#25311;ASR&#27169;&#22411;&#21487;&#33021;&#22312;&#29289;&#29702;&#19990;&#30028;&#21644;&#25968;&#23383;&#19990;&#30028;&#20013;&#36935;&#21040;&#30340;&#21508;&#31181;&#30772;&#22351;&#12290;&#25105;&#20204;&#20351;&#29992;SRB&#26469;&#35780;&#20272;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;ASR&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#22823;&#23567;&#21644;&#26576;&#20123;&#24314;&#27169;&#36873;&#25321;&#65288;&#22914;&#31163;&#25955;&#34920;&#31034;&#21644;&#33258;&#25105;&#35757;&#32451;&#65289;&#20284;&#20046;&#26377;&#21161;&#20110;&#25552;&#39640;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#27492;&#20998;&#26512;&#25193;&#23637;&#21040;&#34913;&#37327;ASR&#27169;&#22411;&#22312;&#26469;&#33258;&#21508;&#31181;&#20154;&#21475;&#20122;&#32452;&#30340;&#25968;&#25454;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#33521;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#20351;&#29992;&#32773;&#20197;&#21450;&#30007;&#24615;&#21644;&#22899;&#24615;&#65292;&#24182;&#35266;&#23519;&#21040;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22312;&#19981;&#21516;&#20122;&#32452;&#20043;&#38388;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07937v1 Announce Type: cross  Abstract: As Automatic Speech Recognition (ASR) models become ever more pervasive, it is important to ensure that they make reliable predictions under corruptions present in the physical and digital world. We propose Speech Robust Bench (SRB), a comprehensive benchmark for evaluating the robustness of ASR models to diverse corruptions. SRB is composed of 69 input perturbations which are intended to simulate various corruptions that ASR models may encounter in the physical and digital world. We use SRB to evaluate the robustness of several state-of-the-art ASR models and observe that model size and certain modeling choices such as discrete representations, and self-training appear to be conducive to robustness. We extend this analysis to measure the robustness of ASR models on data from various demographic subgroups, namely English and Spanish speakers, and males and females, and observed noticeable disparities in the model's robustness across su
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;</title><link>https://arxiv.org/abs/2403.05720</link><description>&lt;p&gt;
&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
A Benchmark of Domain-Adapted Large Language Models for Generating Brief Hospital Course Summaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05720
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#29992;&#20110;&#29983;&#25104;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#25688;&#35201;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#20581;&#24247;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#24615;&#33021;&#24182;&#25552;&#20986;&#30456;&#24212;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#25688;&#35201;&#26159;&#36890;&#36807;&#24635;&#32467;&#20020;&#24202;&#35760;&#24405;&#32780;&#29983;&#25104;&#30340;&#24120;&#35265;&#20020;&#24202;&#25991;&#20214;&#12290;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#21160;&#21270;&#23454;&#38469;&#20219;&#21153;&#26041;&#38754;&#23637;&#29616;&#20986;&#26174;&#33879;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#21307;&#30103;&#24212;&#29992;&#65288;&#22914;BHC&#21512;&#25104;&#65289;&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#24471;&#21040;&#23637;&#31034;&#12290;&#20026;&#20102;&#20351;LLMs&#33021;&#22815;&#36866;&#24212;BHC&#21512;&#25104;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#20174;MIMIC-IV&#35760;&#24405;&#20013;&#25552;&#21462;&#30340;&#32463;&#36807;&#39044;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#65292;&#23553;&#35013;&#20102;&#20020;&#24202;&#35760;&#24405;&#21644;&#31616;&#35201;&#20303;&#38498;&#30149;&#31243;&#65288;BHC&#65289;&#23545;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#20010;&#36890;&#29992;LLMs&#21644;&#19977;&#20010;&#21307;&#30103;&#39046;&#22495;&#36866;&#24212;&#30340;LLMs&#30340;&#24615;&#33021;&#65292;&#20197;&#25913;&#36827;&#20174;&#20020;&#24202;&#35760;&#24405;&#29983;&#25104;BHC&#12290;&#25105;&#20204;&#20351;&#29992;&#20020;&#24202;&#35760;&#24405;&#20316;&#20026;&#36755;&#20837;&#26469;&#29983;&#25104;BHC&#65292;&#37319;&#29992;&#22522;&#20110;&#25552;&#31034;&#30340;&#65288;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65289;&#21644;&#22522;&#20110;&#24494;&#35843;&#30340;&#33258;&#36866;&#24212;&#31574;&#30053;&#26469;&#24212;&#29992;&#20110;&#19977;&#20010;&#24320;&#28304;LLMs&#65288;Clinical-T5-Large&#65292;Llama2-13B&#65292;FLAN-UL2&#65289;&#21644;&#20004;&#20010;&#19987;&#26377;LLMs&#65288;GPT-3.5&#65292;GPT-4&#65289;&#12290;&#25105;&#20204;&#23450;&#37327;&#35780;&#20272;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05720v1 Announce Type: cross  Abstract: Brief hospital course (BHC) summaries are common clinical documents generated by summarizing clinical notes. While large language models (LLMs) depict remarkable capabilities in automating real-world tasks, their capabilities for healthcare applications such as BHC synthesis have not been shown. To enable the adaptation of LLMs for BHC synthesis, we introduce a novel benchmark consisting of a pre-processed dataset extracted from MIMIC-IV notes, encapsulating clinical note, and brief hospital course (BHC) pairs. We assess the performance of two general-purpose LLMs and three healthcare-adapted LLMs to improve BHC synthesis from clinical notes. Using clinical notes as input for generating BHCs, we apply prompting-based (using in-context learning) and fine-tuning-based adaptation strategies to three open-source LLMs (Clinical-T5-Large, Llama2-13B, FLAN-UL2) and two proprietary LLMs (GPT-3.5, GPT-4). We quantitatively evaluate the performa
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#37325;&#26500;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.03945</link><description>&lt;p&gt;
SPEAR&#65306;&#32852;&#37030;&#23398;&#20064;&#20013;&#25209;&#37327;&#31934;&#30830;&#26799;&#24230;&#21453;&#28436;
&lt;/p&gt;
&lt;p&gt;
SPEAR:Exact Gradient Inversion of Batches in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#31934;&#30830;&#37325;&#26500;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35299;&#20915;&#20102;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#22810;&#20010;&#23458;&#25143;&#31471;&#20165;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#20182;&#20204;&#26412;&#22320;&#25968;&#25454;&#30340;&#26799;&#24230;&#26356;&#26032;&#65292;&#32780;&#19981;&#26159;&#23454;&#38469;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#26368;&#36817;&#21457;&#29616;&#26799;&#24230;&#21453;&#28436;&#25915;&#20987;&#21487;&#20197;&#20174;&#36825;&#20123;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#26500;&#20986;&#25968;&#25454;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#33021;&#22312;&#37325;&#35201;&#30340;&#35802;&#23454;&#20294;&#22909;&#22855;&#35774;&#32622;&#20013;&#23545;&#25209;&#37327;&#22823;&#23567;&#20026;$b=1$&#30340;&#25968;&#25454;&#36827;&#34892;&#31934;&#30830;&#37325;&#26500;&#65292;&#23545;&#20110;&#26356;&#22823;&#30340;&#25209;&#37327;&#21482;&#33021;&#36827;&#34892;&#36817;&#20284;&#37325;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;\emph{&#31532;&#19968;&#20010;&#20934;&#30830;&#37325;&#24314;&#25209;&#37327;$b &gt;1$&#30340;&#31639;&#27861;}&#12290;&#36825;&#31181;&#26041;&#27861;&#32467;&#21512;&#20102;&#23545;&#26799;&#24230;&#26174;&#24335;&#20302;&#31209;&#32467;&#26500;&#30340;&#25968;&#23398;&#35265;&#35299;&#21644;&#22522;&#20110;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#25105;&#20204;&#21033;&#29992;ReLU&#35825;&#23548;&#30340;&#26799;&#24230;&#31232;&#30095;&#24615;&#65292;&#31934;&#30830;&#22320;&#36807;&#28388;&#25481;&#22823;&#37327;&#38169;&#35823;&#30340;&#26679;&#26412;&#65292;&#20351;&#26368;&#32456;&#30340;&#37325;&#24314;&#27493;&#39588;&#21487;&#34892;&#12290;&#25105;&#20204;&#20026;&#20840;&#36830;&#25509;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;GPU&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03945v1 Announce Type: new  Abstract: Federated learning is a popular framework for collaborative machine learning where multiple clients only share gradient updates on their local data with the server and not the actual data. Unfortunately, it was recently shown that gradient inversion attacks can reconstruct this data from these shared gradients. Existing attacks enable exact reconstruction only for a batch size of $b=1$ in the important honest-but-curious setting, with larger batches permitting only approximate reconstruction. In this work, we propose \emph{the first algorithm reconstructing whole batches with $b &gt;1$ exactly}. This approach combines mathematical insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#35770;&#25209;&#37327;&#22823;&#23567;&#22914;&#20309;&#12290;</title><link>https://arxiv.org/abs/2403.02967</link><description>&lt;p&gt;
&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38750;&#20984;&#38543;&#26426;&#22797;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-Convex Stochastic Composite Optimization with Polyak Momentum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#26080;&#35770;&#25209;&#37327;&#22823;&#23567;&#22914;&#20309;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#27861;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#27861;&#30340;&#19968;&#20010;&#24378;&#22823;&#27867;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#24403;&#38543;&#26426;&#22122;&#22768;&#26174;&#33879;&#26102;&#65288;&#21363;&#20165;&#20351;&#29992;&#23567;&#22411;&#25110;&#26377;&#30028;&#25209;&#37327;&#22823;&#23567;&#26102;&#65289;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#20984;&#29615;&#22659;&#20013;&#26080;&#27861;&#25910;&#25947;&#12290;&#26412;&#25991;&#20851;&#27880;&#20855;&#26377;Polyak&#21160;&#37327;&#30340;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#38750;&#20984;&#22797;&#21512;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#20102;&#26368;&#20339;&#25910;&#25947;&#36895;&#24230;&#65292;&#32780;&#25209;&#37327;&#22823;&#23567;&#22823;&#23567;&#26080;&#20851;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;Polyak&#21160;&#37327;&#22312;&#22797;&#21512;&#20248;&#21270;&#29615;&#22659;&#20013;&#30340;&#26041;&#24046;&#20943;&#23569;&#25928;&#24212;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#65292;&#24182;&#19988;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36817;&#31471;&#27493;&#39588;&#21482;&#33021;&#36890;&#36807;&#36817;&#20284;&#35299;&#26469;&#27714;&#35299;&#26102;&#65292;&#35813;&#26041;&#27861;&#20063;&#20250;&#25910;&#25947;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02967v1 Announce Type: cross  Abstract: The stochastic proximal gradient method is a powerful generalization of the widely used stochastic gradient descent (SGD) method and has found numerous applications in Machine Learning. However, it is notoriously known that this method fails to converge in non-convex settings where the stochastic noise is significant (i.e. when only small or bounded batch sizes are used). In this paper, we focus on the stochastic proximal gradient method with Polyak momentum. We prove this method attains an optimal convergence rate for non-convex composite optimization problems, regardless of batch size. Additionally, we rigorously analyze the variance reduction effect of the Polyak momentum in the composite optimization setting and we show the method also converges when the proximal step can only be solved inexactly. Finally, we provide numerical experiments to validate our theoretical results.
&lt;/p&gt;</description></item><item><title>KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.02648</link><description>&lt;p&gt;
&#31227;&#38500;&#24179;&#26041;&#26681;&#65306;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26631;&#24230;&#19981;&#21464;&#29256;&#26412;&#30340;AdaGrad
&lt;/p&gt;
&lt;p&gt;
Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02648
&lt;/p&gt;
&lt;p&gt;
KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#35843;&#25972;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;KATE&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;AdaGrad&#31639;&#27861;&#30340;&#26631;&#24230;&#19981;&#21464;&#36866;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KATE&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26696;&#20363;&#20013;&#30340;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;KATE&#24314;&#31435;&#20102;&#19968;&#20010;&#25910;&#25947;&#36895;&#29575;&#20026;$O \left(\frac{\log T}{\sqrt{T}} \right)$&#65292;&#19982;AdaGrad&#21644;Adam&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#23558;KATE&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;Adam&#21644;AdaGrad&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#22330;&#26223;&#20013;&#65292;KATE&#22987;&#32456;&#32988;&#36807;AdaGrad&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21305;&#37197;/&#36229;&#36234;Adam&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.18510</link><description>&lt;p&gt;
RNNs&#36824;&#19981;&#26159;Transformer&#65306;&#22312;&#19978;&#19979;&#25991;&#26816;&#32034;&#20013;&#30340;&#20851;&#38190;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
RNNs are not Transformers (Yet): The Key Bottleneck on In-context Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;RNNs&#21644;Transformer&#22312;&#22788;&#29702;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#29616;&#33021;&#21147;&#24046;&#36317;&#65292;&#21457;&#29616;RNNs&#23384;&#22312;&#20851;&#38190;&#29942;&#39048;&#65292;&#21363;&#26080;&#27861;&#23436;&#32654;&#22320;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#23548;&#33268;&#26080;&#27861;&#20687;Transformer&#37027;&#26679;&#36731;&#26494;&#35299;&#20915;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#21644;Transformer&#22312;&#35299;&#20915;&#31639;&#27861;&#38382;&#39064;&#26102;&#30340;&#34920;&#31034;&#33021;&#21147;&#24046;&#36317;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;RNNs&#26159;&#21542;&#33021;&#22312;&#22788;&#29702;&#38271;&#24207;&#21015;&#26102;&#65292;&#36890;&#36807;Chain-of-Thought (CoT)&#25552;&#31034;&#65292;&#19982;Transformer&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#26174;&#31034;CoT&#21487;&#20197;&#25913;&#36827;RNNs&#65292;&#20294;&#26080;&#27861;&#24357;&#34917;&#19982;Transformer&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#20851;&#38190;&#29942;&#39048;&#22312;&#20110;RNNs&#26080;&#27861;&#23436;&#20840;&#20174;&#19978;&#19979;&#25991;&#20013;&#26816;&#32034;&#20449;&#24687;&#65292;&#21363;&#20351;&#32463;&#36807;CoT&#30340;&#22686;&#24378;&#65306;&#23545;&#20110;&#20960;&#20010;&#26126;&#30830;&#25110;&#38544;&#24335;&#38656;&#35201;&#36825;&#31181;&#33021;&#21147;&#30340;&#20219;&#21153;&#65292;&#22914;&#32852;&#24819;&#21484;&#22238;&#21644;&#30830;&#23450;&#22270;&#26159;&#21542;&#20026;&#26641;&#65292;&#25105;&#20204;&#35777;&#26126;RNNs&#34920;&#36798;&#33021;&#21147;&#19981;&#36275;&#20197;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#65292;&#32780;Transformer&#21487;&#20197;&#36731;&#26494;&#35299;&#20915;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#35777;&#26126;&#37319;&#29992;&#22686;&#24378;RNNs&#19978;&#19979;&#25991;&#26816;&#32034;&#33021;&#21147;&#30340;&#25216;&#26415;&#65292;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18510v1 Announce Type: cross  Abstract: This paper investigates the gap in representation powers of Recurrent Neural Networks (RNNs) and Transformers in the context of solving algorithmic problems. We focus on understanding whether RNNs, known for their memory efficiency in handling long sequences, can match the performance of Transformers, particularly when enhanced with Chain-of-Thought (CoT) prompting. Our theoretical analysis reveals that CoT improves RNNs but is insufficient to close the gap with Transformers. A key bottleneck lies in the inability of RNNs to perfectly retrieve information from the context, even with CoT: for several tasks that explicitly or implicitly require this capability, such as associative recall and determining if a graph is a tree, we prove that RNNs are not expressive enough to solve the tasks while Transformers can solve them with ease. Conversely, we prove that adopting techniques to enhance the in-context retrieval capability of RNNs, inclu
&lt;/p&gt;</description></item><item><title>FENs&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#19988;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.15883</link><description>&lt;p&gt;
&#34701;&#21512;&#32534;&#30721;&#22120;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fusion Encoder Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15883
&lt;/p&gt;
&lt;p&gt;
FENs&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#65292;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#19988;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65292;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34701;&#21512;&#32534;&#30721;&#22120;&#32593;&#32476;&#65288;FENs&#65289;&#30340;&#31639;&#27861;&#31867;&#65306;&#29992;&#20110;&#21019;&#24314;&#23558;&#22266;&#23450;&#38271;&#24230;&#24207;&#21015;&#26144;&#23556;&#21040;&#36755;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#29983;&#25104;&#30340;&#31070;&#32463;&#32593;&#32476;&#20165;&#20855;&#26377;&#23545;&#25968;&#28145;&#24230;&#65288;&#20943;&#36731;&#25968;&#25454;&#22312;&#32593;&#32476;&#20013;&#20256;&#25773;&#26102;&#30340;&#36864;&#21270;&#65289;&#65292;&#21487;&#20197;&#22312;&#32447;&#24615;&#26102;&#38388;&#20869;&#22788;&#29702;&#24207;&#21015;&#65288;&#25110;&#32773;&#22312;&#20855;&#26377;&#32447;&#24615;&#22788;&#29702;&#22120;&#25968;&#37327;&#30340;&#23545;&#25968;&#26102;&#38388;&#20869;&#65289;&#12290;FENs&#30340;&#20851;&#38190;&#23646;&#24615;&#26159;&#23427;&#20204;&#36890;&#36807;&#35757;&#32451;&#22823;&#33268;&#32447;&#24615;&#25968;&#37327;&#30340;&#24120;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24182;&#34892;&#23398;&#20064;&#12290;&#36825;&#20123;&#32593;&#32476;&#20855;&#26377;&#24120;&#28145;&#24230;&#24847;&#21619;&#30528;&#21453;&#21521;&#20256;&#25773;&#25928;&#26524;&#33391;&#22909;&#12290;&#38656;&#35201;&#27880;&#24847;&#30340;&#26159;&#65292;&#30446;&#21069;FENs&#30340;&#24615;&#33021;&#20165;&#20165;&#26159;&#25512;&#27979;&#65292;&#22240;&#20026;&#25105;&#20204;&#23578;&#26410;&#23454;&#29616;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15883v1 Announce Type: new  Abstract: In this paper we present fusion encoder networks (FENs): a class of algorithms for creating neural networks that map fixed-length sequences to outputs. The resulting neural network has only logarithmic depth (alleviating the degradation of data as it propagates through the network) and can process sequences in linear time (or in logarithmic time with a linear number of processors). The crucial property of FENs is that they learn by training a quasi-linear number of constant-depth neural networks in parallel. The fact that these networks are constant depth means that backpropagation works well. We note that currently the performance of FENs is only conjectured as we are yet to implement them.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#26144;&#23556;&#30340;&#38750;&#22855;&#24322;&#24615;&#20197;&#21450;&#20840;&#23616;&#26368;&#23567;&#20540;&#28857;&#38598;&#30340;&#20809;&#28369;&#27969;&#24418;&#29305;&#24615;&#65292;&#20026;&#29702;&#35299;&#22823;&#23398;&#20064;&#29575;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;</title><link>https://arxiv.org/abs/2402.13108</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#23398;&#20064;&#29575;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Gradient Descent for Large Learning Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#19979;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#26144;&#23556;&#30340;&#38750;&#22855;&#24322;&#24615;&#20197;&#21450;&#20840;&#23616;&#26368;&#23567;&#20540;&#28857;&#38598;&#30340;&#20809;&#28369;&#27969;&#24418;&#29305;&#24615;&#65292;&#20026;&#29702;&#35299;&#22823;&#23398;&#20064;&#29575;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#31283;&#23450;&#24615;&#25552;&#20379;&#20102;&#37325;&#35201;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23545;&#29702;&#35299;&#8220;&#31283;&#23450;&#24615;&#36793;&#32536;&#65288;EoS&#65289;&#8221;&#29616;&#35937;&#23384;&#22312;&#30528;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#65292;&#36825;&#19968;&#29616;&#35937;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20013;&#34987;&#35266;&#23519;&#21040;&#65292;&#20854;&#29305;&#28857;&#26159;&#25439;&#22833;&#20989;&#25968;&#22312;&#19981;&#21516;&#32426;&#20803;&#38388;&#30340;&#38750;&#21333;&#35843;&#19979;&#38477;&#65292;&#32780;&#25439;&#22833;&#30340;&#38497;&#23789;&#24230;&#65288;Hessian&#30340;&#35889;&#33539;&#25968;&#65289;&#36880;&#28176;&#25509;&#36817;&#24182;&#31283;&#23450;&#22312;2/(&#23398;&#20064;&#29575;)&#38468;&#36817;&#12290;&#26368;&#36817;&#26377;&#20154;&#25552;&#20986;&#20102;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#26102;&#20986;&#29616;EoS&#30340;&#21407;&#22240;&#8212;&#8212;&#27839;&#26799;&#24230;&#19979;&#38477;&#36712;&#36857;&#38468;&#36817;&#32570;&#20047;&#24179;&#22374;&#30340;&#26497;&#23567;&#20540;&#28857;&#65292;&#21516;&#26102;&#23384;&#22312;&#32039;&#33268;&#30340;&#27491;&#21521;&#19981;&#21464;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#20108;&#27425;&#25439;&#22833;&#20989;&#25968;&#19979;&#20248;&#21270;&#30340;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#28385;&#36275;&#31532;&#19968;&#20010;&#20551;&#35774;&#20197;&#21450;&#31532;&#20108;&#20010;&#20551;&#35774;&#30340;&#19968;&#20010;&#24517;&#35201;&#26465;&#20214;&#12290;&#26356;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#19979;&#38477;&#26144;&#23556;&#26159;&#38750;&#22855;&#24322;&#30340;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;&#28857;&#38598;&#26500;&#25104;&#19968;&#20010;&#20809;&#28369;&#27969;&#24418;&#65292;&#24182;&#19988;&#31283;&#23450;&#30340;&#26497;&#23567;&#20540;&#26500;&#25104;&#26377;&#30028;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13108v1 Announce Type: new  Abstract: There currently is a significant interest in understanding the Edge of Stability (EoS) phenomenon, which has been observed in neural networks training, characterized by a non-monotonic decrease of the loss function over epochs, while the sharpness of the loss (spectral norm of the Hessian) progressively approaches and stabilizes around 2/(learning rate). Reasons for the existence of EoS when training using gradient descent have recently been proposed -- a lack of flat minima near the gradient descent trajectory together with the presence of compact forward-invariant sets. In this paper, we show that linear neural networks optimized under a quadratic loss function satisfy the first assumption and also a necessary condition for the second assumption. More precisely, we prove that the gradient descent map is non-singular, the set of global minimizers of the loss function forms a smooth manifold, and the stable minima form a bounded subset i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.12329</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Query-Based Adversarial Prompt Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12329
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#30340; API &#35775;&#38382;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#26356;&#39640;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#38750;&#20165;&#20165;&#22522;&#20110;&#27169;&#22411;&#20043;&#38388;&#30340;&#36716;&#31227;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#23548;&#33268;&#19968;&#20010;&#23545;&#20854;&#36827;&#34892;&#20102;&#35843;&#25972;&#30340;&#35821;&#35328;&#27169;&#22411;&#20135;&#29983;&#26377;&#23475;&#23383;&#31526;&#20018;&#25110;&#25191;&#34892;&#26377;&#23475;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#25915;&#20987;&#35201;&#20040;&#22312;&#30333;&#30418;&#35774;&#32622;&#20013;&#65288;&#23436;&#20840;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21487;&#36716;&#31227;&#24615;&#65306;&#19968;&#31181;&#29616;&#35937;&#65292;&#21363;&#22312;&#19968;&#20010;&#27169;&#22411;&#19978;&#31934;&#24515;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#36890;&#24120;&#22312;&#20854;&#20182;&#27169;&#22411;&#19978;&#20173;&#28982;&#26377;&#25928;&#12290;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#26597;&#35810;&#30340;&#25915;&#20987;&#25913;&#36827;&#20197;&#21069;&#30340;&#24037;&#20316;&#65292;&#21033;&#29992; API &#35775;&#38382;&#36828;&#31243;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20351;&#27169;&#22411;&#20197;&#65288;&#26126;&#26174;&#65289;&#26356;&#39640;&#30340;&#27010;&#29575;&#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#19981;&#33021;&#20165;&#20165;&#20351;&#29992;&#36716;&#31227;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312; GPT-3.5 &#21644; OpenAI &#30340;&#23433;&#20840;&#20998;&#31867;&#22120;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#65307;&#25105;&#20204;&#33021;&#22815;&#35753; GPT-3.5 &#21457;&#20986;&#26377;&#23475;&#23383;&#31526;&#20018;&#65292;&#32780;&#30446;&#21069;&#30340;&#36716;&#31227;&#25915;&#20987;&#22833;&#36133;&#20102;&#65292;&#24182;&#19988;&#25105;&#20204;&#20960;&#20046;&#20197; 100% &#30340;&#27010;&#29575;&#35268;&#36991;&#20102;&#23433;&#20840;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12329v1 Announce Type: cross  Abstract: Recent work has shown it is possible to construct adversarial examples that cause an aligned language model to emit harmful strings or perform harmful behavior. Existing attacks work either in the white-box setting (with full access to the model weights), or through transferability: the phenomenon that adversarial examples crafted on one model often remain effective on other models. We improve on prior work with a query-based attack that leverages API access to a remote language model to construct adversarial examples that cause the model to emit harmful strings with (much) higher probability than with transfer-only attacks. We validate our attack on GPT-3.5 and OpenAI's safety classifier; we can cause GPT-3.5 to emit harmful strings that current transfer attacks fail at, and we can evade the safety classifier with nearly 100% probability.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2402.12220</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Bayesian Parameter-Efficient Fine-Tuning for Overcoming Catastrophic Forgetting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12220
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36125;&#21494;&#26031;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#30693;&#35782;&#30340;&#20445;&#30041;&#65292;&#24182;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#26368;&#21021;&#26159;&#34987;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#25152;&#28608;&#21457;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#36890;&#29992;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26159;&#36827;&#34892;&#36825;&#31181;&#33258;&#36866;&#24212;&#30340;&#36866;&#24403;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#20173;&#28982;&#26159;PEFT&#38754;&#20020;&#30340;&#38382;&#39064;&#65292;&#23427;&#25439;&#23475;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#22266;&#26377;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#29616;&#26377;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#24212;&#29992;&#20110;PEFT&#65292;&#20197;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21482;&#35201;&#33021;&#22815;&#21487;&#24494;&#22320;&#35745;&#31639;&#24494;&#35843;&#23618;&#30340;&#21442;&#25968;&#36716;&#25442;&#12290;&#22312;&#19968;&#31995;&#21015;&#20851;&#20110;&#35821;&#35328;&#24314;&#27169;&#21644;&#35821;&#38899;&#21512;&#25104;&#20219;&#21153;&#30340;&#22522;&#30784;&#24615;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#24314;&#31435;&#30340;&#25289;&#26222;&#25289;&#26031;&#36817;&#20284;&#65292;&#21253;&#25324;&#23545;&#35282;&#32447;&#21644;Kronecker&#20998;&#35299;&#26041;&#27861;&#65292;&#26469;&#27491;&#21017;&#21270;PEFT&#19982;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#24182;&#27604;&#36739;&#23427;&#20204;&#22312;&#20445;&#30041;&#39044;&#35757;&#32451;&#30693;&#35782;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#24494;&#35843;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12220v1 Announce Type: cross  Abstract: Although motivated by the adaptation of text-to-speech synthesis models, we argue that more generic parameter-efficient fine-tuning (PEFT) is an appropriate framework to do such adaptation. However, catastrophic forgetting remains an issue with PEFT, damaging the pre-trained model's inherent capabilities. We demonstrate that existing Bayesian learning techniques can be applied to PEFT to prevent catastrophic forgetting as long as the parameter shift of the fine-tuned layers can be calculated differentiably. In a principled series of experiments on language modeling and speech synthesis tasks, we utilize established Laplace approximations, including diagonal and Kronecker factored approaches, to regularize PEFT with the low-rank adaptation (LoRA) and compare their performance in pre-training knowledge preservation. Our results demonstrate that catastrophic forgetting can be overcome by our methods without degrading the fine-tuning perfo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;</title><link>https://arxiv.org/abs/2402.10980</link><description>&lt;p&gt;
CHEMREASONER&#65306;&#20351;&#29992;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#36827;&#34892;&#21551;&#21457;&#24335;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
CHEMREASONER: Heuristic Search over a Large Language Model's Knowledge Space using Quantum-Chemical Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10980
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#19982;&#37327;&#23376;&#21270;&#23398;&#21453;&#39304;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;AI&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#20652;&#21270;&#21058;&#30340;&#31215;&#26497;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 &#31867;&#22411;&#20844;&#21578;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21457;&#29616;&#26032;&#30340;&#20652;&#21270;&#21058;&#23545;&#20110;&#35774;&#35745;&#26032;&#30340;&#26356;&#39640;&#25928;&#30340;&#21270;&#23398;&#36807;&#31243;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#23454;&#29616;&#21521;&#21487;&#25345;&#32493;&#26410;&#26469;&#30340;&#36807;&#28193;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#24341;&#23548;&#30340;&#35745;&#31639;&#31579;&#36873;&#26694;&#26550;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#22522;&#20110;&#37327;&#23376;&#21270;&#23398;&#30340;&#19977;&#32500;&#21407;&#23376;&#34920;&#31034;&#30340;&#21453;&#39304;&#32479;&#19968;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20652;&#21270;&#21058;&#21457;&#29616;&#26500;&#24314;&#20026;&#19968;&#20010;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#20854;&#20013;&#19968;&#20010;&#20195;&#29702;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#23548;&#30340;&#20551;&#35774;&#19982;&#22522;&#20110;&#21407;&#23376;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#21453;&#39304;&#30340;&#36845;&#20195;&#32452;&#21512;&#65292;&#31215;&#26497;&#25628;&#32034;&#39640;&#25928;&#20652;&#21270;&#21058;&#12290;&#22312;&#20013;&#38388;&#25628;&#32034;&#27493;&#39588;&#30830;&#23450;&#30340;&#20652;&#21270;&#21058;&#32463;&#36807;&#22522;&#20110;&#31354;&#38388;&#23450;&#21521;&#12289;&#21453;&#24212;&#36884;&#24452;&#21644;&#31283;&#23450;&#24615;&#30340;&#32467;&#26500;&#35780;&#20272;&#12290;&#22522;&#20110;&#21560;&#38468;&#33021;&#21644;&#21183;&#22418;&#30340;&#35780;&#20998;&#20989;&#25968;&#24341;&#23548;&#22312;LLM&#30340;&#30693;&#35782;&#31354;&#38388;&#20013;&#21521;&#33021;&#37327;&#26377;&#21033;&#12289;&#39640;&#25928;&#30340;&#20652;&#21270;&#21058;&#25506;&#32034;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#20197;&#33258;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10980v1 Announce Type: cross  Abstract: The discovery of new catalysts is essential for the design of new and more efficient chemical processes in order to transition to a sustainable future. We introduce an AI-guided computational screening framework unifying linguistic reasoning with quantum-chemistry based feedback from 3D atomistic representations. Our approach formulates catalyst discovery as an uncertain environment where an agent actively searches for highly effective catalysts via the iterative combination of large language model (LLM)-derived hypotheses and atomistic graph neural network (GNN)-derived feedback. Identified catalysts in intermediate search steps undergo structural evaluation based on spatial orientation, reaction pathways, and stability. Scoring functions based on adsorption energies and barriers steer the exploration in the LLM's knowledge space toward energetically favorable, high-efficiency catalysts. We introduce planning methods that automaticall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#36817;&#20284;&#26041;&#27861;&#65292;&#22312;&#24322;&#27493;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#26377;&#25928;&#32467;&#21512;&#20102;&#21160;&#37327;&#21644;&#24322;&#27493;&#21327;&#35758;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21160;&#37327;&#26356;&#26032;&#30340;&#20559;&#24046;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#21160;&#37327;&#36817;&#20284;&#22312;&#22522;&#20934;FL&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09247</link><description>&lt;p&gt;
&#24322;&#27493;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#21160;&#37327;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Momentum Approximation in Asynchronous Private Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09247
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21160;&#37327;&#36817;&#20284;&#26041;&#27861;&#65292;&#22312;&#24322;&#27493;&#31169;&#26377;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#26377;&#25928;&#32467;&#21512;&#20102;&#21160;&#37327;&#21644;&#24322;&#27493;&#21327;&#35758;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#21160;&#37327;&#26356;&#26032;&#30340;&#20559;&#24046;&#26469;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#12290;&#23454;&#35777;&#30740;&#31350;&#35777;&#26126;&#20102;&#21160;&#37327;&#36817;&#20284;&#22312;&#22522;&#20934;FL&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#27493;&#21327;&#35758;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#25552;&#39640;&#22823;&#35268;&#27169;&#23458;&#25143;&#31471;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#21516;&#26102;&#65292;&#22522;&#20110;&#21160;&#37327;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21516;&#27493;FL&#20013;&#23454;&#29616;&#26368;&#20339;&#27169;&#22411;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#27493;FL&#31639;&#27861;&#20013;&#31616;&#21333;&#22320;&#24212;&#29992;&#21160;&#37327;&#20250;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#21464;&#24930;&#21644;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#32467;&#21512;&#36825;&#20004;&#31181;&#25216;&#26415;&#20197;&#23454;&#29616;&#21452;&#36194;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24322;&#27493;&#24615;&#24341;&#20837;&#20102;&#23545;&#21160;&#37327;&#26356;&#26032;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#37327;&#36817;&#20284;&#65292;&#36890;&#36807;&#25214;&#21040;&#25152;&#26377;&#21382;&#21490;&#27169;&#22411;&#26356;&#26032;&#30340;&#26368;&#20339;&#21152;&#26435;&#24179;&#22343;&#20540;&#26469;&#26368;&#23567;&#21270;&#20559;&#24046;&#12290;&#21160;&#37327;&#36817;&#20284;&#19982;&#23433;&#20840;&#32858;&#21512;&#21644;&#24046;&#20998;&#38544;&#31169;&#26159;&#20860;&#23481;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#29983;&#20135;&#30340;FL&#31995;&#32479;&#20013;&#24456;&#23481;&#26131;&#22320;&#38598;&#25104;&#65292;&#21482;&#38656;&#36739;&#23567;&#30340;&#36890;&#20449;&#21644;&#23384;&#20648;&#25104;&#26412;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;FL&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21160;&#37327;&#36817;&#20284;&#22312;&#24615;&#33021;&#19978;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09247v1 Announce Type: new Abstract: Asynchronous protocols have been shown to improve the scalability of federated learning (FL) with a massive number of clients. Meanwhile, momentum-based methods can achieve the best model quality in synchronous FL. However, naively applying momentum in asynchronous FL algorithms leads to slower convergence and degraded model performance. It is still unclear how to effective combinie these two techniques together to achieve a win-win. In this paper, we find that asynchrony introduces implicit bias to momentum updates. In order to address this problem, we propose momentum approximation that minimizes the bias by finding an optimal weighted average of all historical model updates. Momentum approximation is compatible with secure aggregation as well as differential privacy, and can be easily integrated in production FL systems with a minor communication and storage cost. We empirically demonstrate that on benchmark FL datasets, momentum appro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.09236</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#27010;&#24565;&#65306;&#32479;&#19968;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#19982;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Concepts: Unifying Causal Representation Learning and Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09236
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#30784;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#30740;&#31350;&#20102;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#26234;&#33021;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#26377;&#20004;&#31181;&#24191;&#27867;&#30340;&#26041;&#27861;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#22825;&#29983;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#39046;&#22495;&#30340;&#21162;&#21147;&#26041;&#21521;&#12290;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#26500;&#24314;&#39640;&#24615;&#33021;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#28982;&#21518;&#25237;&#20837;&#21162;&#21147;&#21435;&#29702;&#35299;&#23427;&#20204;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#26412;&#30740;&#31350;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#65292;&#30740;&#31350;&#22914;&#20309;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#32467;&#21512;&#36825;&#20004;&#20010;&#39046;&#22495;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#27491;&#24335;&#23450;&#20041;&#20102;&#27010;&#24565;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#21487;&#20197;&#20174;&#22810;&#26679;&#30340;&#25968;&#25454;&#20013;&#34987;&#21487;&#38752;&#22320;&#24674;&#22797;&#20986;&#26469;&#12290;&#23545;&#20110;&#21512;&#25104;&#25968;&#25454;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#32479;&#19968;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09236v1 Announce Type: cross Abstract: To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.05133</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#22522;&#20110;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Personalized Language Modeling from Personalized Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05133
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20110;&#29992;&#25143;&#30340;&#21453;&#39304;&#25968;&#25454;&#20013;&#24341;&#20837;&#20010;&#24615;&#21270;&#29305;&#24449;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#22810;&#26679;&#21270;&#29992;&#25143;&#20559;&#22909;&#19979;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#30446;&#21069;&#20027;&#27969;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35843;&#25972;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#26356;&#22909;&#22320;&#31526;&#21512;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#24320;&#21457;&#30340;&#31639;&#27861;&#30340;&#22522;&#26412;&#21069;&#25552;&#22312;&#29992;&#25143;&#20559;&#22909;&#22810;&#26679;&#21270;&#30340;&#24773;&#20917;&#19979;&#21487;&#33021;&#20250;&#20986;&#29616;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#24320;&#21457;&#20010;&#24615;&#21270;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#27491;&#24335;&#20171;&#32461;&#20102;&#20174;&#20010;&#24615;&#21270;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30340;&#20219;&#21153;&#65292;&#24182;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#26222;&#36890;&#30340;RLHF&#21487;&#33021;&#20250;&#23384;&#22312;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#20010;&#24615;&#21270;-RLHF&#65288;P-RLHF&#65289;&#26694;&#26550;&#65292;&#38656;&#35201;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#27169;&#22411;&#21644;&#35821;&#35328;&#65288;&#25110;&#22870;&#21169;&#65289;&#27169;&#22411;&#12290;&#29992;&#25143;&#27169;&#22411;&#25509;&#25910;&#29992;&#25143;&#20449;&#24687;&#24182;&#36755;&#20986;&#29992;&#25143;&#34920;&#31034;&#12290;&#20854;&#32467;&#26500;&#32534;&#30721;&#20102;&#25105;&#20204;&#23545;&#21453;&#39304;&#25968;&#25454;&#20013;&#29992;&#25143;&#20559;&#22909;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#20026;&#20010;&#24615;&#21270;&#22870;&#21169;&#24314;&#27169;&#21644;&#20010;&#24615;&#21270;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#24320;&#21457;&#20102;&#26032;&#30340;&#23398;&#20064;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is the current dominating framework to fine-tune large language models to better align with human preferences. However, the underlying premise of algorithms developed under this framework can be problematic when user preferences encoded in human feedback are diverse. In this work, we aim to address this problem by developing methods for building personalized language models. We first formally introduce the task of learning from personalized human feedback and explain why vanilla RLHF can be problematic in this context. We then propose a general Personalized-RLHF (P-RLHF) framework, which requires one to jointly learn a user model and a language (or reward) model. The user model takes in user information and outputs user representations. Its structure encodes our assumptions about user preferences underlying the feedback data. We develop new learning objectives for personalized reward modeling and personalized Direct Preference Optimizat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;ActiveAnno3D&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20351;&#29992;&#19968;&#21322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#23454;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#36817;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.03235</link><description>&lt;p&gt;
ActiveAnno3D - &#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ActiveAnno3D - An Active Learning Framework for Multi-Modal 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;ActiveAnno3D&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#65292;&#25105;&#20204;&#33021;&#22815;&#22312;&#20351;&#29992;&#19968;&#21322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#23454;&#29616;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#36817;&#30340;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31579;&#36873;&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#65292;&#25968;&#25454;&#36890;&#24120;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#65292;&#21019;&#24314;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#30340;&#38590;&#39064;&#20381;&#28982;&#23384;&#22312;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#20013;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ActiveAnno3D&#65292;&#19968;&#20010;&#29992;&#20110;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#36827;&#34892;&#26631;&#27880;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#21508;&#31181;&#36830;&#32493;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#38598;&#25104;&#20102;&#22312;&#35745;&#31639;&#38656;&#27714;&#21644;&#26816;&#27979;&#24615;&#33021;&#26041;&#38754;&#26368;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;nuScenes&#21644;TUM Traffic Intersection&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#21644;&#28040;&#34701;&#30740;&#31350;&#65292;&#20351;&#29992;BEVFusion&#21644;PV-RCNN&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#20165;&#20351;&#29992;TUM Traffic Intersection&#25968;&#25454;&#38598;&#30340;&#19968;&#21322;&#35757;&#32451;&#25968;&#25454;&#65288;77.25 mAP&#30456;&#27604;&#20110;83.50 mAP&#65289;&#26102;&#65292;&#20351;&#29992;PV-RCNN&#21644;&#22522;&#20110;&#29109;&#30340;&#26597;&#35810;&#31574;&#30053;&#20960;&#20046;&#21487;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#24615;&#33021;&#65292;&#32780;BEVFusion&#21017;&#22312;&#20351;&#29992;&#19968;&#21322;&#30340;&#35757;&#32451;&#25968;&#25454;&#26102;&#33719;&#24471;&#20102;64.31&#30340;mAP&#12290;
&lt;/p&gt;
&lt;p&gt;
The curation of large-scale datasets is still costly and requires much time and resources. Data is often manually labeled, and the challenge of creating high-quality datasets remains. In this work, we fill the research gap using active learning for multi-modal 3D object detection. We propose ActiveAnno3D, an active learning framework to select data samples for labeling that are of maximum informativeness for training. We explore various continuous training methods and integrate the most efficient method regarding computational demand and detection performance. Furthermore, we perform extensive experiments and ablation studies with BEVFusion and PV-RCNN on the nuScenes and TUM Traffic Intersection dataset. We show that we can achieve almost the same performance with PV-RCNN and the entropy-based query strategy when using only half of the training data (77.25 mAP compared to 83.50 mAP) of the TUM Traffic Intersection dataset. BEVFusion achieved an mAP of 64.31 when using half of the trai
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuseMoE&#30340;&#19987;&#23478;&#28151;&#21512;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#38376;&#25511;&#20989;&#25968;&#23454;&#29616;&#28789;&#27963;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#65292;&#21516;&#26102;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03226</link><description>&lt;p&gt;
FuseMoE&#65306;&#29992;&#20110;&#28789;&#27963;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#19987;&#23478;&#28151;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuseMoE&#30340;&#19987;&#23478;&#28151;&#21512;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#38376;&#25511;&#20989;&#25968;&#23454;&#29616;&#28789;&#27963;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#65292;&#21516;&#26102;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23427;&#20204;&#38754;&#20020;&#22788;&#29702;&#22810;&#31181;&#27169;&#24577;&#30340;&#21452;&#37325;&#25361;&#25112;&#65292;&#36825;&#20123;&#27169;&#24577;&#32463;&#24120;&#22240;&#32570;&#22833;&#20803;&#32032;&#32780;&#19981;&#23436;&#25972;&#65292;&#20197;&#21450;&#25910;&#38598;&#26679;&#26412;&#30340;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#21644;&#31232;&#30095;&#24615;&#12290;&#25104;&#21151;&#21033;&#29992;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#65292;&#21516;&#26102;&#20811;&#26381;&#39640;&#36136;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#31232;&#32570;&#24615;&#65292;&#26159;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;``FuseMoE''&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#21019;&#26032;&#38376;&#25511;&#20989;&#25968;&#30340;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#12290;FuseMoE&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#29420;&#29305;&#30340;&#38376;&#25511;&#20989;&#25968;&#26377;&#21161;&#20110;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;FuseMoE&#30340;&#23454;&#38469;&#23454;&#29992;&#24615;&#36890;&#36807;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.
&lt;/p&gt;</description></item><item><title>TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;</title><link>https://arxiv.org/abs/2402.02441</link><description>&lt;p&gt;
TopoX: &#19968;&#20010;&#29992;&#20110;&#25299;&#25169;&#22495;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
TopoX: A Suite of Python Packages for Machine Learning on Topological Domains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02441
&lt;/p&gt;
&lt;p&gt;
TopoX&#26159;&#19968;&#20010;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#19978;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#21253;&#21547;&#20102;&#26500;&#24314;&#12289;&#35745;&#31639;&#21644;&#23884;&#20837;&#25299;&#25169;&#22495;&#30340;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;topox&#65292;&#19968;&#20010;&#25552;&#20379;&#21487;&#38752;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#22871;&#20214;&#65292;&#29992;&#20110;&#22312;&#25299;&#25169;&#22495;&#65288;&#25193;&#23637;&#20102;&#22270;&#30340;&#39046;&#22495;&#65289;&#19978;&#36827;&#34892;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#65306;&#36229;&#22270;&#12289;&#21333;&#32431;&#12289;&#32990;&#33108;&#12289;&#36335;&#24452;&#21644;&#32452;&#21512;&#22797;&#21512;&#20307;&#12290;topox&#30001;&#19977;&#20010;&#36719;&#20214;&#21253;&#32452;&#25104;&#65306;toponetx&#29992;&#20110;&#26500;&#24314;&#21644;&#35745;&#31639;&#36825;&#20123;&#22495;&#65292;&#21253;&#25324;&#33410;&#28857;&#12289;&#36793;&#21644;&#39640;&#38454;&#21333;&#20803;&#30340;&#22788;&#29702;&#65307;topoembedx&#25552;&#20379;&#20102;&#23558;&#25299;&#25169;&#22495;&#23884;&#20837;&#21040;&#21521;&#37327;&#31354;&#38388;&#30340;&#26041;&#27861;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#22522;&#20110;&#22270;&#30340;&#23884;&#20837;&#31639;&#27861;&#65292;&#22914;node2vec&#65307;topomodelx&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#20026;&#25299;&#25169;&#22495;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#22871;&#20840;&#38754;&#30340;&#39640;&#38454;&#28040;&#24687;&#20256;&#36882;&#21151;&#33021;&#24037;&#20855;&#31665;&#12290;topox&#30340;&#28304;&#20195;&#30721;&#32463;&#36807;&#24191;&#27867;&#30340;&#25991;&#26723;&#21270;&#21644;&#21333;&#20803;&#27979;&#35797;&#65292;&#24182;&#22312;https://github.com/pyt-team&#20197;MIT&#35768;&#21487;&#35777;&#30340;&#24418;&#24335;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce topox, a Python software suite that provides reliable and user-friendly building blocks for computing and machine learning on topological domains that extend graphs: hypergraphs, simplicial, cellular, path and combinatorial complexes. topox consists of three packages: toponetx facilitates constructing and computing on these domains, including working with nodes, edges and higher-order cells; topoembedx provides methods to embed topological domains into vector spaces, akin to popular graph-based embedding algorithms such as node2vec; topomodelx is built on top of PyTorch and offers a comprehensive toolbox of higher-order message passing functions for neural networks on topological domains. The extensively documented and unit-tested source code of topox is available under MIT license at https://github.com/pyt-team.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.02322</link><description>&lt;p&gt;
&#21160;&#24577;&#22686;&#37327;&#20248;&#21270;&#29992;&#20110;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Dynamic Incremental Optimization for Best Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#31232;&#30095;&#23398;&#20064;&#38382;&#39064;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#25915;&#20987;&#36825;&#20010;&#38750;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#22522;&#20110;&#21407;&#22987;&#38382;&#39064;&#21644;&#23545;&#20598;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#28508;&#22312;&#22320;&#20943;&#23569;&#20102;&#20887;&#20313;&#35745;&#31639;&#24182;&#25913;&#36827;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.
&lt;/p&gt;</description></item><item><title>GD-CAF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38477;&#27700;&#39044;&#25253;&#20316;&#20026;&#19968;&#20010;&#26102;&#31354;&#22270;&#24207;&#21015;&#39044;&#25253;&#38382;&#39064;&#65292;&#21033;&#29992;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;&#26469;&#23398;&#20064;&#21382;&#21490;&#38477;&#27700;&#22270;&#24182;&#22312;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#19978;&#39044;&#27979;&#26410;&#26469;&#30340;&#38477;&#27700;&#12290;</title><link>https://arxiv.org/abs/2401.07958</link><description>&lt;p&gt;
GD-CAF&#65306;&#29992;&#20110;&#38477;&#27700;&#39044;&#25253;&#30340;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
GD-CAF: Graph Dual-stream Convolutional Attention Fusion for Precipitation Nowcasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.07958
&lt;/p&gt;
&lt;p&gt;
GD-CAF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38477;&#27700;&#39044;&#25253;&#20316;&#20026;&#19968;&#20010;&#26102;&#31354;&#22270;&#24207;&#21015;&#39044;&#25253;&#38382;&#39064;&#65292;&#21033;&#29992;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;&#26469;&#23398;&#20064;&#21382;&#21490;&#38477;&#27700;&#22270;&#24182;&#22312;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#19978;&#39044;&#27979;&#26410;&#26469;&#30340;&#38477;&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#30340;&#38477;&#27700;&#39044;&#25253;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#21253;&#25324;&#27946;&#27700;&#39044;&#27979;&#12289;&#28798;&#23475;&#31649;&#29702;&#12289;&#20248;&#21270;&#20892;&#19994;&#27963;&#21160;&#12289;&#31649;&#29702;&#20132;&#36890;&#36335;&#32447;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#12290;&#26412;&#25991;&#23558;&#38477;&#27700;&#39044;&#25253;&#24418;&#24335;&#21270;&#20026;&#26102;&#31354;&#22270;&#24207;&#21015;&#39044;&#25253;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24418;&#21452;&#27969;&#21367;&#31215;&#27880;&#24847;&#21147;&#34701;&#21512;&#65288;GD-CAF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#26088;&#22312;&#20174;&#21382;&#21490;&#38477;&#27700;&#22270;&#30340;&#26102;&#31354;&#22270;&#20013;&#23398;&#20064;&#65292;&#24182;&#39044;&#27979;&#26410;&#26469;&#19981;&#21516;&#31354;&#38388;&#20301;&#32622;&#30340;&#38477;&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.07958v2 Announce Type: replace  Abstract: Accurate precipitation nowcasting is essential for various applications, including flood prediction, disaster management, optimizing agricultural activities, managing transportation routes and renewable energy. While several studies have addressed this challenging task from a sequence-to-sequence perspective, most of them have focused on a single area without considering the existing correlation between multiple disjoint regions. In this paper, we formulate precipitation nowcasting as a spatiotemporal graph sequence nowcasting problem. In particular, we introduce Graph Dual-stream Convolutional Attention Fusion (GD-CAF), a novel approach designed to learn from historical spatiotemporal graph of precipitation maps and nowcast future time step ahead precipitation at different spatial locations. GD-CAF consists of spatio-temporal convolutional attention as well as gated fusion modules which are equipped with depthwise-separable convolut
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayes&#30028;&#38480;&#65292;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20013;&#27979;&#35797;&#25439;&#22833;&#20998;&#24067;&#25110;&#20998;&#31867;&#20013;&#19981;&#21516;&#38169;&#35823;&#20998;&#31867;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2202.05560</link><description>&lt;p&gt;
&#20351;&#29992;PAC-Bayes&#30028;&#38480;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Controlling Multiple Errors Simultaneously with a PAC-Bayes Bound
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.05560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;PAC-Bayes&#30028;&#38480;&#65292;&#33021;&#22815;&#21516;&#26102;&#25511;&#21046;&#22810;&#20010;&#38169;&#35823;&#65292;&#24182;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#65292;&#36866;&#29992;&#20110;&#22238;&#24402;&#20013;&#27979;&#35797;&#25439;&#22833;&#20998;&#24067;&#25110;&#20998;&#31867;&#20013;&#19981;&#21516;&#38169;&#35823;&#20998;&#31867;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;PAC-Bayes&#27867;&#21270;&#30028;&#38480;&#20165;&#38480;&#20110;&#24615;&#33021;&#30340;&#26631;&#37327;&#24230;&#37327;&#65292;&#22914;&#25439;&#22833;&#25110;&#38169;&#35823;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#33021;&#22815;&#25552;&#20379;&#20016;&#23500;&#20449;&#24687;&#30340;PAC-Bayes&#30028;&#38480;&#65292;&#36890;&#36807;&#30028;&#23450;&#19968;&#32452;M&#31181;&#38169;&#35823;&#31867;&#22411;&#30340;&#32463;&#39564;&#27010;&#29575;&#19982;&#30495;&#23454;&#27010;&#29575;&#20043;&#38388;&#30340;Kullback-Leibler&#24046;&#24322;&#26469;&#25511;&#21046;&#21487;&#33021;&#32467;&#26524;&#30340;&#25972;&#20010;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.05560v2 Announce Type: replace-cross  Abstract: Current PAC-Bayes generalisation bounds are restricted to scalar metrics of performance, such as the loss or error rate. However, one ideally wants more information-rich certificates that control the entire distribution of possible outcomes, such as the distribution of the test loss in regression, or the probabilities of different mis classifications. We provide the first PAC-Bayes bound capable of providing such rich information by bounding the Kullback-Leibler divergence between the empirical and true probabilities of a set of M error types, which can either be discretized loss values for regression, or the elements of the confusion matrix (or a partition thereof) for classification. We transform our bound into a differentiable training objective. Our bound is especially useful in cases where the severity of different mis-classifications may change over time; existing PAC-Bayes bounds can only bound a particular pre-decided w
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#24490;&#29615;&#27169;&#22411;&#20013;&#21547;&#26377;&#38544;&#34255;&#22240;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#33021;&#22815;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#22810;&#31181;&#25216;&#26415;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13009</link><description>&lt;p&gt;
&#24490;&#29615;&#27169;&#22411;&#20013;&#21547;&#26377;&#38544;&#34255;&#22240;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Causal Discovery Methods for Cyclic Models with Hidden Confounders. (arXiv:2401.13009v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13009
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#24490;&#29615;&#27169;&#22411;&#20013;&#21547;&#26377;&#38544;&#34255;&#22240;&#21464;&#37327;&#30340;&#22240;&#26524;&#21457;&#29616;&#65292;&#24050;&#32463;&#20986;&#29616;&#20102;&#33021;&#22815;&#22788;&#29702;&#36825;&#31181;&#24773;&#20917;&#30340;&#22810;&#31181;&#25216;&#26415;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20170;&#65292;&#23545;&#22240;&#26524;&#21457;&#29616;&#30340;&#38656;&#27714;&#26080;&#22788;&#19981;&#22312;&#12290;&#29702;&#35299;&#31995;&#32479;&#20013;&#37096;&#20998;&#20043;&#38388;&#30340;&#38543;&#26426;&#20381;&#36182;&#24615;&#20197;&#21450;&#23454;&#38469;&#30340;&#22240;&#26524;&#20851;&#31995;&#23545;&#31185;&#23398;&#30340;&#21508;&#20010;&#37096;&#20998;&#37117;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#23547;&#25214;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#26816;&#27979;&#22240;&#26524;&#26041;&#21521;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#22312;&#36807;&#21435;&#30340;50&#24180;&#37324;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#20165;&#36866;&#29992;&#20110;&#31995;&#32479;&#27809;&#26377;&#21453;&#39304;&#29615;&#36335;&#24182;&#19988;&#20855;&#26377;&#22240;&#26524;&#20805;&#20998;&#24615;&#30340;&#20551;&#35774;&#65292;&#21363;&#27809;&#26377;&#26410;&#27979;&#37327;&#30340;&#23376;&#31995;&#32479;&#33021;&#22815;&#24433;&#21709;&#22810;&#20010;&#24050;&#27979;&#37327;&#21464;&#37327;&#12290;&#36825;&#26159;&#19981;&#24184;&#30340;&#65292;&#22240;&#20026;&#36825;&#20123;&#38480;&#21046;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#19981;&#33021;&#20551;&#23450;&#12290;&#21453;&#39304;&#26159;&#35768;&#22810;&#36807;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#31995;&#32479;&#24456;&#23569;&#26159;&#23436;&#20840;&#38548;&#31163;&#21644;&#23436;&#20840;&#27979;&#37327;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#24050;&#32463;&#21457;&#23637;&#20102;&#20960;&#31181;&#33021;&#22815;&#22788;&#29702;&#24490;&#29615;&#30340;&#12289;&#22240;&#26524;&#19981;&#20805;&#20998;&#30340;&#31995;&#32479;&#30340;&#25216;&#26415;&#12290;&#38543;&#30528;&#22810;&#31181;&#26041;&#27861;&#30340;&#20986;&#29616;&#65292;&#19968;&#31181;&#23454;&#38469;&#30340;&#24212;&#29992;&#26041;&#27861;&#24320;&#22987;&#21464;&#24471;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, the need for causal discovery is ubiquitous. A better understanding of not just the stochastic dependencies between parts of a system, but also the actual cause-effect relations, is essential for all parts of science. Thus, the need for reliable methods to detect causal directions is growing constantly. In the last 50 years, many causal discovery algorithms have emerged, but most of them are applicable only under the assumption that the systems have no feedback loops and that they are causally sufficient, i.e. that there are no unmeasured subsystems that can affect multiple measured variables. This is unfortunate since those restrictions can often not be presumed in practice. Feedback is an integral feature of many processes, and real-world systems are rarely completely isolated and fully measured. Fortunately, in recent years, several techniques, that can cope with cyclic, causally insufficient systems, have been developed. And with multiple methods available, a practical ap
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#23631;&#34109;&#26041;&#27861;&#26469;&#28040;&#38500;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#31227;&#38500;&#23427;&#20204;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#32508;&#21512;&#32771;&#34385;&#29305;&#24449;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.12644</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#29305;&#24449;&#23631;&#34109;&#20248;&#21270;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Binary Feature Mask Optimization for Feature Selection. (arXiv:2401.12644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#23631;&#34109;&#26041;&#27861;&#26469;&#28040;&#38500;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#31227;&#38500;&#23427;&#20204;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#32508;&#21512;&#32771;&#34385;&#29305;&#24449;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#36873;&#25321;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#23631;&#34109;&#26041;&#27861;&#65292;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#28040;&#38500;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#23436;&#20840;&#31227;&#38500;&#23427;&#20204;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#20351;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20687;&#20854;&#20182;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#37027;&#26679;&#38656;&#35201;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#33719;&#21462;&#23631;&#34109;&#25805;&#20316;&#31526;&#65292;&#36825;&#20026;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#20379;&#20102;&#23545;&#29305;&#24449;&#23376;&#38598;&#30340;&#20840;&#38754;&#35266;&#23519;&#12290;&#29305;&#24449;&#36873;&#25321;&#25991;&#29486;&#20013;&#23384;&#22312;&#21508;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#30740;&#31350;&#24341;&#20837;&#19968;&#20010;&#38024;&#23545;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#20197;&#25972;&#20307;&#32771;&#34385;&#29305;&#24449;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#26159;&#21482;&#20851;&#27880;&#21333;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate feature selection problem for generic machine learning (ML) models. We introduce a novel framework that selects features considering the predictions of the model. Our framework innovates by using a novel feature masking approach to eliminate the features during the selection process, instead of completely removing them from the dataset. This allows us to use the same ML model during feature selection, unlike other feature selection methods where we need to train the ML model again as the dataset has different dimensions on each iteration. We obtain the mask operator using the predictions of the ML model, which offers a comprehensive view on the subsets of the features essential for the predictive performance of the model. A variety of approaches exist in the feature selection literature. However, no study has introduced a training-free framework for a generic ML model to select features while considering the importance of the feature subsets as a whole, instead of focusi
&lt;/p&gt;</description></item><item><title>xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;</title><link>http://arxiv.org/abs/2401.06199</link><description>&lt;p&gt;
xTrimoPGLM: &#32479;&#19968;&#30340;&#30334;&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#26512;&#34507;&#30333;&#36136;&#30340;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein. (arXiv:2401.06199v1 [q-bio.QM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06199
&lt;/p&gt;
&lt;p&gt;
xTrimoPGLM&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;100&#20159;&#35268;&#27169;&#39044;&#35757;&#32451;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21644;&#22823;&#35268;&#27169;&#30340;&#21442;&#25968;&#35757;&#32451;&#65292;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#65292;&#22312;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#21407;&#23376;&#20998;&#36776;&#29575;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#22312;&#23398;&#20064;&#34507;&#30333;&#36136;&#24207;&#21015;&#20013;&#30340;&#29983;&#29289;&#20449;&#24687;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#23616;&#38480;&#20110;&#33258;&#32534;&#30721;&#25110;&#33258;&#22238;&#24402;&#30340;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#22788;&#29702;&#34507;&#30333;&#36136;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#26102;&#24456;&#38590;&#21516;&#26102;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#34507;&#30333;&#36136;&#35821;&#35328;&#27169;&#22411;&#65292;xTrimoPGLM&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#21516;&#26102;&#35299;&#20915;&#36825;&#20004;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#25506;&#32034;&#36825;&#20004;&#31867;&#30446;&#26631;&#30340;&#20860;&#23481;&#24615;&#21644;&#32852;&#21512;&#20248;&#21270;&#30340;&#28508;&#21147;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#19968;&#20010;&#20197;&#21069;&#25152;&#26410;&#26377;&#30340;&#35268;&#27169;&#65292;&#20351;&#29992;1000&#20159;&#21442;&#25968;&#21644;1&#19975;&#20159;&#35757;&#32451;&#26631;&#35760;&#26469;&#35757;&#32451;xTrimoPGLM&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;1&#65289;xTrimoPGLM&#22312;&#22235;&#20010;&#31867;&#21035;&#30340;18&#20010;&#34507;&#30333;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20808;&#36827;&#22522;&#32447;&#12290;&#35813;&#27169;&#22411;&#36824;&#26377;&#21161;&#20110;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#36827;&#34892;&#21407;&#23376;&#20998;&#36776;&#29575;&#30340;&#35266;&#23519;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Protein language models have shown remarkable success in learning biological information from protein sequences. However, most existing models are limited by either autoencoding or autoregressive pre-training objectives, which makes them struggle to handle protein understanding and generation tasks concurrently. We propose a unified protein language model, xTrimoPGLM, to address these two types of tasks simultaneously through an innovative pre-training framework. Our key technical contribution is an exploration of the compatibility and the potential for joint optimization of the two types of objectives, which has led to a strategy for training xTrimoPGLM at an unprecedented scale of 100 billion parameters and 1 trillion training tokens. Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms other advanced baselines in 18 protein understanding benchmarks across four categories. The model also facilitates an atomic-resolution view of protein structures, leading to 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#26032;&#22411;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#30340;&#39044;&#27979;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#23646;&#24615;&#21644;&#29305;&#24449;&#21464;&#25442;&#26680;&#20989;&#25968;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#29305;&#24449;&#65292;&#24182;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#20934;&#30830;&#35782;&#21035;&#23395;&#33410;&#24615;&#21644;&#21046;&#24230;&#36716;&#25442;&#12290;</title><link>http://arxiv.org/abs/2401.04857</link><description>&lt;p&gt;
&#20351;&#29992;&#29305;&#24449;&#21464;&#25442;&#36827;&#34892;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transportation Market Rate Forecast Using Signature Transform. (arXiv:2401.04857v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#26032;&#22411;&#32479;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#30340;&#39044;&#27979;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#23646;&#24615;&#21644;&#29305;&#24449;&#21464;&#25442;&#26680;&#20989;&#25968;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#29305;&#24449;&#65292;&#24182;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#20934;&#30830;&#35782;&#21035;&#23395;&#33410;&#24615;&#21644;&#21046;&#24230;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#20122;&#39532;&#36874;&#22312;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#39044;&#27979;&#19978;&#20381;&#36182;&#31532;&#19977;&#26041;&#65292;&#23613;&#31649;&#36825;&#20123;&#39044;&#27979;&#36136;&#37327;&#24046;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#20132;&#36890;&#24066;&#22330;&#21033;&#29575;&#36890;&#24120;&#24456;&#38590;&#20934;&#30830;&#39044;&#27979;&#65292;&#20294;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#26032;&#22411;&#32479;&#35745;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#39044;&#27979;&#21644;&#33258;&#36866;&#24212;&#27169;&#22411;&#26469;&#39044;&#27979;&#24066;&#22330;&#21033;&#29575;&#12290;&#36825;&#31181;&#26032;&#25216;&#26415;&#22522;&#20110;&#29305;&#24449;&#21464;&#25442;&#30340;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#12290;&#31532;&#19968;&#20010;&#26159;&#20854;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#65292;&#23427;&#32447;&#24615;&#21270;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23558;&#39044;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#32447;&#24615;&#22238;&#24402;&#20998;&#26512;&#65307;&#31532;&#20108;&#20010;&#26159;&#29305;&#24449;&#21464;&#25442;&#26680;&#20989;&#25968;&#65292;&#23427;&#20801;&#35768;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#36827;&#34892;&#35745;&#31639;&#26377;&#25928;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#12290;&#32467;&#21512;&#36215;&#26469;&#65292;&#36825;&#20123;&#23646;&#24615;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#30340;&#29305;&#24449;&#29983;&#25104;&#65292;&#24182;&#22312;&#39044;&#27979;&#36807;&#31243;&#20013;&#26356;&#31934;&#30830;&#22320;&#35782;&#21035;&#23395;&#33410;&#24615;&#21644;&#21046;&#24230;&#36716;&#25442;&#12290;&#27169;&#22411;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26032;&#26041;&#27861;&#21487;&#20197;&#25913;&#21892;&#24066;&#22330;&#21033;&#29575;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Currently, Amazon relies on third parties for transportation marketplace rate forecasts, despite the poor quality and lack of interpretability of these forecasts. While transportation marketplace rates are typically very challenging to forecast accurately, we have developed a novel signature-based statistical technique to address these challenges and built a predictive and adaptive model to forecast marketplace rates. This novel technique is based on two key properties of the signature transform. The first is its universal nonlinearity which linearizes the feature space and hence translates the forecasting problem into a linear regression analysis; the second is the signature kernel which allows for comparing computationally efficiently similarities between time series data. Combined, these properties allow for efficient feature generation and more precise identification of seasonality and regime switching in the forecasting process. Preliminary result by the model shows that this new 
&lt;/p&gt;</description></item><item><title>&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#65292;Generative AI&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#30528;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#23433;&#20840;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.01923</link><description>&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;: &#35270;&#37326;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
IoT in the Era of Generative AI: Vision and Challenges. (arXiv:2401.01923v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01923
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#30340;&#29289;&#32852;&#32593;&#65292;Generative AI&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#65292;&#21516;&#26102;&#20063;&#38754;&#20020;&#30528;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#23433;&#20840;&#31561;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#24863;&#30693;&#12289;&#32593;&#32476;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#29289;&#32852;&#32593;&#35774;&#22791;&#65292;&#22914;&#26234;&#33021;&#25163;&#26426;&#12289;&#21487;&#31359;&#25140;&#35774;&#22791;&#12289;&#26234;&#33021;&#38899;&#31665;&#21644;&#23478;&#24237;&#26426;&#22120;&#20154;&#65292;&#24050;&#32463;&#26080;&#32541;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;Generative AI&#65289;&#30340;&#36827;&#23637;&#65292;&#22914;GPT&#12289;LLaMA&#12289;DALL-E&#21644;&#31283;&#23450;&#25193;&#25955;&#31561;&#65292;&#32473;&#29289;&#32852;&#32593;&#30340;&#21457;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#12290;&#26412;&#25991;&#20998;&#20139;&#20102;&#25105;&#20204;&#23545;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#24102;&#26469;&#30340;&#22909;&#22788;&#30340;&#30475;&#27861;&#21644;&#24895;&#26223;&#65292;&#24182;&#35752;&#35770;&#20102;Generative AI&#22312;&#29289;&#32852;&#32593;&#30456;&#20851;&#39046;&#22495;&#30340;&#19968;&#20123;&#37325;&#35201;&#24212;&#29992;&#12290;&#20805;&#20998;&#21033;&#29992;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20123;&#26368;&#20851;&#38190;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;Generative AI&#27169;&#22411;&#30340;&#39640;&#36164;&#28304;&#38656;&#27714;&#12289;&#21450;&#26102;&#24037;&#31243;&#12289;&#35774;&#22791;&#31471;&#25512;&#29702;&#12289;&#21368;&#36733;&#12289;&#35774;&#22791;&#31471;&#24494;&#35843;&#12289;&#32852;&#37030;&#23398;&#20064;&#12289;&#23433;&#20840;&#20197;&#21450;&#24320;&#21457;&#24037;&#20855;&#21644;&#22522;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#23384;&#22312;&#30340;&#24046;&#36317;&#20197;&#21450;&#20351;Generative AI&#22312;&#29289;&#32852;&#32593;&#20013;&#23454;&#29616;&#30340;&#26377;&#24076;&#26395;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#25991;&#31456;&#33021;&#22815;&#28608;&#21457;&#26032;&#30340;&#30740;&#31350;&#21644;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with sensing, networking, and computing capabilities, Internet of Things (IoT) such as smartphones, wearables, smart speakers, and household robots have been seamlessly weaved into our daily lives. Recent advancements in Generative AI exemplified by GPT, LLaMA, DALL-E, and Stable Difussion hold immense promise to push IoT to the next level. In this article, we share our vision and views on the benefits that Generative AI brings to IoT, and discuss some of the most important applications of Generative AI in IoT-related domains. Fully harnessing Generative AI in IoT is a complex challenge. We identify some of the most critical challenges including high resource demands of the Generative AI models, prompt engineering, on-device inference, offloading, on-device fine-tuning, federated learning, security, as well as development tools and benchmarks, and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new res
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#26469;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#24212;&#23545;&#26410;&#26366;&#35265;&#36807;&#30340;&#24773;&#20917;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;ROAM&#21487;&#20197;&#22312;&#21333;&#20010;&#38454;&#27573;&#20869;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01059</link><description>&lt;p&gt;
&#22312;&#37096;&#32626;&#26102;&#36827;&#34892;&#23454;&#26102;&#35843;&#33410;&#65306;&#29992;&#20110;&#21333;&#26426;&#22120;&#20154;&#37096;&#32626;&#30340;&#34892;&#20026;&#35843;&#25511;
&lt;/p&gt;
&lt;p&gt;
Adapt On-the-Go: Behavior Modulation for Single-Life Robot Deployment. (arXiv:2311.01059v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01059
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROAM&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#34892;&#20026;&#26469;&#23454;&#26102;&#35843;&#33410;&#26426;&#22120;&#20154;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#24212;&#23545;&#26410;&#26366;&#35265;&#36807;&#30340;&#24773;&#20917;&#12290;&#22312;&#27979;&#35797;&#20013;&#65292;ROAM&#21487;&#20197;&#22312;&#21333;&#20010;&#38454;&#27573;&#20869;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#25928;&#29575;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#24212;&#23545;&#35757;&#32451;&#36807;&#31243;&#20013;&#26410;&#26366;&#35265;&#36807;&#30340;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#37096;&#32626;&#36807;&#31243;&#20013;&#38024;&#23545;&#36825;&#20123;&#26032;&#22330;&#26223;&#30340;&#23454;&#26102;&#35843;&#33410;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#20808;&#21069;&#23398;&#20064;&#21040;&#30340;&#22810;&#26679;&#21270;&#34892;&#20026;&#24211;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;RObust Autonomous Modulation&#65288;ROAM&#65289;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#39044;&#35757;&#32451;&#34892;&#20026;&#30340;&#24863;&#30693;&#20215;&#20540;&#30340;&#26426;&#21046;&#65292;&#20197;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#36873;&#25321;&#21644;&#35843;&#25972;&#39044;&#35757;&#32451;&#34892;&#20026;&#12290;&#20851;&#38190;&#26159;&#65292;&#36825;&#31181;&#35843;&#33410;&#36807;&#31243;&#22312;&#27979;&#35797;&#26102;&#30340;&#21333;&#20010;&#38454;&#27573;&#20869;&#23436;&#25104;&#65292;&#26080;&#38656;&#20219;&#20309;&#20154;&#31867;&#30417;&#30563;&#12290;&#25105;&#20204;&#23545;&#36873;&#25321;&#26426;&#21046;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#35777;&#26126;&#20102;ROAM&#20351;&#24471;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#30495;&#23454;&#30340;&#22235;&#36275;&#21160;&#29289;Go1&#19978;&#24555;&#36895;&#36866;&#24212;&#21160;&#24577;&#21464;&#21270;&#65292;&#29978;&#33267;&#22312;&#33050;&#19978;&#22871;&#30528;&#28378;&#36718;&#28369;&#38795;&#30340;&#24773;&#20917;&#19979;&#25104;&#21151;&#21069;&#36827;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38754;&#23545;&#21508;&#31181;&#20998;&#24067;&#24773;&#20917;&#30340;&#37096;&#32626;&#26102;&#33021;&#22815;&#20197;&#36229;&#36807;2&#20493;&#30340;&#25928;&#29575;&#36827;&#34892;&#35843;&#33410;&#65292;&#36890;&#36807;&#26377;&#25928;&#36873;&#25321;&#26469;&#23454;&#29616;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
To succeed in the real world, robots must cope with situations that differ from those seen during training. We study the problem of adapting on-the-fly to such novel scenarios during deployment, by drawing upon a diverse repertoire of previously learned behaviors. Our approach, RObust Autonomous Modulation (ROAM), introduces a mechanism based on the perceived value of pre-trained behaviors to select and adapt pre-trained behaviors to the situation at hand. Crucially, this adaptation process all happens within a single episode at test time, without any human supervision. We provide theoretical analysis of our selection mechanism and demonstrate that ROAM enables a robot to adapt rapidly to changes in dynamics both in simulation and on a real Go1 quadruped, even successfully moving forward with roller skates on its feet. Our approach adapts over 2x as efficiently compared to existing methods when facing a variety of out-of-distribution situations during deployment by effectively choosing
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36335;&#30001;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#37096;&#32626;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#26469;&#26368;&#22823;&#21270;&#28304;&#33410;&#28857;&#30340;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#25152;&#38656;&#20449;&#24687;&#36335;&#30001;&#21040;&#30446;&#26631;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2310.00248</link><description>&lt;p&gt;
&#22312;&#36890;&#20449;&#32593;&#32476;&#20013;&#23398;&#20064;&#22686;&#24378;&#29366;&#24577;&#31574;&#30053;&#36827;&#34892;&#20449;&#24687;&#36335;&#30001;
&lt;/p&gt;
&lt;p&gt;
Learning State-Augmented Policies for Information Routing in Communication Networks. (arXiv:2310.00248v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36335;&#30001;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#37096;&#32626;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#26469;&#26368;&#22823;&#21270;&#28304;&#33410;&#28857;&#30340;&#32858;&#21512;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#23558;&#25152;&#38656;&#20449;&#24687;&#36335;&#30001;&#21040;&#30446;&#26631;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#35268;&#27169;&#36890;&#20449;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#36335;&#30001;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21482;&#33021;&#35775;&#38382;&#23616;&#37096;&#20449;&#24687;&#30340;&#32422;&#26463;&#32479;&#35745;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29366;&#24577;&#22686;&#24378;&#65288;SA&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#36890;&#20449;&#32593;&#32476;&#30340;&#25299;&#25169;&#38142;&#36335;&#19978;&#37096;&#32626;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#26469;&#26368;&#22823;&#21270;&#28304;&#33410;&#28857;&#30340;&#32858;&#21512;&#20449;&#24687;&#12290;&#25152;&#25552;&#20986;&#30340;&#25216;&#26415;&#20165;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#19978;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#24182;&#26377;&#25928;&#22320;&#23558;&#25152;&#38656;&#30340;&#20449;&#24687;&#36335;&#30001;&#21040;&#30446;&#26631;&#33410;&#28857;&#12290;&#25105;&#20204;&#21033;&#29992;&#26080;&#30417;&#30563;&#23398;&#20064;&#36807;&#31243;&#23558;GNN&#26550;&#26500;&#30340;&#36755;&#20986;&#36716;&#25442;&#20026;&#26368;&#20248;&#30340;&#20449;&#24687;&#36335;&#30001;&#31574;&#30053;&#12290;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23545;&#23454;&#26102;&#32593;&#32476;&#25299;&#25169;&#36827;&#34892;&#35780;&#20272;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#31639;&#27861;&#12290;&#25968;&#20540;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#20986;&#19982;&#22522;&#32447;&#31639;&#27861;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;GNN&#21442;&#25968;&#21270;&#26041;&#38754;&#30340;&#24615;&#33021;&#26377;&#25152;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the problem of information routing in a large-scale communication network, which can be formulated as a constrained statistical learning problem having access to only local information. We delineate a novel State Augmentation (SA) strategy to maximize the aggregate information at source nodes using graph neural network (GNN) architectures, by deploying graph convolutions over the topological links of the communication network. The proposed technique leverages only the local information available at each node and efficiently routes desired information to the destination nodes. We leverage an unsupervised learning procedure to convert the output of the GNN architecture to optimal information routing strategies. In the experiments, we perform the evaluation on real-time network topologies to validate our algorithms. Numerical simulations depict the improved performance of the proposed method in training a GNN parameterization as compared to baseline algorithms.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#32570;&#20047;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#24314;&#35758;&#37319;&#29992;&#24191;&#27867;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2308.04137</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#24615;&#33021;&#30340;&#32508;&#21512;&#35780;&#20272;&#25581;&#31034;&#20986;&#24778;&#20154;&#30340;&#32570;&#20047;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of the Performance of Deep Learning Classifiers Reveals a Surprising Lack of Robustness. (arXiv:2308.04137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04137
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#65292;&#21457;&#29616;&#23427;&#20204;&#32570;&#20047;&#31283;&#23450;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#24314;&#35758;&#37319;&#29992;&#24191;&#27867;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#32479;&#19968;&#30340;&#35780;&#20272;&#25351;&#26631;&#36827;&#34892;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#32780;&#31283;&#20581;&#30340;&#35780;&#20272;&#26041;&#27861;&#26159;&#24320;&#21457;&#26412;&#36523;&#31283;&#20581;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24517;&#35201;&#31532;&#19968;&#27493;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#29992;&#20110;&#35780;&#20272;&#20998;&#31867;&#22120;&#30340;&#24120;&#35268;&#35780;&#20272;&#21327;&#35758;&#22312;&#32508;&#21512;&#35780;&#20272;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#20381;&#36182;&#20110;&#26377;&#38480;&#31867;&#22411;&#30340;&#27979;&#35797;&#25968;&#25454;&#65292;&#24573;&#35270;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#20351;&#29992;&#26631;&#20934;&#27979;&#35797;&#25968;&#25454;&#26080;&#27861;&#35780;&#20272;&#20998;&#31867;&#22120;&#23545;&#20110;&#26410;&#32463;&#35757;&#32451;&#30340;&#31867;&#21035;&#26679;&#26412;&#30340;&#39044;&#27979;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20351;&#29992;&#21253;&#21547;&#26410;&#30693;&#31867;&#21035;&#26679;&#26412;&#30340;&#25968;&#25454;&#36827;&#34892;&#27979;&#35797;&#26080;&#27861;&#35780;&#20272;&#20998;&#31867;&#22120;&#23545;&#20110;&#24050;&#30693;&#31867;&#21035;&#26631;&#31614;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20513;&#20351;&#29992;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#24615;&#33021;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21487;&#24212;&#29992;&#20110;&#25152;&#26377;&#36825;&#20123;&#25968;&#25454;&#31867;&#22411;&#30340;&#21333;&#19968;&#25351;&#26631;&#65292;&#20197;&#20135;&#29983;&#19968;&#33268;&#30340;&#24615;&#33021;&#35780;&#20272;&#32467;&#26524;&#12290;&#36890;&#36807;&#36825;&#26679;&#30340;&#22522;&#20934;&#27979;&#35797;&#21457;&#29616;&#65292;&#30446;&#21069;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21253;&#25324;&#20351;&#29992;&#35748;&#20026;&#26159;&#20840;&#38754;&#30340;&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#30340;&#32593;&#32476;&#65292;&#20063;&#23384;&#22312;&#32570;&#20047;&#31283;&#23450;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable and robust evaluation methods are a necessary first step towards developing machine learning models that are themselves robust and reliable. Unfortunately, current evaluation protocols typically used to assess classifiers fail to comprehensively evaluate performance as they tend to rely on limited types of test data, and ignore others. For example, using the standard test data fails to evaluate the predictions made by the classifier to samples from classes it was not trained on. On the other hand, testing with data containing samples from unknown classes fails to evaluate how well the classifier can predict the labels for known classes. This article advocates bench-marking performance using a wide range of different types of data and using a single metric that can be applied to all such data types to produce a consistent evaluation of performance. Using such a benchmark it is found that current deep neural networks, including those trained with methods that are believed to pro
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09273</link><description>&lt;p&gt;
&#20320;&#30340;&#25151;&#38388;&#19981;&#26159;&#31169;&#23494;&#30340;&#65306;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Your Room is not Private: Gradient Inversion Attack on Reinforcement Learning. (arXiv:2306.09273v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09273
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#20197;&#35299;&#20915;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#26174;&#33879;&#21457;&#23637;&#21560;&#24341;&#20102;&#20154;&#20204;&#30340;&#26497;&#22823;&#20851;&#27880;&#65292;&#35813;&#25216;&#26415;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#23548;&#33322;&#12289;&#24863;&#30693;&#21644;&#20114;&#21160;&#12290;&#30001;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#38544;&#31169;&#38382;&#39064;&#22312;&#23884;&#20837;&#24335;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#21487;&#20197;&#35775;&#38382;&#22823;&#37327;&#20010;&#20154;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#38382;&#39064;&#65292;&#23588;&#20854;&#26159;&#20851;&#20110;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#22312;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#32771;&#34385;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#25915;&#20987;&#20540;&#20989;&#25968;&#31639;&#27861;&#21644;&#26799;&#24230;&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26799;&#24230;&#21453;&#36716;&#37325;&#24314;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#30417;&#30563;&#20449;&#21495;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36873;&#25321;&#20351;&#29992;&#26799;&#24230;&#36827;&#34892;&#25915;&#20987;&#26159;&#22240;&#20026;&#24120;&#29992;&#30340;&#32852;&#37030;&#23398;&#20064;&#25216;&#26415;&#20165;&#21033;&#29992;&#22522;&#20110;&#31169;&#20154;&#29992;&#25143;&#25968;&#25454;&#35745;&#31639;&#30340;&#26799;&#24230;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#32780;&#19981;&#23384;&#20648;&#25110;&#20256;&#36755;&#29992;&#25143;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prominence of embodied Artificial Intelligence (AI), which empowers robots to navigate, perceive, and engage within virtual environments, has attracted significant attention, owing to the remarkable advancements in computer vision and large language models. Privacy emerges as a pivotal concern within the realm of embodied AI, as the robot accesses substantial personal information. However, the issue of privacy leakage in embodied AI tasks, particularly in relation to reinforcement learning algorithms, has not received adequate consideration in research. This paper aims to address this gap by proposing an attack on the value-based algorithm and the gradient-based algorithm, utilizing gradient inversion to reconstruct states, actions, and supervision signals. The choice of using gradients for the attack is motivated by the fact that commonly employed federated learning techniques solely utilize gradients computed based on private user data to optimize models, without storing or trans
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;DiffEEG&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#30315;&#30187;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08256</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#30340;&#30315;&#30187;&#39044;&#27979;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation for Seizure Prediction with Generative Diffusion Model. (arXiv:2306.08256v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;DiffEEG&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#30315;&#30187;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#65306;&#30315;&#30187;&#39044;&#27979;&#23545;&#20110;&#25913;&#21892;&#24739;&#32773;&#29983;&#27963;&#36136;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#37325;&#28857;&#22312;&#20110;&#21306;&#20998;&#21457;&#20316;&#21069;&#29366;&#24577;&#19982;&#21457;&#20316;&#21518;&#29366;&#24577;&#12290;&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#23637;&#65292;&#30315;&#30187;&#39044;&#27979;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#21457;&#20316;&#21069;&#19982;&#21457;&#20316;&#21518;&#29366;&#24577;&#25968;&#25454;&#20043;&#38388;&#30340;&#20005;&#37325;&#19981;&#24179;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#25968;&#25454;&#25193;&#22686;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#30452;&#35266;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#27861;&#36890;&#36807;&#37325;&#21472;&#25110;&#37325;&#26032;&#32452;&#21512;&#25968;&#25454;&#26469;&#29983;&#25104;&#26679;&#26412;&#12290;&#30001;&#20110;&#36825;&#20123;&#36716;&#25442;&#26080;&#27861;&#23436;&#20840;&#25506;&#32034;&#29305;&#24449;&#31354;&#38388;&#24182;&#25552;&#20379;&#26032;&#20449;&#24687;&#65292;&#25152;&#20197;&#29983;&#25104;&#30340;&#26679;&#26412;&#20998;&#24067;&#21463;&#21040;&#21407;&#22987;&#25968;&#25454;&#30340;&#38480;&#21046;&#12290;&#30001;&#20110;&#30315;&#30187;&#33041;&#30005;&#22270;&#34920;&#31034;&#22312;&#19981;&#21516;&#21457;&#20316;&#20043;&#38388;&#20855;&#26377;&#24046;&#24322;&#24615;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#26679;&#26412;&#19981;&#33021;&#25552;&#20379;&#36275;&#22815;&#30340;&#22810;&#26679;&#24615;&#20197;&#22312;&#26032;&#30340;&#30315;&#30187;&#21457;&#20316;&#20013;&#23454;&#29616;&#39640;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#22411;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;DiffEEG&#12290;&#26041;&#27861;&#65306;&#25193;&#25955;&#27169;&#22411;&#26159;&#19968;&#31181;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#25105;&#20204;&#20351;&#29992;&#27492;&#27169;&#22411;&#26469;&#23545;&#21407;&#22987;&#33041;&#30005;&#22270;&#25968;&#25454;&#36827;&#34892;&#36716;&#25442;&#20197;&#29983;&#25104;&#22810;&#26679;&#24615;&#30340;&#26679;&#26412;&#65292;&#36827;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#65306;DiffEEG&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;SVM&#27169;&#22411;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#39640;&#30315;&#30187;&#39044;&#27979;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#25193;&#22686;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective: Seizure prediction is of great importance to improve the life of patients. The focal point is to distinguish preictal states from interictal ones. With the development of machine learning, seizure prediction methods have achieved significant progress. However, the severe imbalance problem between preictal and interictal data still poses a great challenge, restricting the performance of classifiers. Data augmentation is an intuitive way to solve this problem. Existing data augmentation methods generate samples by overlapping or recombining data. The distribution of generated samples is limited by original data, because such transformations cannot fully explore the feature space and offer new information. As the epileptic EEG representation varies among seizures, these generated samples cannot provide enough diversity to achieve high performance on a new seizure. As a consequence, we propose a novel data augmentation method with diffusion model called DiffEEG. Methods: Diffusi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;FAME&#65292;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#65292;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.01423</link><description>&lt;p&gt;
&#21033;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#23454;&#29616;&#24555;&#36895;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Leveraging the Triple Exponential Moving Average for Fast-Adaptive Moment Estimation. (arXiv:2306.01423v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;FAME&#65292;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#65292;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#21487;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#20248;&#21270;&#26159;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#30452;&#25509;&#24433;&#21709;&#35745;&#31639;&#26426;&#35270;&#35273;&#31561;&#22810;&#31181;&#39046;&#22495;&#20013;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#34429;&#28982;&#22810;&#31181;&#20248;&#21270;&#22120;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24555;&#36895;&#22320;&#35782;&#21035;&#26799;&#24230;&#36235;&#21183;&#26041;&#38754;&#20173;&#28982;&#26377;&#38480;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#32593;&#32476;&#24615;&#33021;&#19981;&#20339;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#20248;&#21270;&#22120;&#65292;&#31216;&#20026;&#24555;&#36895;&#33258;&#36866;&#24212;&#30697;&#20272;&#35745;&#65288;FAME&#65289;&#65292;&#23427;&#39318;&#27425;&#20351;&#29992;&#19977;&#37325;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#65288;TEMA&#65289;&#26469;&#20272;&#35745;&#26799;&#24230;&#30697;&#12290;&#23558;TEMA&#32435;&#20837;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#21487;&#20197;&#25552;&#20379;&#26356;&#20016;&#23500;&#21644;&#20934;&#30830;&#30340;&#25968;&#25454;&#21464;&#21270;&#21644;&#36235;&#21183;&#20449;&#24687;&#65292;&#19982;&#30446;&#21069;&#25152;&#26377;&#20027;&#35201;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#26631;&#20934;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#20540;&#30456;&#27604;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;FAME&#20248;&#21270;&#22120;&#24050;&#32463;&#22312;&#24191;&#27867;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;&#21253;&#25324;CIFAR-10&#65292;CIFAR-100&#65292;PASCAL-VOC&#65292;MS-COCO&#21644;Cityscapes&#12290;
&lt;/p&gt;
&lt;p&gt;
Network optimization is a crucial step in the field of deep learning, as it directly affects the performance of models in various domains such as computer vision. Despite the numerous optimizers that have been developed over the years, the current methods are still limited in their ability to accurately and quickly identify gradient trends, which can lead to sub-optimal network performance. In this paper, we propose a novel deep optimizer called Fast-Adaptive Moment Estimation (FAME), which for the first time estimates gradient moments using a Triple Exponential Moving Average (TEMA). Incorporating TEMA into the optimization process provides richer and more accurate information on data changes and trends, as compared to the standard Exponential Moving Average used in essentially all current leading adaptive optimization methods. Our proposed FAME optimizer has been extensively validated through a wide range of benchmarks, including CIFAR-10, CIFAR-100, PASCAL-VOC, MS-COCO, and Cityscap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#35777;&#26126;&#35813;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.14537</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#29305;&#24449;&#26368;&#20248;&#20998;&#21306;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal partition of feature using Bayesian classifier. (arXiv:2304.14537v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#35777;&#26126;&#35813;&#25216;&#26415;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#29575;&#21644;&#26356;&#20302;&#30340;&#38169;&#35823;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26420;&#32032;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#26159;&#19968;&#31181;&#24212;&#29992;&#36125;&#21494;&#26031;&#21407;&#29702;&#30340;&#27969;&#34892;&#20998;&#31867;&#26041;&#27861;&#65292;&#23613;&#31649;&#36755;&#20837;&#21464;&#37327;&#20043;&#38388;&#30340;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#21548;&#36215;&#26469;&#24456;&#22909;&#65292;&#20294;&#23454;&#38469;&#19978;&#20250;&#23548;&#33268;&#22823;&#22810;&#25968;&#25237;&#31080;&#39118;&#26684;&#30340;&#34892;&#20026;&#12290;&#26420;&#32032;&#36125;&#21494;&#26031;&#31639;&#27861;&#20013;&#30340;&#26576;&#20123;&#29305;&#24449;&#34987;&#31216;&#20026;&#29420;&#31435;&#29305;&#24449;&#65292;&#22240;&#20026;&#22312;&#39044;&#27979;&#20998;&#31867;&#26102;&#23427;&#20204;&#27809;&#26377;&#26465;&#20214;&#30456;&#20851;&#24615;&#25110;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;&#8220;&#20849;&#21333;&#35843;&#29420;&#31435;&#20998;&#31867;&#22120;&#8221;(CIBer)&#30340;&#26032;&#25216;&#26415;&#65292;&#19987;&#27880;&#20110;&#29305;&#24449;&#30340;&#26368;&#20248;&#20998;&#21306;&#65292;&#26088;&#22312;&#20811;&#26381;&#26420;&#32032;&#36125;&#21494;&#26031;&#26041;&#27861;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#26126;&#30830;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#38169;&#35823;&#29575;&#26356;&#20302;&#12289;&#20934;&#30830;&#29575;&#26356;&#39640;&#25110;&#30456;&#24403;&#30340;&#24773;&#20917;&#19979;&#65292;&#19982;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#31561;&#27169;&#22411;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Naive Bayesian classifier is a popular classification method employing the Bayesian paradigm. The concept of having conditional dependence among input variables sounds good in theory but can lead to a majority vote style behaviour. Achieving conditional independence is often difficult, and they introduce decision biases in the estimates. In Naive Bayes, certain features are called independent features as they have no conditional correlation or dependency when predicting a classification. In this paper, we focus on the optimal partition of features by proposing a novel technique called the Comonotone-Independence Classifier (CIBer) which is able to overcome the challenges posed by the Naive Bayes method. For different datasets, we clearly demonstrate the efficacy of our technique, where we achieve lower error rates and higher or equivalent accuracy compared to models such as Random Forests and XGBoost.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.09825</link><description>&lt;p&gt;
&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#21152;&#36895;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Using Offline Data to Speed-up Reinforcement Learning in Procedurally Generated Environments. (arXiv:2304.09825v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#30740;&#31350;&#35777;&#26126;&#65292;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#20195;&#29702;&#33021;&#22815;&#23558;&#20854;&#23398;&#20064;&#31574;&#30053;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29615;&#22659;&#20013;&#12290;&#27492;&#22806;&#65292;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#22823;&#37327;&#20132;&#20114;&#12290;&#21463;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#27169;&#20223;&#23398;&#20064;&#30340;&#26368;&#36817;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#20197;&#35843;&#26597;&#20195;&#29702;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#36712;&#36857;&#30340;&#31163;&#32447;&#25968;&#25454;&#26469;&#25552;&#39640;&#31243;&#24207;&#29983;&#25104;&#29615;&#22659;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20351;&#29992;&#31163;&#32447;&#25968;&#25454;&#30340;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65306;&#65288;1&#65289;&#22312;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#20043;&#21069;&#39044;&#35757;&#32451;&#31574;&#30053;&#21644;&#65288;2&#65289;&#21516;&#26102;&#35757;&#32451;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#21644;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#30340;&#27169;&#20223;&#23398;&#20064;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#21487;&#29992;&#30340;&#31163;&#32447;&#36712;&#36857;&#30340;&#36136;&#37327;&#65288;&#36712;&#36857;&#30340;&#26368;&#20339;&#24615;&#65289;&#21644;&#22810;&#26679;&#24615;&#65288;&#36712;&#36857;&#25968;&#37327;&#21644;&#35206;&#30422;&#32423;&#21035;&#65289;&#23545;&#20004;&#31181;&#26041;&#27861;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#12290;&#22312;MiniGrid&#29615;&#22659;&#20013;&#30340;&#22235;&#20010;&#30693;&#21517;&#31232;&#30095;&#22870;&#21169;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#27169;&#20223;&#23398;&#20064;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#21516;&#26102;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#21644;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21487;&#20197;&#25552;&#20379;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the key challenges of Reinforcement Learning (RL) is the ability of agents to generalise their learned policy to unseen settings. Moreover, training RL agents requires large numbers of interactions with the environment. Motivated by the recent success of Offline RL and Imitation Learning (IL), we conduct a study to investigate whether agents can leverage offline data in the form of trajectories to improve the sample-efficiency in procedurally generated environments. We consider two settings of using IL from offline data for RL: (1) pre-training a policy before online RL training and (2) concurrently training a policy with online RL and IL from offline data. We analyse the impact of the quality (optimality of trajectories) and diversity (number of trajectories and covered level) of available offline trajectories on the effectiveness of both approaches. Across four well-known sparse reward tasks in the MiniGrid environment, we find that using IL for pre-training and concurrently d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;</title><link>http://arxiv.org/abs/2303.13763</link><description>&lt;p&gt;
&#26080;&#38656;&#36793;&#32536;&#20294;&#20855;&#26377;&#32467;&#26500;&#24863;&#30693;&#24615;&#65306;&#20174;GNN&#21040;MLP&#30340;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#31934;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20219;&#21153;&#20013;&#21387;&#32553;&#25104;&#20302;&#24310;&#36831;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20250;&#23558;&#22270;&#30340;&#36793;&#32536;&#22788;&#29702;&#25104;&#39069;&#22806;&#30340;&#36755;&#20837;&#32473;MLP&#65292;&#20294;&#36825;&#26679;&#30340;&#22270;&#32467;&#26500;&#23545;&#20110;&#21508;&#31181;&#22330;&#26223;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GNN&#25945;&#24072;&#20013;&#30340;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21407;&#22411;&#22312;&#26080;&#36793;&#32536;&#35774;&#32622;&#20013;&#20174;GNN&#21040;MLP&#36827;&#34892;&#20102;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#27969;&#34892;&#30340;&#22270;&#24418;&#22522;&#20934;&#23454;&#39564;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PGKD&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. However, MLPs rely exclusively on the node features and fail to capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26159;&#24191;&#20041;&#20102;DeepFool&#25915;&#20987;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12481</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;DeepFool&#65306;&#27867;&#21270;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Revisiting DeepFool: generalization and improvement. (arXiv:2303.12481v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#35813;&#25915;&#20987;&#26159;&#24191;&#20041;&#20102;DeepFool&#25915;&#20987;&#65292;&#26082;&#26377;&#25928;&#21448;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#36866;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#34987;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#26679;&#26412;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#36755;&#20837;&#31245;&#21152;&#20462;&#25913;&#20415;&#20250;&#23548;&#33268;&#32593;&#32476;&#20570;&#20986;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#36825;&#23548;&#33268;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#36825;&#20123;&#32593;&#32476;&#23545;&#27492;&#31867;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#12290;&#26368;&#23567;l2&#23545;&#25239;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#26159;&#19968;&#31181;&#29305;&#21035;&#37325;&#35201;&#30340;&#40065;&#26834;&#24615;&#24230;&#37327;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#29992;&#20110;&#35780;&#20272;&#27492;&#31867;&#40065;&#26834;&#24615;&#24230;&#37327;&#30340;&#26041;&#27861;&#65292;&#35201;&#20040;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#35201;&#20040;&#19981;&#22826;&#20934;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#23427;&#22312;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#26159;&#24191;&#20041;&#20102;&#28145;&#24230;&#27450;&#39575;&#65288;DeepFool&#65289;&#25915;&#20987;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#26131;&#20110;&#29702;&#35299;&#21644;&#23454;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#22312;&#25928;&#26524;&#21644;&#35745;&#31639;&#25928;&#29575;&#26041;&#38754;&#22343;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25915;&#20987;&#20063;&#36866;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have been known to be vulnerable to adversarial examples, which are inputs that are modified slightly to fool the network into making incorrect predictions. This has led to a significant amount of research on evaluating the robustness of these networks against such perturbations. One particularly important robustness metric is the robustness to minimal l2 adversarial perturbations. However, existing methods for evaluating this robustness metric are either computationally expensive or not very accurate. In this paper, we introduce a new family of adversarial attacks that strike a balance between effectiveness and computational efficiency. Our proposed attacks are generalizations of the well-known DeepFool (DF) attack, while they remain simple to understand and implement. We demonstrate that our attacks outperform existing methods in terms of both effectiveness and computational efficiency. Our proposed attacks are also suitable for evaluating the robustness of large
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15629</link><description>&lt;p&gt;
&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#65306;&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#20219;&#21153;&#39640;&#25928;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65292;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#35299;&#20915;&#38271;&#26102;&#38388;&#36328;&#24230;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#19979;&#30340;&#25511;&#21046;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#65292;&#24182;&#26497;&#22823;&#22320;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#36890;&#29992;&#22411;&#26234;&#33021;&#20307;&#22312;&#21508;&#20010;&#26041;&#38754;&#37117;&#24456;&#22256;&#38590;&#65292;&#38656;&#35201;&#22788;&#29702;&#39640;&#32500;&#36755;&#20837;&#65288;&#31354;&#38388;&#65289;&#12289;&#38271;&#26102;&#38388;&#36328;&#24230;&#65288;&#26102;&#38388;&#65289;&#21644;&#22810;&#20010;&#26032;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#32467;&#26500;&#26041;&#38754;&#30340;&#36827;&#23637;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#27839;&#30528;&#20854;&#20013;&#19968;&#20010;&#25110;&#20004;&#20010;&#32500;&#24230;&#25552;&#39640;&#25193;&#23637;&#24615;&#33021;&#21147;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#24456;&#39640;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#35821;&#35328;&#25511;&#21046;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#26465;&#20214;&#30340;&#20998;&#23618;&#35268;&#21010;&#22120;&#65288;LCD&#65289;&#26469;&#24212;&#23545;&#36825;&#19977;&#20010;&#26041;&#38754;&#12290;&#25105;&#20204;&#26377;&#25928;&#32780;&#39640;&#25928;&#22320;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#12289;&#29366;&#24577;&#21644;&#20219;&#21153;&#31354;&#38388;&#32500;&#24230;&#30340;&#38271;&#26102;&#38388;&#36328;&#24230;&#25511;&#21046;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;CALVIN&#35821;&#35328;&#26426;&#22120;&#20154;&#22522;&#20934;&#27979;&#35797;&#20013;&#23558;LCD&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#21457;&#29616;LCD&#22312;&#22810;&#20219;&#21153;&#25104;&#21151;&#29575;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#21333;&#20219;&#21153;&#25104;&#21151;&#29575;&#65288;SR&#65289;&#20026;88.7%&#65292;&#36828;&#39640;&#20110;&#20197;&#21069;&#30340;&#26368;&#20339;&#25104;&#32489;82.6%&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generalist agents is difficult across several axes, requiring us to deal with high-dimensional inputs (space), long horizons (time), and multiple and new tasks. Recent advances with architectures have allowed for improved scaling along one or two of these dimensions, but are still prohibitive computationally. In this paper, we propose to address all three axes by leveraging Language to Control Diffusion models as a hierarchical planner conditioned on language (LCD). We effectively and efficiently scale diffusion models for planning in extended temporal, state, and task dimensions to tackle long horizon control problems conditioned on natural language instructions. We compare LCD with other state-of-the-art models on the CALVIN language robotics benchmark and find that LCD outperforms other SOTA methods in multi task success rates while dramatically improving computational efficiency with a single task success rate (SR) of 88.7% against the previous best of 82.6%. We show that 
&lt;/p&gt;</description></item></channel></rss>