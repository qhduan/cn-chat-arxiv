<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#26080;&#38480;&#28145;&#24230;&#21644;&#20219;&#24847;&#23485;&#24230;&#30340;ResNet&#30340;&#8220;&#22343;&#22330;&#8221;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#27969;&#25910;&#25947;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#31616;&#21333;&#20248;&#21270;&#31639;&#27861;&#22914;&#20309;&#25104;&#21151;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12887</link><description>&lt;p&gt;
&#29702;&#35299;&#20855;&#26377;&#26465;&#20214;&#26368;&#20248;&#36816;&#36755;&#30340;&#26080;&#38480;&#28145;&#24230;&#21644;&#23485;&#24230;ResNets&#30340;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Understanding the training of infinitely deep and wide ResNets with Conditional Optimal Transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12887
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#30740;&#31350;&#26080;&#38480;&#28145;&#24230;&#21644;&#20219;&#24847;&#23485;&#24230;&#30340;ResNet&#30340;&#8220;&#22343;&#22330;&#8221;&#27169;&#22411;&#65292;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26799;&#24230;&#27969;&#25910;&#25947;&#24615;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#31616;&#21333;&#20248;&#21270;&#31639;&#27861;&#22914;&#20309;&#25104;&#21151;&#35299;&#20915;&#36825;&#19968;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#26799;&#24230;&#27969;&#30340;&#25910;&#25947;&#24615;&#12290; &#22914;&#26524;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#26159;&#38750;&#24120;&#28145;&#30340;&#26550;&#26500;&#30340;&#19968;&#20010;&#24120;&#35265;&#20363;&#23376;&#65292;&#37027;&#20040;&#30001;&#20110;&#30446;&#26631;&#30340;&#38750;&#20984;&#24615;&#21644;&#38750;&#24378;&#20984;&#24615;&#65292;&#23427;&#20204;&#30340;&#35757;&#32451;&#26500;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290; Yet, &#22312;&#24212;&#29992;&#20013;&#65292;&#36825;&#20123;&#20219;&#21153;&#21487;&#20197;&#36890;&#36807;&#35832;&#22914;&#26799;&#24230;&#19979;&#38477;&#31561;&#31616;&#21333;&#30340;&#20248;&#21270;&#31639;&#27861;&#25104;&#21151;&#35299;&#20915;&#12290; &#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#19987;&#27880;&#20110;&#19968;&#20010;&#26080;&#38480;&#28145;&#24230;&#21644;&#20219;&#24847;&#23485;&#24230;&#30340;ResNet&#30340;&#8220;&#22343;&#22330;&#8221;&#27169;&#22411;&#65292;&#20854;&#21442;&#25968;&#30001;&#23618;&#21644;&#21442;&#25968;&#30340;&#20056;&#31215;&#38598;&#19978;&#30340;&#27010;&#29575;&#27979;&#24230;&#21442;&#25968;&#21270;&#65292;&#24182;&#22312;&#23618;&#38598;&#19978;&#20855;&#26377;&#24120;&#25968;&#36793;&#38469;&#12290; &#23454;&#38469;&#19978;&#65292;&#22312;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#24773;&#20917;&#19979;&#65292;&#22343;&#22330;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#22312;&#29992;&#26799;&#24230;&#27969;&#35757;&#32451;&#27010;&#29575;&#27979;&#24230;&#38598;&#19978;&#30340;Wasserstein&#24230;&#37327;&#26102;&#21463;&#30410;&#20110;&#31616;&#21270;&#30340;&#25439;&#22833;&#26223;&#35266;&#21644;&#33391;&#22909;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290; &#21463;&#36825;&#31181;&#26041;&#27861;&#30340;&#21551;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12887v1 Announce Type: new  Abstract: We study the convergence of gradient flow for the training of deep neural networks. If Residual Neural Networks are a popular example of very deep architectures, their training constitutes a challenging optimization problem due notably to the non-convexity and the non-coercivity of the objective. Yet, in applications, those tasks are successfully solved by simple optimization algorithms such as gradient descent. To better understand this phenomenon, we focus here on a ``mean-field'' model of infinitely deep and arbitrarily wide ResNet, parameterized by probability measures over the product set of layers and parameters and with constant marginal on the set of layers. Indeed, in the case of shallow neural networks, mean field models have proven to benefit from simplified loss-landscapes and good theoretical guarantees when trained with gradient flow for the Wasserstein metric on the set of probability measures. Motivated by this approach, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.12510</link><description>&lt;p&gt;
&#22270;&#20687;&#25805;&#20316;&#30340;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generalized Consistency Trajectory Models for Image Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#24191;&#20041;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;GCTMs&#65289;&#65292;&#33021;&#22815;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#23454;&#29616;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#26080;&#26465;&#20214;&#29983;&#25104;&#20197;&#21450;&#22270;&#20687;&#32534;&#36753;&#21644;&#24674;&#22797;&#31561;&#24212;&#29992;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#25193;&#25955;&#27169;&#22411;&#30340;&#25104;&#21151;&#22312;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#24615;&#36136;&#65306;&#25193;&#25955;&#23558;&#23558;&#22122;&#22768;&#21040;&#25968;&#25454;&#30340;&#22797;&#26434;&#26144;&#23556;&#36807;&#31243;&#20998;&#35299;&#20026;&#19968;&#31995;&#21015;&#31616;&#21333;&#30340;&#21435;&#22122;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#21435;&#22122;&#27493;&#39588;&#20013;&#27880;&#20837;&#24341;&#23548;&#39033;&#65292;&#25105;&#20204;&#33021;&#22815;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#36845;&#20195;&#36807;&#31243;&#20063;&#24120;&#24120;&#35745;&#31639;&#23494;&#38598;&#65292;&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#25968;&#21313;&#27425;&#29978;&#33267;&#25968;&#21315;&#27425;&#20989;&#25968;&#35780;&#20272;&#12290;&#34429;&#28982;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTMs&#65289;&#21487;&#20197;&#22312;&#27010;&#29575;&#27969;ODE&#65288;PFODE&#65289;&#19978;&#20219;&#24847;&#26102;&#38388;&#28857;&#20043;&#38388;&#36827;&#34892;&#36941;&#21382;&#65292;&#24182;&#19988;&#36890;&#36807;&#21333;&#27425;&#20989;&#25968;&#35780;&#20272;&#36827;&#34892;&#24471;&#20998;&#25512;&#23548;&#65292;&#20294;CTMs&#20165;&#20801;&#35768;&#20174;&#39640;&#26031;&#22122;&#22768;&#36716;&#25442;&#20026;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#24191;&#20041;CTMs&#65288;GCTMs&#65289;&#26469;&#21457;&#25381;CTMs&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#23454;&#29616;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#21644;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12510v1 Announce Type: cross  Abstract: Diffusion-based generative models excel in unconditional generation, as well as on applied tasks such as image editing and restoration. The success of diffusion models lies in the iterative nature of diffusion: diffusion breaks down the complex process of mapping noise to data into a sequence of simple denoising tasks. Moreover, we are able to exert fine-grained control over the generation process by injecting guidance terms into each denoising step. However, the iterative process is also computationally intensive, often taking from tens up to thousands of function evaluations. Although consistency trajectory models (CTMs) enable traversal between any time points along the probability flow ODE (PFODE) and score inference with a single function evaluation, CTMs only allow translation from Gaussian noise to data. Thus, this work aims to unlock the full potential of CTMs by proposing generalized CTMs (GCTMs), which translate between arbit
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#25972;&#21512;&#26377;&#26395;&#25552;&#21319;&#33258;&#21160;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#65292;&#23545;&#20132;&#36890;&#34892;&#19994;&#30340;&#23433;&#20840;&#21644;&#21019;&#26032;&#20855;&#26377;&#28508;&#22312;&#25512;&#21160;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.10559</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65306;&#25506;&#32034;&#20132;&#36890;&#21644;&#20154;&#24037;&#26234;&#33021;&#20132;&#21449;&#39046;&#22495;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Generative Models and Connected and Automated Vehicles: A Survey in Exploring the Intersection of Transportation and AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10559
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#19982;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#25972;&#21512;&#26377;&#26395;&#25552;&#21319;&#33258;&#21160;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#65292;&#23545;&#20132;&#36890;&#34892;&#19994;&#30340;&#23433;&#20840;&#21644;&#21019;&#26032;&#20855;&#26377;&#28508;&#22312;&#25512;&#21160;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25253;&#21578;&#35843;&#26597;&#20102;&#29983;&#25104;&#27169;&#22411;&#21644;&#32852;&#32593;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;CAVs&#65289;&#20004;&#31181;&#25512;&#21160;&#25216;&#26415;&#21644;&#20132;&#36890;&#36827;&#27493;&#30340;&#31361;&#30772;&#24615;&#21147;&#37327;&#30340;&#21382;&#21490;&#21644;&#24433;&#21709;&#12290;&#36890;&#36807;&#20851;&#27880;&#29983;&#25104;&#27169;&#22411;&#22312;CAVs&#32972;&#26223;&#19979;&#30340;&#24212;&#29992;&#65292;&#35813;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#36825;&#31181;&#25972;&#21512;&#22914;&#20309;&#25552;&#21319;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#39044;&#27979;&#24314;&#27169;&#12289;&#27169;&#25311;&#31934;&#24230;&#21644;&#20915;&#31574;&#27969;&#31243;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#22312;&#20132;&#36890;&#39046;&#22495;&#25972;&#21512;&#29983;&#25104;&#27169;&#22411;&#21644;CAV&#25216;&#26415;&#30340;&#30410;&#22788;&#21644;&#25361;&#25112;&#65292;&#26088;&#22312;&#24378;&#35843;&#21462;&#24471;&#30340;&#36827;&#23637;&#12289;&#21097;&#20313;&#30340;&#38556;&#30861;&#20197;&#21450;&#22312;&#23433;&#20840;&#21644;&#21019;&#26032;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10559v1 Announce Type: cross  Abstract: This report investigates the history and impact of Generative Models and Connected and Automated Vehicles (CAVs), two groundbreaking forces pushing progress in technology and transportation. By focusing on the application of generative models within the context of CAVs, the study aims to unravel how this integration could enhance predictive modeling, simulation accuracy, and decision-making processes in autonomous vehicles. This thesis discusses the benefits and challenges of integrating generative models and CAV technology in transportation. It aims to highlight the progress made, the remaining obstacles, and the potential for advancements in safety and innovation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#38450;&#24481;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defending Against Unforeseen Failure Modes with Latent Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#26102;&#22312;&#37096;&#32626;&#21518;&#20250;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#23613;&#31649;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20102;&#22823;&#37327;&#35786;&#26029;&#21644;&#35843;&#35797;&#65292;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#30001;&#20110;&#25915;&#20987;&#38754;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#27169;&#22411;&#20013;&#20943;&#23569;&#39118;&#38505;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32791;&#23613;&#22320;&#25628;&#32034;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#36755;&#20837;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32418;&#38431;&#21644;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36890;&#24120;&#29992;&#20110;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#21152;&#20581;&#22766;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36275;&#20197;&#36991;&#20813;&#35768;&#22810;&#19982;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25925;&#38556;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;&#28431;&#27934;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#24341;&#21457;&#36825;&#20123;&#28431;&#27934;&#30340;&#36755;&#20837;&#12290;LAT&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;LAT&#26469;&#28165;&#38500;&#24694;&#24847;&#36719;&#20214;&#24182;&#38450;&#24481;&#38024;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21382;&#21490;&#38477;&#38632;&#25968;&#25454;&#30340;&#39044;&#27979;&#27604;NWP&#39044;&#25253;&#21644;&#22522;&#20110;&#25345;&#32493;&#24615;&#30340;&#39044;&#27979;&#26356;&#20934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2402.07851</link><description>&lt;p&gt;
&#23558;&#21382;&#21490;&#38477;&#38632;&#25968;&#25454;&#19982;NCEP-NWP&#39044;&#25253;&#30456;&#27604;&#36739;&#65292;&#39044;&#27979;&#21360;&#24230;&#23395;&#39118;&#38477;&#38632;&#30340;&#25216;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Comparing skill of historical rainfall data based monsoon rainfall prediction in India with NCEP-NWP forecasts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07851
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21382;&#21490;&#38477;&#38632;&#25968;&#25454;&#30340;&#39044;&#27979;&#27604;NWP&#39044;&#25253;&#21644;&#22522;&#20110;&#25345;&#32493;&#24615;&#30340;&#39044;&#27979;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#21360;&#24230;&#30340;&#22235;&#20010;&#23395;&#39118;&#26376;&#20221;&#65292;&#22312;&#19968;&#22825;&#21644;&#19977;&#22825;&#20043;&#21069;&#39044;&#27979;&#38477;&#38632;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;IMD&#30340;&#21360;&#24230;&#21382;&#21490;&#26085;&#38477;&#27700;&#25968;&#25454;&#35757;&#32451;&#20102;&#31070;&#32463;&#32593;&#32476;&#65292;&#26102;&#38388;&#27573;&#20026;1901&#24180;&#33267;2022&#24180;&#65292;&#31354;&#38388;&#20998;&#36776;&#29575;&#20026;1&#176;&#215;1&#176;&#12290;&#36825;&#19982;&#26469;&#33258;NCEP&#65288;&#32654;&#22269;&#22269;&#23478;&#29615;&#22659;&#39044;&#25253;&#20013;&#24515;&#65289;&#30340;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#39044;&#27979;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#35813;&#25968;&#25454;&#21487;&#29992;&#20110;2011&#24180;&#33267;2022&#24180;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#20840;&#22269;&#33539;&#22260;&#20998;&#26512;&#65292;&#24182;&#20998;&#21035;&#20998;&#26512;&#20102;&#21360;&#24230;&#19968;&#20123;&#20154;&#21475;&#26368;&#22810;&#30340;&#22478;&#24066;&#12290;&#25105;&#20204;&#30340;&#32467;&#35770;&#26159;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#24212;&#29992;&#20110;&#21382;&#21490;&#38477;&#38632;&#25968;&#25454;&#30340;&#39044;&#27979;&#27604;NWP&#39044;&#25253;&#21644;&#22522;&#20110;&#25345;&#32493;&#24615;&#30340;&#39044;&#27979;&#26356;&#20934;&#30830;&#12290;&#24179;&#22343;&#32780;&#35328;&#65292;&#19982;&#25105;&#20204;&#30340;&#39044;&#27979;&#30456;&#27604;&#65292;NCEP-NWP&#27169;&#22411;&#30340;&#39044;&#27979;&#22312;&#21333;&#26085;&#39044;&#27979;&#20013;&#30340;&#35823;&#24046;&#32422;&#39640;&#20986;34%&#65292;&#22312;&#19977;&#22825;&#39044;&#27979;&#20013;&#30340;&#35823;&#24046;&#39640;&#20986;68%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this draft we consider the problem of forecasting rainfall across India during the four monsoon months, one day as well as three days in advance. We train neural networks using historical daily gridded precipitation data for India obtained from IMD for the time period $1901- 2022$, at a spatial resolution of $1^{\circ} \times 1^{\circ}$. This is compared with the numerical weather prediction (NWP) forecasts obtained from NCEP (National Centre for Environmental Prediction) available for the period 2011-2022. We conduct a detailed country wide analysis and separately analyze some of the most populated cities in India. Our conclusion is that forecasts obtained by applying deep learning to historical rainfall data are more accurate compared to NWP forecasts as well as predictions based on persistence. On average, compared to our predictions, forecasts from NCEP-NWP model have about 34% higher error for a single day prediction, and over 68% higher error for a three day prediction. Simila
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06919</link><description>&lt;p&gt;
TREET: &#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TREET: TRansfer Entropy Estimation via Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;TREET&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;Donsker-Vardhan&#34920;&#31034;&#27861;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#31283;&#23450;&#36807;&#31243;&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20272;&#35745;TE&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#24182;&#23637;&#31034;&#20102;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#20248;&#21270;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#21644;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#36755;&#29109;&#65288;TE&#65289;&#26159;&#20449;&#24687;&#35770;&#20013;&#25581;&#31034;&#36807;&#31243;&#20043;&#38388;&#20449;&#24687;&#27969;&#21160;&#26041;&#21521;&#30340;&#24230;&#37327;&#65292;&#23545;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TREET&#30340;&#22522;&#20110;Transformer&#30340;&#20256;&#36755;&#29109;&#20272;&#35745;&#26041;&#27861;&#65292;&#29992;&#20110;&#20272;&#35745;&#31283;&#23450;&#36807;&#31243;&#30340;TE&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;Donsker-Vardhan&#65288;DV&#65289;&#34920;&#31034;&#27861;&#23545;TE&#36827;&#34892;&#20272;&#35745;&#65292;&#24182;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#36827;&#34892;&#31070;&#32463;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;TREET&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#30740;&#31350;&#65292;&#24182;&#23558;&#20854;&#19982;&#29616;&#26377;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#20026;&#20102;&#22686;&#21152;&#20854;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21151;&#33021;&#34920;&#31034;&#24341;&#29702;&#30340;&#20272;&#35745;TE&#20248;&#21270;&#26041;&#26696;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#32852;&#21512;&#20248;&#21270;&#26041;&#26696;&#26469;&#20248;&#21270;&#20855;&#26377;&#35760;&#24518;&#24615;&#30340;&#36890;&#20449;&#36890;&#36947;&#23481;&#37327;&#65292;&#36825;&#26159;&#20449;&#24687;&#35770;&#20013;&#30340;&#19968;&#20010;&#20856;&#22411;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transfer entropy (TE) is a measurement in information theory that reveals the directional flow of information between processes, providing valuable insights for a wide range of real-world applications. This work proposes Transfer Entropy Estimation via Transformers (TREET), a novel transformer-based approach for estimating the TE for stationary processes. The proposed approach employs Donsker-Vardhan (DV) representation to TE and leverages the attention mechanism for the task of neural estimation. We propose a detailed theoretical and empirical study of the TREET, comparing it to existing methods. To increase its applicability, we design an estimated TE optimization scheme that is motivated by the functional representation lemma. Afterwards, we take advantage of the joint optimization scheme to optimize the capacity of communication channels with memory, which is a canonical optimization problem in information theory, and show the memory capabilities of our estimator. Finally, we apply
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.04161</link><description>&lt;p&gt;
&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35268;&#33539;&#20998;&#26512;&#26694;&#26550;&#65306;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Attention with Markov: A Framework for Principled Analysis of Transformers via Markov Chains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#65292;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#24182;&#21457;&#29616;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;Transformer&#22312;&#21253;&#25324;&#33258;&#28982;&#35821;&#35328;&#22312;&#20869;&#30340;&#22810;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#26159;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#27169;&#22411;&#22312;&#27492;&#36807;&#31243;&#20013;&#36890;&#36807;&#33258;&#22238;&#24402;&#30340;&#26041;&#24335;&#22312;&#22823;&#22411;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#35270;&#35282;&#65292;&#20801;&#35768;&#29702;&#35770;&#21644;&#31995;&#32479;&#23454;&#39564;&#26469;&#30740;&#31350;Transformer&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#12290;&#21463;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#25968;&#25454;&#24314;&#27169;&#20026;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#28304;&#65292;&#24182;&#21033;&#29992;&#36825;&#20010;&#26694;&#26550;&#31995;&#32479;&#22320;&#30740;&#31350;&#25968;&#25454;&#20998;&#24067;&#29305;&#24615;&#12289;Transformer&#26550;&#26500;&#12289;&#23398;&#21040;&#30340;&#20998;&#24067;&#21644;&#26368;&#32456;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#21051;&#30011;&#20102;&#21333;&#23618;Transformer&#30340;&#25439;&#22833;&#26223;&#35266;&#65292;&#24182;&#23637;&#31034;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#21644;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#23384;&#22312;&#65292;&#36825;&#21462;&#20915;&#20110;&#20855;&#20307;&#30340;&#25968;&#25454;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, attention-based transformers have achieved tremendous success across a variety of disciplines including natural languages. A key ingredient behind their success is the generative pretraining procedure, during which these models are trained on a large text corpus in an auto-regressive manner. To shed light on this phenomenon, we propose a new framework that allows both theory and systematic experiments to study the sequential modeling capabilities of transformers through the lens of Markov chains. Inspired by the Markovianity of natural languages, we model the data as a Markovian source and utilize this framework to systematically study the interplay between the data-distributional properties, the transformer architecture, the learnt distribution, and the final model performance. In particular, we theoretically characterize the loss landscape of single-layer transformers and show the existence of global minima and bad local minima contingent upon the specific data chara
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00849</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score-based Causal Representation Learning: Linear and General Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00849
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#38024;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#21644;&#23558;&#28508;&#22312;&#21464;&#37327;&#26144;&#23556;&#21040;&#35266;&#27979;&#21464;&#37327;&#30340;&#26410;&#30693;&#36716;&#21270;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#12290;&#30740;&#31350;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;&#12290;&#36825;&#31687;&#35770;&#25991;&#21516;&#26102;&#35752;&#35770;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#21487;&#35782;&#21035;&#24615;&#26159;&#25351;&#30830;&#23450;&#31639;&#27861;&#19981;&#30456;&#20851;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#24674;&#22797;&#30495;&#23454;&#30340;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#21644;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;&#23454;&#29616;&#24615;&#26159;&#25351;&#31639;&#27861;&#26041;&#38754;&#65292;&#35299;&#20915;&#35774;&#35745;&#31639;&#27861;&#26469;&#23454;&#29616;&#21487;&#35782;&#21035;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24471;&#20998;&#20989;&#25968;&#65288;&#21363;&#23494;&#24230;&#20989;&#25968;&#23545;&#25968;&#30340;&#26799;&#24230;&#65289;&#19982;CRL&#20043;&#38388;&#24314;&#31435;&#26032;&#32852;&#31995;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#24471;&#20998;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#31867;&#65292;&#30830;&#20445;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#32447;&#24615;&#36716;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;n&#20010;&#38543;&#26426;&#30828;&#24178;&#39044;&#19979;&#35813;&#36716;&#21270;&#30340;&#22240;&#26524;&#34920;&#31034;&#21487;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#38477;&#32500;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36710;&#36742;&#21160;&#21147;&#23398;&#20272;&#35745;&#20013;&#30340;&#21508;&#20010;&#21464;&#37327;&#29420;&#31435;&#35745;&#31639;&#21644;&#26657;&#20934;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#25511;&#21046;&#21462;&#21521;&#36710;&#36742;&#27169;&#22411;&#26367;&#25442;&#20026;&#36710;&#36742;&#27169;&#25311;&#22120;&#25110;&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#26469;&#23454;&#29616;&#65292;&#28982;&#21518;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#33410;&#28388;&#27874;&#22120;&#12290;</title><link>http://arxiv.org/abs/2401.10945</link><description>&lt;p&gt;
Twin-in-the-Loop Observers&#30340;&#33258;&#21160;&#38477;&#32500;
&lt;/p&gt;
&lt;p&gt;
Automatic dimensionality reduction of Twin-in-the-Loop Observers. (arXiv:2401.10945v1 [cs.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#38477;&#32500;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36710;&#36742;&#21160;&#21147;&#23398;&#20272;&#35745;&#20013;&#30340;&#21508;&#20010;&#21464;&#37327;&#29420;&#31435;&#35745;&#31639;&#21644;&#26657;&#20934;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#32463;&#20856;&#25511;&#21046;&#21462;&#21521;&#36710;&#36742;&#27169;&#22411;&#26367;&#25442;&#20026;&#36710;&#36742;&#27169;&#25311;&#22120;&#25110;&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#26469;&#23454;&#29616;&#65292;&#28982;&#21518;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#26469;&#35843;&#33410;&#28388;&#27874;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#36710;&#36742;&#21160;&#21147;&#23398;&#20272;&#35745;&#25216;&#26415;&#36890;&#24120;&#23384;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#32570;&#28857;&#65306;&#27599;&#20010;&#35201;&#20272;&#35745;&#30340;&#21464;&#37327;&#37117;&#26159;&#29992;&#29420;&#31435;&#30340;&#31616;&#21270;&#28388;&#27874;&#27169;&#22359;&#35745;&#31639;&#30340;&#12290;&#36825;&#20123;&#27169;&#22359;&#24182;&#34892;&#36816;&#34892;&#24182;&#38656;&#35201;&#21333;&#29420;&#26657;&#20934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;Twin-in-the-Loop(TiL)&#35266;&#27979;&#22120;&#26550;&#26500;&#65306;&#20272;&#35745;&#22120;&#20013;&#30340;&#32463;&#20856;&#31616;&#21270;&#25511;&#21046;&#21462;&#21521;&#36710;&#36742;&#27169;&#22411;&#34987;&#19968;&#20010;&#23436;&#25972;&#30340;&#36710;&#36742;&#27169;&#25311;&#22120;&#25110;&#25968;&#23383;&#21452;&#32990;&#32974;(DT)&#26367;&#20195;&#12290;DT&#30340;&#29366;&#24577;&#36890;&#36807;&#32447;&#24615;&#26102;&#19981;&#21464;&#30340;&#36755;&#20986;&#35823;&#24046;&#23450;&#24459;&#23454;&#26102;&#26657;&#27491;&#12290;&#30001;&#20110;&#27169;&#25311;&#22120;&#26159;&#19968;&#20010;&#40657;&#30418;&#23376;&#65292;&#27809;&#26377;&#26126;&#30830;&#30340;&#20998;&#26512;&#20844;&#24335;&#21487;&#29992;&#65292;&#22240;&#27492;&#26080;&#27861;&#20351;&#29992;&#32463;&#20856;&#30340;&#28388;&#27874;&#22120;&#35843;&#33410;&#25216;&#26415;&#12290;&#20986;&#20110;&#36825;&#20010;&#21407;&#22240;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#23558;&#29992;&#20110;&#35299;&#20915;&#19968;&#20010;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#38382;&#39064;&#26469;&#35843;&#33410;&#28388;&#27874;&#22120;&#12290;&#30001;&#20110;DT&#30340;&#22797;&#26434;&#24615;&#65292;&#20248;&#21270;&#38382;&#39064;&#26159;&#39640;&#32500;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#25214;&#21040;&#19968;&#31181;&#35843;&#33410;&#39640;&#22797;&#26434;&#24230;&#35266;&#27979;&#22120;&#30340;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
State-of-the-art vehicle dynamics estimation techniques usually share one common drawback: each variable to estimate is computed with an independent, simplified filtering module. These modules run in parallel and need to be calibrated separately. To solve this issue, a unified Twin-in-the-Loop (TiL) Observer architecture has recently been proposed: the classical simplified control-oriented vehicle model in the estimators is replaced by a full-fledged vehicle simulator, or digital twin (DT). The states of the DT are corrected in real time with a linear time invariant output error law. Since the simulator is a black-box, no explicit analytical formulation is available, hence classical filter tuning techniques cannot be used. Due to this reason, Bayesian Optimization will be used to solve a data-driven optimization problem to tune the filter. Due to the complexity of the DT, the optimization problem is high-dimensional. This paper aims to find a procedure to tune the high-complexity obser
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26041;&#27861;&#29992;&#20110;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;MFG&#38382;&#39064;&#36716;&#21270;&#20026;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65288;GNEP&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.06566</link><description>&lt;p&gt;
&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22343;&#22330;&#21338;&#24328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Maximum Causal Entropy Inverse Reinforcement Learning for Mean-Field Games. (arXiv:2401.06566v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#26041;&#27861;&#29992;&#20110;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#23558;MFG&#38382;&#39064;&#36716;&#21270;&#20026;&#24191;&#20041;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65288;GNEP&#65289;&#30340;&#26032;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#26080;&#38480;&#26102;&#38388;&#38388;&#38548;&#25240;&#25187;&#22238;&#25253;&#26368;&#20248;&#24615;&#20934;&#21017;&#19979;&#65292;&#38024;&#23545;&#31163;&#25955;&#26102;&#38388;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#30340;&#26368;&#22823;&#22240;&#26524;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;IRL&#65289;&#38382;&#39064;&#12290;&#20856;&#22411;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#31354;&#38388;&#26159;&#26377;&#38480;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20840;&#38754;&#22238;&#39038;&#20102;&#20851;&#20110;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#22312;&#26377;&#38480;&#21644;&#26080;&#38480;&#26102;&#38388;&#38388;&#38548;&#24773;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the maximum casual entropy Inverse Reinforcement Learning (IRL) problem for discrete-time mean-field games (MFGs) under an infinite-horizon discounted-reward optimality criterion. The state space of a typical agent is finite. Our approach begins with a comprehensive review of the maximum entropy IRL problem concerning deterministic and stochastic Markov decision processes (MDPs) in both finite and infinite-horizon scenarios. Subsequently, we formulate the maximum casual entropy IRL problem for MFGs - a non-convex optimization problem with respect to policies. Leveraging the linear programming formulation of MDPs, we restructure this IRL problem into a convex optimization problem and establish a gradient descent algorithm to compute the optimal solution with a rate of convergence. Finally, we present a new algorithm by formulating the MFG problem as a generalized Nash equilibrium problem (GNEP), which is capable of computing the mean-field equilibrium (MFE) f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#21046;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#24773;&#20917;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#33539;&#24335;&#12290;&#20351;&#29992;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23545;&#22810;&#26679;&#24615;&#20559;&#31227;&#21644;&#30456;&#20851;&#24615;&#20559;&#31227;&#36827;&#34892;&#20102;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#25945;&#23398;&#24615;&#33021;&#36739;&#24046;&#30340;&#29616;&#35937;&#12290;</title><link>http://arxiv.org/abs/2312.16242</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Revisiting Knowledge Distillation under Distribution Shift. (arXiv:2312.16242v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37325;&#26032;&#21046;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#22312;&#20998;&#24067;&#20559;&#31227;&#24773;&#20917;&#19979;&#37325;&#26032;&#23457;&#35270;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#33539;&#24335;&#12290;&#20351;&#29992;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23545;&#22810;&#26679;&#24615;&#20559;&#31227;&#21644;&#30456;&#20851;&#24615;&#20559;&#31227;&#36827;&#34892;&#20102;&#22522;&#20934;&#35780;&#20272;&#65292;&#24182;&#25581;&#31034;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#25945;&#23398;&#24615;&#33021;&#36739;&#24046;&#30340;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#33976;&#39311;&#23558;&#22823;&#22411;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23567;&#22411;&#27169;&#22411;&#65292;&#24182;&#22312;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#30693;&#35782;&#33976;&#39311;&#22312;&#38754;&#23545;&#20998;&#24067;&#20559;&#31227;&#26102;&#30340;&#26426;&#21046;&#12290;&#20998;&#24067;&#20559;&#31227;&#26159;&#25351;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#38454;&#27573;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#21457;&#29983;&#30340;&#28418;&#31227;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#20102;&#30693;&#35782;&#33976;&#39311;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20559;&#31227;&#24773;&#20917;&#19979;&#37325;&#26032;&#21046;&#23450;&#30446;&#26631;&#20989;&#25968;&#12290;&#22312;&#30495;&#23454;&#22330;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#32780;&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20004;&#31181;&#24120;&#35265;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#21363;&#22810;&#26679;&#24615;&#20559;&#31227;&#21644;&#30456;&#20851;&#24615;&#20559;&#31227;&#65292;&#36827;&#34892;&#30693;&#35782;&#33976;&#39311;&#30340;&#22522;&#20934;&#35780;&#20272;&#12290;&#35813;&#35780;&#20272;&#22522;&#20934;&#35206;&#30422;&#20102;&#26469;&#33258;&#31639;&#27861;&#12289;&#25968;&#25454;&#39537;&#21160;&#21644;&#20248;&#21270;&#35270;&#35282;&#30340;30&#22810;&#31181;&#26041;&#27861;&#65292;&#38024;&#23545;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#12290;&#24635;&#20307;&#19978;&#65292;&#25105;&#20204;&#23545;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#25945;&#23398;&#24615;&#33021;&#36739;&#24046;&#30340;&#26377;&#36259;&#35266;&#23519;&#32467;&#26524;&#65307;&#29305;&#21035;&#26159;&#65292;&#22797;&#26434;&#30340;&#31639;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21487;&#33021;&#23545;&#30693;&#35782;&#33976;&#39311;&#25928;&#26524;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge distillation transfers knowledge from large models into small models, and has recently made remarkable achievements. However, few studies has investigated the mechanism of knowledge distillation against distribution shift. Distribution shift refers to the data distribution drifts between training and testing phases. In this paper, we reconsider the paradigm of knowledge distillation by reformulating the objective function in shift situations. Under the real scenarios, we propose a unified and systematic framework to benchmark knowledge distillation against two general distributional shifts including diversity and correlation shift. The evaluation benchmark covers more than 30 methods from algorithmic, data-driven, and optimization perspectives for five benchmark datasets. Overall, we conduct extensive experiments on the student model. We reveal intriguing observations of poor teaching performance under distribution shifts; in particular, complex algorithms and data augmentati
&lt;/p&gt;</description></item><item><title>zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02554</link><description>&lt;p&gt;
zkFL: &#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
zkFL: Zero-Knowledge Proof-based Gradient Aggregation for Federated Learning. (arXiv:2310.02554v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02554
&lt;/p&gt;
&lt;p&gt;
zkFL&#26159;&#19968;&#31181;&#22522;&#20110;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#32858;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#27599;&#36718;&#30340;&#35777;&#26126;&#26469;&#35299;&#20915;&#21327;&#35843;&#32773;&#24694;&#24847;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20351;&#22810;&#20010;&#20998;&#25955;&#30340;&#23458;&#25143;&#31471;&#22312;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#32452;&#32455;&#19979;&#20849;&#21516;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#12290;&#20256;&#32479;&#30340;&#32852;&#37030;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#20381;&#36182;&#20110;&#23545;&#20013;&#22830;&#21327;&#35843;&#32773;&#30340;&#20449;&#20219;&#65292;&#23427;&#20197;&#20844;&#24179;&#35802;&#23454;&#30340;&#26041;&#24335;&#24418;&#25104;&#23458;&#25143;&#31471;&#30340;&#32676;&#20307;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#24694;&#24847;&#30340;&#21327;&#35843;&#32773;&#21487;&#33021;&#20250;&#25918;&#24323;&#24182;&#26367;&#25442;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#27169;&#22411;&#65292;&#25110;&#32773;&#21457;&#21160;&#34394;&#20551;&#23458;&#25143;&#31471;&#30340;&#32902;&#24847;&#25915;&#20987;&#12290;&#36825;&#31181;&#24694;&#24847;&#34892;&#20026;&#35753;&#21327;&#35843;&#32773;&#22312;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#25317;&#26377;&#26356;&#22810;&#25511;&#21046;&#23458;&#25143;&#31471;&#21644;&#20915;&#23450;&#26368;&#32456;&#35757;&#32451;&#32467;&#26524;&#30340;&#26435;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;zkFL&#65292;&#23427;&#21033;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;(ZKPs)&#26469;&#35299;&#20915;&#35757;&#32451;&#27169;&#22411;&#32858;&#21512;&#36807;&#31243;&#20013;&#30340;&#24694;&#24847;&#21327;&#35843;&#32773;&#38382;&#39064;&#12290;&#20026;&#20102;&#20445;&#35777;&#27491;&#30830;&#30340;&#32858;&#21512;&#32467;&#26524;&#65292;&#21327;&#35843;&#32773;&#38656;&#35201;&#27599;&#36718;&#25552;&#20379;&#19968;&#20010;&#35777;&#26126;&#12290;&#36825;&#20010;&#35777;&#26126;&#21487;&#20197;&#21521;&#23458;&#25143;&#31471;&#35777;&#26126;&#21327;&#35843;&#32773;&#24544;&#23454;&#25191;&#34892;&#39044;&#26399;&#34892;&#20026;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20445;&#25252;&#23458;&#25143;&#31471;&#38544;&#31169;&#21644;&#25968;&#25454;&#23433;&#20840;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#65292;&#24182;&#23545;zkFL&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm, which enables multiple and decentralized clients to collaboratively train a model under the orchestration of a central aggregator. Traditional FL solutions rely on the trust assumption of the centralized aggregator, which forms cohorts of clients in a fair and honest manner. However, a malicious aggregator, in reality, could abandon and replace the client's training models, or launch Sybil attacks to insert fake clients. Such malicious behaviors give the aggregator more power to control clients in the FL setting and determine the final training results. In this work, we introduce zkFL, which leverages zero-knowledge proofs (ZKPs) to tackle the issue of a malicious aggregator during the training model aggregation process. To guarantee the correct aggregation results, the aggregator needs to provide a proof per round. The proof can demonstrate to the clients that the aggregator executes the intended behavior faithfully. To further r
&lt;/p&gt;</description></item><item><title>RACR-MIL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#24369;&#30417;&#30563;&#30340;&#30382;&#32932;&#30284;&#20998;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#32454;&#31890;&#24230;&#30340;&#32959;&#30244;&#27880;&#37322;&#65292;&#36890;&#36807;&#22312;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#29926;&#29255;&#31456;&#33410;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#20999;&#29255;&#22270;&#20687;&#20998;&#37197;&#20998;&#32423;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#20351;&#29992;&#31354;&#38388;&#21644;&#35821;&#20041;&#25509;&#36817;&#24615;&#23450;&#20041;&#20999;&#29255;&#22270;&#20687;&#22270;&#20687;&#20197;&#32534;&#30721;&#32959;&#30244;&#21306;&#22495;&#30340;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#20351;&#29992;&#24207;&#25968;&#25490;&#21517;&#32422;&#26463;&#20445;&#35777;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24615;&#33021;</title><link>http://arxiv.org/abs/2308.15618</link><description>&lt;p&gt;
RACR-MIL&#65306;&#20351;&#29992;&#22522;&#20110;&#25490;&#21517;&#24863;&#30693;&#32972;&#26223;&#25512;&#29702;&#30340;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#24369;&#30417;&#30563;&#30340;&#30382;&#32932;&#30284;&#20998;&#32423;
&lt;/p&gt;
&lt;p&gt;
RACR-MIL: Weakly Supervised Skin Cancer Grading using Rank-Aware Contextual Reasoning on Whole Slide Images. (arXiv:2308.15618v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15618
&lt;/p&gt;
&lt;p&gt;
RACR-MIL&#26159;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#24369;&#30417;&#30563;&#30340;&#30382;&#32932;&#30284;&#20998;&#32423;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#25972;&#24352;&#20999;&#29255;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#65292;&#26080;&#38656;&#32454;&#31890;&#24230;&#30340;&#32959;&#30244;&#27880;&#37322;&#65292;&#36890;&#36807;&#22312;&#20999;&#29255;&#22270;&#20687;&#20013;&#30340;&#29926;&#29255;&#31456;&#33410;&#20013;&#20351;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#22810;&#23454;&#20363;&#23398;&#20064;&#65292;&#21487;&#20197;&#20026;&#20999;&#29255;&#22270;&#20687;&#20998;&#37197;&#20998;&#32423;&#12290;&#35813;&#26041;&#27861;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#20351;&#29992;&#31354;&#38388;&#21644;&#35821;&#20041;&#25509;&#36817;&#24615;&#23450;&#20041;&#20999;&#29255;&#22270;&#20687;&#22270;&#20687;&#20197;&#32534;&#30721;&#32959;&#30244;&#21306;&#22495;&#30340;&#23616;&#37096;&#21644;&#38750;&#23616;&#37096;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#20351;&#29992;&#24207;&#25968;&#25490;&#21517;&#32422;&#26463;&#20445;&#35777;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cutaneous squamous cell cancer (cSCC) is the second most common skin cancer in the US. It is diagnosed by manual multi-class tumor grading using a tissue whole slide image (WSI), which is subjective and suffers from inter-pathologist variability. We propose an automated weakly-supervised grading approach for cSCC WSIs that is trained using WSI-level grade and does not require fine-grained tumor annotations. The proposed model, RACR-MIL, transforms each WSI into a bag of tiled patches and leverages attention-based multiple-instance learning to assign a WSI-level grade. We propose three key innovations to address general as well as cSCC-specific challenges in tumor grading. First, we leverage spatial and semantic proximity to define a WSI graph that encodes both local and non-local dependencies between tumor regions and leverage graph attention convolution to derive contextual patch features. Second, we introduce a novel ordinal ranking constraint on the patch attention network to ensure
&lt;/p&gt;
&lt;p&gt;
Cutaneous squamous cell cancer (cSCC) is the second most common skin cancer in the US. It is diagnosed by manual multi-class tumor grading using a tissue whole slide image (WSI), which is subjective and suffers from inter-pathologist variability. We propose an automated weakly-supervised grading approach for cSCC WSIs that is trained using WSI-level grade and does not require fine-grained tumor annotations. The proposed model, RACR-MIL, transforms each WSI into a bag of tiled patches and leverages attention-based multiple-instance learning to assign a WSI-level grade. We propose three key innovations to address general as well as cSCC-specific challenges in tumor grading. First, we leverage spatial and semantic proximity to define a WSI graph that encodes both local and non-local dependencies between tumor regions and leverage graph attention convolution to derive contextual patch features. Second, we introduce a novel ordinal ranking constraint on the patch attention network to ensure
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14474</link><description>&lt;p&gt;
&#27827;&#24029;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limits to Reservoir Learning. (arXiv:2307.14474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#26469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#65288;IPC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#20449;&#21495;&#38598;&#21512;&#21040;&#23436;&#25972;&#20989;&#25968;&#22522;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;IPC&#26469;&#34913;&#37327;&#22122;&#22768;&#19979;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#65288;&#19968;&#31181;&#29305;&#27530;&#30340;&#24490;&#29615;&#32593;&#32476;&#65289;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;IPC&#22312;&#31995;&#32479;&#23610;&#23544;n&#19978;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;n&#20010;&#36755;&#20986;&#20449;&#21495;&#30340;$2^n$&#20010;&#21487;&#33021;&#30340;&#36880;&#28857;&#20056;&#31215;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36864;&#21270;&#24847;&#21619;&#30528;&#22312;&#20648;&#27700;&#24211;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20648;&#27700;&#24211;&#25152;&#34920;&#31034;&#30340;&#20989;&#25968;&#26063;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#19968;&#38598;&#21512;&#30340;$2^n$&#20010;&#20989;&#25968;&#22312;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-SSAD&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12033</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#22686;&#24378;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection. (arXiv:2306.12033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-SSAD&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#23427;&#20026;&#29616;&#23454;&#38382;&#39064;&#25552;&#20379;&#33258;&#20135;&#29983;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#25163;&#21160;&#26631;&#27880;&#24037;&#20316;&#12290;SSL&#23545;&#20110;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#23588;&#20854;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;&#24322;&#24120;&#36890;&#24120;&#19981;&#23384;&#22312;&#25110;&#38590;&#20197;&#33719;&#24471;&#12290;&#34429;&#28982;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#25991;&#29486;&#21364;&#26410;&#23558;&#25968;&#25454;&#22686;&#24378;&#35270;&#20026;&#36229;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#24378;&#36873;&#25321;&#23545;&#26816;&#27979;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ST-SSAD&#65288;&#33258;&#25105;&#35843;&#25972;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20851;&#20110;&#20005;&#26684;&#35843;&#25972;&#22686;&#24378;&#30340;SSAD&#30340;&#31532;&#19968;&#20010;&#31995;&#32479;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#31532;&#19968;&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#39564;&#35777;&#25439;&#22833;&#20989;&#25968;&#65292;&#37327;&#21270;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#19982;&#65288;&#26080;&#26631;&#31614;&#65289;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#22312;&#21407;&#21017;&#19978;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#36817;&#39640;&#25928;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20511;&#37492;&#30340;&#26080;&#30417;&#30563;&#39564;&#35777;&#26041;&#26696;&#21644;&#22686;&#24378;&#25968;&#25454;&#25628;&#32034;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;SSAD&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24418;&#24335;&#65292;&#23558;&#36731;&#37327;&#32423;&#25968;&#25454;&#22686;&#24378;&#25628;&#32034;&#22120;&#30340;&#31616;&#21333;&#38598;&#25104;&#12290;&#22312;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22686;&#24378;&#35843;&#25972;&#26041;&#27861;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#26368;&#26032;&#32467;&#26524;&#21487;&#20197;&#33719;&#24471;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#36817;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising paradigm that presents self-generated supervisory signals to real-world problems, bypassing the extensive manual labeling burden. SSL is especially attractive for unsupervised tasks such as anomaly detection, where labeled anomalies are often nonexistent and costly to obtain. While self-supervised anomaly detection (SSAD) has seen a recent surge of interest, the literature has failed to treat data augmentation as a hyperparameter. Meanwhile, recent works have reported that the choice of augmentation has significant impact on detection performance. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised Anomaly Detection), the first systematic approach to SSAD in regards to rigorously tuning augmentation. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between the augmented training data and the (unlabeled) test data. In principle we adop
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;(OIC)&#30340;&#36890;&#29992;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#30340;&#20048;&#35266;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30452;&#25509;&#36817;&#20284;&#19968;&#38454;&#20559;&#24046;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35299;&#20915;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26159;&#22312;&#20915;&#31574;&#36873;&#25321;&#26041;&#38754;&#30340;&#19968;&#20010;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2306.10081</link><description>&lt;p&gt;
&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;&#65306;&#21078;&#26512;&#21644;&#32416;&#27491;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#30340;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Optimizer's Information Criterion: Dissecting and Correcting Bias in Data-Driven Optimization. (arXiv:2306.10081v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;(OIC)&#30340;&#36890;&#29992;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#30340;&#20048;&#35266;&#20559;&#24046;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#30452;&#25509;&#36817;&#20284;&#19968;&#38454;&#20559;&#24046;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#35299;&#20915;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#26159;&#22312;&#20915;&#31574;&#36873;&#25321;&#26041;&#38754;&#30340;&#19968;&#20010;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#20248;&#21270;&#20013;&#65292;&#25152;&#24471;&#20915;&#31574;&#30340;&#26679;&#26412;&#34920;&#29616;&#36890;&#24120;&#23384;&#22312;&#30528;&#23545;&#30495;&#23454;&#34920;&#29616;&#30340;&#20048;&#35266;&#20559;&#24046;&#65292;&#36825;&#31181;&#29616;&#35937;&#36890;&#24120;&#34987;&#31216;&#20026;&#20248;&#21270;&#22120;&#30340;&#35781;&#21650;&#65292;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36807;&#25311;&#21512;&#23494;&#20999;&#30456;&#20851;&#12290;&#20256;&#32479;&#30340;&#32416;&#27491;&#36825;&#31181;&#20559;&#24046;&#30340;&#25216;&#26415;&#65292;&#22914;&#20132;&#21449;&#39564;&#35777;&#65292;&#38656;&#35201;&#21453;&#22797;&#35299;&#20915;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#22240;&#27492;&#35745;&#31639;&#20195;&#20215;&#24456;&#39640;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20559;&#24046;&#26657;&#27491;&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;&#25105;&#20204;&#31216;&#20043;&#20026;&#20248;&#21270;&#22120;&#30340;&#20449;&#24687;&#20934;&#21017;&#65288;OIC&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#30452;&#25509;&#36817;&#20284;&#19968;&#38454;&#20559;&#24046;&#65292;&#19981;&#38656;&#35201;&#35299;&#20915;&#20219;&#20309;&#39069;&#22806;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;OIC&#23558;&#33879;&#21517;&#30340;&#36196;&#27744;&#20449;&#24687;&#20934;&#21017;&#25512;&#24191;&#21040;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#20013;&#65292;&#20851;&#38190;&#26159;&#35780;&#20272;&#23458;&#35266;&#34920;&#29616;&#65292;&#19981;&#20165;&#28041;&#21450;&#27169;&#22411;&#25311;&#21512;&#65292;&#36824;&#28041;&#21450;&#20854;&#19982;&#19979;&#28216;&#20248;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#29992;&#20110;&#20915;&#31574;&#36873;&#25321;&#32780;&#19981;&#20165;&#20165;&#26159;&#27169;&#22411;&#36873;&#25321;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#38382;&#39064;&#30340;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven optimization, the sample performance of the obtained decision typically incurs an optimistic bias against the true performance, a phenomenon commonly known as the Optimizer's Curse and intimately related to overfitting in machine learning. Common techniques to correct this bias, such as cross-validation, require repeatedly solving additional optimization problems and are therefore computationally expensive. We develop a general bias correction approach, building on what we call Optimizer's Information Criterion (OIC), that directly approximates the first-order bias and does not require solving any additional optimization problems. Our OIC generalizes the celebrated Akaike Information Criterion to evaluate the objective performance in data-driven optimization, which crucially involves not only model fitting but also its interplay with the downstream optimization. As such it can be used for decision selection instead of only model selection. We apply our approach to a rang
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.10442</link><description>&lt;p&gt;
CBAGAN-RRT: &#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29992;&#20110;&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
CBAGAN-RRT: Convolutional Block Attention Generative Adversarial Network for Sampling-Based Path Planning. (arXiv:2305.10442v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19982;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#30456;&#27604;&#65292;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#26041;&#38754;&#37117;&#34920;&#29616;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#36335;&#24452;&#35268;&#21010;&#31639;&#27861;&#22312;&#33258;&#20027;&#26426;&#22120;&#20154;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#20294;&#26159;&#65292;&#22522;&#20110;RRT&#31639;&#27861;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#29983;&#25104;&#30340;&#21021;&#22987;&#36335;&#24452;&#19981;&#26159;&#26368;&#20248;&#30340;&#65292;&#32780;&#19988;&#25910;&#25947;&#36895;&#24230;&#36807;&#24930;&#65292;&#26080;&#27861;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21367;&#31215;&#22359;&#27880;&#24847;&#21147;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#22270;&#20687;&#22788;&#29702;&#23398;&#20064;&#31639;&#27861;&#65288;CBAGAN-RRT&#65289;&#65292;&#20197;&#35774;&#35745;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25214;&#21040;&#26356;&#20248;&#30340;&#26368;&#20339;&#36335;&#24452;&#65292;&#24182;&#25552;&#39640;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;GAN&#27169;&#22411;&#29983;&#25104;&#30340;&#36335;&#24452;&#27010;&#29575;&#20998;&#24067;&#29992;&#20110;&#24341;&#23548;RRT&#31639;&#27861;&#30340;&#37319;&#26679;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#30001; \cite {zhang2021generative} &#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#32593;&#32476;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22270;&#20687;&#36136;&#37327;&#29983;&#25104;&#25351;&#26631;&#65288;&#22914;IOU&#20998;&#25968;&#65292;Dice&#20998;&#25968;&#65289;&#21644;&#36335;&#24452;&#35268;&#21010;&#25351;&#26631;&#65288;&#22914;&#36335;&#24452;&#38271;&#24230;&#21644;&#25104;&#21151;&#29575;&#65289;&#26041;&#38754;&#22343;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sampling-based path planning algorithms play an important role in autonomous robotics. However, a common problem among the RRT-based algorithms is that the initial path generated is not optimal and the convergence is too slow to be used in real-world applications. In this paper, we propose a novel image-based learning algorithm (CBAGAN-RRT) using a Convolutional Block Attention Generative Adversarial Network with a combination of spatial and channel attention and a novel loss function to design the heuristics, find a better optimal path, and improve the convergence of the algorithm both concerning time and speed. The probability distribution of the paths generated from our GAN model is used to guide the sampling process for the RRT algorithm. We train and test our network on the dataset generated by \cite{zhang2021generative} and demonstrate that our algorithm outperforms the previous state-of-the-art algorithms using both the image quality generation metrics like IOU Score, Dice Score
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;</title><link>http://arxiv.org/abs/2303.09823</link><description>&lt;p&gt;
Transformers&#21644;Ensemble&#26041;&#27861;&#65306;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#30340;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
Transformers and Ensemble methods: A solution for Hate Speech Detection in Arabic languages. (arXiv:2303.09823v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09823
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#21644;Ensemble&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#38463;&#35821;&#24694;&#24847;&#35328;&#35770;&#30340;&#26816;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#20855;&#26377;&#26368;&#20339;&#25928;&#26524;&#65292;&#20854;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#20934;&#30830;&#29575;&#20026;0.86&#65292;F1&#20998;&#25968;&#20026;0.60&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#21442;&#21152;CERIST NLP&#25361;&#25112;&#36187;2022&#20013;&#24694;&#24847;&#35328;&#35770;&#26816;&#27979;&#20849;&#20139;&#20219;&#21153;&#30340;&#23454;&#39564;&#36807;&#31243;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;6&#20010;Transformer&#27169;&#22411;&#21450;&#20854;&#32452;&#21512;&#30340;&#24615;&#33021;&#65292;&#24182;&#20351;&#29992;&#20102;2&#31181;&#38598;&#25104;&#26041;&#27861;&#12290;&#22312;&#20116;&#25240;&#20132;&#21449;&#39564;&#35777;&#30340;&#35757;&#32451;&#38598;&#19978;&#65292;&#22522;&#20110;&#22810;&#25968;&#34920;&#20915;&#30340;&#38598;&#25104;&#26041;&#27861;&#33719;&#24471;&#20102;&#26368;&#20339;&#32467;&#26524;&#12290;&#22312;&#27979;&#35797;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#20026;F1&#20998;&#25968;&#20026;0.60&#65292;&#20934;&#30830;&#24615;&#20026;0.86&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes our participation in the shared task of hate speech detection, which is one of the subtasks of the CERIST NLP Challenge 2022. Our experiments evaluate the performance of six transformer models and their combination using 2 ensemble approaches. The best results on the training set, in a five-fold cross validation scenario, were obtained by using the ensemble approach based on the majority vote. The evaluation of this approach on the test set resulted in an F1-score of 0.60 and an Accuracy of 0.86.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#25299;&#23637;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.07409</link><description>&lt;p&gt;
&#12298;&#36229;&#36234;&#25209;&#22788;&#29702;&#20108;&#20803;&#20998;&#31867;&#30340;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#12299;
&lt;/p&gt;
&lt;p&gt;
Quantum Learning Theory Beyond Batch Binary Classification. (arXiv:2302.07409v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#30740;&#31350;&#37327;&#23376;&#23398;&#20064;&#29702;&#35770;&#25299;&#23637;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Arunachalam&#21644;de Wolf&#65288;2018&#65289;&#35777;&#26126;&#20102;&#22312;&#21487;&#23454;&#29616;&#21644;&#31946;&#28034;&#35774;&#32622;&#19979;&#65292;&#37327;&#23376;&#25209;&#22788;&#29702;&#23398;&#20064;&#24067;&#23572;&#20989;&#25968;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#19982;&#30456;&#24212;&#30340;&#32463;&#20856;&#26679;&#26412;&#22797;&#26434;&#24615;&#20855;&#26377;&#30456;&#21516;&#30340;&#24418;&#24335;&#21644;&#25968;&#37327;&#32423;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#26126;&#26174;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#25209;&#22788;&#29702;&#22810;&#31867;&#23398;&#20064;&#12289;&#22312;&#32447;&#24067;&#23572;&#23398;&#20064;&#21644;&#22312;&#32447;&#22810;&#31867;&#23398;&#20064;&#12290;&#23545;&#20110;&#25105;&#20204;&#30340;&#22312;&#32447;&#23398;&#20064;&#32467;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;Dawid&#21644;Tewari&#65288;2022&#65289;&#32463;&#20856;&#27169;&#22411;&#30340;&#33258;&#36866;&#24212;&#23545;&#25163;&#21464;&#20307;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#65288;&#25454;&#25105;&#20204;&#25152;&#30693;&#65289;&#20855;&#26377;&#37327;&#23376;&#31034;&#20363;&#30340;&#22312;&#32447;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Arunachalam and de Wolf (2018) showed that the sample complexity of quantum batch learning of boolean functions, in the realizable and agnostic settings, has the same form and order as the corresponding classical sample complexities. In this paper, we extend this, ostensibly surprising, message to batch multiclass learning, online boolean learning, and online multiclass learning. For our online learning results, we first consider an adaptive adversary variant of the classical model of Dawid and Tewari (2022). Then, we introduce the first (to the best of our knowledge) model of online learning with quantum examples.
&lt;/p&gt;</description></item></channel></rss>