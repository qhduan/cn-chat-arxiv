<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;NN TSC&#65289;&#39044;&#27979;&#20891;&#20107;&#32972;&#26223;&#19979;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#25112;&#26415;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;NN TSC&#22312;&#24555;&#36895;&#25512;&#26029;&#25915;&#20987;&#32676;&#20307;&#24773;&#25253;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19572</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;&#32676;&#20307;&#29305;&#24615;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Swarm Characteristics Classification Using Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;NN TSC&#65289;&#39044;&#27979;&#20891;&#20107;&#32972;&#26223;&#19979;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#25112;&#26415;&#65292;&#20197;&#21450;&#23637;&#31034;&#20102;NN TSC&#22312;&#24555;&#36895;&#25512;&#26029;&#25915;&#20987;&#32676;&#20307;&#24773;&#25253;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#29305;&#24615;&#23545;&#20110;&#22269;&#38450;&#21644;&#23433;&#20840;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#30417;&#30563;&#31070;&#32463;&#32593;&#32476;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65288;NN TSC&#65289;&#26469;&#39044;&#27979;&#20891;&#20107;&#29615;&#22659;&#20013;&#32676;&#20307;&#33258;&#20027;&#20307;&#30340;&#20851;&#38190;&#23646;&#24615;&#21644;&#25112;&#26415;&#30340;&#30740;&#31350;&#12290;&#20855;&#20307;&#22320;&#65292;NN TSC&#34987;&#24212;&#29992;&#20110;&#25512;&#26029;&#20004;&#20010;&#20108;&#36827;&#21046;&#23646;&#24615; - &#36890;&#20449;&#21644;&#27604;&#20363;&#23548;&#33322; - &#36825;&#20004;&#32773;&#32467;&#21512;&#23450;&#20041;&#20102;&#22235;&#31181;&#20114;&#26021;&#30340;&#32676;&#20307;&#25112;&#26415;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23545;&#20110;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32676;&#20307;&#20998;&#31867;&#23384;&#22312;&#19968;&#23450;&#30340;&#31354;&#30333;&#65292;&#24182;&#23637;&#31034;&#20102;NN TSC&#22312;&#24555;&#36895;&#25512;&#26029;&#26377;&#20851;&#25915;&#20987;&#32676;&#20307;&#24773;&#25253;&#20197;&#25351;&#23548;&#21453;&#21046;&#21160;&#20316;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#27169;&#25311;&#30340;&#32676;&#20307;&#23545;&#25112;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;NN TSC&#22312;&#35266;&#23519;&#31383;&#21475;&#35201;&#27714;&#12289;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#23545;&#32676;&#20307;&#35268;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#20851;&#38190;&#21457;&#29616;&#26174;&#31034;NN&#33021;&#22815;&#20351;&#29992;&#36739;&#30701;&#30340;&#35266;&#23519;&#31383;&#21475;&#20197;97%&#30340;&#20934;&#30830;&#29575;&#39044;&#27979;&#32676;&#20307;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19572v1 Announce Type: new  Abstract: Understanding the characteristics of swarming autonomous agents is critical for defense and security applications. This article presents a study on using supervised neural network time series classification (NN TSC) to predict key attributes and tactics of swarming autonomous agents for military contexts. Specifically, NN TSC is applied to infer two binary attributes - communication and proportional navigation - which combine to define four mutually exclusive swarm tactics. We identify a gap in literature on using NNs for swarm classification and demonstrate the effectiveness of NN TSC in rapidly deducing intelligence about attacking swarms to inform counter-maneuvers. Through simulated swarm-vs-swarm engagements, we evaluate NN TSC performance in terms of observation window requirements, noise robustness, and scalability to swarm size. Key findings show NNs can predict swarm behaviors with 97% accuracy using short observation windows of
&lt;/p&gt;</description></item><item><title>&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15365</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#22270;&#20687;&#27700;&#21360;&#30340;&#36716;&#31227;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
A Transfer Attack to Image Watermarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15365
&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#39046;&#22495;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#30340;&#24773;&#20917;&#19979;&#65292;&#27700;&#21360;&#22522;&#30784;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#20063;&#26080;&#27861;&#25269;&#25239;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27700;&#21360;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#39046;&#22495;&#65292;&#29992;&#20110;&#26816;&#27979;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#25991;&#29486;&#20013;&#23545;&#36825;&#31181;&#22522;&#20110;&#27700;&#21360;&#30340;&#26816;&#27979;&#22120;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#29615;&#22659;&#19979;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#26377;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#31283;&#20581;&#24615;&#21364;&#30693;&#20043;&#29978;&#23569;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22810;&#39033;&#30740;&#31350;&#22768;&#31216;&#22270;&#20687;&#27700;&#21360;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#26159;&#31283;&#20581;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36716;&#31227;&#23545;&#25239;&#25915;&#20987;&#26469;&#38024;&#23545;&#26080;&#30418;&#29615;&#22659;&#19979;&#30340;&#22270;&#20687;&#27700;&#21360;&#12290;&#25105;&#20204;&#30340;&#36716;&#31227;&#25915;&#20987;&#21521;&#24102;&#27700;&#21360;&#30340;&#22270;&#20687;&#28155;&#21152;&#24494;&#25200;&#65292;&#20197;&#36530;&#36991;&#34987;&#25915;&#20987;&#32773;&#35757;&#32451;&#30340;&#22810;&#20010;&#26367;&#20195;&#27700;&#21360;&#27169;&#22411;&#65292;&#24182;&#19988;&#32463;&#36807;&#25200;&#21160;&#30340;&#24102;&#27700;&#21360;&#22270;&#20687;&#20063;&#33021;&#36530;&#36991;&#30446;&#26631;&#27700;&#21360;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#29702;&#35770;&#19978;&#21644;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;&#65292;&#22522;&#20110;&#27700;&#21360;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#22270;&#20687;&#26816;&#27979;&#22120;&#21363;&#20351;&#25915;&#20987;&#32773;&#27809;&#26377;&#35775;&#38382;&#27700;&#21360;&#27169;&#22411;&#25110;&#26816;&#27979;API&#65292;&#20063;&#19981;&#20855;&#26377;&#23545;&#25239;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15365v1 Announce Type: cross  Abstract: Watermark has been widely deployed by industry to detect AI-generated images. The robustness of such watermark-based detector against evasion attacks in the white-box and black-box settings is well understood in the literature. However, the robustness in the no-box setting is much less understood. In particular, multiple studies claimed that image watermark is robust in such setting. In this work, we propose a new transfer evasion attack to image watermark in the no-box setting. Our transfer attack adds a perturbation to a watermarked image to evade multiple surrogate watermarking models trained by the attacker itself, and the perturbed watermarked image also evades the target watermarking model. Our major contribution is to show that, both theoretically and empirically, watermark-based AI-generated image detector is not robust to evasion attacks even if the attacker does not have access to the watermarking model nor the detection API.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861; Describe-and-Dissect&#65288;DnD&#65289;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#26174;&#31034;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.13771</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#35270;&#35273;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#20803;&#65306;&#25551;&#36848;&#19982;&#35299;&#21078;
&lt;/p&gt;
&lt;p&gt;
Describe-and-Dissect: Interpreting Neurons in Vision Networks with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13771
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861; Describe-and-Dissect&#65288;DnD&#65289;&#65292;&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#36873;&#25321;&#65292;&#24182;&#19988;&#36890;&#36807;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#26174;&#31034;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Describe-and-Dissect&#65288;DnD&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#35270;&#35273;&#32593;&#32476;&#20013;&#38544;&#34255;&#31070;&#32463;&#20803;&#30340;&#20316;&#29992;&#12290;DnD&#21033;&#29992;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#29983;&#25104;&#22797;&#26434;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#26080;&#38656;&#26631;&#35760;&#30340;&#35757;&#32451;&#25968;&#25454;&#25110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;DnD&#26159;&#26080;&#38656;&#35757;&#32451;&#30340;&#65292;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#19981;&#35757;&#32451;&#20219;&#20309;&#26032;&#27169;&#22411;&#65292;&#26410;&#26469;&#21487;&#20197;&#36731;&#26494;&#21033;&#29992;&#26356;&#24378;&#22823;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#34920;&#26126;DnD&#36890;&#36807;&#25552;&#20379;&#26356;&#39640;&#36136;&#37327;&#30340;&#31070;&#32463;&#20803;&#25551;&#36848;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#20379;&#26368;&#39640;&#36136;&#37327;&#30340;&#26631;&#31614;&#65292;&#24182;&#19988;&#34987;&#36873;&#20026;&#31070;&#32463;&#20803;&#30340;&#26368;&#20339;&#35299;&#37322;&#30340;&#27010;&#29575;&#26159;&#26368;&#20339;&#22522;&#32447;&#30340;&#20004;&#20493;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13771v1 Announce Type: cross  Abstract: In this paper, we propose Describe-and-Dissect (DnD), a novel method to describe the roles of hidden neurons in vision networks. DnD utilizes recent advancements in multimodal deep learning to produce complex natural language descriptions, without the need for labeled training data or a predefined set of concepts to choose from. Additionally, DnD is training-free, meaning we don't train any new models and can easily leverage more capable general purpose models in the future. We have conducted extensive qualitative and quantitative analysis to show that DnD outperforms prior work by providing higher quality neuron descriptions. Specifically, our method on average provides the highest quality labels and is more than 2 times as likely to be selected as the best explanation for a neuron than the best baseline.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;</title><link>https://arxiv.org/abs/2403.07904</link><description>&lt;p&gt;
&#27491;&#35270;&#30417;&#31649;&#31354;&#30333;&#65306;&#36890;&#36807;&#32435;&#20837;&#31038;&#20250;&#20844;&#27665;&#25171;&#36896;&#36229;&#36234;AIA&#30340;&#27431;&#30431;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Addressing the Regulatory Gap: Moving Towards an EU AI Audit Ecosystem Beyond the AIA by Including Civil Society
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#27954;&#31435;&#27861;&#26426;&#26500;&#25552;&#20986;&#20102;&#25968;&#23383;&#26381;&#21153;&#27861;&#26696;&#65288;DSA&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#65288;AIA&#65289;&#26469;&#35268;&#33539;&#24179;&#21488;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20135;&#21697;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;&#31532;&#19977;&#26041;&#23457;&#35745;&#22312;&#36825;&#20004;&#39033;&#27861;&#24459;&#20013;&#30340;&#22320;&#20301;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#20379;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#36890;&#36807;&#32771;&#34385;&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#20013;&#31532;&#19977;&#26041;&#23457;&#35745;&#21644;&#31532;&#19977;&#26041;&#25968;&#25454;&#35775;&#38382;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#30417;&#31649;&#31354;&#30333;&#65292;&#21363;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#27809;&#26377;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23450;&#20041;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#12290;&#65288;2&#65289;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#38459;&#30861;&#20102;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#30340;&#24314;&#31435;&#12290;&#65288;3&#65289;&#24378;&#35843;&#30740;&#31350;&#21644;&#31038;&#20250;&#20844;&#27665;&#30340;&#31532;&#19977;&#26041;&#23457;&#35745;&#24517;&#39035;&#25104;&#20026;&#35813;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#35201;&#27714;AIA&#21253;&#25324;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07904v1 Announce Type: cross  Abstract: The European legislature has proposed the Digital Services Act (DSA) and Artificial Intelligence Act (AIA) to regulate platforms and Artificial Intelligence (AI) products. We review to what extent third-party audits are part of both laws and to what extent access to models and data is provided. By considering the value of third-party audits and third-party data access in an audit ecosystem, we identify a regulatory gap in that the Artificial Intelligence Act does not provide access to data for researchers and civil society. Our contributions to the literature include: (1) Defining an AI audit ecosystem that incorporates compliance and oversight. (2) Highlighting a regulatory gap within the DSA and AIA regulatory framework, preventing the establishment of an AI audit ecosystem. (3) Emphasizing that third-party audits by research and civil society must be part of that ecosystem and demand that the AIA include data and model access for ce
&lt;/p&gt;</description></item><item><title>Stacking&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#37322;&#65292;&#21363;&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.04978</link><description>&lt;p&gt;
Stacking&#20316;&#20026;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stacking as Accelerated Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04978
&lt;/p&gt;
&lt;p&gt;
Stacking&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#35299;&#37322;&#65292;&#21363;&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#24418;&#24335;&#65292;&#24182;&#35777;&#26126;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Stacking&#26159;&#19968;&#31181;&#21551;&#21457;&#24335;&#25216;&#26415;&#65292;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#23618;&#25968;&#24182;&#36890;&#36807;&#20174;&#26087;&#23618;&#22797;&#21046;&#21442;&#25968;&#26469;&#21021;&#22987;&#21270;&#26032;&#23618;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#27531;&#24046;&#32593;&#32476;&#65292;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#29575;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#20110;Stacking&#26377;&#25928;&#24615;&#30340;&#29702;&#35770;&#35299;&#37322;&#65306;&#21363;&#65292;Stacking&#23454;&#29616;&#20102;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#19979;&#38477;&#30340;&#19968;&#31181;&#24418;&#24335;&#12290;&#35813;&#29702;&#35770;&#36824;&#28085;&#30422;&#20102;&#35832;&#22914;&#25552;&#21319;&#26041;&#27861;&#20013;&#26500;&#24314;&#30340;&#21152;&#27861;&#38598;&#25104;&#31561;&#26356;&#31616;&#21333;&#30340;&#27169;&#22411;&#65292;&#24182;&#20026;&#27599;&#19968;&#36718;&#25552;&#21319;&#36807;&#31243;&#20013;&#21021;&#22987;&#21270;&#26032;&#20998;&#31867;&#22120;&#30340;&#31867;&#20284;&#24191;&#27867;&#20351;&#29992;&#30340;&#23454;&#29992;&#21551;&#21457;&#24335;&#25552;&#20379;&#20102;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#23545;&#20110;&#26576;&#20123;&#28145;&#24230;&#32447;&#24615;&#27531;&#24046;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;Nesterov&#30340;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#19968;&#20010;&#26032;&#30340;&#28508;&#33021;&#20989;&#25968;&#20998;&#26512;&#65292;Stacking&#30830;&#23454;&#25552;&#20379;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#20174;&#32780;&#20801;&#35768;&#26356;&#26032;&#20013;&#30340;&#35823;&#24046;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#27010;&#24565;&#39564;&#35777;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04978v1 Announce Type: new  Abstract: Stacking, a heuristic technique for training deep residual networks by progressively increasing the number of layers and initializing new layers by copying parameters from older layers, has proven quite successful in improving the efficiency of training deep neural networks. In this paper, we propose a theoretical explanation for the efficacy of stacking: viz., stacking implements a form of Nesterov's accelerated gradient descent. The theory also covers simpler models such as the additive ensembles constructed in boosting methods, and provides an explanation for a similar widely-used practical heuristic for initializing the new classifier in each round of boosting. We also prove that for certain deep linear residual networks, stacking does provide accelerated training, via a new potential function analysis of the Nesterov's accelerated gradient method which allows errors in updates. We conduct proof-of-concept experiments to validate our
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15106</link><description>&lt;p&gt;
&#22522;&#20110;&#37319;&#26679;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#20998;&#24067;&#24335;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Sampling-based Distributed Training with Message Passing Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15106
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#26102;&#30340;&#25193;&#23637;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22495;&#20998;&#35299;&#30340;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNN&#65289;&#20998;&#24067;&#24335;&#35757;&#32451;&#21644;&#25512;&#26029;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35299;&#20915;&#38543;&#30528;&#33410;&#28857;&#25968;&#37327;&#22686;&#21152;&#32780;&#25193;&#23637;&#36793;&#32536;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;Nystrom-&#36817;&#20284;&#37319;&#26679;&#25216;&#26415;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;DS-MPNN&#65288;&#20854;&#20013;D&#21644;S&#20998;&#21035;&#20195;&#34920;&#20998;&#24067;&#24335;&#21644;&#37319;&#26679;&#65289;&#65292;&#33021;&#22815;&#25193;&#23637;&#21040;$O(10^5)$&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#26696;&#20363;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#37319;&#26679;&#21644;&#20998;&#24067;&#24335;&#35757;&#32451;&#26041;&#27861;&#65306;&#65288;a&#65289;Darcy&#27969;&#25968;&#25454;&#38598;&#21644;&#65288;b&#65289;2-D&#26426;&#32764;&#30340;&#31283;&#24577;RANS&#27169;&#25311;&#65292;&#25552;&#20379;&#20102;&#19982;&#21333;GPU&#23454;&#29616;&#21644;&#22522;&#20110;&#33410;&#28857;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCNs&#65289;&#30340;&#27604;&#36739;&#12290;DS-MPNN&#27169;&#22411;&#34920;&#29616;&#20986;&#19982;&#21333;GPU&#23454;&#29616;&#30456;&#24403;&#30340;&#20934;&#30830;&#24615;&#65292;&#33021;&#22815;&#23481;&#32435;&#27604;&#21333;&#20010;GPU&#23454;&#29616;&#26356;&#22810;&#25968;&#37327;&#30340;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15106v1 Announce Type: new  Abstract: In this study, we introduce a domain-decomposition-based distributed training and inference approach for message-passing neural networks (MPNN). Our objective is to address the challenge of scaling edge-based graph neural networks as the number of nodes increases. Through our distributed training approach, coupled with Nystr\"om-approximation sampling techniques, we present a scalable graph neural network, referred to as DS-MPNN (D and S standing for distributed and sampled, respectively), capable of scaling up to $O(10^5)$ nodes. We validate our sampling and distributed training approach on two cases: (a) a Darcy flow dataset and (b) steady RANS simulations of 2-D airfoils, providing comparisons with both single-GPU implementation and node-based graph convolution networks (GCNs). The DS-MPNN model demonstrates comparable accuracy to single-GPU implementation, can accommodate a significantly larger number of nodes compared to the single-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12812</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#29992;&#20110;&#22312;&#32447;&#20010;&#24615;&#21270;&#22343;&#20540;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Decentralized Algorithms for Online Personalized Mean Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20998;&#25955;&#31639;&#27861;&#26694;&#26550;&#65292;&#20351;&#20195;&#29702;&#33021;&#22815;&#33258;&#32452;&#32455;&#25104;&#22270;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#21327;&#21516;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#23458;&#25143;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20195;&#29702;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#30452;&#25509;&#23398;&#20064;&#27169;&#22411;&#12290;&#19982;&#20854;&#20182;&#20195;&#29702;&#21512;&#20316;&#21487;&#33021;&#26377;&#25152;&#24110;&#21161;&#65292;&#20294;&#24403;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#26102;&#65292;&#20250;&#24341;&#20837;&#20559;&#24046;-&#26041;&#24046;&#26435;&#34913;&#12290;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#27599;&#20010;&#20195;&#29702;&#22312;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#20998;&#24067;&#30340;&#23458;&#25143;&#65292;&#36825;&#20010;&#38382;&#39064;&#20027;&#35201;&#20173;&#26410;&#35299;&#20915;&#12290;&#26412;&#30740;&#31350;&#30528;&#30524;&#20110;&#19968;&#20010;&#31616;&#21270;&#29256;&#26412;&#30340;&#26222;&#36941;&#38382;&#39064;&#65292;&#21363;&#27599;&#20010;&#20195;&#29702;&#38543;&#26102;&#38388;&#20174;&#23454;&#20540;&#20998;&#24067;&#20013;&#25910;&#38598;&#26679;&#26412;&#26469;&#20272;&#35745;&#20854;&#22343;&#20540;&#12290;&#29616;&#26377;&#31639;&#27861;&#38754;&#20020;&#30528;&#19981;&#20999;&#23454;&#38469;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#65288;&#19982;&#20195;&#29702;&#25968;&#37327;A&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#21487;&#25193;&#23637;&#24615;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20195;&#29702;&#33258;&#32452;&#32455;&#25104;&#19968;&#20010;&#22270;&#65292;&#20351;&#24471;&#27599;&#20010;&#20195;&#29702;&#21482;&#33021;&#19982;&#36873;&#23450;&#25968;&#37327;&#30340;&#23545;&#31561;&#20307;r&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#21327;&#20316;&#22343;&#20540;&#20272;&#35745;&#31639;&#27861;&#65306;&#19968;&#31181;&#28789;&#24863;&#26469;&#28304;&#20110;&#20449;&#24565;&#20256;&#25773;&#65292;&#21478;&#19968;&#31181;&#37319;&#29992;&#22522;&#20110;&#20849;&#35782;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12812v1 Announce Type: new  Abstract: In numerous settings, agents lack sufficient data to directly learn a model. Collaborating with other agents may help, but it introduces a bias-variance trade-off, when local data distributions differ. A key challenge is for each agent to identify clients with similar distributions while learning the model, a problem that remains largely unresolved. This study focuses on a simplified version of the overarching problem, where each agent collects samples from a real-valued distribution over time to estimate its mean. Existing algorithms face impractical space and time complexities (quadratic in the number of agents A). To address scalability challenges, we propose a framework where agents self-organize into a graph, allowing each agent to communicate with only a selected number of peers r. We introduce two collaborative mean estimation algorithms: one draws inspiration from belief propagation, while the other employs a consensus-based appr
&lt;/p&gt;</description></item><item><title>&#33258;&#33976;&#39311;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#25198;&#28436;&#30528;&#26631;&#31614;&#24179;&#22343;&#21270;&#30340;&#35282;&#33394;&#65292;&#26377;&#21161;&#20110;&#27169;&#22411;&#20851;&#27880;&#19982;&#29305;&#23450;&#23454;&#20363;&#30456;&#20851;&#30340;&#29305;&#24449;&#31751;&#20197;&#39044;&#27979;&#26631;&#31614;&#65292;&#20294;&#38543;&#30528;&#33976;&#39311;&#36718;&#27425;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#24773;&#26223;&#19979;&#33258;&#33976;&#39311;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#25214;&#21040;&#20102;&#23454;&#29616;100%&#20998;&#31867;&#20934;&#30830;&#29575;&#25152;&#38656;&#30340;&#26368;&#23567;&#33976;&#39311;&#36718;&#27425;&#12290;</title><link>https://arxiv.org/abs/2402.10482</link><description>&lt;p&gt;
&#29702;&#35299;&#24102;&#26377;&#26631;&#31614;&#22122;&#38899;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;&#33258;&#33976;&#39311;&#21644;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Self-Distillation and Partial Label Learning in Multi-Class Classification with Label Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10482
&lt;/p&gt;
&lt;p&gt;
&#33258;&#33976;&#39311;&#22312;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#25198;&#28436;&#30528;&#26631;&#31614;&#24179;&#22343;&#21270;&#30340;&#35282;&#33394;&#65292;&#26377;&#21161;&#20110;&#27169;&#22411;&#20851;&#27880;&#19982;&#29305;&#23450;&#23454;&#20363;&#30456;&#20851;&#30340;&#29305;&#24449;&#31751;&#20197;&#39044;&#27979;&#26631;&#31614;&#65292;&#20294;&#38543;&#30528;&#33976;&#39311;&#36718;&#27425;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#38477;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#26631;&#31614;&#22122;&#22768;&#24773;&#26223;&#19979;&#33258;&#33976;&#39311;&#34987;&#35777;&#26126;&#26159;&#26377;&#25928;&#30340;&#65292;&#25214;&#21040;&#20102;&#23454;&#29616;100%&#20998;&#31867;&#20934;&#30830;&#29575;&#25152;&#38656;&#30340;&#26368;&#23567;&#33976;&#39311;&#36718;&#27425;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#33976;&#39311;&#65288;SD&#65289;&#26159;&#20351;&#29992;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20986;&#35757;&#32451;&#23398;&#29983;&#27169;&#22411;&#30340;&#36807;&#31243;&#65292;&#20004;&#20010;&#27169;&#22411;&#20849;&#20139;&#30456;&#21516;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20174;&#29702;&#35770;&#19978;&#32771;&#23519;&#20102;&#20351;&#29992;&#20132;&#21449;&#29109;&#25439;&#22833;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#20013;&#30340;SD&#65292;&#25506;&#32034;&#20102;&#22810;&#36718;SD&#21644;&#20855;&#26377;&#31934;&#28860;&#25945;&#24072;&#36755;&#20986;&#30340;SD&#65292;&#36825;&#20123;&#28789;&#24863;&#26469;&#33258;&#37096;&#20998;&#26631;&#31614;&#23398;&#20064;&#65288;PLL&#65289;&#12290;&#36890;&#36807;&#25512;&#23548;&#23398;&#29983;&#27169;&#22411;&#36755;&#20986;&#30340;&#23553;&#38381;&#24418;&#24335;&#35299;&#65292;&#25105;&#20204;&#21457;&#29616;SD&#26412;&#36136;&#19978;&#26159;&#22312;&#20855;&#26377;&#39640;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#23454;&#20363;&#20043;&#38388;&#36827;&#34892;&#26631;&#31614;&#24179;&#22343;&#12290;&#26368;&#21021;&#26377;&#30410;&#30340;&#24179;&#22343;&#21270;&#26377;&#21161;&#20110;&#27169;&#22411;&#19987;&#27880;&#20110;&#19982;&#32473;&#23450;&#23454;&#20363;&#30456;&#20851;&#32852;&#30340;&#29305;&#24449;&#31751;&#20197;&#39044;&#27979;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#33976;&#39311;&#36718;&#27425;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;SD&#22312;&#26631;&#31614;&#22122;&#22768;&#24773;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#30830;&#23450;&#23454;&#29616;100%&#20998;&#31867;&#20934;&#30830;&#29575;&#25152;&#38656;&#30340;&#26631;&#31614;&#25439;&#22351;&#26465;&#20214;&#21644;&#26368;&#23567;&#33976;&#39311;&#36718;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10482v1 Announce Type: new  Abstract: Self-distillation (SD) is the process of training a student model using the outputs of a teacher model, with both models sharing the same architecture. Our study theoretically examines SD in multi-class classification with cross-entropy loss, exploring both multi-round SD and SD with refined teacher outputs, inspired by partial label learning (PLL). By deriving a closed-form solution for the student model's outputs, we discover that SD essentially functions as label averaging among instances with high feature correlations. Initially beneficial, this averaging helps the model focus on feature clusters correlated with a given instance for predicting the label. However, it leads to diminishing performance with increasing distillation rounds. Additionally, we demonstrate SD's effectiveness in label noise scenarios and identify the label corruption condition and minimum number of distillation rounds needed to achieve 100% classification accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20844;&#24179;&#25512;&#33616;&#31995;&#32479;&#65292;&#21517;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20844;&#24179;&#27880;&#24847;&#21147;&#21644;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#20004;&#20010;&#32452;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2402.03365</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#30064;&#36136;&#21451;&#21892;&#25512;&#33616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heterophily-Aware Fair Recommendation using Graph Convolutional Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03365
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#30340;&#20844;&#24179;&#25512;&#33616;&#31995;&#32479;&#65292;&#21517;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20844;&#24179;&#27880;&#24847;&#21147;&#21644;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#20004;&#20010;&#32452;&#20214;&#26469;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#19981;&#20165;&#35774;&#35745;&#20026;&#20026;&#26368;&#32456;&#29992;&#25143;&#26381;&#21153;&#65292;&#36824;&#35201;&#35753;&#20854;&#20182;&#21442;&#19982;&#32773;&#65288;&#22914;&#39033;&#30446;&#21644;&#39033;&#30446;&#20379;&#24212;&#21830;&#65289;&#20174;&#20013;&#21463;&#30410;&#12290;&#36825;&#20123;&#21442;&#19982;&#32773;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#25110;&#20914;&#31361;&#30340;&#30446;&#26631;&#21644;&#21033;&#30410;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#20844;&#24179;&#24615;&#21644;&#27969;&#34892;&#24230;&#20559;&#24046;&#32771;&#34385;&#30340;&#38656;&#27714;&#12290;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#26041;&#27861;&#20063;&#38754;&#20020;&#19981;&#20844;&#24179;&#24615;&#21644;&#27969;&#34892;&#24230;&#20559;&#24046;&#30340;&#25361;&#25112;&#65292;&#20854;&#24402;&#19968;&#21270;&#21644;&#32858;&#21512;&#36807;&#31243;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#30340;&#22522;&#20110;GNN&#30340;&#25512;&#33616;&#31995;&#32479;&#65292;&#31216;&#20026;HetroFair&#65292;&#26088;&#22312;&#25552;&#39640;&#39033;&#30446;&#20391;&#30340;&#20844;&#24179;&#24615;&#12290;HetroFair&#20351;&#29992;&#20004;&#20010;&#29420;&#31435;&#30340;&#32452;&#20214;&#29983;&#25104;&#20855;&#26377;&#20844;&#24179;&#24615;&#24847;&#35782;&#30340;&#23884;&#20837;&#65306;i&#65289;&#20844;&#24179;&#27880;&#24847;&#21147;&#65292;&#23427;&#22312;GNN&#30340;&#24402;&#19968;&#21270;&#36807;&#31243;&#20013;&#32467;&#21512;&#20102;&#28857;&#31215;&#65292;&#20197;&#20943;&#23569;&#33410;&#28857;&#24230;&#25968;&#30340;&#24433;&#21709;&#65307;ii&#65289;&#24322;&#36136;&#24615;&#29305;&#24449;&#21152;&#26435;&#65292;&#20026;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#37197;&#19981;&#21516;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, graph neural networks (GNNs) have become a popular tool to improve the accuracy and performance of recommender systems. Modern recommender systems are not only designed to serve the end users, but also to benefit other participants, such as items and items providers. These participants may have different or conflicting goals and interests, which raise the need for fairness and popularity bias considerations. GNN-based recommendation methods also face the challenges of unfairness and popularity bias and their normalization and aggregation processes suffer from these challenges. In this paper, we propose a fair GNN-based recommender system, called HetroFair, to improve items' side fairness. HetroFair uses two separate components to generate fairness-aware embeddings: i) fairness-aware attention which incorporates dot product in the normalization process of GNNs, to decrease the effect of nodes' degrees, and ii) heterophily feature weighting to assign distinct weights to 
&lt;/p&gt;</description></item><item><title>&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2311.01412</link><description>&lt;p&gt;
Castor: &#22240;&#26524;&#26102;&#24207;&#21306;&#22495;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Castor: Causal Temporal Regime Structure Learning. (arXiv:2311.01412v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01412
&lt;/p&gt;
&lt;p&gt;
&#8220;Castor&#8221;&#26159;&#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#32508;&#21512;&#23398;&#20064;&#21508;&#20010;&#21306;&#22495;&#30340;&#22240;&#26524;&#22270;&#12290;&#23427;&#36890;&#36807;&#26368;&#22823;&#21270;&#24471;&#20998;&#20989;&#25968;&#26469;&#25512;&#26029;&#21306;&#22495;&#30340;&#25968;&#37327;&#65292;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25581;&#31034;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#65292;&#28041;&#21450;&#21040;&#20174;&#27668;&#20505;&#31185;&#23398;&#21040;&#21307;&#30103;&#20445;&#20581;&#31561;&#21508;&#20010;&#23398;&#31185;&#30340;&#24191;&#27867;&#33539;&#22260;&#12290;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#20851;&#31995;&#65292;&#24182;&#19988;&#36890;&#24120;&#36981;&#24490;&#22810;&#20010;&#20808;&#39564;&#26410;&#30693;&#30340;&#21306;&#22495;&#12290;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#21487;&#20197;&#20174;&#20855;&#26377;&#24050;&#30693;&#21306;&#22495;&#30340;&#24322;&#26500;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#25688;&#35201;&#22240;&#26524;&#22270;&#65292;&#20294;&#22312;&#20840;&#38754;&#23398;&#20064;&#21306;&#22495;&#21644;&#30456;&#24212;&#30340;&#22240;&#26524;&#22270;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;CASTOR&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#23398;&#20064;&#30001;&#19981;&#21516;&#22240;&#26524;&#22270;&#32479;&#27835;&#30340;&#21508;&#31181;&#24322;&#26500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36890;&#36807;EM&#31639;&#27861;&#36890;&#36807;&#26368;&#22823;&#21270;&#19968;&#20010;&#24471;&#20998;&#20989;&#25968;&#65292;CASTOR&#25512;&#26029;&#20986;&#21306;&#22495;&#30340;&#25968;&#37327;&#24182;&#23398;&#20064;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CASTOR&#30340;&#31283;&#20581;&#25910;&#25947;&#24615;&#36136;&#65292;&#29305;&#21035;&#31361;&#20986;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of uncovering causal relationships among multivariate time series data stands as an essential and challenging objective that cuts across a broad array of disciplines ranging from climate science to healthcare. Such data entails linear or non-linear relationships, and usually follow multiple a priori unknown regimes. Existing causal discovery methods can infer summary causal graphs from heterogeneous data with known regimes, but they fall short in comprehensively learning both regimes and the corresponding causal graph. In this paper, we introduce CASTOR, a novel framework designed to learn causal relationships in heterogeneous time series data composed of various regimes, each governed by a distinct causal graph. Through the maximization of a score function via the EM algorithm, CASTOR infers the number of regimes and learns linear or non-linear causal relationships in each regime. We demonstrate the robust convergence properties of CASTOR, specifically highlighting its profic
&lt;/p&gt;</description></item><item><title>AUTOPARLLM&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#24182;&#29983;&#25104;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#24182;&#34892;&#29256;&#26412;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;GNN&#30340;&#24182;&#34892;&#24615;&#21457;&#29616;&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#12290;</title><link>http://arxiv.org/abs/2310.04047</link><description>&lt;p&gt;
AUTOPARLLM&#65306;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;GNN&#24341;&#23548;&#30340;&#33258;&#21160;&#20195;&#30721;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
AUTOPARLLM: GNN-Guided Automatic Code Parallelization using Large Language Models. (arXiv:2310.04047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04047
&lt;/p&gt;
&lt;p&gt;
AUTOPARLLM&#26159;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#24182;&#29983;&#25104;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#24182;&#34892;&#29256;&#26412;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#22522;&#20110;GNN&#30340;&#24182;&#34892;&#24615;&#21457;&#29616;&#27169;&#22359;&#21644;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24182;&#34892;&#21270;&#39034;&#24207;&#32534;&#20889;&#30340;&#31243;&#24207;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21363;&#20351;&#26159;&#32463;&#39564;&#20016;&#23500;&#30340;&#24320;&#21457;&#20154;&#21592;&#20063;&#38656;&#35201;&#33457;&#36153;&#30456;&#24403;&#22810;&#30340;&#26102;&#38388;&#26469;&#23547;&#25214;&#24182;&#34892;&#24615;&#26426;&#20250;&#65292;&#28982;&#21518;&#23454;&#38469;&#32534;&#20889;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#30340;&#24182;&#34892;&#29256;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUTOPARLLM&#65292;&#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#24182;&#34892;&#24615;&#24182;&#29983;&#25104;&#39034;&#24207;&#32534;&#20889;&#31243;&#24207;&#30340;&#24182;&#34892;&#29256;&#26412;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;i&#65289;&#22522;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24182;&#34892;&#24615;&#21457;&#29616;&#21644;&#24182;&#34892;&#27169;&#24335;&#26816;&#27979;&#27169;&#22359;&#65292;&#20197;&#21450;ii&#65289;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#22120;&#65292;&#29992;&#20110;&#29983;&#25104;&#39034;&#24207;&#31243;&#24207;&#30340;&#24182;&#34892;&#23545;&#24212;&#29256;&#26412;&#12290;&#25105;&#20204;&#20351;&#29992;GNN&#23398;&#20064;&#31243;&#24207;&#30340;&#27969;&#25935;&#24863;&#29305;&#24449;&#65292;&#20197;&#35782;&#21035;&#39034;&#24207;&#31243;&#24207;&#20013;&#30340;&#24182;&#34892;&#21306;&#22495;&#65292;&#24182;&#20351;&#29992;GNN&#30340;&#32467;&#26524;&#26500;&#24314;&#22686;&#24378;&#25552;&#31034;&#65292;&#20197;&#20379;LLM&#22522;&#30784;&#29983;&#25104;&#22120;&#26368;&#32456;&#20135;&#29983;&#39034;&#24207;&#31243;&#24207;&#30340;&#24182;&#34892;&#23545;&#24212;&#29256;&#26412;&#12290;&#25105;&#20204;&#22312;11&#20010;&#24212;&#29992;&#19978;&#35780;&#20272;&#20102;AUTOPARLLM
&lt;/p&gt;
&lt;p&gt;
Parallelizing sequentially written programs is a challenging task. Even experienced developers need to spend considerable time finding parallelism opportunities and then actually writing parallel versions of sequentially written programs. To address this issue, we present AUTOPARLLM, a framework for automatically discovering parallelism and generating the parallel version of the sequentially written program. Our framework consists of two major components: i) a heterogeneous Graph Neural Network (GNN) based parallelism discovery and parallel pattern detection module, and ii) an LLM-based code generator to generate the parallel counterpart of the sequential programs. We use the GNN to learn the flow-aware characteristics of the programs to identify parallel regions in sequential programs and then construct an enhanced prompt using the GNN's results for the LLM-based generator to finally produce the parallel counterparts of the sequential programs. We evaluate AUTOPARLLM on 11 application
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;&#65292;&#20026;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#24182;&#36866;&#24212;&#26032;&#25968;&#25454;&#21040;&#26469;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2309.07117</link><description>&lt;p&gt;
PILOT&#65306;&#19968;&#20010;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
PILOT: A Pre-Trained Model-Based Continual Learning Toolbox. (arXiv:2309.07117v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07117
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;&#65292;&#20026;&#22312;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#24182;&#36866;&#24212;&#26032;&#25968;&#25454;&#21040;&#26469;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22686;&#37327;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#65292;&#20294;&#20027;&#35201;&#22312;&#23553;&#38381;&#29615;&#22659;&#20013;&#36816;&#20316;&#65292;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#26102;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20316;&#20026;&#35299;&#20915;&#26041;&#26696;&#65292;&#22686;&#37327;&#23398;&#20064;&#24212;&#36816;&#32780;&#29983;&#65292;&#29992;&#20110;&#22788;&#29702;&#28041;&#21450;&#26032;&#25968;&#25454;&#21040;&#26469;&#30340;&#29616;&#23454;&#22330;&#26223;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#22312;&#19981;&#26029;&#21462;&#24471;&#37325;&#35201;&#36827;&#23637;&#65292;&#24182;&#24341;&#36215;&#20102;&#20247;&#22810;&#30740;&#31350;&#20154;&#21592;&#30340;&#20851;&#27880;&#12290;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PTMs&#65289;&#30340;&#24378;&#22823;&#24615;&#33021;&#20026;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#36866;&#24212;&#29616;&#23454;&#22330;&#26223;&#30340;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#22240;&#27492;&#65292;&#25506;&#32034;&#22312;&#22686;&#37327;&#23398;&#20064;&#20013;&#21033;&#29992;PTMs&#24050;&#32463;&#25104;&#20026;&#24517;&#38656;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PILOT&#30340;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#24037;&#20855;&#31665;&#12290;&#19968;&#26041;&#38754;&#65292;PILOT&#23454;&#26045;&#20102;&#19968;&#20123;&#22522;&#20110;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#26032;&#29677;&#32423;&#22686;&#37327;&#23398;&#20064;&#31639;&#27861;&#65292;&#22914;L2P&#12289;DualPrompt&#21644;CODA-Prompt&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;PILOT&#20063;&#36866;&#24212;&#20102;&#20856;&#22411;&#30340;&#29677;&#32423;&#22686;&#37327;&#23398;&#20064;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
While traditional machine learning can effectively tackle a wide range of problems, it primarily operates within a closed-world setting, which presents limitations when dealing with streaming data. As a solution, incremental learning emerges to address real-world scenarios involving new data's arrival. Recently, pre-training has made significant advancements and garnered the attention of numerous researchers. The strong performance of these pre-trained models (PTMs) presents a promising avenue for developing continual learning algorithms that can effectively adapt to real-world scenarios. Consequently, exploring the utilization of PTMs in incremental learning has become essential. This paper introduces a pre-trained model-based continual learning toolbox known as PILOT. On the one hand, PILOT implements some state-of-the-art class-incremental learning algorithms based on pre-trained models, such as L2P, DualPrompt, and CODA-Prompt. On the other hand, PILOT also fits typical class-incre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#19968;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#25968;&#30334;&#27425;&#30340;&#20050;&#20051;&#29699;&#22238;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#29699;&#36820;&#22238;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#25972;&#21512;&#20102;&#24863;&#30693;&#23376;&#31995;&#32479;&#12289;&#39640;&#36895;&#20302;&#24310;&#36831;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12289;&#20223;&#30495;&#33539;&#20363;&#12289;&#33258;&#21160;&#37325;&#32622;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#31561;&#21151;&#33021;&#65292;&#24182;&#23545;&#31995;&#32479;&#30340;&#35774;&#35745;&#20915;&#31574;&#21644;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2309.03315</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#20050;&#20051;&#29699;&#65306;&#19968;&#20010;&#39640;&#36895;&#23398;&#20064;&#31995;&#32479;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robotic Table Tennis: A Case Study into a High Speed Learning System. (arXiv:2309.03315v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#30740;&#31350;&#20102;&#19968;&#31181;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#25968;&#30334;&#27425;&#30340;&#20050;&#20051;&#29699;&#22238;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#29699;&#36820;&#22238;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#25972;&#21512;&#20102;&#24863;&#30693;&#23376;&#31995;&#32479;&#12289;&#39640;&#36895;&#20302;&#24310;&#36831;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12289;&#20223;&#30495;&#33539;&#20363;&#12289;&#33258;&#21160;&#37325;&#32622;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#31561;&#21151;&#33021;&#65292;&#24182;&#23545;&#31995;&#32479;&#30340;&#35774;&#35745;&#20915;&#31574;&#21644;&#37325;&#35201;&#24615;&#36827;&#34892;&#20102;&#35814;&#32454;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#30340;&#28145;&#20837;&#30740;&#31350;&#65292;&#27492;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#34920;&#26126;&#35813;&#31995;&#32479;&#33021;&#22815;&#19982;&#20154;&#31867;&#36827;&#34892;&#25968;&#30334;&#27425;&#30340;&#20050;&#20051;&#29699;&#22238;&#21512;&#65292;&#24182;&#19988;&#33021;&#22815;&#31934;&#30830;&#22320;&#23558;&#29699;&#36820;&#22238;&#21040;&#39044;&#23450;&#30340;&#30446;&#26631;&#20301;&#32622;&#12290;&#35813;&#31995;&#32479;&#32467;&#21512;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;&#24863;&#30693;&#23376;&#31995;&#32479;&#12289;&#39640;&#36895;&#20302;&#24310;&#36831;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#22120;&#12289;&#38450;&#27490;&#29616;&#23454;&#19990;&#30028;&#20013;&#25439;&#22351;&#24182;&#33021;&#22815;&#36827;&#34892;&#38646;-shot&#36716;&#31227;&#31574;&#30053;&#35757;&#32451;&#30340;&#20223;&#30495;&#33539;&#20363;&#65292;&#20197;&#21450;&#33258;&#21160;&#37325;&#32622;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#65292;&#20351;&#33258;&#20027;&#35757;&#32451;&#21644;&#35780;&#20272;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#36890;&#36807;&#35814;&#32454;&#25551;&#36848;&#25972;&#20010;&#31995;&#32479;&#65292;&#21253;&#25324;&#36890;&#24120;&#19981;&#24191;&#27867;&#20256;&#25773;&#30340;&#22823;&#37327;&#35774;&#35745;&#20915;&#31574;&#65292;&#24182;&#32467;&#21512;&#19968;&#31995;&#21015;&#30740;&#31350;&#26469;&#38416;&#26126;&#32531;&#35299;&#21508;&#31181;&#24310;&#36831;&#28304;&#30340;&#37325;&#35201;&#24615;&#12289;&#32771;&#34385;&#35757;&#32451;&#21644;&#37096;&#32626;&#20998;&#24067;&#21464;&#21270;&#12289;&#24863;&#30693;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#12289;&#31574;&#30053;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#21644;&#21160;&#20316;&#31354;&#38388;&#36873;&#25321;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#35270;&#39057;&#23637;&#31034;&#20102;&#31995;&#32479;&#30340;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a deep-dive into a real-world robotic learning system that, in previous work, was shown to be capable of hundreds of table tennis rallies with a human and has the ability to precisely return the ball to desired targets. This system puts together a highly optimized perception subsystem, a high-speed low-latency robot controller, a simulation paradigm that can prevent damage in the real world and also train policies for zero-shot transfer, and automated real world environment resets that enable autonomous training and evaluation on physical robots. We complement a complete system description, including numerous design decisions that are typically not widely disseminated, with a collection of studies that clarify the importance of mitigating various sources of latency, accounting for training and deployment distribution shifts, robustness of the perception system, sensitivity to policy hyper-parameters, and choice of action space. A video demonstrating the components of the sys
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10968</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;MRI&#22330;&#36716;&#31227;&#37325;&#24314;&#65306;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;
&lt;/p&gt;
&lt;p&gt;
MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer. (arXiv:2308.10968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25253;&#21578;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#38477;&#22122;&#65288;RED&#65289;&#27491;&#35268;&#21270;&#26159;&#19968;&#31181;&#23558;&#38477;&#22122;&#22120;&#20316;&#20026;&#22270;&#20687;&#37325;&#24314;&#20808;&#39564;&#30340;&#36890;&#29992;&#27969;&#31243;&#12290;RED&#30340;&#28508;&#21147;&#24050;&#32463;&#22312;&#22810;&#20010;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65288;&#22914;&#38477;&#22122;&#12289;&#21435;&#27169;&#31946;&#21644;&#36229;&#20998;&#36776;&#29575;&#65289;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;RNST&#65289;&#26041;&#27861;&#36827;&#34892;&#27491;&#35268;&#21270;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#31070;&#32463;&#36716;&#31227;&#21644;&#38477;&#22122;&#24341;&#25806;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;RNST&#33021;&#22815;&#20174;&#26377;&#22122;&#22768;&#30340;&#20302;&#36136;&#37327;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#22270;&#20687;&#39118;&#26684;&#21644;&#26377;&#38480;&#25968;&#25454;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;1.5T&#21644;3T&#30340;&#20020;&#24202;MRI&#25195;&#25551;&#39564;&#35777;&#20102;RNST&#65292;&#24182;&#19988;&#26174;&#31034;RNST&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;RNST&#26694;&#26550;&#22312;MRI&#37325;&#24314;&#21644;&#26377;&#38480;&#25968;&#25454;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated success in MRI reconstruction using deep learning-based models. However, most reported approaches require training on a task-specific, large-scale dataset. Regularization by denoising (RED) is a general pipeline which embeds a denoiser as a prior for image reconstruction. The potential of RED has been demonstrated for multiple image-related tasks such as denoising, deblurring and super-resolution. In this work, we propose a regularization by neural style transfer (RNST) method to further leverage the priors from the neural transfer and denoising engine. This enables RNST to reconstruct a high-quality image from a noisy low-quality image with different image styles and limited data. We validate RNST with clinical MRI scans from 1.5T and 3T and show that RNST can significantly boost image quality. Our results highlight the capability of the RNST framework for MRI reconstruction and the potential for reconstruction tasks with limited data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.04690</link><description>&lt;p&gt;
&#29992;&#20110;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Finite Element Operator Network for Solving Parametric PDEs. (arXiv:2308.04690v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#23427;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#27809;&#26377;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#26159;&#25105;&#20204;&#29702;&#35299;&#21644;&#39044;&#27979;&#29289;&#29702;&#12289;&#24037;&#31243;&#21644;&#37329;&#34701;&#31561;&#20247;&#22810;&#39046;&#22495;&#33258;&#28982;&#29616;&#35937;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#21442;&#25968;PDE&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#25968;&#20540;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26377;&#38480;&#20803;&#31639;&#23376;&#32593;&#32476;&#65288;FEONet&#65289;&#35299;&#20915;&#21442;&#25968;PDE&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#25968;&#20540;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#26377;&#38480;&#20803;&#27861;&#65292;&#20197;&#22312;&#27809;&#26377;&#20219;&#20309;&#37197;&#23545;&#30340;&#36755;&#20837;-&#36755;&#20986;&#35757;&#32451;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#21442;&#25968;PDE&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#38382;&#39064;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#34920;&#26126;&#23427;&#22312;&#20934;&#30830;&#24230;&#12289;&#27867;&#21270;&#24615;&#21644;&#35745;&#31639;&#28789;&#27963;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;FEONet&#26694;&#26550;&#22312;&#27169;&#25311;&#20855;&#26377;&#19981;&#21516;&#36793;&#30028;&#26465;&#20214;&#21644;&#22797;&#26434;&#22495;&#30340;&#21508;&#31181;&#39046;&#22495;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Partial differential equations (PDEs) underlie our understanding and prediction of natural phenomena across numerous fields, including physics, engineering, and finance. However, solving parametric PDEs is a complex task that necessitates efficient numerical methods. In this paper, we propose a novel approach for solving parametric PDEs using a Finite Element Operator Network (FEONet). Our proposed method leverages the power of deep learning in conjunction with traditional numerical methods, specifically the finite element method, to solve parametric PDEs in the absence of any paired input-output training data. We demonstrate the effectiveness of our approach on several benchmark problems and show that it outperforms existing state-of-the-art methods in terms of accuracy, generalization, and computational flexibility. Our FEONet framework shows potential for application in various fields where PDEs play a crucial role in modeling complex domains with diverse boundary conditions and sin
&lt;/p&gt;</description></item><item><title>ArrayBot&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#65292;&#36890;&#36807;&#23545;&#21160;&#20316;&#31354;&#38388;&#30340;&#37325;&#26032;&#23450;&#20041;&#21644;&#37319;&#29992;&#35302;&#35273;&#35266;&#23519;&#35757;&#32451;&#65292;&#20854;&#25511;&#21046;&#31574;&#30053;&#19981;&#20165;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#36824;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#36716;&#31227;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.16857</link><description>&lt;p&gt;
ArrayBot: &#36890;&#36807;&#35302;&#35273;&#23454;&#29616;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ArrayBot: Reinforcement Learning for Generalizable Distributed Manipulation through Touch. (arXiv:2306.16857v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16857
&lt;/p&gt;
&lt;p&gt;
ArrayBot&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#20102;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#65292;&#36890;&#36807;&#23545;&#21160;&#20316;&#31354;&#38388;&#30340;&#37325;&#26032;&#23450;&#20041;&#21644;&#37319;&#29992;&#35302;&#35273;&#35266;&#23519;&#35757;&#32451;&#65292;&#20854;&#25511;&#21046;&#31574;&#30053;&#19981;&#20165;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#36824;&#33021;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#20013;&#36827;&#34892;&#36716;&#31227;&#65292;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;ArrayBot&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;16&#215;16&#30340;&#31446;&#21521;&#28369;&#21160;&#26609;&#21644;&#35302;&#35273;&#20256;&#24863;&#22120;&#32452;&#25104;&#30340;&#20998;&#24067;&#24335;&#25805;&#20316;&#31995;&#32479;&#65292;&#21487;&#20197;&#21516;&#26102;&#25903;&#25345;&#12289;&#24863;&#30693;&#21644;&#25805;&#20316;&#26700;&#38754;&#19978;&#30340;&#29289;&#20307;&#12290;&#20026;&#20102;&#23454;&#29616;&#36890;&#29992;&#20998;&#24067;&#24335;&#25805;&#20316;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#33258;&#21160;&#21457;&#29616;&#25511;&#21046;&#31574;&#30053;&#12290;&#38754;&#23545;&#22823;&#37327;&#20887;&#20313;&#30340;&#21160;&#20316;&#65292;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#31354;&#38388;&#23616;&#37096;&#21160;&#20316;&#22270;&#22359;&#21644;&#39057;&#22495;&#20013;&#20302;&#39057;&#21160;&#20316;&#26469;&#37325;&#26032;&#23450;&#20041;&#21160;&#20316;&#31354;&#38388;&#12290;&#36890;&#36807;&#36825;&#20010;&#37325;&#26032;&#23450;&#20041;&#30340;&#21160;&#20316;&#31354;&#38388;&#65292;&#25105;&#20204;&#35757;&#32451;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#21482;&#36890;&#36807;&#35302;&#35273;&#35266;&#23519;&#21363;&#21487;&#37325;&#26032;&#23450;&#20301;&#19981;&#21516;&#30340;&#29289;&#20307;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#21457;&#29616;&#30340;&#31574;&#30053;&#19981;&#20165;&#21487;&#20197;&#25512;&#24191;&#21040;&#27169;&#25311;&#22120;&#20013;&#30475;&#19981;&#35265;&#30340;&#29289;&#20307;&#24418;&#29366;&#65292;&#32780;&#19988;&#21487;&#20197;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#36827;&#34892;&#36716;&#31227;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#22495;&#38543;&#26426;&#21270;&#12290;&#21033;&#29992;&#37096;&#32626;&#30340;&#31574;&#30053;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20016;&#23500;&#30340;&#30495;&#23454;&#19990;&#30028;&#25805;&#20316;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#20854;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present ArrayBot, a distributed manipulation system consisting of a $16 \times 16$ array of vertically sliding pillars integrated with tactile sensors, which can simultaneously support, perceive, and manipulate the tabletop objects. Towards generalizable distributed manipulation, we leverage reinforcement learning (RL) algorithms for the automatic discovery of control policies. In the face of the massively redundant actions, we propose to reshape the action space by considering the spatially local action patch and the low-frequency actions in the frequency domain. With this reshaped action space, we train RL agents that can relocate diverse objects through tactile observations only. Surprisingly, we find that the discovered policy can not only generalize to unseen object shapes in the simulator but also transfer to the physical robot without any domain randomization. Leveraging the deployed policy, we present abundant real-world manipulation tasks, illustrating the vast potential of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2306.06081</link><description>&lt;p&gt;
CARSO: &#23545;&#25239;&#24615;&#21512;&#25104;&#35266;&#27979;&#30340;&#21453;&#23545;&#25239;&#24615;&#21484;&#22238;
&lt;/p&gt;
&lt;p&gt;
CARSO: Counter-Adversarial Recall of Synthetic Observations. (arXiv:2306.06081v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#20998;&#31867;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#65292;&#24182;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26426;&#21046;CARSO&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#28789;&#24863;&#26469;&#33258;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#32447;&#32034;&#12290;&#35813;&#26041;&#27861;&#19982;&#23545;&#25239;&#35757;&#32451;&#20855;&#26377;&#21327;&#21516;&#20114;&#34917;&#24615;&#65292;&#24182;&#20381;&#36182;&#20110;&#34987;&#25915;&#20987;&#20998;&#31867;&#22120;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#30693;&#35782;&#12290;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#20928;&#21270;&#65292;&#35813;&#26041;&#27861;&#37319;&#26679;&#36755;&#20837;&#30340;&#37325;&#26500;&#26469;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#22312;&#21508;&#31181;&#22270;&#20687;&#25968;&#25454;&#38598;&#21644;&#20998;&#31867;&#22120;&#20307;&#31995;&#32467;&#26500;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;CARSO&#33021;&#22815;&#27604;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26356;&#22909;&#22320;&#20445;&#25252;&#20998;&#31867;&#22120;&#8212;&#8212;&#21516;&#26102;&#20855;&#26377;&#21487;&#25509;&#21463;&#30340;&#28165;&#27905;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;&#27492;&#22806;&#65292;&#38450;&#24481;&#20307;&#31995;&#32467;&#26500;&#25104;&#21151;&#22320;&#20445;&#25252;&#33258;&#24049;&#20813;&#21463;&#26410;&#39044;&#35265;&#30340;&#23041;&#32961;&#21644;&#26368;&#32456;&#25915;&#20987;&#12290;&#20195;&#30721;&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#22312;https://github.com/&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel adversarial defence mechanism for image classification -- CARSO -- inspired by cues from cognitive neuroscience. The method is synergistically complementary to adversarial training and relies on knowledge of the internal representation of the attacked classifier. Exploiting a generative model for adversarial purification, conditioned on such representation, it samples reconstructions of inputs to be finally classified. Experimental evaluation by a well-established benchmark of varied, strong adaptive attacks, across diverse image datasets and classifier architectures, shows that CARSO is able to defend the classifier significantly better than state-of-the-art adversarial training alone -- with a tolerable clean accuracy toll. Furthermore, the defensive architecture succeeds in effectively shielding itself from unforeseen threats, and end-to-end attacks adapted to fool stochastic defences. Code and pre-trained models are available at https://github.com/
&lt;/p&gt;</description></item><item><title>&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;</title><link>http://arxiv.org/abs/2306.04940</link><description>&lt;p&gt;
&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Layer-level activation mechanism. (arXiv:2306.04940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04940
&lt;/p&gt;
&lt;p&gt;
&#21435;&#22122;&#22768;&#26356;&#22909;&#65292;&#34920;&#29616;&#26356;&#22909;&#30340;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28608;&#27963;&#26426;&#21046;&#65292;&#26088;&#22312;&#24314;&#31435;&#20998;&#23618;&#32423;&#21035;&#28608;&#27963;&#21151;&#33021;&#65288;LayerAct&#65289;&#12290;&#36825;&#20123;&#21151;&#33021;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36755;&#20837;&#20559;&#31227;&#25152;&#23548;&#33268;&#30340;&#28608;&#27963;&#36755;&#20986;&#30340;&#20998;&#23618;&#32423;&#27874;&#21160;&#26469;&#38477;&#20302;&#20256;&#32479;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#30340;&#22122;&#38899;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;LayerAct&#21151;&#33021;&#23454;&#29616;&#20102;&#31867;&#20284;&#20110;&#38646;&#30340;&#24179;&#22343;&#28608;&#27963;&#36755;&#20986;&#65292;&#32780;&#19981;&#38480;&#21046;&#28608;&#27963;&#36755;&#20986;&#31354;&#38388;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#35777;&#26126;LayerAct&#21151;&#33021;&#22312;&#22122;&#22768;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#65292;&#24182;&#19988;&#32463;&#39564;&#35777;&#26126;&#36825;&#20123;&#21151;&#33021;&#30340;&#24179;&#22343;&#28608;&#27963;&#32467;&#26524;&#31867;&#20284;&#20110;&#38646;&#12290;&#22312;&#19977;&#20010;&#22522;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22788;&#29702;&#22024;&#26434;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#26102;&#65292;LayerAct&#21151;&#33021;&#27604;&#20803;&#32032;&#32423;&#28608;&#27963;&#21151;&#33021;&#34920;&#29616;&#26356;&#22909;&#65292;&#32780;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#28165;&#27905;&#25968;&#25454;&#38598;&#30340;&#34920;&#29616;&#20063;&#26159;&#20248;&#36234;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose a novel activation mechanism aimed at establishing layer-level activation (LayerAct) functions. These functions are designed to be more noise-robust compared to traditional element-level activation functions by reducing the layer-level fluctuation of the activation outputs due to shift in inputs. Moreover, the LayerAct functions achieve a zero-like mean activation output without restricting the activation output space. We present an analysis and experiments demonstrating that LayerAct functions exhibit superior noise-robustness compared to element-level activation functions, and empirically show that these functions have a zero-like mean activation. Experimental results on three benchmark image classification tasks show that LayerAct functions excel in handling noisy image datasets, outperforming element-level activation functions, while the performance on clean datasets is also superior in most cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#38271;&#26399;&#25345;&#32493;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26159;&#21542;&#33021;&#25552;&#39640;&#33021;&#37327;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#39044;&#31639;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07973</link><description>&lt;p&gt;
&#35770;&#38543;&#26426;&#23433;&#20840;&#24615;&#30340;&#35745;&#31639;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
On the Computational Cost of Stochastic Security. (arXiv:2305.07973v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#20351;&#29992;&#38271;&#26399;&#25345;&#32493;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26159;&#21542;&#33021;&#25552;&#39640;&#33021;&#37327;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#22686;&#21152;&#35745;&#31639;&#39044;&#31639;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#30340;&#38271;&#26399;&#25345;&#32493;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#26159;&#21542;&#20250;&#25552;&#39640;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBM&#65289;&#25152;&#36798;&#21040;&#30340;&#34920;&#24449;&#36136;&#37327;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#35757;&#32451;&#36807;&#30340;EBM&#30340;&#25193;&#25955;&#36807;&#31243;&#30340;&#33945;&#29305;&#21345;&#32599;&#27169;&#25311;&#65292;&#29992;&#20110;&#25552;&#39640;&#29420;&#31435;&#20998;&#31867;&#22120;&#32593;&#32476;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#21644;&#26657;&#20934;&#20998;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25345;&#32493;&#23545;&#27604;&#25955;&#24230;&#30340;&#35745;&#31639;&#39044;&#31639;&#22686;&#21152;&#21513;&#24067;&#26031;&#37319;&#26679;&#30340;&#24773;&#20917;&#19979;&#65292;&#25913;&#36827;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#28548;&#28165;&#20102;&#23454;&#29616;&#26377;&#25928;&#20174;&#36830;&#32493;&#33021;&#37327;&#21183;&#20013;&#36827;&#34892;&#21513;&#24067;&#26031;&#37319;&#26679;&#30340;&#26032;&#37327;&#23376;&#21644;&#32463;&#20856;&#30828;&#20214;&#21644;&#36719;&#20214;&#30340;&#23454;&#38469;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate whether long-run persistent chain Monte Carlo simulation of Langevin dynamics improves the quality of the representations achieved by energy-based models (EBM). We consider a scheme wherein Monte Carlo simulation of a diffusion process using a trained EBM is used to improve the adversarial robustness and the calibration score of an independent classifier network. Our results show that increasing the computational budget of Gibbs sampling in persistent contrastive divergence improves the calibration and adversarial robustness of the model, elucidating the practical merit of realizing new quantum and classical hardware and software for efficient Gibbs sampling from continuous energy potentials.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.02011</link><description>&lt;p&gt;
FakET: &#21033;&#29992;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#27169;&#25311;&#20919;&#20923;&#30005;&#23376;&#26029;&#23618;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer. (arXiv:2304.02011v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#26469;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#65292;&#20197;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31890;&#23376;&#23450;&#20301;&#21644;&#20998;&#31867;&#26159;&#35745;&#31639;&#26174;&#24494;&#23398;&#20013;&#26368;&#22522;&#26412;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#36825;&#20123;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#28857;&#26159;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#36890;&#24120;&#26159;&#19982;&#27169;&#25311;&#36879;&#23556;&#30005;&#23376;&#26174;&#24494;&#38236;&#29289;&#29702;&#30340;&#22797;&#26434;&#25968;&#20540;&#27491;&#21521;&#27169;&#22411;&#20013;&#30340;&#31890;&#23376;&#27169;&#22411;&#32467;&#21512;&#29983;&#25104;&#30340;&#12290;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#26426;&#23454;&#29616;&#38750;&#24120;&#32791;&#36153;&#35745;&#31639;&#36164;&#28304;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#33539;&#22260;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21152;&#24615;&#22122;&#22768;&#21644;&#31070;&#32463;&#39118;&#26684;&#36801;&#31227;&#25216;&#26415;&#27169;&#25311;&#30005;&#23376;&#26174;&#24494;&#38236;&#27491;&#21521;&#31639;&#23376;&#30340;&#31616;&#21333;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#24050;&#32463;&#24314;&#31435;&#30340;&#29366;&#24577;&#20043;&#19968;&#23545;&#23450;&#20301;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#19982;&#22522;&#20934;&#27979;&#35797;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21152;&#36895;&#20102;&#36816;&#31639;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Particle localization and -classification constitute two of the most fundamental problems in computational microscopy. In recent years, deep learning based approaches have been introduced for these tasks with great success. A key shortcoming of these supervised learning methods is their need for large training data sets, typically generated from particle models in conjunction with complex numerical forward models simulating the physics of transmission electron microscopes. Computer implementations of such forward models are computationally extremely demanding and limit the scope of their applicability. In this paper we propose a simple method for simulating the forward operator of an electron microscope based on additive noise and Neural Style Transfer techniques. We evaluate the method on localization and classification tasks using one of the established state-of-the-art architectures showing performance on par with the benchmark. In contrast to previous approaches, our method acceler
&lt;/p&gt;</description></item></channel></rss>