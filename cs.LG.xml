<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#21463;&#30410;&#20110;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2206.00395</link><description>&lt;p&gt;
&#20855;&#22791;&#36741;&#21161;&#20449;&#24687;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimization with access to auxiliary information. (arXiv:2206.00395v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.00395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#21463;&#30410;&#20110;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the fundamental optimization question of minimizing a target function with expensive or limited gradient computation, given access to some auxiliary side function with cheaper or more available gradients. The authors propose two generic new algorithms and prove that this framework can benefit from the Hessian similarity assumption between the target and side information.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#26412;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#22312;&#35745;&#31639;&#30446;&#26631;&#20989;&#25968;$f(x)$&#30340;&#26799;&#24230;&#24456;&#26114;&#36149;&#25110;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#32473;&#23450;&#19968;&#20123;&#36741;&#21161;&#20989;&#25968;$h(x)$&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#26368;&#23567;&#21270;&#30446;&#26631;&#20989;&#25968;&#12290;&#36825;&#20010;&#20844;&#24335;&#28085;&#30422;&#20102;&#35768;&#22810;&#23454;&#38469;&#30456;&#20851;&#30340;&#35774;&#32622;&#65292;&#22914;i&#65289;&#22312;SGD&#20013;&#37325;&#22797;&#20351;&#29992;&#25209;&#27425;&#65292;ii&#65289;&#36801;&#31227;&#23398;&#20064;&#65292;iii&#65289;&#32852;&#37030;&#23398;&#20064;&#65292;iv&#65289;&#20351;&#29992;&#21387;&#32553;&#27169;&#22411;/&#20002;&#24323;&#31561;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#36890;&#29992;&#30340;&#26032;&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#25152;&#26377;&#36825;&#20123;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20165;&#20351;&#29992;&#30446;&#26631;&#21644;&#36741;&#21161;&#20449;&#24687;&#20043;&#38388;&#30340;Hessian&#30456;&#20284;&#24615;&#20551;&#35774;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#36825;&#20010;&#26694;&#26550;&#20013;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the fundamental optimization question of minimizing a target function $f(x)$ whose gradients are expensive to compute or have limited availability, given access to some auxiliary side function $h(x)$ whose gradients are cheap or more available. This formulation captures many settings of practical relevance such as i) re-using batches in SGD, ii) transfer learning, iii) federated learning, iv) training with compressed models/dropout, etc. We propose two generic new algorithms which are applicable in all these settings and prove using only an assumption on the Hessian similarity between the target and side information that we can benefit from this framework.
&lt;/p&gt;</description></item></channel></rss>