<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.17827</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21512;&#25104;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DiffH2O: Diffusion-Based Synthesis of Hand-Object Interactions from Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17827
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#25991;&#26412;&#25551;&#36848;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#25163;-&#29289;&#20307;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#31181;&#25216;&#26415;&#23454;&#29616;&#20102;&#26377;&#25928;&#23398;&#20064;&#65292;&#21253;&#25324;&#20219;&#21153;&#20998;&#35299;&#12289;&#32039;&#23494;&#32806;&#21512;&#30340;&#23039;&#21183;&#34920;&#31034;&#21644;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#33258;&#28982;&#30340;3D&#25163;-&#29289;&#20307;&#20132;&#20114;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#26399;&#26395;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#29289;&#20307;&#21160;&#20316;&#22312;&#29289;&#29702;&#19978;&#26159;&#21512;&#29702;&#30340;&#65292;&#24182;&#19988;&#22312;&#35821;&#20041;&#19978;&#26159;&#26377;&#24847;&#20041;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiffH2O&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25552;&#20379;&#30340;&#25991;&#26412;&#25552;&#31034;&#21644;&#29289;&#20307;&#20960;&#20309;&#24418;&#29366;&#20013;&#21512;&#25104;&#36924;&#30495;&#30340;&#21333;&#25163;&#25110;&#21452;&#25163;&#29289;&#20307;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#19977;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#25235;&#21462;&#38454;&#27573;&#21644;&#22522;&#20110;&#25991;&#26412;&#20132;&#20114;&#38454;&#27573;&#65292;&#24182;&#20026;&#27599;&#20010;&#38454;&#27573;&#20351;&#29992;&#21333;&#29420;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#22312;&#25235;&#21462;&#38454;&#27573;&#20013;&#65292;&#27169;&#22411;&#20165;&#29983;&#25104;&#25163;&#37096;&#21160;&#20316;&#65292;&#32780;&#22312;&#20132;&#20114;&#38454;&#27573;&#20013;&#65292;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#37117;&#34987;&#21512;&#25104;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#23494;&#32806;&#21512;&#25163;&#37096;&#21644;&#29289;&#20307;&#23039;&#21183;&#30340;&#32039;&#20945;&#34920;&#31034;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#24341;&#23548;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17827v1 Announce Type: cross  Abstract: Generating natural hand-object interactions in 3D is challenging as the resulting hand and object motions are expected to be physically plausible and semantically meaningful. Furthermore, generalization to unseen objects is hindered by the limited scale of available hand-object interaction datasets. We propose DiffH2O, a novel method to synthesize realistic, one or two-handed object interactions from provided text prompts and geometry of the object. The method introduces three techniques that enable effective learning from limited data. First, we decompose the task into a grasping stage and a text-based interaction stage and use separate diffusion models for each. In the grasping stage, the model only generates hand motions, whereas in the interaction phase both hand and object poses are synthesized. Second, we propose a compact representation that tightly couples hand and object poses. Third, we propose two different guidance schemes 
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01919</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20248;&#21270;&#21644;&#21015;&#23376;&#38598;&#36873;&#25321;&#30340;&#30697;&#38453;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Matrix Completion with Convex Optimization and Column Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27599;&#19968;&#27493;&#20013;&#35299;&#20915;&#19968;&#20010;&#20984;&#20248;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#25105;&#20204;&#30340;&#21015;&#36873;&#25321;&#30697;&#38453;&#23436;&#25104;&#65288;CSMC&#65289;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#27599;&#31181;&#31639;&#27861;&#38024;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27491;&#24335;&#20998;&#26512;&#65292;&#22312;&#20998;&#26512;&#20013;&#25105;&#20204;&#38416;&#26126;&#20102;&#24517;&#35201;&#30340;&#20551;&#35774;&#21644;&#25214;&#21040;&#27491;&#30830;&#35299;&#30340;&#27010;&#29575;&#12290;&#22312;&#35770;&#25991;&#30340;&#31532;&#20108;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#39564;&#24037;&#20316;&#30340;&#32467;&#26524;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#30740;&#31350;&#30697;&#38453;&#22823;&#23567;&#12289;&#31209;&#21644;&#32570;&#22833;&#20803;&#32032;&#27604;&#20363;&#23545;&#35299;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01919v1 Announce Type: new  Abstract: We introduce a two-step method for the matrix recovery problem. Our approach combines the theoretical foundations of the Column Subset Selection and Low-rank Matrix Completion problems. The proposed method, in each step, solves a convex optimization task. We present two algorithms that implement our Columns Selected Matrix Completion (CSMC) method, each dedicated to a different size problem. We performed a formal analysis of the presented method, in which we formulated the necessary assumptions and the probability of finding a correct solution. In the second part of the paper, we present the results of the experimental work. Numerical experiments verified the correctness and performance of the algorithms. To study the influence of the matrix size, rank, and the proportion of missing elements on the quality of the solution and the computation time, we performed experiments on synthetic data. The presented method was applied to two real-li
&lt;/p&gt;</description></item><item><title>&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#32531;&#35299;&#36739;&#24046;&#30340;&#22806;&#25512;&#65292;&#20294;&#23545;&#25968;&#25454;&#38598;&#20559;&#35265;&#26080;&#27982;&#20110;&#20107;&#12290;</title><link>https://arxiv.org/abs/2403.00194</link><description>&lt;p&gt;
&#35810;&#38382;&#24744;&#30340;&#20998;&#24067;&#36716;&#31227;&#26159;&#21542;&#36866;&#21512;&#36827;&#34892;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Ask Your Distribution Shift if Pre-Training is Right for You
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00194
&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#26377;&#21161;&#20110;&#32531;&#35299;&#36739;&#24046;&#30340;&#22806;&#25512;&#65292;&#20294;&#23545;&#25968;&#25454;&#38598;&#20559;&#35265;&#26080;&#27982;&#20110;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#24320;&#21457;&#23545;&#20998;&#24067;&#36716;&#31227;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#20854;&#26377;&#25928;&#24615;&#22240;&#24773;&#20917;&#32780;&#24322;&#65306;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#40065;&#26834;&#24615;&#65292;&#20294;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#21017;&#23436;&#20840;&#19981;&#21516;&#65288;&#19982;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30456;&#27604;&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#25551;&#36848;&#39044;&#35757;&#32451;&#21487;&#20197;&#21644;&#19981;&#33021;&#35299;&#20915;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20851;&#27880;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#21487;&#33021;&#20986;&#29616;&#30340;&#20004;&#31181;&#22833;&#36133;&#27169;&#24335;&#65306;&#36739;&#24046;&#30340;&#22806;&#25512;&#65288;&#20363;&#22914;&#65292;&#23427;&#20204;&#26080;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#39046;&#22495;&#65289;&#21644;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65288;&#20363;&#22914;&#65292;&#23427;&#20204;&#20381;&#36182;&#20110;&#34394;&#20551;&#29305;&#24449;&#65289;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#19968;&#20010;&#32463;&#39564;&#20934;&#21017;&#65292;&#39044;&#35757;&#32451;&#21487;&#20197;&#24110;&#21161;&#32531;&#35299;&#36739;&#24046;&#30340;&#22806;&#25512;&#65292;&#20294;&#19981;&#33021;&#32531;&#35299;&#25968;&#25454;&#38598;&#20559;&#35265;&#12290;&#22312;&#25552;&#20379;&#20102;&#36825;&#19968;&#21457;&#29616;&#30340;&#29702;&#35770;&#21160;&#26426;&#21644;&#23454;&#35777;&#35777;&#25454;&#21518;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24320;&#21457;&#40065;&#26834;&#27169;&#22411;&#30340;&#20004;&#20010;&#28508;&#22312;&#21547;&#20041;&#65306;&#65288;1&#65289;&#39044;&#35757;&#32451;&#21644;&#26088;&#22312;&#38450;&#27490;&#21033;&#29992;&#20559;&#35265;&#30340;&#24178;&#39044;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00194v1 Announce Type: new  Abstract: Pre-training is a widely used approach to develop models that are robust to distribution shifts. However, in practice, its effectiveness varies: fine-tuning a pre-trained model improves robustness significantly in some cases but not at all in others (compared to training from scratch). In this work, we seek to characterize the failure modes that pre-training can and cannot address. In particular, we focus on two possible failure modes of models under distribution shift: poor extrapolation (e.g., they cannot generalize to a different domain) and biases in the training data (e.g., they rely on spurious features). Our study suggests that, as a rule of thumb, pre-training can help mitigate poor extrapolation but not dataset biases. After providing theoretical motivation and empirical evidence for this finding, we explore two of its implications for developing robust models: (1) pre-training and interventions designed to prevent exploiting bi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.14547</link><description>&lt;p&gt;
OmniPred&#65306;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#22238;&#24402;&#22120;
&lt;/p&gt;
&lt;p&gt;
OmniPred: Language Models as Universal Regressors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;OmniPred&#26694;&#26550;&#65292;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#30340;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#65292;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#35757;&#32451;&#26102;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#39564;&#35774;&#35745;&#30340;&#24191;&#38420;&#39046;&#22495;&#20013;&#65292;&#22238;&#24402;&#19968;&#30452;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#31995;&#32479;&#25110;&#27169;&#22411;&#22312;&#32473;&#23450;&#19968;&#32452;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#30340;&#32467;&#26524;&#25351;&#26631;&#65292;&#20294;&#20256;&#32479;&#19978;&#21482;&#38480;&#20110;&#36866;&#29992;&#20110;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OmniPred&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36890;&#29992;&#31471;&#21040;&#31471;&#22238;&#24402;&#22120;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#26469;&#33258;&#22810;&#26679;&#30495;&#23454;&#19990;&#30028;&#23454;&#39564;&#30340;$(x,y)$&#35780;&#20272;&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#28304;&#33258;Google Vizier&#30340;&#25968;&#25454;&#65292;&#36825;&#26159;&#19990;&#30028;&#19978;&#26368;&#22823;&#30340;&#40657;&#30418;&#20248;&#21270;&#25968;&#25454;&#24211;&#20043;&#19968;&#65292;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#25968;&#23398;&#21442;&#25968;&#21644;&#20540;&#30340;&#25991;&#26412;&#34920;&#31034;&#65292;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#36827;&#34892;&#38750;&#24120;&#31934;&#30830;&#30340;&#25968;&#20540;&#22238;&#24402;&#65292;&#22914;&#26524;&#26377;&#26426;&#20250;&#35757;&#32451;&#22810;&#20010;&#20219;&#21153;&#65292;&#21017;&#21487;&#20197;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#30340;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14547v1 Announce Type: cross  Abstract: Over the broad landscape of experimental design, regression has been a powerful tool to accurately predict the outcome metrics of a system or model given a set of parameters, but has been traditionally restricted to methods which are only applicable to a specific task. In this paper, we propose OmniPred, a framework for training language models as universal end-to-end regressors over $(x,y)$ evaluation data from diverse real world experiments. Using data sourced from Google Vizier, one of the largest blackbox optimization databases in the world, our extensive experiments demonstrate that through only textual representations of mathematical parameters and values, language models are capable of very precise numerical regression, and if given the opportunity to train over multiple tasks, can significantly outperform traditional regression models.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;</title><link>https://arxiv.org/abs/2402.14228</link><description>&lt;p&gt;
COPR:&#36890;&#36807;&#26368;&#20248;&#31574;&#30053;&#27491;&#21017;&#21270;&#23454;&#29616;&#25345;&#32493;&#20154;&#31867;&#20559;&#22909;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
COPR: Continual Human Preference Learning via Optimal Policy Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14228
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#65292;&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#20197;&#21160;&#24577;&#22320;&#23545;&#24403;&#21069;&#31574;&#30053;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20174;&#32780;&#20351;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#22312;&#25345;&#32493;&#23398;&#20064;&#24773;&#22659;&#19979;&#26356;&#21152;&#31283;&#20581;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#30028; &#25688;&#35201;: &#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#36890;&#24120;&#29992;&#20110;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#23545;&#40784;&#12290;&#37492;&#20110;&#20154;&#31867;&#20559;&#22909;&#30340;&#19981;&#26029;&#21464;&#21270;&#65292;&#25345;&#32493;&#23545;&#40784;&#30456;&#23545;&#20110;&#20256;&#32479;&#38745;&#24577;&#23545;&#40784;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#21644;&#23454;&#38469;&#12290;&#28982;&#32780;&#65292;&#20351;RLHF&#19982;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#20860;&#23481;&#30001;&#20110;&#20854;&#22797;&#26434;&#36807;&#31243;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21516;&#26102;&#65292;&#30452;&#25509;&#23398;&#20064;&#26032;&#30340;&#20154;&#31867;&#20559;&#22909;&#21487;&#33021;&#23548;&#33268;&#21382;&#21490;&#20559;&#22909;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#65292;&#23548;&#33268;&#26080;&#21161;&#25110;&#26377;&#23475;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Continual Optimal Policy Regularization (COPR) &#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20511;&#37492;&#20102;&#26368;&#20248;&#31574;&#30053;&#29702;&#35770;&#12290;COPR&#21033;&#29992;&#37319;&#26679;&#20998;&#24067;&#20316;&#20026;&#31034;&#33539;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#29992;&#20110;&#25345;&#32493;&#23398;&#20064;&#12290;&#23427;&#37319;&#29992;Lagrange&#23545;&#20598;&#65288;LD&#65289;&#26041;&#27861;&#26681;&#25454;&#21382;&#21490;&#19978;&#30340;&#26368;&#20248;&#31574;&#30053;&#21160;&#24577;&#22320;&#27491;&#21017;&#21270;&#24403;&#21069;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14228v1 Announce Type: cross  Abstract: Reinforcement Learning from Human Feedback (RLHF) is commonly utilized to improve the alignment of Large Language Models (LLMs) with human preferences. Given the evolving nature of human preferences, continual alignment becomes more crucial and practical in comparison to traditional static alignment. Nevertheless, making RLHF compatible with Continual Learning (CL) is challenging due to its complex process. Meanwhile, directly learning new human preferences may lead to Catastrophic Forgetting (CF) of historical preferences, resulting in helpless or harmful outputs. To overcome these challenges, we propose the Continual Optimal Policy Regularization (COPR) method, which draws inspiration from the optimal policy theory. COPR utilizes a sampling distribution as a demonstration and regularization constraints for CL. It adopts the Lagrangian Duality (LD) method to dynamically regularize the current policy based on the historically optimal p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#39640;&#25928;&#35757;&#32451;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#20856;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#20855;&#26377;&#20219;&#24847;&#24120;&#25968;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#20869;&#23384;&#20998;&#31163;&#12290;&#27599;&#20010;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#37117;&#21487;&#20197;&#22312;&#24120;&#25968;&#26102;&#38388;&#20869;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;</title><link>https://arxiv.org/abs/2402.08606</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20219;&#24847;&#22810;&#39033;&#24335;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Arbitrary Polynomial Separations in Trainable Quantum Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08606
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#39640;&#25928;&#35757;&#32451;&#30340;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#22312;&#32463;&#20856;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#20013;&#20855;&#26377;&#20219;&#24847;&#24120;&#25968;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#20869;&#23384;&#20998;&#31163;&#12290;&#27599;&#20010;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21333;&#20803;&#37117;&#21487;&#20197;&#22312;&#24120;&#25968;&#26102;&#38388;&#20869;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#36827;&#34892;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#29702;&#35770;&#30740;&#31350;&#34920;&#26126;&#65292;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#35757;&#32451;&#24615;&#20043;&#38388;&#23384;&#22312;&#19968;&#31181;&#26222;&#36941;&#30340;&#26435;&#34913;&#65307;&#20316;&#20026;&#36825;&#20123;&#32467;&#26524;&#30340;&#25512;&#35770;&#65292;&#23454;&#38469;&#19978;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#23454;&#29616;&#25351;&#25968;&#32423;&#30340;&#36229;&#36234;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20998;&#31163;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#20026;&#36825;&#26679;&#30340;QNN&#35757;&#32451;&#26102;&#38388;&#22312;&#27169;&#22411;&#35268;&#27169;&#19978;&#26159;&#25351;&#25968;&#32423;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#23618;&#27425;&#32467;&#26500;&#30340;&#21487;&#39640;&#25928;&#35757;&#32451;&#30340;QNNs&#26469;&#32469;&#24320;&#36825;&#20123;&#36127;&#38754;&#32467;&#26524;&#65292;&#22312;&#25191;&#34892;&#32463;&#20856;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#26102;&#65292;&#36825;&#20123;QNNs&#21487;&#20197;&#23637;&#31034;&#20986;&#20219;&#24847;&#24120;&#25968;&#27425;&#25968;&#30340;&#22810;&#39033;&#24335;&#20869;&#23384;&#20998;&#31163;&#65292;&#19988;&#22312;&#37327;&#23376;&#35774;&#22791;&#19978;&#27599;&#20010;&#21333;&#20803;&#26684;&#37117;&#21487;&#20197;&#22312;&#24120;&#25968;&#26102;&#38388;&#20869;&#36827;&#34892;&#35745;&#31639;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#20998;&#31163;&#36866;&#29992;&#20110;&#21253;&#25324;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#22312;&#20869;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#32463;&#20856;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#37327;&#23376;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent theoretical results in quantum machine learning have demonstrated a general trade-off between the expressive power of quantum neural networks (QNNs) and their trainability; as a corollary of these results, practical exponential separations in expressive power over classical machine learning models are believed to be infeasible as such QNNs take a time to train that is exponential in the model size. We here circumvent these negative results by constructing a hierarchy of efficiently trainable QNNs that exhibit unconditionally provable, polynomial memory separations of arbitrary constant degree over classical neural networks in performing a classical sequence modeling task. Furthermore, each unit cell of the introduced class of QNNs is computationally efficient, implementable in constant time on a quantum device. The classical networks we prove a separation over include well-known examples such as recurrent neural networks and Transformers. We show that quantum contextuality is th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#36817;&#20284;&#26799;&#24230;&#26368;&#23567;&#21270;&#25237;&#24433;&#32676;&#20307;&#39118;&#38505;&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#20202;&#22120;&#21464;&#37327;&#22238;&#24402;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#31454;&#20105;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#22788;&#29702;&#20102;&#20108;&#20803;&#32467;&#26524;&#30340;&#24773;&#20917;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.05639</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#20202;&#22120;&#21464;&#37327;&#22238;&#24402;&#36890;&#36807;&#38543;&#26426;&#36817;&#20284;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Instrumental Variable Regression through Stochastic Approximate Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#36817;&#20284;&#26799;&#24230;&#26368;&#23567;&#21270;&#25237;&#24433;&#32676;&#20307;&#39118;&#38505;&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#20202;&#22120;&#21464;&#37327;&#22238;&#24402;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#31454;&#20105;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#22788;&#29702;&#20102;&#20108;&#20803;&#32467;&#26524;&#30340;&#24773;&#20917;&#65292;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;SAGD-IV&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#36817;&#20284;&#26799;&#24230;&#26469;&#26368;&#23567;&#21270;&#25237;&#24433;&#32676;&#20307;&#39118;&#38505;&#30340;&#26032;&#22411;&#38750;&#21442;&#25968;&#20202;&#22120;&#21464;&#37327;&#65288;NPIV&#65289;&#22238;&#24402;&#26694;&#26550;&#12290;&#20202;&#22120;&#21464;&#37327;&#65288;IV&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#65292;&#20197;&#35299;&#20915;&#22312;&#23384;&#22312;&#19981;&#21487;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#24773;&#20917;&#19979;&#30340;&#20272;&#35745;&#38382;&#39064;&#65292;&#24182;&#19988;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#33268;&#21147;&#20110;&#25913;&#36827;&#29616;&#26377;&#26041;&#27861;&#24182;&#22312;NPIV&#35774;&#32622;&#19979;&#35774;&#35745;&#26032;&#26041;&#27861;&#65292;&#35813;&#35774;&#32622;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#19981;&#36866;&#23450;&#30340;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#25105;&#20204;&#31639;&#27861;&#30340;&#29702;&#35770;&#25903;&#25345;&#65292;&#24182;&#36890;&#36807;&#23454;&#35777;&#23454;&#39564;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#20854;&#31454;&#20105;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22788;&#29702;&#20102;&#20108;&#20803;&#32467;&#26524;&#30340;&#24773;&#20917;&#65292;&#24182;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#32780;&#35813;&#24773;&#20917;&#22312;&#31038;&#21306;&#20013;&#27809;&#26377;&#24471;&#21040;&#19982;&#20854;&#36830;&#32493;&#23545;&#24212;&#29289;&#30340;&#21516;&#26679;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes SAGD-IV, a novel framework for conducting nonparametric instrumental variable (NPIV) regression by employing stochastic approximate gradients to minimize the projected populational risk. Instrumental Variables (IVs) are widely used in econometrics to address estimation problems in the presence of unobservable confounders, and the Machine Learning community has devoted significant effort to improving existing methods and devising new ones in the NPIV setting, which is known to be an ill-posed linear inverse problem. We provide theoretical support for our algorithm and further exemplify its competitive performance through empirical experiments. Furthermore, we address, with promising results, the case of binary outcomes, which has not received as much attention from the community as its continuous counterpart.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.04875</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Provable Length and Compositional Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#31561;&#22810;&#31181;&#26550;&#26500;&#65292;&#25506;&#32034;&#20102;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#35748;&#20026;&#23545;&#20110;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#65292;&#19981;&#21516;&#26550;&#26500;&#38656;&#35201;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#24230;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#26356;&#38271;&#24207;&#21015;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#32452;&#21512;&#27867;&#21270;&#8212;&#8212;&#23545;&#35757;&#32451;&#26102;&#26410;&#35265;&#21040;&#30340;&#20196;&#29260;&#32452;&#21512;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#22312;&#24207;&#21015;&#21040;&#24207;&#21015;&#27169;&#22411;&#20013;&#26159;&#37325;&#35201;&#30340;&#38750;&#20998;&#24067;&#21270;&#27867;&#21270;&#24418;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;&#28145;&#24230;&#38598;&#21512;&#12289;&#21464;&#21387;&#22120;&#12289;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#21644;&#31616;&#21333;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#19968;&#31995;&#21015;&#26550;&#26500;&#20013;&#65292;&#26397;&#30528;&#21487;&#35777;&#26126;&#30340;&#38271;&#24230;&#21644;&#32452;&#21512;&#27867;&#21270;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;&#26681;&#25454;&#26550;&#26500;&#30340;&#19981;&#21516;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#34920;&#31034;&#35782;&#21035;&#30340;&#24517;&#35201;&#24615;&#65292;&#20363;&#22914;&#19982;&#30495;&#23454;&#34920;&#31034;&#20855;&#26377;&#32447;&#24615;&#25110;&#25490;&#21015;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Length generalization -- the ability to generalize to longer sequences than ones seen during training, and compositional generalization -- the ability to generalize to token combinations not seen during training, are crucial forms of out-of-distribution generalization in sequence-to-sequence models. In this work, we take the first steps towards provable length and compositional generalization for a range of architectures, including deep sets, transformers, state space models, and simple recurrent neural nets. Depending on the architecture, we prove different degrees of representation identification, e.g., a linear or a permutation relation with ground truth representation, is necessary for length and compositional generalization.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#26657;&#20934;&#23545;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#23548;&#33268;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#65292;&#32780;&#36807;&#20110;&#33258;&#20449;&#30340;&#24773;&#20917;&#26377;&#21033;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861; (ConfTS)&#65292;&#20854;&#36890;&#36807;&#20248;&#21270;&#28201;&#24230;&#20540;&#26469;&#25913;&#36827;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.04344</link><description>&lt;p&gt;
&#21435;&#26657;&#20934;&#26159;&#21542;&#26377;&#21161;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#65311;
&lt;/p&gt;
&lt;p&gt;
Does Confidence Calibration Help Conformal Prediction?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21435;&#26657;&#20934;&#23545;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#23548;&#33268;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#65292;&#32780;&#36807;&#20110;&#33258;&#20449;&#30340;&#24773;&#20917;&#26377;&#21033;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;&#22522;&#20110;&#36825;&#19968;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19968;&#33268;&#24615;&#28201;&#24230;&#32553;&#25918;&#26041;&#27861; (ConfTS)&#65292;&#20854;&#36890;&#36807;&#20248;&#21270;&#28201;&#24230;&#20540;&#26469;&#25913;&#36827;&#19968;&#33268;&#24615;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25216;&#26415;&#65292;&#19968;&#33268;&#24615;&#39044;&#27979;&#26500;&#24314;&#20102;&#19968;&#32452;&#20855;&#26377;&#39640;&#27010;&#29575;&#21253;&#21547;&#30495;&#23454;&#26631;&#31614;&#30340;&#39044;&#27979;&#38598;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#37319;&#29992;&#28201;&#24230;&#32553;&#25918;&#26469;&#26657;&#20934;&#20998;&#31867;&#22120;&#65292;&#20551;&#35774;&#20449;&#24515;&#26657;&#20934;&#21487;&#20197;&#20026;&#19968;&#33268;&#24615;&#39044;&#27979;&#24102;&#26469;&#22909;&#22788;&#12290;&#26412;&#25991;&#39318;&#20808;&#34920;&#26126;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#20250;&#24847;&#22806;&#22320;&#23548;&#33268;&#26356;&#22823;&#30340;&#39044;&#27979;&#38598;&#65292;&#24182;&#25913;&#21892;&#20102;&#26657;&#20934;&#24615;&#33021;&#65292;&#32780;&#36807;&#20110;&#33258;&#20449;&#19988;&#28201;&#24230;&#36739;&#23567;&#30340;&#24773;&#20917;&#21017;&#26377;&#21161;&#20110;&#19968;&#33268;&#24615;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#35777;&#26126;&#39640;&#32622;&#20449;&#24230;&#20250;&#38477;&#20302;&#22312;&#39044;&#27979;&#38598;&#20013;&#28155;&#21152;&#26032;&#31867;&#30340;&#27010;&#29575;&#12290;&#21463;&#21040;&#36825;&#19968;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;$\textbf{&#19968;&#33268;&#24615;&#28201;&#24230;&#32553;&#25918;}$ (ConfTS)&#65292;&#36890;&#36807;&#38408;&#20540;&#19982;&#30495;&#23454;&#26631;&#31614;&#30340;&#38750;&#19968;&#33268;&#24615;&#20998;&#25968;&#20043;&#38388;&#30340;&#24046;&#36317;&#26469;&#20462;&#27491;&#30446;&#26631;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ConfTS&#30340;&#26032;&#30446;&#26631;&#23558;&#20351;&#28201;&#24230;&#20540;&#26397;&#30528;&#20248;&#21270;&#38598;&#30340;&#26041;&#21521;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conformal prediction, as an emerging uncertainty qualification technique, constructs prediction sets that are guaranteed to contain the true label with high probability. Previous works usually employ temperature scaling to calibrate the classifier, assuming that confidence calibration can benefit conformal prediction. In this work, we first show that post-hoc calibration methods surprisingly lead to larger prediction sets with improved calibration, while over-confidence with small temperatures benefits the conformal prediction performance instead. Theoretically, we prove that high confidence reduces the probability of appending a new class in the prediction set. Inspired by the analysis, we propose a novel method, $\textbf{Conformal Temperature Scaling}$ (ConfTS), which rectifies the objective through the gap between the threshold and the non-conformity score of the ground-truth label. In this way, the new objective of ConfTS will optimize the temperature value toward an optimal set th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;</title><link>https://arxiv.org/abs/2402.02322</link><description>&lt;p&gt;
&#21160;&#24577;&#22686;&#37327;&#20248;&#21270;&#29992;&#20110;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Dynamic Incremental Optimization for Best Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02322
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#31232;&#30095;&#23398;&#20064;&#38382;&#39064;&#30340;&#8220;&#40644;&#37329;&#26631;&#20934;&#8221;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#26469;&#25915;&#20987;&#36825;&#20010;&#38750;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;$\ell_0$&#27491;&#21017;&#21270;&#38382;&#39064;&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#22522;&#20110;&#21407;&#22987;&#38382;&#39064;&#21644;&#23545;&#20598;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21407;&#23545;&#20598;&#31639;&#27861;&#12290;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#23545;&#20598;&#33539;&#22260;&#20272;&#35745;&#21644;&#22686;&#37327;&#31574;&#30053;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#28508;&#22312;&#22320;&#20943;&#23569;&#20102;&#20887;&#20313;&#35745;&#31639;&#24182;&#25913;&#36827;&#20102;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23545;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#35299;&#20915;&#26041;&#26696;&#30340;&#25928;&#29575;&#21644;&#32479;&#35745;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Best subset selection is considered the `gold standard' for many sparse learning problems. A variety of optimization techniques have been proposed to attack this non-smooth non-convex problem. In this paper, we investigate the dual forms of a family of $\ell_0$-regularized problems. An efficient primal-dual algorithm is developed based on the primal and dual problem structures. By leveraging the dual range estimation along with the incremental strategy, our algorithm potentially reduces redundant computation and improves the solutions of best subset selection. Theoretical analysis and experiments on synthetic and real-world datasets validate the efficiency and statistical properties of the proposed solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2401.16412</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#32463;&#20856;&#32467;&#26524;&#65292;&#20219;&#20309;&#21512;&#29702;&#30340;&#20559;&#22909;&#25237;&#31080;&#26041;&#27861;&#26377;&#26102;&#20250;&#32473;&#20010;&#20307;&#25552;&#20379;&#25253;&#21578;&#19981;&#30495;&#23454;&#20559;&#22909;&#30340;&#28608;&#21169;&#12290;&#23545;&#20110;&#27604;&#36739;&#25237;&#31080;&#26041;&#27861;&#26469;&#35828;&#65292;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26356;&#25110;&#32773;&#26356;&#23569;&#25269;&#25239;&#36825;&#31181;&#31574;&#30053;&#24615;&#25805;&#32437;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#35268;&#27169;&#19979;&#23545;&#38480;&#21046;&#20449;&#24687;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#32473;&#23450;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#30340;&#25104;&#21151;&#31243;&#24230;&#26469;&#34913;&#37327;&#25805;&#32437;&#30340;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#23558;&#36817;40,000&#20010;&#19981;&#21516;&#35268;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23545;&#25239;8&#31181;&#19981;&#21516;&#30340;&#25237;&#31080;&#26041;&#27861;&#65292;&#22312;6&#31181;&#38480;&#21046;&#20449;&#24687;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#21253;&#21547;5-21&#21517;&#36873;&#27665;&#21644;3-6&#21517;&#20505;&#36873;&#20154;&#30340;&#22996;&#21592;&#20250;&#35268;&#27169;&#36873;&#20030;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#25237;&#31080;&#26041;&#27861;&#65292;&#22914;Borda&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;Instant Runoff&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#19968;&#20010;&#29702;&#24819;&#30340;&#25805;&#32437;&#32773;&#21033;&#28070;&#21270;&#25805;&#32437;&#65292;&#20294;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#19981;&#20250;&#21463;&#21040;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
&lt;/p&gt;</description></item><item><title>Ensembler&#26159;&#19968;&#20010;&#38450;&#27490;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#38598;&#25104;&#21644;&#24341;&#20837;&#25200;&#21160;&#30340;&#26041;&#24335;&#65292;&#22312;&#21327;&#20316;&#25512;&#29702;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>http://arxiv.org/abs/2401.10859</link><description>&lt;p&gt;
Ensembler:&#20351;&#29992;&#27169;&#22411;&#38598;&#25104;&#22312;&#21327;&#20316;&#25512;&#29702;&#36807;&#31243;&#20013;&#38450;&#27490;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Ensembler: Combating model inversion attacks using model ensemble during collaborative inference. (arXiv:2401.10859v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10859
&lt;/p&gt;
&lt;p&gt;
Ensembler&#26159;&#19968;&#20010;&#38450;&#27490;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#21487;&#25193;&#23637;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#27169;&#22411;&#38598;&#25104;&#21644;&#24341;&#20837;&#25200;&#21160;&#30340;&#26041;&#24335;&#65292;&#22312;&#21327;&#20316;&#25512;&#29702;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#24222;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#20419;&#20351;&#36793;&#32536;&#35774;&#22791;&#23558;&#25512;&#29702;&#36807;&#31243;&#30340;&#22823;&#37096;&#20998;&#36716;&#31227;&#21040;&#20113;&#31471;&#12290;&#34429;&#28982;&#36825;&#31181;&#20570;&#27861;&#24102;&#26469;&#20102;&#35768;&#22810;&#20248;&#21183;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#20851;&#20110;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#20113;&#26381;&#21153;&#22120;&#30340;&#21487;&#20449;&#24230;&#21463;&#21040;&#36136;&#30097;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#23454;&#29992;&#21644;&#36866;&#24212;&#24615;&#26041;&#27861;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Ensembler&#65292;&#36825;&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#22823;&#22823;&#22686;&#21152;&#23545;&#25239;&#26041;&#36827;&#34892;&#27169;&#22411;&#21453;&#28436;&#25915;&#20987;&#30340;&#38590;&#24230;&#12290;Ensembler&#21033;&#29992;&#22312;&#23545;&#25239;&#26381;&#21153;&#22120;&#19978;&#36816;&#34892;&#30340;&#27169;&#22411;&#32452;&#21512;&#65292;&#19982;&#29616;&#26377;&#30340;&#22312;&#21327;&#20316;&#25512;&#29702;&#36807;&#31243;&#20013;&#24341;&#20837;&#25200;&#21160;&#21040;&#25935;&#24863;&#25968;&#25454;&#30340;&#26041;&#27861;&#24182;&#34892;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#19982;&#22522;&#26412;&#30340;&#39640;&#26031;&#22122;&#22768;&#30456;&#32467;&#21512;&#65292;Ensembler&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#20445;&#25252;&#22270;&#20687;&#20813;&#21463;&#37325;&#24314;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have exhibited remarkable performance across various domains. Nevertheless, the burgeoning model sizes compel edge devices to offload a significant portion of the inference process to the cloud. While this practice offers numerous advantages, it also raises critical concerns regarding user data privacy. In scenarios where the cloud server's trustworthiness is in question, the need for a practical and adaptable method to safeguard data privacy becomes imperative. In this paper, we introduce Ensembler, an extensible framework designed to substantially increase the difficulty of conducting model inversion attacks for adversarial parties. Ensembler leverages model ensembling on the adversarial server, running in parallel with existing approaches that introduce perturbations to sensitive data during colloborative inference. Our experiments demonstrate that when combined with even basic Gaussian noise, Ensembler can effectively shield images from reconstruction attacks, 
&lt;/p&gt;</description></item><item><title>MSHyper&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#36229;&#22270;&#36716;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#31934;&#30830;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#36229;&#22270;&#21644;&#36229;&#36793;&#22270;&#65292;&#24182;&#20351;&#29992;&#19977;&#38454;&#27573;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20840;&#38754;&#30340;&#39640;&#38454;&#27169;&#24335;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.09261</link><description>&lt;p&gt;
MSHyper&#65306;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#22810;&#23610;&#24230;&#36229;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
MSHyper: Multi-Scale Hypergraph Transformer for Long-Range Time Series Forecasting. (arXiv:2401.09261v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09261
&lt;/p&gt;
&lt;p&gt;
MSHyper&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#36229;&#22270;&#36716;&#25442;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#31934;&#30830;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#36229;&#22270;&#21644;&#36229;&#36793;&#22270;&#65292;&#24182;&#20351;&#29992;&#19977;&#38454;&#27573;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#26356;&#20840;&#38754;&#30340;&#39640;&#38454;&#27169;&#24335;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#65292;&#21462;&#24471;&#20102;&#39046;&#20808;&#20110;&#20854;&#20182;&#26041;&#27861;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#26512;&#19981;&#21516;&#23610;&#24230;&#26102;&#38388;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#31934;&#30830;&#30340;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#32570;&#20047;&#23545;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#20419;&#36827;&#26356;&#20840;&#38754;&#30340;&#27169;&#24335;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#29992;&#20110;&#38271;&#26399;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#36229;&#22270;&#36716;&#25442;&#22120;&#65288;MSHyper&#65289;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25552;&#20379;&#39640;&#38454;&#27169;&#24335;&#30456;&#20114;&#20316;&#29992;&#24314;&#27169;&#30340;&#22522;&#30784;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#36229;&#36793;&#35270;&#20026;&#33410;&#28857;&#65292;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#19968;&#20010;&#36229;&#36793;&#22270;&#26469;&#22686;&#24378;&#36229;&#22270;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#19977;&#38454;&#27573;&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#26469;&#32858;&#21512;&#27169;&#24335;&#20449;&#24687;&#24182;&#23398;&#20064;&#19981;&#21516;&#23610;&#24230;&#26102;&#38388;&#27169;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#24378;&#24230;&#12290;&#22312;&#20116;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;MSHyper&#22312;&#22343;&#26041;&#35823;&#24046;(MSE)&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;(MAE)&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#20943;&#23567;&#20102;8.73%&#21644;7.15%&#30340;&#39044;&#27979;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Demystifying interactions between temporal patterns of different scales is fundamental to precise long-range time series forecasting. However, previous works lack the ability to model high-order interactions. To promote more comprehensive pattern interaction modeling for long-range time series forecasting, we propose a Multi-Scale Hypergraph Transformer (MSHyper) framework. Specifically, a multi-scale hypergraph is introduced to provide foundations for modeling high-order pattern interactions. Then by treating hyperedges as nodes, we also build a hyperedge graph to enhance hypergraph modeling. In addition, a tri-stage message passing mechanism is introduced to aggregate pattern information and learn the interaction strength between temporal patterns of different scales. Extensive experiments on five real-world datasets demonstrate that MSHyper achieves state-of-the-art performance, reducing prediction errors by an average of 8.73% and 7.15% over the best baseline in MSE and MAE, respec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#26469;&#23454;&#29616;&#27491;&#20132;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20248;&#21270;&#22240;&#23376;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#22312;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.02592</link><description>&lt;p&gt;
&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery. (arXiv:2401.02592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#26469;&#23454;&#29616;&#27491;&#20132;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20248;&#21270;&#22240;&#23376;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#22312;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#35299;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#36991;&#20813;&#23610;&#24230;&#27495;&#20041;&#24182;&#20415;&#20110;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#20248;&#21270;&#25152;&#35859;&#30340;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#65292;&#24378;&#21046;&#20351;&#22823;&#37096;&#20998;&#22240;&#23376;&#24444;&#27492;&#27491;&#20132;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#20132;&#32467;&#26500;&#65292;&#25105;&#20204;&#21033;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#65288;RGD&#65289;&#26469;&#20248;&#21270;Stiefel&#27969;&#24418;&#19978;&#30340;&#36825;&#20123;&#22240;&#23376;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;TT&#20998;&#35299;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;RGD&#30340;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#30528;&#24352;&#37327;&#38454;&#25968;&#30340;&#22686;&#21152;&#65292;&#25910;&#25947;&#36895;&#29575;&#20165;&#32463;&#21382;&#32447;&#24615;&#19979;&#38477;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24863;&#30693;&#38382;&#39064;&#65292;&#21363;&#20174;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;TT&#26684;&#24335;&#24352;&#37327;&#12290;&#20551;&#35774;&#24863;&#30693;&#31639;&#23376;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#65288;RIP&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#36890;&#36807;&#35889;&#21021;&#22987;&#21270;&#33719;&#24471;&#65292;RGD&#20063;&#20250;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide the first convergence guarantee for the factorization approach. Specifically, to avoid the scaling ambiguity and to facilitate theoretical analysis, we optimize over the so-called left-orthogonal TT format which enforces orthonormality among most of the factors. To ensure the orthonormal structure, we utilize the Riemannian gradient descent (RGD) for optimizing those factors over the Stiefel manifold. We first delve into the TT factorization problem and establish the local linear convergence of RGD. Notably, the rate of convergence only experiences a linear decline as the tensor order increases. We then study the sensing problem that aims to recover a TT format tensor from linear measurements. Assuming the sensing operator satisfies the restricted isometry property (RIP), we show that with a proper initialization, which could be obtained through spectral initialization, RGD also converges to the ground-truth tensor at a linear rate. Furthermore, we expand our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#37319;&#26679;&#29575;&#19982;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22312;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36866;&#24403;&#30340;&#37319;&#26679;&#29575;&#65292;&#20197;&#35299;&#20915;MLP&#20013;&#22122;&#22768;&#20266;&#24433;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01391</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#20301;&#32622;&#32534;&#30721;&#30340;MLP&#23398;&#20064;SDF&#30340;&#26368;&#20248;&#37319;&#26679;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
On Optimal Sampling for Learning SDF Using MLPs Equipped with Positional Encoding. (arXiv:2401.01391v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#37319;&#26679;&#29575;&#19982;&#23398;&#20064;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#22312;&#20613;&#37324;&#21494;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#23450;&#36866;&#24403;&#30340;&#37319;&#26679;&#29575;&#65292;&#20197;&#35299;&#20915;MLP&#20013;&#22122;&#22768;&#20266;&#24433;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#38544;&#24335;&#22330;&#65292;&#22914;&#24418;&#29366;&#30340;&#31070;&#32463;&#26377;&#31526;&#21495;&#36317;&#31163;&#22330;&#65288;SDF&#65289;&#65292;&#24050;&#25104;&#20026;&#35768;&#22810;&#24212;&#29992;&#20013;&#30340;&#24378;&#22823;&#34920;&#31034;&#26041;&#27861;&#65292;&#20363;&#22914;&#32534;&#30721;3D&#24418;&#29366;&#21644;&#25191;&#34892;&#30896;&#25758;&#26816;&#27979;&#12290;&#36890;&#24120;&#65292;&#38544;&#24335;&#22330;&#30001;&#24102;&#26377;&#20301;&#32622;&#32534;&#30721;&#65288;PE&#65289;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#36827;&#34892;&#32534;&#30721;&#20197;&#25429;&#25417;&#39640;&#39057;&#20960;&#20309;&#32454;&#33410;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24102;&#26377;PE&#30340;MLP&#30340;&#19968;&#20010;&#26174;&#33879;&#21103;&#20316;&#29992;&#26159;&#23398;&#20064;&#21040;&#30340;&#38544;&#24335;&#22330;&#20013;&#23384;&#22312;&#22122;&#22768;&#20266;&#24433;&#12290;&#23613;&#31649;&#22686;&#21152;&#37319;&#26679;&#29575;&#36890;&#24120;&#21487;&#20197;&#32531;&#35299;&#36825;&#20123;&#20266;&#24433;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20613;&#31435;&#21494;&#20998;&#26512;&#30340;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#31181;&#19981;&#33391;&#29616;&#35937;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#24037;&#20855;&#26469;&#30830;&#23450;&#23398;&#20064;&#31934;&#30830;&#31070;&#32463;&#38544;&#24335;&#22330;&#30340;&#36866;&#24403;&#37319;&#26679;&#29575;&#65292;&#32780;&#19981;&#20250;&#20135;&#29983;&#19981;&#33391;&#30340;&#21103;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#32593;&#32476;&#21709;&#24212;&#30340;&#20613;&#37324;&#21494;&#20998;&#26512;&#65292;&#29992;&#20110;&#20272;&#35745;&#24102;&#26377;&#38543;&#26426;&#26435;&#37325;&#30340;&#32473;&#23450;&#32593;&#32476;&#30340;&#20869;&#22312;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural implicit fields, such as the neural signed distance field (SDF) of a shape, have emerged as a powerful representation for many applications, e.g., encoding a 3D shape and performing collision detection. Typically, implicit fields are encoded by Multi-layer Perceptrons (MLP) with positional encoding (PE) to capture high-frequency geometric details. However, a notable side effect of such PE-equipped MLPs is the noisy artifacts present in the learned implicit fields. While increasing the sampling rate could in general mitigate these artifacts, in this paper we aim to explain this adverse phenomenon through the lens of Fourier analysis. We devise a tool to determine the appropriate sampling rate for learning an accurate neural implicit field without undesirable side effects. Specifically, we propose a simple yet effective method to estimate the intrinsic frequency of a given network with randomized weights based on the Fourier analysis of the network's responses. It is observed that
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;VAE&#20013;&#32534;&#30721;&#22120;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19653</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#25552;&#20379;&#30340;&#26080;&#38480;&#25968;&#25454;&#35745;&#21010;&#21319;&#32423;VAE&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Upgrading VAE Training With Unlimited Data Plans Provided by Diffusion Models. (arXiv:2310.19653v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19653
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#26377;&#25928;&#20943;&#36731;&#20102;VAE&#20013;&#32534;&#30721;&#22120;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#20854;&#32534;&#30721;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#30495;&#23454;&#65288;&#36830;&#32493;&#65289;&#25968;&#25454;&#20998;&#24067;$p_{\mathrm{data}}(\mathbf{x})$&#12290;&#19982;&#20043;&#30456;&#21453;&#65292;&#25193;&#25955;&#27169;&#22411;&#36890;&#36807;&#22266;&#23450;&#32534;&#30721;&#22120;&#36991;&#20813;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20351;&#24471;&#23427;&#20204;&#30340;&#34920;&#31034;&#19981;&#22826;&#21487;&#35299;&#37322;&#65292;&#20294;&#31616;&#21270;&#20102;&#35757;&#32451;&#65292;&#21487;&#20197;&#31934;&#30830;&#21644;&#36830;&#32493;&#22320;&#36924;&#36817;$p_{\mathrm{data}}(\mathbf{x})$&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#26679;&#26412;&#19978;&#35757;&#32451;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#36731;VAE&#20013;&#32534;&#30721;&#22120;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#36825;&#20123;&#32467;&#26524;&#26377;&#20123;&#20986;&#20154;&#24847;&#26009;&#65292;&#22240;&#20026;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#20351;&#29992;&#21478;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#26102;&#65292;&#29983;&#25104;&#24615;&#33021;&#20250;&#19979;&#38477;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#26041;&#27861;&#35757;&#32451;&#30340;VAE&#30340;&#27867;&#21270;&#24615;&#33021;&#12289;&#20998;&#25674;&#24046;&#36317;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational autoencoders (VAEs) are popular models for representation learning but their encoders are susceptible to overfitting (Cremer et al., 2018) because they are trained on a finite training set instead of the true (continuous) data distribution $p_{\mathrm{data}}(\mathbf{x})$. Diffusion models, on the other hand, avoid this issue by keeping the encoder fixed. This makes their representations less interpretable, but it simplifies training, enabling accurate and continuous approximations of $p_{\mathrm{data}}(\mathbf{x})$. In this paper, we show that overfitting encoders in VAEs can be effectively mitigated by training on samples from a pre-trained diffusion model. These results are somewhat unexpected as recent findings (Alemohammad et al., 2023; Shumailov et al., 2023) observe a decay in generative performance when models are trained on data generated by another generative model. We analyze generalization performance, amortization gap, and robustness of VAEs trained with our pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21516;&#26102;&#26041;&#31243;&#27169;&#22411;&#21644;&#25511;&#21046;&#20989;&#25968;&#19982;&#20998;&#24067;&#27010;&#25324;&#30340;&#26032;&#36830;&#25509;&#65292;&#35299;&#20915;&#20102;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24773;&#20917;&#19979;&#65292;&#38024;&#23545;&#19981;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.05805</link><description>&lt;p&gt;
&#25552;&#21319;&#25511;&#21046;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Boosted Control Functions. (arXiv:2310.05805v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24314;&#31435;&#21516;&#26102;&#26041;&#31243;&#27169;&#22411;&#21644;&#25511;&#21046;&#20989;&#25968;&#19982;&#20998;&#24067;&#27010;&#25324;&#30340;&#26032;&#36830;&#25509;&#65292;&#35299;&#20915;&#20102;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#24773;&#20917;&#19979;&#65292;&#38024;&#23545;&#19981;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20026;&#20174;&#22823;&#37327;&#30340;&#21327;&#21464;&#37327;&#20013;&#20934;&#30830;&#39044;&#27979;&#30446;&#26631;&#25968;&#37327;&#25171;&#24320;&#20102;&#22823;&#38376;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#38544;&#34255;&#28151;&#28102;&#30340;&#24773;&#20917;&#19979;&#12290;&#34429;&#28982;&#23545;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65288;&#20363;&#22914;&#20202;&#22120;&#21464;&#37327;&#65289;&#24050;&#32463;&#23545;&#38544;&#34255;&#28151;&#28102;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20294;&#23545;&#20110;&#39044;&#27979;&#20219;&#21153;&#26469;&#35828;&#24182;&#38750;&#22914;&#27492;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#35299;&#20915;&#22312;&#23384;&#22312;&#26410;&#35266;&#23519;&#21040;&#30340;&#28151;&#28102;&#30340;&#24773;&#20917;&#19979;&#65292;&#38024;&#23545;&#19981;&#21516;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#30340;&#39044;&#27979;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#20998;&#24067;&#27010;&#25324;&#39046;&#22495;&#65292;&#20197;&#21450;&#35745;&#37327;&#32463;&#27982;&#23398;&#20013;&#30340;&#21516;&#26102;&#26041;&#31243;&#27169;&#22411;&#21644;&#25511;&#21046;&#20989;&#25968;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#30340;&#26680;&#24515;&#26159;&#25551;&#36848;&#22312;&#19968;&#32452;&#20998;&#24067;&#36716;&#21464;&#19979;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#20998;&#24067;&#27010;&#25324;&#21516;&#26102;&#26041;&#31243;&#27169;&#22411;&#65288;SIMDGs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning methods and the availability of large-scale data opened the door to accurately predict target quantities from large sets of covariates. However, existing prediction methods can perform poorly when the training and testing data are different, especially in the presence of hidden confounding. While hidden confounding is well studied for causal effect estimation (e.g., instrumental variables), this is not the case for prediction tasks. This work aims to bridge this gap by addressing predictions under different training and testing distributions in the presence of unobserved confounding. In particular, we establish a novel connection between the field of distribution generalization from machine learning, and simultaneous equation models and control function from econometrics. Central to our contribution are simultaneous equation models for distribution generalization (SIMDGs) which describe the data-generating process under a set of distributional shifts. Within thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02211</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Machine Learning with Multi-source Data. (arXiv:2309.02211v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#30446;&#26631;&#20998;&#24067;&#19982;&#28304;&#25968;&#25454;&#38598;&#19981;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#36739;&#24046;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#20248;&#21270;&#20851;&#20110;&#30446;&#26631;&#20998;&#24067;&#31867;&#30340;&#21487;&#35299;&#37322;&#26041;&#24046;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#25913;&#21892;&#20102;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26159;&#28304;&#25968;&#25454;&#38598;&#26465;&#20214;&#32467;&#26524;&#27169;&#22411;&#30340;&#21152;&#26435;&#24179;&#22343;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#37492;&#21035;&#32467;&#26524;&#26469;&#25552;&#39640;&#20219;&#24847;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#26657;&#27491;&#20272;&#35745;&#22120;&#26469;&#20272;&#35745;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;c&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09797</link><description>&lt;p&gt;
&#20855;&#26377;&#19968;&#33268;&#32858;&#21512;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting with Coherent Aggregation. (arXiv:2307.09797v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09797
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#26469;&#20135;&#29983;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#12290;&#27169;&#22411;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26679;&#26412;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#39044;&#27979;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#20934;&#30830;&#33719;&#24471;&#36981;&#23432;&#23618;&#27425;&#32467;&#26500;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#36816;&#33829;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33021;&#28304;&#31649;&#29702;&#12289;&#20379;&#24212;&#38142;&#35268;&#21010;&#21644;&#36164;&#28304;&#37197;&#32622;&#31561;&#39046;&#22495;&#12290;&#23545;&#20110;&#22810;&#21464;&#37327;&#39044;&#27979;&#65292;&#22522;&#26412;&#25361;&#25112;&#22312;&#20110;&#39044;&#27979;&#36890;&#24120;&#38656;&#35201;&#19982;&#23618;&#27425;&#32467;&#26500;&#20445;&#25345;&#19968;&#33268;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#21033;&#29992;&#22240;&#23376;&#27169;&#22411;&#32467;&#26500;&#36890;&#36807;&#26500;&#24314;&#26469;&#20135;&#29983;&#19968;&#33268;&#30340;&#39044;&#27979;&#12290;&#36825;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#35266;&#23519;&#32467;&#26524;&#65288;&#21487;&#20132;&#25442;&#24615;&#65289;&#65306;&#32622;&#25442;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22522;&#26412;&#32423;&#21035;&#24207;&#21015;&#19981;&#20250;&#25913;&#21464;&#23427;&#20204;&#30340;&#32858;&#21512;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#29983;&#25104;&#22240;&#23376;&#12289;&#23427;&#20204;&#30340;&#21152;&#36733;&#21644;&#22522;&#26412;&#32423;&#21035;&#20998;&#24067;&#30340;&#21442;&#25968;&#65307;&#23427;&#20135;&#29983;&#21487;&#20197;&#26681;&#25454;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#24494;&#20998;&#30340;&#26679;&#26412;&#65307;&#22240;&#27492;&#23427;&#21487;&#20197;&#23545;&#20219;&#20309;&#22522;&#20110;&#26679;&#26412;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#21253;&#25324;&#36830;&#32493;&#25490;&#21517;&#27010;&#29575;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Obtaining accurate probabilistic forecasts while respecting hierarchical information is an important operational challenge in many applications, perhaps most obviously in energy management, supply chain planning, and resource allocation. The basic challenge, especially for multivariate forecasting, is that forecasts are often required to be coherent with respect to the hierarchical structure. In this paper, we propose a new model which leverages a factor model structure to produce coherent forecasts by construction. This is a consequence of a simple (exchangeability) observation: permuting \textit{}base-level series in the hierarchy does not change their aggregates. Our model uses a convolutional neural network to produce parameters for the factors, their loadings and base-level distributions; it produces samples which can be differentiated with respect to the model's parameters; and it can therefore optimize for any sample-based loss function, including the Continuous Ranked Probabili
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#21644;&#20449;&#24687;&#22686;&#30410;&#25928;&#29992;&#30340;&#21464;&#20998;&#24207;&#21015;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#26368;&#20248;&#35774;&#35745;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;OED&#38382;&#39064;&#65292;&#32467;&#26524;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#23569;&#30340;&#21069;&#21521;&#27169;&#22411;&#27169;&#25311;&#27425;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.10430</link><description>&lt;p&gt;
&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21464;&#20998;&#24207;&#21015;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational Sequential Optimal Experimental Design using Reinforcement Learning. (arXiv:2306.10430v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26694;&#26550;&#21644;&#20449;&#24687;&#22686;&#30410;&#25928;&#29992;&#30340;&#21464;&#20998;&#24207;&#21015;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#27714;&#35299;&#26368;&#20248;&#35774;&#35745;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;OED&#38382;&#39064;&#65292;&#32467;&#26524;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#26356;&#23569;&#30340;&#21069;&#21521;&#27169;&#22411;&#27169;&#25311;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#21464;&#20998;&#24207;&#21015;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745; (vsOED) &#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#21644;&#20449;&#24687;&#22686;&#30410;&#25928;&#29992;&#26469;&#26368;&#20248;&#22320;&#35774;&#35745;&#26377;&#38480;&#24207;&#21015;&#30340;&#23454;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#36817;&#20284;&#36125;&#21494;&#26031;&#21518;&#39564;&#30340;&#19979;&#30028;&#20272;&#35745;&#26399;&#26395;&#25928;&#29992;&#12290;&#36890;&#36807;&#21516;&#26102;&#26368;&#22823;&#21270;&#21464;&#20998;&#19979;&#30028;&#21644;&#25191;&#34892;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#26469;&#25968;&#20540;&#35299;&#20915;&#26368;&#20248;&#35774;&#35745;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#31995;&#21015;&#38754;&#21521;&#21442;&#25968;&#25512;&#26029;&#12289;&#27169;&#22411;&#21306;&#20998;&#21644;&#30446;&#26631;&#23548;&#21521;&#39044;&#27979;&#30340;OED&#38382;&#39064;&#12290;&#36825;&#20123;&#26696;&#20363;&#28085;&#30422;&#20102;&#26174;&#24335;&#21644;&#38544;&#24335;&#20284;&#28982;&#20989;&#25968;&#12289;&#40635;&#28902;&#21442;&#25968;&#21644;&#22522;&#20110;&#29289;&#29702;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;vsOED&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20197;&#21069;&#30340;&#39034;&#24207;&#35774;&#35745;&#31639;&#27861;&#30456;&#27604;&#65292;&#26679;&#26412;&#25928;&#29575;&#22823;&#22823;&#25552;&#39640;&#65292;&#25152;&#38656;&#21069;&#21521;&#27169;&#22411;&#27169;&#25311;&#27425;&#25968;&#20943;&#23569;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce variational sequential Optimal Experimental Design (vsOED), a new method for optimally designing a finite sequence of experiments under a Bayesian framework and with information-gain utilities. Specifically, we adopt a lower bound estimator for the expected utility through variational approximation to the Bayesian posteriors. The optimal design policy is solved numerically by simultaneously maximizing the variational lower bound and performing policy gradient updates. We demonstrate this general methodology for a range of OED problems targeting parameter inference, model discrimination, and goal-oriented prediction. These cases encompass explicit and implicit likelihoods, nuisance parameters, and physics-based partial differential equation models. Our vsOED results indicate substantially improved sample efficiency and reduced number of forward model simulations compared to previous sequential design algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#26174;&#24335;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10189</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#23398;&#20064;&#39640;&#32500;&#38750;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning High-Dimensional Nonparametric Differential Equations via Multivariate Occupation Kernel Functions. (arXiv:2306.10189v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#26174;&#24335;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;$d$&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;$n$&#20010;&#36712;&#36857;&#24555;&#29031;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#31995;&#32479;&#38656;&#35201;&#23398;&#20064;$d$&#20010;&#20989;&#25968;&#12290;&#38500;&#38750;&#20855;&#26377;&#39069;&#22806;&#30340;&#31995;&#32479;&#23646;&#24615;&#30693;&#35782;&#65292;&#20363;&#22914;&#31232;&#30095;&#24615;&#21644;&#23545;&#31216;&#24615;&#65292;&#21542;&#21017;&#26174;&#24335;&#30340;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21521;&#37327;&#20540;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25552;&#20379;&#30340;&#38544;&#24335;&#20844;&#24335;&#23398;&#20064;&#30340;&#32447;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;ODE&#37325;&#20889;&#20026;&#26356;&#24369;&#30340;&#31215;&#20998;&#24418;&#24335;&#65292;&#25105;&#20204;&#38543;&#21518;&#36827;&#34892;&#26368;&#23567;&#21270;&#24182;&#25512;&#23548;&#20986;&#25105;&#20204;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#21521;&#37327;&#22330;&#20381;&#36182;&#20110;&#19982;&#35299;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;$d$&#21487;&#33021;&#36229;&#36807;100&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20174;&#22270;&#20687;&#25968;&#25454;&#23398;&#20064;&#38750;&#21442;&#25968;&#19968;&#38454;&#25311;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a nonparametric system of ordinary differential equations (ODEs) from $n$ trajectory snapshots in a $d$-dimensional state space requires learning $d$ functions of $d$ variables. Explicit formulations scale quadratically in $d$ unless additional knowledge about system properties, such as sparsity and symmetries, is available. In this work, we propose a linear approach to learning using the implicit formulation provided by vector-valued Reproducing Kernel Hilbert Spaces. By rewriting the ODEs in a weaker integral form, which we subsequently minimize, we derive our learning algorithm. The minimization problem's solution for the vector field relies on multivariate occupation kernel functions associated with the solution trajectories. We validate our approach through experiments on highly nonlinear simulated and real data, where $d$ may exceed 100. We further demonstrate the versatility of the proposed method by learning a nonparametric first order quasilinear partial differential 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#25968;&#23383;&#29305;&#24449;&#32534;&#30721;&#20026;&#22522;&#20989;&#25968;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22240;&#23376;&#26426;&#20013;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22240;&#23376;&#26426;&#20013;&#65292;&#21487;&#20197;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14528</link><description>&lt;p&gt;
&#22522;&#20989;&#25968;&#32534;&#30721;&#25913;&#21892;&#22240;&#23376;&#26426;&#20013;&#25968;&#23383;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Basis Function Encoding of Numerical Features in Factorization Machines for Improved Accuracy. (arXiv:2305.14528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#25968;&#23383;&#29305;&#24449;&#32534;&#30721;&#20026;&#22522;&#20989;&#25968;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22240;&#23376;&#26426;&#20013;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22240;&#23376;&#26426;&#20013;&#65292;&#21487;&#20197;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#26426;(FM)&#21464;&#20307;&#34987;&#24191;&#27867;&#29992;&#20110;&#22823;&#35268;&#27169;&#23454;&#26102;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25512;&#29702;&#30340;&#20302;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#25552;&#20379;&#20102;&#20986;&#33394;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#12289;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#29305;&#24449;&#32534;&#30721;&#20026;&#25152;&#36873;&#20989;&#25968;&#38598;&#30340;&#20989;&#25968;&#20540;&#21521;&#37327;&#23558;&#25968;&#20540;&#29305;&#24449;&#32435;&#20837;FM&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factorization machine (FM) variants are widely used for large scale real-time content recommendation systems, since they offer an excellent balance between model accuracy and low computational costs for training and inference. These systems are trained on tabular data with both numerical and categorical columns. Incorporating numerical columns poses a challenge, and they are typically incorporated using a scalar transformation or binning, which can be either learned or chosen a-priori. In this work, we provide a systematic and theoretically-justified way to incorporate numerical features into FM variants by encoding them into a vector of function values for a set of functions of one's choice.  We view factorization machines as approximators of segmentized functions, namely, functions from a field's value to the real numbers, assuming the remaining fields are assigned some given constants, which we refer to as the segment. From this perspective, we show that our technique yields a model
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;</title><link>http://arxiv.org/abs/2303.10446</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Content Adaptive Learnable Time-Frequency Representation For Audio Signal Processing. (arXiv:2303.10446v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10446
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21487;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#28388;&#27874;&#22120;&#19982;&#21464;&#25442;&#22120;&#26550;&#26500;&#26469;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#20869;&#23481;&#33258;&#36866;&#24212;&#21069;&#31471;&#65292;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#22788;&#29702;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#29616;&#20195;&#20986;&#29616;&#20043;&#21069;&#65292;&#25105;&#20204;&#20351;&#29992;&#22266;&#23450;&#34920;&#31034;&#30340;&#12289;&#19981;&#21487;&#23398;&#20064;&#30340;&#21069;&#31471;&#65292;&#22914;&#35889;&#22270;&#25110;&#26757;&#23572;&#35889;&#22270;&#65292;&#24102;/&#19981;&#24102;&#31070;&#32463;&#32467;&#26500;&#12290;&#38543;&#30528;&#21367;&#31215;&#26550;&#26500;&#25903;&#25345;ASR&#21644;&#22768;&#23398;&#22330;&#26223;&#29702;&#35299;&#31561;&#21508;&#31181;&#24212;&#29992;&#65292;&#36716;&#21521;&#21487;&#23398;&#20064;&#21069;&#31471;&#65292;&#21363;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#21644;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#25152;&#38656;&#30340;&#22522;&#30784;&#20989;&#25968;&#21644;&#26435;&#37325;&#12290;&#22312;&#27809;&#26377;&#21367;&#31215;&#22359;&#30340;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#65292;&#32447;&#24615;&#23618;&#23558;&#23567;&#30340;&#27874;&#24418;&#22359;&#25237;&#24433;&#21040;&#23567;&#30340;&#28508;&#22312;&#32500;&#24230;&#19978;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#39304;&#36865;&#21040;&#21464;&#24418;&#22120;&#26550;&#26500;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#20869;&#23481;&#33258;&#36866;&#24212;&#23398;&#20064;&#26102;&#39057;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a learnable content adaptive front end for audio signal processing. Before the modern advent of deep learning, we used fixed representation non-learnable front-ends like spectrogram or mel-spectrogram with/without neural architectures. With convolutional architectures supporting various applications such as ASR and acoustic scene understanding, a shift to a learnable front ends occurred in which both the type of basis functions and the weight were learned from scratch and optimized for the particular task of interest. With the shift to transformer-based architectures with no convolutional blocks present, a linear layer projects small waveform patches onto a small latent dimension before feeding them to a transformer architecture. In this work, we propose a way of computing a content-adaptive learnable time-frequency representation. We pass each audio signal through a bank of convolutional filters, each giving a fixed-dimensional vector. It is akin to learning a bank of finit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#20026;2&#30340;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#36275;&#22815;&#24179;&#28369;&#20984;&#30340;&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;SGD&#21487;&#20197;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#36807;&#31243;&#20013;&#24341;&#20837;Frobenius&#33539;&#25968;&#27491;&#21017;&#21270;&#19982;&#24688;&#24403;&#20998;&#24067;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#65292;&#21516;&#26102;&#25299;&#23637;&#20102;&#36830;&#32493;&#26102;&#38388;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.11452</link><description>&lt;p&gt;
&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#19978;SGD&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Global Convergence of SGD On Two Layer Neural Nets. (arXiv:2210.11452v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11452
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#20026;2&#30340;&#31070;&#32463;&#32593;&#32476;&#37319;&#29992;&#36275;&#22815;&#24179;&#28369;&#20984;&#30340;&#28608;&#27963;&#20989;&#25968;&#26102;&#65292;SGD&#21487;&#20197;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#65292;&#35777;&#26126;&#36807;&#31243;&#20013;&#24341;&#20837;Frobenius&#33539;&#25968;&#27491;&#21017;&#21270;&#19982;&#24688;&#24403;&#20998;&#24067;&#30340;&#21442;&#25968;&#21021;&#22987;&#21270;&#65292;&#21516;&#26102;&#25299;&#23637;&#20102;&#36830;&#32493;&#26102;&#38388;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#28145;&#24230;&#20026;2&#30340;&#32593;&#32476;&#37319;&#29992;&#36275;&#22815;&#24179;&#28369;&#19988;&#26377;&#36793;&#30028;&#30340;&#28608;&#27963;&#20989;&#25968;&#65288;&#27604;&#22914;sigmoid&#21644;tanh&#65289;&#26102;&#65292;SGD&#21487;&#20197;&#35777;&#26126;&#24615;&#22320;&#25910;&#25947;&#21040;&#36866;&#24403;&#27491;&#21017;&#21270;&#30340;$\ell_2-$&#32463;&#39564;&#39118;&#38505;&#30340;&#20840;&#23616;&#26368;&#23567;&#20540;--&#23545;&#20110;&#20219;&#24847;&#25968;&#25454;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#38376;&#12290;&#25105;&#20204;&#22312;[1]&#30340;&#30740;&#31350;&#25104;&#26524;&#19978;&#36827;&#34892;&#20102;&#25193;&#23637;&#65292;&#24182;&#22312;&#26435;&#37325;&#19978;&#28155;&#21152;&#20102;&#24658;&#23450;&#37327;&#30340;Frobenius&#33539;&#25968;&#27491;&#21017;&#21270;&#65292;&#21516;&#26102;&#36873;&#21462;&#20102;&#24688;&#24403;&#30340;&#20998;&#24067;&#23545;&#21021;&#22987;&#26435;&#37325;&#36827;&#34892;&#37319;&#26679;&#12290;&#25105;&#20204;&#36824;&#32473;&#20986;&#20102;&#19968;&#20010;&#36830;&#32493;&#26102;&#38388;&#30340;SGD&#25910;&#25947;&#32467;&#26524;&#65292;&#21516;&#26679;&#36866;&#29992;&#20110;&#22914;SoftPlus&#36825;&#26679;&#30340;&#24179;&#28369;&#26080;&#36793;&#30028;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#23637;&#31034;&#20102;&#23384;&#22312;&#20110;&#22266;&#23450;&#22823;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#20204;&#26159;&#8220;Villani&#20989;&#25968;&#8221;[1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning rates and schr\"odinger operators, 2020. arXiv:2004.06977
&lt;/p&gt;
&lt;p&gt;
In this note we demonstrate provable convergence of SGD to the global minima of appropriately regularized $\ell_2-$empirical risk of depth $2$ nets -- for arbitrary data and with any number of gates, if they are using adequately smooth and bounded activations like sigmoid and tanh. We build on the results in [1] and leverage a constant amount of Frobenius norm regularization on the weights, along with sampling of the initial weights from an appropriate distribution. We also give a continuous time SGD convergence result that also applies to smooth unbounded activations like SoftPlus. Our key idea is to show the existence loss functions on constant sized neural nets which are "Villani Functions". [1] Bin Shi, Weijie J. Su, and Michael I. Jordan. On learning rates and schr\"odinger operators, 2020. arXiv:2004.06977
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;Transformer&#32467;&#26500;&#21644;&#30334;&#19975;&#32423;&#26679;&#26412;&#19978;&#19979;&#25991;&#36827;&#34892;&#21407;&#22987;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#26550;&#26500;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24314;&#27169;&#38899;&#39057;&#20449;&#21495;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.08297</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;Transformer&#32467;&#26500;&#24182;&#21033;&#29992;&#30334;&#19975;&#32423;&#26679;&#26412;&#19978;&#19979;&#25991;&#36827;&#34892;&#21407;&#22987;&#38899;&#39057;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Language Model With Million Sample Context For Raw Audio Using Transformer Architectures. (arXiv:2206.08297v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;Transformer&#32467;&#26500;&#21644;&#30334;&#19975;&#32423;&#26679;&#26412;&#19978;&#19979;&#25991;&#36827;&#34892;&#21407;&#22987;&#38899;&#39057;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#26550;&#26500;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#24314;&#27169;&#38899;&#39057;&#20449;&#21495;&#30340;&#38271;&#26399;&#20381;&#36182;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38899;&#39057;&#20449;&#21495;&#36827;&#34892;&#38271;&#26399;&#20381;&#36182;&#24615;&#24314;&#27169;&#26159;&#19968;&#20010;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#21363;&#20351;&#22312;&#23567;&#30340;&#26102;&#38388;&#23610;&#24230;&#19978;&#65292;&#20063;&#20250;&#20135;&#29983;&#25968;&#21313;&#19975;&#20010;&#26679;&#26412;&#12290;&#26368;&#36817;&#65292;&#38543;&#30528;Transformer&#30340;&#20986;&#29616;&#65292;&#31070;&#32463;&#32467;&#26500;&#21464;&#24471;&#25797;&#38271;&#20110;&#23545;&#38271;&#26399;&#20381;&#36182;&#24615;&#24314;&#27169;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#20108;&#27425;&#32422;&#26463;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#33258;&#22238;&#24402;&#26550;&#26500;&#65292;&#21487;&#20197;&#27169;&#25311;&#30456;&#24403;&#22823;&#30340;&#19978;&#19979;&#25991;&#36229;&#36807;500,000&#20010;&#26679;&#26412;&#30340;&#38899;&#39057;&#27874;&#24418;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;CNN&#21069;&#31471;&#26469;&#23398;&#20064;&#28508;&#22312;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;Transformer&#32534;&#30721;&#22120;&#22312;&#36825;&#20123;&#34920;&#31034;&#20043;&#19978;&#23398;&#20064;&#20381;&#36182;&#39033;&#65292;&#23436;&#20840;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#20102;&#35757;&#32451;&#65306;&#20174;&#32780;&#20801;&#35768;&#23427;&#26681;&#25454;&#19979;&#19968;&#20010;&#26679;&#26412;&#33258;&#34892;&#23398;&#20064;&#34920;&#31034;&#12290;&#19982;&#20197;&#21069;&#29992;&#19981;&#21516;&#30340;&#26102;&#38388;&#23610;&#24230;&#36827;&#34892;&#27604;&#36739;&#20197;&#23637;&#31034;&#25913;&#36827;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#30456;&#21516;&#25968;&#30446;&#30340;&#21442;&#25968;/&#19978;&#19979;&#25991;&#26174;&#31034;&#20102;&#25913;&#36827;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling long-term dependencies for audio signals is a particularly challenging problem, as even small-time scales yield on the order of a hundred thousand samples. With the recent advent of Transformers, neural architectures became good at modeling dependencies over longer time scales, but they suffered from quadratic constraints to scale them. We propose a generative auto-regressive architecture that can model audio waveforms over quite a large context, greater than 500,000 samples. Our work is adapted to learn time dependencies by learning a latent representation by a CNN front-end, and then learning dependencies over these representations using Transformer encoders, fully trained end-to-end: thereby allowing to learn representations as it deems fit for the next sample. Unlike previous works that compared different time scales to show improvement, we use a standard dataset, with the same number of parameters/context to show improvements. We achieve a state-of-the-art performance as 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cal-PIT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#35786;&#26029;&#21644;&#26657;&#20934;&#38382;&#39064;&#65292;&#26469;&#23454;&#29616;&#26377;&#26465;&#20214;&#26657;&#20934;&#12290;</title><link>http://arxiv.org/abs/2205.14568</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#23454;&#29616;&#26377;&#26465;&#20214;&#26657;&#20934;&#30340;&#39044;&#27979;&#20998;&#24067;&#65306;&#22312;&#38134;&#27827;&#32418;&#31227;&#20272;&#35745;&#21644;&#27010;&#29575;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conditionally Calibrated Predictive Distributions by Probability-Probability Map: Application to Galaxy Redshift Estimation and Probabilistic Forecasting. (arXiv:2205.14568v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cal-PIT&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#65292;&#35299;&#20915;&#20102;&#39044;&#27979;&#20998;&#24067;&#30340;&#35786;&#26029;&#21644;&#26657;&#20934;&#38382;&#39064;&#65292;&#26469;&#23454;&#29616;&#26377;&#26465;&#20214;&#26657;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#35780;&#20272;AI&#31639;&#27861;&#30340;&#39044;&#27979;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#25551;&#36848;&#30446;&#26631;&#21464;&#37327;$y \in \mathbb{R}$&#22312;&#32473;&#23450;&#22797;&#26434;&#36755;&#20837;&#29305;&#24449;$\mathbf{x} \in \mathcal{X}$&#30340;&#26465;&#20214;&#19979;&#30340;&#39044;&#27979;&#20998;&#24067;$F(y|\mathbf{x})$&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#39044;&#27979;&#20998;&#24067;&#65288;&#20363;&#22914;&#65292;&#24402;&#19968;&#21270;&#27969;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65289;&#24448;&#24448;&#32570;&#20047;&#26465;&#20214;&#26657;&#20934;&#65292;&#21363;&#32473;&#23450;&#36755;&#20837;$\mathbf{x}$&#30340;&#20107;&#20214;&#21457;&#29983;&#30340;&#27010;&#29575;&#19982;&#39044;&#27979;&#27010;&#29575;&#26174;&#33879;&#19981;&#21516;&#12290;&#24403;&#21069;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#33021;&#23436;&#20840;&#35780;&#20272;&#21644;&#23454;&#26045;&#26377;&#26465;&#20214;&#26657;&#20934;&#30340;&#39044;&#27979;&#20998;&#24067;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Cal-PIT&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20174;&#26657;&#20934;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#27010;&#29575;-&#27010;&#29575;&#26144;&#23556;&#26469;&#21516;&#26102;&#35299;&#20915;&#39044;&#27979;&#20998;&#24067;&#30340;&#35786;&#26029;&#21644;&#26657;&#20934;&#38382;&#39064;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#23545;&#27010;&#29575;&#31215;&#20998;&#21464;&#25442;&#20998;&#25968;&#36827;&#34892;$\mathbf{x}$&#30340;&#22238;&#24402;&#12290;&#20272;&#35745;&#30340;&#22238;&#24402;&#25552;&#20379;&#20102;&#23545;&#29305;&#24449;&#31354;&#38388;&#20013;&#26465;&#20214;&#35206;&#30422;&#30340;&#21487;&#35299;&#37322;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty quantification is crucial for assessing the predictive ability of AI algorithms. Much research has been devoted to describing the predictive distribution (PD) $F(y|\mathbf{x})$ of a target variable $y \in \mathbb{R}$ given complex input features $\mathbf{x} \in \mathcal{X}$. However, off-the-shelf PDs (from, e.g., normalizing flows and Bayesian neural networks) often lack conditional calibration with the probability of occurrence of an event given input $\mathbf{x}$ being significantly different from the predicted probability. Current calibration methods do not fully assess and enforce conditionally calibrated PDs. Here we propose \texttt{Cal-PIT}, a method that addresses both PD diagnostics and recalibration by learning a single probability-probability map from calibration data. The key idea is to regress probability integral transform scores against $\mathbf{x}$. The estimated regression provides interpretable diagnostics of conditional coverage across the feature space. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#30340;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#20505;&#36873;&#29238;&#33410;&#28857;&#38598;&#21512;&#30340;&#30830;&#23450;&#21644;&#20998;&#35299;&#65292;&#20197;&#21450;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#33021;&#22815;&#22312;&#27604;&#29305;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#35299;&#20915;&#22522;&#20110;&#35780;&#20998;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2006.06926</link><description>&lt;p&gt;
&#29992;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#23398;&#20064;&#36125;&#21494;&#26031;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Learning Bayesian Networks with Annealing Machine. (arXiv:2006.06926v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.06926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#30340;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#20505;&#36873;&#29238;&#33410;&#28857;&#38598;&#21512;&#30340;&#30830;&#23450;&#21644;&#20998;&#35299;&#65292;&#20197;&#21450;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#30340;&#35299;&#20915;&#65292;&#33021;&#22815;&#22312;&#27604;&#29305;&#23481;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#22320;&#35299;&#20915;&#22522;&#20110;&#35780;&#20998;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#26377;&#28508;&#21147;&#29992;&#20110;&#22522;&#20110;&#35780;&#20998;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#32467;&#26500;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#30340;&#27604;&#29305;&#23481;&#37327;&#30446;&#21069;&#26377;&#38480;&#12290;&#20026;&#20102;&#21033;&#29992;&#27169;&#25311;&#36864;&#28779;&#25216;&#26415;&#65292;&#38656;&#35201;&#23558;&#22522;&#20110;&#35780;&#20998;&#30340;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#22312;&#27604;&#29305;&#23481;&#37327;&#20869;&#30340;&#20108;&#27425;&#26080;&#32422;&#26463;&#20108;&#20803;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36716;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#36827;&#30340;&#20505;&#36873;&#29238;&#33410;&#28857;&#38598;&#21512;&#30340;&#30830;&#23450;&#21644;&#20854;&#20998;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#25972;&#25968;&#35268;&#21010;&#38382;&#39064;&#65292;&#20197;&#25214;&#21040;&#26368;&#23567;&#21270;&#25152;&#38656;&#27604;&#29305;&#25968;&#30340;&#20998;&#35299;&#12290;&#22312;&#21253;&#21547;&#21464;&#37327;&#20174;75&#21040;223&#30340;7&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25152;&#38656;&#30340;&#27604;&#29305;&#25968;&#27604;&#22235;&#20195;&#23500;&#22763;&#36890;&#25968;&#23383;&#36864;&#28779;&#22120;&#65288;&#19968;&#31181;&#37319;&#29992;&#21322;&#23548;&#20307;&#25216;&#26415;&#24320;&#21457;&#30340;&#20840;&#32806;&#21512;&#27169;&#25311;&#36864;&#28779;&#26426;&#22120;&#65289;&#30340;100K&#27604;&#29305;&#23481;&#37327;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies have reported that annealing machines are capable of solving combinatorial optimization problems with high accuracy. Annealing machines can potentially be applied to score-based Bayesian network structure learning. However, the bit capacity of an annealing machine is currently limited. To utilize the annealing technology, converting score-based learning problems into quadratic unconstrained binary optimizations within the bit capacity is necessary. In this paper, we propose an efficient conversion method with the advanced identification of candidate parent sets and their decomposition. We also provide an integer programming problem to find the decomposition that minimizes the number of required bits. Experimental results on $7$ benchmark datasets with variables from $75$ to $223$ show that our approach requires less bits than the $100$K bit capacity of the fourth-generation Fujitsu Digital Annealer, a fully coupled annealing machine developed with semiconductor technolog
&lt;/p&gt;</description></item></channel></rss>