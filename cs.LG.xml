<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#8220;&#24369;&#20449;&#21495;&#20998;&#26512;&#8221;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24448;&#36820;&#35774;&#35745;&#23545;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22823;&#37096;&#20998;&#22870;&#21169;&#35823;&#24046;&#20026;&#27491;&#30456;&#20851;&#26102;&#65292;&#24448;&#36820;&#35774;&#35745;&#27604;&#27599;&#26085;&#20999;&#25442;&#31574;&#30053;&#26356;&#26377;&#25928;&#65292;&#22686;&#21152;&#25919;&#31574;&#20999;&#25442;&#39057;&#29575;&#21487;&#20197;&#38477;&#20302;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;</title><link>https://arxiv.org/abs/2403.17285</link><description>&lt;p&gt;
&#23545;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24448;&#36820;&#35774;&#35745;&#36827;&#34892;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
An Analysis of Switchback Designs in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#8220;&#24369;&#20449;&#21495;&#20998;&#26512;&#8221;&#26694;&#26550;&#65292;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#24448;&#36820;&#35774;&#35745;&#23545;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#22312;&#22823;&#37096;&#20998;&#22870;&#21169;&#35823;&#24046;&#20026;&#27491;&#30456;&#20851;&#26102;&#65292;&#24448;&#36820;&#35774;&#35745;&#27604;&#27599;&#26085;&#20999;&#25442;&#31574;&#30053;&#26356;&#26377;&#25928;&#65292;&#22686;&#21152;&#25919;&#31574;&#20999;&#25442;&#39057;&#29575;&#21487;&#20197;&#38477;&#20302;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;A/B&#27979;&#35797;&#20013;&#24448;&#36820;&#35774;&#35745;&#30340;&#35814;&#32454;&#30740;&#31350;&#65292;&#36825;&#20123;&#35774;&#35745;&#38543;&#26102;&#38388;&#22312;&#22522;&#20934;&#21644;&#26032;&#31574;&#30053;&#20043;&#38388;&#20132;&#26367;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20840;&#38754;&#35780;&#20272;&#36825;&#20123;&#35774;&#35745;&#23545;&#20854;&#20135;&#29983;&#30340;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#20272;&#35745;&#22120;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#8220;&#24369;&#20449;&#21495;&#20998;&#26512;&#8221;&#26694;&#26550;&#65292;&#22823;&#22823;&#31616;&#21270;&#20102;&#36825;&#20123;ATE&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#22312;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#29615;&#22659;&#20013;&#30340;&#35745;&#31639;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65306;(i) &#24403;&#22823;&#37096;&#20998;&#22870;&#21169;&#35823;&#24046;&#21576;&#27491;&#30456;&#20851;&#26102;&#65292;&#24448;&#36820;&#35774;&#35745;&#27604;&#27599;&#26085;&#20999;&#25442;&#31574;&#30053;&#30340;&#20132;&#26367;&#35774;&#35745;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#22686;&#21152;&#25919;&#31574;&#20999;&#25442;&#30340;&#39057;&#29575;&#24448;&#24448;&#20250;&#38477;&#20302;ATE&#20272;&#35745;&#22120;&#30340;MSE&#12290;(ii) &#28982;&#32780;&#65292;&#24403;&#35823;&#24046;&#19981;&#30456;&#20851;&#26102;&#65292;&#25152;&#26377;&#36825;&#20123;&#35774;&#35745;&#21464;&#24471;&#28176;&#36817;&#31561;&#25928;&#12290;(iii) &#22312;&#22823;&#22810;&#25968;&#35823;&#24046;&#20026;&#36127;&#30456;&#20851;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17285v1 Announce Type: cross  Abstract: This paper offers a detailed investigation of switchback designs in A/B testing, which alternate between baseline and new policies over time. Our aim is to thoroughly evaluate the effects of these designs on the accuracy of their resulting average treatment effect (ATE) estimators. We propose a novel "weak signal analysis" framework, which substantially simplifies the calculations of the mean squared errors (MSEs) of these ATEs in Markov decision process environments. Our findings suggest that (i) when the majority of reward errors are positively correlated, the switchback design is more efficient than the alternating-day design which switches policies in a daily basis. Additionally, increasing the frequency of policy switches tends to reduce the MSE of the ATE estimator. (ii) When the errors are uncorrelated, however, all these designs become asymptotically equivalent. (iii) In cases where the majority of errors are negative correlate
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.06759</link><description>&lt;p&gt;
&#24179;&#22343;&#26657;&#20934;&#35823;&#24046;&#65306;&#19968;&#31181;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#20135;&#29983;&#19982;&#32463;&#39564;&#35266;&#23519;&#19981;&#19968;&#33268;&#30340;&#36807;&#20110;&#33258;&#20449;&#30340;&#32467;&#26524;&#65292;&#36825;&#31181;&#26657;&#20934;&#38169;&#35823;&#25361;&#25112;&#30528;&#23427;&#20204;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25913;&#21892;&#20687;&#32032;&#32423;&#26657;&#20934;&#32780;&#19981;&#20250;&#25439;&#23475;&#20998;&#21106;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;&#20351;&#29992;&#30828;&#20998;&#31665;&#65292;&#36825;&#31181;&#25439;&#22833;&#26159;&#30452;&#25509;&#21487;&#24494;&#30340;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#36817;&#20284;&#20294;&#21487;&#24494;&#30340;&#26367;&#20195;&#25110;&#36719;&#20998;&#31665;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#30340;&#27010;&#24565;&#65292;&#36825;&#19968;&#27010;&#24565;&#25512;&#24191;&#20102;&#26631;&#20934;&#30340;&#21487;&#38752;&#24615;&#22270;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#32423;&#21035;&#32858;&#21512;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#32454;&#21270;&#26657;&#20934;&#30340;&#35270;&#35273;&#35780;&#20272;&#12290;&#20351;&#29992;mL1-ACE&#65292;&#25105;&#20204;&#23558;&#24179;&#22343;&#21644;&#26368;&#22823;&#26657;&#20934;&#35823;&#24046;&#20998;&#21035;&#38477;&#20302;&#20102;45%&#21644;55%&#65292;&#21516;&#26102;&#22312;BraTS 2021&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;87%&#30340;Dice&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20998;&#20139;&#25105;&#20204;&#30340;&#20195;&#30721;: https://github
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06759v1 Announce Type: cross  Abstract: Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: https://github
&lt;/p&gt;</description></item><item><title>&#20445;&#30041;&#36793;&#30340;&#31526;&#21495;&#22312;&#32593;&#32476;&#26500;&#24314;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#20272;&#35745;&#21644;&#32858;&#31867;&#31934;&#24230;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.10242</link><description>&lt;p&gt;
&#26377;&#31526;&#21495;&#22810;&#26679;&#21270;&#22810;&#37325;&#32593;&#32476;&#65306;&#32858;&#31867;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Signed Diverse Multiplex Networks: Clustering and Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10242
&lt;/p&gt;
&lt;p&gt;
&#20445;&#30041;&#36793;&#30340;&#31526;&#21495;&#22312;&#32593;&#32476;&#26500;&#24314;&#36807;&#31243;&#20013;&#25552;&#39640;&#20102;&#20272;&#35745;&#21644;&#32858;&#31867;&#31934;&#24230;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26377;&#31526;&#21495;&#30340;&#24191;&#20041;&#38543;&#26426;&#28857;&#31215;&#22270;&#65288;SGRDPG&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#24191;&#20041;&#38543;&#26426;&#28857;&#31215;&#22270;&#65288;GRDPG&#65289;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#20854;&#20013;&#36793;&#21487;&#20197;&#26159;&#27491;&#30340;&#20063;&#21487;&#20197;&#26159;&#36127;&#30340;&#12290;&#35813;&#35774;&#32622;&#34987;&#25193;&#23637;&#20026;&#22810;&#37325;&#32593;&#32476;&#29256;&#26412;&#65292;&#20854;&#20013;&#25152;&#26377;&#23618;&#20855;&#26377;&#30456;&#21516;&#30340;&#33410;&#28857;&#38598;&#21512;&#24182;&#36981;&#24490;SGRDPG&#12290;&#32593;&#32476;&#23618;&#30340;&#21807;&#19968;&#20844;&#20849;&#29305;&#24449;&#26159;&#23427;&#20204;&#21487;&#20197;&#34987;&#21010;&#20998;&#20026;&#20855;&#26377;&#20849;&#21516;&#23376;&#31354;&#38388;&#32467;&#26500;&#30340;&#32452;&#65292;&#32780;&#20854;&#20182;&#24773;&#20917;&#19979;&#25152;&#26377;&#36830;&#25509;&#27010;&#29575;&#30697;&#38453;&#21487;&#33021;&#26159;&#23436;&#20840;&#19981;&#21516;&#30340;&#12290;&#19978;&#36848;&#35774;&#32622;&#38750;&#24120;&#28789;&#27963;&#65292;&#24182;&#21253;&#25324;&#21508;&#31181;&#29616;&#26377;&#22810;&#37325;&#32593;&#32476;&#27169;&#22411;&#20316;&#20026;&#20854;&#29305;&#20363;&#12290;&#35770;&#25991;&#23454;&#29616;&#20102;&#20004;&#20010;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#23427;&#34920;&#26126;&#22312;&#32593;&#32476;&#26500;&#24314;&#36807;&#31243;&#20013;&#20445;&#30041;&#36793;&#30340;&#31526;&#21495;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#20272;&#35745;&#21644;&#32858;&#31867;&#31934;&#24230;&#65292;&#22240;&#27492;&#26377;&#21161;&#20110;&#24212;&#23545;&#35832;&#22914;&#22823;&#33041;&#32593;&#32476;&#20998;&#26512;&#20043;&#31867;&#30340;&#29616;&#23454;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10242v1 Announce Type: cross  Abstract: The paper introduces a Signed Generalized Random Dot Product Graph (SGRDPG) model, which is a variant of the Generalized Random Dot Product Graph (GRDPG), where, in addition, edges can be positive or negative. The setting is extended to a multiplex version, where all layers have the same collection of nodes and follow the SGRDPG. The only common feature of the layers of the network is that they can be partitioned into groups with common subspace structures, while otherwise all matrices of connection probabilities can be all different. The setting above is extremely flexible and includes a variety of existing multiplex network models as its particular cases. The paper fulfills two objectives. First, it shows that keeping signs of the edges in the process of network construction leads to a better precision of estimation and clustering and, hence, is beneficial for tackling real world problems such as analysis of brain networks. Second, b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26080;&#31351;&#29366;&#24577;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#31639;&#27861;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#33391;&#22909;&#30340;&#21021;&#22987;&#31574;&#30053;&#26465;&#20214;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20197;$O(1/\sqrt{T})$&#30340;&#25910;&#25947;&#36895;&#29575;&#25910;&#25947;&#12290;&#21516;&#26102;&#65292;&#23545;&#20110;&#19968;&#31867;&#22823;&#31867;&#25490;&#38431;MDPs&#65292;MaxWeight&#31574;&#30053;&#36275;&#20197;&#28385;&#36275;&#21021;&#22987;&#31574;&#30053;&#35201;&#27714;&#24182;&#23454;&#29616;&#25910;&#25947;&#12290;</title><link>https://arxiv.org/abs/2402.05274</link><description>&lt;p&gt;
&#26080;&#31351;&#29366;&#24577;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence for Natural Policy Gradient on Infinite-State Average-Reward Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26080;&#31351;&#29366;&#24577;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#31639;&#27861;&#36827;&#34892;&#20102;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#22312;&#33391;&#22909;&#30340;&#21021;&#22987;&#31574;&#30053;&#26465;&#20214;&#19979;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20197;$O(1/\sqrt{T})$&#30340;&#25910;&#25947;&#36895;&#29575;&#25910;&#25947;&#12290;&#21516;&#26102;&#65292;&#23545;&#20110;&#19968;&#31867;&#22823;&#31867;&#25490;&#38431;MDPs&#65292;MaxWeight&#31574;&#30053;&#36275;&#20197;&#28385;&#36275;&#21021;&#22987;&#31574;&#30053;&#35201;&#27714;&#24182;&#23454;&#29616;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#31351;&#29366;&#24577;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#22312;&#24314;&#27169;&#21644;&#20248;&#21270;&#21508;&#31181;&#24037;&#31243;&#38382;&#39064;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#29615;&#22659;&#20013;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#20248;&#21270;&#36825;&#20123;MDPs&#12290;&#22312;&#35768;&#22810;&#27969;&#34892;&#30340;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#23398;&#20064;&#31639;&#27861;&#20013;&#65292;&#22914;&#33258;&#28982;&#28436;&#21592;-&#35780;&#35770;&#23478;&#12289;TRPO&#21644;PPO&#65292;&#37117;&#22522;&#20110;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65288;NPG&#65289;&#31639;&#27861;&#12290;&#36825;&#20123;RL&#31639;&#27861;&#30340;&#25910;&#25947;&#32467;&#26524;&#24314;&#31435;&#22312;NPG&#31639;&#27861;&#30340;&#25910;&#25947;&#32467;&#26524;&#19978;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#29616;&#26377;&#30340;NPG&#31639;&#27861;&#25910;&#25947;&#24615;&#32467;&#26524;&#22343;&#20165;&#38480;&#20110;&#26377;&#38480;&#29366;&#24577;&#35774;&#32622;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;NPG&#31639;&#27861;&#22312;&#26080;&#31351;&#29366;&#24577;&#24179;&#22343;&#22870;&#21169;MDPs&#20013;&#30340;&#39318;&#20010;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#65292;&#35777;&#26126;&#20102;$O(1/\sqrt{T})$&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#22914;&#26524;NPG&#31639;&#27861;&#20197;&#33391;&#22909;&#30340;&#21021;&#22987;&#31574;&#30053;&#36827;&#34892;&#21021;&#22987;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#22823;&#31867;&#25490;&#38431;MDPs&#30340;&#24773;&#20917;&#19979;&#65292;MaxWeight&#31574;&#30053;&#36275;&#22815;&#28385;&#36275;&#25105;&#20204;&#30340;&#21021;&#22987;&#31574;&#30053;&#35201;&#27714;&#65292;&#24182;&#23454;&#29616;&#20102;$O(1/...
&lt;/p&gt;
&lt;p&gt;
Infinite-state Markov Decision Processes (MDPs) are essential in modeling and optimizing a wide variety of engineering problems. In the reinforcement learning (RL) context, a variety of algorithms have been developed to learn and optimize these MDPs. At the heart of many popular policy-gradient based learning algorithms, such as natural actor-critic, TRPO, and PPO, lies the Natural Policy Gradient (NPG) algorithm. Convergence results for these RL algorithms rest on convergence results for the NPG algorithm. However, all existing results on the convergence of the NPG algorithm are limited to finite-state settings.   We prove the first convergence rate bound for the NPG algorithm for infinite-state average-reward MDPs, proving a $O(1/\sqrt{T})$ convergence rate, if the NPG algorithm is initialized with a good initial policy. Moreover, we show that in the context of a large class of queueing MDPs, the MaxWeight policy suffices to satisfy our initial-policy requirement and achieve a $O(1/\
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02672</link><description>&lt;p&gt;
&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#30340;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;&#65306;&#19968;&#31181;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects on distributed data: A privacy-preserving approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATE&#65289;&#27169;&#22411;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#35813;&#26041;&#27861;&#30340;&#19977;&#20010;&#20027;&#35201;&#36129;&#29486;&#26159;&#65306;&#23454;&#29616;&#20102;&#23545;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#30340;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#31561;&#21508;&#20010;&#39046;&#22495;&#20013;&#65292;&#23545;&#26465;&#20214;&#24179;&#22343;&#27835;&#30103;&#25928;&#26524;&#65288;CATEs&#65289;&#30340;&#20272;&#35745;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#35838;&#39064;&#12290;&#22914;&#26524;&#20998;&#24067;&#22312;&#22810;&#20010;&#21442;&#19982;&#26041;&#20043;&#38388;&#30340;&#25968;&#25454;&#21487;&#20197;&#38598;&#20013;&#65292;&#21487;&#20197;&#23545;CATEs&#36827;&#34892;&#39640;&#31934;&#24230;&#30340;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#36825;&#20123;&#25968;&#25454;&#21253;&#21547;&#38544;&#31169;&#20449;&#24687;&#65292;&#21017;&#24456;&#38590;&#36827;&#34892;&#25968;&#25454;&#32858;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25968;&#25454;&#21327;&#20316;&#21452;&#26426;&#22120;&#23398;&#20064;&#65288;DC-DML&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#25252;&#20998;&#24067;&#24335;&#25968;&#25454;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;CATE&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23545;&#35813;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#24635;&#32467;&#22914;&#19979;&#19977;&#28857;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#36827;&#34892;&#38750;&#36845;&#20195;&#36890;&#20449;&#30340;&#21322;&#21442;&#25968;CATE&#27169;&#22411;&#30340;&#20272;&#35745;&#21644;&#27979;&#35797;&#12290;&#21322;&#21442;&#25968;&#25110;&#38750;&#21442;&#25968;&#30340;CATE&#27169;&#22411;&#33021;&#22815;&#27604;&#21442;&#25968;&#27169;&#22411;&#26356;&#31283;&#20581;&#22320;&#36827;&#34892;&#20272;&#35745;&#21644;&#27979;&#35797;&#65292;&#23545;&#20110;&#27169;&#22411;&#20559;&#24046;&#30340;&#40065;&#26834;&#24615;&#26356;&#24378;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#25552;&#20986;&#26377;&#25928;&#30340;&#36890;&#20449;&#26041;&#27861;&#26469;&#20272;&#35745;&#21644;&#27979;&#35797;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimation of conditional average treatment effects (CATEs) is an important topic in various fields such as medical and social sciences. CATEs can be estimated with high accuracy if distributed data across multiple parties can be centralized. However, it is difficult to aggregate such data if they contain privacy information. To address this issue, we proposed data collaboration double machine learning (DC-DML), a method that can estimate CATE models with privacy preservation of distributed data, and evaluated the method through numerical experiments. Our contributions are summarized in the following three points. First, our method enables estimation and testing of semi-parametric CATE models without iterative communication on distributed data. Semi-parametric or non-parametric CATE models enable estimation and testing that is more robust to model mis-specification than parametric models. However, to our knowledge, no communication-efficient method has been proposed for estimating and 
&lt;/p&gt;</description></item></channel></rss>