<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#35770;&#23478;&#30340;&#22343;&#26041;&#35823;&#24046;&#19978;&#30028;&#20026;$\epsilon$&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.08})$&#65292;&#20248;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01371</link><description>&lt;p&gt;
Critic-Actor&#31639;&#27861;&#22312;&#24179;&#22343;&#22870;&#21169;MDPs&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65306;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#35770;&#23478;&#30340;&#22343;&#26041;&#35823;&#24046;&#19978;&#30028;&#20026;$\epsilon$&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.08})$&#65292;&#20248;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#28176;&#36817;&#21644;&#38750;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#30340;&#30740;&#31350;&#24037;&#20316;&#38750;&#24120;&#27963;&#36291;&#65292;&#20854;&#20013;&#28436;&#21592;&#30340;&#26356;&#26032;&#36895;&#24230;&#27604;&#35780;&#35770;&#23478;&#24930;&#12290;&#22312;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#25240;&#25187;&#25104;&#26412;&#35774;&#32622;&#20013;&#30340;&#26597;&#25214;&#34920;&#24773;&#20917;&#65292;&#20854;&#20013;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#30340;&#26102;&#38388;&#23610;&#24230;&#30456;&#21453;&#65292;&#24182;&#32473;&#20986;&#20102;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#24182;&#22312;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#26377;&#38480;&#26102;&#38388;&#65288;&#38750;&#28176;&#36817;&#65289;&#20998;&#26512;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#20248;&#30340;&#23398;&#20064;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#35780;&#35770;&#23478;&#30340;&#22343;&#26041;&#35823;&#24046;&#19978;&#30028;&#20026;$\epsilon$&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.08})$&#65292;&#27492;&#32467;&#26524;&#27604;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#33719;&#24471;&#30340;&#32467;&#26524;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic. In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented. In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\epsilon$ which is better than the one obtained for actor-critic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14488</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#22240;&#26524;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23433;&#20840;&#31283;&#20581;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action Selection in Robot Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#29289;&#20307;&#25805;&#20316;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#20851;&#38190;&#25512;&#25163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25361;&#25112;&#22312;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#24517;&#39035;&#23545;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35753;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#23545;&#20505;&#36873;&#21160;&#20316;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#19968;&#20010;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#21018;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#19982;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23450;&#20041;&#20102;&#26426;&#22120;&#20154;&#20915;&#31574;&#36807;&#31243;&#30340;&#22240;&#26524;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#12290;&#36890;&#36807;&#22522;&#20110;&#20223;&#30495;&#30340;&#33945;&#29305;&#21345;&#27931;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#33021;&#22815;&#65306;(1) &#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#31215;&#26408;&#22612;&#30340;&#31283;&#23450;&#24615;&#65288;&#39044;&#27979;&#20934;&#30830;&#29575;&#65306;88.6%&#65289;&#65307;&#21644;&#65292;(2) &#20026;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#36873;&#25321;&#19968;&#20010;&#36817;&#20284;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#65292;&#20379;&#25972;&#21512;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#65292;&#23454;&#29616;94.2%&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14488v1 Announce Type: cross  Abstract: Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task succe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35780;&#20998;&#20989;&#25968;&#30340;&#19968;&#23545;&#19968;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26435;&#37325;&#26368;&#23567;&#21270;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;</title><link>https://arxiv.org/abs/2403.12367</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#35745;&#20998;&#21305;&#37197;&#31639;&#27861;&#35780;&#20272;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
Semisupervised score based matching algorithm to evaluate the effect of public health interventions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12367
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35780;&#20998;&#20989;&#25968;&#30340;&#19968;&#23545;&#19968;&#21305;&#37197;&#31639;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#26435;&#37325;&#26368;&#23567;&#21270;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#21305;&#37197;&#31639;&#27861;&#22312;&#35266;&#23519;&#24615;&#30740;&#31350;&#20013;&#8220;&#37197;&#23545;&#8221;&#30456;&#20284;&#30340;&#30740;&#31350;&#21333;&#20803;&#65292;&#20197;&#28040;&#38500;&#30001;&#20110;&#32570;&#20047;&#38543;&#26426;&#24615;&#32780;&#24341;&#36215;&#30340;&#28508;&#22312;&#20559;&#20506;&#21644;&#28151;&#26434;&#25928;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20108;&#27425;&#35780;&#20998;&#20989;&#25968;&#30340;&#26032;&#22411;&#19968;&#23545;&#19968;&#21305;&#37197;&#31639;&#27861;&#65292;&#26435;&#37325;$\beta$&#34987;&#35774;&#35745;&#20026;&#26368;&#23567;&#21270;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#26410;&#37197;&#23545;&#35757;&#32451;&#21333;&#20803;&#20043;&#38388;&#30340;&#24471;&#20998;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12367v1 Announce Type: cross  Abstract: Multivariate matching algorithms "pair" similar study units in an observational study to remove potential bias and confounding effects caused by the absence of randomizations. In one-to-one multivariate matching algorithms, a large number of "pairs" to be matched could mean both the information from a large sample and a large number of tasks, and therefore, to best match the pairs, such a matching algorithm with efficiency and comparatively limited auxiliary matching knowledge provided through a "training" set of paired units by domain experts, is practically intriguing.   We proposed a novel one-to-one matching algorithm based on a quadratic score function $S_{\beta}(x_i,x_j)= \beta^T (x_i-x_j)(x_i-x_j)^T \beta$. The weights $\beta$, which can be interpreted as a variable importance measure, are designed to minimize the score difference between paired training units while maximizing the score difference between unpaired training units
&lt;/p&gt;</description></item><item><title>TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.</title><link>https://arxiv.org/abs/2402.12683</link><description>&lt;p&gt;
TorchCP&#65306;&#22522;&#20110;PyTorch&#30340;&#19968;&#31181;&#36866;&#29992;&#20110;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TorchCP: A Library for Conformal Prediction based on PyTorch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12683
&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#30340;Python&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#20102;&#29992;&#20110;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#21508;&#31181;&#23454;&#29616;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65288;&#21253;&#25324;&#22810;&#32500;&#36755;&#20986;&#65289;&#12290;TorchCP&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#24182;&#21033;&#29992;&#30697;&#38453;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#31616;&#27905;&#39640;&#25928;&#30340;&#25512;&#29702;&#23454;&#29616;&#12290;&#35813;&#20195;&#30721;&#37319;&#29992;LGPL&#35768;&#21487;&#35777;&#65292;&#24182;&#22312;$\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12683v1 Announce Type: new  Abstract: TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#21644;&#23545;&#31574;&#65292;&#24182;&#23545;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#12289;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#32852;&#37030;&#23398;&#20064;&#30340;&#20856;&#22411;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2402.00342</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20013;&#38544;&#31169;&#23041;&#32961;&#21644;&#23545;&#31574;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of Privacy Threats and Countermeasures in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00342
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#38544;&#31169;&#23041;&#32961;&#21644;&#23545;&#31574;&#65292;&#24182;&#23545;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#12289;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#32852;&#37030;&#23398;&#20064;&#30340;&#20856;&#22411;&#31867;&#22411;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#27880;&#37325;&#38544;&#31169;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#22240;&#20026;&#23458;&#25143;&#31471;&#20043;&#38388;&#27809;&#26377;&#30452;&#25509;&#20132;&#25442;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#38544;&#31169;&#23041;&#32961;&#65292;&#24182;&#19988;&#24050;&#32463;&#23545;&#38544;&#31169;&#23545;&#31574;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#24120;&#35265;&#21644;&#29420;&#29305;&#30340;&#38544;&#31169;&#23041;&#32961;&#22312;&#20856;&#22411;&#31867;&#22411;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20197;&#20840;&#38754;&#21644;&#20855;&#20307;&#30340;&#26041;&#24335;&#36827;&#34892;&#20998;&#31867;&#21644;&#25551;&#36848;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#12289;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#21644;&#36801;&#31227;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#23041;&#32961;&#21644;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is widely considered to be as a privacy-aware learning method because no training data is exchanged directly between clients. Nevertheless, there are threats to privacy in federated learning, and privacy countermeasures have been studied. However, we note that common and unique privacy threats among typical types of federated learning have not been categorized and described in a comprehensive and specific way. In this paper, we describe privacy threats and countermeasures for the typical types of federated learning; horizontal federated learning, vertical federated learning, and transfer federated learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#26469;&#23454;&#29616;&#27491;&#20132;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20248;&#21270;&#22240;&#23376;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#22312;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.02592</link><description>&lt;p&gt;
&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Guaranteed Nonconvex Factorization Approach for Tensor Train Recovery. (arXiv:2401.02592v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#38750;&#20984;&#20998;&#35299;&#26041;&#27861;&#29992;&#20110;&#24352;&#37327;&#21015;&#36710;&#24674;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20248;&#21270;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#26469;&#23454;&#29616;&#27491;&#20132;&#32467;&#26500;&#65292;&#24182;&#20351;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#26469;&#20248;&#21270;&#22240;&#23376;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#24182;&#19988;&#22312;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#30340;&#26465;&#20214;&#19979;&#33021;&#22815;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20379;&#20102;&#23545;&#20110;&#20998;&#35299;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20026;&#20102;&#36991;&#20813;&#23610;&#24230;&#27495;&#20041;&#24182;&#20415;&#20110;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#20248;&#21270;&#25152;&#35859;&#30340;&#24038;&#27491;&#20132;TT&#26684;&#24335;&#65292;&#24378;&#21046;&#20351;&#22823;&#37096;&#20998;&#22240;&#23376;&#24444;&#27492;&#27491;&#20132;&#12290;&#20026;&#20102;&#30830;&#20445;&#27491;&#20132;&#32467;&#26500;&#65292;&#25105;&#20204;&#21033;&#29992;&#40654;&#26364;&#26799;&#24230;&#19979;&#38477;&#65288;RGD&#65289;&#26469;&#20248;&#21270;Stiefel&#27969;&#24418;&#19978;&#30340;&#36825;&#20123;&#22240;&#23376;&#12290;&#25105;&#20204;&#39318;&#20808;&#28145;&#20837;&#30740;&#31350;TT&#20998;&#35299;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;RGD&#30340;&#23616;&#37096;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#30528;&#24352;&#37327;&#38454;&#25968;&#30340;&#22686;&#21152;&#65292;&#25910;&#25947;&#36895;&#29575;&#20165;&#32463;&#21382;&#32447;&#24615;&#19979;&#38477;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24863;&#30693;&#38382;&#39064;&#65292;&#21363;&#20174;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;TT&#26684;&#24335;&#24352;&#37327;&#12290;&#20551;&#35774;&#24863;&#30693;&#31639;&#23376;&#28385;&#36275;&#21463;&#38480;&#31561;&#35889;&#24615;&#36136;&#65288;RIP&#65289;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#36866;&#24403;&#30340;&#21021;&#22987;&#21270;&#19979;&#65292;&#36890;&#36807;&#35889;&#21021;&#22987;&#21270;&#33719;&#24471;&#65292;RGD&#20063;&#20250;&#20197;&#32447;&#24615;&#36895;&#29575;&#25910;&#25947;&#21040;&#30495;&#23454;&#24352;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide the first convergence guarantee for the factorization approach. Specifically, to avoid the scaling ambiguity and to facilitate theoretical analysis, we optimize over the so-called left-orthogonal TT format which enforces orthonormality among most of the factors. To ensure the orthonormal structure, we utilize the Riemannian gradient descent (RGD) for optimizing those factors over the Stiefel manifold. We first delve into the TT factorization problem and establish the local linear convergence of RGD. Notably, the rate of convergence only experiences a linear decline as the tensor order increases. We then study the sensing problem that aims to recover a TT format tensor from linear measurements. Assuming the sensing operator satisfies the restricted isometry property (RIP), we show that with a proper initialization, which could be obtained through spectral initialization, RGD also converges to the ground-truth tensor at a linear rate. Furthermore, we expand our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#33021;&#25214;&#21040;&#24615;&#33021;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.16363</link><description>&lt;p&gt;
&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Finite Time Analysis of Constrained Actor Critic and Constrained Natural Actor Critic Algorithms. (arXiv:2310.16363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21463;&#32422;&#26463;&#30340;Actor Critic&#21644;&#21463;&#32422;&#26463;&#30340;Natural Actor Critic&#31639;&#27861;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#31639;&#27861;&#33021;&#25214;&#21040;&#24615;&#33021;&#20989;&#25968;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Actor Critic&#26041;&#27861;&#22312;&#24191;&#27867;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#25214;&#21040;&#20102;&#24040;&#22823;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#24403;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#24456;&#22823;&#30340;&#26102;&#20505;&#12290;&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#20989;&#25968;&#36924;&#36817;&#30340;actor critic&#21644;natural actor critic&#31639;&#27861;&#26469;&#22788;&#29702;&#28041;&#21450;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;C-MDP&#65289;&#65292;&#24182;&#22312;&#38750; i.i.d&#65288;&#39532;&#23572;&#21487;&#22827;&#65289;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#38750;&#28176;&#36817;&#20998;&#26512;&#12290;&#25105;&#20204;&#32771;&#34385;&#38271;&#26399;&#24179;&#22343;&#25104;&#26412;&#20934;&#21017;&#65292;&#20854;&#20013;&#30446;&#26631;&#21644;&#32422;&#26463;&#20989;&#25968;&#37117;&#26159;&#26576;&#20123;&#35268;&#23450;&#25104;&#26412;&#20989;&#25968;&#30340;&#36866;&#24403;&#31574;&#30053;&#20381;&#36182;&#30340;&#38271;&#26399;&#24179;&#22343;&#12290;&#25105;&#20204;&#20351;&#29992;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#27861;&#22788;&#29702;&#19981;&#31561;&#24335;&#32422;&#26463;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#31639;&#27861;&#20445;&#35777;&#33021;&#25214;&#21040;&#24615;&#33021;&#65288;&#25289;&#26684;&#26391;&#26085;&#65289;&#20989;&#25968;$L(\theta,\gamma)$&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;&#21363;$\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$&#65289;&#65292;&#24182;&#19988;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.5})$&#12290;
&lt;/p&gt;
&lt;p&gt;
Actor Critic methods have found immense applications on a wide range of Reinforcement Learning tasks especially when the state-action space is large. In this paper, we consider actor critic and natural actor critic algorithms with function approximation for constrained Markov decision processes (C-MDP) involving inequality constraints and carry out a non-asymptotic analysis for both of these algorithms in a non-i.i.d (Markovian) setting. We consider the long-run average cost criterion where both the objective and the constraint functions are suitable policy-dependent long-run averages of certain prescribed cost functions. We handle the inequality constraints using the Lagrange multiplier method. We prove that these algorithms are guaranteed to find a first-order stationary point (i.e., $\Vert \nabla L(\theta,\gamma)\Vert_2^2 \leq \epsilon$) of the performance (Lagrange) function $L(\theta,\gamma)$, with a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.5})$ in the case of both C
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2309.08289</link><description>&lt;p&gt;
&#21033;&#29992;&#28857;&#25193;&#25955;&#27169;&#22411;&#23545;&#22823;&#32928;&#30340;3D&#24418;&#29366;&#36827;&#34892;&#31934;&#21270;&#20197;&#29983;&#25104;&#25968;&#23383;&#24187;&#24433;
&lt;/p&gt;
&lt;p&gt;
Large Intestine 3D Shape Refinement Using Point Diffusion Models for Digital Phantom Generation. (arXiv:2309.08289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20248;&#21270;&#22823;&#32928;&#30340;&#20998;&#21106;&#32467;&#26524;&#65292;&#24182;&#32467;&#21512;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#23454;&#29616;&#23545;&#22823;&#32928;3D&#24418;&#29366;&#30340;&#31934;&#21270;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#24314;&#27169;&#20154;&#20307;&#22120;&#23448;&#22312;&#26500;&#24314;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#30340;&#35745;&#31639;&#20223;&#30495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#20174;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20013;&#29983;&#25104;&#35299;&#21078;&#23398;&#19978;&#21487;&#20449;&#30340;&#22120;&#23448;&#34920;&#38754;&#37325;&#24314;&#20173;&#28982;&#23545;&#20154;&#20307;&#32467;&#26500;&#20013;&#30340;&#35768;&#22810;&#22120;&#23448;&#26469;&#35828;&#26159;&#20010;&#25361;&#25112;&#12290;&#22312;&#22788;&#29702;&#22823;&#32928;&#26102;&#65292;&#36825;&#20010;&#25361;&#25112;&#23588;&#20026;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#21644;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#20248;&#21270;&#22823;&#32928;&#20998;&#21106;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#22120;&#23448;&#34920;&#31034;&#20026;&#20174;3D&#20998;&#21106;&#25513;&#27169;&#34920;&#38754;&#37319;&#26679;&#24471;&#21040;&#30340;&#28857;&#20113;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33719;&#24471;&#22120;&#23448;&#24418;&#29366;&#30340;&#20840;&#23616;&#21644;&#23616;&#37096;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#20998;&#23618;&#28508;&#22312;&#31354;&#38388;&#20013;&#35757;&#32451;&#20004;&#20010;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26469;&#36827;&#34892;&#24418;&#29366;&#31934;&#21270;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#20808;&#36827;&#30340;&#34920;&#38754;&#37325;&#26500;&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#24418;&#29366;&#30340;&#26356;&#22909;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D modeling of human organs plays a crucial role in building computational phantoms for virtual imaging trials. However, generating anatomically plausible reconstructions of organ surfaces from computed tomography scans remains challenging for many structures in the human body. This challenge is particularly evident when dealing with the large intestine. In this study, we leverage recent advancements in geometric deep learning and denoising diffusion probabilistic models to refine the segmentation results of the large intestine. We begin by representing the organ as point clouds sampled from the surface of the 3D segmentation mask. Subsequently, we employ a hierarchical variational autoencoder to obtain global and local latent representations of the organ's shape. We train two conditional denoising diffusion models in the hierarchical latent space to perform shape refinement. To further enhance our method, we incorporate a state-of-the-art surface reconstruction model, allowin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.12968</link><description>&lt;p&gt;
&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#31751;&#24674;&#22797;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model. (arXiv:2306.12968v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31751;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#24674;&#22797;&#38544;&#34255;&#30340;&#31038;&#32676;&#65292;&#20854;&#20013;&#31751;&#22823;&#23567;&#38543;&#30528;&#29289;&#21697;&#24635;&#25968;$n$&#30340;&#22686;&#38271;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#22312;LSBM&#20013;&#65292;&#20026;&#27599;&#23545;&#29289;&#21697;&#65288;&#29420;&#31435;&#22320;&#65289;&#35266;&#27979;&#21040;&#19968;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#35266;&#27979;&#21040;&#30340;&#26631;&#31614;&#26469;&#24674;&#22797;&#31751;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#26399;&#26395;&#34987;&#20219;&#20309;&#32858;&#31867;&#31639;&#27861;&#35823;&#20998;&#31867;&#30340;&#29289;&#21697;&#25968;&#37327;&#30340;&#23454;&#20363;&#29305;&#23450;&#19979;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#26399;&#26395;&#21644;&#39640;&#27010;&#29575;&#19979;&#37117;&#33021;&#21305;&#37197;&#36825;&#20123;&#19979;&#30028;&#34920;&#29616;&#30340;&#31639;&#27861;&#12290;IAC&#30001;&#19968;&#27425;&#35889;&#32858;&#31867;&#31639;&#27861;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#32452;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#23454;&#20363;&#29305;&#23450;&#30340;&#19979;&#30028;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#21253;&#25324;&#31751;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20165;&#25191;&#34892;&#19968;&#27425;&#35889;&#32858;&#31867;&#65292;IAC&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#26041;&#38754;&#37117;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters, where cluster sizes grow linearly with the total number $n$ of items. In the LSBM, a label is (independently) observed for each pair of items. Our objective is to devise an efficient algorithm that recovers clusters using the observed labels. To this end, we revisit instance-specific lower bounds on the expected number of misclassified items satisfied by any clustering algorithm. We present Instance-Adaptive Clustering (IAC), the first algorithm whose performance matches these lower bounds both in expectation and with high probability. IAC consists of a one-time spectral clustering algorithm followed by an iterative likelihood-based cluster assignment improvement. This approach is based on the instance-specific lower bound and does not require any model parameters, including the number of clusters. By performing the spectral clustering only once, IAC m
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.19470</link><description>&lt;p&gt;
&#29992;Johnson-Lindenstrauss&#30697;&#38453;&#36827;&#34892;&#26631;&#31614;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Label Embedding by Johnson-Lindenstrauss Matrices. (arXiv:2305.19470v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19470
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;JLMs&#30340;&#26631;&#31614;&#23884;&#20837;&#26041;&#27861;&#65292;&#23558;&#22810;&#20803;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#26377;&#38480;&#22238;&#24402;&#38382;&#39064;&#65292;&#20855;&#26377;&#36739;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Johnson-Lindenstrauss&#30697;&#38453;&#65288;JLMs&#65289;&#30340;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#26497;&#31471;&#22810;&#20803;&#20998;&#31867;&#26694;&#26550;&#12290;&#21033;&#29992;JLM&#30340;&#21015;&#26469;&#23884;&#20837;&#26631;&#31614;&#65292;&#23558;&#19968;&#20010;C&#31867;&#20998;&#31867;&#38382;&#39064;&#36716;&#21270;&#20026;&#20855;&#26377;$\cO(\log C)$&#36755;&#20986;&#32500;&#24230;&#30340;&#22238;&#24402;&#38382;&#39064;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#36229;&#37327;&#39118;&#38505;&#38480;&#21046;&#65292;&#38416;&#26126;&#20102;&#35745;&#31639;&#25928;&#29575;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#36827;&#19968;&#27493;&#34920;&#26126;&#65292;&#22312;Massart&#22122;&#22768;&#26465;&#20214;&#19979;&#65292;&#38477;&#32500;&#30340;&#24809;&#32602;&#20250;&#28040;&#22833;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#24182;&#34892;&#21270;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#20854;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple and scalable framework for extreme multiclass classification based on Johnson-Lindenstrauss matrices (JLMs). Using the columns of a JLM to embed the labels, a $C$-class classification problem is transformed into a regression problem with $\cO(\log C)$ output dimension. We derive an excess risk bound, revealing a tradeoff between computational efficiency and prediction accuracy, and further show that under the Massart noise condition, the penalty for dimension reduction vanishes. Our approach is easily parallelizable, and experimental results demonstrate its effectiveness and scalability in large-scale applications.
&lt;/p&gt;</description></item></channel></rss>