<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01454</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#19968;&#31181;&#32479;&#35745;&#22240;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#65288;SCD&#65289;&#20013;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#23884;&#20837;&#21040;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#21019;&#24314;&#19968;&#33268;&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#27169;&#22411;&#26159;&#37325;&#35201;&#30340;&#65292;&#23613;&#31649;&#35782;&#21035;&#32972;&#26223;&#30693;&#35782;&#30340;&#25361;&#25112;&#34987;&#35748;&#21487;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;LLM&#30340;&#8220;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#65288;SCP&#65289;&#8221;&#19982;SCD&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#22240;&#26524;&#25512;&#26029;&#65288;KBCI&#65289;&#30456;&#32467;&#21512;&#65292;&#23545;SCD&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4&#21487;&#20197;&#20351;LLM-KBCI&#30340;&#36755;&#20986;&#19982;&#24102;&#26377;LLM-KBCI&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;SCD&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#65292;&#22914;&#26524;GPT-4&#32463;&#21382;&#20102;SCP&#65292;&#37027;&#20040;SCD&#30340;&#32467;&#26524;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;LLM&#19981;&#21547;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;LLM&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20854;&#32972;&#26223;&#30693;&#35782;&#26469;&#25913;&#36827;SCD&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
&lt;/p&gt;</description></item><item><title>&#35780;&#35770;&#20102;&#21478;&#19968;&#31687;&#20851;&#20110;&#20174;&#24494;&#20998;&#26041;&#31243;&#20013;&#23398;&#20064;&#23432;&#24658;&#23450;&#24459;&#30340;&#25991;&#31456;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#25512;&#23548;&#38169;&#35823;</title><link>https://arxiv.org/abs/2404.02896</link><description>&lt;p&gt;
&#23545;&#8220;&#20174;&#24494;&#20998;&#26041;&#31243;&#20013;&#23398;&#20064;&#23432;&#24658;&#23450;&#24459;&#8221;&#19968;&#25991;&#30340;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Comment on "Machine learning conservation laws from differential equations"
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02896
&lt;/p&gt;
&lt;p&gt;
&#35780;&#35770;&#20102;&#21478;&#19968;&#31687;&#20851;&#20110;&#20174;&#24494;&#20998;&#26041;&#31243;&#20013;&#23398;&#20064;&#23432;&#24658;&#23450;&#24459;&#30340;&#25991;&#31456;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#25512;&#23548;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27492;&#35780;&#35770;&#20013;&#65292;&#20316;&#32773;&#22238;&#39038;&#20102;&#21016;, &#39532;&#24503;&#21704;&#19975;&#21644;&#27888;&#26684;&#39532;&#20811;&#25552;&#20986;&#30340;&#19982;&#20316;&#32773;&#25552;&#20986;&#30340;&#19968;&#32500;&#38459;&#23612;&#35856;&#25391;&#23376;&#30340;&#23432;&#24658;&#37327;&#30456;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#25351;&#20986;&#20182;&#20204;&#25512;&#23548;&#20013;&#23384;&#22312;&#20845;&#20010;&#20005;&#37325;&#38169;&#35823;&#65292;&#23548;&#33268;&#20182;&#20204;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#22343;&#19981;&#27491;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02896v1 Announce Type: new  Abstract: In lieu of abstract, first paragraph reads: Six months after the author derived a constant of motion for a 1D damped harmonic oscillator [1], a similar result appeared by Liu, Madhavan, and Tegmark [2, 3], without citing the author. However, their derivation contained six serious errors, causing both their method and result to be incorrect. In this Comment, those errors are reviewed.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#36890;&#29992;&#34920;&#31034;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02047</link><description>&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#30340;&#36890;&#29992;&#34920;&#31034;&#65306;&#34701;&#21512;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;
&lt;/p&gt;
&lt;p&gt;
Universal representations for financial transactional data: embracing local, global, and external contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#36890;&#29992;&#34920;&#31034;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#30340;&#26377;&#25928;&#22788;&#29702;&#23545;&#38134;&#34892;&#25968;&#25454;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#19987;&#27880;&#20110;&#20026;&#29420;&#31435;&#38382;&#39064;&#25552;&#20379;&#19987;&#38376;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#26159;&#26500;&#24314;&#36866;&#29992;&#20110;&#35768;&#22810;&#38382;&#39064;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20225;&#19994;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32771;&#34385;&#25968;&#25454;&#29305;&#23450;&#24615;&#30340;&#26032;&#39062;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#21040;&#23458;&#25143;&#34920;&#31034;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20854;&#20182;&#23458;&#25143;&#34892;&#21160;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#25551;&#36848;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#34920;&#31034;&#36136;&#37327;&#65292;&#28041;&#21450;&#25972;&#20010;&#20132;&#26131;&#21382;&#21490;&#65307;&#26412;&#22320;&#33539;&#22260;&#20869;&#65292;&#21453;&#26144;&#23458;&#25143;&#24403;&#21069;&#29366;&#24577;&#65307;&#21160;&#24577;&#33539;&#22260;&#20869;&#65292;&#25429;&#25417;&#34920;&#31034;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#19979;&#19968;&#20010;MCC&#39044;&#27979;&#20219;&#21153;&#30340;ROC-AUC&#25552;&#21319;&#39640;&#36798;14&#65285;&#65292;&#23545;&#20110;dow...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02047v1 Announce Type: cross  Abstract: Effective processing of financial transactions is essential for banking data analysis. However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal representations suitable for many problems. We present a representation learning framework that addresses diverse business challenges. We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client's representation, leveraging insights from other customers' actions. Finally, we offer a benchmark, describing representation quality globally, concerning the entire transaction history; locally, reflecting the client's current state; and dynamically, capturing representation evolution over time. Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14\% for the next MCC prediction task and up to 46\% for dow
&lt;/p&gt;</description></item><item><title>ADVREPAIR&#26159;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26500;&#24314;&#34917;&#19969;&#27169;&#22359;&#65292;&#22312;&#31283;&#20581;&#37051;&#22495;&#20869;&#25552;&#20379;&#21487;&#35777;&#21644;&#19987;&#38376;&#30340;&#20462;&#22797;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#21040;&#20854;&#20182;&#36755;&#20837;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01642</link><description>&lt;p&gt;
ADVREPAIR&#65306;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
ADVREPAIR:Provable Repair of Adversarial Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01642
&lt;/p&gt;
&lt;p&gt;
ADVREPAIR&#26159;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#24418;&#24335;&#39564;&#35777;&#26500;&#24314;&#34917;&#19969;&#27169;&#22359;&#65292;&#22312;&#31283;&#20581;&#37051;&#22495;&#20869;&#25552;&#20379;&#21487;&#35777;&#21644;&#19987;&#38376;&#30340;&#20462;&#22797;&#65292;&#21516;&#26102;&#20855;&#26377;&#27867;&#21270;&#21040;&#20854;&#20182;&#36755;&#20837;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#30340;&#37096;&#32626;&#26085;&#30410;&#22686;&#21152;&#65292;&#20294;&#23427;&#20204;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#26500;&#25104;&#20005;&#37325;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;&#31070;&#32463;&#20803;&#32423;&#26041;&#27861;&#22312;&#20462;&#22797;&#23545;&#25163;&#26041;&#38754;&#32570;&#20047;&#25928;&#21147;&#65292;&#22240;&#20026;&#23545;&#25239;&#25915;&#20987;&#26426;&#21046;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#32780;&#23545;&#25239;&#35757;&#32451;&#65292;&#21033;&#29992;&#22823;&#37327;&#23545;&#25239;&#26679;&#26412;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#32570;&#20047;&#21487;&#35777;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ADVREPAIR&#65292;&#19968;&#31181;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#36827;&#34892;&#23545;&#25239;&#25915;&#20987;&#30340;&#21487;&#35777;&#20462;&#22797;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#24418;&#24335;&#39564;&#35777;&#65292;ADVREPAIR&#26500;&#24314;&#34917;&#19969;&#27169;&#22359;&#65292;&#24403;&#19982;&#21407;&#22987;&#32593;&#32476;&#38598;&#25104;&#26102;&#65292;&#22312;&#31283;&#20581;&#37051;&#22495;&#20869;&#25552;&#20379;&#21487;&#35777;&#21644;&#19987;&#38376;&#30340;&#20462;&#22797;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21253;&#25324;&#19968;&#31181;&#21551;&#21457;&#24335;&#26426;&#21046;&#26469;&#20998;&#37197;&#34917;&#19969;&#27169;&#22359;&#65292;&#20351;&#24471;&#36825;&#31181;&#38450;&#24481;&#23545;&#25239;&#25915;&#20987;&#27867;&#21270;&#21040;&#20854;&#20182;&#36755;&#20837;&#12290;ADVREPAIR&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01642v1 Announce Type: new  Abstract: Deep neural networks (DNNs) are increasingly deployed in safety-critical domains, but their vulnerability to adversarial attacks poses serious safety risks. Existing neuron-level methods using limited data lack efficacy in fixing adversaries due to the inherent complexity of adversarial attack mechanisms, while adversarial training, leveraging a large number of adversarial samples to enhance robustness, lacks provability. In this paper, we propose ADVREPAIR, a novel approach for provable repair of adversarial attacks using limited data. By utilizing formal verification, ADVREPAIR constructs patch modules that, when integrated with the original network, deliver provable and specialized repairs within the robustness neighborhood. Additionally, our approach incorporates a heuristic mechanism for assigning patch modules, allowing this defense against adversarial attacks to generalize to other inputs. ADVREPAIR demonstrates superior efficienc
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24471;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#32467;&#21512;&#26446;&#23545;&#31216;&#25216;&#26415;&#20998;&#26512;&#24471;&#21040;&#20102;&#23432;&#24658;&#21644;&#38750;&#23432;&#24658;&#24773;&#20917;&#19979;1D&#21644;2D&#35856;&#25391;&#23376;&#30340;&#36816;&#21160;&#31215;&#20998;&#65292;&#23637;&#31034;&#20102;&#38750;&#23432;&#24658;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#33021;&#37327;&#23432;&#24658;&#24120;&#25968;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#39057;&#29575;&#27604;&#20363;&#24773;&#20917;&#19979;&#25512;&#24191;&#20102;&#35282;&#21160;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.19418</link><description>&lt;p&gt;
&#36816;&#21160;&#30340;&#23432;&#24658;&#21644;&#38750;&#23432;&#24658;&#21160;&#21147;&#23398;&#30340;&#36816;&#21160;&#31215;&#20998;
&lt;/p&gt;
&lt;p&gt;
Constants of Motion for Conserved and Non-conserved Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19418
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24471;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#32467;&#21512;&#26446;&#23545;&#31216;&#25216;&#26415;&#20998;&#26512;&#24471;&#21040;&#20102;&#23432;&#24658;&#21644;&#38750;&#23432;&#24658;&#24773;&#20917;&#19979;1D&#21644;2D&#35856;&#25391;&#23376;&#30340;&#36816;&#21160;&#31215;&#20998;&#65292;&#23637;&#31034;&#20102;&#38750;&#23432;&#24658;&#27169;&#22411;&#20013;&#23384;&#22312;&#30340;&#33021;&#37327;&#23432;&#24658;&#24120;&#25968;&#65292;&#20197;&#21450;&#22312;&#21508;&#31181;&#39057;&#29575;&#27604;&#20363;&#24773;&#20917;&#19979;&#25512;&#24191;&#20102;&#35282;&#21160;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#36807;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;FJet&#65289;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#33719;&#24471;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65307;&#28982;&#21518;&#21033;&#29992;&#26446;&#23545;&#31216;&#25216;&#26415;&#23545;&#35813;&#21160;&#21147;&#23398;&#27169;&#22411;&#36827;&#34892;&#20998;&#26512;&#20197;&#33719;&#24471;&#36816;&#21160;&#31215;&#20998;&#12290;&#35813;&#20998;&#26512;&#38024;&#23545;1D&#21644;2D&#35856;&#25391;&#23376;&#30340;&#23432;&#24658;&#21644;&#38750;&#23432;&#24658;&#24773;&#20917;&#36827;&#34892;&#12290;&#23545;&#20110;1D&#35856;&#25391;&#23376;&#65292;&#22312;&#27424;&#38459;&#23612;&#12289;&#36807;&#38459;&#23612;&#21644;&#20020;&#30028;&#38459;&#23612;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#36816;&#21160;&#31215;&#20998;&#12290;&#23545;&#20110;&#38750;&#23432;&#24658;&#27169;&#22411;&#30340;&#23384;&#22312;&#36825;&#26679;&#30340;&#24120;&#25968;&#26159;&#23545;&#25972;&#20010;&#31995;&#32479;&#65288;&#21363;&#25391;&#33633;&#22120;&#21152;&#32791;&#25955;&#29615;&#22659;&#65289;&#33021;&#37327;&#23432;&#24658;&#30340;&#19968;&#31181;&#34920;&#29616;&#30340;&#26032;&#39062;&#35299;&#37322;&#12290;&#23545;&#20110;2D&#35856;&#25391;&#23376;&#65292;&#22312;&#31561;&#21521;&#21644;&#38750;&#31561;&#21521;&#24773;&#20917;&#19979;&#25214;&#21040;&#20102;&#36816;&#21160;&#31215;&#20998;&#65292;&#21253;&#25324;&#39057;&#29575;&#19981;&#21487;&#20849;&#36717;&#30340;&#24773;&#20917;&#65307;&#36824;&#25512;&#24191;&#21040;&#20219;&#24847;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#36824;&#30830;&#23450;&#20102;&#19968;&#20010;&#24120;&#25968;&#65292;&#23427;&#23558;&#35282;&#21160;&#37327;&#25512;&#24191;&#21040;&#25152;&#26377;&#39057;&#29575;&#27604;&#20363;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19418v1 Announce Type: new  Abstract: This paper begins with a dynamical model that was obtained by applying a machine learning technique (FJet) to time-series data; this dynamical model is then analyzed with Lie symmetry techniques to obtain constants of motion. This analysis is performed on both the conserved and non-conserved cases of the 1D and 2D harmonic oscillators. For the 1D oscillator, constants are found in the cases where the system is underdamped, overdamped, and critically damped. The novel existence of such a constant for a non-conserved model is interpreted as a manifestation of the conservation of energy of the {\em total} system (i.e., oscillator plus dissipative environment). For the 2D oscillator, constants are found for the isotropic and anisotropic cases, including when the frequencies are incommensurate; it is also generalized to arbitrary dimensions. In addition, a constant is identified which generalizes angular momentum for all ratios of the frequen
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#39044;&#27979;&#30446;&#26631;&#23548;&#21521;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;QoIs&#30340;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26469;&#30830;&#23450;&#23454;&#39564;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.18072</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#30446;&#26631;&#23548;&#21521;&#36125;&#21494;&#26031;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#19982;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models using Markov Chain Monte Carlo
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18072
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#27169;&#22411;&#30340;&#39044;&#27979;&#30446;&#26631;&#23548;&#21521;&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;QoIs&#30340;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#26469;&#30830;&#23450;&#23454;&#39564;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#23454;&#39564;&#35774;&#35745;&#65288;OED&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#21270;&#30340;&#26041;&#27861;&#26469;&#37327;&#21270;&#21644;&#26368;&#22823;&#21270;&#23454;&#39564;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#22312;&#36125;&#21494;&#26031;&#26041;&#27861;&#19979;&#65292;&#20256;&#32479;&#30340;OED&#20250;&#26368;&#22823;&#21270;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#26399;&#26395;&#20449;&#24687;&#22686;&#30410;&#65288;EIG&#65289;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36890;&#24120;&#24863;&#20852;&#36259;&#30340;&#19981;&#26159;&#21442;&#25968;&#26412;&#36523;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#26041;&#24335;&#30340;&#39044;&#27979;&#24863;&#20852;&#36259;&#37327;&#65288;QoIs&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#38750;&#32447;&#24615;&#35266;&#27979;&#21644;&#39044;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#30446;&#26631;&#23548;&#21521;OED&#65288;GO-OED&#65289;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23547;&#27714;&#25552;&#20379;&#23545;QoIs&#30340;&#26368;&#22823;EIG&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;QoI EIG&#30340;&#23884;&#22871;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#22120;&#65292;&#20854;&#20013;&#37319;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#21033;&#29992;&#26680;&#23494;&#24230;&#20272;&#35745;&#26469;&#35780;&#20272;&#21518;&#39564;&#39044;&#27979;&#23494;&#24230;&#21450;&#20854;&#19982;&#20808;&#39564;&#39044;&#27979;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#12290;GO-OED&#35774;&#35745;&#36890;&#36807;&#22312;&#35774;&#35745;&#31354;&#38388;&#20013;&#26368;&#22823;&#21270;EIG&#26469;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18072v1 Announce Type: cross  Abstract: Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the des
&lt;/p&gt;</description></item><item><title>&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#23637;&#31034;&#20102;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#30456;&#36739;&#20110;&#20840;&#25209;&#27425;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17105</link><description>&lt;p&gt;
&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Langevin Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#26041;&#27861;&#65292;&#20026;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#25552;&#20379;&#20102;&#38544;&#31169;&#20445;&#38556;&#65292;&#24182;&#23637;&#31034;&#20102;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#30456;&#36739;&#20110;&#20840;&#25209;&#27425;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#26159;&#29992;&#25143;&#25968;&#25454;&#38544;&#31169;&#30340;&#27861;&#24459;&#25152;&#30830;&#20445;&#30340;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26426;&#22120;&#21453;&#36951;&#24536;&#26088;&#22312;&#39640;&#25928;&#22320;&#28040;&#38500;&#24050;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#19978;&#26576;&#20123;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#36817;&#20284;&#20110;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38543;&#26426;&#26799;&#24230; Langevin &#21453;&#36951;&#24536;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#24102;&#26377;&#38544;&#31169;&#20445;&#38556;&#30340;&#22122;&#22768;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#21453;&#36951;&#24536;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20984;&#24615;&#20551;&#35774;&#19979;&#30340;&#36817;&#20284;&#21453;&#36951;&#24536;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20840;&#25209;&#27425;&#23545;&#24212;&#26041;&#27861;&#30456;&#27604;&#65292;&#23567;&#25209;&#27425;&#26799;&#24230;&#26356;&#26032;&#22312;&#38544;&#31169;&#22797;&#26434;&#24230;&#26435;&#34913;&#26041;&#38754;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21453;&#36951;&#24536;&#26041;&#27861;&#20855;&#26377;&#35832;&#22810;&#31639;&#27861;&#20248;&#21183;&#65292;&#21253;&#25324;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#30340;&#22797;&#26434;&#24230;&#33410;&#30465;&#65292;&#20197;&#21450;&#25903;&#25345;&#39034;&#24207;&#21644;&#25209;&#37327;&#21453;&#36951;&#24536;&#12290;&#20026;&#20102;&#26816;&#39564;&#25105;&#20204;&#26041;&#27861;&#30340;&#38544;&#31169;-&#25928;&#29992;-&#22797;&#26434;&#24230;&#26435;&#34913;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17105v1 Announce Type: new  Abstract: ``The right to be forgotten'' ensured by laws for user data privacy becomes increasingly important. Machine unlearning aims to efficiently remove the effect of certain data points on the trained model parameters so that it can be approximately the same as if one retrains the model from scratch. This work proposes stochastic gradient Langevin unlearning, the first unlearning framework based on noisy stochastic gradient descent (SGD) with privacy guarantees for approximate unlearning problems under convexity assumption. Our results show that mini-batch gradient updates provide a superior privacy-complexity trade-off compared to the full-batch counterpart. There are numerous algorithmic benefits of our unlearning approach, including complexity saving compared to retraining, and supporting sequential and batch unlearning. To examine the privacy-utility-complexity trade-off of our method, we conduct experiments on benchmark datasets compared 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#36923;&#36753;&#38376;&#26469;&#23454;&#29616;&#30005;&#36335;&#35774;&#35745;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13838</link><description>&lt;p&gt;
&#30005;&#36335;&#21464;&#21387;&#22120;&#65306;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#38376;&#23454;&#29616;&#31471;&#21040;&#31471;&#30005;&#36335;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Circuit Transformer: End-to-end Circuit Design by Predicting the Next Gate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#39044;&#27979;&#19979;&#19968;&#20010;&#36923;&#36753;&#38376;&#26469;&#23454;&#29616;&#30005;&#36335;&#35774;&#35745;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26159;&#20154;&#31867;&#36890;&#36807;&#24207;&#21015;&#31526;&#21495;&#34920;&#36798;&#30340;&#31361;&#20986;&#33021;&#21147;&#65292;&#36817;&#24180;&#26469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#22312;&#35745;&#31639;&#19978;&#25484;&#25569;&#20102;&#36825;&#31181;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;&#24040;&#22823;&#30340;&#31070;&#32463;&#27169;&#22411;&#19981;&#26029;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#65292;LLMs&#23637;&#29616;&#20986;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#30005;&#36335;&#20316;&#20026;&#30005;&#23376;&#35774;&#35745;&#30340;&#8220;&#35821;&#35328;&#8221;&#65292;&#36890;&#36807;&#36923;&#36753;&#38376;&#30340;&#32423;&#32852;&#36830;&#25509;&#26469;&#25351;&#23450;&#30005;&#23376;&#35774;&#22791;&#30340;&#21151;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#36825;&#31181;&#21487;&#33021;&#24615;&#65292;&#20197;&#36890;&#36807;&#31616;&#21333;&#22320;&#39044;&#27979;&#19979;&#19968;&#20010;&#36923;&#36753;&#38376;&#26469;&#24449;&#26381;&#30005;&#23376;&#35774;&#35745;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13838v1 Announce Type: new  Abstract: Language, a prominent human ability to express through sequential symbols, has been computationally mastered by recent advances of large language models (LLMs). By predicting the next word recurrently with huge neural models, LLMs have shown unprecedented capabilities in understanding and reasoning. Circuit, as the "language" of electronic design, specifies the functionality of an electronic device by cascade connections of logic gates. Then, can circuits also be mastered by a a sufficiently large "circuit model", which can conquer electronic design tasks by simply predicting the next logic gate? In this work, we take the first step to explore such possibilities. Two primary barriers impede the straightforward application of LLMs to circuits: their complex, non-sequential structure, and the intolerance of hallucination due to strict constraints (e.g., equivalence). For the first barrier, we encode a circuit as a memory-less, depth-first 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#21464;&#31181;&#22312;&#35782;&#21035;&#12289;&#25512;&#29702;&#21644;&#22522;&#20934;&#30830;&#23450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24191;&#27867;&#33021;&#21147;&#21644;&#38480;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;</title><link>https://arxiv.org/abs/2403.13164</link><description>&lt;p&gt;
VL-ICL Bench: &#22522;&#20110;&#32454;&#33410;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#32454;&#33410;&#20043;&#39764;
&lt;/p&gt;
&lt;p&gt;
VL-ICL Bench: The Devil in the Details of Benchmarking Multimodal In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13164
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#21464;&#31181;&#22312;&#35782;&#21035;&#12289;&#25512;&#29702;&#21644;&#22522;&#20934;&#30830;&#23450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24191;&#27867;&#33021;&#21147;&#21644;&#38480;&#21046;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20854;&#33879;&#21517;&#30340;&#20986;&#29616;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#32780;&#38395;&#21517;&#8212;&#8212;&#21363;&#22312;&#20165;&#25552;&#20379;&#20960;&#20010;&#31034;&#20363;&#20316;&#20026;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#32780;&#26080;&#38656;&#26356;&#26032;&#27169;&#22411;&#30340;&#26435;&#37325;&#12290;&#26500;&#24314;&#22312;LLMs&#20043;&#19978;&#30340;&#35270;&#35273;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;VLLMs&#65289;&#22312;&#35782;&#21035;&#12289;&#25512;&#29702;&#21644;&#22522;&#20934;&#30830;&#23450;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;\emph{&#22810;&#27169;&#24577;ICL}&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23569;&#26679;&#26412;&#35270;&#35273;&#38382;&#39064;&#22238;&#31572;&#65288;VQA&#65289;&#21644;&#22270;&#20687;&#23383;&#24149;&#19978;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#20108;&#32773;&#26082;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;ICL&#30340;&#20248;&#21183;&#65292;&#20063;&#27809;&#26377;&#27979;&#35797;&#20854;&#38480;&#21046;&#12290;&#23545;&#22810;&#27169;&#24577;ICL&#30340;&#26356;&#24191;&#27867;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797; VL-ICL Bench&#65292;&#28085;&#30422;&#20102;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#20316;&#20026;&#36755;&#20837;&#21644;&#36755;&#20986;&#30340;&#24191;&#27867;&#20219;&#21153;&#33539;&#22260;&#65292;&#24182;&#28085;&#30422;&#20102;&#20174;{&#24863;&#30693;&#21040;&#25512;&#29702;&#21644;&#38271;&#26399;&#19978;&#19979;&#25991;&#38271;&#24230;}&#30340;&#19981;&#21516;&#31867;&#22411;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13164v1 Announce Type: new  Abstract: Large language models (LLMs) famously exhibit emergent in-context learning (ICL) -- the ability to rapidly adapt to new tasks using few-shot examples provided as a prompt, without updating the model's weights. Built on top of LLMs, vision large language models (VLLMs) have advanced significantly in areas such as recognition, reasoning, and grounding. However, investigations into \emph{multimodal ICL} have predominantly focused on few-shot visual question answering (VQA), and image captioning, which we will show neither exploit the strengths of ICL, nor test its limitations. The broader capabilities and limitations of multimodal ICL remain under-explored. In this study, we introduce a comprehensive benchmark VL-ICL Bench for multimodal in-context learning, encompassing a broad spectrum of tasks that involve both images and text as inputs and outputs, and different types of challenges, from {perception to reasoning and long context length}
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#8220;&#30417;&#30563;&#24494;&#35843;+&#20154;&#31867;&#27604;&#36739;&#8221;&#26694;&#26550;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#20154;&#31867;&#27604;&#36739;&#26469;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#22024;&#26434;&#25968;&#25454;&#21644;&#39640;&#32500;&#27169;&#22411;&#26102;&#12290;</title><link>https://arxiv.org/abs/2403.10771</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#30340;&#20154;&#31867;&#27604;&#36739;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Probabilistic Approach for Alignment with Human Comparisons
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10771
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#20004;&#38454;&#27573;&#8220;&#30417;&#30563;&#24494;&#35843;+&#20154;&#31867;&#27604;&#36739;&#8221;&#26694;&#26550;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#20154;&#31867;&#27604;&#36739;&#26469;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#23545;&#40784;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#22024;&#26434;&#25968;&#25454;&#21644;&#39640;&#32500;&#27169;&#22411;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22686;&#38271;&#30340;&#36235;&#21183;&#26159;&#23558;&#20154;&#31867;&#30693;&#35782;&#25972;&#21512;&#21040;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#21033;&#29992;&#24494;&#22937;&#30340;&#20154;&#31867;&#21453;&#39304;&#26469;&#23436;&#21892;AI&#27169;&#22411;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#23578;&#26410;&#24320;&#21457;&#20986;&#25551;&#36848;&#20154;&#31867;&#27604;&#36739;&#20309;&#26102;&#25913;&#21892;&#20256;&#32479;&#30417;&#30563;&#24494;&#35843;&#36807;&#31243;&#30340;&#29305;&#23450;&#26465;&#20214;&#30340;&#20840;&#38754;&#29702;&#35770;&#26694;&#26550;&#12290;&#20026;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#26377;&#25928;&#21033;&#29992;&#20154;&#31867;&#27604;&#36739;&#26469;&#35299;&#20915;&#30001;&#22024;&#26434;&#25968;&#25454;&#21644;&#39640;&#32500;&#27169;&#22411;&#24341;&#36215;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26426;&#22120;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#36890;&#36807;&#27010;&#29575;&#20108;&#20998;&#26041;&#27861;&#32852;&#31995;&#36215;&#26469;&#30340;&#20004;&#38454;&#27573;&#8220;&#30417;&#30563;&#24494;&#35843;+&#20154;&#31867;&#27604;&#36739;&#8221;&#65288;SFT+HC&#65289;&#26694;&#26550;&#12290;&#36825;&#20004;&#38454;&#27573;&#26694;&#26550;&#39318;&#20808;&#36890;&#36807;SFT&#36807;&#31243;&#20174;&#24102;&#26377;&#22122;&#22768;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#20302;&#32500;&#34920;&#31034;&#65292;&#28982;&#21518;&#21033;&#29992;&#20154;&#31867;&#27604;&#36739;&#26469;&#25913;&#36827;&#27169;&#22411;&#23545;&#40784;&#12290;&#20026;&#20102;&#26816;&#39564;&#23545;&#40784;&#38454;&#27573;&#30340;&#25928;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#27010;&#24565;&#65292;&#31216;&#20026;&#8220;&#26631;&#31614;&#22122;&#22768;&#21040;&#19968;&#33268;&#24615;&#8221;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10771v1 Announce Type: new  Abstract: A growing trend involves integrating human knowledge into learning frameworks, leveraging subtle human feedback to refine AI models. Despite these advances, no comprehensive theoretical framework describing the specific conditions under which human comparisons improve the traditional supervised fine-tuning process has been developed. To bridge this gap, this paper studies the effective use of human comparisons to address limitations arising from noisy data and high-dimensional models. We propose a two-stage "Supervised Fine Tuning+Human Comparison" (SFT+HC) framework connecting machine learning with human feedback through a probabilistic bisection approach. The two-stage framework first learns low-dimensional representations from noisy-labeled data via an SFT procedure, and then uses human comparisons to improve the model alignment. To examine the efficacy of the alignment phase, we introduce a novel concept termed the "label-noise-to-co
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IPRO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20998;&#35299;&#20219;&#21153;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#26041;&#27861;&#65292;&#21487;&#21487;&#38752;&#22320;&#25581;&#31034;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26368;&#20248;&#34920;&#29616;&#30340;&#31574;&#30053;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#21516;&#26102;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#21644;&#26410;&#21457;&#29616;&#35299;&#30340;&#36317;&#31163;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.07182</link><description>&lt;p&gt;
&#20998;&#32780;&#27835;&#20043;&#65306;&#29992;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#21487;&#38752;&#22320;&#25581;&#31034;&#24085;&#32047;&#25176;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Divide and Conquer: Provably Unveiling the Pareto Front with Multi-Objective Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07182
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IPRO&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#20998;&#35299;&#20219;&#21153;&#20026;&#19968;&#31995;&#21015;&#21333;&#30446;&#26631;&#38382;&#39064;&#26041;&#27861;&#65292;&#21487;&#21487;&#38752;&#22320;&#25581;&#31034;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#29616;&#26368;&#20248;&#34920;&#29616;&#30340;&#31574;&#30053;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#21516;&#26102;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#21644;&#26410;&#21457;&#29616;&#35299;&#30340;&#36317;&#31163;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#33719;&#21462;&#22312;&#19981;&#21516;&#20559;&#22909;&#19979;&#23454;&#29616;&#26368;&#20248;&#34920;&#29616;&#30340;&#31574;&#30053;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;&#24085;&#32047;&#25176;&#21442;&#32771;&#20248;&#21270;&#65288;IPRO&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#21407;&#21017;&#24615;&#31639;&#27861;&#65292;&#23427;&#23558;&#25214;&#21040;&#24085;&#32047;&#25176;&#21069;&#27839;&#30340;&#20219;&#21153;&#20998;&#35299;&#25104;&#19968;&#31995;&#21015;&#20855;&#26377;&#21508;&#31181;&#35299;&#20915;&#26041;&#27861;&#30340;&#21333;&#30446;&#26631;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#27599;&#20010;&#27493;&#39588;&#20013;&#24314;&#31435;&#25910;&#25947;&#20445;&#35777;&#24182;&#25552;&#20379;&#26410;&#21457;&#29616;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#30340;&#36317;&#31163;&#19978;&#38480;&#12290;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;IPRO&#33021;&#22815;&#19982;&#38656;&#35201;&#39069;&#22806;&#39046;&#22495;&#30693;&#35782;&#30340;&#26041;&#27861;&#30456;&#21305;&#37197;&#25110;&#20248;&#20110;&#23427;&#20204;&#12290;&#36890;&#36807;&#21033;&#29992;&#38382;&#39064;&#29305;&#23450;&#30340;&#21333;&#30446;&#26631;&#27714;&#35299;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#26377;&#26395;&#22312;&#22810;&#30446;&#26631;&#24378;&#21270;&#23398;&#20064;&#20043;&#22806;&#30340;&#24212;&#29992;&#20013;&#21457;&#25381;&#20316;&#29992;&#65292;&#27604;&#22914;&#36335;&#24452;&#35268;&#21010;&#21644;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant challenge in multi-objective reinforcement learning is obtaining a Pareto front of policies that attain optimal performance under different preferences. We introduce Iterated Pareto Referent Optimisation (IPRO), a principled algorithm that decomposes the task of finding the Pareto front into a sequence of single-objective problems for which various solution methods exist. This enables us to establish convergence guarantees while providing an upper bound on the distance to undiscovered Pareto optimal solutions at each step. Empirical evaluations demonstrate that IPRO matches or outperforms methods that require additional domain knowledge. By leveraging problem-specific single-objective solvers, our approach also holds promise for applications beyond multi-objective reinforcement learning, such as in pathfinding and optimisation.
&lt;/p&gt;</description></item><item><title>ForestColl&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#35843;&#24230;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#26500;&#24314;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#30340;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#20110;&#20379;&#24212;&#21830;&#33258;&#24102;&#36890;&#20449;&#24211;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06787</link><description>&lt;p&gt;
ForestColl: &#24322;&#26500;&#32593;&#32476;&#32467;&#26500;&#19978;&#39640;&#25928;&#30340;&#38598;&#21512;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
ForestColl: Efficient Collective Communications on Heterogeneous Network Fabrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06787
&lt;/p&gt;
&lt;p&gt;
ForestColl&#26159;&#19968;&#31181;&#38024;&#23545;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#35843;&#24230;&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#26500;&#24314;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#30340;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#20110;&#20379;&#24212;&#21830;&#33258;&#24102;&#36890;&#20449;&#24211;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#36234;&#26469;&#36234;&#22823;&#65292;&#21152;&#36895;&#22120;&#20043;&#38388;&#30340;&#38598;&#21512;&#36890;&#20449;&#65288;&#22914;allreduce&#31561;&#65289;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24615;&#33021;&#29942;&#39048;&#12290;&#22312;&#24403;&#20170;&#39640;&#24230;&#22810;&#26679;&#21270;&#21644;&#24322;&#26500;&#30340;&#32593;&#32476;&#32467;&#26500;&#19979;&#35774;&#35745;&#39640;&#25928;&#30340;&#36890;&#20449;&#35843;&#24230;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ForestColl&#30340;&#24037;&#20855;&#65292;&#23427;&#33021;&#22815;&#20026;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#29983;&#25104;&#39640;&#25928;&#30340;&#35843;&#24230;&#12290;ForestColl&#20351;&#29992;&#24191;&#25773;/&#32858;&#21512;&#29983;&#25104;&#36328;&#36234;&#26641;&#20316;&#20026;&#36890;&#20449;&#35843;&#24230;&#65292;&#23454;&#29616;&#20102;&#29702;&#35770;&#19978;&#30340;&#26368;&#23567;&#32593;&#32476;&#25317;&#22622;&#12290;&#20854;&#35843;&#24230;&#29983;&#25104;&#36816;&#34892;&#22312;&#24378;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#65292;&#19988;&#20855;&#26377;&#39640;&#25193;&#23637;&#24615;&#12290;ForestColl&#25903;&#25345;&#21253;&#25324;&#20132;&#25442;&#32593;&#32476;&#21644;&#30452;&#25509;&#36830;&#25509;&#22312;&#20869;&#30340;&#20219;&#20309;&#32593;&#32476;&#32467;&#26500;&#65292;&#20197;&#21450;&#20219;&#20309;&#32593;&#32476;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#22312;&#22810;&#38598;&#32676;&#30340;AMD MI250&#21644;NVIDIA A100&#24179;&#21488;&#19978;&#35780;&#20272;&#20102;ForestColl&#12290;&#19982;&#20379;&#24212;&#21830;&#33258;&#24049;&#20248;&#21270;&#30340;&#36890;&#20449;&#24211;RCCL&#21644;NCCL&#30456;&#27604;&#65292;ForestColl&#30340;&#35843;&#24230;&#24615;&#33021;&#25552;&#39640;&#20102;&#39640;&#36798;52&#65285;&#12290;ForestColl&#36824;&#20248;&#20110;&#20854;&#20182;...
&lt;/p&gt;
&lt;p&gt;
As modern DNN models grow ever larger, collective communications between the accelerators (allreduce, etc.) emerge as a significant performance bottleneck. Designing efficient communication schedules is challenging given today's highly diverse and heterogeneous network fabrics. In this paper, we present ForestColl, a tool that generates efficient schedules for any network topology. ForestColl constructs broadcast/aggregation spanning trees as the communication schedule, achieving theoretically minimum network congestion. Its schedule generation runs in strongly polynomial time and is highly scalable. ForestColl supports any network fabrics, including both switching fabrics and direct connections, as well as any network graph structure. We evaluated ForestColl on multi-cluster AMD MI250 and NVIDIA A100 platforms. ForestColl's schedules achieved up to 52\% higher performance compared to the vendors' own optimized communication libraries, RCCL and NCCL. ForestColl also outperforms other s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.06104</link><description>&lt;p&gt;
&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65306;&#19968;&#31181;&#20174;&#25968;&#25454;&#20013;&#26126;&#30830;&#23398;&#20064;&#20989;&#25968;&#23548;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Function Aligned Regression: A Method Explicitly Learns Functional Derivatives from Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06104
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#22320;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20843;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#22238;&#24402;&#26041;&#27861;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#25439;&#22833;&#20989;&#25968;&#26469;&#23558;&#27169;&#22411;&#39044;&#27979;&#19982;&#27599;&#20010;&#20010;&#20307;&#25968;&#25454;&#26679;&#26412;&#30340;&#30495;&#23454;&#20540;&#23545;&#40784;&#65292;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#20851;&#31995;&#30340;&#39044;&#27979;&#19981;&#22815;&#20248;&#21270;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#24037;&#20316;&#24341;&#20837;&#20102;&#26631;&#31614;&#30456;&#20284;&#24615;&#20449;&#24687;&#26469;&#25913;&#36827;&#22238;&#24402;&#26041;&#27861;&#65292;&#20294;&#22312;&#23436;&#20840;&#25429;&#25417;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#30340;&#22797;&#26434;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FAR&#65288;&#21151;&#33021;&#23545;&#40784;&#22238;&#24402;&#65289;&#20316;&#20026;&#19968;&#31181;&#26356;&#22909;&#12289;&#26356;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#25429;&#25417;&#20989;&#25968;&#23548;&#25968;&#26469;&#25311;&#21512;&#24213;&#23618;&#30495;&#23454;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#20845;&#20010;&#39046;&#22495;&#30340;&#20843;&#20010;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#20013;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regression is a fundamental task in machine learning that has garnered extensive attention over the past decades. The conventional approach for regression involves employing loss functions that primarily concentrate on aligning model prediction with the ground truth for each individual data sample, which, as we show, can result in sub-optimal prediction of the relationships between the different samples. Recent research endeavors have introduced novel perspectives by incorporating label similarity information to regression. However, a notable gap persists in these approaches when it comes to fully capturing the intricacies of the underlying ground truth function. In this work, we propose FAR (Function Aligned Regression) as a arguably better and more efficient solution to fit the underlying function of ground truth by capturing functional derivatives. We demonstrate the effectiveness of the proposed method practically on 2 synthetic datasets and on 8 extensive real-world tasks from 6 b
&lt;/p&gt;</description></item><item><title>EUGENE&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#22270;&#32534;&#36753;&#36317;&#31163;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#32534;&#36753;&#36335;&#24452;&#26469;&#36817;&#20284;&#35745;&#31639;&#22270;&#32534;&#36753;&#36317;&#31163;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;ground-truth&#29983;&#25104;&#21644;&#25968;&#25454;&#29305;&#23450;&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2402.05885</link><description>&lt;p&gt;
EUGENE: &#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#22270;&#32534;&#36753;&#36317;&#31163;&#36817;&#20284;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EUGENE: Explainable Unsupervised Approximation of Graph Edit Distance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05885
&lt;/p&gt;
&lt;p&gt;
EUGENE&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#26080;&#30417;&#30563;&#22270;&#32534;&#36753;&#36317;&#31163;&#36817;&#20284;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#32534;&#36753;&#36335;&#24452;&#26469;&#36817;&#20284;&#35745;&#31639;&#22270;&#32534;&#36753;&#36317;&#31163;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;ground-truth&#29983;&#25104;&#21644;&#25968;&#25454;&#29305;&#23450;&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#23398;&#12289;&#21270;&#23398;&#12289;&#25512;&#33616;&#31995;&#32479;&#21644;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#31561;&#39046;&#22495;&#65292;&#38656;&#35201;&#35782;&#21035;&#19982;&#26597;&#35810;&#22270;&#32467;&#26500;&#36317;&#31163;&#36739;&#23567;&#30340;&#22270;&#24418;&#12290;&#22312;&#22810;&#31181;&#27979;&#37327;&#22270;&#38388;&#36317;&#31163;&#30340;&#26041;&#27861;&#20013;&#65292;&#22270;&#32534;&#36753;&#36317;&#31163;&#65288;GED&#65289;&#22240;&#20854;&#21487;&#29702;&#35299;&#24615;&#32780;&#34987;&#35748;&#20026;&#26159;&#39318;&#36873;&#65292;&#20294;&#20854;&#35745;&#31639;&#30340;NP&#38590;&#24230;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;GED&#36817;&#20284;&#26041;&#27861;&#20027;&#35201;&#37319;&#29992;&#31070;&#32463;&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#65288;i&#65289;&#32570;&#23569;&#19982;&#36817;&#20284;&#30340;GED&#23545;&#24212;&#30340;&#35299;&#37322;&#24615;&#32534;&#36753;&#36335;&#24452;&#65307;&#65288;ii&#65289;&#38656;&#35201;&#36890;&#36807;NP&#38590;&#38382;&#39064;&#29983;&#25104;ground-truth GED&#36827;&#34892;&#35757;&#32451;&#65307;&#65288;iii&#65289;&#38656;&#35201;&#22312;&#27599;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#29420;&#31435;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20195;&#25968;&#26080;&#30417;&#30563;&#26041;&#27861;EUGENE&#65292;&#23427;&#36817;&#20284;&#35745;&#31639;GED&#24182;&#29983;&#25104;&#19982;&#36817;&#20284;&#25104;&#26412;&#23545;&#24212;&#30340;&#32534;&#36753;&#36335;&#24452;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29983;&#25104;ground-truth&#21644;&#25968;&#25454;&#29305;&#23450;&#35757;&#32451;&#30340;&#38656;&#27714;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;EUGENE&#30340;&#19978;&#36848;&#20248;&#28857;&#24182;&#19981;&#20197;&#25928;&#21147;&#20026;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
The need to identify graphs having small structural distance from a query arises in biology, chemistry, recommender systems, and social network analysis. Among several methods to measure inter graph distance, Graph Edit Distance (GED) is preferred for its comprehensibility, yet hindered by the NP-hardness of its computation. State-of-the-art GED approximations predominantly employ neural methods, which, however, (i) lack an explanatory edit path corresponding to the approximated GED; (ii) require the NP-hard generation of ground-truth GEDs for training; and (iii) necessitate separate training on each dataset. In this paper, we propose an efficient algebraic unsuper vised method, EUGENE, that approximates GED and yields edit paths corresponding to the approx imated cost, while eliminating the need for ground truth generation and data-specific training. Extensive experimental evaluation demonstrates that the aforementioned benefits of EUGENE do not come at the cost of efficacy. Specifica
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04676</link><description>&lt;p&gt;
&#24102;&#39118;&#38505;&#26368;&#23567;&#21270;&#30340;&#20998;&#32452;&#20998;&#24067;&#40065;&#26834;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Group Distributionally Robust Dataset Distillation with Risk Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04676
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#38598;&#33976;&#39311;&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#22914;&#20309;&#30830;&#20445;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#30340;&#35757;&#32451;&#21487;&#20197;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;DD&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#22312;&#25429;&#25417;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#26041;&#38754;&#36215;&#21040;&#37325;&#35201;&#20316;&#29992;&#65292;&#20174;&#32780;&#26041;&#20415;&#20934;&#30830;&#35757;&#32451;&#31070;&#32463;&#27169;&#22411;&#12290;&#20854;&#24212;&#29992;&#28085;&#30422;&#20102;&#36716;&#31227;&#23398;&#20064;&#12289;&#32852;&#37030;&#23398;&#20064;&#21644;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31561;&#21508;&#20010;&#39046;&#22495;&#12290;&#26500;&#24314;&#21512;&#25104;&#25968;&#25454;&#30340;&#26368;&#27969;&#34892;&#26041;&#27861;&#20381;&#36182;&#20110;&#20351;&#27169;&#22411;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#26159;&#23558;&#35757;&#32451;&#25968;&#25454;&#38598;&#35270;&#20026;&#36741;&#21161;&#65292;&#23601;&#20687;&#35757;&#32451;&#38598;&#26159;&#20154;&#21475;&#20998;&#24067;&#30340;&#36817;&#20284;&#26367;&#20195;&#21697;&#19968;&#26679;&#65292;&#32780;&#21518;&#32773;&#25165;&#26159;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#12290;&#23613;&#31649;&#20854;&#21463;&#27426;&#36814;&#31243;&#24230;&#24456;&#39640;&#65292;&#20294;&#23578;&#26410;&#25506;&#32034;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;DD&#19982;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#20851;&#31995;&#65292;&#29305;&#21035;&#26159;&#36328;&#19981;&#24120;&#35265;&#30340;&#23376;&#32452;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24403;&#38754;&#23545;&#26469;&#33258;&#32597;&#35265;&#23376;&#32452;&#30340;&#26679;&#26412;&#26102;&#65292;&#25105;&#20204;&#22914;&#20309;&#30830;&#20445;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset distillation (DD) has emerged as a widely adopted technique for crafting a synthetic dataset that captures the essential information of a training dataset, facilitating the training of accurate neural models. Its applications span various domains, including transfer learning, federated learning, and neural architecture search. The most popular methods for constructing the synthetic data rely on matching the convergence properties of training the model with the synthetic dataset and the training dataset. However, targeting the training dataset must be thought of as auxiliary in the same sense that the training set is an approximate substitute for the population distribution, and the latter is the data of interest. Yet despite its popularity, an aspect that remains unexplored is the relationship of DD to its generalization, particularly across uncommon subgroups. That is, how can we ensure that a model trained on the synthetic dataset performs well when faced with samples from re
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03973</link><description>&lt;p&gt;
&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#65292;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#20987;&#36133;&#20102;&#28145;&#24230;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Humans Beat Deep Networks at Recognizing Objects in Unusual Poses, Given Enough Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03973
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#19978;&#34920;&#29616;&#20248;&#20110;&#28145;&#24230;&#32593;&#32476;&#65292;&#24403;&#32473;&#20104;&#36275;&#22815;&#26102;&#38388;&#26102;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#30340;&#38480;&#21046;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#38477;&#33267;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#26263;&#31034;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#19982;&#32593;&#32476;&#20043;&#38388;&#30340;&#38169;&#35823;&#27169;&#24335;&#20063;&#23384;&#22312;&#19981;&#21516;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#20960;&#20010;&#29289;&#20307;&#35782;&#21035;&#22522;&#20934;&#19978;&#27491;&#22312;&#32553;&#23567;&#19982;&#20154;&#31867;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#22312;&#28041;&#21450;&#20174;&#19981;&#23547;&#24120;&#35270;&#35282;&#35266;&#23519;&#29289;&#20307;&#30340;&#25361;&#25112;&#24615;&#22270;&#20687;&#20013;&#23545;&#36825;&#19968;&#24046;&#36317;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#34920;&#29616;&#20986;&#33394;&#65292;&#19982;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#65288;EfficientNet&#12289;SWAG&#12289;ViT&#12289;SWIN&#12289;BEiT&#12289;ConvNext&#65289;&#30456;&#27604;&#65292;&#36825;&#20123;&#32593;&#32476;&#22312;&#27492;&#24773;&#20917;&#19979;&#26222;&#36941;&#33030;&#24369;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#38543;&#30528;&#25105;&#20204;&#38480;&#21046;&#22270;&#20687;&#26333;&#20809;&#26102;&#38388;&#65292;&#20154;&#31867;&#30340;&#34920;&#29616;&#19979;&#38477;&#21040;&#28145;&#24230;&#32593;&#32476;&#30340;&#27700;&#24179;&#65292;&#36825;&#34920;&#26126;&#20154;&#31867;&#22312;&#35782;&#21035;&#19981;&#23547;&#24120;&#23039;&#21183;&#20013;&#30340;&#29289;&#20307;&#26102;&#38656;&#35201;&#39069;&#22806;&#30340;&#24515;&#29702;&#36807;&#31243;&#65288;&#38656;&#35201;&#39069;&#22806;&#30340;&#26102;&#38388;&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20154;&#31867;&#19982;&#32593;&#32476;&#30340;&#38169;&#35823;&#27169;&#24335;&#65292;&#21457;&#29616;&#21363;&#20351;&#22312;&#38480;&#21046;&#26102;&#38388;&#30340;&#24773;&#20917;&#19979;&#65292;&#20154;&#31867;&#19982;&#21069;&#39304;&#28145;&#24230;&#32593;&#32476;&#20063;&#26377;&#19981;&#21516;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#24037;&#20316;&#23558;&#35745;&#31639;&#26426;&#35270;&#35273;&#31995;&#32479;&#24102;&#21040;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#27700;&#24179;&#12290;&#29702;&#35299;&#22312;&#22806;&#37096;&#24773;&#20917;&#19979;&#21457;&#29983;&#30340;&#24515;&#29702;&#36807;&#31243;&#30340;&#26412;&#36136;&#26159;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is closing the gap with humans on several object recognition benchmarks. Here we investigate this gap in the context of challenging images where objects are seen from unusual viewpoints. We find that humans excel at recognizing objects in unusual poses, in contrast with state-of-the-art pretrained networks (EfficientNet, SWAG, ViT, SWIN, BEiT, ConvNext) which are systematically brittle in this condition. Remarkably, as we limit image exposure time, human performance degrades to the level of deep networks, suggesting that additional mental processes (requiring additional time) take place when humans identify objects in unusual poses. Finally, our analysis of error patterns of humans vs. networks reveals that even time-limited humans are dissimilar to feed-forward deep networks. We conclude that more work is needed to bring computer vision systems to the level of robustness of the human visual system. Understanding the nature of the mental processes taking place during extr
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03055</link><description>&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#23398;&#20064;&#20197;PAC-Bayes&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03055
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;PAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#32531;&#35299;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#12290;PAC&#36890;&#36807;&#23558;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#26080;&#32541;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28436;&#21592;&#35757;&#32451;&#20043;&#38388;&#30340;&#21160;&#24577;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;PAC&#31639;&#27861;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;Probably Approximately Correct-Bayesian&#65288;PAC-Bayes&#65289;&#20998;&#26512;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#25512;&#26029;&#35780;&#35770;&#23478;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#23545;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#30340;&#34701;&#20837;&#20351;PAC&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#35843;&#25972;&#20854;&#25506;&#32034;&#31574;&#30053;&#65292;&#25351;&#23548;&#28436;&#21592;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#20013;&#30340;&#22266;&#23450;&#25110;&#39044;&#23450;&#30340;&#25506;&#32034;&#26041;&#26696;&#30456;&#27604;&#65292;PAC&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;PAC-Bayes&#20998;&#26512;&#24341;&#23548;&#30340;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26159;&#21521;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26356;&#20855;&#33258;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#31574;&#30053;&#36808;&#20986;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement lear
&lt;/p&gt;</description></item><item><title>&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#35299;&#37322;&#38169;&#35823;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#35299;&#37322;&#31639;&#27861;&#38656;&#35201;&#26126;&#30830;&#20854;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#24182;&#28548;&#28165;&#21487;&#20197;&#21644;&#19981;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02870</link><description>&lt;p&gt;
&#27809;&#26377;&#35299;&#37322;&#30340;&#32479;&#35745;&#23398;&#65306;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#20919;&#38745;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Statistics without Interpretation: A Sober Look at Explainable Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02870
&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#35299;&#37322;&#38169;&#35823;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#35299;&#37322;&#31639;&#27861;&#38656;&#35201;&#26126;&#30830;&#20854;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#24182;&#28548;&#28165;&#21487;&#20197;&#21644;&#19981;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#20110;&#35299;&#37322;&#31639;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#25991;&#29486;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#24448;&#24448;&#19981;&#28165;&#26970;&#25152;&#29992;&#20110;&#20309;&#22788;&#21450;&#20854;&#20351;&#29992;&#26041;&#24335;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#22312;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#28165;&#26224;&#35299;&#37322;&#30340;&#22797;&#26434;&#32479;&#35745;&#26041;&#27861;&#24456;&#21487;&#33021;&#23548;&#33268;&#35299;&#37322;&#30340;&#38169;&#35823;&#65292;&#36825;&#19968;&#20107;&#23454;&#22312;&#25991;&#29486;&#20013;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#20851;&#20110;&#35299;&#37322;&#31639;&#27861;&#30340;&#35770;&#25991;&#24212;&#26126;&#30830;&#35299;&#37322;&#31639;&#27861;&#30340;&#36755;&#20986;&#22914;&#20309;&#35299;&#37322;&#12290;&#20182;&#20204;&#36824;&#24212;&#28548;&#28165;&#22312;&#32473;&#20986;&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22238;&#31572;&#21738;&#20123;&#20851;&#20110;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21738;&#20123;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#23427;&#20204;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#23427;&#36824;&#20381;&#36182;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used. We argue that this is because explanation algorithms are often mathematically complex but don't admit a clear interpretation. Unfortunately, complex statistical methods that don't have a clear interpretation are bound to lead to errors in interpretation, a fact that has become increasingly apparent in the literature. In order to move forward, papers on explanation algorithms should make clear how precisely the output of the algorithms should be interpreted. They should also clarify what questions about the function can and cannot be answered given the explanations. Our argument is based on the distinction between statistics and their interpretation. It also relies on parallels between explainable machine learning and applied statistics.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.02325</link><description>&lt;p&gt;
&#21160;&#37327;&#22312;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#20013;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#20316;&#29992;&#30340;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02325
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#21644;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23545;&#27492;&#32570;&#20047;&#29702;&#35770;&#35299;&#37322;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;SGD&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#31243;&#24230;&#30001;&#23398;&#20064;&#29575;&#12289;&#25209;&#22823;&#23567;&#12289;&#21160;&#37327;&#22240;&#23376;&#12289;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#20197;&#21450;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#30830;&#23450;&#12290;&#36825;&#19968;&#29702;&#35770;&#21457;&#29616;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#21160;&#37327;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21160;&#37327;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#20316;&#29992;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SGD&#21160;&#37327;&#24179;&#28369;&#29305;&#24615;&#30340;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#21363;SGD&#21160;&#37327;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While stochastic gradient descent (SGD) with momentum has fast convergence and excellent generalizability, a theoretical explanation for this is lacking. In this paper, we show that SGD with momentum smooths the objective function, the degree of which is determined by the learning rate, the batch size, the momentum factor, the variance of the stochastic gradient, and the upper bound of the gradient norm. This theoretical finding reveals why momentum improves generalizability and provides new insights into the role of the hyperparameters, including momentum factor. We also present an implicit graduated optimization algorithm that exploits the smoothing properties of SGD with momentum and provide experimental results supporting our assertion that SGD with momentum smooths the objective function.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2311.18703</link><description>&lt;p&gt;
&#36890;&#36807;&#29109;&#29575;&#26368;&#23567;&#21270;&#23454;&#29616;&#21487;&#39044;&#27979;&#30340;&#24378;&#21270;&#23398;&#20064;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Predictable Reinforcement Learning Dynamics through Entropy Rate Minimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18703
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PA-RL&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#29109;&#29575;&#26469;&#24341;&#23548;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#23454;&#29616;&#30830;&#23450;&#24615;&#31574;&#30053;&#65292;&#24182;&#22312;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#27809;&#26377;&#21160;&#26426;&#23637;&#31034;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#36890;&#24120;&#36890;&#36807;&#31574;&#30053;&#29109;&#27491;&#21017;&#21270;&#25512;&#21160;&#26234;&#33021;&#20307;&#22312;&#25506;&#32034;&#19978;&#38543;&#26426;&#21270;&#20854;&#34892;&#20026;&#12290;&#20174;&#20154;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#24456;&#38590;&#35299;&#37322;&#21644;&#39044;&#27979;&#65307;&#20174;&#23433;&#20840;&#35282;&#24230;&#26469;&#30475;&#65292;&#26356;&#38590;&#20197;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21487;&#39044;&#27979;&#24615;&#24863;&#30693;&#24378;&#21270;&#23398;&#20064;&#65288;PA-RL&#65289;&#65292;&#29992;&#20110;&#24341;&#23548;&#26234;&#33021;&#20307;&#23637;&#29616;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#65292;&#20854;&#21033;&#29992;&#29366;&#24577;&#24207;&#21015;&#29109;&#29575;&#20316;&#20026;&#21487;&#39044;&#27979;&#24615;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#29109;&#29575;&#21046;&#23450;&#20026;&#24179;&#22343;&#22870;&#21169;&#30446;&#26631;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#29109;&#22870;&#21169;&#20989;&#25968;&#20381;&#36182;&#20110;&#31574;&#30053;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21160;&#20316;&#30456;&#20851;&#30340;&#26367;&#20195;&#29109;&#65292;&#20197;&#21033;&#29992;PG&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#24179;&#22343;&#26367;&#20195;&#22870;&#21169;&#30340;&#30830;&#23450;&#24615;&#31574;&#30053;&#23384;&#22312;&#65292;&#24182;&#19988;&#26368;&#23567;&#21270;&#20102;&#23454;&#38469;&#29109;&#29575;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#23398;&#20064;&#21040;&#30340;&#21160;&#24577;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#36817;&#20284;&#35745;&#31639;&#19982;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning (RL), agents have no incentive to exhibit predictable behaviors, and are often pushed (through e.g. policy entropy regularization) to randomize their actions in favor of exploration. From a human perspective, this makes RL agents hard to interpret and predict, and from a safety perspective, even harder to formally verify. We propose a novel method to induce predictable behavior in RL agents, referred to as Predictability-Aware RL (PA-RL), which employs the state sequence entropy rate as a predictability measure. We show how the entropy rate can be formulated as an average reward objective, and since its entropy reward function is policy-dependent, we introduce an action-dependent surrogate entropy enabling the use of PG methods. We prove that deterministic policies minimizing the average surrogate reward exist and also minimize the actual entropy rate, and show how, given a learned dynamical model, we are able to approximate the value function associated to th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;DermaMNIST&#21644;Fitzpatrick17k&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#23545;&#25968;&#25454;&#37325;&#22797;&#12289;&#25968;&#25454;&#27844;&#28431;&#12289;&#38169;&#35823;&#26631;&#35760;&#21644;&#32570;&#20047;&#27979;&#35797;&#20998;&#21306;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#32416;&#27491;&#25514;&#26045;&#12290;</title><link>http://arxiv.org/abs/2401.14497</link><description>&lt;p&gt;
&#30740;&#31350;DermaMNIST&#21644;Fitzpatrick17k&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;
&lt;/p&gt;
&lt;p&gt;
Investigating the Quality of DermaMNIST and Fitzpatrick17k Dermatological Image Datasets. (arXiv:2401.14497v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;DermaMNIST&#21644;Fitzpatrick17k&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#38382;&#39064;&#65292;&#23545;&#25968;&#25454;&#37325;&#22797;&#12289;&#25968;&#25454;&#27844;&#28431;&#12289;&#38169;&#35823;&#26631;&#35760;&#21644;&#32570;&#20047;&#27979;&#35797;&#20998;&#21306;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#24182;&#25552;&#20986;&#32416;&#27491;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#30382;&#32932;&#31185;&#20219;&#21153;&#20013;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#20351;&#25105;&#20204;&#26356;&#25509;&#36817;&#20110;&#36798;&#21040;&#19982;&#20154;&#31867;&#19987;&#23478;&#30456;&#24403;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#22823;&#22411;&#25968;&#25454;&#38598;&#22312;&#21487;&#38752;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#24320;&#21457;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20294;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#36136;&#37327;&#21644;&#20854;&#27491;&#30830;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#22810;&#31181;&#22240;&#32032;&#21487;&#20197;&#24433;&#21709;&#25968;&#25454;&#36136;&#37327;&#65292;&#22914;&#37325;&#22797;&#25968;&#25454;&#30340;&#23384;&#22312;&#65292;&#35757;&#32451;-&#27979;&#35797;&#20998;&#21306;&#30340;&#25968;&#25454;&#27844;&#28431;&#65292;&#38169;&#35823;&#26631;&#35760;&#30340;&#22270;&#20687;&#20197;&#21450;&#32570;&#20047;&#26126;&#30830;&#23450;&#20041;&#30340;&#27979;&#35797;&#20998;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#20010;&#27969;&#34892;&#30340;&#30382;&#32932;&#31185;&#22270;&#20687;&#25968;&#25454;&#38598;DermaMNIST&#21644;Fitzpatrick17k&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#36825;&#20123;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#65292;&#27979;&#37327;&#20102;&#36825;&#20123;&#38382;&#39064;&#23545;&#22522;&#20934;&#32467;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#23545;&#25968;&#25454;&#38598;&#25552;&#20986;&#20102;&#32416;&#27491;&#25514;&#26045;&#12290;&#36890;&#36807;&#20844;&#24320;&#25105;&#20204;&#30340;&#20998;&#26512;&#27969;&#31243;&#21644;&#37197;&#22871;&#20195;&#30721;&#65292;&#30830;&#20445;&#25105;&#20204;&#20998;&#26512;&#30340;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#26088;&#22312;&#40723;&#21169;&#31867;&#20284;&#30340;&#25506;&#32034;&#24182;&#20419;&#36827;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable progress of deep learning in dermatological tasks has brought us closer to achieving diagnostic accuracies comparable to those of human experts. However, while large datasets play a crucial role in the development of reliable deep neural network models, the quality of data therein and their correct usage are of paramount importance. Several factors can impact data quality, such as the presence of duplicates, data leakage across train-test partitions, mislabeled images, and the absence of a well-defined test partition. In this paper, we conduct meticulous analyses of two popular dermatological image datasets: DermaMNIST and Fitzpatrick17k, uncovering these data quality issues, measure the effects of these problems on the benchmark results, and propose corrections to the datasets. Besides ensuring the reproducibility of our analysis, by making our analysis pipeline and the accompanying code publicly available, we aim to encourage similar explorations and to facilitate the 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#21464;&#32447;&#24615;&#24178;&#25200;&#20449;&#36947;&#19978;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#30450;&#20449;&#36947;&#20272;&#35745;&#21644;&#32852;&#21512;&#31526;&#21495;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#30456;&#20114;&#20132;&#32455;&#30340;&#36845;&#20195;&#65292;&#21487;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#22312;&#31163;&#32447;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.12627</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#22240;&#23376;&#22270;&#30340;&#30450;&#20449;&#36947;&#20272;&#35745;&#21644;&#32852;&#21512;&#31526;&#21495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Blind Channel Estimation and Joint Symbol Detection with Data-Driven Factor Graphs. (arXiv:2401.12627v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12627
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26102;&#21464;&#32447;&#24615;&#24178;&#25200;&#20449;&#36947;&#19978;&#22522;&#20110;&#22240;&#23376;&#22270;&#30340;&#30450;&#20449;&#36947;&#20272;&#35745;&#21644;&#32852;&#21512;&#31526;&#21495;&#26816;&#27979;&#26041;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#32622;&#20449;&#20256;&#25773;&#31639;&#27861;&#21644;&#26399;&#26395;&#26368;&#22823;&#21270;&#31639;&#27861;&#30456;&#20114;&#20132;&#32455;&#30340;&#36845;&#20195;&#65292;&#21487;&#20197;&#38477;&#20302;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;&#36890;&#36807;&#24341;&#20837;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#31639;&#27861;&#22312;&#31163;&#32447;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#21462;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26102;&#21464;&#32447;&#24615;&#24178;&#25200;&#20449;&#36947;&#19978;&#30450;&#32852;&#21512;&#20449;&#36947;&#20272;&#35745;&#21644;&#31526;&#21495;&#26816;&#27979;&#30340;&#22240;&#23376;&#22270;&#26694;&#26550;&#30340;&#24212;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#26399;&#26395;&#26368;&#22823;&#21270;&#65288;EM&#65289;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#24120;&#30001;&#20110;&#38656;&#35201;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#35745;&#31639;&#36880;&#31526;&#21495;&#21518;&#39564;&#20998;&#24067;&#32780;&#23548;&#33268;&#35745;&#31639;&#22797;&#26434;&#24230;&#39640;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#36866;&#24403;&#30340;&#22240;&#23376;&#22270;&#19978;&#20351;&#29992;&#32622;&#20449;&#20256;&#25773;&#65288;BP&#65289;&#31639;&#27861;&#26469;&#26377;&#25928;&#22320;&#36924;&#36817;&#21518;&#39564;&#20998;&#24067;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#36890;&#36807;&#20132;&#32455;BP&#21644;EM&#30340;&#36845;&#20195;&#65292;&#26816;&#27979;&#22797;&#26434;&#24230;&#36827;&#19968;&#27493;&#20943;&#23569;&#21040;&#27599;&#20010;EM&#27493;&#39588;&#21482;&#38656;&#35201;&#19968;&#27425;BP&#36845;&#20195;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#31639;&#27861;&#30340;&#25968;&#25454;&#39537;&#21160;&#29256;&#26412;&#65292;&#23427;&#24341;&#20837;&#20102;BP&#26356;&#26032;&#30340;&#21160;&#37327;&#65292;&#24182;&#23398;&#20064;&#20102;&#36866;&#24403;&#30340;EM&#21442;&#25968;&#26356;&#26032;&#35745;&#21010;&#65292;&#20174;&#32780;&#22312;&#20165;&#26377;&#23569;&#37327;&#31163;&#32447;&#35757;&#32451;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25913;&#21892;&#20102;&#24615;&#33021;-&#22797;&#26434;&#24230;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the application of the factor graph framework for blind joint channel estimation and symbol detection on time-variant linear inter-symbol interference channels. In particular, we consider the expectation maximization (EM) algorithm for maximum likelihood estimation, which typically suffers from high complexity as it requires the computation of the symbol-wise posterior distributions in every iteration. We address this issue by efficiently approximating the posteriors using the belief propagation (BP) algorithm on a suitable factor graph. By interweaving the iterations of BP and EM, the detection complexity can be further reduced to a single BP iteration per EM step. In addition, we propose a data-driven version of our algorithm that introduces momentum in the BP updates and learns a suitable EM parameter update schedule, thereby significantly improving the performance-complexity tradeoff with a few offline training samples. Our numerical experiments demonstrate the excel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.05858</link><description>&lt;p&gt;
DSAC-T: &#24102;&#26377;&#19977;&#20010;&#25913;&#36827;&#30340;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#8212;&#35780;&#35770;&#32773;
&lt;/p&gt;
&lt;p&gt;
DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;DSAC-T&#65292;&#36890;&#36807;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#31561;&#19977;&#20010;&#25913;&#36827;&#23545;&#26631;&#20934;DSAC&#36827;&#34892;&#20102;&#25913;&#36827;&#65292;&#35299;&#20915;&#20102;&#26631;&#20934;DSAC&#23384;&#22312;&#30340;&#19981;&#31283;&#23450;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#22870;&#21169;&#32553;&#25918;&#30340;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#20915;&#31574;&#21644;&#25511;&#21046;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#34987;&#35777;&#26126;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24448;&#24448;&#38754;&#20020;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#20247;&#25152;&#21608;&#30693;&#30340;&#36807;&#20272;&#35745;&#38382;&#39064;&#25152;&#24341;&#36215;&#30340;&#12290;&#20316;&#20026;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#26368;&#36817;&#24341;&#20837;&#20102;&#19968;&#31181;&#31163;&#32447;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#31216;&#20026;&#20998;&#24067;&#24335;&#36719;&#35282;&#33394;&#25198;&#28436;&#32773;&#35780;&#35770;&#32773;&#65288;DSAC&#25110;DSAC-v1&#65289;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;&#36830;&#32493;&#30340;&#39640;&#26031;&#20540;&#20998;&#24067;&#26469;&#26377;&#25928;&#25552;&#39640;&#20540;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;DSAC&#20063;&#23384;&#22312;&#19968;&#20123;&#32570;&#28857;&#65292;&#21253;&#25324;&#26102;&#32780;&#19981;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#21644;&#23545;&#20219;&#21153;&#29305;&#23450;&#30340;&#22870;&#21169;&#32553;&#25918;&#30340;&#38656;&#27714;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#20854;&#22312;&#19968;&#20123;&#29305;&#27530;&#20219;&#21153;&#20013;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;&#26412;&#25991;&#36827;&#19968;&#27493;&#24341;&#20837;&#20102;&#19977;&#20010;&#23545;&#26631;&#20934;DSAC&#30340;&#37325;&#35201;&#25913;&#36827;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#20123;&#25913;&#36827;&#21253;&#25324;&#35780;&#35770;&#32773;&#26799;&#24230;&#35843;&#25972;&#12289;&#21452;&#20540;&#20998;&#24067;&#23398;&#20064;&#21644;&#22522;&#20110;&#26041;&#24046;&#30340;&#30446;&#26631;&#22238;&#25253;&#35009;&#21098;&#12290;&#20462;&#25913;&#21518;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#31216;&#20026;DSAC-T&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has proven to be highly effective in tackling complex decision-making and control tasks. However, prevalent model-free RL methods often face severe performance degradation due to the well-known overestimation issue. In response to this problem, we recently introduced an off-policy RL algorithm, called distributional soft actor-critic (DSAC or DSAC-v1), which can effectively improve the value estimation accuracy by learning a continuous Gaussian value distribution. Nonetheless, standard DSAC has its own shortcomings, including occasionally unstable learning processes and needs for task-specific reward scaling, which may hinder its overall performance and adaptability in some special tasks. This paper further introduces three important refinements to standard DSAC in order to address these shortcomings. These refinements consist of critic gradient adjusting, twin value distribution learning, and variance-based target return clipping. The modified RL algorithm 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#23548;&#21521;&#30340;&#26041;&#27861;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#24847;OT&#25104;&#26412;&#20989;&#25968;&#30340;&#36830;&#32493;&#29109;OT&#24052;&#27663;&#20013;&#24515;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#19982;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23398;&#20064;&#36807;&#31243;&#26080;&#32541;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2310.01105</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#23548;&#21521;&#30340;&#36830;&#32493;&#29109;&#24052;&#27663;&#20013;&#24515;&#20272;&#35745;&#26041;&#27861;&#21450;&#20854;&#22312;&#19968;&#33324;&#25104;&#26412;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Energy-Guided Continuous Entropic Barycenter Estimation for General Costs. (arXiv:2310.01105v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01105
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33021;&#37327;&#23548;&#21521;&#30340;&#26041;&#27861;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#24847;OT&#25104;&#26412;&#20989;&#25968;&#30340;&#36830;&#32493;&#29109;OT&#24052;&#27663;&#20013;&#24515;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#33021;&#19982;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23398;&#20064;&#36807;&#31243;&#26080;&#32541;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#36755;&#36816;&#65288;OT&#65289;&#24052;&#27663;&#20013;&#24515;&#26159;&#19968;&#31181;&#22312;&#25429;&#25417;&#27010;&#29575;&#20998;&#24067;&#20960;&#20309;&#29305;&#24615;&#30340;&#21516;&#26102;&#23545;&#20854;&#36827;&#34892;&#24179;&#22343;&#30340;&#25968;&#23398;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#36817;&#20284;&#35745;&#31639;&#20219;&#24847;OT&#25104;&#26412;&#20989;&#25968;&#30340;&#36830;&#32493;&#29109;OT&#24052;&#27663;&#20013;&#24515;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#21463;&#21040;&#20851;&#27880;&#30340;&#22522;&#20110;&#24369;OT&#30340;&#36830;&#32493;&#29109;&#26368;&#20248;&#36755;&#36816;&#38382;&#39064;&#30340;&#23545;&#20598;&#37325;&#26500;&#12290;&#38500;&#20102;&#21019;&#26032;&#24615;&#20043;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#20855;&#26377;&#20197;&#19979;&#33509;&#24178;&#20248;&#21183;&#29305;&#28857;&#65306;&#65288;i&#65289;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#24674;&#22797;&#35299;&#30340;&#36136;&#37327;&#30028;&#38480;&#65307;&#65288;ii&#65289;&#35813;&#26041;&#27861;&#19982;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65288;EBMs&#65289;&#23398;&#20064;&#36807;&#31243;&#26080;&#32541;&#36830;&#25509;&#65292;&#21487;&#20197;&#20351;&#29992;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#31639;&#27861;&#35299;&#20915;&#24863;&#20852;&#36259;&#30340;&#38382;&#39064;&#65307;&#65288;iii&#65289;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#30452;&#35266;&#30340;&#20248;&#21270;&#26041;&#26696;&#65292;&#36991;&#20813;&#20351;&#29992;&#26497;&#23567;-&#26497;&#22823;&#12289;&#24378;&#21270;&#31561;&#22797;&#26434;&#25216;&#24039;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;s
&lt;/p&gt;
&lt;p&gt;
Optimal transport (OT) barycenters are a mathematically grounded way of averaging probability distributions while capturing their geometric properties. In short, the barycenter task is to take the average of a collection of probability distributions w.r.t. given OT discrepancies. We propose a novel algorithm for approximating the continuous Entropic OT (EOT) barycenter for arbitrary OT cost functions. Our approach is built upon the dual reformulation of the EOT problem based on weak OT, which has recently gained the attention of the ML community. Beyond its novelty, our method enjoys several advantageous properties: (i) we establish quality bounds for the recovered solution; (ii) this approach seemlessly interconnects with the Energy-Based Models (EBMs) learning procedure enabling the use of well-tuned algorithms for the problem of interest; (iii) it provides an intuitive optimization scheme avoiding min-max, reinforce and other intricate technical tricks. For validation, we consider s
&lt;/p&gt;</description></item><item><title>Supersonic &#26159;&#19968;&#20010;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;C/C++&#20013;&#36827;&#34892;&#28304;&#20195;&#30721;&#20248;&#21270;&#12290;&#19982;GPT-3.5-Turbo&#21644;GPT-4&#30456;&#27604;&#65292;&#23427;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2309.14846</link><description>&lt;p&gt;
Supersonic: &#23398;&#20064;&#22312;C/C++&#20013;&#29983;&#25104;&#28304;&#20195;&#30721;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Supersonic: Learning to Generate Source Code Optimisations in C/C++. (arXiv:2309.14846v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14846
&lt;/p&gt;
&lt;p&gt;
Supersonic &#26159;&#19968;&#20010;&#31070;&#32463;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;C/C++&#20013;&#36827;&#34892;&#28304;&#20195;&#30721;&#20248;&#21270;&#12290;&#19982;GPT-3.5-Turbo&#21644;GPT-4&#30456;&#27604;&#65292;&#23427;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#20248;&#21270;&#22312;&#20445;&#25345;&#21151;&#33021;&#30340;&#21516;&#26102;&#25913;&#21892;&#36164;&#28304;&#25928;&#29575;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#26159;&#30001;&#24320;&#21457;&#20154;&#21592;&#21644;&#32534;&#35793;&#22120;&#23436;&#25104;&#30340;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19977;&#31181;&#36873;&#25321;&#65292;&#21363;&#22312;&#28304;&#20195;&#30721;&#32423;&#21035;&#36827;&#34892;&#33258;&#21160;&#20248;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Supersonic&#65292;&#19968;&#20010;&#38024;&#23545;&#20248;&#21270;&#30340;&#36731;&#24494;&#28304;&#20195;&#30721;&#20462;&#25913;&#30340;&#31070;&#32463;&#26041;&#27861;&#12290;&#20351;&#29992;seq2seq&#27169;&#22411;&#65292;Supersonic&#22312;C / C ++&#31243;&#24207;&#23545;&#65288;$x_{t}$&#65292;$x_{t+1}$&#65289;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;$x_{t+1}$&#26159;$x_{t}$&#30340;&#20248;&#21270;&#29256;&#26412;&#65292;&#24182;&#36755;&#20986;&#19968;&#20010;&#24046;&#24322;&#12290;Supersonic&#30340;&#24615;&#33021;&#22312;&#31454;&#25216;&#32534;&#31243;&#20219;&#21153;&#19978;&#19982;OpenAI&#30340;GPT-3.5-Turbo&#21644;GPT-4&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;Supersonic&#19981;&#20165;&#22312;&#20195;&#30721;&#20248;&#21270;&#20219;&#21153;&#19978;&#32988;&#36807;&#20102;&#36825;&#20004;&#20010;&#27169;&#22411;&#65292;&#32780;&#19988;&#25913;&#21464;&#30340;&#31243;&#24230;&#27604;GPT-3.5-Turbo&#23567;&#20102;600&#22810;&#20493;&#65292;&#27604;GPT-4&#23567;&#20102;3700&#22810;&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Software optimization refines programs for resource efficiency while preserving functionality. Traditionally, it is a process done by developers and compilers. This paper introduces a third option, automated optimization at the source code level. We present Supersonic, a neural approach targeting minor source code modifications for optimization. Using a seq2seq model, Supersonic is trained on C/C++ program pairs ($x_{t}$, $x_{t+1}$), where $x_{t+1}$ is an optimized version of $x_{t}$, and outputs a diff. Supersonic's performance is benchmarked against OpenAI's GPT-3.5-Turbo and GPT-4 on competitive programming tasks. The experiments show that Supersonic not only outperforms both models on the code optimization task, but also minimizes the extent of change with a more than 600x smaller than GPT-3.5-Turbo and 3700x smaller than GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26497;&#20302;&#21151;&#29575;&#19979;&#25805;&#20316;&#30340;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#19968;&#20123;&#23618;&#21482;&#20351;&#29992;&#19968;&#20010;&#20809;&#23376;&#26469;&#24341;&#21457;&#31070;&#32463;&#20803;&#28608;&#27963;&#12290;&#23613;&#31649;&#23384;&#22312;&#26497;&#39640;&#30340;&#22122;&#22768;&#65292;&#20173;&#21487;&#20197;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#20197;&#39640;&#31934;&#24230;&#25191;&#34892;&#30830;&#23450;&#24615;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.15712</link><description>&lt;p&gt;
&#23569;&#37327;&#23376;&#28608;&#27963;&#19979;&#37327;&#23376;&#22122;&#22768;&#21463;&#38480;&#30340;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantum-noise-limited optical neural networks operating at a few quanta per activation. (arXiv:2307.15712v1 [physics.optics])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26497;&#20302;&#21151;&#29575;&#19979;&#25805;&#20316;&#30340;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#20013;&#19968;&#20123;&#23618;&#21482;&#20351;&#29992;&#19968;&#20010;&#20809;&#23376;&#26469;&#24341;&#21457;&#31070;&#32463;&#20803;&#28608;&#27963;&#12290;&#23613;&#31649;&#23384;&#22312;&#26497;&#39640;&#30340;&#22122;&#22768;&#65292;&#20173;&#21487;&#20197;&#35757;&#32451;&#36825;&#20123;&#32593;&#32476;&#20197;&#39640;&#31934;&#24230;&#25191;&#34892;&#30830;&#23450;&#24615;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#29289;&#29702;&#31070;&#32463;&#32593;&#32476;&#34987;&#36890;&#24120;&#22312;&#30456;&#23545;&#39640;&#21151;&#29575;&#19979;&#25805;&#20316;&#65292;&#20197;&#20445;&#35777;&#20449;&#22122;&#27604;&#22823;&#20110;10&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26497;&#20302;&#21151;&#29575;&#19979;&#25805;&#20316;&#27169;&#25311;&#31995;&#32479;&#26102;&#20250;&#21457;&#29983;&#20160;&#20040;&#65292;&#21363;&#31995;&#32479;&#34892;&#20026;&#21464;&#24471;&#39640;&#24230;&#38543;&#26426;&#19988;&#22122;&#22768;&#19981;&#20877;&#26159;&#20449;&#21495;&#30340;&#23567;&#25200;&#21160;&#12290;&#25105;&#20204;&#22312;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20013;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#19968;&#20123;&#23618;&#21482;&#20351;&#29992;&#19968;&#20010;&#20809;&#23376;&#26469;&#24341;&#21457;&#31070;&#32463;&#20803;&#28608;&#27963;&#12290;&#22312;&#36825;&#31181;&#26497;&#20302;&#21151;&#29575;&#19979;&#65292;&#31070;&#32463;&#20803;&#28608;&#27963;&#21463;&#21040;&#37327;&#23376;&#22122;&#22768;&#30340;&#20027;&#23548;&#65292;&#36825;&#26159;&#30001;&#20110;&#21333;&#20809;&#23376;&#26816;&#27979;&#24369;&#20809;&#20449;&#21495;&#30340;&#22522;&#26412;&#27010;&#29575;&#24615;&#36136;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23613;&#31649;&#22122;&#22768;&#26497;&#22823;&#65288;&#20449;&#22122;&#27604;&#32422;&#20026;1&#65289;&#65292;&#20173;&#28982;&#21487;&#20197;&#35757;&#32451;&#38543;&#26426;&#20809;&#23398;&#31070;&#32463;&#32593;&#32476;&#20197;&#39640;&#31934;&#24230;&#25191;&#34892;&#30830;&#23450;&#24615;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analog physical neural networks, which hold promise for improved energy efficiency and speed compared to digital electronic neural networks, are nevertheless typically operated in a relatively high-power regime so that the signal-to-noise ratio (SNR) is large (&gt;10). What happens if an analog system is instead operated in an ultra-low-power regime, in which the behavior of the system becomes highly stochastic and the noise is no longer a small perturbation on the signal? In this paper, we study this question in the setting of optical neural networks operated in the limit where some layers use only a single photon to cause a neuron activation. Neuron activations in this limit are dominated by quantum noise from the fundamentally probabilistic nature of single-photon detection of weak optical signals. We show that it is possible to train stochastic optical neural networks to perform deterministic image-classification tasks with high accuracy in spite of the extremely high noise (SNR ~ 1) 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2306.12968</link><description>&lt;p&gt;
&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#20013;&#30340;&#26368;&#20248;&#31751;&#24674;&#22797;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Instance-Optimal Cluster Recovery in the Labeled Stochastic Block Model. (arXiv:2306.12968v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21517;&#20026;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#23427;&#33021;&#22815;&#22312;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#20013;&#24674;&#22797;&#38544;&#34255;&#30340;&#32676;&#38598;&#12290;IAC&#21253;&#25324;&#19968;&#27425;&#35889;&#32858;&#31867;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#22312;&#26377;&#38480;&#25968;&#37327;&#30340;&#31751;&#30340;&#24773;&#20917;&#19979;&#65292;&#29992;&#26631;&#35760;&#38543;&#26426;&#22359;&#27169;&#22411;&#65288;LSBM&#65289;&#24674;&#22797;&#38544;&#34255;&#30340;&#31038;&#32676;&#65292;&#20854;&#20013;&#31751;&#22823;&#23567;&#38543;&#30528;&#29289;&#21697;&#24635;&#25968;$n$&#30340;&#22686;&#38271;&#32780;&#32447;&#24615;&#22686;&#38271;&#12290;&#22312;LSBM&#20013;&#65292;&#20026;&#27599;&#23545;&#29289;&#21697;&#65288;&#29420;&#31435;&#22320;&#65289;&#35266;&#27979;&#21040;&#19968;&#20010;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35774;&#35745;&#19968;&#31181;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#21033;&#29992;&#35266;&#27979;&#21040;&#30340;&#26631;&#31614;&#26469;&#24674;&#22797;&#31751;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20851;&#20110;&#26399;&#26395;&#34987;&#20219;&#20309;&#32858;&#31867;&#31639;&#27861;&#35823;&#20998;&#31867;&#30340;&#29289;&#21697;&#25968;&#37327;&#30340;&#23454;&#20363;&#29305;&#23450;&#19979;&#30028;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#20363;&#33258;&#36866;&#24212;&#32858;&#31867;&#65288;IAC&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#26399;&#26395;&#21644;&#39640;&#27010;&#29575;&#19979;&#37117;&#33021;&#21305;&#37197;&#36825;&#20123;&#19979;&#30028;&#34920;&#29616;&#30340;&#31639;&#27861;&#12290;IAC&#30001;&#19968;&#27425;&#35889;&#32858;&#31867;&#31639;&#27861;&#21644;&#19968;&#20010;&#36845;&#20195;&#30340;&#22522;&#20110;&#20284;&#28982;&#30340;&#31751;&#20998;&#37197;&#25913;&#36827;&#32452;&#25104;&#12290;&#36825;&#31181;&#26041;&#27861;&#22522;&#20110;&#23454;&#20363;&#29305;&#23450;&#30340;&#19979;&#30028;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#27169;&#22411;&#21442;&#25968;&#65292;&#21253;&#25324;&#31751;&#30340;&#25968;&#37327;&#12290;&#36890;&#36807;&#20165;&#25191;&#34892;&#19968;&#27425;&#35889;&#32858;&#31867;&#65292;IAC&#22312;&#35745;&#31639;&#21644;&#23384;&#20648;&#26041;&#38754;&#37117;&#26159;&#39640;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of recovering hidden communities in the Labeled Stochastic Block Model (LSBM) with a finite number of clusters, where cluster sizes grow linearly with the total number $n$ of items. In the LSBM, a label is (independently) observed for each pair of items. Our objective is to devise an efficient algorithm that recovers clusters using the observed labels. To this end, we revisit instance-specific lower bounds on the expected number of misclassified items satisfied by any clustering algorithm. We present Instance-Adaptive Clustering (IAC), the first algorithm whose performance matches these lower bounds both in expectation and with high probability. IAC consists of a one-time spectral clustering algorithm followed by an iterative likelihood-based cluster assignment improvement. This approach is based on the instance-specific lower bound and does not require any model parameters, including the number of clusters. By performing the spectral clustering only once, IAC m
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.04802</link><description>&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;&#32508;&#36848;&#65306;&#36164;&#28304;&#12289;&#24212;&#29992;&#21644;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
A Survey on Knowledge Graphs for Healthcare: Resources, Applications, and Promises. (arXiv:2306.04802v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#30340;&#26500;&#24314;&#27969;&#31243;&#12289;&#20851;&#38190;&#25216;&#26415;&#21644;&#21033;&#29992;&#26041;&#27861;&#20197;&#21450;&#29616;&#26377;&#36164;&#28304;&#65292;&#24182;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#30103;&#30693;&#35782;&#22270;&#35889;(HKGs)&#24050;&#25104;&#20026;&#32452;&#32455;&#21307;&#23398;&#30693;&#35782;&#30340;&#26377;&#32467;&#26500;&#19988;&#21487;&#35299;&#37322;&#30340;&#26377;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#20102;&#21307;&#23398;&#27010;&#24565;&#21450;&#20854;&#20851;&#31995;&#30340;&#20840;&#38754;&#35270;&#22270;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#31561;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#24378;&#35843;&#20102;&#22312;HKG&#39046;&#22495;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#24517;&#35201;&#24615;&#12290;&#26412;&#32508;&#36848;&#26159;HKG&#30340;&#31532;&#19968;&#20221;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;HKG&#26500;&#24314;&#30340;&#27969;&#31243;&#21644;&#20851;&#38190;&#25216;&#26415;&#65288;&#21363;&#20174;&#22836;&#24320;&#22987;&#21644;&#36890;&#36807;&#38598;&#25104;&#65289;&#65292;&#20197;&#21450;&#24120;&#35265;&#30340;&#21033;&#29992;&#26041;&#27861;&#65288;&#21363;&#22522;&#20110;&#27169;&#22411;&#21644;&#38750;&#22522;&#20110;&#27169;&#22411;&#65289;&#12290;&#20026;&#20102;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#25105;&#20204;&#26681;&#25454;&#23427;&#20204;&#25429;&#33719;&#30340;&#25968;&#25454;&#31867;&#22411;&#21644;&#24212;&#29992;&#39046;&#22495;&#65288;&#35813;&#36164;&#28304;&#23384;&#20648;&#20110;https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase&#65289;&#32452;&#32455;&#20102;&#29616;&#26377;&#30340;HKG&#65292;&#24182;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#22312;&#24212;&#29992;&#37096;&#20998;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;HKG&#22312;&#21508;&#31181;&#21307;&#30103;&#39046;&#22495;&#30340;&#21464;&#38761;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Healthcare knowledge graphs (HKGs) have emerged as a promising tool for organizing medical knowledge in a structured and interpretable way, which provides a comprehensive view of medical concepts and their relationships. However, challenges such as data heterogeneity and limited coverage remain, emphasizing the need for further research in the field of HKGs. This survey paper serves as the first comprehensive overview of HKGs. We summarize the pipeline and key techniques for HKG construction (i.e., from scratch and through integration), as well as the common utilization approaches (i.e., model-free and model-based). To provide researchers with valuable resources, we organize existing HKGs (The resource is available at https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the data types they capture and application domains, supplemented with pertinent statistical information. In the application section, we delve into the transformative impact of HKGs across various hea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;</title><link>http://arxiv.org/abs/2306.03303</link><description>&lt;p&gt;
&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#21151;&#33021;&#24615;&#36755;&#20837;&#26144;&#23556;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03303
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#23436;&#25104;&#20840;&#23616;&#20989;&#25968;&#36924;&#36817;&#12290;&#36825;&#19968;&#26041;&#27861;&#36866;&#29992;&#20110;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#65292;&#36824;&#21487;&#29992;&#20110;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#30340;&#36924;&#36817;&#65292;&#21516;&#26102;&#20063;&#21487;&#20197;&#36924;&#36817;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#23450;&#20041;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#65292;&#20854;&#20540;&#20063;&#22312;&#21487;&#33021;&#26159;&#26080;&#38480;&#32500;&#30340;&#36755;&#20986;&#31354;&#38388;&#20013;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21152;&#24615;&#26063;&#20316;&#20026;&#38544;&#34255;&#23618;&#26144;&#23556;&#65292;&#20197;&#21450;&#19968;&#20010;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#24212;&#29992;&#20110;&#27599;&#20010;&#38544;&#34255;&#23618;&#12290;&#20381;&#38752;&#24102;&#26435;&#37325;&#31354;&#38388;&#19978;&#30340;Stone-Weierstrass&#23450;&#29702;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#36830;&#32493;&#20989;&#25968;&#30340;&#25512;&#24191;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#65292;&#36229;&#36234;&#20102;&#24120;&#35268;&#32039;&#38598;&#36924;&#36817;&#12290;&#36825;&#29305;&#21035;&#36866;&#29992;&#20110;&#36890;&#36807;&#21151;&#33021;&#24615;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#36924;&#36817;&#65288;&#38750;&#20808;&#35265;&#20043;&#26126;&#30340;&#65289;&#36335;&#24452;&#31354;&#38388;&#20989;&#25968;&#12290;&#20316;&#20026;&#24102;&#26435;Stone-Weierstrass&#23450;&#29702;&#30340;&#36827;&#19968;&#27493;&#24212;&#29992;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#32447;&#24615;&#20989;&#25968;&#31614;&#21517;&#30340;&#20840;&#23616;&#26222;&#36866;&#36924;&#36817;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#24341;&#20837;&#20102;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#35266;&#28857;&#65292;&#24182;&#23637;&#31034;&#20102;&#31614;&#21517;&#20869;&#26680;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#26159;&#26576;&#20123;&#39640;&#26031;&#36807;&#31243;&#30340;Cameron-Martin&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#20869;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#26368;&#23567;&#21518;&#38376;&#27010;&#24565;&#21644;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#26408;&#39532;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#23545;&#30446;&#21069;&#30340;&#38450;&#24481;&#25514;&#26045;&#26500;&#25104;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.08411</link><description>&lt;p&gt;
&#26469;&#33258;&#20869;&#37096;&#30340;&#37034;&#24694;: &#36890;&#36807;&#30828;&#20214;&#26408;&#39532;&#36827;&#34892;&#26426;&#22120;&#23398;&#20064;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Evil from Within: Machine Learning Backdoors through Hardware Trojans. (arXiv:2304.08411v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#24120;&#35265;&#26426;&#22120;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#20869;&#30340;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#23558;&#26368;&#23567;&#21518;&#38376;&#27010;&#24565;&#21644;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#26408;&#39532;&#32467;&#21512;&#20351;&#29992;&#65292;&#20174;&#32780;&#23545;&#30446;&#21069;&#30340;&#38450;&#24481;&#25514;&#26045;&#26500;&#25104;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#20250;&#23545;&#26426;&#22120;&#23398;&#20064;&#36896;&#25104;&#20005;&#37325;&#23041;&#32961;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#30772;&#22351;&#23433;&#20840;&#20851;&#38190;&#30340;&#31995;&#32479;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21518;&#38376;&#25915;&#20987;&#26041;&#27861;&#65292;&#23436;&#20840;&#23621;&#20110;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24120;&#35265;&#30828;&#20214;&#21152;&#36895;&#22120;&#20869;&#65292;&#20174;&#32780;&#23545;&#24403;&#21069;&#38450;&#24481;&#25514;&#26045;&#26500;&#25104;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#36825;&#31181;&#25915;&#20987;&#23454;&#29992;&#65292;&#25105;&#20204;&#20811;&#26381;&#20102;&#20004;&#20010;&#25361;&#25112;&#65306;&#39318;&#20808;&#65292;&#30001;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#30340;&#23384;&#20648;&#31354;&#38388;&#20005;&#37325;&#21463;&#38480;&#65292;&#22240;&#27492;&#25105;&#20204;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#26368;&#23567;&#21518;&#38376;&#27010;&#24565;&#65292;&#21482;&#25913;&#21464;&#23569;&#37327;&#27169;&#22411;&#21442;&#25968;&#21363;&#21487;&#28608;&#27963;&#21518;&#38376;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#37197;&#32622;&#30340;&#30828;&#20214;&#26408;&#39532;&#65292;&#21487;&#20197;&#19982;&#21518;&#38376;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoors pose a serious threat to machine learning, as they can compromise the integrity of security-critical systems, such as self-driving cars. While different defenses have been proposed to address this threat, they all rely on the assumption that the hardware on which the learning models are executed during inference is trusted. In this paper, we challenge this assumption and introduce a backdoor attack that completely resides within a common hardware accelerator for machine learning. Outside of the accelerator, neither the learning model nor the software is manipulated, so that current defenses fail. To make this attack practical, we overcome two challenges: First, as memory on a hardware accelerator is severely limited, we introduce the concept of a minimal backdoor that deviates as little as possible from the original model and is activated by replacing a few model parameters only. Second, we develop a configurable hardware trojan that can be provisioned with the backdoor and p
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26041;&#27861;&#26469;&#35745;&#31639;&#37096;&#20998;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#65292;&#24182;&#22312;&#21512;&#25104;&#20363;&#23376;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2303.07988</link><description>&lt;p&gt;
&#37096;&#20998;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Partial Neural Optimal Transport. (arXiv:2303.07988v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07988
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#26041;&#27861;&#26469;&#35745;&#31639;&#37096;&#20998;&#26368;&#20248;&#36755;&#36816;&#26144;&#23556;&#65292;&#24182;&#22312;&#21512;&#25104;&#20363;&#23376;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#26041;&#27861;&#26469;&#35745;&#31639;&#37096;&#20998;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#26144;&#23556;&#65292;&#21363;&#25351;&#23450;&#36136;&#37327;&#30340;&#24230;&#37327;&#37096;&#20998;&#20043;&#38388;&#30340;OT&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#20363;&#23376;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#37096;&#20998;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel neural method to compute partial optimal transport (OT) maps, i.e., OT maps between parts of measures of the specified masses. We test our partial neural optimal transport algorithm on synthetic examples.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#31283;&#20581;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#65292;&#33021;&#22815;&#21516;&#26102;&#20445;&#25252;&#19977;&#20010;&#36807;&#25311;&#21512;&#30340;&#28304;&#22836;&#65306;&#26377;&#38480;&#26679;&#26412;&#25968;&#25454;&#30340;&#32479;&#35745;&#35823;&#24046;&#12289;&#25968;&#25454;&#28857;&#30340;&#26377;&#38480;&#31934;&#24230;&#27979;&#37327;&#24341;&#36215;&#30340;&#25968;&#25454;&#22122;&#22768;&#65292;&#20197;&#21450;&#34987;&#30772;&#22351;&#30340;&#37096;&#20998;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.09560</link><description>&lt;p&gt;
&#20840;&#38754;&#31283;&#20581;&#30340;&#25968;&#25454;&#39537;&#21160;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Holistic Robust Data-Driven Decisions. (arXiv:2207.09560v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09560
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#31283;&#20581;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#65292;&#33021;&#22815;&#21516;&#26102;&#20445;&#25252;&#19977;&#20010;&#36807;&#25311;&#21512;&#30340;&#28304;&#22836;&#65306;&#26377;&#38480;&#26679;&#26412;&#25968;&#25454;&#30340;&#32479;&#35745;&#35823;&#24046;&#12289;&#25968;&#25454;&#28857;&#30340;&#26377;&#38480;&#31934;&#24230;&#27979;&#37327;&#24341;&#36215;&#30340;&#25968;&#25454;&#22122;&#22768;&#65292;&#20197;&#21450;&#34987;&#30772;&#22351;&#30340;&#37096;&#20998;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20855;&#26377;&#33391;&#22909;&#26679;&#26412;&#22806;&#24615;&#33021;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#20915;&#31574;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#12290;&#22909;&#30340;&#26679;&#26412;&#20869;&#24615;&#33021;&#19981;&#19968;&#23450;&#33021;&#20445;&#35777;&#22909;&#30340;&#26679;&#26412;&#22806;&#24615;&#33021;&#65292;&#36825;&#34987;&#26222;&#36941;&#35748;&#20026;&#26159;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#23454;&#38469;&#30340;&#36807;&#25311;&#21512;&#36890;&#24120;&#19981;&#33021;&#24402;&#22240;&#20110;&#21333;&#19968;&#21407;&#22240;&#65292;&#32780;&#26159;&#30001;&#22810;&#20010;&#22240;&#32032;&#21516;&#26102;&#24341;&#36215;&#30340;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#32771;&#34385;&#20102;&#19977;&#20010;&#36807;&#25311;&#21512;&#30340;&#28304;&#22836;&#65306;&#65288;&#19968;&#65289;&#32479;&#35745;&#35823;&#24046;&#65292;&#30001;&#20110;&#20351;&#29992;&#26377;&#38480;&#30340;&#26679;&#26412;&#25968;&#25454;&#32780;&#20135;&#29983;&#30340;&#35823;&#24046;&#65292;&#65288;&#20108;&#65289;&#25968;&#25454;&#22122;&#22768;&#65292;&#24403;&#25968;&#25454;&#28857;&#21482;&#29992;&#26377;&#38480;&#31934;&#24230;&#27979;&#37327;&#26102;&#20135;&#29983;&#30340;&#22122;&#22768;&#65292;&#65288;&#19977;&#65289;&#25968;&#25454;&#38169;&#35823;&#65292;&#21363;&#20840;&#37096;&#25968;&#25454;&#20013;&#26377;&#19968;&#23567;&#37096;&#20998;&#25968;&#25454;&#34987;&#23436;&#20840;&#30772;&#22351;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#23613;&#31649;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#22312;&#21333;&#29420;&#22788;&#29702;&#36825;&#19977;&#20010;&#28304;&#22836;&#26102;&#21487;&#33021;&#26159;&#31283;&#20581;&#30340;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#21516;&#26102;&#25552;&#20379;&#23545;&#25152;&#26377;&#36807;&#25311;&#21512;&#28304;&#22836;&#30340;&#20840;&#38754;&#20445;&#25252;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39537;&#21160;&#20844;&#24335;&#65292;&#21487;&#20197;&#20445;&#35777;&#36825;&#31181;&#20840;&#38754;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of data-driven formulations for machine learning and decision-making with good out-of-sample performance is a key challenge. The observation that good in-sample performance does not guarantee good out-of-sample performance is generally known as overfitting. Practical overfitting can typically not be attributed to a single cause but instead is caused by several factors all at once. We consider here three overfitting sources: (i) statistical error as a result of working with finite sample data, (ii) data noise which occurs when the data points are measured only with finite precision, and finally (iii) data misspecification in which a small fraction of all data may be wholly corrupted. We argue that although existing data-driven formulations may be robust against one of these three sources in isolation they do not provide holistic protection against all overfitting sources simultaneously. We design a novel data-driven formulation which does guarantee such holistic protection an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24182;&#19988;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#38543;&#26426;&#23545;&#35937;&#65292;&#25552;&#20379;&#20102;&#35813;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24180;&#40836;&#20998;&#24067;&#21644;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2207.05442</link><description>&lt;p&gt;
Wasserstein&#22810;&#20803;&#33258;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#24314;&#27169;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#21450;&#20854;&#22312;&#22270;&#24418;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Wasserstein multivariate auto-regressive models for modeling distributional time series and its application in graph learning. (arXiv:2207.05442v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05442
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24182;&#19988;&#22312;Wasserstein&#31354;&#38388;&#20013;&#24314;&#27169;&#20102;&#38543;&#26426;&#23545;&#35937;&#65292;&#25552;&#20379;&#20102;&#35813;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#21644;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#27492;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#24180;&#40836;&#20998;&#24067;&#21644;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20110;&#32479;&#35745;&#20998;&#26512;&#22810;&#20803;&#20998;&#24067;&#26102;&#38388;&#24207;&#21015;&#12290;&#24863;&#20852;&#36259;&#30340;&#25968;&#25454;&#21253;&#25324;&#19968;&#32452;&#22312;&#23454;&#32447;&#26377;&#30028;&#38388;&#38548;&#19978;&#25903;&#25345;&#30340;&#27010;&#29575;&#27979;&#24230;&#30340;&#22810;&#20010;&#31995;&#21015;&#65292;&#24182;&#19988;&#34987;&#19981;&#21516;&#26102;&#38388;&#30636;&#38388;&#25152;&#32034;&#24341;&#12290;&#27010;&#29575;&#27979;&#24230;&#34987;&#24314;&#27169;&#20026;Wasserstein&#31354;&#38388;&#20013;&#30340;&#38543;&#26426;&#23545;&#35937;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;Lebesgue&#27979;&#24230;&#30340;&#20999;&#31354;&#38388;&#20013;&#24314;&#31435;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#39318;&#20808;&#23545;&#25152;&#26377;&#21407;&#22987;&#27979;&#24230;&#36827;&#34892;&#23621;&#20013;&#22788;&#29702;&#65292;&#20197;&#20415;&#23427;&#20204;&#30340;Fr&#233;chet&#24179;&#22343;&#20540;&#25104;&#20026;Lebesgue&#27979;&#24230;&#12290;&#21033;&#29992;&#36845;&#20195;&#38543;&#26426;&#20989;&#25968;&#31995;&#32479;&#30340;&#29702;&#35770;&#65292;&#25552;&#20379;&#20102;&#36825;&#26679;&#19968;&#20010;&#27169;&#22411;&#30340;&#35299;&#30340;&#23384;&#22312;&#24615;&#12289;&#21807;&#19968;&#24615;&#21644;&#24179;&#31283;&#24615;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#27169;&#22411;&#31995;&#25968;&#30340;&#19968;&#33268;&#20272;&#35745;&#22120;&#12290;&#38500;&#20102;&#23545;&#27169;&#25311;&#25968;&#25454;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27169;&#22411;&#28436;&#31034;&#65306;&#19968;&#20010;&#26159;&#19981;&#21516;&#22269;&#23478;&#24180;&#40836;&#20998;&#24067;&#30340;&#35266;&#23519;&#25968;&#25454;&#38598;&#65292;&#21478;&#19968;&#20010;&#26159;&#24052;&#40654;&#33258;&#34892;&#36710;&#20849;&#20139;&#32593;&#32476;&#30340;&#35266;&#23519;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new auto-regressive model for the statistical analysis of multivariate distributional time series. The data of interest consist of a collection of multiple series of probability measures supported over a bounded interval of the real line, and that are indexed by distinct time instants. The probability measures are modelled as random objects in the Wasserstein space. We establish the auto-regressive model in the tangent space at the Lebesgue measure by first centering all the raw measures so that their Fr\'echet means turn to be the Lebesgue measure. Using the theory of iterated random function systems, results on the existence, uniqueness and stationarity of the solution of such a model are provided. We also propose a consistent estimator for the model coefficient. In addition to the analysis of simulated data, the proposed model is illustrated with two real data sets made of observations from age distribution in different countries and bike sharing network in Paris. Final
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#27491;&#36127;&#37197;&#23545;&#19981;&#30456;&#20284;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#27492;&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2002.07756</link><description>&lt;p&gt;
&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#21644;&#32500;&#25345;&#26641;&#32467;&#26500;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Correlation Clustering and Tree Preserving Embedding. (arXiv:2002.07756v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.07756
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#24212;&#29992;&#20110;&#27491;&#36127;&#37197;&#23545;&#19981;&#30456;&#20284;&#24230;&#65292;&#24182;&#30740;&#31350;&#20102;&#20351;&#29992;&#27492;&#26041;&#27861;&#36827;&#34892;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#33879;&#21517;&#30340;&#30456;&#20851;&#32858;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#20135;&#29983;&#36866;&#29992;&#20110;&#27491;&#36127;&#37197;&#23545;&#19981;&#30456;&#20284;&#24230;&#30340;&#20998;&#23618;&#32858;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#36825;&#31181;&#20998;&#23618;&#30456;&#20851;&#32858;&#31867;&#30340;&#26080;&#30417;&#30563;&#34920;&#24449;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#23558;&#30456;&#24212;&#30340;&#20998;&#23618;&#23884;&#20837;&#29992;&#20110;&#32500;&#25345;&#26641;&#32467;&#26500;&#23884;&#20837;&#21644;&#29305;&#24449;&#25552;&#21462;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#26368;&#23567;&#26368;&#22823;&#36317;&#31163;&#24230;&#37327;&#25193;&#23637;&#21040;&#30456;&#20851;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#20316;&#20026;&#21478;&#19968;&#31181;&#34920;&#24449;&#23398;&#20064;&#33539;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a hierarchical correlation clustering method that extends the well-known correlation clustering to produce hierarchical clusters applicable to both positive and negative pairwise dissimilarities. Then, in the following, we study unsupervised representation learning with such hierarchical correlation clustering. For this purpose, we first investigate embedding the respective hierarchy to be used for tree-preserving embedding and feature extraction. Thereafter, we study the extension of minimax distance measures to correlation clustering, as another representation learning paradigm. Finally, we demonstrate the performance of our methods on several datasets.
&lt;/p&gt;</description></item></channel></rss>