<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMP-Attack&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25915;&#20987;&#21830;&#19994;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01369</link><description>&lt;p&gt;
&#20351;&#29992;&#22810;&#27169;&#24335;&#20808;&#39564;&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cheating Suffix: Targeted Attack to Text-To-Image Diffusion Models with Multi-Modal Priors
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMP-Attack&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25915;&#20987;&#21830;&#19994;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#65292;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#23637;&#29616;&#20102;&#22270;&#20687;&#21644;&#25991;&#26412;&#27169;&#24577;&#20043;&#38388;&#30340;&#21331;&#36234;&#32852;&#31995;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#34987;&#24694;&#24847;&#21033;&#29992;&#30340;&#25361;&#25112;&#65292;&#36890;&#36807;&#22312;&#21407;&#22987;&#25552;&#31034;&#21518;&#38468;&#21152;&#29305;&#23450;&#21518;&#32512;&#26469;&#29983;&#25104;&#26377;&#23475;&#25110;&#25935;&#24863;&#22270;&#20687;&#12290;&#29616;&#26377;&#20316;&#21697;&#20027;&#35201;&#20851;&#27880;&#20351;&#29992;&#21333;&#27169;&#24577;&#20449;&#24687;&#36827;&#34892;&#25915;&#20987;&#65292;&#26410;&#33021;&#21033;&#29992;&#22810;&#27169;&#24577;&#29305;&#24449;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#23613;&#22914;&#20154;&#24847;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MMP-Attack&#30340;&#26377;&#38024;&#23545;&#24615;&#25915;&#20987;&#26041;&#27861;&#65292;&#23427;&#23558;&#22810;&#27169;&#24577;&#20808;&#39564;&#65288;MMP&#65289;&#21363;&#25991;&#26412;&#21644;&#22270;&#20687;&#29305;&#24449;&#36827;&#34892;&#25972;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MMP-Attack&#30340;&#30446;&#26631;&#26159;&#22312;&#22270;&#20687;&#20869;&#23481;&#20013;&#28155;&#21152;&#30446;&#26631;&#23545;&#35937;&#30340;&#21516;&#26102;&#65292;&#21516;&#26102;&#31227;&#38500;&#21407;&#22987;&#23545;&#35937;&#12290;&#19982;&#29616;&#26377;&#20316;&#21697;&#30456;&#27604;&#65292;MMP-Attack&#20855;&#26377;&#26356;&#39640;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#36716;&#31227;&#24615;&#65292;&#22312;&#25915;&#20987;&#21830;&#19994;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#65288;&#22914;DALL-E 3&#65289;&#26041;&#38754;&#34920;&#29616;&#20986;&#26126;&#26174;&#20248;&#21183;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26631;&#24535;&#30528;&#24403;&#21069;&#26368;&#20339;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have been widely deployed in various image generation tasks, demonstrating an extraordinary connection between image and text modalities. However, they face challenges of being maliciously exploited to generate harmful or sensitive images by appending a specific suffix to the original prompt. Existing works mainly focus on using single-modal information to conduct attacks, which fails to utilize multi-modal features and results in less than satisfactory performance. Integrating multi-modal priors (MMP), i.e. both text and image features, we propose a targeted attack method named MMP-Attack in this work. Specifically, the goal of MMP-Attack is to add a target object into the image content while simultaneously removing the original object. The MMP-Attack shows a notable advantage over existing works with superior universality and transferability, which can effectively attack commercial text-to-image (T2I) models such as DALL-E 3. To the best of our knowledge, this marks 
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#22312;&#33021;&#22815;&#36827;&#34892;&#26799;&#24230;&#24179;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#20013;&#30340;&#25277;&#26679;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24182;&#34892;&#21270;&#30340;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#25216;&#26415;&#23548;&#20986;&#20102;&#23545;&#25277;&#26679;&#21644;&#30446;&#26631;&#23494;&#24230;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#37327;&#21270;&#20102;&#24182;&#34892;&#22788;&#29702;&#21333;&#20803;&#24102;&#26469;&#30340;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.14434</link><description>&lt;p&gt;
&#24182;&#34892;&#20013;&#28857;&#38543;&#26426;&#21270;&#30340; Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
Parallelized Midpoint Randomization for Langevin Monte Carlo
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14434
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#22312;&#33021;&#22815;&#36827;&#34892;&#26799;&#24230;&#24179;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#20013;&#30340;&#25277;&#26679;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24182;&#34892;&#21270;&#30340;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26032;&#25216;&#26415;&#23548;&#20986;&#20102;&#23545;&#25277;&#26679;&#21644;&#30446;&#26631;&#23494;&#24230;&#20043;&#38388;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#65292;&#37327;&#21270;&#20102;&#24182;&#34892;&#22788;&#29702;&#21333;&#20803;&#24102;&#26469;&#30340;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#21487;&#20197;&#36827;&#34892;&#26799;&#24230;&#30340;&#24179;&#34892;&#35780;&#20272;&#30340;&#26694;&#26550;&#20013;&#30340;&#25277;&#26679;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#25918;&#22312;&#30001;&#24179;&#28369;&#21644;&#24378;log-&#20985;&#23494;&#24230;&#34920;&#24449;&#30340;&#30446;&#26631;&#20998;&#24067;&#19978;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#24182;&#34892;&#21270;&#30340;&#38543;&#26426;&#20013;&#28857;&#26041;&#27861;&#65292;&#24182;&#36816;&#29992;&#26368;&#36817;&#24320;&#21457;&#29992;&#20110;&#20998;&#26512;&#20854;&#32431;&#39034;&#24207;&#29256;&#26412;&#30340;&#35777;&#26126;&#25216;&#26415;&#12290;&#21033;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#25277;&#26679;&#21644;&#30446;&#26631;&#23494;&#24230;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#19978;&#30028;&#12290;&#36825;&#20123;&#30028;&#38480;&#37327;&#21270;&#20102;&#36890;&#36807;&#21033;&#29992;&#24182;&#34892;&#22788;&#29702;&#21333;&#20803;&#25152;&#23454;&#29616;&#30340;&#36816;&#34892;&#26102;&#25913;&#36827;&#65292;&#36825;&#21487;&#33021;&#26159;&#30456;&#24403;&#21487;&#35266;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14434v1 Announce Type: cross  Abstract: We explore the sampling problem within the framework where parallel evaluations of the gradient of the log-density are feasible. Our investigation focuses on target distributions characterized by smooth and strongly log-concave densities. We revisit the parallelized randomized midpoint method and employ proof techniques recently developed for analyzing its purely sequential version. Leveraging these techniques, we derive upper bounds on the Wasserstein distance between the sampling and target densities. These bounds quantify the runtime improvement achieved by utilizing parallel processing units, which can be considerable.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24773;&#20917;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#22810;&#26102;&#24207;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#22810;&#31354;&#38388;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;&#36825;&#31687;&#35770;&#25991;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.06289</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Evaluating Membership Inference Attacks and Defenses in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06289
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24773;&#20917;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#22810;&#26102;&#24207;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#22810;&#31354;&#38388;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;&#36825;&#31687;&#35770;&#25991;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIAs)&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#23041;&#32961;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26085;&#30410;&#22686;&#38271;&#12290;&#21322;&#35802;&#23454;&#30340;&#25915;&#20987;&#32773;&#65292;&#20363;&#22914;&#26381;&#21153;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#20449;&#24687;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30446;&#26631;&#23458;&#25143;&#31471;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;MIAs&#21644;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;MIAs&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#21512;&#22810;&#20010;&#36890;&#20449;&#36718;&#27425;&#30340;&#27169;&#22411;&#20449;&#24687;(&#22810;&#26102;&#24207;)&#30456;&#27604;&#20110;&#21033;&#29992;&#21333;&#20010;&#26102;&#26399;&#30340;&#27169;&#22411;&#20449;&#24687;&#25552;&#39640;&#20102;MIAs&#30340;&#25972;&#20307;&#26377;&#25928;&#24615;&#12290;&#20854;&#27425;&#65292;&#22312;&#38750;&#30446;&#26631;&#23458;&#25143;&#31471;(Multi-spatial)&#20013;&#34701;&#20837;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;MIAs&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#24403;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26159;&#21516;&#36136;&#30340;&#26102;&#20505;&#12290;&#36825;&#20984;&#26174;&#20102;&#22312;MIAs&#20013;&#32771;&#34385;&#26102;&#24207;&#21644;&#31354;&#38388;&#27169;&#22411;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#35780;&#20272;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#26426;&#21046;&#23545;MIAs&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks (MIAs) pose a growing threat to privacy preservation in federated learning. The semi-honest attacker, e.g., the server, may determine whether a particular sample belongs to a target client according to the observed model information. This paper conducts an evaluation of existing MIAs and corresponding defense strategies. Our evaluation on MIAs reveals two important findings about the trend of MIAs. Firstly, combining model information from multiple communication rounds (Multi-temporal) enhances the overall effectiveness of MIAs compared to utilizing model information from a single epoch. Secondly, incorporating models from non-target clients (Multi-spatial) significantly improves the effectiveness of MIAs, particularly when the clients' data is homogeneous. This highlights the importance of considering the temporal and spatial model information in MIAs. Next, we assess the effectiveness via privacy-utility tradeoff for two type defense mechanisms against MI
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#20998;&#23376;&#24314;&#27169;&#21644;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#22522;&#20934;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#19982;&#25968;&#25454;&#27169;&#24577;&#21305;&#37197;&#30340;&#37327;&#21270;&#20449;&#24687;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#23398;&#20064;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.04119</link><description>&lt;p&gt;
&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#65306;&#20998;&#23376;&#31185;&#23398;&#20013;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23450;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26469;&#35299;&#20915;&#20998;&#23376;&#24314;&#27169;&#21644;&#35774;&#35745;&#20013;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22810;&#27169;&#24577;&#22522;&#20934;&#21644;&#23454;&#39564;&#35780;&#20272;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#27169;&#22411;&#19982;&#25968;&#25454;&#27169;&#24577;&#21305;&#37197;&#30340;&#37327;&#21270;&#20449;&#24687;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#27169;&#22411;&#30340;&#30693;&#35782;&#23398;&#20064;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#20998;&#23376;&#24314;&#27169;&#21644;&#35774;&#35745;&#23545;&#20110;&#21457;&#29616;&#21644;&#25506;&#32034;&#26032;&#22411;&#20998;&#23376;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24341;&#20837;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#20135;&#29983;&#20102;&#38761;&#21629;&#24615;&#30340;&#24433;&#21709;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20197;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#35270;&#35282;&#20026;&#31185;&#23398;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#31185;&#23398;&#35821;&#35328;&#24314;&#27169;&#65288;SLM&#65289;&#30340;&#30740;&#31350;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#22914;&#20309;&#37327;&#21270;&#27169;&#22411;&#19982;&#25968;&#25454;&#27169;&#24577;&#20043;&#38388;&#30340;&#21305;&#37197;&#20197;&#21450;&#22914;&#20309;&#35782;&#21035;&#27169;&#22411;&#30340;&#30693;&#35782;&#23398;&#20064;&#20559;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ChEBI-20-MM&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#65292;&#24182;&#36827;&#34892;&#20102;1263&#20010;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#27169;&#22411;&#19982;&#25968;&#25454;&#27169;&#24577;&#30340;&#20860;&#23481;&#24615;&#21644;&#30693;&#35782;&#33719;&#21462;&#33021;&#21147;&#12290;&#36890;&#36807;&#27169;&#24577;&#36716;&#31227;&#27010;&#29575;&#30697;&#38453;&#65292;&#25105;&#20204;&#20026;&#20219;&#21153;&#25552;&#20379;&#20102;&#26368;&#21512;&#36866;&#30340;&#27169;&#24577;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#32479;&#35745;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26412;&#22320;&#21270;&#26469;&#21457;&#29616;&#29305;&#23450;&#19978;&#19979;&#25991;&#30340;&#30693;&#35782;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient molecular modeling and design are crucial for the discovery and exploration of novel molecules, and the incorporation of deep learning methods has revolutionized this field. In particular, large language models (LLMs) offer a fresh approach to tackle scientific problems from a natural language processing (NLP) perspective, introducing a research paradigm called scientific language modeling (SLM). However, two key issues remain: how to quantify the match between model and data modalities and how to identify the knowledge-learning preferences of models. To address these challenges, we propose a multi-modal benchmark, named ChEBI-20-MM, and perform 1263 experiments to assess the model's compatibility with data modalities and knowledge acquisition. Through the modal transition probability matrix, we provide insights into the most suitable modalities for tasks. Furthermore, we introduce a statistically interpretable approach to discover context-specific knowledge mapping by locali
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;PCA&#36827;&#34892;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#25104;&#20998;&#23376;&#31354;&#38388;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#23545;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#30340;&#26377;&#25928;&#21306;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.02949</link><description>&lt;p&gt;
&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#26680;PCA
&lt;/p&gt;
&lt;p&gt;
Kernel PCA for Out-of-Distribution Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#26680;PCA&#36827;&#34892;&#22806;&#20998;&#24067;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20027;&#25104;&#20998;&#23376;&#31354;&#38388;&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#26144;&#23556;&#65292;&#23454;&#29616;&#20102;&#23545;&#20869;&#20998;&#24067;&#21644;&#22806;&#20998;&#24067;&#25968;&#25454;&#30340;&#26377;&#25928;&#21306;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#20998;&#24067;&#65288;OoD&#65289;&#26816;&#27979;&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;&#20110;DNN&#29305;&#24449;&#30340;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#22312;&#26816;&#27979;&#26469;&#33258;&#20869;&#20998;&#24067;&#65288;InD&#65289;&#25968;&#25454;&#30340;OoD&#25968;&#25454;&#26041;&#38754;&#19981;&#36275;&#22815;&#12290;PCA&#30340;&#22833;&#36133;&#34920;&#26126;&#65292;&#20165;&#36890;&#36807;&#22312;&#32447;&#24615;&#23376;&#31354;&#38388;&#20013;&#36827;&#34892;&#31616;&#21333;&#22788;&#29702;&#26080;&#27861;&#24456;&#22909;&#22320;&#23558;OoD&#21644;InD&#20013;&#30340;&#32593;&#32476;&#29305;&#24449;&#20998;&#31163;&#24320;&#26469;&#65292;&#32780;&#21487;&#20197;&#36890;&#36807;&#36866;&#24403;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#26469;&#35299;&#20915;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26680;PCA&#65288;KPCA&#65289;&#26694;&#26550;&#36827;&#34892;OoD&#26816;&#27979;&#65292;&#23547;&#25214;OoD&#21644;InD&#29305;&#24449;&#20197;&#26174;&#33879;&#19981;&#21516;&#30340;&#27169;&#24335;&#20998;&#37197;&#30340;&#23376;&#31354;&#38388;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;&#29305;&#24449;&#26144;&#23556;&#65292;&#22312;KPCA&#20013;&#24341;&#20837;&#38750;&#32447;&#24615;&#20869;&#26680;&#65292;&#20197;&#20419;&#36827;&#22312;&#20027;&#25104;&#20998;&#24352;&#25104;&#30340;&#23376;&#31354;&#38388;&#20013;InD&#21644;OoD&#25968;&#25454;&#20043;&#38388;&#30340;&#21487;&#20998;&#24615;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#22312;&#36825;&#31181;&#23376;&#31354;&#38388;&#20013;&#30340;&#37325;&#26500;&#35823;&#24046;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24471;&#21040;$\mathcal{O}(1)$&#26102;&#38388;&#22797;&#26434;&#24230;&#30340;&#26816;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OoD) detection is vital for the reliability of Deep Neural Networks (DNNs). Existing works have shown the insufficiency of Principal Component Analysis (PCA) straightforwardly applied on the features of DNNs in detecting OoD data from In-Distribution (InD) data. The failure of PCA suggests that the network features residing in OoD and InD are not well separated by simply proceeding in a linear subspace, which instead can be resolved through proper nonlinear mappings. In this work, we leverage the framework of Kernel PCA (KPCA) for OoD detection, seeking subspaces where OoD and InD features are allocated with significantly different patterns. We devise two feature mappings that induce non-linear kernels in KPCA to advocate the separability between InD and OoD data in the subspace spanned by the principal components. Given any test sample, the reconstruction error in such subspace is then used to efficiently obtain the detection result with $\mathcal{O}(1)$ time comp
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15113</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#23454;&#29616;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;
&lt;/p&gt;
&lt;p&gt;
Towards Global Glacier Mapping with Deep Learning and Open Earth Observation Data. (arXiv:2401.15113v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24320;&#25918;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#27169;&#22411;&#21644;&#31574;&#30053;&#65292;&#22312;&#22810;&#31181;&#22320;&#24418;&#21644;&#20256;&#24863;&#22120;&#19978;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#36890;&#36807;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#24182;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#23545;&#20110;&#29702;&#35299;&#27668;&#20505;&#21464;&#21270;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#36807;&#31243;&#21463;&#21040;&#20912;&#24029;&#22810;&#26679;&#24615;&#12289;&#38590;&#20197;&#20998;&#31867;&#30340;&#30862;&#30707;&#21644;&#22823;&#25968;&#25454;&#22788;&#29702;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Glacier-VisionTransformer-U-Net (GlaViTU)&#65292;&#19968;&#20010;&#21367;&#31215;-Transformer&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#20116;&#31181;&#21033;&#29992;&#24320;&#25918;&#21355;&#26143;&#24433;&#20687;&#36827;&#34892;&#22810;&#26102;&#30456;&#20840;&#29699;&#20912;&#24029;&#21046;&#22270;&#30340;&#31574;&#30053;&#12290;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#36328;&#20256;&#24863;&#22120;&#30340;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26368;&#20339;&#31574;&#30053;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;IoU&#65288;&#20132;&#24182;&#27604;&#65289;&gt; 0.85&#65292;&#24182;&#19988;&#22312;&#20197;&#20912;&#38634;&#20026;&#20027;&#30340;&#22320;&#21306;&#22686;&#21152;&#21040;&#20102;&gt; 0.90&#65292;&#32780;&#22312;&#39640;&#23665;&#20122;&#27954;&#31561;&#30862;&#30707;&#20016;&#23500;&#30340;&#21306;&#22495;&#21017;&#38477;&#33267;&gt; 0.75&#12290;&#27492;&#22806;&#65292;&#28155;&#21152;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#25968;&#25454;&#65292;&#21363;&#22238;&#27874;&#21644;&#24178;&#28041;&#30456;&#24178;&#24230;&#65292;&#21487;&#20197;&#25552;&#39640;&#25152;&#26377;&#21487;&#29992;&#22320;&#21306;&#30340;&#20934;&#30830;&#24615;&#12290;&#25253;&#21578;&#20912;&#24029;&#33539;&#22260;&#30340;&#26657;&#20934;&#32622;&#20449;&#24230;&#20351;&#39044;&#27979;&#26356;&#21487;&#38752;&#21644;&#21487;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate global glacier mapping is critical for understanding climate change impacts. It is challenged by glacier diversity, difficult-to-classify debris and big data processing. Here we propose Glacier-VisionTransformer-U-Net (GlaViTU), a convolutional-transformer deep learning model, and five strategies for multitemporal global-scale glacier mapping using open satellite imagery. Assessing the spatial, temporal and cross-sensor generalisation shows that our best strategy achieves intersection over union &gt;0.85 on previously unobserved images in most cases, which drops to &gt;0.75 for debris-rich areas such as High-Mountain Asia and increases to &gt;0.90 for regions dominated by clean ice. Additionally, adding synthetic aperture radar data, namely, backscatter and interferometric coherence, increases the accuracy in all regions where available. The calibrated confidence for glacier extents is reported making the predictions more reliable and interpretable. We also release a benchmark dataset 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28176;&#36827;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28176;&#36827;&#33258;&#35757;&#32451;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#20013;&#38388;&#22495;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#22343;&#21248;&#25918;&#32622;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13852</link><description>&lt;p&gt;
&#28176;&#36827;&#22495;&#33258;&#36866;&#24212;&#65306;&#29702;&#35770;&#19982;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Gradual Domain Adaptation: Theory and Algorithms. (arXiv:2310.13852v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28176;&#36827;&#22495;&#33258;&#36866;&#24212;&#20013;&#30340;&#28176;&#36827;&#33258;&#35757;&#32451;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25351;&#20986;&#20102;&#20013;&#38388;&#22495;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#22343;&#21248;&#25918;&#32622;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#26159;&#23558;&#27169;&#22411;&#20174;&#26377;&#26631;&#35760;&#30340;&#28304;&#22495;&#36866;&#24212;&#21040;&#26080;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#30340;&#19968;&#31181;&#19968;&#27425;&#24615;&#26041;&#27861;&#12290;&#23613;&#31649;&#34987;&#24191;&#27867;&#24212;&#29992;&#65292;&#20294;&#24403;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#36739;&#22823;&#26102;&#65292;UDA&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#28176;&#36827;&#22495;&#33258;&#36866;&#24212;&#65288;GDA&#65289;&#36890;&#36807;&#20351;&#29992;&#20013;&#38388;&#22495;&#36880;&#28176;&#20174;&#28304;&#22495;&#36866;&#24212;&#21040;&#30446;&#26631;&#22495;&#26469;&#32531;&#35299;&#36825;&#20010;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#19968;&#31181;&#24120;&#35265;&#30340;GDA&#31639;&#27861;&#8212;&#8212;&#28176;&#36827;&#33258;&#35757;&#32451;&#65292;&#24182;&#25552;&#20379;&#20102;&#19982;Kumar&#31561;&#20154;&#65288;2020&#65289;&#30456;&#27604;&#26174;&#33879;&#25913;&#36827;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#24471;&#20986;&#19968;&#20010;&#26377;&#36259;&#30340;&#35266;&#28857;&#65306;&#20026;&#20102;&#26368;&#23567;&#21270;&#30446;&#26631;&#22495;&#19978;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#20013;&#38388;&#22495;&#30340;&#39034;&#24207;&#24212;&#35813;&#22343;&#21248;&#22320;&#25918;&#32622;&#22312;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;Wasserstein&#27979;&#22320;&#32447;&#19978;&#12290;&#36825;&#20010;&#35266;&#28857;&#22312;&#20013;&#38388;&#22495;&#32570;&#22833;&#25110;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#29992;&#65292;&#32780;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#32463;&#24120;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) adapts a model from a labeled source domain to an unlabeled target domain in a one-off way. Though widely applied, UDA faces a great challenge whenever the distribution shift between the source and the target is large. Gradual domain adaptation (GDA) mitigates this limitation by using intermediate domains to gradually adapt from the source to the target domain. In this work, we first theoretically analyze gradual self-training, a popular GDA algorithm, and provide a significantly improved generalization bound compared with Kumar et al. (2020). Our theoretical analysis leads to an interesting insight: to minimize the generalization error on the target domain, the sequence of intermediate domains should be placed uniformly along the Wasserstein geodesic between the source and target domains. The insight is particularly useful under the situation where intermediate domains are missing or scarce, which is often the case in real-world applications. Based
&lt;/p&gt;</description></item><item><title>HPCR&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22522;&#20110;&#20195;&#29702;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#37325;&#25918;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#20351;&#29992;&#38170;&#28857;-&#20195;&#29702;&#23545;&#26367;&#25442;&#38170;&#28857;-&#26679;&#26412;&#23545;&#65292;HPCR&#33021;&#22815;&#20943;&#36731;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#26377;&#25928;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HPCR&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15038</link><description>&lt;p&gt;
HPCR: &#22522;&#20110;&#20195;&#29702;&#30340;&#32508;&#21512;&#23545;&#27604;&#37325;&#25918;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HPCR: Holistic Proxy-based Contrastive Replay for Online Continual Learning. (arXiv:2309.15038v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15038
&lt;/p&gt;
&lt;p&gt;
HPCR&#26159;&#19968;&#31181;&#29992;&#20110;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32508;&#21512;&#20102;&#22522;&#20110;&#20195;&#29702;&#21644;&#23545;&#27604;&#25439;&#22833;&#30340;&#37325;&#25918;&#26041;&#24335;&#12290;&#36890;&#36807;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#20351;&#29992;&#38170;&#28857;-&#20195;&#29702;&#23545;&#26367;&#25442;&#38170;&#28857;-&#26679;&#26412;&#23545;&#65292;HPCR&#33021;&#22815;&#20943;&#36731;&#36951;&#24536;&#29616;&#35937;&#65292;&#24182;&#26377;&#25928;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HPCR&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#65288;OCL&#65289;&#26088;&#22312;&#36890;&#36807;&#19968;&#27425;&#22312;&#32447;&#25968;&#25454;&#27969;&#20256;&#36882;&#25345;&#32493;&#23398;&#20064;&#26032;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23427;&#36890;&#24120;&#20250;&#38754;&#20020;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#37325;&#25918;&#30340;&#26041;&#27861;&#36890;&#36807;&#20197;&#20195;&#29702;&#20026;&#22522;&#30784;&#25110;&#23545;&#27604;&#20026;&#22522;&#30784;&#30340;&#37325;&#25918;&#26041;&#24335;&#26377;&#25928;&#22320;&#32531;&#35299;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20004;&#31181;&#37325;&#25918;&#26041;&#24335;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#21457;&#29616;&#23427;&#20204;&#21487;&#20197;&#30456;&#20114;&#34917;&#20805;&#12290;&#21463;&#21040;&#36825;&#19968;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#25918;&#30340;&#26041;&#27861;&#31216;&#20026;&#20195;&#29702;&#23545;&#27604;&#37325;&#25918;&#65288;PCR&#65289;&#65292;&#23427;&#23558;&#23545;&#27604;&#25439;&#22833;&#20013;&#30340;&#38170;&#28857;-&#26679;&#26412;&#23545;&#26367;&#25442;&#20026;&#38170;&#28857;-&#20195;&#29702;&#23545;&#65292;&#20197;&#20943;&#36731;&#36951;&#24536;&#29616;&#35937;&#12290;&#22522;&#20110;PCR&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#26356;&#39640;&#32423;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#32508;&#21512;&#20195;&#29702;&#23545;&#27604;&#37325;&#25918;&#65288;HPCR&#65289;&#65292;&#23427;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#12290;&#23545;&#27604;&#32452;&#20214;&#22312;PCR&#30340;&#22522;&#30784;&#19978;&#26465;&#20214;&#24615;&#22320;&#23558;&#38170;&#28857;-&#26679;&#26412;&#23545;&#32435;&#20837;&#20854;&#20013;&#65292;&#36890;&#36807;&#22823;&#22411;&#35757;&#32451;&#25209;&#27425;&#23398;&#20064;&#26356;&#32454;&#31890;&#24230;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#31532;&#20108;&#20010;&#32452;&#20214;&#26159;&#37325;&#25918;&#32452;&#20214;&#65292;&#23427;&#22312;&#26679;&#26412;&#36873;&#25321;&#19978;&#37319;&#29992;&#20102;&#22810;&#26679;&#24615;&#31574;&#30053;&#65292;&#20197;&#30830;&#20445;&#20195;&#29702;&#25968;&#25454;&#19982;&#24403;&#21069;&#20219;&#21153;&#20855;&#26377;&#26356;&#39640;&#30340;&#20851;&#32852;&#24615;&#12290;&#31532;&#19977;&#20010;&#32452;&#20214;&#26159;&#27491;&#21017;&#21270;&#32452;&#20214;&#65292;&#36890;&#36807;&#32553;&#23567;&#26679;&#26412;&#31354;&#38388;&#65292;&#20419;&#36827;&#23398;&#20064;&#27169;&#22411;&#23545;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#30340;&#26356;&#22909;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;HPCR&#26041;&#27861;&#22312;&#22810;&#20010;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online continual learning (OCL) aims to continuously learn new data from a single pass over the online data stream. It generally suffers from the catastrophic forgetting issue. Existing replay-based methods effectively alleviate this issue by replaying part of old data in a proxy-based or contrastive-based replay manner. In this paper, we conduct a comprehensive analysis of these two replay manners and find they can be complementary. Inspired by this finding, we propose a novel replay-based method called proxy-based contrastive replay (PCR), which replaces anchor-to-sample pairs with anchor-to-proxy pairs in the contrastive-based loss to alleviate the phenomenon of forgetting. Based on PCR, we further develop a more advanced method named holistic proxy-based contrastive replay (HPCR), which consists of three components. The contrastive component conditionally incorporates anchor-to-sample pairs to PCR, learning more fine-grained semantic information with a large training batch. The sec
&lt;/p&gt;</description></item><item><title>Ano-SuPs&#26159;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#21487;&#30097;&#21306;&#22359;&#26469;&#36827;&#34892;&#21046;&#36896;&#20135;&#21697;&#30340;&#22810;&#23610;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#35299;&#20915;&#22270;&#20687;&#32972;&#26223;&#22797;&#26434;&#24615;&#21644;&#24322;&#24120;&#27169;&#24335;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.11120</link><description>&lt;p&gt;
Ano-SuPs: &#36890;&#36807;&#35782;&#21035;&#21487;&#30097;&#30340;&#21306;&#22359;&#36827;&#34892;&#21046;&#36896;&#20135;&#21697;&#30340;&#22810;&#23610;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Ano-SuPs: Multi-size anomaly detection for manufactured products by identifying suspected patches. (arXiv:2309.11120v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11120
&lt;/p&gt;
&lt;p&gt;
Ano-SuPs&#26159;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#21487;&#30097;&#21306;&#22359;&#26469;&#36827;&#34892;&#21046;&#36896;&#20135;&#21697;&#30340;&#22810;&#23610;&#24230;&#24322;&#24120;&#26816;&#27979;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#35299;&#20915;&#22270;&#20687;&#32972;&#26223;&#22797;&#26434;&#24615;&#21644;&#24322;&#24120;&#27169;&#24335;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#20687;&#30340;&#31995;&#32479;&#22240;&#20854;&#25552;&#20379;&#20016;&#23500;&#30340;&#21046;&#36896;&#29366;&#24577;&#20449;&#24687;&#12289;&#20302;&#23454;&#26045;&#25104;&#26412;&#21644;&#39640;&#37319;&#38598;&#36895;&#24230;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#22270;&#20687;&#32972;&#26223;&#30340;&#22797;&#26434;&#24615;&#21644;&#21508;&#31181;&#24322;&#24120;&#27169;&#24335;&#32473;&#29616;&#26377;&#30340;&#30697;&#38453;&#20998;&#35299;&#26041;&#27861;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36275;&#20197;&#28385;&#36275;&#24314;&#27169;&#38656;&#27714;&#12290;&#27492;&#22806;&#65292;&#24322;&#24120;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#23548;&#33268;&#24322;&#24120;&#30340;&#27745;&#26579;&#38382;&#39064;&#65292;&#20351;&#24471;&#35774;&#35745;&#30340;&#27169;&#22411;&#21644;&#26041;&#27861;&#23545;&#22806;&#37096;&#24178;&#25200;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35782;&#21035;&#21487;&#30097;&#21306;&#22359;&#65288;Ano-SuPs&#65289;&#26469;&#26816;&#27979;&#24322;&#24120;&#30340;&#20004;&#38454;&#27573;&#31574;&#30053;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#20004;&#27425;&#37325;&#24314;&#36755;&#20837;&#22270;&#20687;&#26469;&#26816;&#27979;&#24102;&#26377;&#24322;&#24120;&#30340;&#21306;&#22359;&#30340;&#26041;&#27861;&#65306;&#31532;&#19968;&#27493;&#26159;&#36890;&#36807;&#21435;&#38500;&#37027;&#20123;&#21487;&#30097;&#21306;&#22359;&#26469;&#33719;&#24471;&#19968;&#32452;&#27491;&#24120;&#21306;&#22359;&#65292;&#31532;&#20108;&#27493;&#26159;&#20351;&#29992;&#36825;&#20123;&#27491;&#24120;&#21306;&#22359;&#26469;&#20248;&#21270;&#23545;&#24102;&#26377;&#24322;&#24120;&#21306;&#22359;&#30340;&#35782;&#21035;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image-based systems have gained popularity owing to their capacity to provide rich manufacturing status information, low implementation costs and high acquisition rates. However, the complexity of the image background and various anomaly patterns pose new challenges to existing matrix decomposition methods, which are inadequate for modeling requirements. Moreover, the uncertainty of the anomaly can cause anomaly contamination problems, making the designed model and method highly susceptible to external disturbances. To address these challenges, we propose a two-stage strategy anomaly detection method that detects anomalies by identifying suspected patches (Ano-SuPs). Specifically, we propose to detect the patches with anomalies by reconstructing the input image twice: the first step is to obtain a set of normal patches by removing those suspected patches, and the second step is to use those normal patches to refine the identification of the patches with anomalies. To demonstrate its ef
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03572</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#21487;&#35777;&#25928;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Learning in Partially Observable Contextual Bandit. (arXiv:2308.03572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#20165;&#26377;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#26377;&#38480;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#21482;&#26377;&#37096;&#20998;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#26469;&#35782;&#21035;&#25110;&#37096;&#20998;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26410;&#30693;&#20998;&#24067;&#30340;&#21407;&#22987;&#21151;&#33021;&#32422;&#26463;&#31163;&#25955;&#21270;&#20026;&#32447;&#24615;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#39034;&#24207;&#35299;&#32447;&#24615;&#35268;&#21010;&#26469;&#37319;&#26679;&#20860;&#23481;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#20272;&#35745;&#35823;&#24046;&#24471;&#21040;&#22240;&#26524;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20026;&#36866;&#24403;&#30340;&#37319;&#26679;&#20998;&#24067;&#25552;&#20379;&#20102;&#29702;&#24819;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#32422;&#26463;&#24212;&#29992;&#20110;&#25913;&#36827;&#32463;&#20856;&#30340;&#36718;&#30424;&#36172;&#31639;&#27861;&#65292;&#24182;&#20197;&#34892;&#21160;&#38598;&#21644;&#20989;&#25968;&#31354;&#38388;&#35268;&#27169;&#20026;&#21442;&#32771;&#25913;&#21464;&#20102;&#36951;&#25022;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#19968;&#33324;&#24773;&#22659;&#20998;&#24067;&#30340;&#20989;&#25968;&#36924;&#36817;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SplitFC&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31574;&#30053;&#26469;&#20943;&#23569;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#36825;&#20123;&#31574;&#30053;&#20998;&#21035;&#26159;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#25481;&#33853;&#21644;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.10805</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#21387;&#32553;&#23454;&#29616;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Split Learning via Adaptive Feature-Wise Compression. (arXiv:2307.10805v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10805
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;SplitFC&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#31181;&#33258;&#36866;&#24212;&#21387;&#32553;&#31574;&#30053;&#26469;&#20943;&#23569;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#36825;&#20123;&#31574;&#30053;&#20998;&#21035;&#26159;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#25481;&#33853;&#21644;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SplitFC&#30340;&#26032;&#39062;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#21106;&#23398;&#20064;&#65288;SL&#65289;&#26694;&#26550;&#65292;&#23427;&#20943;&#23569;&#20102;&#22312;SL&#22521;&#35757;&#36807;&#31243;&#20013;&#20256;&#36755;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#25152;&#38656;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;SplitFC&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#30697;&#38453;&#30340;&#21015;&#25152;&#23637;&#31034;&#30340;&#19981;&#21516;&#30340;&#31163;&#25955;&#31243;&#24230;&#12290;SplitFC&#25972;&#21512;&#20102;&#20004;&#31181;&#21387;&#32553;&#31574;&#30053;&#65306;&#65288;i&#65289;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#25481;&#33853;&#21644;&#65288;ii&#65289;&#33258;&#36866;&#24212;&#29305;&#24449;&#36880;&#28176;&#37327;&#21270;&#12290;&#22312;&#31532;&#19968;&#31181;&#31574;&#30053;&#20013;&#65292;&#20013;&#38388;&#29305;&#24449;&#21521;&#37327;&#26681;&#25454;&#36825;&#20123;&#21521;&#37327;&#30340;&#26631;&#20934;&#20559;&#24046;&#30830;&#23450;&#33258;&#36866;&#24212;&#25481;&#33853;&#27010;&#29575;&#36827;&#34892;&#25481;&#33853;&#12290;&#28982;&#21518;&#65292;&#30001;&#20110;&#38142;&#24335;&#35268;&#21017;&#65292;&#19982;&#34987;&#20002;&#24323;&#30340;&#29305;&#24449;&#21521;&#37327;&#30456;&#20851;&#32852;&#30340;&#20013;&#38388;&#26799;&#24230;&#21521;&#37327;&#20063;&#20250;&#34987;&#20002;&#24323;&#12290;&#22312;&#31532;&#20108;&#31181;&#31574;&#30053;&#20013;&#65292;&#38750;&#20002;&#24323;&#30340;&#20013;&#38388;&#29305;&#24449;&#21644;&#26799;&#24230;&#21521;&#37327;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#33539;&#22260;&#30830;&#23450;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#32423;&#21035;&#36827;&#34892;&#37327;&#21270;&#12290;&#20026;&#20102;&#23613;&#37327;&#20943;&#23567;&#37327;&#21270;&#35823;&#24046;&#65292;&#26368;&#20248;&#37327;&#21270;&#26159;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel communication-efficient split learning (SL) framework, named SplitFC, which reduces the communication overhead required for transmitting intermediate feature and gradient vectors during the SL training process. The key idea of SplitFC is to leverage different dispersion degrees exhibited in the columns of the matrices. SplitFC incorporates two compression strategies: (i) adaptive feature-wise dropout and (ii) adaptive feature-wise quantization. In the first strategy, the intermediate feature vectors are dropped with adaptive dropout probabilities determined based on the standard deviation of these vectors. Then, by the chain rule, the intermediate gradient vectors associated with the dropped feature vectors are also dropped. In the second strategy, the non-dropped intermediate feature and gradient vectors are quantized using adaptive quantization levels determined based on the ranges of the vectors. To minimize the quantization error, the optimal quantizatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.08939</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22312;&#19978;&#19979;&#25991;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#23454;&#39564;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Experimental Security Analysis of DNN-based Adaptive Cruise Control under Context-Aware Perception Attacks. (arXiv:2307.08939v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08939
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#21644;&#22522;&#20110;&#20248;&#21270;&#30340;&#22270;&#20687;&#25200;&#21160;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39550;&#39542;&#21592;&#36741;&#21161;&#21151;&#33021;&#65292;&#29992;&#20110;&#20445;&#25345;&#26399;&#26395;&#36895;&#24230;&#21644;&#19982;&#21069;&#26041;&#36710;&#36742;&#30340;&#23433;&#20840;&#36317;&#31163;&#12290;&#26412;&#25991;&#35780;&#20272;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;ACC&#31995;&#32479;&#22312;&#38544;&#34109;&#24863;&#30693;&#25915;&#20987;&#19979;&#30340;&#23433;&#20840;&#24615;&#65292;&#35813;&#25915;&#20987;&#20250;&#23545;&#25668;&#20687;&#26426;&#25968;&#25454;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#25200;&#21160;&#65292;&#20197;&#23548;&#33268;&#21069;&#26041;&#30896;&#25758;&#20107;&#25925;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#31574;&#30053;&#65292;&#29992;&#20110;&#36873;&#25321;&#35302;&#21457;&#25915;&#20987;&#26368;&#20851;&#38190;&#30340;&#26102;&#38388;&#28857;&#65292;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#36816;&#34892;&#26102;&#29983;&#25104;&#36866;&#24212;&#24615;&#22270;&#20687;&#25200;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#39550;&#39542;&#25968;&#25454;&#38598;&#21644;&#36924;&#30495;&#30340;&#20223;&#30495;&#24179;&#21488;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#20223;&#30495;&#24179;&#21488;&#20351;&#29992;&#20102;&#26469;&#33258;&#29983;&#20135;ACC&#31995;&#32479;&#30340;&#25511;&#21046;&#36719;&#20214;&#21644;&#29289;&#29702;&#19990;&#30028;&#39550;&#39542;&#27169;&#25311;&#22120;&#65292;&#24182;&#32771;&#34385;&#20102;&#39550;&#39542;&#21592;&#30340;&#24178;&#39044;&#20197;&#21450;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#21644;&#21069;&#21521;&#30896;&#25758;&#35686;&#31034;&#65288;FCW&#65289;&#31561;&#23433;&#20840;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive Cruise Control (ACC) is a widely used driver assistance feature for maintaining desired speed and safe distance to the leading vehicles. This paper evaluates the security of the deep neural network (DNN) based ACC systems under stealthy perception attacks that strategically inject perturbations into camera data to cause forward collisions. We present a combined knowledge-and-data-driven approach to design a context-aware strategy for the selection of the most critical times for triggering the attacks and a novel optimization-based method for the adaptive generation of image perturbations at run-time. We evaluate the effectiveness of the proposed attack using an actual driving dataset and a realistic simulation platform with the control software from a production ACC system and a physical-world driving simulator while considering interventions by the driver and safety features such as Automatic Emergency Braking (AEB) and Forward Collision Warning (FCW). Experimental results sh
&lt;/p&gt;</description></item></channel></rss>