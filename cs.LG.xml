<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#22312;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#24341;&#20837;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20302;&#19988;&#32593;&#32476;&#39640;&#24230;&#36830;&#36890;&#26102;&#26377;&#25928;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.15654</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#26412;&#22320;&#26356;&#26032;&#23545;&#20998;&#25955;&#24335;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Effectiveness of Local Updates for Decentralized Learning under Data Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15654
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#24341;&#20837;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#65292;&#21487;&#20197;&#38477;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#65292;&#20174;&#32780;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20302;&#19988;&#32593;&#32476;&#39640;&#24230;&#36830;&#36890;&#26102;&#26377;&#25928;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#20004;&#31181;&#22522;&#26412;&#30340;&#20998;&#25955;&#24335;&#20248;&#21270;&#26041;&#27861;&#65292;&#21363;Decentralized Gradient Tracking (DGT) &#21644; Decentralized Gradient Descent (DGD)&#65292;&#24182;&#24341;&#20837;&#20102;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#24773;&#22659;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#21152;&#20837; $K &gt; 1$ &#20010;&#26412;&#22320;&#26356;&#26032;&#27493;&#39588;&#33021;&#22815;&#38477;&#20302;&#36890;&#20449;&#22797;&#26434;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110; $\mu$-&#24378;&#20984;&#21644; $L$-&#20809;&#28369;&#25439;&#22833;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26412;&#22320; DGT &#26041;&#27861;&#23454;&#29616;&#20102;&#36890;&#20449;&#22797;&#26434;&#24230;&#20026; $\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} + \frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+ \delta}{\mu}\Big)$&#65292;&#20854;&#20013; $\rho$ &#34913;&#37327;&#32593;&#32476;&#36830;&#36890;&#24615;&#65292;$\delta$ &#34920;&#31034;&#26412;&#22320;&#25439;&#22833;&#30340;&#20108;&#38454;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#34920;&#26126;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#20302;&#19988;&#32593;&#32476;&#39640;&#24230;&#36830;&#36890;&#26102;&#65292;&#22686;&#21152; $K$ &#33021;&#26377;&#25928;&#38477;&#20302;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15654v1 Announce Type: new  Abstract: We revisit two fundamental decentralized optimization methods, Decentralized Gradient Tracking (DGT) and Decentralized Gradient Descent (DGD), with multiple local updates. We consider two settings and demonstrate that incorporating $K &gt; 1$ local update steps can reduce communication complexity. Specifically, for $\mu$-strongly convex and $L$-smooth loss functions, we proved that local DGT achieves communication complexity $\tilde{\mathcal{O}} \Big(\frac{L}{\mu K} + \frac{\delta}{\mu (1 - \rho)} + \frac{\rho }{(1 - \rho)^2} \cdot \frac{L+ \delta}{\mu}\Big)$, where $\rho$ measures the network connectivity and $\delta$ measures the second-order heterogeneity of the local loss. Our result reveals the tradeoff between communication and computation and shows increasing $K$ can effectively reduce communication costs when the data heterogeneity is low and the network is well-connected. We then consider the over-parameterization regime where the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.01875</link><description>&lt;p&gt;
ICLN&#65306;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#29992;&#20110;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ICLN: Input Convex Loss Network for Decision Focused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#36890;&#24120;&#34987;&#35748;&#20026;&#19982;&#20248;&#21270;&#37096;&#20998;&#26080;&#20851;&#12290;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#20010;&#38754;&#21521;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#20197;&#20026;&#30456;&#24212;&#20219;&#21153;&#25552;&#20379;&#26356;&#22909;&#30340;&#20915;&#31574;&#26469;&#25972;&#21512;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#30340;DFL&#33539;&#24335;&#20013;&#23454;&#29616;&#12290;ICLN&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#24050;&#32463;&#34987;&#20445;&#35777;&#20026;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01875v1 Announce Type: cross  Abstract: In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time. In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some in
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#22312;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#20811;&#26381;&#39640;&#32500;&#25968;&#25454;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.00669</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#25512;&#21160;&#22686;&#26448;&#21046;&#36896;&#65306;&#24403;&#21069;&#36827;&#23637;&#21644;&#26410;&#26469;&#25361;&#25112;&#30340;&#32508;&#21512;&#35780;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advancing Additive Manufacturing through Deep Learning: A Comprehensive Review of Current Progress and Future Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00669
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#26174;&#31034;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#20811;&#26381;&#39640;&#32500;&#25968;&#25454;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#25512;&#21160;&#35813;&#39046;&#22495;&#19981;&#26029;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#26448;&#21046;&#36896;&#65288;AM&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#20943;&#23569;&#21046;&#36896;&#30340;&#28508;&#22312;&#26367;&#20195;&#21697;&#65292;&#22240;&#20026;&#20854;&#22312;&#26368;&#23567;&#26448;&#26009;&#28010;&#36153;&#30340;&#24773;&#20917;&#19979;&#21046;&#36896;&#39640;&#24230;&#23450;&#21046;&#20135;&#21697;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21253;&#25324;&#22797;&#26434;&#21644;&#21160;&#24577;&#36807;&#31243;&#30456;&#20114;&#20316;&#29992;&#22312;&#20869;&#30340;&#19968;&#20123;&#20027;&#35201;&#22266;&#26377;&#25361;&#25112;&#65292;&#21363;&#20351;&#20351;&#29992;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#26377;&#26102;&#20063;&#38590;&#20197;&#23436;&#20840;&#29702;&#35299;&#65292;&#22240;&#20026;&#28041;&#21450;&#21040;&#39640;&#32500;&#25968;&#25454;&#65292;&#22914;&#22270;&#20687;&#12289;&#28857;&#20113;&#21644;&#20307;&#32032;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#20986;&#29616;&#30340;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#22312;&#20811;&#26381;&#35768;&#22810;&#36825;&#20123;&#25361;&#25112;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;DL&#33021;&#22815;&#33258;&#21160;&#20174;&#39640;&#32500;&#25968;&#25454;&#20013;&#25429;&#25417;&#22797;&#26434;&#20851;&#31995;&#65292;&#32780;&#26080;&#38656;&#25163;&#24037;&#21046;&#20316;&#29305;&#24449;&#25552;&#21462;&#12290;&#22240;&#27492;&#65292;AM&#21644;DL&#20132;&#21449;&#39046;&#22495;&#30340;&#30740;&#31350;&#37327;&#27599;&#24180;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#36825;&#21487;&#33021;&#20250;&#23558;&#22686;&#26448;&#21046;&#36896;&#25512;&#21521;&#26356;&#24191;&#38420;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00669v1 Announce Type: new  Abstract: Additive manufacturing (AM) has already proved itself to be the potential alternative to widely-used subtractive manufacturing due to its extraordinary capacity of manufacturing highly customized products with minimum material wastage. Nevertheless, it is still not being considered as the primary choice for the industry due to some of its major inherent challenges, including complex and dynamic process interactions, which are sometimes difficult to fully understand even with traditional machine learning because of the involvement of high-dimensional data such as images, point clouds, and voxels. However, the recent emergence of deep learning (DL) is showing great promise in overcoming many of these challenges as DL can automatically capture complex relationships from high-dimensional data without hand-crafted feature extraction. Therefore, the volume of research in the intersection of AM and DL is exponentially growing each year which ma
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.16991</link><description>&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#30456;&#21464;&#25581;&#31034;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
A Phase Transition in Diffusion Models Reveals the Hierarchical Nature of Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16991
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#30740;&#31350;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#23637;&#31034;&#20986;&#20102;&#22312;&#38408;&#20540;&#26102;&#38388;&#21457;&#29983;&#30456;&#21464;&#30340;&#29305;&#24615;&#65292;&#36825;&#24433;&#21709;&#20102;&#39640;&#32423;&#29305;&#24449;&#21644;&#20302;&#32423;&#29305;&#24449;&#30340;&#37325;&#24314;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#30495;&#23454;&#25968;&#25454;&#30340;&#32467;&#26500;&#22312;&#25512;&#21160;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#33258;&#28982;&#25968;&#25454;&#65292;&#22914;&#22270;&#20687;&#65292;&#34987;&#35748;&#20026;&#26159;&#30001;&#20197;&#23618;&#27425;&#21644;&#32452;&#21512;&#26041;&#24335;&#32452;&#32455;&#30340;&#29305;&#24449;&#32452;&#25104;&#30340;&#65292;&#31070;&#32463;&#32593;&#32476;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#25429;&#25417;&#21040;&#36825;&#20123;&#29305;&#24449;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#26174;&#31034;&#65292;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#26263;&#31034;&#20102;&#23427;&#20204;&#25429;&#25417;&#21040;&#36825;&#31181;&#28508;&#22312;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25968;&#25454;&#30340;&#20998;&#23618;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#36825;&#19968;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#26102;&#38388;$t$&#21518;&#20316;&#29992;&#30340;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#21463;&#21040;&#26576;&#20010;&#38408;&#20540;&#26102;&#38388;&#22788;&#30340;&#30456;&#21464;&#25511;&#21046;&#65292;&#27492;&#26102;&#37325;&#24314;&#39640;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#31867;&#21035;&#65289;&#30340;&#27010;&#29575;&#31361;&#28982;&#19979;&#38477;&#12290;&#30456;&#21453;&#65292;&#20302;&#32423;&#29305;&#24449;&#65288;&#22914;&#22270;&#20687;&#30340;&#20855;&#20307;&#32454;&#33410;&#65289;&#30340;&#37325;&#24314;&#22312;&#25972;&#20010;&#25193;&#25955;&#36807;&#31243;&#20013;&#24179;&#31283;&#28436;&#21464;&#12290;&#36825;&#19968;&#32467;&#26524;&#26263;&#31034;&#65292;&#22312;&#36229;&#20986;&#36716;&#21464;&#26102;&#38388;&#30340;&#26102;&#21051;&#65292;&#31867;&#21035;&#24050;&#21464;&#21270;&#65292;&#20294;&#26159;&#22522;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16991v1 Announce Type: cross  Abstract: Understanding the structure of real data is paramount in advancing modern deep-learning methodologies. Natural data such as images are believed to be composed of features organised in a hierarchical and combinatorial manner, which neural networks capture during learning. Recent advancements show that diffusion models can generate high-quality images, hinting at their ability to capture this underlying structure. We study this phenomenon in a hierarchical generative model of data. We find that the backward diffusion process acting after a time $t$ is governed by a phase transition at some threshold time, where the probability of reconstructing high-level features, like the class of an image, suddenly drops. Instead, the reconstruction of low-level features, such as specific details of an image, evolves smoothly across the whole diffusion process. This result implies that at times beyond the transition, the class has changed but the gene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;</title><link>https://arxiv.org/abs/2402.10142</link><description>&lt;p&gt;
&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Tracking Changing Probabilities via Dynamic Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10142
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#21160;&#24577;&#23398;&#20064;&#22120;&#36861;&#36394;&#27010;&#29575;&#21464;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36755;&#20986;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#31163;&#25955;&#39033;&#30446;&#24207;&#21015;&#20013;&#19979;&#19968;&#20010;&#21487;&#33021;&#20986;&#29616;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32771;&#34385;&#19968;&#20010;&#39044;&#27979;&#22120;&#65292;&#21363;&#19968;&#20010;&#23398;&#20064;&#22120;&#65292;&#20854;&#36755;&#20837;&#26159;&#19968;&#31995;&#21015;&#31163;&#25955;&#39033;&#30446;&#12290;&#39044;&#27979;&#22120;&#30340;&#20219;&#21153;&#26159;&#22312;&#27599;&#20010;&#26102;&#38388;&#28857;&#36827;&#34892;&#27010;&#29575;&#22810;&#31867;&#21035;&#39044;&#27979;&#65292;&#21363;&#36890;&#36807;&#36755;&#20986;&#26377;&#38646;&#20010;&#25110;&#22810;&#20010;&#20505;&#36873;&#39033;&#30446;&#21450;&#20854;&#27010;&#29575;&#26469;&#39044;&#27979;&#25509;&#19979;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#39033;&#30446;&#65292;&#28982;&#21518;&#25581;&#31034;&#23454;&#38469;&#39033;&#30446;&#24182;&#20174;&#20013;&#23398;&#20064;&#12290;&#20026;&#20102;&#36755;&#20986;&#27010;&#29575;&#65292;&#39044;&#27979;&#22120;&#20250;&#36319;&#36394;&#20854;&#25152;&#35265;&#39033;&#30446;&#30340;&#27604;&#20363;&#12290;&#39044;&#27979;&#22120;&#20855;&#26377;&#24658;&#23450;&#65288;&#26377;&#38480;&#65289;&#30340;&#31354;&#38388;&#65292;&#25105;&#20204;&#23547;&#27714;&#39640;&#25928;&#30340;&#39044;&#27979;&#21644;&#26356;&#26032;&#25216;&#26415;&#65306;&#27969;&#26159;&#26080;&#30028;&#30340;&#65292;&#39033;&#30446;&#30340;&#38598;&#21512;&#23545;&#39044;&#27979;&#22120;&#26159;&#26410;&#30693;&#30340;&#65292;&#23427;&#20204;&#30340;&#24635;&#25968;&#20063;&#21487;&#33021;&#26080;&#38480;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#23384;&#22312;&#38750;&#24179;&#31283;&#24615;&#65306;&#39033;&#30446;&#30340;&#28508;&#22312;&#39057;&#29575;&#21487;&#33021;&#20250;&#19981;&#26102;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#20363;&#22914;&#65292;&#26032;&#39033;&#30446;&#21487;&#33021;&#24320;&#22987;&#20986;&#29616;&#65292;&#19968;&#20123;&#24403;&#21069;&#39057;&#32321;&#20986;&#29616;&#30340;&#39033;&#30446;&#21487;&#33021;&#20877;&#27425;&#20572;&#27490;&#20986;&#29616;&#12290;&#30001;&#20110;&#26377;&#31354;&#38388;&#38480;&#21046;&#65292;&#39044;&#27979;&#22120;&#21482;&#38656;&#35201;&#25552;&#20379;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10142v1 Announce Type: cross  Abstract: Consider a predictor, a learner, whose input is a stream of discrete items. The predictor's task, at every time point, is probabilistic multiclass prediction, i.e., to predict which item may occur next by outputting zero or more candidate items, each with a probability, after which the actual item is revealed and the predictor learns from this observation. To output probabilities, the predictor keeps track of the proportions of the items it has seen. The predictor has constant (limited) space and we seek efficient prediction and update techniques: The stream is unbounded, the set of items is unknown to the predictor and their totality can also grow unbounded. Moreover, there is non-stationarity: the underlying frequencies of items may change, substantially, from time to time. For instance, new items may start appearing and a few currently frequent items may cease to occur again. The predictor, being space-bounded, need only provide pro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08309</link><description>&lt;p&gt;
&#36890;&#36807;&#25552;&#31034;&#30340;&#19978;&#19979;&#25991;&#21521;&#37327;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Prompted Contextual Vectors for Spear-Phishing Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08309
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26816;&#27979;&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#30340;&#30005;&#23376;&#37038;&#20214;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#39640;&#25928;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38035;&#40060;&#32593;&#32476;&#25915;&#20987;&#26159;&#19968;&#20010;&#37325;&#22823;&#30340;&#23433;&#20840;&#25361;&#25112;&#65292;&#32780;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#29983;&#25104;&#20196;&#20154;&#20449;&#26381;&#30340;&#30005;&#23376;&#37038;&#20214;&#24182;&#26041;&#20415;&#30446;&#26631;&#20390;&#23519;&#26469;&#21319;&#32423;&#20102;&#23041;&#32961;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;LLMs&#30340;&#38598;&#21512;&#26469;&#21019;&#24314;&#34920;&#31034;&#21521;&#37327;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#26469;&#25512;&#29702;&#21644;&#22238;&#31572;&#20154;&#24037;&#21046;&#23450;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#37327;&#21270;&#30005;&#23376;&#37038;&#20214;&#20869;&#23481;&#20013;&#24120;&#35265;&#35828;&#26381;&#21407;&#21017;&#30340;&#23384;&#22312;&#65292;&#20026;&#19979;&#28216;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#29983;&#25104;&#25552;&#31034;&#19978;&#19979;&#25991;&#25991;&#26723;&#21521;&#37327;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19987;&#26377;&#31995;&#32479;&#29983;&#25104;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#31995;&#32479;&#33258;&#21160;&#21270;&#30446;&#26631;&#20390;&#23519;&#21644;&#38035;&#40060;&#30005;&#23376;&#37038;&#20214;&#30340;&#21019;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20165;&#21253;&#21547;&#20256;&#32479;&#38035;&#40060;&#21644;&#33391;&#24615;&#30005;&#23376;&#37038;&#20214;&#30340;&#35757;&#32451;&#38598;&#20013;&#23454;&#29616;&#20102;91%&#30340;F1&#24471;&#20998;&#65292;&#20854;&#20013;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#19968;&#31181;&#21019;&#26032;&#30340;&#25991;&#26723;&#21521;&#37327;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spear-phishing attacks present a significant security challenge, with large language models (LLMs) escalating the threat by generating convincing emails and facilitating target reconnaissance. To address this, we propose a detection approach based on a novel document vectorization method that utilizes an ensemble of LLMs to create representation vectors. By prompting LLMs to reason and respond to human-crafted questions, we quantify the presence of common persuasion principles in the email's content, producing prompted contextual document vectors for a downstream supervised machine learning model. We evaluate our method using a unique dataset generated by a proprietary system that automates target reconnaissance and spear-phishing email creation. Our method achieves a 91% F1 score in identifying LLM-generated spear-phishing emails, with the training set comprising only traditional phishing and benign emails. Key contributions include an innovative document vectorization method utilizin
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.06535</link><description>&lt;p&gt;
Bandit Convex Optimisation&#65288;&#24378;&#30423;&#20984;&#20248;&#21270;&#65289;
&lt;/p&gt;
&lt;p&gt;
Bandit Convex Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06535
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#30423;&#20984;&#20248;&#21270;&#26159;&#30740;&#31350;&#38646;&#38454;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35768;&#22810;&#24037;&#20855;&#65292;&#21253;&#25324;&#20999;&#24179;&#38754;&#26041;&#27861;&#12289;&#20869;&#28857;&#26041;&#27861;&#12289;&#36830;&#32493;&#25351;&#25968;&#26435;&#37325;&#12289;&#26799;&#24230;&#19979;&#38477;&#21644;&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;&#12290;&#35299;&#37322;&#20102;&#35768;&#22810;&#20551;&#35774;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#23613;&#31649;&#22312;&#36825;&#37324;&#27809;&#26377;&#22826;&#22810;&#30495;&#27491;&#26032;&#30340;&#19996;&#35199;&#65292;&#20294;&#19968;&#20123;&#29616;&#26377;&#24037;&#20855;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#33719;&#24471;&#26032;&#31639;&#27861;&#12290;&#19968;&#20123;&#30028;&#38480;&#31245;&#24494;&#25913;&#36827;&#20102;&#19968;&#20123;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandit convex optimisation is a fundamental framework for studying zeroth-order convex optimisation. These notes cover the many tools used for this problem, including cutting plane methods, interior point methods, continuous exponential weights, gradient descent and online Newton step. The nuances between the many assumptions and setups are explained. Although there is not much truly new here, some existing tools are applied in novel ways to obtain new algorithms. A few bounds are improved in minor ways.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#30456;&#20114;&#28608;&#21169;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;me-GCN&#65289;&#65292;&#29992;&#20110;&#25163;&#23545;&#25163;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#35782;&#21035;&#12290;&#36890;&#36807;&#22534;&#21472;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#23618;&#65288;me-GC&#65289;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#25104;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#30456;&#32422;&#26463;&#65292;&#24182;&#25552;&#21462;&#21644;&#21512;&#24182;&#28145;&#24230;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.02431</link><description>&lt;p&gt;
&#23398;&#20064;&#30456;&#20114;&#28608;&#21169;&#20197;&#23454;&#29616;&#25163;&#23545;&#25163;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning Mutual Excitation for Hand-to-Hand and Human-to-Human Interaction Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#30456;&#20114;&#28608;&#21169;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;me-GCN&#65289;&#65292;&#29992;&#20110;&#25163;&#23545;&#25163;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#35782;&#21035;&#12290;&#36890;&#36807;&#22534;&#21472;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#23618;&#65288;me-GC&#65289;&#65292;&#35813;&#32593;&#32476;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#24314;&#27169;&#25104;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#30456;&#32422;&#26463;&#65292;&#24182;&#25552;&#21462;&#21644;&#21512;&#24182;&#28145;&#24230;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#20132;&#20114;&#21160;&#20316;&#65292;&#21253;&#25324;&#25163;&#23545;&#25163;&#20132;&#20114;&#21644;&#20154;&#23545;&#20154;&#20132;&#20114;&#65292;&#22312;&#35270;&#39057;&#20998;&#26512;&#21644;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#32771;&#34385;&#21040;&#22270;&#21367;&#31215;&#22312;&#24314;&#27169;&#39592;&#39612;&#25968;&#25454;&#30340;&#25299;&#25169;&#24863;&#30693;&#29305;&#24449;&#26041;&#38754;&#30340;&#25104;&#21151;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#24120;&#23558;&#22270;&#21367;&#31215;&#24212;&#29992;&#20110;&#29420;&#31435;&#23454;&#20307;&#65292;&#24182;&#22312;&#20132;&#20114;&#21160;&#20316;&#35782;&#21035;&#26102;&#20351;&#29992;&#21518;&#26399;&#34701;&#21512;&#65292;&#36825;&#20960;&#20046;&#26080;&#27861;&#24314;&#27169;&#25104;&#23545;&#23454;&#20307;&#20043;&#38388;&#30340;&#20114;&#30456;&#35821;&#20041;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22534;&#21472;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#65288;me-GC&#65289;&#23618;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#30456;&#20114;&#28608;&#21169;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;me-GCN&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;me-GC&#20351;&#29992;&#30456;&#20114;&#25299;&#25169;&#28608;&#21169;&#27169;&#22359;&#39318;&#20808;&#20174;&#21333;&#20010;&#23454;&#20307;&#20013;&#25552;&#21462;&#37051;&#25509;&#30697;&#38453;&#65292;&#28982;&#21518;&#33258;&#36866;&#24212;&#22320;&#23545;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#32422;&#26463;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;me-GC&#36827;&#19968;&#27493;&#20351;&#29992;&#30456;&#20114;&#29305;&#24449;&#28608;&#21169;&#27169;&#22359;&#20174;&#25104;&#23545;&#23454;&#20307;&#20013;&#25552;&#21462;&#21644;&#21512;&#24182;&#28145;&#24230;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recognizing interactive actions, including hand-to-hand interaction and human-to-human interaction, has attracted increasing attention for various applications in the field of video analysis and human-robot interaction. Considering the success of graph convolution in modeling topology-aware features from skeleton data, recent methods commonly operate graph convolution on separate entities and use late fusion for interactive action recognition, which can barely model the mutual semantic relationships between pairwise entities. To this end, we propose a mutual excitation graph convolutional network (me-GCN) by stacking mutual excitation graph convolution (me-GC) layers. Specifically, me-GC uses a mutual topology excitation module to firstly extract adjacency matrices from individual entities and then adaptively model the mutual constraints between them. Moreover, me-GC extends the above idea and further uses a mutual feature excitation module to extract and merge deep features from pairw
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2210.01708</link><description>&lt;p&gt;
&#20811;&#26381;&#36890;&#20449;&#32422;&#26463;&#65292;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#20013;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Conquering the Communication Constraints to Enable Large Pre-Trained Models in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.01708
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20811;&#26381;&#32852;&#37030;&#23398;&#20064;&#20013;&#36890;&#20449;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;FL&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26088;&#22312;&#22312;&#26412;&#22320;&#35774;&#22791;&#19978;&#21327;&#21147;&#35757;&#32451;&#27169;&#22411;&#32780;&#19981;&#38656;&#35201;&#23545;&#21407;&#22987;&#25968;&#25454;&#36827;&#34892;&#20013;&#24515;&#21270;&#35775;&#38382;&#30340;&#26377;&#21069;&#26223;&#30340;&#33539;&#24335;&#12290;&#22312;&#20856;&#22411;&#30340;FL&#33539;&#24335;&#65288;&#20363;&#22914;FedAvg&#65289;&#20013;&#65292;&#27599;&#19968;&#36718;&#27169;&#22411;&#26435;&#37325;&#37117;&#20250;&#34987;&#21457;&#36865;&#21040;&#21442;&#19982;&#23458;&#25143;&#31471;&#24182;&#22238;&#20256;&#21040;&#26381;&#21153;&#22120;&#12290;&#26368;&#36817;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20248;&#21270;&#21644;&#25910;&#25947;&#25913;&#36827;&#26041;&#38754;&#23637;&#31034;&#20102;&#20351;&#29992;&#23567;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#39044;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#24378;&#22823;&#65292;&#20294;&#20063;&#25317;&#26377;&#26356;&#22810;&#21442;&#25968;&#12290;&#22312;&#20256;&#32479;&#30340;FL&#20013;&#65292;&#20849;&#20139;&#24040;&#22823;&#30340;&#27169;&#22411;&#26435;&#37325;&#21487;&#20197;&#36805;&#36895;&#32473;&#31995;&#32479;&#24102;&#26469;&#24040;&#22823;&#30340;&#36890;&#20449;&#36127;&#25285;&#65292;&#23588;&#20854;&#26159;&#22914;&#26524;&#37319;&#29992;&#26356;&#21152;&#24378;&#22823;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#33021;&#21542;&#25214;&#21040;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;FL&#20013;&#21551;&#29992;&#36825;&#20123;&#24378;&#22823;&#19988;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23454;&#29616;&#20986;&#33394;&#24615;&#33021;&#30340;&#21516;&#26102;&#20943;&#23569;&#36890;&#20449;&#36127;&#25285;&#65311;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.01708v3 Announce Type: replace  Abstract: Federated learning (FL) has emerged as a promising paradigm for enabling the collaborative training of models without centralized access to the raw data on local devices. In the typical FL paradigm (e.g., FedAvg), model weights are sent to and from the server each round to participating clients. Recently, the use of small pre-trained models has been shown effective in federated learning optimization and improving convergence. However, recent state-of-the-art pre-trained models are getting more capable but also have more parameters. In conventional FL, sharing the enormous model weights can quickly put a massive communication burden on the system, especially if more capable models are employed. Can we find a solution to enable those strong and readily-available pre-trained models in FL to achieve excellent performance while simultaneously reducing the communication burden? To this end, we investigate the use of parameter-efficient fin
&lt;/p&gt;</description></item><item><title>ProCNS&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#21407;&#21017;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.14074</link><description>&lt;p&gt;
ProCNS: &#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;
&lt;/p&gt;
&lt;p&gt;
ProCNS: Progressive Prototype Calibration and Noise Suppression for Weakly-Supervised Medical Image Segmentation. (arXiv:2401.14074v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14074
&lt;/p&gt;
&lt;p&gt;
ProCNS&#26159;&#19968;&#31181;&#29992;&#20110;&#24369;&#30417;&#30563;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#30340;&#21407;&#21017;&#26469;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#20998;&#21106;&#65288;WSS&#65289;&#20316;&#20026;&#32531;&#35299;&#27880;&#37322;&#25104;&#26412;&#21644;&#27169;&#22411;&#24615;&#33021;&#20043;&#38388;&#20914;&#31361;&#30340;&#35299;&#20915;&#26041;&#26696;&#32780;&#20986;&#29616;&#65292;&#37319;&#29992;&#31232;&#30095;&#30340;&#27880;&#37322;&#26684;&#24335;&#65288;&#20363;&#22914;&#28857;&#12289;&#28034;&#40486;&#12289;&#22359;&#31561;&#65289;&#12290;&#20856;&#22411;&#30340;&#26041;&#27861;&#35797;&#22270;&#21033;&#29992;&#35299;&#21078;&#21644;&#25299;&#25169;&#20808;&#39564;&#23558;&#31232;&#30095;&#27880;&#37322;&#30452;&#25509;&#25193;&#23637;&#20026;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23545;&#21307;&#23398;&#22270;&#20687;&#20013;&#27169;&#31946;&#36793;&#32536;&#30340;&#20851;&#27880;&#19981;&#36275;&#21644;&#23545;&#31232;&#30095;&#30417;&#30563;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#20250;&#22312;&#22122;&#22768;&#21306;&#22495;&#29983;&#25104;&#38169;&#35823;&#19988;&#36807;&#20110;&#33258;&#20449;&#30340;&#20266;&#24314;&#35758;&#65292;&#23548;&#33268;&#27169;&#22411;&#35823;&#24046;&#32047;&#31215;&#21644;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;WSS&#26041;&#27861;&#65292;&#21517;&#20026;ProCNS&#65292;&#23427;&#21253;&#21547;&#20004;&#20010;&#21327;&#21516;&#27169;&#22359;&#65292;&#35774;&#35745;&#21407;&#21017;&#26159;&#28176;&#36827;&#24335;&#21407;&#22411;&#26657;&#20934;&#21644;&#22122;&#22768;&#25233;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#21407;&#22411;&#30340;&#21306;&#22495;&#31354;&#38388;&#30456;&#20284;&#24615;&#65288;PRSA&#65289;&#25439;&#22833;&#20989;&#25968;&#65292;&#26368;&#22823;&#21270;&#31354;&#38388;&#21644;&#35821;&#20041;&#20803;&#32032;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20284;&#24230;&#65292;&#20026;&#25105;&#20204;&#24863;&#20852;&#36259;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
Weakly-supervised segmentation (WSS) has emerged as a solution to mitigate the conflict between annotation cost and model performance by adopting sparse annotation formats (e.g., point, scribble, block, etc.). Typical approaches attempt to exploit anatomy and topology priors to directly expand sparse annotations into pseudo-labels. However, due to a lack of attention to the ambiguous edges in medical images and insufficient exploration of sparse supervision, existing approaches tend to generate erroneous and overconfident pseudo proposals in noisy regions, leading to cumulative model error and performance degradation. In this work, we propose a novel WSS approach, named ProCNS, encompassing two synergistic modules devised with the principles of progressive prototype calibration and noise suppression. Specifically, we design a Prototype-based Regional Spatial Affinity (PRSA) loss to maximize the pair-wise affinities between spatial and semantic elements, providing our model of interest 
&lt;/p&gt;</description></item><item><title>AgentBoard&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;&#20998;&#26512;&#35780;&#20272;LLM&#26234;&#33021;&#20307;&#32780;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#36718;&#20132;&#20114;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#12290;</title><link>http://arxiv.org/abs/2401.13178</link><description>&lt;p&gt;
AgentBoard: &#19968;&#31181;&#22810;&#36718;LLM&#26234;&#33021;&#20307;&#30340;&#20998;&#26512;&#35780;&#20272;&#26495;
&lt;/p&gt;
&lt;p&gt;
AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents. (arXiv:2401.13178v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13178
&lt;/p&gt;
&lt;p&gt;
AgentBoard&#26159;&#19968;&#20010;&#32508;&#21512;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;&#20998;&#26512;&#35780;&#20272;LLM&#26234;&#33021;&#20307;&#32780;&#35774;&#35745;&#65292;&#35299;&#20915;&#20102;&#22312;&#22810;&#36718;&#20132;&#20114;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#23545;&#26234;&#33021;&#20307;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#21644;&#35780;&#20272;&#24037;&#20855;&#21253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20316;&#20026;&#36890;&#29992;&#26234;&#33021;&#20307;&#23545;&#20110;&#29702;&#35299;&#20854;&#33021;&#21147;&#24182;&#20419;&#36827;&#20854;&#34701;&#20837;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#36807;&#31243;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#22312;&#32479;&#19968;&#26694;&#26550;&#20869;&#23545;&#26234;&#33021;&#20307;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#29305;&#21035;&#26159;&#22312;&#32500;&#25252;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#21644;&#30830;&#20445;&#22810;&#36718;&#20132;&#20114;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#35780;&#20272;&#26694;&#26550;&#20027;&#35201;&#20851;&#27880;&#26368;&#32456;&#25104;&#21151;&#29575;&#65292;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#35265;&#35299;&#24456;&#23569;&#65292;&#26080;&#27861;&#28145;&#20837;&#29702;&#35299;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;AgentBoard&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#32508;&#21512;&#22522;&#20934;&#21644;&#20276;&#38543;&#30340;&#24320;&#28304;&#35780;&#20272;&#26694;&#26550;&#65292;&#19987;&#20026;LLM&#26234;&#33021;&#20307;&#30340;&#20998;&#26512;&#35780;&#20272;&#32780;&#35774;&#35745;&#12290;AgentBoard&#25552;&#20379;&#20102;&#19968;&#31181;&#32454;&#31890;&#24230;&#30340;&#36827;&#23637;&#29575;&#25351;&#26631;&#65292;&#25429;&#25417;&#36880;&#27493;&#30340;&#36827;&#23637;&#65292;&#20197;&#21450;&#19968;&#20010;&#32508;&#21512;&#30340;&#35780;&#20272;&#24037;&#20855;&#21253;&#65292;&#20855;&#26377;&#26131;&#20110;&#35780;&#20272;&#21644;&#20998;&#26512;&#27169;&#22411;&#33021;&#21147;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assess
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20013;&#22686;&#24378;&#26041;&#27861;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20027;&#35201;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2310.03977</link><description>&lt;p&gt;
&#23436;&#32654;&#23545;&#40784;&#21487;&#33021;&#23545;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Perfect Alignment May be Poisonous to Graph Contrastive Learning. (arXiv:2310.03977v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20013;&#22686;&#24378;&#26041;&#27861;&#21644;&#19979;&#28216;&#24615;&#33021;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20027;&#35201;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#30340;&#33410;&#28857;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23545;&#40784;&#27491;&#26679;&#26412;&#21644;&#20998;&#31163;&#36127;&#26679;&#26412;&#26469;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#22522;&#20110;&#22270;&#24418;&#30340;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#29305;&#23450;&#22686;&#24378;&#26041;&#27861;&#32972;&#21518;&#30340;&#20869;&#22312;&#35268;&#24459;&#30340;&#30740;&#31350;&#26377;&#38480;&#12290;&#20160;&#20040;&#26679;&#30340;&#22686;&#24378;&#26041;&#27861;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#24615;&#33021;&#65311;&#23545;&#27604;&#23398;&#20064;&#22914;&#20309;&#23454;&#38469;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#65311;&#20026;&#20160;&#20040;&#22686;&#24378;&#30340;&#24133;&#24230;&#24456;&#37325;&#35201;&#65311;&#26412;&#25991;&#35797;&#22270;&#36890;&#36807;&#24314;&#31435;&#22686;&#24378;&#26041;&#27861;&#21644;&#19979;&#28216;&#24615;&#33021;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20197;&#21450;&#23545;&#23545;&#27604;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#36827;&#34892;&#30740;&#31350;&#26469;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#22270;&#24418;&#23545;&#27604;&#23398;&#20064;&#20027;&#35201;&#36890;&#36807;&#20998;&#31163;&#19981;&#21516;&#31867;&#21035;&#32780;&#19981;&#26159;&#32858;&#38598;&#21516;&#19968;&#31867;&#21035;&#30340;&#33410;&#28857;&#26469;&#20026;&#19979;&#28216;&#20219;&#21153;&#20570;&#20986;&#36129;&#29486;&#12290;&#22240;&#27492;&#65292;&#26080;&#27861;&#35299;&#37322;&#23545;&#27604;&#23398;&#20064;&#30340;&#25104;&#21151;&#65292;&#21363;&#20840;&#37096;&#26679;&#26412;&#23436;&#32654;&#23545;&#40784;&#21644;&#22686;&#24378;&#37325;&#21472;&#12290;&#20026;&#20102;&#29702;&#35299;&#22686;&#24378;&#22914;&#20309;&#36741;&#21161;&#23545;&#27604;&#23398;&#20064;&#36807;&#31243;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Contrastive Learning (GCL) aims to learn node representations by aligning positive pairs and separating negative ones. However, limited research has been conducted on the inner law behind specific augmentations used in graph-based learning. What kind of augmentation will help downstream performance, how does contrastive learning actually influence downstream tasks, and why the magnitude of augmentation matters? This paper seeks to address these questions by establishing a connection between augmentation and downstream performance, as well as by investigating the generalization of contrastive learning. Our findings reveal that GCL contributes to downstream tasks mainly by separating different classes rather than gathering nodes of the same class. So perfect alignment and augmentation overlap which draw all intra-class samples the same can not explain the success of contrastive learning. Then in order to comprehend how augmentation aids the contrastive learning process, we conduct 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.16534</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Conditioning Score-Based Generative Models by Neuro-Symbolic Constraints. (arXiv:2308.16534v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#31526;&#21495;&#32422;&#26463;&#26469;&#35843;&#33410;&#22522;&#20110;&#35780;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#38750;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#19979;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#12289;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35780;&#20998;&#21644;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#26465;&#20214;&#21644;&#38750;&#26465;&#20214;&#29983;&#25104;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26465;&#20214;&#29983;&#25104;&#22522;&#20110;&#29305;&#23450;&#35757;&#32451;&#30340;&#26465;&#20214;&#27169;&#22411;&#25110;&#20998;&#31867;&#22120;&#25351;&#23548;&#65292;&#36825;&#38656;&#35201;&#35757;&#32451;&#19968;&#20010;&#22122;&#22768;&#20381;&#36182;&#30340;&#20998;&#31867;&#22120;&#65292;&#21363;&#20351;&#23545;&#20110;&#26410;&#25439;&#22351;&#25968;&#25454;&#30340;&#20998;&#31867;&#22120;&#24050;&#32463;&#32473;&#20986;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#38750;&#26465;&#20214;&#35780;&#20998;&#29983;&#25104;&#27169;&#22411;&#20013;&#37319;&#26679;&#65292;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#20219;&#24847;&#30340;&#36923;&#36753;&#32422;&#26463;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25805;&#32437;&#23398;&#20064;&#24471;&#21040;&#30340;&#35780;&#20998;&#65292;&#20197;&#20415;&#22312;&#29992;&#25143;&#23450;&#20041;&#30340;&#32422;&#26463;&#26465;&#20214;&#19979;&#20174;&#38750;&#24402;&#19968;&#21270;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#28789;&#27963;&#32780;&#25968;&#20540;&#31283;&#23450;&#30340;&#31070;&#32463;&#31526;&#21495;&#26694;&#26550;&#65292;&#29992;&#20110;&#32534;&#30721;&#36719;&#36923;&#36753;&#32422;&#26463;&#12290;&#23558;&#36825;&#20004;&#20010;&#32452;&#25104;&#37096;&#20998;&#32467;&#21512;&#36215;&#26469;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#20294;&#26159;&#36817;&#20284;&#30340;&#26465;&#20214;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25913;&#36827;&#36817;&#20284;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based and diffusion models have emerged as effective approaches for both conditional and unconditional generation. Still conditional generation is based on either a specific training of a conditional model or classifier guidance, which requires training a noise-dependent classifier, even when the classifier for uncorrupted data is given. We propose an approach to sample from unconditional score-based generative models enforcing arbitrary logical constraints, without any additional training. Firstly, we show how to manipulate the learned score in order to sample from an un-normalized distribution conditional on a user-defined constraint. Then, we define a flexible and numerically stable neuro-symbolic framework for encoding soft logical constraints. Combining these two ingredients we obtain a general, but approximate, conditional sampling algorithm. We further developed effective heuristics aimed at improving the approximation. Finally, we show the effectiveness of our approach fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#21487;&#20197;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.07189</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25104;&#20687;&#20013;&#20248;&#21270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimizing Convolutional Neural Networks for Chronic Obstructive Pulmonary Disease Detection in Clinical Computed Tomography Imaging. (arXiv:2303.07189v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#20020;&#24202;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#23454;&#29616;&#30340;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#21487;&#20197;&#25913;&#21892;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#36890;&#36807;&#25506;&#32034;&#25163;&#21160;&#35843;&#25972;&#21644;&#33258;&#21160;&#21270;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#65292;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22312;&#32954;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#20013;&#26816;&#27979;&#24930;&#24615;&#38459;&#22622;&#24615;&#32954;&#30142;&#30149;&#65288;COPD&#65289;&#30340;&#23384;&#22312;&#65292;&#26469;&#20248;&#21270;&#20108;&#36827;&#21046;COPD&#30340;&#26816;&#27979;&#12290;&#26041;&#27861;&#65306;&#22238;&#39038;&#24615;&#36873;&#25321;&#20102;78&#21517;&#21463;&#35797;&#32773;&#65288;43&#21517;COPD&#24739;&#32773;&#65307;35&#21517;&#20581;&#24247;&#23545;&#29031;&#32452;&#65289;&#30340;7,194&#20010;CT&#22270;&#20687;&#65288;3,597&#20010;COPD&#65307;3,597&#20010;&#20581;&#24247;&#23545;&#29031;&#32452;&#65289;&#65288;2018&#24180;10&#26376;&#33267;2019&#24180;12&#26376;&#65289;&#12290;&#23545;&#27599;&#20010;&#22270;&#20687;&#65292;&#23558;&#24378;&#24230;&#20540;&#25163;&#21160;&#35009;&#21098;&#21040;&#32954;&#27668;&#32959;&#31383;&#21475;&#35774;&#32622;&#21644;&#22522;&#20934;&#30340;&#8220;&#20840;&#33539;&#22260;&#8221;&#31383;&#21475;&#35774;&#32622;&#12290;&#31867;&#24179;&#34913;&#30340;&#35757;&#32451;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#38598;&#21253;&#21547;&#20102;3,392&#12289;1,114&#21644;2,688&#20010;&#22270;&#20687;&#12290;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#30340;CNN&#26550;&#26500;&#26469;&#20248;&#21270;&#32593;&#32476;&#20027;&#24178;&#12290;&#27492;&#22806;&#65292;&#36824;&#36890;&#36807;&#21521;&#27169;&#22411;&#28155;&#21152;&#33258;&#23450;&#20041;&#23618;&#26469;&#23454;&#29616;&#33258;&#21160;&#21270;&#30340;&#31383;&#21475;&#35774;&#32622;&#20248;&#21270;&#12290;&#26681;&#25454;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#65288;ROC&#65289;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#30340;&#22270;&#20687;&#27700;&#24179;&#65292;&#35745;&#31639;&#20986;P&#20540;&#26469;&#35780;&#20272;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Purpose: To optimize the binary detection of Chronic Obstructive Pulmonary Disease (COPD) based on emphysema presence in the lung with convolutional neural networks (CNN) by exploring manually adjusted versus automated window-setting optimization (WSO) on computed tomography (CT) images.  Methods: 7,194 CT images (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with COPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and preprocessed. For each image, intensity values were manually clipped to the emphysema window setting and a baseline 'full-range' window setting. Class-balanced train, validation, and test sets contained 3,392, 1,114, and 2,688 images. The network backbone was optimized by comparing various CNN architectures. Furthermore, automated WSO was implemented by adding a customized layer to the model. The image-level area under the Receiver Operating Characteristics curve (AUC) [lower, upper limit 95% confidence] and P-values calculated from
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#65292;&#21457;&#29616;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#20248;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#20316;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#30340;&#21478;&#19968;&#20010;&#36873;&#25321;&#12290;</title><link>http://arxiv.org/abs/2211.00181</link><description>&lt;p&gt;
&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Numerical Stability of Hyperbolic Representation Learning. (arXiv:2211.00181v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00181
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36229;&#20960;&#20309;&#34920;&#24449;&#23398;&#20064;&#20013;&#30340;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#27969;&#34892;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#65292;&#21457;&#29616;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#20248;&#21270;&#24615;&#33021;&#65292;&#21516;&#26102;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#20316;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#30340;&#21478;&#19968;&#20010;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36229;&#29699;&#30340;&#23481;&#37327;&#38543;&#21322;&#24452;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#36229;&#20960;&#20309;&#31354;&#38388;&#33021;&#22815;&#23558;&#20855;&#26377;&#23618;&#27425;&#32467;&#26500;&#30340;&#25968;&#25454;&#38598;&#23884;&#20837;&#20854;&#20013;&#32780;&#19981;&#22833;&#30495;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25351;&#25968;&#22686;&#38271;&#30340;&#24615;&#36136;&#24120;&#24120;&#23548;&#33268;&#25968;&#20540;&#19981;&#31283;&#23450;&#24615;&#65292;&#20351;&#24471;&#35757;&#32451;&#36229;&#20960;&#20309;&#23398;&#20064;&#27169;&#22411;&#26377;&#26102;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#30340;NaN&#38382;&#39064;&#21644;&#28014;&#28857;&#31639;&#26415;&#20013;&#36935;&#21040;&#26080;&#27861;&#34920;&#31034;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#20004;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36229;&#20960;&#20309;&#27169;&#22411;&#8212;&#8212;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20102;&#20180;&#32454;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#65292;&#22312;64&#20301;&#31639;&#26415;&#31995;&#32479;&#19979;&#65292;Poincar\'e&#29699;&#30456;&#23545;&#20110;Lorentz&#27169;&#22411;&#20855;&#26377;&#26356;&#22823;&#30340;&#33021;&#21147;&#26469;&#27491;&#30830;&#34920;&#31034;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#29702;&#35770;&#19978;&#39564;&#35777;&#20102;Lorentz&#27169;&#22411;&#20248;&#20110;Poincar\'e&#29699;&#30340;&#20248;&#36234;&#24615;&#12290;&#37492;&#20110;&#20004;&#31181;&#27169;&#22411;&#30340;&#25968;&#20540;&#38480;&#21046;&#65292;&#25105;&#20204;&#30830;&#23450;&#19968;&#31181;&#27431;&#20960;&#37324;&#24471;&#20248;&#21270;&#26041;&#26696;&#65292;&#22312;Poincar\'e&#29699;&#21644;Lorentz&#27169;&#22411;&#20043;&#22806;&#20026;&#36229;&#20960;&#20309;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the exponential growth of the volume of the ball w.r.t. its radius, the hyperbolic space is capable of embedding trees with arbitrarily small distortion and hence has received wide attention for representing hierarchical datasets. However, this exponential growth property comes at a price of numerical instability such that training hyperbolic learning models will sometimes lead to catastrophic NaN problems, encountering unrepresentable values in floating point arithmetic. In this work, we carefully analyze the limitation of two popular models for the hyperbolic space, namely, the Poincar\'e ball and the Lorentz model. We first show that, under the 64 bit arithmetic system, the Poincar\'e ball has a relatively larger capacity than the Lorentz model for correctly representing points. Then, we theoretically validate the superiority of the Lorentz model over the Poincar\'e ball from the perspective of optimization. Given the numerical limitations of both models, we identify one Eucli
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;RealAEs&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;Android&#39046;&#22495;&#32422;&#26463;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2205.15128</link><description>&lt;p&gt;
&#21033;&#29992;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#39046;&#22495;&#32422;&#26463;&#20197;&#22686;&#24378;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;&#65306;&#36890;&#36807;RealAEs&#21319;&#32423;
&lt;/p&gt;
&lt;p&gt;
Level Up with RealAEs: Leveraging Domain Constraints in Feature Space to Strengthen Robustness of Android Malware Detection. (arXiv:2205.15128v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;RealAEs&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#35299;&#37322;Android&#39046;&#22495;&#32422;&#26463;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#26469;&#23454;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#39640;&#20102;&#26816;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#38754;&#20020;&#30340;&#23545;&#25239;&#31034;&#20363;&#23481;&#26131;&#21463;&#25915;&#20987;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#8212;&#8212;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#29983;&#25104;&#39046;&#22495;&#32422;&#26463;&#19979;&#30340;&#21487;&#34892;&#23545;&#25239;&#26679;&#26412;(RealAEs)&#12290; &#22312;&#29616;&#23454;&#25915;&#20987;&#19979;&#65292;RealAEs&#27604;&#19981;&#21487;&#34892;&#30340;&#23545;&#25239;&#26679;&#26412;&#26356;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35299;Android&#39046;&#22495;&#32422;&#26463;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#39318;&#20808;&#23398;&#20064;&#29305;&#24449;&#65292;&#24182;&#23558;&#39046;&#22495;&#32422;&#26463;&#35299;&#37322;&#20026;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#12290; &#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#38477;&#20302;&#26816;&#27979;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability to adversarial examples remains one major obstacle for Machine Learning (ML)-based Android malware detection. Realistic attacks in the Android malware domain create Realizable Adversarial Examples (RealAEs), i.e., AEs that satisfy the domain constraints of Android malware. Recent studies have shown that using such RealAEs in Adversarial Training (AT) is more effective in defending against realistic attacks than using unrealizable AEs (unRealAEs). This is because RealAEs allow defenders to explore certain pockets in the feature space that are vulnerable to realistic attacks. However, existing defenses commonly generate RealAEs in the problem space, which is known to be time-consuming and impractical for AT. In this paper, we propose to generate RealAEs in the feature space, leading to a simpler and more efficient solution. Our approach is driven by a novel interpretation of Android domain constraints in the feature space. More concretely, our defense first learns featu
&lt;/p&gt;</description></item></channel></rss>