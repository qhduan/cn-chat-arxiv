<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>EmSHAP&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;GRU&#26469;&#28040;&#38500;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;Shapley&#20540;&#36129;&#29486;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.01078</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#20934;&#30830;Shapley&#20540;&#20272;&#35745;&#29992;&#20110;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01078
&lt;/p&gt;
&lt;p&gt;
EmSHAP&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;GRU&#26469;&#28040;&#38500;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;Shapley&#20540;&#36129;&#29486;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26377;&#21033;&#24037;&#20855;&#65292;Shapley&#20540;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#36127;&#36733;&#38543;&#30528;&#36755;&#20837;&#29305;&#24449;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#20272;&#35745;Shapley&#20540;&#26159;&#19968;&#39033;&#22256;&#38590;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21152;&#36895;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#24517;&#39035;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EmSHAP&#65288;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#65289;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#39044;&#26399;Shapley&#36129;&#29486;&#20989;&#25968;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#32473;&#20986;&#20854;&#20313;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#30830;&#23450;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#25552;&#35758;&#26465;&#20214;&#20998;&#24067;&#65292;&#24341;&#20837;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#21040;&#38544;&#34255;&#31354;&#38388;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#21160;&#24577;&#25513;&#34109;&#26041;&#26696;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01078v1 Announce Type: new  Abstract: As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models. However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features. Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency. In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest. In order to determine the proposal conditional distribution in the energy model, a gated recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated. In addition, a dynamic masking scheme is 
&lt;/p&gt;</description></item><item><title>I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.06069</link><description>&lt;p&gt;
&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schrodinger&#26725;&#29992;&#20110;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;
&lt;/p&gt;
&lt;p&gt;
Implicit Image-to-Image Schrodinger Bridge for CT Super-Resolution and Denoising
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06069
&lt;/p&gt;
&lt;p&gt;
I3SB&#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65292;&#32467;&#21512;&#25439;&#22351;&#30340;&#22270;&#20687;&#25913;&#21892;&#32441;&#29702;&#24674;&#22797;&#65292;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#22312;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#32780;&#24471;&#21040;&#35748;&#21487;&#65292;&#28982;&#32780;&#65292;&#20854;&#20174;&#39640;&#26031;&#22122;&#22768;&#24320;&#22987;&#30340;&#36845;&#20195;&#21435;&#22122;&#36807;&#31243;&#24448;&#24448;&#23548;&#33268;&#25512;&#26029;&#36895;&#24230;&#24930;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I2SB&#65289;&#20174;&#25439;&#22351;&#30340;&#22270;&#20687;&#24320;&#22987;&#21021;&#22987;&#21270;&#29983;&#25104;&#36807;&#31243;&#65292;&#24182;&#38598;&#25104;&#20102;&#26377;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#35757;&#32451;&#25216;&#26415;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#38544;&#24335;&#22270;&#20687;&#23545;&#22270;&#20687;Schr&#246;dinger&#26725;&#65288;I3SB&#65289;&#25193;&#23637;&#20102;I2SB&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27599;&#19968;&#29983;&#25104;&#27493;&#39588;&#20013;&#32435;&#20837;&#25439;&#22351;&#30340;&#22270;&#20687;&#65292;&#23558;&#20854;&#29983;&#25104;&#36807;&#31243;&#36716;&#25442;&#20026;&#38750;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#12290;&#36825;&#31181;&#22686;&#24378;&#20351;&#24471;I3SB&#33021;&#22815;&#22312;&#23569;&#37327;&#29983;&#25104;&#27493;&#39588;&#20013;&#29983;&#25104;&#20855;&#26377;&#26356;&#22909;&#32441;&#29702;&#24674;&#22797;&#30340;&#22270;&#20687;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;CT&#36229;&#20998;&#36776;&#29575;&#21644;&#21435;&#22122;&#20219;&#21153;&#19978;&#24471;&#21040;&#39564;&#35777;&#65292;&#24182;&#36229;&#36234;&#20102;&#21253;&#25324;&#26377;&#26465;&#20214;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#20869;&#30340;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06069v1 Announce Type: cross  Abstract: Conditional diffusion models have gained recognition for their effectiveness in image restoration tasks, yet their iterative denoising process, starting from Gaussian noise, often leads to slow inference speeds. As a promising alternative, the Image-to-Image Schr\"odinger Bridge (I2SB) initializes the generative process from corrupted images and integrates training techniques from conditional diffusion models. In this study, we extended the I2SB method by introducing the Implicit Image-to-Image Schrodinger Bridge (I3SB), transitioning its generative process to a non-Markovian process by incorporating corrupted images in each generative step. This enhancement empowers I3SB to generate images with better texture restoration using a small number of generative steps. The proposed method was validated on CT super-resolution and denoising tasks and outperformed existing methods, including the conditional denoising diffusion probabilistic mod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36164;&#28304;&#32422;&#26463;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36830;&#32493;&#20943;&#21322;&#31639;&#27861;&#65288;SH-RR&#65289;&#65292;&#22312;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#20197;&#25509;&#36817;&#26368;&#20248;&#36895;&#24230;&#25104;&#21151;&#35782;&#21035;&#26368;&#20339;&#33218;&#65292;&#24182;&#21457;&#29616;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#36164;&#28304;&#28040;&#32791;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.19090</link><description>&lt;p&gt;
&#20855;&#26377;&#36164;&#28304;&#32422;&#26463;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification with Resource Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19090
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#36164;&#28304;&#32422;&#26463;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#36830;&#32493;&#20943;&#21322;&#31639;&#27861;&#65288;SH-RR&#65289;&#65292;&#22312;&#38750;&#28176;&#36817;&#24773;&#20917;&#19979;&#20197;&#25509;&#36817;&#26368;&#20248;&#36895;&#24230;&#25104;&#21151;&#35782;&#21035;&#26368;&#20339;&#33218;&#65292;&#24182;&#21457;&#29616;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#36164;&#28304;&#28040;&#32791;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#19981;&#21516;&#26367;&#20195;&#26041;&#26696;&#23454;&#39564;&#25104;&#26412;&#24322;&#36136;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#36164;&#28304;&#32422;&#26463;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAIwRC&#65289;&#38382;&#39064;&#12290;&#20195;&#29702;&#21830;&#26088;&#22312;&#22312;&#36164;&#28304;&#32422;&#26463;&#19979;&#35782;&#21035;&#26368;&#20339;&#33218;&#65292;&#20854;&#20013;&#36164;&#28304;&#34987;&#27599;&#27425;&#25289;&#21160;&#25163;&#33218;&#28040;&#32791;&#12290;&#25105;&#20204;&#20570;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#35774;&#35745;&#24182;&#20998;&#26512;&#20102;&#20855;&#26377;&#36164;&#28304;&#27604;&#20363;&#30340;&#36830;&#32493;&#20943;&#21322;&#31639;&#27861;&#65288;SH-RR&#65289;&#12290;SH-RR&#20197;&#36817;&#20046;&#26368;&#20248;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#24230;&#23454;&#29616;&#20102;&#25104;&#21151;&#35782;&#21035;&#26368;&#20339;&#33218;&#30340;&#27010;&#29575;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#36164;&#28304;&#28040;&#32791;&#24773;&#20917;&#19979;&#25910;&#25947;&#36895;&#24230;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19090v1 Announce Type: new  Abstract: Motivated by the cost heterogeneity in experimentation across different alternatives, we study the Best Arm Identification with Resource Constraints (BAIwRC) problem. The agent aims to identify the best arm under resource constraints, where resources are consumed for each arm pull. We make two novel contributions. We design and analyze the Successive Halving with Resource Rationing algorithm (SH-RR). The SH-RR achieves a near-optimal non-asymptotic rate of convergence in terms of the probability of successively identifying an optimal arm. Interestingly, we identify a difference in convergence rates between the cases of deterministic and stochastic resource consumption.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550; - &#22810;&#32423;&#20248;&#21270;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MLO-MAE&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#21453;&#39304;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26368;&#20339;&#30340;&#36974;&#32617;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2402.18128</link><description>&lt;p&gt;
&#22312;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#20351;&#29992;&#22810;&#32423;&#20248;&#21270;&#30340;&#19979;&#28216;&#20219;&#21153;&#24341;&#23548;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Downstream Task Guided Masking Learning in Masked Autoencoders Using Multi-Level Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18128
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550; - &#22810;&#32423;&#20248;&#21270;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MLO-MAE&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#21453;&#39304;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26368;&#20339;&#30340;&#36974;&#32617;&#31574;&#30053;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#26159;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#30340;&#19968;&#20010;&#26174;&#33879;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#38543;&#26426;&#36974;&#32617;&#22270;&#20687;&#34917;&#19969;&#65292;&#24182;&#20351;&#29992;&#26410;&#36974;&#32617;&#30340;&#34917;&#19969;&#37325;&#24314;&#36825;&#20123;&#36974;&#32617;&#34917;&#19969;&#12290; MAE&#30340;&#19968;&#20010;&#20851;&#38190;&#23616;&#38480;&#24615;&#22312;&#20110;&#20854;&#24573;&#35270;&#19981;&#21516;&#34917;&#19969;&#30340;&#20449;&#24687;&#37327;&#19981;&#21516;&#65292;&#22240;&#20026;&#23427;&#20250;&#32479;&#19968;&#36873;&#25321;&#35201;&#36974;&#32617;&#30340;&#34917;&#19969;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#19968;&#20123;&#26041;&#27861;&#25552;&#20986;&#22522;&#20110;&#34917;&#19969;&#20449;&#24687;&#37327;&#36827;&#34892;&#36974;&#32617;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#19981;&#32771;&#34385;&#19979;&#28216;&#20219;&#21153;&#30340;&#29305;&#23450;&#38656;&#27714;&#65292;&#21487;&#33021;&#23548;&#33268;&#36825;&#20123;&#20219;&#21153;&#30340;&#34920;&#31034;&#27425;&#20248;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22810;&#32423;&#20248;&#21270;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MLO-MAE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26469;&#33258;&#19979;&#28216;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#21453;&#39304;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#26368;&#20339;&#36974;&#32617;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;MLO-MAE&#22312;&#35270;&#35273;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#26174;&#33879;&#36827;&#23637;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18128v1 Announce Type: cross  Abstract: Masked Autoencoder (MAE) is a notable method for self-supervised pretraining in visual representation learning. It operates by randomly masking image patches and reconstructing these masked patches using the unmasked ones. A key limitation of MAE lies in its disregard for the varying informativeness of different patches, as it uniformly selects patches to mask. To overcome this, some approaches propose masking based on patch informativeness. However, these methods often do not consider the specific requirements of downstream tasks, potentially leading to suboptimal representations for these tasks. In response, we introduce the Multi-level Optimized Mask Autoencoder (MLO-MAE), a novel framework that leverages end-to-end feedback from downstream tasks to learn an optimal masking strategy during pretraining. Our experimental findings highlight MLO-MAE's significant advancements in visual representation learning. Compared to existing metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#22411;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#36890;&#36807;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#20005;&#26684;&#35777;&#26126;&#20102;&#36807;&#24230;&#24179;&#28369;&#19982;&#25193;&#25955;&#31639;&#23376;&#30340;&#36941;&#21382;&#24615;&#23494;&#20999;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2402.15326</link><description>&lt;p&gt;
&#20174;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#30340;&#35270;&#35282;&#29702;&#35299;&#25193;&#25955;&#22411;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Understanding Oversmoothing in Diffusion-Based GNNs From the Perspective of Operator Semigroup Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#22411;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#36890;&#36807;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#20005;&#26684;&#35777;&#26126;&#20102;&#36807;&#24230;&#24179;&#28369;&#19982;&#25193;&#25955;&#31639;&#23376;&#30340;&#36941;&#21382;&#24615;&#23494;&#20999;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#20943;&#36731;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#30340;&#35282;&#24230;&#23545;&#25193;&#25955;&#22411;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#36827;&#34892;&#20102;&#20840;&#26032;&#30740;&#31350;&#12290;&#19982;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#20998;&#26512;&#25110;&#31890;&#23376;&#31995;&#32479;&#30340;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#31639;&#23376;&#21322;&#32676;&#29702;&#35770;&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#36825;&#19968;&#29702;&#35770;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#20005;&#26684;&#35777;&#26126;&#36807;&#24230;&#24179;&#28369;&#19982;&#25193;&#25955;&#31639;&#23376;&#30340;&#36941;&#21382;&#24615;&#24687;&#24687;&#30456;&#20851;&#12290;&#36825;&#19968;&#21457;&#29616;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#28201;&#21644;&#30340;&#36941;&#21382;&#24615;&#30772;&#22351;&#26465;&#20214;&#65292;&#21253;&#25324;&#20808;&#21069;&#25552;&#20986;&#30340;&#21508;&#31181;&#29305;&#23450;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#26222;&#36941;&#21644;&#29702;&#35770;&#19978;&#22522;&#30784;&#30340;&#26041;&#27861;&#26469;&#20943;&#36731;&#25193;&#25955;&#22411;GNN&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#23545;&#25105;&#20204;&#29702;&#35770;&#30340;&#27010;&#29575;&#35299;&#37322;&#65292;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#24314;&#31435;&#20102;&#32852;&#31995;&#65292;&#25299;&#23485;&#20102;&#29702;&#35770;&#35270;&#37326;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#19968;&#36941;&#21382;&#24615;&#30772;&#22351;&#39033;&#26377;&#25928;&#22320;&#20943;&#23569;&#20102;&#20197;&#36842;&#21033;&#20811;&#38647;&#33021;&#37327;&#34913;&#37327;&#30340;&#36807;&#24230;&#24179;&#28369;&#65292;&#24182;&#36798;&#21040;&#20102;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15326v1 Announce Type: new  Abstract: This paper presents a novel study of the oversmoothing issue in diffusion-based Graph Neural Networks (GNNs). Diverging from extant approaches grounded in random walk analysis or particle systems, we approach this problem through operator semigroup theory. This theoretical framework allows us to rigorously prove that oversmoothing is intrinsically linked to the ergodicity of the diffusion operator. This finding further poses a general and mild ergodicity-breaking condition, encompassing the various specific solutions previously offered, thereby presenting a more universal and theoretically grounded approach to mitigating oversmoothing in diffusion-based GNNs. Additionally, we offer a probabilistic interpretation of our theory, forging a link with prior works and broadening the theoretical horizon. Our experimental results reveal that this ergodicity-breaking term effectively mitigates oversmoothing measured by Dirichlet energy, and simul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.12198</link><description>&lt;p&gt;
&#38646;-shot &#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#30446;&#26631;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Zero shot VLMs for hate meme detection: Are we there yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12198
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#38646;-shot&#20998;&#31867;&#22312;&#22788;&#29702;&#22797;&#26434;&#20219;&#21153;&#22914;&#24694;&#24847;&#27169;&#22240;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#22810;&#23186;&#20307;&#20869;&#23481;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#20854;&#20013;&#27169;&#22240;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#24418;&#24335;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#19968;&#20123;&#24694;&#24847;&#29992;&#25143;&#21033;&#29992;&#27169;&#22240;&#38024;&#23545;&#20010;&#20154;&#25110;&#26131;&#21463;&#25915;&#20987;&#30340;&#31038;&#21306;&#65292;&#22240;&#27492;&#26377;&#24517;&#35201;&#35782;&#21035;&#21644;&#35299;&#20915;&#27492;&#31867;&#24694;&#24847;&#27169;&#22240;&#12290;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#24320;&#21457;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26174;&#33879;&#23616;&#38480;&#24615;&#26159;&#38656;&#35201;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#38598;&#25165;&#33021;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#30028;&#35265;&#35777;&#20102;&#20960;&#31181;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#36825;&#20123;&#21487;&#35265;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#35832;&#22914;&#20167;&#24680;&#27169;&#22240;&#26816;&#27979;&#31561;&#22797;&#26434;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25552;&#31034;&#35774;&#32622;&#26469;&#19987;&#27880;&#20110;&#23545;&#24694;&#24847;/&#26377;&#23475;&#27169;&#22240;&#30340;&#38646;-shot &#20998;&#31867;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;o
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12198v1 Announce Type: new  Abstract: Multimedia content on social media is rapidly evolving, with memes gaining prominence as a distinctive form. Unfortunately, some malicious users exploit memes to target individuals or vulnerable communities, making it imperative to identify and address such instances of hateful memes. Extensive research has been conducted to address this issue by developing hate meme detection models. However, a notable limitation of traditional machine/deep learning models is the requirement for labeled datasets for accurate classification. Recently, the research community has witnessed the emergence of several visual language models that have exhibited outstanding performance across various tasks. In this study, we aim to investigate the efficacy of these visual language models in handling intricate tasks such as hate meme detection. We use various prompt settings to focus on zero-shot classification of hateful/harmful memes. Through our analysis, we o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#24378;&#21147;&#37325;&#26500;&#25915;&#20987;&#65292;&#21487;&#20197;&#37325;&#26500;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38450;&#24481;&#26426;&#21046;&#20013;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.09478</link><description>&lt;p&gt;
&#25968;&#25454;&#37325;&#26500;&#25915;&#20987;&#19982;&#38450;&#24481;&#65306;&#19968;&#20010;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Data Reconstruction Attacks and Defenses: A Systematic Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#24378;&#21147;&#37325;&#26500;&#25915;&#20987;&#65292;&#21487;&#20197;&#37325;&#26500;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#19988;&#23545;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#38450;&#24481;&#26426;&#21046;&#20013;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#25915;&#20987;&#21644;&#38450;&#24481;&#23545;&#20110;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26799;&#24230;&#21453;&#36716;&#25915;&#20987;&#30340;&#32463;&#39564;&#35266;&#23519;&#19978;&#65292;&#32570;&#20047;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#19988;&#26080;&#27861;&#21306;&#20998;&#38450;&#24481;&#26041;&#27861;&#30340;&#26377;&#29992;&#24615;&#19982;&#25915;&#20987;&#26041;&#27861;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32852;&#21512;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#24378;&#21147;&#37325;&#26500;&#25915;&#20987;&#12290;&#35813;&#25915;&#20987;&#21487;&#20197;&#37325;&#26500;&#20013;&#38388;&#29305;&#24449;&#65292;&#24182;&#19982;&#22823;&#37096;&#20998;&#20808;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#26356;&#22909;&#12290;&#22312;&#36825;&#31181;&#26356;&#24378;&#30340;&#25915;&#20987;&#19979;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#20004;&#26041;&#38754;&#20840;&#38754;&#35843;&#26597;&#20102;&#26368;&#24120;&#35265;&#30340;&#38450;&#24481;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#38450;&#24481;&#26426;&#21046;&#20013;&#65292;&#22914;&#26799;&#24230;&#21098;&#36753;&#12289;dropout&#12289;&#28155;&#21152;&#22122;&#38899;&#12289;&#23616;&#37096;&#32858;&#21512;&#31561;&#31561;&#65292;&#26799;&#24230;&#20462;&#21098;&#26159;&#23545;&#25239;&#26368;&#20808;&#36827;&#25915;&#20987;&#26368;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09478v1 Announce Type: cross  Abstract: Reconstruction attacks and defenses are essential in understanding the data leakage problem in machine learning. However, prior work has centered around empirical observations of gradient inversion attacks, lacks theoretical groundings, and was unable to disentangle the usefulness of defending methods versus the computational limitation of attacking methods. In this work, we propose a strong reconstruction attack in the setting of federated learning. The attack reconstructs intermediate features and nicely integrates with and outperforms most of the previous methods. On this stronger attack, we thoroughly investigate both theoretically and empirically the effect of the most common defense methods. Our findings suggest that among various defense mechanisms, such as gradient clipping, dropout, additive noise, local aggregation, etc., gradient pruning emerges as the most effective strategy to defend against state-of-the-art attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.07625</link><description>&lt;p&gt;
AutoMathText&#65306;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
AutoMathText: Autonomous Data Selection with Language Models for Mathematical Texts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07625
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#31574;&#30053;&#65292;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#23398;&#25991;&#26412;&#30340;&#33258;&#21160;&#35780;&#20272;&#21644;&#36873;&#25321;&#65292;&#24182;&#36890;&#36807;&#36830;&#32493;&#39044;&#35757;&#32451;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#20027;&#35201;&#21019;&#26032;&#21253;&#25324;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#39564;&#35777;&#22120;&#65292;&#21457;&#24067;&#20102;&#39640;&#36136;&#37327;&#30340;AutoMathText&#25968;&#25454;&#38598;&#65292;&#24182;&#23454;&#29616;&#20102;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36890;&#36807;&#25345;&#32493;&#30340;&#39044;&#35757;&#32451;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#23398;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#20027;&#25968;&#25454;&#36873;&#25321;&#12290;&#19982;&#20256;&#32479;&#30340;&#26377;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#30340;&#30417;&#30563;&#24494;&#35843;&#25110;&#35757;&#32451;&#36807;&#30340;&#20998;&#31867;&#22120;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20803;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38646;&#26679;&#26412;&#39564;&#35777;&#22120;&#65292;&#33258;&#20027;&#35780;&#20272;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#25968;&#23398;&#20869;&#23481;&#65292;&#24182;&#21457;&#24067;&#20102;&#32463;&#36807;&#31574;&#21010;&#30340;&#24320;&#28304;AutoMathText&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;200GB&#30340;&#25968;&#25454;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;AutoMathText&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#36830;&#32493;&#39044;&#35757;&#32451;&#65292;&#20351;&#24471;7B&#21442;&#25968;&#30340;Mistral&#35821;&#35328;&#27169;&#22411;&#22312;MATH&#25968;&#25454;&#38598;&#19978;&#30340;&#19979;&#28216;&#24615;&#33021;&#22823;&#24133;&#25552;&#21319;&#65292;&#32780;&#20196;&#29260;&#25968;&#37327;&#27604;&#20043;&#21069;&#30340;&#36830;&#32493;&#39044;&#35757;&#32451;&#24037;&#20316;&#20943;&#23569;&#20102;&#20960;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22522;&#20934;&#30340;&#39044;&#35757;&#32451;&#20196;&#29260;&#25928;&#29575;&#25552;&#39640;&#20102;2&#20493;&#65292;&#31361;&#26174;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#22686;&#24378;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
To improve language models' proficiency in mathematical reasoning via continual pretraining, we introduce a novel strategy that leverages base language models for autonomous data selection. Departing from conventional supervised fine-tuning or trained classifiers with human-annotated data, our approach utilizes meta-prompted language models as zero-shot verifiers to autonomously evaluate and select high-quality mathematical content, and we release the curated open-source AutoMathText dataset encompassing over 200GB of data. To demonstrate the efficacy of our method, we continuously pretrained a 7B-parameter Mistral language model on the AutoMathText dataset, achieving substantial improvements in downstream performance on the MATH dataset with a token amount reduced by orders of magnitude compared to previous continuous pretraining works. Our method showcases a 2 times increase in pretraining token efficiency compared to baselines, underscoring the potential of our approach in enhancing
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#37327;&#22238;&#31572;&#31639;&#27861;&#24615;&#33021;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;&#38598;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#29305;&#23450;&#27169;&#22411;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.07388</link><description>&lt;p&gt;
&#26080;&#20551;&#35774;&#27979;&#35797;&#31639;&#27861;&#24615;&#33021;&#30340;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Limits of Assumption-free Tests for Algorithm Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07388
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#37327;&#22238;&#31572;&#31639;&#27861;&#24615;&#33021;&#38382;&#39064;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#35777;&#26126;&#20102;&#40657;&#30418;&#27979;&#35797;&#26041;&#27861;&#26080;&#27861;&#20934;&#30830;&#22238;&#31572;&#31639;&#27861;&#22312;&#19981;&#21516;&#35757;&#32451;&#38598;&#19978;&#30340;&#25972;&#20307;&#24615;&#33021;&#21644;&#29305;&#23450;&#27169;&#22411;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#35780;&#20215;&#21644;&#27604;&#36739;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#22522;&#26412;&#30340;&#38382;&#39064;&#65292;&#19968;&#20010;&#31639;&#27861;&#22312;&#32473;&#23450;&#30340;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#22914;&#20309;&#65292;&#21738;&#20010;&#31639;&#27861;&#34920;&#29616;&#26368;&#20339;&#65311;&#35768;&#22810;&#26041;&#27861;&#24050;&#32463;&#24320;&#21457;&#20986;&#26469;&#35780;&#20272;&#31639;&#27861;&#24615;&#33021;&#65292;&#36890;&#24120;&#22522;&#20110;&#20132;&#21449;&#39564;&#35777;&#31574;&#30053;&#65292;&#23558;&#24863;&#20852;&#36259;&#30340;&#31639;&#27861;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#23376;&#38598;&#19978;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20854;&#22312;&#30041;&#20986;&#25968;&#25454;&#28857;&#19978;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#36825;&#20123;&#31243;&#24207;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#26041;&#27861;&#30340;&#29702;&#35770;&#24615;&#36136;&#23578;&#26410;&#23436;&#20840;&#29702;&#35299;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#37327;&#19979;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#30340;&#19968;&#20123;&#22522;&#26412;&#38480;&#21046;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21306;&#20998;&#20102;&#20004;&#20010;&#38382;&#39064;: &#31639;&#27861;$A$&#22312;&#22823;&#23567;&#20026;$n$&#30340;&#35757;&#32451;&#38598;&#19978;&#23398;&#20064;&#38382;&#39064;&#26377;&#22810;&#22909;&#65292;&#20197;&#21450;&#22312;&#29305;&#23450;&#22823;&#23567;&#20026;$n$&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36816;&#34892;$A$&#25152;&#20135;&#29983;&#30340;&#29305;&#23450;&#25311;&#21512;&#27169;&#22411;&#26377;&#22810;&#22909;&#65311;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#35777;&#26126;&#65292;&#23545;&#20110;&#20219;&#20309;&#23558;&#31639;&#27861;&#35270;&#20026;&#40657;&#30418;&#30340;&#27979;&#35797;&#26041;&#27861;&#65292;&#26080;&#27861;&#20934;&#30830;&#22320;&#22238;&#31572;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithm evaluation and comparison are fundamental questions in machine learning and statistics -- how well does an algorithm perform at a given modeling task, and which algorithm performs best? Many methods have been developed to assess algorithm performance, often based around cross-validation type strategies, retraining the algorithm of interest on different subsets of the data and assessing its performance on the held-out data points. Despite the broad use of such procedures, the theoretical properties of these methods are not yet fully understood. In this work, we explore some fundamental limits for answering these questions with limited amounts of data. In particular, we make a distinction between two questions: how good is an algorithm $A$ at the problem of learning from a training set of size $n$, versus, how good is a particular fitted model produced by running $A$ on a particular training data set of size $n$?   Our main results prove that, for any test that treats the algor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#38388;&#29615;&#22659;&#20013;&#39564;&#35777;&#39044;&#27979;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#19981;&#21305;&#37197;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.03527</link><description>&lt;p&gt;
&#22312;&#31354;&#38388;&#29615;&#22659;&#20013;&#19968;&#33268;&#39564;&#35777;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Consistent Validation for Predictive Methods in Spatial Settings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#31354;&#38388;&#29615;&#22659;&#20013;&#39564;&#35777;&#39044;&#27979;&#26041;&#27861;&#30340;&#19968;&#33268;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22788;&#29702;&#19981;&#21305;&#37197;&#24773;&#20917;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#39044;&#27979;&#20219;&#21153;&#23545;&#20110;&#22825;&#27668;&#39044;&#25253;&#12289;&#31354;&#27668;&#27745;&#26579;&#30740;&#31350;&#21644;&#20854;&#20182;&#31185;&#23398;&#24037;&#20316;&#33267;&#20851;&#37325;&#35201;&#12290;&#30830;&#23450;&#25105;&#20204;&#23545;&#32479;&#35745;&#25110;&#29289;&#29702;&#26041;&#27861;&#25152;&#20316;&#39044;&#27979;&#30340;&#21487;&#20449;&#24230;&#26159;&#31185;&#23398;&#32467;&#35770;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20256;&#32479;&#30340;&#39564;&#35777;&#26041;&#27861;&#26080;&#27861;&#22788;&#29702;&#39564;&#35777;&#20301;&#32622;&#21644;&#25105;&#20204;&#24076;&#26395;&#36827;&#34892;&#39044;&#27979;&#30340;&#65288;&#27979;&#35797;&#65289;&#20301;&#32622;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;&#36825;&#31181;&#19981;&#21305;&#37197;&#36890;&#24120;&#19981;&#26159;&#21327;&#21464;&#37327;&#20559;&#31227;&#30340;&#19968;&#20010;&#23454;&#20363;&#65288;&#24120;&#24120;&#34987;&#24418;&#24335;&#21270;&#65289;&#65292;&#22240;&#20026;&#39564;&#35777;&#21644;&#27979;&#35797;&#20301;&#32622;&#26159;&#22266;&#23450;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#32593;&#26684;&#19978;&#25110;&#36873;&#23450;&#30340;&#28857;&#19978;&#65289;&#65292;&#32780;&#19981;&#26159;&#20174;&#20004;&#20010;&#20998;&#24067;&#20013;&#29420;&#31435;&#21516;&#20998;&#24067;&#22320;&#37319;&#26679;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24418;&#24335;&#21270;&#20102;&#23545;&#39564;&#35777;&#26041;&#27861;&#30340;&#26816;&#26597;&#65306;&#38543;&#30528;&#39564;&#35777;&#25968;&#25454;&#30340;&#23494;&#24230;&#36234;&#26469;&#36234;&#22823;&#65292;&#23427;&#20204;&#33021;&#22815;&#21464;&#24471;&#20219;&#24847;&#31934;&#30830;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20256;&#32479;&#26041;&#27861;&#21644;&#21327;&#21464;&#37327;&#20559;&#31227;&#26041;&#27861;&#21487;&#33021;&#19981;&#28385;&#36275;&#36825;&#20010;&#26816;&#26597;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23427;&#20511;&#37492;&#20102;&#21327;&#21464;&#37327;&#20559;&#31227;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#24605;&#24819;&#65292;&#20294;&#23545;&#39564;&#35777;&#25968;&#25454;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spatial prediction tasks are key to weather forecasting, studying air pollution, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We instead propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.02196</link><description>&lt;p&gt;
&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#26679;&#26412;&#39640;&#25928;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02196
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#32858;&#31867;&#21644;&#24449;&#26381;"&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25171;&#30772;&#26679;&#26412;&#25928;&#29575;&#30340;&#29942;&#39048;&#12290;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#32858;&#31867;&#21487;&#20197;&#23454;&#29616;O(p)&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20943;&#23569;&#36895;&#24230;&#65292;&#36825;&#26159;&#29702;&#35770;&#19978;&#21487;&#36798;&#21040;&#30340;&#26368;&#20339;&#20943;&#23569;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#22312;&#22266;&#23450;&#39044;&#31639;&#21644;&#22266;&#23450;&#31934;&#24230;&#30340;&#33539;&#24335;&#19979;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21508;&#31181;&#24120;&#35265;&#30340;&#25490;&#24207;&#21644;&#36873;&#25321;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#22312;&#26080;&#38656;&#39640;&#31934;&#30830;&#24230;&#30456;&#20851;&#20272;&#35745;&#21644;&#31934;&#30830;&#32858;&#31867;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25913;&#36827;&#12290;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#65292;&#22914;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#30340;&#26080;&#31579;&#36873;&#29256;&#26412;&#30340;&#26041;&#27861;&#24778;&#20154;&#22320;&#36229;&#36807;&#20102;&#23436;&#20840;&#39034;&#24207;&#21270;&#30340;&#22522;&#20934;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#34920;&#26126;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#30456;&#20851;&#24615;&#65292;&#26159;&#32469;&#36807;&#20256;&#32479;&#26041;&#27861;&#30340;&#19968;&#26465;&#21487;&#34892;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose novel "clustering and conquer" procedures for the parallel large-scale ranking and selection (R&amp;S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency. In parallel computing environments, correlation-based clustering can achieve an $\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal reduction rate theoretically attainable. Our proposed framework is versatile, allowing for seamless integration of various prevalent R&amp;S methods under both fixed-budget and fixed-precision paradigms. It can achieve improvements without the necessity of highly accurate correlation estimation and precise clustering. In large-scale AI applications such as neural architecture search, a screening-free version of our procedure surprisingly surpasses fully-sequential benchmarks in terms of sample efficiency. This suggests that leveraging valuable structural information, such as correlation, is a viable path to bypassing the trad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2302.05614</link><description>&lt;p&gt;
&#20855;&#26377;&#21407;&#22411;&#30340;&#36328;&#39046;&#22495;&#38543;&#26426;&#39044;&#35757;&#32451;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Random Pre-training with Prototypes for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#24037;&#20316;&#24050;&#25552;&#20132;&#32473;IEEE&#36827;&#34892;&#21487;&#33021;&#30340;&#20986;&#29256;&#12290; CRPTpro&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;RL&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#12290; CRPTpro&#37319;&#29992;&#20102;&#36328;&#39046;&#22495;&#38543;&#26426;&#31574;&#30053;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#20174;&#22810;&#20010;&#39046;&#22495;&#20013;&#25277;&#26679;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#25439;&#22833;&#36827;&#34892;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36328;&#39046;&#22495;&#32534;&#30721;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290; &#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#22914;APT&#21644;Proto-RL&#30456;&#27604;&#65292;CRP
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16458</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65306;&#20174;P2P&#20511;&#36151;&#30340;&#36151;&#27454;&#25551;&#36848;&#20013;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
P2P&#20511;&#36151;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#34701;&#36164;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#23558;&#20511;&#27454;&#20154;&#19982;&#25918;&#27454;&#20154;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;P2P&#20511;&#36151;&#38754;&#20020;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25918;&#27454;&#20154;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#20511;&#27454;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#20511;&#27454;&#20154;&#22312;&#36151;&#27454;&#30003;&#35831;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22788;&#29702;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#21644;&#35821;&#20041;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23558;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;Lending Club&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#22266;&#26377;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#20197;&#21450;&#28508;&#22312;&#20559;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Tensor PCA&#27169;&#22411;&#20013;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#25910;&#25947;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#21644;&#31639;&#27861;&#38408;&#20540;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20572;&#27490;&#20934;&#21017;&#26469;&#33719;&#24471;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.01047</link><description>&lt;p&gt;
Tensor PCA&#30340;&#21151;&#29575;&#36845;&#20195;&#30340;&#23574;&#38160;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Sharp Analysis of Power Iteration for Tensor PCA. (arXiv:2401.01047v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;Tensor PCA&#27169;&#22411;&#20013;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#65292;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#38480;&#21046;&#65292;&#24182;&#24314;&#31435;&#20102;&#20851;&#20110;&#25910;&#25947;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#21644;&#31639;&#27861;&#38408;&#20540;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20572;&#27490;&#20934;&#21017;&#26469;&#33719;&#24471;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35843;&#26597;&#20102;Richard&#21644;Montanari&#65288;2014&#65289;&#24341;&#20837;&#30340;Tensor PCA&#27169;&#22411;&#30340;&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#12290;&#20043;&#21069;&#30740;&#31350;Tensor&#21151;&#29575;&#36845;&#20195;&#31639;&#27861;&#30340;&#24037;&#20316;&#35201;&#20040;&#20165;&#38480;&#20110;&#22266;&#23450;&#27425;&#25968;&#30340;&#36845;&#20195;&#65292;&#35201;&#20040;&#38656;&#35201;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#19982;&#25968;&#25454;&#26080;&#20851;&#30340;&#21021;&#22987;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36229;&#36234;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#24182;&#23545;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;Tensor&#21151;&#29575;&#36845;&#20195;&#30340;&#21160;&#24577;&#36827;&#34892;&#20102;&#22810;&#39033;&#24335;&#25968;&#37327;&#32423;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#24191;&#27867;&#30340;&#20449;&#22122;&#27604;&#33539;&#22260;&#19979;&#65292;&#21151;&#29575;&#36845;&#20195;&#25910;&#25947;&#21040;&#31181;&#26893;&#20449;&#21495;&#25152;&#38656;&#36845;&#20195;&#27425;&#25968;&#30340;&#23574;&#38160;&#30028;&#38480;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#23454;&#38469;&#30340;&#31639;&#27861;&#38408;&#20540;&#27604;&#25991;&#29486;&#20013;&#29468;&#27979;&#30340;&#35201;&#23567;&#19968;&#20010;polylog(n)&#30340;&#22240;&#23376;&#65292;&#20854;&#20013;n&#26159;&#29615;&#22659;&#32500;&#24230;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21151;&#29575;&#36845;&#20195;&#20572;&#27490;&#20934;&#21017;&#65292;&#21487;&#20197;&#20445;&#35777;&#36755;&#20986;&#19982;&#30495;&#23454;&#20449;&#21495;&#39640;&#24230;&#30456;&#20851;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the power iteration algorithm for the tensor PCA model introduced in Richard and Montanari (2014). Previous work studying the properties of tensor power iteration is either limited to a constant number of iterations, or requires a non-trivial data-independent initialization. In this paper, we move beyond these limitations and analyze the dynamics of randomly initialized tensor power iteration up to polynomially many steps. Our contributions are threefold: First, we establish sharp bounds on the number of iterations required for power method to converge to the planted signal, for a broad range of the signal-to-noise ratios. Second, our analysis reveals that the actual algorithmic threshold for power iteration is smaller than the one conjectured in literature by a polylog(n) factor, where n is the ambient dimension. Finally, we propose a simple and effective stopping criterion for power iteration, which provably outputs a solution that is highly correlated with the true si
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26041;&#27861;&#12290;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2310.00093</link><description>&lt;p&gt;
DataDAM: &#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
DataDAM: Efficient Dataset Distillation with Attention Matching. (arXiv:2310.00093v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00093
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26041;&#27861;&#12290;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#22312;&#23613;&#37327;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#22810;&#26679;&#21270;&#25968;&#25454;&#38598;&#19978;&#30340;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;&#26356;&#22823;&#30495;&#23454;&#25968;&#25454;&#38598;&#20449;&#24687;&#30340;&#23567;&#22411;&#21512;&#25104;&#25968;&#25454;&#38598;&#26469;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#65292;&#24182;&#26368;&#32456;&#23454;&#29616;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#26041;&#27861;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#24182;&#19981;&#33021;&#20687;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#37027;&#26679;&#20998;&#24067;&#21644;&#21306;&#20998;&#65292;&#32780;&#19988;&#20250;&#24102;&#26469;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#65292;&#20294;&#31934;&#28860;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#19982;&#25972;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#26126;&#26174;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#21305;&#37197;&#30340;&#39640;&#25928;&#25968;&#25454;&#38598;&#31934;&#28860;(DataDAM)&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20943;&#23569;&#20102;&#35757;&#32451;&#25104;&#26412;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#31354;&#38388;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers have long tried to minimize training costs in deep learning while maintaining strong generalization across diverse datasets. Emerging research on dataset distillation aims to reduce training costs by creating a small synthetic set that contains the information of a larger real dataset and ultimately achieves test accuracy equivalent to a model trained on the whole dataset. Unfortunately, the synthetic data generated by previous methods are not guaranteed to distribute and discriminate as well as the original training data, and they incur significant computational costs. Despite promising results, there still exists a significant performance gap between models trained on condensed synthetic sets and those trained on the whole dataset. In this paper, we address these challenges using efficient Dataset Distillation with Attention Matching (DataDAM), achieving state-of-the-art performance while reducing training costs. Specifically, we learn synthetic images by matching the spa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.16633</link><description>&lt;p&gt;
&#28151;&#21512;&#20320;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;
&lt;/p&gt;
&lt;p&gt;
Mixup Your Own Pairs. (arXiv:2309.16633v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SupReMix&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#26679;&#26412;&#65292;&#29305;&#21035;&#26159;&#28151;&#21512;&#36127;&#26679;&#26412;&#21644;&#28151;&#21512;&#27491;&#26679;&#26412;&#65292;&#26469;&#35299;&#20915;&#22238;&#24402;&#38382;&#39064;&#20013;&#34920;&#31034;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#26356;&#20934;&#30830;&#30340;&#22238;&#24402;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#22238;&#24402;&#38382;&#39064;&#20256;&#32479;&#19978;&#27604;&#20998;&#31867;&#38382;&#39064;&#21463;&#21040;&#30340;&#20851;&#27880;&#36739;&#23569;&#12290;&#30452;&#25509;&#24212;&#29992;&#20026;&#20998;&#31867;&#35774;&#35745;&#30340;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#21040;&#22238;&#24402;&#38382;&#39064;&#24448;&#24448;&#20250;&#23548;&#33268;&#28508;&#31354;&#38388;&#20013;&#30862;&#29255;&#21270;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#20135;&#29983;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#30001;&#20110;&#24573;&#35270;&#20102;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#65306;&#24207;&#24207;&#24863;&#30693;&#21644;&#38590;&#24230;&#65292;&#23545;&#20110;&#22238;&#24402;&#38382;&#39064;&#32780;&#35328;&#65292;&#23545;&#27604;&#23398;&#20064;&#30340;&#28508;&#33021;&#34987;&#24573;&#35270;&#20102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#8220;&#28151;&#21512;&#33258;&#24049;&#30340;&#23545;&#27604;&#23545;&#36827;&#34892;&#30417;&#30563;&#24615;&#23545;&#27604;&#22238;&#24402;&#8221;&#65292;&#32780;&#19981;&#20165;&#20165;&#20381;&#38752;&#30495;&#23454;/&#22686;&#24378;&#26679;&#26412;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24335;&#30417;&#30563;&#23545;&#27604;&#22238;&#24402;&#23398;&#20064;&#65288;SupReMix&#65289;&#12290;&#23427;&#22312;&#23884;&#20837;&#32423;&#21035;&#19978;&#20197;&#38170;&#28857;&#21253;&#21547;&#30340;&#28151;&#21512;&#65288;&#38170;&#28857;&#21644;&#19968;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#36127;&#23545;&#65292;&#20197;&#38170;&#28857;&#25490;&#38500;&#30340;&#28151;&#21512;&#65288;&#20004;&#20010;&#19981;&#21516;&#30340;&#36127;&#26679;&#26412;&#30340;&#28151;&#21512;&#65289;&#20316;&#20026;&#22256;&#38590;&#27491;&#23545;&#12290;&#36825;&#19968;&#31574;&#30053;&#24418;&#25104;&#20102;&#22256;&#38590;&#26679;&#26412;&#23545;&#23398;&#20064;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
In representation learning, regression has traditionally received less attention than classification. Directly applying representation learning techniques designed for classification to regression often results in fragmented representations in the latent space, yielding sub-optimal performance. In this paper, we argue that the potential of contrastive learning for regression has been overshadowed due to the neglect of two crucial aspects: ordinality-awareness and hardness. To address these challenges, we advocate "mixup your own contrastive pairs for supervised contrastive regression", instead of relying solely on real/augmented samples. Specifically, we propose Supervised Contrastive Learning for Regression with Mixup (SupReMix). It takes anchor-inclusive mixtures (mixup of the anchor and a distinct negative sample) as hard negative pairs and anchor-exclusive mixtures (mixup of two distinct negative samples) as hard positive pairs at the embedding level. This strategy formulates harde
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.02211</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Machine Learning with Multi-source Data. (arXiv:2309.02211v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#28304;&#25968;&#25454;&#30340;&#20998;&#24067;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#25552;&#39640;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#30446;&#26631;&#20998;&#24067;&#19982;&#28304;&#25968;&#25454;&#38598;&#19981;&#21516;&#26102;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#36739;&#24046;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#26412;&#25991;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#28304;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26469;&#20248;&#21270;&#20851;&#20110;&#30446;&#26631;&#20998;&#24067;&#31867;&#30340;&#21487;&#35299;&#37322;&#26041;&#24046;&#30340;&#23545;&#25239;&#24615;&#22870;&#21169;&#12290;&#19982;&#20256;&#32479;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#25913;&#21892;&#20102;&#20855;&#26377;&#20998;&#24067;&#20559;&#31227;&#30340;&#30446;&#26631;&#20154;&#32676;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#32452;&#20998;&#24067;&#40065;&#26834;&#39044;&#27979;&#27169;&#22411;&#26159;&#28304;&#25968;&#25454;&#38598;&#26465;&#20214;&#32467;&#26524;&#27169;&#22411;&#30340;&#21152;&#26435;&#24179;&#22343;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#20851;&#38190;&#37492;&#21035;&#32467;&#26524;&#26469;&#25552;&#39640;&#20219;&#24847;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#65292;&#21253;&#25324;&#38543;&#26426;&#26862;&#26519;&#21644;&#31070;&#32463;&#32593;&#32476;&#31561;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#26657;&#27491;&#20272;&#35745;&#22120;&#26469;&#20272;&#35745;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26368;&#20248;&#32858;&#21512;&#26435;&#37325;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;c&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classical machine learning methods may lead to poor prediction performance when the target distribution differs from the source populations. This paper utilizes data from multiple sources and introduces a group distributionally robust prediction model defined to optimize an adversarial reward about explained variance with respect to a class of target distributions. Compared to classical empirical risk minimization, the proposed robust prediction model improves the prediction accuracy for target populations with distribution shifts. We show that our group distributionally robust prediction model is a weighted average of the source populations' conditional outcome models. We leverage this key identification result to robustify arbitrary machine learning algorithms, including, for example, random forests and neural networks. We devise a novel bias-corrected estimator to estimate the optimal aggregation weight for general machine-learning algorithms and demonstrate its improvement in the c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#29615;&#22659;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#25552;&#20986;&#20102;&#25209;&#21028;&#24615;&#35270;&#35282;&#65292;&#35748;&#20026;&#20165;&#20165;&#25552;&#39640;&#25928;&#29575;&#36824;&#19981;&#36275;&#20197;&#20351;&#26426;&#22120;&#23398;&#20064;&#25104;&#20026;&#19968;&#31181;&#29615;&#22659;&#21487;&#25345;&#32493;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2309.02065</link><description>&lt;p&gt;
&#25928;&#29575;&#19981;&#26159;&#21807;&#19968;&#26631;&#20934;&#65306;&#23545;&#29615;&#22659;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#30340;&#25209;&#21028;&#24615;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Efficiency is Not Enough: A Critical Perspective of Environmentally Sustainable AI. (arXiv:2309.02065v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#29615;&#22659;&#21487;&#25345;&#32493;&#20154;&#24037;&#26234;&#33021;&#25552;&#20986;&#20102;&#25209;&#21028;&#24615;&#35270;&#35282;&#65292;&#35748;&#20026;&#20165;&#20165;&#25552;&#39640;&#25928;&#29575;&#36824;&#19981;&#36275;&#20197;&#20351;&#26426;&#22120;&#23398;&#20064;&#25104;&#20026;&#19968;&#31181;&#29615;&#22659;&#21487;&#25345;&#32493;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30446;&#21069;&#30001;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#31561;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#25512;&#21160;&#65292;&#36825;&#20123;&#26041;&#27861;&#21152;&#36895;&#20102;&#22312;&#35768;&#22810;&#21407;&#26412;&#34987;&#35748;&#20026;&#36229;&#20986;AI&#33539;&#22260;&#30340;&#20219;&#21153;&#19978;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;ML&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12289;&#33021;&#28304;&#28040;&#32791;&#22823;&#65292;&#24182;&#23548;&#33268;&#22823;&#37327;&#30340;&#30899;&#25490;&#25918;&#65292;&#36825;&#26159;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#30340;&#19968;&#20010;&#24050;&#30693;&#39537;&#21160;&#22240;&#32032;&#12290;&#27492;&#22806;&#65292;ML&#31995;&#32479;&#36816;&#34892;&#30340;&#24179;&#21488;&#19982;&#29615;&#22659;&#24433;&#21709;&#26377;&#20851;&#65292;&#21253;&#25324;&#30899;&#25490;&#25918;&#20043;&#22806;&#30340;&#20854;&#20182;&#26041;&#38754;&#12290;&#24037;&#19994;&#30028;&#21644;ML&#31038;&#21306;&#24191;&#27867;&#25512;&#23815;&#30340;&#25552;&#39640;ML&#31995;&#32479;&#22312;&#35745;&#31639;&#21644;&#33021;&#28304;&#28040;&#32791;&#26041;&#38754;&#30340;&#25928;&#29575;&#26469;&#25913;&#21892;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#35748;&#20026;&#20165;&#20165;&#20381;&#38752;&#25928;&#29575;&#36824;&#19981;&#36275;&#20197;&#20351;ML&#20316;&#20026;&#19968;&#31181;&#29615;&#22659;&#21487;&#25345;&#32493;&#30340;&#25216;&#26415;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19977;&#20010;&#39640;&#23618;&#27425;&#30340;&#24046;&#24322;&#26469;&#38416;&#36848;&#32771;&#34385;&#20247;&#22810;&#21464;&#37327;&#23545;ML&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#24433;&#21709;&#26102;&#65292;&#20165;&#20381;&#38752;&#25928;&#29575;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) is currently spearheaded by machine learning (ML) methods such as deep learning (DL) which have accelerated progress on many tasks thought to be out of reach of AI. These ML methods can often be compute hungry, energy intensive, and result in significant carbon emissions, a known driver of anthropogenic climate change. Additionally, the platforms on which ML systems run are associated with environmental impacts including and beyond carbon emissions. The solution lionized by both industry and the ML community to improve the environmental sustainability of ML is to increase the efficiency with which ML systems operate in terms of both compute and energy consumption. In this perspective, we argue that efficiency alone is not enough to make ML as a technology environmentally sustainable. We do so by presenting three high level discrepancies between the effect of efficiency on the environmental sustainability of ML when considering the many variables which it in
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;&#20004;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;Zonoid&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#22312;$d=2,3$&#26102;&#30340;&#23545;&#25968;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#21069;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#33021;&#22815;&#22343;&#21248;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.15285</link><description>&lt;p&gt;
Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Optimal Approximation of Zonoids and Uniform Approximation by Shallow Neural Networks. (arXiv:2307.15285v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35299;&#20915;&#20102;Zonoid&#30340;&#26368;&#20248;&#36924;&#36817;&#21644;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#22343;&#21248;&#36924;&#36817;&#20004;&#20010;&#38382;&#39064;&#12290;&#23545;&#20110;Zonoid&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#22312;$d=2,3$&#26102;&#30340;&#23545;&#25968;&#24046;&#36317;&#65292;&#23454;&#29616;&#20102;&#22312;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#30446;&#21069;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#33021;&#22815;&#22343;&#21248;&#36924;&#36817;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#20004;&#20010;&#30456;&#20851;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#30830;&#23450;&#19968;&#20010;&#20219;&#24847;&#30340;&#22312;$\mathbb{R}^{d+1}$&#31354;&#38388;&#20013;&#30340;Zonoid&#21487;&#20197;&#36890;&#36807;$n$&#20010;&#32447;&#27573;&#30340;Hausdorff&#36317;&#31163;&#26469;&#36924;&#36817;&#30340;&#35823;&#24046;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#30830;&#23450;&#27973;&#23618;ReLU$^k$&#31070;&#32463;&#32593;&#32476;&#22312;&#20854;&#21464;&#20998;&#31354;&#38388;&#20013;&#30340;&#22343;&#21248;&#33539;&#25968;&#30340;&#26368;&#20248;&#36924;&#36817;&#29575;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#24050;&#32463;&#22312;$d \neq 2, 3$&#26102;&#24471;&#21040;&#35299;&#20915;&#65292;&#20294;&#24403;$d = 2, 3$&#26102;&#65292;&#26368;&#20248;&#19978;&#30028;&#21644;&#26368;&#20248;&#19979;&#30028;&#20043;&#38388;&#20173;&#23384;&#22312;&#19968;&#20010;&#23545;&#25968;&#24046;&#36317;&#12290;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#20010;&#24046;&#36317;&#65292;&#23436;&#25104;&#20102;&#25152;&#26377;&#32500;&#24230;&#19978;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23545;&#20110;&#31532;&#20108;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;$k \geq 1$&#26102;&#26174;&#33879;&#25552;&#39640;&#20102;&#29616;&#26377;&#30340;&#36924;&#36817;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#30446;&#26631;&#20989;&#25968;&#21450;&#20854;&#23548;&#25968;&#30340;&#22343;&#21248;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the following two related problems. The first is to determine to what error an arbitrary zonoid in $\mathbb{R}^{d+1}$ can be approximated in the Hausdorff distance by a sum of $n$ line segments. The second is to determine optimal approximation rates in the uniform norm for shallow ReLU$^k$ neural networks on their variation spaces. The first of these problems has been solved for $d\neq 2,3$, but when $d=2,3$ a logarithmic gap between the best upper and lower bounds remains. We close this gap, which completes the solution in all dimensions. For the second problem, our techniques significantly improve upon existing approximation rates when $k\geq 1$, and enable uniform approximation of both the target function and its derivatives.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14474</link><description>&lt;p&gt;
&#27827;&#24029;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limits to Reservoir Learning. (arXiv:2307.14474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#26469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#65288;IPC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#20449;&#21495;&#38598;&#21512;&#21040;&#23436;&#25972;&#20989;&#25968;&#22522;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;IPC&#26469;&#34913;&#37327;&#22122;&#22768;&#19979;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#65288;&#19968;&#31181;&#29305;&#27530;&#30340;&#24490;&#29615;&#32593;&#32476;&#65289;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;IPC&#22312;&#31995;&#32479;&#23610;&#23544;n&#19978;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;n&#20010;&#36755;&#20986;&#20449;&#21495;&#30340;$2^n$&#20010;&#21487;&#33021;&#30340;&#36880;&#28857;&#20056;&#31215;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36864;&#21270;&#24847;&#21619;&#30528;&#22312;&#20648;&#27700;&#24211;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20648;&#27700;&#24211;&#25152;&#34920;&#31034;&#30340;&#20989;&#25968;&#26063;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#19968;&#38598;&#21512;&#30340;$2^n$&#20010;&#20989;&#25968;&#22312;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24615;&#33021;&#20272;&#35745;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#21442;&#25968;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#19982;&#20998;&#21106;&#38382;&#39064;&#30340;&#29305;&#28857;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.10926</link><description>&lt;p&gt;
&#23545;3D&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#24615;&#33021;&#20272;&#35745;&#30340;&#32622;&#20449;&#21306;&#38388;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Confidence intervals for performance estimates in 3D medical image segmentation. (arXiv:2307.10926v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10926
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#24615;&#33021;&#20272;&#35745;&#30340;&#32622;&#20449;&#21306;&#38388;&#65292;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#21442;&#25968;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#19982;&#20998;&#21106;&#38382;&#39064;&#30340;&#29305;&#28857;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#20998;&#21106;&#27169;&#22411;&#30340;&#35780;&#20272;&#26159;&#22522;&#20110;&#26377;&#38480;&#30340;&#20363;&#22270;&#20687;&#65292;&#22240;&#27492;&#35780;&#20272;&#32467;&#26524;&#23384;&#22312;&#22122;&#22768;&#12290;&#38500;&#20102;&#25253;&#21578;&#24179;&#22343;&#24615;&#33021;&#25351;&#26631;&#22806;&#65292;&#25253;&#21578;&#32622;&#20449;&#21306;&#38388;&#20063;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#65292;&#24456;&#23569;&#26377;&#20154;&#36825;&#26679;&#20570;&#12290;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#21462;&#20915;&#20110;&#27979;&#35797;&#38598;&#22823;&#23567;&#21644;&#24615;&#33021;&#25351;&#26631;&#30340;&#25955;&#24067;&#31243;&#24230;&#65288;&#21363;&#27979;&#35797;&#38598;&#19978;&#30340;&#26631;&#20934;&#24046;&#65289;&#12290;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;&#38656;&#35201;&#35768;&#22810;&#27979;&#35797;&#22270;&#20687;&#20197;&#36991;&#20813;&#23485;&#27867;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20998;&#21106;&#38382;&#39064;&#65292;&#36825;&#20010;&#24773;&#20917;&#23578;&#26410;&#30740;&#31350;&#65292;&#22240;&#20026;&#32473;&#23450;&#30340;&#27979;&#35797;&#22270;&#20687;&#25152;&#25552;&#20379;&#30340;&#20449;&#24687;&#37327;&#19981;&#21516;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20013;&#20856;&#22411;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#30340;nnU-net&#26694;&#26550;&#22312;&#20004;&#20010;&#26469;&#33258;Medical Decathlon&#25361;&#25112;&#36187;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;3D&#22270;&#20687;&#20998;&#21106;&#30340;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;Dice&#20934;&#30830;&#24230;&#21644;Hausdorff&#36317;&#31163;&#20004;&#20010;&#24615;&#33021;&#25351;&#26631;&#12290;&#25105;&#20204;&#21457;&#29616;&#21442;&#25968;&#32622;&#20449;&#21306;&#38388;&#30340;&#23485;&#24230;&#19982;&#20998;&#21106;&#38382;&#39064;&#30340;&#29305;&#28857;&#26377;&#20851;&#65292;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#25165;&#33021;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Medical segmentation models are evaluated empirically. As such an evaluation is based on a limited set of example images, it is unavoidably noisy. Beyond a mean performance measure, reporting confidence intervals is thus crucial. However, this is rarely done in medical image segmentation. The width of the confidence interval depends on the test set size and on the spread of the performance measure (its standard-deviation across of the test set). For classification, many test images are needed to avoid wide confidence intervals. Segmentation, however, has not been studied, and it differs by the amount of information brought by a given test image. In this paper, we study the typical confidence intervals in medical image segmentation. We carry experiments on 3D image segmentation using the standard nnU-net framework, two datasets from the Medical Decathlon challenge and two performance measures: the Dice accuracy and the Hausdorff distance. We show that the parametric confidence intervals
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;&#65292;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#20869;&#20998;&#24067;&#39044;&#27979;&#21644;&#22240;&#26524;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#24178;&#39044;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2307.10299</link><description>&lt;p&gt;
&#22240;&#26524;&#24615;&#23548;&#21521;&#30340;&#40065;&#26834;&#24615;&#65306;&#21033;&#29992;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;
&lt;/p&gt;
&lt;p&gt;
Causality-oriented robustness: exploiting general additive interventions. (arXiv:2307.10299v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRIG&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;&#65292;&#22312;&#39044;&#27979;&#27169;&#22411;&#20013;&#32467;&#21512;&#20102;&#20869;&#20998;&#24067;&#39044;&#27979;&#21644;&#22240;&#26524;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#24178;&#39044;&#30340;&#40065;&#26834;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#32463;&#24120;&#21457;&#29983;&#20998;&#24067;&#21464;&#21270;&#65292;&#24613;&#38656;&#24320;&#21457;&#23545;&#36825;&#31181;&#21464;&#21270;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#29616;&#26377;&#30340;&#26694;&#26550;&#65292;&#22914;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#25110;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65292;&#35201;&#20040;&#23545;&#26410;&#35265;&#20998;&#24067;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#20551;&#23450;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22240;&#26524;&#24615;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#21644;&#32467;&#26500;&#30340;&#31283;&#20581;&#39044;&#27979;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;&#22240;&#26524;&#25512;&#26029;&#25152;&#38656;&#30340;&#20551;&#35774;&#21487;&#33021;&#36807;&#20110;&#20005;&#26684;&#65292;&#36825;&#31181;&#22240;&#26524;&#27169;&#22411;&#25552;&#20379;&#30340;&#40065;&#26834;&#24615;&#24120;&#24120;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22240;&#26524;&#24615;&#23548;&#21521;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DRIG&#65288;Distributional Robustness via Invariant Gradients&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19968;&#33324;&#24615;&#21487;&#21152;&#24178;&#39044;&#65292;&#20197;&#23454;&#29616;&#23545;&#26410;&#35265;&#24178;&#39044;&#30340;&#40065;&#26834;&#39044;&#27979;&#65292;&#24182;&#22312;&#20869;&#20998;&#24067;&#39044;&#27979;&#21644;&#22240;&#26524;&#24615;&#20043;&#38388;&#33258;&#28982;&#22320;&#36827;&#34892;&#25554;&#20540;&#12290;&#22312;&#32447;&#24615;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DRIG&#20135;&#29983;&#30340;&#39044;&#27979;&#26159;
&lt;/p&gt;
&lt;p&gt;
Since distribution shifts are common in real-world applications, there is a pressing need for developing prediction models that are robust against such shifts. Existing frameworks, such as empirical risk minimization or distributionally robust optimization, either lack generalizability for unseen distributions or rely on postulated distance measures. Alternatively, causality offers a data-driven and structural perspective to robust predictions. However, the assumptions necessary for causal inference can be overly stringent, and the robustness offered by such causal models often lacks flexibility. In this paper, we focus on causality-oriented robustness and propose Distributional Robustness via Invariant Gradients (DRIG), a method that exploits general additive interventions in training data for robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality. In a linear setting, we prove that DRIG yields predictions that are 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2307.06608</link><description>&lt;p&gt;
&#23558;&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#24341;&#20837;&#65306;&#26397;&#30528;&#26356;&#23454;&#29992;&#30340;&#23545;&#25239;&#25915;&#20987;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Introducing Foundation Models as Surrogate Models: Advancing Towards More Practical Adversarial Attacks. (arXiv:2307.06608v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06608
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#36890;&#36807;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#26469;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65292;&#24182;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#12290;&#34429;&#28982;&#22522;&#30784;&#27169;&#22411;&#30340;&#34920;&#29616;&#19981;&#20339;&#65292;&#20294;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#32570;&#20047;&#23545;&#24212;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#26080;&#30418;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#20102;&#26368;&#23454;&#29992;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25915;&#20987;&#26041;&#24335;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#35775;&#38382;&#27169;&#22411;&#30340;&#26550;&#26500;&#12289;&#26435;&#37325;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26080;&#30418;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20195;&#29702;&#27169;&#22411;&#36873;&#25321;&#36807;&#31243;&#30340;&#28508;&#21147;&#21644;&#28789;&#27963;&#24615;&#32570;&#20047;&#35748;&#35782;&#12290;&#21463;&#21040;&#21033;&#29992;&#22522;&#30784;&#27169;&#22411;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#30340;&#20852;&#36259;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#37319;&#29992;&#20102;1&#65289;&#23558;&#23545;&#25239;&#25915;&#20987;&#37325;&#26032;&#35774;&#23450;&#20026;&#19979;&#28216;&#20219;&#21153;&#65292;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;&#29983;&#25104;&#22270;&#20687;&#22122;&#22768;&#20197;&#28385;&#36275;&#26032;&#20852;&#36235;&#21183;&#65307;2&#65289;&#23558;&#22522;&#30784;&#27169;&#22411;&#24341;&#20837;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#30340;&#21019;&#26032;&#24605;&#24819;&#12290;&#36890;&#36807;&#21033;&#29992;&#38750;&#40065;&#26834;&#29305;&#24449;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#36873;&#25321;&#20195;&#29702;&#27169;&#22411;&#30340;&#20004;&#20010;&#25351;&#23548;&#21407;&#21017;&#65292;&#20197;&#35299;&#37322;&#20026;&#20160;&#20040;&#22522;&#30784;&#27169;&#22411;&#26159;&#36825;&#19968;&#35282;&#33394;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#30683;&#30462;&#22320;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20998;&#26512;&#36825;&#31181;&#24847;&#22806;&#34892;&#20026;&#65292;&#25105;&#20204;&#24402;&#22240;&#20110;&#32570;&#20047;&#19978;&#36848;&#25351;&#23548;&#21407;&#21017;&#25152;&#38656;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the no-box adversarial attack, in which the attacker lacks access to the model's architecture, weights, and training data, become the most practical and challenging attack setup. However, there is an unawareness of the potential and flexibility inherent in the surrogate model selection process on no-box setting. Inspired by the burgeoning interest in utilizing foundational models to address downstream tasks, this paper adopts an innovative idea that 1) recasting adversarial attack as a downstream task. Specifically, image noise generation to meet the emerging trend and 2) introducing foundational models as surrogate models. Harnessing the concept of non-robust features, we elaborate on two guiding principles for surrogate model selection to explain why the foundational model is an optimal choice for this role. However, paradoxically, we observe that these foundational models underperform. Analyzing this unexpected behavior within the feature space, we attribute the lackluster
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#30340;&#31639;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#20135;&#29983;&#34394;&#20551;&#20851;&#32852;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.04777</link><description>&lt;p&gt;
&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;
&lt;/p&gt;
&lt;p&gt;
Invariant Causal Set Covering Machines. (arXiv:2306.04777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#19981;&#21464;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#30340;&#31639;&#27861;&#65292;&#23427;&#36991;&#20813;&#20102;&#20135;&#29983;&#34394;&#20551;&#20851;&#32852;&#65292;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65292;&#22914;&#20915;&#31574;&#26641;&#65292;&#22240;&#20854;&#21487;&#35299;&#37322;&#30340;&#29305;&#24615;&#21463;&#21040;&#20174;&#19994;&#32773;&#30340;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#36825;&#31181;&#27169;&#22411;&#30340;&#23398;&#20064;&#31639;&#27861;&#24448;&#24448;&#23481;&#26131;&#21463;&#21040;&#34394;&#20551;&#20851;&#32852;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#19981;&#33021;&#20445;&#35777;&#25552;&#21462;&#30340;&#26159;&#20855;&#26377;&#22240;&#26524;&#20851;&#31995;&#30340;&#27934;&#35265;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#19981;&#21464;&#22240;&#26524;&#39044;&#27979;&#25991;&#29486;&#20013;&#30340;&#24605;&#24819;&#65292;&#25552;&#20986;&#20102;&#19981;&#21464;&#30340;&#22240;&#26524;&#38598;&#35206;&#30422;&#26426;&#65292;&#36825;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#38598;&#35206;&#30422;&#26426;&#31639;&#27861;&#30340;&#25193;&#23637;&#65292;&#29992;&#20110;&#20108;&#20540;&#35268;&#21017;&#30340;&#21512;&#21462;/&#26512;&#21462;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#36991;&#20813;&#20102;&#34394;&#20551;&#20851;&#32852;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21644;&#23454;&#36341;&#19978;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#35782;&#21035;&#24863;&#20852;&#36259;&#21464;&#37327;&#30340;&#22240;&#26524;&#29238;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based models, such as decision trees, appeal to practitioners due to their interpretable nature. However, the learning algorithms that produce such models are often vulnerable to spurious associations and thus, they are not guaranteed to extract causally-relevant insights. In this work, we build on ideas from the invariant causal prediction literature to propose Invariant Causal Set Covering Machines, an extension of the classical Set Covering Machine algorithm for conjunctions/disjunctions of binary-valued rules that provably avoids spurious associations. We demonstrate both theoretically and empirically that our method can identify the causal parents of a variable of interest in polynomial time.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#35745;&#21010;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38271;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06361</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of Multi-task Neural Solver with Multi-armed Bandits. (arXiv:2305.06361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#35745;&#21010;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38271;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22914;&#20309;&#39640;&#25928;&#22320;&#20026;&#21508;&#31181;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064; (COP) &#35757;&#32451;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#19979;&#30340;&#22810;&#20219;&#21153;&#29702;&#35770;&#25439;&#22833;&#20998;&#35299;&#65292;&#36890;&#36807;&#19968;&#20010;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36890;&#36807;&#27491;&#30830;&#30340;&#36172;&#21338;&#31639;&#27861;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#30456;&#27604;&#26631;&#20934;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#27573;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#36825;&#21487;&#20197;&#20026;&#20854;&#20182;&#22810;&#20219;&#21153;&#22823;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#25552;&#20379;&#25351;&#23548;&#65292;&#27492;&#22806;&#65292;&#24433;&#21709;&#30697;&#38453;&#21487;&#20197;&#25552;&#20379;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#24120;&#35265;&#23454;&#36341;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#20174;&#32780;&#25903;&#25345;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently training a multi-task neural solver for various combinatorial optimization problems (COPs) has been less studied so far. In this paper, we propose a general and efficient training paradigm based on multi-armed bandits to deliver a unified multi-task neural solver. To this end, we resort to the theoretical loss decomposition for multiple tasks under an encoder-decoder framework, which enables more efficient training via proper bandit task-sampling algorithms through an intra-task influence matrix. Our method achieves much higher overall performance with either limited training budgets or the same training epochs, compared to standard training schedules, which can be promising for advising efficient training of other multi-task large models. Additionally, the influence matrix can provide empirical evidence of some common practices in the area of learning to optimize, which in turn supports the validity of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;</title><link>http://arxiv.org/abs/2304.11960</link><description>&lt;p&gt;
ThreatCrawl&#65306;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23433;&#20840;&#28966;&#28857;&#29228;&#34411;
&lt;/p&gt;
&lt;p&gt;
ThreatCrawl: A BERT-based Focused Crawler for the Cybersecurity Domain. (arXiv:2304.11960v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;BERT&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#31579;&#36873;&#20986;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20844;&#24320;&#33719;&#21462;&#30340;&#20449;&#24687;&#23545;&#20110;&#32593;&#32476;&#23041;&#32961;&#24773;&#25253;&#65288;CTI&#65289;&#26469;&#35828;&#21253;&#21547;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#36825;&#21487;&#20197;&#29992;&#20110;&#39044;&#38450;&#24050;&#32463;&#22312;&#20854;&#20182;&#31995;&#32479;&#19978;&#21457;&#29983;&#30340;&#25915;&#20987;&#12290;&#20294;&#26159;&#65292;&#34429;&#28982;&#26377;&#19981;&#21516;&#30340;&#26631;&#20934;&#26469;&#20132;&#27969;&#36825;&#20123;&#20449;&#24687;&#65292;&#20294;&#24456;&#22810;&#20449;&#24687;&#26159;&#20197;&#38750;&#26631;&#20934;&#21270;&#30340;&#26041;&#24335;&#22312;&#25991;&#31456;&#25110;&#21338;&#23458;&#24086;&#23376;&#20013;&#20849;&#20139;&#30340;&#12290;&#25163;&#21160;&#27983;&#35272;&#22810;&#20010;&#22312;&#32447;&#38376;&#25143;&#21644;&#26032;&#38395;&#39029;&#38754;&#20197;&#21457;&#29616;&#26032;&#23041;&#32961;&#24182;&#25552;&#21462;&#23427;&#20204;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#36825;&#20010;&#25195;&#25551;&#36807;&#31243;&#30340;&#19968;&#37096;&#20998;&#65292;&#22810;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#23041;&#32961;&#25351;&#31034;&#22120;&#65288;IOCs&#65289;&#30340;&#25552;&#21462;&#22120;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#24050;&#32463;&#35299;&#20915;&#20102;&#20174;&#25991;&#26723;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#38382;&#39064;&#65292;&#20294;&#24456;&#23569;&#32771;&#34385;&#25628;&#32034;&#36825;&#20123;&#25991;&#26723;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28966;&#28857;&#29228;&#34411;ThreatCrawl&#65292;&#23427;&#20351;&#29992;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#65288;BERT&#65289;&#25628;&#32034;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#30456;&#20851;&#25991;&#26723;&#12290;ThreatCrawl&#20351;&#29992;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#35789;&#25552;&#21462;&#25216;&#26415;&#26469;&#35782;&#21035;&#30456;&#20851;&#32593;&#31449;&#21644;&#32593;&#39029;&#65292;&#28982;&#21518;&#24212;&#29992;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#22120;&#26469;&#20248;&#20808;&#32771;&#34385;&#26368;&#21487;&#33021;&#21253;&#21547;&#26377;&#20215;&#20540;CTI&#20449;&#24687;&#30340;&#32593;&#39029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Publicly available information contains valuable information for Cyber Threat Intelligence (CTI). This can be used to prevent attacks that have already taken place on other systems. Ideally, only the initial attack succeeds and all subsequent ones are detected and stopped. But while there are different standards to exchange this information, a lot of it is shared in articles or blog posts in non-standardized ways. Manually scanning through multiple online portals and news pages to discover new threats and extracting them is a time-consuming task. To automize parts of this scanning process, multiple papers propose extractors that use Natural Language Processing (NLP) to extract Indicators of Compromise (IOCs) from documents. However, while this already solves the problem of extracting the information out of documents, the search for these documents is rarely considered. In this paper, a new focused crawler is proposed called ThreatCrawl, which uses Bidirectional Encoder Representations 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#28548;&#28165;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26356;&#36866;&#21512;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#26377;&#30528;&#30452;&#25509;&#24433;&#21709;&#65292;&#20063;&#24341;&#36215;&#20102;&#21746;&#23398;&#23478;&#23545;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2303.12032</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;
&lt;/p&gt;
&lt;p&gt;
The Representational Status of Deep Learning Models. (arXiv:2303.12032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#28548;&#28165;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#23454;&#38469;&#19978;&#23427;&#20204;&#26356;&#36866;&#21512;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#65292;&#36825;&#19968;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#26377;&#30528;&#30452;&#25509;&#24433;&#21709;&#65292;&#20063;&#24341;&#36215;&#20102;&#21746;&#23398;&#23478;&#23545;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#28548;&#28165;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;DLMs&#65289;&#30340;&#34920;&#24449;&#29366;&#24577;&#12290;&#30001;&#20110;&#21151;&#33021;&#21644;&#20851;&#31995;&#27010;&#24565;&#30340;&#28151;&#28102;&#65292;&#23613;&#31649;&#36890;&#24120;&#31216;&#20026;&#8220;&#34920;&#24449;&#8221;&#65292;&#20294;&#36825;&#24847;&#21619;&#30528;&#21547;&#31946;&#19981;&#28165;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#34429;&#28982;DLM&#20197;&#20851;&#31995;&#24847;&#20041;&#19978;&#30340;&#34920;&#24449;&#20854;&#30446;&#26631;&#65292;&#20294;&#26368;&#22909;&#29702;&#35299;&#20026;&#39640;&#24230;&#29702;&#24819;&#21270;&#30340;&#27169;&#22411;&#12290;&#36825;&#20010;&#32467;&#26524;&#23545;&#21487;&#35299;&#37322;&#30340;AI&#65288;XAI&#65289;&#26377;&#30452;&#25509;&#24433;&#21709;&#65292;&#24182;&#24341;&#23548;&#21746;&#23398;&#20851;&#27880;DLM&#34920;&#24449;&#30340;&#29702;&#24819;&#21270;&#24615;&#36136;&#21450;&#20854;&#22312;&#26410;&#26469;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper aims to clarify the representational status of Deep Learning Models (DLMs). While commonly referred to as 'representations', what this entails is ambiguous due to a conflation of functional and relational conceptions of representation. This paper argues that while DLMs represent their targets in a relational sense, they are best understood as highly idealized models. This result has immediate implications for explainable AI (XAI) and directs philosophical attention toward examining the idealized nature of DLM representations and their role in future scientific investigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#26469;&#21435;&#38500;&#21253;&#21547;&#32467;&#26500;&#24615;&#22122;&#22768;&#30340;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.05290</link><description>&lt;p&gt;
&#37319;&#29992;&#25193;&#25955;&#27169;&#22411;&#21435;&#38500;&#32467;&#26500;&#24615;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
Removing Structured Noise with Diffusion Models. (arXiv:2302.05290v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#26041;&#27861;&#26469;&#21435;&#38500;&#21253;&#21547;&#32467;&#26500;&#24615;&#22122;&#22768;&#30340;&#25968;&#25454;&#65292;&#30456;&#27604;&#20110;&#24120;&#35268;&#26041;&#27861;&#26377;&#36739;&#22909;&#30340;&#34920;&#29616;&#65292;&#23545;&#20110;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#19981;&#36866;&#23450;&#21453;&#38382;&#39064;&#38656;&#35201;&#20180;&#32454;&#21046;&#23450;&#26377;&#20851;&#24863;&#20852;&#36259;&#20449;&#21495;&#30340;&#20808;&#39564;&#20449;&#24565;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#26377;&#22122;&#22768;&#27979;&#37327;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20934;&#30830;&#30340;&#25551;&#36848;&#12290;&#22522;&#20110;&#31232;&#30095;&#24615;&#30340;&#25163;&#24037;&#21046;&#23450;&#20449;&#21495;&#20808;&#39564;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#25968;&#25454;&#39537;&#21160;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#25152;&#21462;&#20195;&#65292;&#24182;&#19988;&#20960;&#20010;&#22242;&#38431;&#26368;&#36817;&#23637;&#31034;&#20102;&#26368;&#26032;&#30340;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#24615;&#33021;&#21644;&#28789;&#27963;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#21518;&#39564;&#37319;&#26679;&#33539;&#24335;&#21487;&#20197;&#25193;&#23637;&#21040;&#21253;&#25324;&#20016;&#23500;&#12289;&#32467;&#26500;&#21270;&#30340;&#22122;&#22768;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#26465;&#20214;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#21253;&#21547;&#22122;&#22768;&#21644;&#20449;&#21495;&#29983;&#25104;&#20998;&#24067;&#30340;&#23398;&#20064;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#23384;&#22312;&#32467;&#26500;&#24615;&#22122;&#22768;&#30340;&#21453;&#38382;&#39064;&#20013;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#20248;&#20110;&#20351;&#29992;&#24402;&#19968;&#21270;&#27969;&#21644;&#23545;&#25239;&#32593;&#32476;&#30340;&#31454;&#20105;&#22522;&#32447;&#12290;&#36825;&#22312;&#21307;&#23398;&#25104;&#20687;&#31561;&#39046;&#22495;&#24320;&#36767;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#23454;&#36341;&#20013;&#36827;&#34892;&#26356;&#20934;&#30830;&#24314;&#27169;&#30340;&#26032;&#26426;&#20250;&#21644;&#30456;&#20851;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Solving ill-posed inverse problems requires careful formulation of prior beliefs over the signals of interest and an accurate description of their manifestation into noisy measurements. Handcrafted signal priors based on e.g. sparsity are increasingly replaced by data-driven deep generative models, and several groups have recently shown that state-of-the-art score-based diffusion models yield particularly strong performance and flexibility. In this paper, we show that the powerful paradigm of posterior sampling with diffusion models can be extended to include rich, structured, noise models. To that end, we propose a joint conditional reverse diffusion process with learned scores for the noise and signal-generating distribution. We demonstrate strong performance gains across various inverse problems with structured noise, outperforming competitive baselines that use normalizing flows and adversarial networks. This opens up new opportunities and relevant practical applications of diffusi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32422;&#26463;&#21644;&#26080;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#24182;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#26469;&#23637;&#31034;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.10932</link><description>&lt;p&gt;
&#20851;&#20110;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Global Convergence of Risk-Averse Policy Gradient Methods with Expected Conditional Risk Measures. (arXiv:2301.10932v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32422;&#26463;&#21644;&#26080;&#32422;&#26463;&#24773;&#20917;&#19979;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#24182;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#26469;&#23637;&#31034;&#26041;&#27861;&#30340;&#23454;&#29992;&#20215;&#20540;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#25511;&#21046;&#19981;&#30830;&#23450;&#32467;&#26524;&#21644;&#30830;&#20445;&#21508;&#31181;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#30340;&#21487;&#38752;&#24615;&#33021;&#30340;&#27969;&#34892;&#24037;&#20855;&#12290;&#34429;&#28982;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#26159;&#21542;&#20855;&#26377;&#19982;&#39118;&#38505;&#20013;&#24615;&#24773;&#20917;&#19979;&#30456;&#21516;&#30340;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#21160;&#24577;&#26102;&#38388;&#19968;&#33268;&#39118;&#38505;&#24230;&#37327;&#65292;&#31216;&#20026;&#26399;&#26395;&#26465;&#20214;&#39118;&#38505;&#24230;&#37327;&#65288;ECRM&#65289;&#65292;&#24182;&#20026;&#22522;&#20110;ECRM&#30340;&#30446;&#26631;&#20989;&#25968;&#25512;&#23548;&#20986;&#31574;&#30053;&#26799;&#24230;&#26356;&#26032;&#12290;&#22312;&#32422;&#26463;&#30452;&#25509;&#21442;&#25968;&#21270;&#21644;&#26080;&#32422;&#26463;softmax&#21442;&#25968;&#21270;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#30456;&#24212;&#30340;&#39118;&#38505;&#21388;&#24694;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#27979;&#35797;&#20102;REINFORCE&#21644;actor-critic&#31639;&#27861;&#30340;&#39118;&#38505;&#21388;&#24694;&#21464;&#20307;&#65292;&#20197;&#23637;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#39118;&#38505;&#25511;&#21046;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Risk-sensitive reinforcement learning (RL) has become a popular tool to control the risk of uncertain outcomes and ensure reliable performance in various sequential decision-making problems. While policy gradient methods have been developed for risk-sensitive RL, it remains unclear if these methods enjoy the same global convergence guarantees as in the risk-neutral case. In this paper, we consider a class of dynamic time-consistent risk measures, called Expected Conditional Risk Measures (ECRMs), and derive policy gradient updates for ECRM-based objective functions. Under both constrained direct parameterization and unconstrained softmax parameterization, we provide global convergence and iteration complexities of the corresponding risk-averse policy gradient algorithms. We further test risk-averse variants of REINFORCE and actor-critic algorithms to demonstrate the efficacy of our method and the importance of risk control.
&lt;/p&gt;</description></item></channel></rss>