<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#12289;&#23545;&#31216;&#21644;&#30830;&#23450;&#24615;&#21160;&#20316;&#19979;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#36817;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.10820</link><description>&lt;p&gt;
&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Offline Reinforcement Learning via Metric Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10820
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24230;&#37327;&#23398;&#20064;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#12289;&#23545;&#31216;&#21644;&#30830;&#23450;&#24615;&#21160;&#20316;&#19979;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#36817;&#20284;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#23398;&#20064;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#38598;&#26041;&#38754;&#30340;&#26174;&#30528;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22312;&#30446;&#26631;&#26465;&#20214;&#19979;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20174;&#27425;&#20248;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26368;&#20248;&#34892;&#20026;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#36817;&#20284;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#12289;&#23545;&#31216;&#19988;&#30830;&#23450;&#24615;&#21160;&#20316;&#19979;&#30340;&#30446;&#26631;&#26465;&#20214;&#31163;&#32447;RL&#38382;&#39064;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#34920;&#31034;&#24674;&#22797;&#20248;&#21270;&#30340;&#23646;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#23548;&#33268;&#35813;&#23646;&#24615;&#30340;&#26032;&#20248;&#21270;&#30446;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#20540;&#20989;&#25968;&#20197;&#28436;&#21592;-&#35780;&#35770;&#32773;&#30340;&#26041;&#24335;&#25351;&#23548;&#31574;&#30053;&#30340;&#23398;&#20064;&#65292;&#36825;&#31181;&#26041;&#27861;&#34987;&#25105;&#20204;&#31216;&#20026;MetricRL&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#22987;&#32456;&#20248;&#20110;&#20854;&#20182;&#31163;&#32447;RL&#22522;&#32447;&#22312;&#20174;&#27425;&#20248;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#35266;&#27979;&#21644;&#22810;&#30446;&#26631;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10820v1 Announce Type: new  Abstract: In this work, we address the problem of learning optimal behavior from sub-optimal datasets in the context of goal-conditioned offline reinforcement learning. To do so, we propose a novel way of approximating the optimal value function for goal-conditioned offline RL problems under sparse rewards, symmetric and deterministic actions. We study a property for representations to recover optimality and propose a new optimization objective that leads to such property. We use the learned value function to guide the learning of a policy in an actor-critic fashion, a method we name MetricRL. Experimentally, we show how our method consistently outperforms other offline RL baselines in learning from sub-optimal offline datasets. Moreover, we show the effectiveness of our method in dealing with high-dimensional observations and in multi-goal tasks.
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#34928;&#20943;&#21644;&#23567;&#30340;&#31867;&#20869;&#21464;&#21270;&#19982;&#20302;&#31209;&#20559;&#24046;&#29616;&#35937;&#26377;&#20851;</title><link>https://arxiv.org/abs/2402.03991</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#26435;&#37325;&#34928;&#20943;&#21644;&#31867;&#20869;&#21464;&#21270;&#23567;&#20250;&#23548;&#33268;&#20302;&#31209;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Neural Rank Collapse: Weight Decay and Small Within-Class Variability Yield Low-Rank Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03991
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#34928;&#20943;&#21644;&#23567;&#30340;&#31867;&#20869;&#21464;&#21270;&#19982;&#20302;&#31209;&#20559;&#24046;&#29616;&#35937;&#26377;&#20851;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#26174;&#31034;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#20302;&#31209;&#20559;&#24046;&#29616;&#35937;&#65306;&#28145;&#24230;&#32593;&#32476;&#20013;&#30340;&#26435;&#37325;&#30697;&#38453;&#24448;&#24448;&#36817;&#20284;&#20026;&#20302;&#31209;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25110;&#20174;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#20013;&#21435;&#38500;&#30456;&#23545;&#36739;&#23567;&#30340;&#22855;&#24322;&#20540;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#21516;&#26102;&#20445;&#25345;&#29978;&#33267;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20302;&#31209;&#20559;&#24046;&#30340;&#29702;&#35770;&#30740;&#31350;&#37117;&#28041;&#21450;&#21040;&#31616;&#21270;&#30340;&#32447;&#24615;&#28145;&#24230;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#24102;&#26377;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#21644;&#26435;&#37325;&#34928;&#20943;&#21442;&#25968;&#30340;&#36890;&#29992;&#32593;&#32476;&#65292;&#24182;&#23637;&#31034;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#31070;&#32463;&#31209;&#23849;&#28291;&#29616;&#35937;&#65292;&#23427;&#23558;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#30340;&#20302;&#31209;&#20559;&#24046;&#19982;&#32593;&#32476;&#30340;&#31070;&#32463;&#23849;&#28291;&#29305;&#24615;&#32852;&#31995;&#36215;&#26469;&#65306;&#38543;&#30528;&#26435;&#37325;&#34928;&#20943;&#21442;&#25968;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#20013;&#27599;&#19968;&#23618;&#30340;&#31209;&#21576;&#27604;&#20363;&#36882;&#20943;&#65292;&#19982;&#21069;&#38754;&#23618;&#30340;&#38544;&#34255;&#31354;&#38388;&#23884;&#20837;&#30340;&#31867;&#20869;&#21464;&#21270;&#25104;&#21453;&#27604;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work in deep learning has shown strong empirical and theoretical evidence of an implicit low-rank bias: weight matrices in deep networks tend to be approximately low-rank and removing relatively small singular values during training or from available trained models may significantly reduce model size while maintaining or even improving model performance. However, the majority of the theoretical investigations around low-rank bias in neural networks deal with oversimplified deep linear networks. In this work, we consider general networks with nonlinear activations and the weight decay parameter, and we show the presence of an intriguing neural rank collapse phenomenon, connecting the low-rank bias of trained networks with networks' neural collapse properties: as the weight decay parameter grows, the rank of each layer in the network decreases proportionally to the within-class variability of the hidden-space embeddings of the previous layers. Our theoretical findings are supporte
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;Gym-DSSAT&#27169;&#25311;&#22120;&#35757;&#32451;&#26234;&#33021;agent&#26469;&#25484;&#25569;&#26368;&#20339;&#27694;&#32933;&#31649;&#29702;&#31574;&#30053;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#24207;&#21015;&#35266;&#27979;&#24320;&#21457;&#26356;&#39640;&#25928;&#27694;&#32933;&#36755;&#20837;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#24182;&#25506;&#35752;&#20102;&#27668;&#20505;&#21464;&#24322;&#23545;&#20892;&#19994;&#31649;&#29702;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.01273</link><description>&lt;p&gt;
&#23398;&#20064;&#22522;&#20110;&#29615;&#22659;&#27668;&#20505;&#21464;&#24322;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#20892;&#19994;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning-based agricultural management in partially observable environments subject to climate variability. (arXiv:2401.01273v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;Gym-DSSAT&#27169;&#25311;&#22120;&#35757;&#32451;&#26234;&#33021;agent&#26469;&#25484;&#25569;&#26368;&#20339;&#27694;&#32933;&#31649;&#29702;&#31574;&#30053;&#12290;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#24207;&#21015;&#35266;&#27979;&#24320;&#21457;&#26356;&#39640;&#25928;&#27694;&#32933;&#36755;&#20837;&#31574;&#30053;&#30340;&#20248;&#21183;&#65292;&#24182;&#25506;&#35752;&#20102;&#27668;&#20505;&#21464;&#24322;&#23545;&#20892;&#19994;&#31649;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#31649;&#29702;&#22312;&#22609;&#36896;&#20316;&#29289;&#20135;&#37327;&#12289;&#32463;&#27982;&#21487;&#30408;&#21033;&#24615;&#21644;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#29305;&#21035;&#20851;&#27880;&#26045;&#32933;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;&#26497;&#31471;&#22825;&#27668;&#26465;&#20214;&#65288;&#22914;&#28909;&#28010;&#21644;&#24178;&#26097;&#65289;&#26102;&#65292;&#20256;&#32479;&#25351;&#23548;&#26041;&#38024;&#30340;&#26377;&#25928;&#24615;&#20943;&#24369;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26694;&#26550;&#65292;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#19982;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;Gym-DSSAT&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26234;&#33021;agent&#26469;&#25484;&#25569;&#26368;&#20339;&#27694;&#32933;&#31649;&#29702;&#12290;&#36890;&#36807;&#22312;&#29233;&#33655;&#21326;&#24030;&#29577;&#31859;&#20892;&#20316;&#29289;&#19978;&#36827;&#34892;&#19968;&#31995;&#21015;&#27169;&#25311;&#23454;&#39564;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#27169;&#22411;&#21644;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#24207;&#21015;&#35266;&#27979;&#26469;&#24320;&#21457;&#26356;&#39640;&#25928;&#30340;&#27694;&#32933;&#36755;&#20837;&#31574;&#30053;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#27668;&#20505;&#30340;&#21464;&#24322;&#24615;&#23545;&#20892;&#19994;&#31649;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Agricultural management, with a particular focus on fertilization strategies, holds a central role in shaping crop yield, economic profitability, and environmental sustainability. While conventional guidelines offer valuable insights, their efficacy diminishes when confronted with extreme weather conditions, such as heatwaves and droughts. In this study, we introduce an innovative framework that integrates Deep Reinforcement Learning (DRL) with Recurrent Neural Networks (RNNs). Leveraging the Gym-DSSAT simulator, we train an intelligent agent to master optimal nitrogen fertilization management. Through a series of simulation experiments conducted on corn crops in Iowa, we compare Partially Observable Markov Decision Process (POMDP) models with Markov Decision Process (MDP) models. Our research underscores the advantages of utilizing sequential observations in developing more efficient nitrogen input policies. Additionally, we explore the impact of climate variability, particularly duri
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;GP-VAEs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#21644;&#34892;&#20026;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22240;&#23376;&#20998;&#26512;&#65288;GPFA&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#21462;&#19981;&#21516;&#23454;&#39564;&#27169;&#24577;&#30340;&#20849;&#20139;&#21644;&#29420;&#31435;&#28508;&#21464;&#37327;&#65292;&#24182;&#19988;&#20855;&#26377;&#35299;&#37322;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.03111</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#31070;&#32463;&#21644;&#34892;&#20026;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Gaussian Process Variational Autoencoders for Neural and Behavioral Data. (arXiv:2310.03111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;GP-VAEs&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#31070;&#32463;&#21644;&#34892;&#20026;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22240;&#23376;&#20998;&#26512;&#65288;GPFA&#65289;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#21462;&#19981;&#21516;&#23454;&#39564;&#27169;&#24577;&#30340;&#20849;&#20139;&#21644;&#29420;&#31435;&#28508;&#21464;&#37327;&#65292;&#24182;&#19988;&#20855;&#26377;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#31070;&#32463;&#32676;&#20307;&#27963;&#21160;&#21644;&#34892;&#20026;&#25968;&#25454;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#31070;&#32463;&#31185;&#23398;&#30340;&#26680;&#24515;&#30446;&#26631;&#12290;&#34429;&#28982;&#28508;&#22312;&#21464;&#37327;&#27169;&#22411;&#65288;LVMs&#65289;&#22312;&#25551;&#36848;&#39640;&#32500;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#21482;&#29992;&#20110;&#21333;&#19968;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#38590;&#20197;&#35782;&#21035;&#19981;&#21516;&#23454;&#39564;&#25968;&#25454;&#27169;&#24577;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;LVM&#26469;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#35813;&#27169;&#22411;&#25552;&#21462;&#20102;&#19981;&#21516;&#12289;&#21516;&#26102;&#35760;&#24405;&#30340;&#23454;&#39564;&#27169;&#24577;&#30340;&#26102;&#38388;&#28436;&#21270;&#20849;&#20139;&#21644;&#29420;&#31435;&#28508;&#21464;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#39640;&#26031;&#36807;&#31243;&#22240;&#23376;&#20998;&#26512;&#65288;GPFA&#65289;&#65292;&#19968;&#31181;&#35299;&#37322;&#24615;&#30340;&#29992;&#20110;&#31070;&#32463;&#23574;&#23792;&#25968;&#25454;&#30340;LVM&#65292;&#24182;&#20855;&#26377;&#26102;&#38388;&#24179;&#28369;&#28508;&#31354;&#38388;&#65292;&#19982;&#39640;&#26031;&#36807;&#31243;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;GP-VAEs&#65289;&#30456;&#32467;&#21512;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;GP-VAEs&#21516;&#26679;&#20351;&#29992;&#39640;&#26031;&#20808;&#39564;&#26469;&#25551;&#36848;&#28508;&#31354;&#38388;&#20013;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26144;&#23556;&#21040;&#35266;&#27979;&#20540;&#20855;&#26377;&#20016;&#23500;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#28508;&#21464;&#37327;&#20998;&#21306;&#26469;&#23454;&#29616;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Characterizing the relationship between neural population activity and behavioral data is a central goal of neuroscience. While latent variable models (LVMs) are successful in describing high-dimensional time-series data, they are typically only designed for a single type of data, making it difficult to identify structure shared across different experimental data modalities. Here, we address this shortcoming by proposing an unsupervised LVM which extracts temporally evolving shared and independent latents for distinct, simultaneously recorded experimental modalities. We do this by combining Gaussian Process Factor Analysis (GPFA), an interpretable LVM for neural spiking data with temporally smooth latent space, with Gaussian Process Variational Autoencoders (GP-VAEs), which similarly use a GP prior to characterize correlations in a latent space, but admit rich expressivity due to a deep neural network mapping to observations. We achieve interpretability in our model by partitioning lat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#24182;&#22522;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#24471;&#20986;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.19640</link><description>&lt;p&gt;
&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#26368;&#20248;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimal Estimates for Pairwise Learning with Deep ReLU Networks. (arXiv:2305.19640v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#30340;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#24182;&#22522;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#24471;&#20986;&#20960;&#20046;&#26368;&#20248;&#30340;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#23545;&#23398;&#20064;&#25351;&#30340;&#26159;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#32771;&#34385;&#19968;&#23545;&#26679;&#26412;&#30340;&#23398;&#20064;&#20219;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;ReLU&#32593;&#32476;&#20013;&#30340;&#25104;&#23545;&#23398;&#20064;&#65292;&#24182;&#20272;&#35745;&#20102;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#12290;&#23545;&#20110;&#28385;&#36275;&#26576;&#20123;&#28201;&#21644;&#26465;&#20214;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#65292;&#24314;&#31435;&#20102;&#35823;&#24046;&#20272;&#35745;&#30340;&#23574;&#38160;&#30028;&#38480;&#65292;&#20854;&#35823;&#24046;&#20272;&#35745;&#30340;&#38454;&#25968;&#20026;O&#65288;&#65288;Vlog&#65288;n&#65289;/ n&#65289;1 /&#65288;2-&#946;&#65289;&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#25104;&#23545;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#36807;&#24230;&#27867;&#21270;&#35823;&#24046;&#30340;&#20960;&#20046;&#26368;&#20248;&#30028;&#38480;&#65292;&#22312;&#30495;&#23454;&#30340;&#39044;&#27979;&#22120;&#28385;&#36275;&#26576;&#20123;&#20809;&#28369;&#24615;&#27491;&#21017;&#24615;&#26102;&#65292;&#26368;&#20248;&#30028;&#38480;&#36798;&#21040;&#20102;&#26368;&#23567;&#21270;&#30028;&#38480;&#65292;&#24046;&#36317;&#20165;&#20026;&#23545;&#25968;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pairwise learning refers to learning tasks where a loss takes a pair of samples into consideration. In this paper, we study pairwise learning with deep ReLU networks and estimate the excess generalization error. For a general loss satisfying some mild conditions, a sharp bound for the estimation error of order $O((V\log(n) /n)^{1/(2-\beta)})$ is established. In particular, with the pairwise least squares loss, we derive a nearly optimal bound of the excess generalization error which achieves the minimax lower bound up to a logrithmic term when the true predictor satisfies some smoothness regularities.
&lt;/p&gt;</description></item></channel></rss>