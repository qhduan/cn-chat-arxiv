<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#21487;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#38024;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#21487;&#33021;&#34987;&#19981;&#24403;&#30340;&#32463;&#39564;&#26041;&#27861;&#25152;&#38459;&#30861;</title><link>https://arxiv.org/abs/2404.02113</link><description>&lt;p&gt;
&#38024;&#23545;&#26410;&#30693;&#36827;&#34892;&#35843;&#25972;&#65306;&#37325;&#26032;&#23457;&#35270;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#35780;&#20272;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Tuning for the Unknown: Revisiting Evaluation Strategies for Lifelong RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02113
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#21487;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#38024;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#36827;&#23637;&#21487;&#33021;&#34987;&#19981;&#24403;&#30340;&#32463;&#39564;&#26041;&#27861;&#25152;&#38459;&#30861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32487;&#32493;&#25110;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#23545;&#29615;&#22659;&#30340;&#35775;&#38382;&#24212;&#35813;&#26159;&#26377;&#38480;&#30340;&#12290;&#22914;&#26524;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#30340;&#31639;&#27861;&#33021;&#22815;&#38271;&#26102;&#38388;&#36816;&#34892;&#65292;&#24182;&#19981;&#26029;&#36866;&#24212;&#26032;&#30340;&#12289;&#24847;&#24819;&#19981;&#21040;&#30340;&#24773;&#20917;&#65292;&#37027;&#20040;&#25105;&#20204;&#24517;&#39035;&#24895;&#24847;&#22312;&#25972;&#20010;&#20195;&#29702;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20869;&#37096;&#32626;&#25105;&#20204;&#30340;&#20195;&#29702;&#32780;&#19981;&#35843;&#25972;&#23427;&#20204;&#30340;&#36229;&#21442;&#25968;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013; -- &#29978;&#33267;&#32487;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013; -- &#20855;&#22791;&#23545;&#20195;&#29702;&#30340;&#37096;&#32626;&#29615;&#22659;&#20855;&#26377;&#26080;&#38480;&#21046;&#35775;&#38382;&#26435;&#30340;&#26631;&#20934;&#20570;&#27861;&#21487;&#33021;&#24050;&#32463;&#38459;&#30861;&#20102;&#23545;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#30340;&#36827;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#25972;&#21644;&#35780;&#20272;&#32456;&#36523;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#20854;&#20013;&#21482;&#26377;&#23454;&#39564;&#25968;&#25454;&#30340;&#30334;&#20998;&#20043;&#19968;&#21487;&#20197;&#29992;&#20110;&#36229;&#21442;&#25968;&#35843;&#25972;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;DQN&#21644;Soft Actor Critic&#22312;&#21508;&#31181;&#25345;&#32493;&#21644;&#38750;&#31283;&#23450;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20004;&#31181;&#26041;&#27861;&#36890;&#24120;&#34920;&#29616;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02113v1 Announce Type: new  Abstract: In continual or lifelong reinforcement learning access to the environment should be limited. If we aspire to design algorithms that can run for long-periods of time, continually adapting to new, unexpected situations then we must be willing to deploy our agents without tuning their hyperparameters over the agent's entire lifetime. The standard practice in deep RL -- and even continual RL -- is to assume unfettered access to deployment environment for the full lifetime of the agent. This paper explores the notion that progress in lifelong RL research has been held back by inappropriate empirical methodologies. In this paper we propose a new approach for tuning and evaluating lifelong RL agents where only one percent of the experiment data can be used for hyperparameter tuning. We then conduct an empirical study of DQN and Soft Actor Critic across a variety of continuing and non-stationary domains. We find both methods generally perform po
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02780</link><description>&lt;p&gt;
&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data Collaboration Analysis Over Matrix Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#25913;&#36827;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#24535;&#30528;&#20248;&#36234;&#30340;&#36136;&#37327;&#65292;&#22686;&#24378;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#20197;&#20943;&#36731;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#21644;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24191;&#27867;&#30340;&#25968;&#25454;&#34701;&#21512;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#36947;&#24503;&#20851;&#20999;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#29992;&#25143;&#38544;&#31169;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#25259;&#38706;&#39118;&#38505;&#12290;&#24050;&#24314;&#31435;&#20102;&#21508;&#31181;&#20840;&#29699;&#31435;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#27861;&#35268;&#23545;&#20445;&#25252;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;ML&#25216;&#26415;&#30340;&#23454;&#38469;&#37096;&#32626;&#21464;&#24471;&#22797;&#26434;&#12290;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;(PPML)&#36890;&#36807;&#20445;&#25252;&#20174;&#20581;&#24247;&#35760;&#24405;&#21040;&#22320;&#29702;&#20301;&#32622;&#25968;&#25454;&#31561;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#23454;&#29616;&#23433;&#20840;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02780v1 Announce Type: new  Abstract: The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;</title><link>https://arxiv.org/abs/2402.10665</link><description>&lt;p&gt;
&#20351;&#29992;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#24615;&#33021;&#21450;&#20854;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Selective Prediction for Semantic Segmentation using Post-Hoc Confidence Estimation and Its Performance under Distribution Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10665
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#20998;&#21106;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#28982;&#32780;&#20854;&#26377;&#25928;&#24615;&#24120;&#24120;&#21463;&#21040;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#32570;&#20047;&#25152;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#19968;&#20010;&#24120;&#35265;&#31574;&#30053;&#26159;&#21033;&#29992;&#22312;&#19981;&#21516;&#31181;&#32676;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22914;&#20844;&#24320;&#21487;&#29992;&#30340;&#25968;&#25454;&#38598;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23548;&#33268;&#20102;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#22312;&#20852;&#36259;&#31181;&#32676;&#19978;&#34920;&#29616;&#20986;&#38477;&#20302;&#30340;&#24615;&#33021;&#12290;&#22312;&#27169;&#22411;&#38169;&#35823;&#21487;&#33021;&#24102;&#26469;&#37325;&#22823;&#21518;&#26524;&#30340;&#24773;&#20917;&#19979;&#65292;&#36873;&#25321;&#24615;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#20943;&#36731;&#39118;&#38505;&#12289;&#20943;&#23569;&#23545;&#19987;&#23478;&#30417;&#30563;&#20381;&#36182;&#30340;&#25163;&#27573;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#36164;&#28304;&#21294;&#20047;&#29615;&#22659;&#19979;&#35821;&#20041;&#20998;&#21106;&#30340;&#36873;&#25321;&#24615;&#39044;&#27979;&#65292;&#30528;&#37325;&#20110;&#24212;&#29992;&#20110;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#36816;&#34892;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20107;&#21518;&#32622;&#20449;&#24230;&#20272;&#35745;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#35821;&#20041;&#20998;&#21106;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#20687;&#32423;&#32622;&#20449;&#24230;&#27979;&#37327;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10665v1 Announce Type: new  Abstract: Semantic segmentation plays a crucial role in various computer vision applications, yet its efficacy is often hindered by the lack of high-quality labeled data. To address this challenge, a common strategy is to leverage models trained on data from different populations, such as publicly available datasets. This approach, however, leads to the distribution shift problem, presenting a reduced performance on the population of interest. In scenarios where model errors can have significant consequences, selective prediction methods offer a means to mitigate risks and reduce reliance on expert supervision. This paper investigates selective prediction for semantic segmentation in low-resource settings, thus focusing on post-hoc confidence estimators applied to pre-trained models operating under distribution shift. We propose a novel image-level confidence measure tailored for semantic segmentation and demonstrate its effectiveness through expe
&lt;/p&gt;</description></item><item><title>&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.04938</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#21305;&#37197;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#25913;&#36827;&#20102;DDIM&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04938
&lt;/p&gt;
&lt;p&gt;
&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22312;Denoising Diffusion Implicit Models (DDIM)&#26694;&#26550;&#20013;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65288;&#20869;&#26680;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;Denoising Diffusion Probabilistic Models (DDPM)&#20013;&#21152;&#36895;&#37319;&#26679;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#20043;&#19968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#32422;&#26463;GMM&#30340;&#21442;&#25968;&#65292;&#21305;&#37197;DDPM&#21069;&#21521;&#36793;&#38469;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20351;&#29992;&#39640;&#26031;&#26680;&#30340;&#21407;&#22987;DDIM&#30456;&#21516;&#25110;&#26356;&#22909;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;CelebAHQ&#21644;FFHQ&#30340;&#26080;&#26465;&#20214;&#27169;&#22411;&#20197;&#21450;ImageNet&#25968;&#25454;&#38598;&#30340;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;GMM&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#34913;&#37327;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;&#20351;&#29992;10&#20010;&#37319;&#26679;&#27493;&#39588;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;FID&#20540;&#20026;...
&lt;/p&gt;
&lt;p&gt;
We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2310.04417</link><description>&lt;p&gt;
&#25193;&#25955;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion Random Feature Model. (arXiv:2310.04417v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#25193;&#25955;&#27169;&#22411;&#20026;&#28789;&#24863;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#24182;&#21487;&#22312;&#25968;&#37327;&#30456;&#21516;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#19979;&#19982;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#21487;&#27604;&#36739;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#36890;&#36807;&#25512;&#23548;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#24182;&#24471;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#24050;&#25104;&#21151;&#29992;&#20110;&#29983;&#25104;&#20174;&#22122;&#22768;&#20013;&#20135;&#29983;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25193;&#25955;&#27169;&#22411;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#38590;&#20197;&#35299;&#37322;&#65292;&#32570;&#20047;&#29702;&#35770;&#20381;&#25454;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#30001;&#20110;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#20854;&#22312;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25193;&#25955;&#27169;&#22411;&#21551;&#21457;&#30340;&#28145;&#24230;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#65292;&#23427;&#26082;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#21448;&#33021;&#32473;&#20986;&#19982;&#20855;&#26377;&#30456;&#21516;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30456;&#24403;&#30340;&#25968;&#20540;&#32467;&#26524;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#38543;&#26426;&#29305;&#24449;&#32467;&#26524;&#65292;&#21033;&#29992;&#24471;&#20998;&#21305;&#37197;&#30340;&#23646;&#24615;&#23548;&#20986;&#20102;&#26679;&#26412;&#25968;&#25454;&#20998;&#24067;&#19982;&#30495;&#23454;&#20998;&#24067;&#20043;&#38388;&#30340;&#27867;&#21270;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#26102;&#23578;MNIST&#25968;&#25454;&#38598;&#21644;&#20048;&#22120;&#38899;&#39057;&#25968;&#25454;&#19978;&#29983;&#25104;&#26679;&#26412;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models have been successfully used to generate data from noise. However, most diffusion models are computationally expensive and difficult to interpret with a lack of theoretical justification. Random feature models on the other hand have gained popularity due to their interpretability but their application to complex machine learning tasks remains limited. In this work, we present a diffusion model-inspired deep random feature model that is interpretable and gives comparable numerical results to a fully connected neural network having the same number of trainable parameters. Specifically, we extend existing results for random features and derive generalization bounds between the distribution of sampled data and the true distribution using properties of score matching. We validate our findings by generating samples on the fashion MNIST dataset and instrumental audio data.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2309.09222</link><description>&lt;p&gt;
&#21452;&#37325;&#26631;&#20934;&#21270;&#27969;&#65306;&#28789;&#27963;&#30340;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Double Normalizing Flows: Flexible Bayesian Gaussian Process ODEs Learning. (arXiv:2309.09222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09222
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#26631;&#20934;&#21270;&#27969;&#24341;&#20837;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#28789;&#27963;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#20808;&#39564;&#20998;&#24067;&#21644;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#30340;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#39640;&#26031;&#36807;&#31243;&#34987;&#29992;&#26469;&#24314;&#27169;&#36830;&#32493;&#21160;&#21147;&#31995;&#32479;&#30340;&#21521;&#37327;&#22330;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#27169;&#22411;&#65292;&#36125;&#21494;&#26031;&#25512;&#26029;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#31561;&#20219;&#21153;&#65292;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#39640;&#26031;&#36807;&#31243;&#24120;&#24494;&#20998;&#26041;&#31243;(ODE)&#27169;&#22411;&#22312;&#20855;&#26377;&#38750;&#39640;&#26031;&#36807;&#31243;&#20808;&#39564;&#30340;&#25968;&#25454;&#38598;&#19978;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#32422;&#26463;&#20808;&#39564;&#21644;&#22343;&#20540;&#22330;&#21518;&#39564;&#21487;&#33021;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#20934;&#21270;&#27969;&#26469;&#37325;&#26032;&#21442;&#25968;&#21270;ODE&#30340;&#21521;&#37327;&#22330;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#26356;&#28789;&#27963;&#12289;&#26356;&#34920;&#36798;&#24615;&#30340;&#20808;&#39564;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#26631;&#20934;&#21270;&#27969;&#30340;&#35299;&#26512;&#21487;&#35745;&#31639;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;GP ODE&#30340;&#21518;&#39564;&#25512;&#26029;&#65292;&#29983;&#25104;&#19968;&#20010;&#38750;&#39640;&#26031;&#30340;&#21518;&#39564;&#12290;&#36890;&#36807;&#36825;&#20123;&#26631;&#20934;&#21270;&#27969;&#30340;&#21452;&#37325;&#24212;&#29992;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#36125;&#21494;&#26031;&#39640;&#26031;&#36807;&#31243;ODE&#20013;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Gaussian processes have been utilized to model the vector field of continuous dynamical systems. Bayesian inference for such models \cite{hegde2022variational} has been extensively studied and has been applied in tasks such as time series prediction, providing uncertain estimates. However, previous Gaussian Process Ordinary Differential Equation (ODE) models may underperform on datasets with non-Gaussian process priors, as their constrained priors and mean-field posteriors may lack flexibility. To address this limitation, we incorporate normalizing flows to reparameterize the vector field of ODEs, resulting in a more flexible and expressive prior distribution. Additionally, due to the analytically tractable probability density functions of normalizing flows, we apply them to the posterior inference of GP ODEs, generating a non-Gaussian posterior. Through these dual applications of normalizing flows, our model improves accuracy and uncertainty estimates for Bayesian Gaussian P
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.14172</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#22522;&#20110;&#20809;&#28369;&#24615;&#20808;&#39564;&#25512;&#26029;&#36229;&#22270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#22312;&#22788;&#29702;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#39640;&#38454;&#20851;&#31995;&#25968;&#25454;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#27809;&#26377;&#26126;&#30830;&#36229;&#22270;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#24076;&#26395;&#33021;&#22815;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#20986;&#26377;&#24847;&#20041;&#30340;&#36229;&#22270;&#32467;&#26500;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#37319;&#29992;&#31616;&#21333;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#65292;&#19981;&#33021;&#31934;&#30830;&#25429;&#25417;&#28508;&#22312;&#36229;&#22270;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#35201;&#20040;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65288;&#21363;&#39044;&#20808;&#23384;&#22312;&#30340;&#36229;&#22270;&#32467;&#26500;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23616;&#38480;&#20110;&#23454;&#38469;&#24773;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#28369;&#24615;&#20808;&#39564;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#34920;&#31034;&#36229;&#36793;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#19982;&#21253;&#21547;&#35813;&#36229;&#36793;&#30340;&#36229;&#36793;&#30340;&#29305;&#24449;&#39640;&#24230;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are important for processing data with higher-order relationships involving more than two entities. In scenarios where explicit hypergraphs are not readily available, it is desirable to infer a meaningful hypergraph structure from the node features to capture the intrinsic relations within the data. However, existing methods either adopt simple pre-defined rules that fail to precisely capture the distribution of the potential hypergraph structure, or learn a mapping between hypergraph structures and node features but require a large amount of labelled data, i.e., pre-existing hypergraph structures, for training. Both restrict their applications in practical scenarios. To fill this gap, we propose a novel smoothness prior that enables us to design a method to infer the probability for each potential hyperedge without labelled data as supervision. The proposed prior indicates features of nodes in a hyperedge are highly correlated by the features of the hyperedge containing th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;EHR&#25968;&#25454;&#24320;&#21457;CDSS&#24037;&#20855;&#30340;&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;SyntHIR&#31995;&#32479;&#21644;FHIR&#26631;&#20934;&#23454;&#29616;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#21644;&#24037;&#20855;&#21487;&#36801;&#31227;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02613</link><description>&lt;p&gt;
&#29992;SyntHIR&#23454;&#29616;&#20114;&#25805;&#20316;&#24615;&#21512;&#25104;&#20581;&#24247;&#25968;&#25454;&#65292;&#20197;&#20415;&#24320;&#21457;CDSS&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Interoperable synthetic health data with SyntHIR to enable the development of CDSS tools. (arXiv:2308.02613v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21512;&#25104;EHR&#25968;&#25454;&#24320;&#21457;CDSS&#24037;&#20855;&#30340;&#20307;&#31995;&#26550;&#26500;&#65292;&#36890;&#36807;&#20351;&#29992;SyntHIR&#31995;&#32479;&#21644;FHIR&#26631;&#20934;&#23454;&#29616;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#21644;&#24037;&#20855;&#21487;&#36801;&#31227;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#39640;&#36136;&#37327;&#30340;&#24739;&#32773;&#26085;&#24535;&#21644;&#20581;&#24247;&#30331;&#35760;&#26469;&#24320;&#21457;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#65288;CDSS&#65289;&#26377;&#24456;&#22823;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#22312;&#20020;&#24202;&#24037;&#20316;&#27969;&#31243;&#20013;&#23454;&#26045;CDSS&#24037;&#20855;&#65292;&#38656;&#35201;&#23558;&#35813;&#24037;&#20855;&#38598;&#25104;&#12289;&#39564;&#35777;&#21644;&#27979;&#35797;&#22312;&#29992;&#20110;&#23384;&#20648;&#21644;&#31649;&#29702;&#24739;&#32773;&#25968;&#25454;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#31995;&#32479;&#19978;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21512;&#35268;&#27861;&#35268;&#65292;&#36890;&#24120;&#19981;&#21487;&#33021;&#33719;&#24471;&#23545;EHR&#31995;&#32479;&#30340;&#24517;&#35201;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21644;&#20351;&#29992;CDSS&#24037;&#20855;&#24320;&#21457;&#30340;&#21512;&#25104;EHR&#25968;&#25454;&#30340;&#20307;&#31995;&#26550;&#26500;&#12290;&#35813;&#20307;&#31995;&#32467;&#26500;&#22312;&#19968;&#20010;&#31216;&#20026;SyntHIR&#30340;&#31995;&#32479;&#20013;&#23454;&#29616;&#12290;SyntHIR&#31995;&#32479;&#20351;&#29992;Fast Healthcare Interoperability Resources (FHIR)&#26631;&#20934;&#36827;&#34892;&#25968;&#25454;&#20114;&#25805;&#20316;&#24615;&#65292;&#20351;&#29992;Gretel&#26694;&#26550;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20351;&#29992;Microsoft Azure FHIR&#26381;&#21153;&#22120;&#20316;&#20026;&#22522;&#20110;FHIR&#30340;EHR&#31995;&#32479;&#65292;&#20197;&#21450;&#20351;&#29992;SMART on FHIR&#26694;&#26550;&#36827;&#34892;&#24037;&#20855;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#25968;&#25454;&#24320;&#21457;&#26426;&#22120;&#23398;&#20064;&#22522;&#20110;CDSS&#24037;&#20855;&#26469;&#23637;&#31034;SyntHIR&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a great opportunity to use high-quality patient journals and health registers to develop machine learning-based Clinical Decision Support Systems (CDSS). To implement a CDSS tool in a clinical workflow, there is a need to integrate, validate and test this tool on the Electronic Health Record (EHR) systems used to store and manage patient data. However, it is often not possible to get the necessary access to an EHR system due to legal compliance. We propose an architecture for generating and using synthetic EHR data for CDSS tool development. The architecture is implemented in a system called SyntHIR. The SyntHIR system uses the Fast Healthcare Interoperability Resources (FHIR) standards for data interoperability, the Gretel framework for generating synthetic data, the Microsoft Azure FHIR server as the FHIR-based EHR system and SMART on FHIR framework for tool transportability. We demonstrate the usefulness of SyntHIR by developing a machine learning-based CDSS tool using data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MESAHA-Net&#30340;&#39640;&#25928;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36880;&#23618;2D&#20998;&#21106;&#65292;&#23454;&#29616;&#20102; CT&#25195;&#25551;&#20013;&#31934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2304.01576</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#30340;&#26368;&#22823;&#24378;&#24230;&#25237;&#24433;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;CT&#25195;&#25551;&#32954;&#32467;&#33410;&#20998;&#21106; MESAHA-Net&#65288;arXiv&#65306;2304.01576v1 [eess.IV]&#65289;
&lt;/p&gt;
&lt;p&gt;
MESAHA-Net: Multi-Encoders based Self-Adaptive Hard Attention Network with Maximum Intensity Projections for Lung Nodule Segmentation in CT Scan. (arXiv:2304.01576v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MESAHA-Net&#30340;&#39640;&#25928;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65292;&#36890;&#36807;&#37319;&#29992;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36880;&#23618;2D&#20998;&#21106;&#65292;&#23454;&#29616;&#20102; CT&#25195;&#25551;&#20013;&#31934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#23545;&#26089;&#26399;&#32954;&#30284;&#35786;&#26029;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#24739;&#32773;&#30340;&#29983;&#23384;&#29575;&#12290;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#34987;&#24191;&#27867;&#29992;&#20110;&#32954;&#32467;&#33410;&#20998;&#26512;&#30340;&#26089;&#26399;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;&#32954;&#32467;&#33410;&#30340;&#24322;&#36136;&#24615;&#65292;&#22823;&#23567;&#22810;&#26679;&#24615;&#20197;&#21450;&#21608;&#22260;&#29615;&#22659;&#30340;&#22797;&#26434;&#24615;&#23545;&#24320;&#21457;&#40065;&#26834;&#30340;&#32467;&#33410;&#20998;&#21106;&#26041;&#27861;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#31471;&#21040;&#31471;&#26694;&#26550;&#65292;&#21363;&#22522;&#20110;&#22810;&#32534;&#30721;&#22120;&#30340;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;MESAHA-Net&#65289;&#65292;&#29992;&#20110;CT&#25195;&#25551;&#20013;&#31934;&#30830;&#30340;&#32954;&#32467;&#33410;&#20998;&#21106;&#12290;MESAHA-Net&#21253;&#25324;&#19977;&#20010;&#32534;&#30721;&#36335;&#24452;&#65292;&#19968;&#20010;&#27880;&#24847;&#21147;&#22359;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#22359;&#65292;&#26377;&#21161;&#20110;&#38598;&#25104;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65306;CT&#20999;&#29255;&#34917;&#19969;&#65292;&#21069;&#21521;&#21644;&#21518;&#21521;&#30340;&#26368;&#22823;&#24378;&#24230;&#25237;&#24433;&#65288;MIP&#65289;&#22270;&#20687;&#20197;&#21450;&#21253;&#21547;&#32467;&#33410;&#30340;&#24863;&#20852;&#36259;&#21306;&#22495;&#65288;ROI&#65289;&#25513;&#30721;&#12290;&#36890;&#36807;&#37319;&#29992;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#30828;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;MESAHA-Net&#36880;&#23618;&#25191;&#34892;&#36880;&#23618;2D&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate lung nodule segmentation is crucial for early-stage lung cancer diagnosis, as it can substantially enhance patient survival rates. Computed tomography (CT) images are widely employed for early diagnosis in lung nodule analysis. However, the heterogeneity of lung nodules, size diversity, and the complexity of the surrounding environment pose challenges for developing robust nodule segmentation methods. In this study, we propose an efficient end-to-end framework, the multi-encoder-based self-adaptive hard attention network (MESAHA-Net), for precise lung nodule segmentation in CT scans. MESAHA-Net comprises three encoding paths, an attention block, and a decoder block, facilitating the integration of three types of inputs: CT slice patches, forward and backward maximum intensity projection (MIP) images, and region of interest (ROI) masks encompassing the nodule. By employing a novel adaptive hard attention mechanism, MESAHA-Net iteratively performs slice-by-slice 2D segmentation 
&lt;/p&gt;</description></item></channel></rss>