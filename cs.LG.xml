<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20351;&#29992;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;ante-hoc&#26041;&#27861;&#23558;&#20020;&#24202;&#27010;&#24565;&#24341;&#20837;&#21040;&#20998;&#31867;&#31649;&#36947;&#20013;&#65292;&#25552;&#20379;&#20102;&#32954;&#30284;&#26816;&#27979;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#30456;&#36739;&#20110;&#22522;&#32447;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65288;F1 &gt; 0.9&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.19444</link><description>&lt;p&gt;
&#36879;&#26126;&#19988;&#20020;&#24202;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#33016;&#37096;X&#23556;&#32447;&#32954;&#30284;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transparent and Clinically Interpretable AI for Lung Cancer Detection in Chest X-Rays
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19444
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;ante-hoc&#26041;&#27861;&#23558;&#20020;&#24202;&#27010;&#24565;&#24341;&#20837;&#21040;&#20998;&#31867;&#31649;&#36947;&#20013;&#65292;&#25552;&#20379;&#20102;&#32954;&#30284;&#26816;&#27979;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#30456;&#36739;&#20110;&#22522;&#32447;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65288;F1 &gt; 0.9&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19444v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#31616;&#35201;&#25688;&#35201;&#65306;&#36879;&#26126;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#27491;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#26088;&#22312;&#35299;&#20915;&#22797;&#26434;&#40657;&#21283;&#23376;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#20449;&#20219;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#20107;&#21518;XAI&#25216;&#26415;&#26368;&#36817;&#24050;&#34987;&#35777;&#26126;&#22312;&#21307;&#30103;&#25968;&#25454;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#35299;&#37322;&#65292;&#19981;&#36866;&#21512;&#20020;&#24202;&#20351;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#30340;ante-hoc&#26041;&#27861;&#65292;&#39318;&#27425;&#23558;&#20020;&#24202;&#27010;&#24565;&#24341;&#20837;&#20998;&#31867;&#31649;&#36947;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#28145;&#20837;&#20102;&#35299;&#20915;&#31574;&#36807;&#31243;&#12290;&#22312;&#19968;&#20010;&#22823;&#22411;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#32858;&#28966;&#20110;&#33016;&#37096;X&#23556;&#32447;&#21644;&#30456;&#20851;&#21307;&#30103;&#25253;&#21578;&#30340;&#20108;&#20803;&#20998;&#31867;&#20219;&#21153;&#65292;&#21363;&#32954;&#30284;&#30340;&#26816;&#27979;&#12290;&#19982;&#22522;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32954;&#30284;&#26816;&#27979;&#20013;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#65288;F1 &gt; 0.9&#65289;&#65292;&#21516;&#26102;&#29983;&#25104;&#20102;&#20020;&#24202;&#30456;&#20851;&#19988;&#26356;&#21487;&#38752;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19444v1 Announce Type: new  Abstract: The rapidly advancing field of Explainable Artificial Intelligence (XAI) aims to tackle the issue of trust regarding the use of complex black-box deep learning models in real-world applications. Existing post-hoc XAI techniques have recently been shown to have poor performance on medical data, producing unreliable explanations which are infeasible for clinical use. To address this, we propose an ante-hoc approach based on concept bottleneck models which introduces for the first time clinical concepts into the classification pipeline, allowing the user valuable insight into the decision-making process. On a large public dataset of chest X-rays and associated medical reports, we focus on the binary classification task of lung cancer detection. Our approach yields improved classification performance in lung cancer detection when compared to baseline deep learning models (F1 &gt; 0.9), while also generating clinically relevant and more reliable
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#33258;&#32447;&#24615;&#20195;&#25968;&#38646;&#31354;&#38388;&#27010;&#24565;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#40065;&#26834;&#24615;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#36817;&#20284;&#38646;&#31354;&#38388;&#20803;&#32032;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10476</link><description>&lt;p&gt;
&#22686;&#24378;&#40065;&#26834;&#24615;&#30340;&#36817;&#20284;&#38646;&#31354;&#38388;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#29992;&#20110;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Approximate Nullspace Augmented Finetuning for Robust Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10476
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21551;&#21457;&#33258;&#32447;&#24615;&#20195;&#25968;&#38646;&#31354;&#38388;&#27010;&#24565;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#40065;&#26834;&#24615;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#36817;&#20284;&#38646;&#31354;&#38388;&#20803;&#32032;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViTs&#65289;&#39046;&#22495;&#20013;&#65292;&#23545;&#20110;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#21551;&#21457;&#33258;&#32447;&#24615;&#20195;&#25968;&#20013;&#38646;&#31354;&#38388;&#27010;&#24565;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#40065;&#26834;&#24615;&#22686;&#24378;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#19968;&#20010;&#38382;&#39064;&#19978;&#65292;&#21363;&#35270;&#35273;&#21464;&#25442;&#22120;&#26159;&#21542;&#21487;&#20197;&#23637;&#29616;&#20986;&#31867;&#20284;&#20110;&#32447;&#24615;&#26144;&#23556;&#20013;&#30340;&#38646;&#31354;&#38388;&#23646;&#24615;&#30340;&#36755;&#20837;&#21464;&#21270;&#38887;&#24615;&#65292;&#36825;&#24847;&#21619;&#30528;&#20174;&#35813;&#38646;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#25200;&#21160;&#28155;&#21152;&#21040;&#36755;&#20837;&#26102;&#19981;&#20250;&#24433;&#21709;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#35768;&#22810;&#39044;&#35757;&#32451;&#30340;ViTs&#65292;&#23384;&#22312;&#19968;&#20010;&#38750;&#24179;&#20961;&#30340;&#38646;&#31354;&#38388;&#65292;&#36825;&#26159;&#30001;&#20110;&#23384;&#22312;&#20462;&#34917;&#23884;&#20837;&#23618;&#12290;&#20854;&#27425;&#65292;&#30001;&#20110;&#38646;&#31354;&#38388;&#26159;&#19982;&#32447;&#24615;&#20195;&#25968;&#30456;&#20851;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#21033;&#29992;&#20248;&#21270;&#31574;&#30053;&#20026;ViTs&#30340;&#38750;&#32447;&#24615;&#22359;&#21512;&#25104;&#36817;&#20284;&#38646;&#31354;&#38388;&#20803;&#32032;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32454;&#33268;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10476v1 Announce Type: cross  Abstract: Enhancing the robustness of deep learning models, particularly in the realm of vision transformers (ViTs), is crucial for their real-world deployment. In this work, we provide a finetuning approach to enhance the robustness of vision transformers inspired by the concept of nullspace from linear algebra. Our investigation centers on whether a vision transformer can exhibit resilience to input variations akin to the nullspace property in linear mappings, implying that perturbations sampled from this nullspace do not influence the model's output when added to the input. Firstly, we show that for many pretrained ViTs, a non-trivial nullspace exists due to the presence of the patch embedding layer. Secondly, as nullspace is a concept associated with linear algebra, we demonstrate that it is possible to synthesize approximate nullspace elements for the non-linear blocks of ViTs employing an optimisation strategy. Finally, we propose a fine-t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21160;&#21147;&#23398;&#32593;&#32476;&#20026;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#29983;&#25104; manipulator &#20960;&#20309;&#35774;&#35745;&#65292;&#36890;&#36807;&#35774;&#35745;&#30446;&#26631;&#26500;&#24314;&#30340;&#26799;&#24230;&#24341;&#23548;&#25163;&#25351;&#20960;&#20309;&#35774;&#35745;&#30340;&#23436;&#21892;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.15038</link><description>&lt;p&gt;
&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26426;&#22120;&#20154; manipulator &#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Dynamics-Guided Diffusion Model for Robot Manipulator Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15038
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#20849;&#20139;&#30340;&#21160;&#21147;&#23398;&#32593;&#32476;&#20026;&#19981;&#21516;&#25805;&#20316;&#20219;&#21153;&#29983;&#25104; manipulator &#20960;&#20309;&#35774;&#35745;&#65292;&#36890;&#36807;&#35774;&#35745;&#30446;&#26631;&#26500;&#24314;&#30340;&#26799;&#24230;&#24341;&#23548;&#25163;&#25351;&#20960;&#20309;&#35774;&#35745;&#30340;&#23436;&#21892;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21160;&#24577;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26694;&#26550;&#65292;&#29992;&#20110;&#20026;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#29983;&#25104; manipulator &#20960;&#20309;&#35774;&#35745;&#12290;&#19982;&#20026;&#27599;&#20010;&#20219;&#21153;&#35757;&#32451;&#19981;&#21516;&#30340;&#35774;&#35745;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#19968;&#20010;&#36328;&#20219;&#21153;&#20849;&#20139;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#32593;&#32476;&#12290;&#23545;&#20110;&#26032;&#30340;&#25805;&#20316;&#20219;&#21153;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#31216;&#20026;&#30446;&#26631;&#30456;&#20114;&#20316;&#29992;&#37197;&#32622;&#25991;&#20214;&#30340;&#20010;&#21035;&#36816;&#21160;&#30446;&#26631;&#65292;&#20854;&#20013;&#27599;&#20010;&#20010;&#21035;&#36816;&#21160;&#21487;&#20197;&#30001;&#20849;&#20139;&#30340;&#21160;&#21147;&#23398;&#32593;&#32476;&#24314;&#27169;&#12290;&#20174;&#30446;&#26631;&#21644;&#39044;&#27979;&#30340;&#30456;&#20114;&#20316;&#29992;&#37197;&#32622;&#25991;&#20214;&#26500;&#24314;&#30340;&#35774;&#35745;&#30446;&#26631;&#20026;&#20219;&#21153;&#30340;&#25163;&#25351;&#20960;&#20309;&#35774;&#35745;&#25552;&#20379;&#20102;&#26799;&#24230;&#24341;&#23548;&#12290;&#36825;&#20010;&#35774;&#35745;&#36807;&#31243;&#34987;&#25191;&#34892;&#20026;&#19968;&#31181;&#20998;&#31867;&#22120;&#24341;&#23548;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#20013;&#35774;&#35745;&#30446;&#26631;&#20316;&#20026;&#20998;&#31867;&#22120;&#24341;&#23548;&#12290;&#25105;&#20204;&#22312;&#21482;&#20351;&#29992;&#24320;&#29615;&#24179;&#34892;&#22841;&#29226;&#36816;&#21160;&#30340;&#26080;&#20256;&#24863;&#22120;&#35774;&#32622;&#19979;&#65292;&#22312;&#21508;&#31181;&#25805;&#20316;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15038v1 Announce Type: cross  Abstract: We present Dynamics-Guided Diffusion Model, a data-driven framework for generating manipulator geometry designs for a given manipulation task. Instead of training different design models for each task, our approach employs a learned dynamics network shared across tasks. For a new manipulation task, we first decompose it into a collection of individual motion targets which we call target interaction profile, where each individual motion can be modeled by the shared dynamics network. The design objective constructed from the target and predicted interaction profiles provides a gradient to guide the refinement of finger geometry for the task. This refinement process is executed as a classifier-guided diffusion process, where the design objective acts as the classifier guidance. We evaluate our framework on various manipulation tasks, under the sensor-less setting using only an open-loop parallel jaw motion. Our generated designs outperfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#30340;&#26368;&#31616;&#21270;&#30340;VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#35780;&#20272;&#20102;beta&#19982;VAE&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21518;&#39564;&#22349;&#32553;&#21644;&#29575;&#22833;&#30495;&#26354;&#32447;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;beta&#30340;&#22686;&#21152;&#65292;&#20135;&#29983;&#36739;&#22823;&#30340;&#24191;&#20041;&#35823;&#24046;&#24179;&#21488;&#65292;&#24182;&#19988;&#36873;&#25321;&#19968;&#20010;&#23567;&#20110;&#29305;&#23450;&#38408;&#20540;&#30340;beta&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.07663</link><description>&lt;p&gt;
&#32447;&#24615;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#29575;&#22833;&#30495;&#26354;&#32447;&#21644;&#21518;&#39564;&#22349;&#32553;&#38408;&#20540;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Dataset Size Dependence of Rate-Distortion Curve and Threshold of Posterior Collapse in Linear VAE. (arXiv:2309.07663v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.07663
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#30340;&#26368;&#31616;&#21270;&#30340;VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#35780;&#20272;&#20102;beta&#19982;VAE&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21518;&#39564;&#22349;&#32553;&#21644;&#29575;&#22833;&#30495;&#26354;&#32447;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#38543;&#30528;beta&#30340;&#22686;&#21152;&#65292;&#20135;&#29983;&#36739;&#22823;&#30340;&#24191;&#20041;&#35823;&#24046;&#24179;&#21488;&#65292;&#24182;&#19988;&#36873;&#25321;&#19968;&#20010;&#23567;&#20110;&#29305;&#23450;&#38408;&#20540;&#30340;beta&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#20013;&#65292;&#21464;&#20998;&#21518;&#39564;&#32463;&#24120;&#19982;&#20808;&#39564;&#23494;&#20999;&#21563;&#21512;&#65292;&#36825;&#34987;&#31216;&#20026;&#21518;&#39564;&#22349;&#32553;&#65292;&#24433;&#21709;&#20102;&#34920;&#31034;&#23398;&#20064;&#30340;&#36136;&#37327;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;VAE&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#21487;&#35843;&#33410;&#30340;&#36229;&#21442;&#25968;beta&#12290;&#26412;&#25991;&#36890;&#36807;&#22312;&#39640;&#32500;&#38480;&#21046;&#19979;&#20998;&#26512;&#26368;&#31616;&#21270;&#30340;VAE&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#34920;&#36798;&#24335;&#65292;&#35780;&#20272;&#20102;beta&#19982;VAE&#20013;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#21518;&#39564;&#22349;&#32553;&#21644;&#29575;&#22833;&#30495;&#26354;&#32447;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#36739;&#22823;&#30340;beta&#20250;&#20135;&#29983;&#19968;&#20010;&#38271;&#30340;&#24191;&#20041;&#35823;&#24046;&#24179;&#21488;&#12290;&#38543;&#30528;beta&#30340;&#22686;&#21152;&#65292;&#24179;&#21488;&#30340;&#38271;&#24230;&#24310;&#38271;&#65292;&#36229;&#36807;&#19968;&#23450;&#30340;&#38408;&#20540;&#21518;&#21464;&#20026;&#26080;&#31351;&#12290;&#36825;&#24847;&#21619;&#30528;&#19982;&#36890;&#24120;&#30340;&#27491;&#21017;&#21270;&#21442;&#25968;&#19981;&#21516;&#65292;beta&#30340;&#36873;&#25321;&#21487;&#33021;&#20250;&#23548;&#33268;&#21518;&#39564;&#22349;&#32553;&#65292;&#32780;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#26080;&#20851;&#12290;&#22240;&#27492;&#65292;beta&#26159;&#19968;&#20010;&#38656;&#35201;&#35880;&#24910;&#35843;&#25972;&#30340;&#39118;&#38505;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#29575;&#22833;&#30495;&#26354;&#32447;&#30340;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#21457;&#29616;&#23384;&#22312;&#19968;&#20010;&#19982;&#25968;&#25454;&#38598;&#22823;&#23567;&#30456;&#20851;&#30340;&#38408;&#20540;&#65292;&#36873;&#25321;&#23567;&#20110;&#36825;&#20010;&#38408;&#20540;&#30340;beta&#20540;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the Variational Autoencoder (VAE), the variational posterior often aligns closely with the prior, which is known as posterior collapse and hinders the quality of representation learning. To mitigate this problem, an adjustable hyperparameter beta has been introduced in the VAE. This paper presents a closed-form expression to assess the relationship between the beta in VAE, the dataset size, the posterior collapse, and the rate-distortion curve by analyzing a minimal VAE in a high-dimensional limit. These results clarify that a long plateau in the generalization error emerges with a relatively larger beta. As the beta increases, the length of the plateau extends and then becomes infinite beyond a certain beta threshold. This implies that the choice of beta, unlike the usual regularization parameters, can induce posterior collapse regardless of the dataset size. Thus, beta is a risky parameter that requires careful tuning. Furthermore, considering the dataset-size dependence on the ra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#21644;&#25552;&#39640;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36873;&#25321;&#27010;&#29575;&#26435;&#37325;&#23545;&#38544;&#31169;&#25918;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#22312;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.10187</link><description>&lt;p&gt;
&#38544;&#31169;&#25918;&#22823;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Privacy Amplification via Importance Sampling. (arXiv:2307.10187v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10187
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#65292;&#21487;&#20197;&#21516;&#26102;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#21644;&#25552;&#39640;&#25928;&#29992;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36873;&#25321;&#27010;&#29575;&#26435;&#37325;&#23545;&#38544;&#31169;&#25918;&#22823;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#22312;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#26356;&#22909;&#30340;&#38544;&#31169;&#21644;&#25928;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#37325;&#35201;&#24615;&#37319;&#26679;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#23376;&#37319;&#26679;&#20316;&#20026;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#30340;&#24615;&#36136;&#12290;&#36825;&#25193;&#23637;&#20102;&#24050;&#26377;&#30340;&#36890;&#36807;&#23376;&#37319;&#26679;&#36827;&#34892;&#38544;&#31169;&#25918;&#22823;&#30340;&#32467;&#26524;&#21040;&#37325;&#35201;&#24615;&#37319;&#26679;&#65292;&#20854;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#26435;&#37325;&#20026;&#20854;&#34987;&#36873;&#25321;&#27010;&#29575;&#30340;&#20498;&#25968;&#12290;&#27599;&#20010;&#28857;&#30340;&#36873;&#25321;&#27010;&#29575;&#30340;&#26435;&#37325;&#23545;&#38544;&#31169;&#30340;&#24433;&#21709;&#24182;&#19981;&#26126;&#26174;&#12290;&#19968;&#26041;&#38754;&#65292;&#36739;&#20302;&#30340;&#36873;&#25321;&#27010;&#29575;&#20250;&#23548;&#33268;&#26356;&#24378;&#30340;&#38544;&#31169;&#25918;&#22823;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26435;&#37325;&#36234;&#39640;&#65292;&#22312;&#28857;&#34987;&#36873;&#25321;&#26102;&#65292;&#28857;&#23545;&#26426;&#21046;&#36755;&#20986;&#30340;&#24433;&#21709;&#23601;&#36234;&#24378;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#33324;&#30340;&#32467;&#26524;&#26469;&#37327;&#21270;&#36825;&#20004;&#20010;&#24433;&#21709;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24322;&#36136;&#37319;&#26679;&#27010;&#29575;&#21487;&#20197;&#21516;&#26102;&#27604;&#22343;&#21248;&#23376;&#37319;&#26679;&#20855;&#26377;&#26356;&#24378;&#30340;&#38544;&#31169;&#21644;&#26356;&#22909;&#30340;&#25928;&#29992;&#65292;&#24182;&#20445;&#25345;&#23376;&#37319;&#26679;&#22823;&#23567;&#19981;&#21464;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21046;&#23450;&#21644;&#35299;&#20915;&#20102;&#38544;&#31169;&#20248;&#21270;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#21363;&#23547;&#25214;...
&lt;/p&gt;
&lt;p&gt;
We examine the privacy-enhancing properties of subsampling a data set via importance sampling as a pre-processing step for differentially private mechanisms. This extends the established privacy amplification by subsampling result to importance sampling where each data point is weighted by the reciprocal of its selection probability. The implications for privacy of weighting each point are not obvious. On the one hand, a lower selection probability leads to a stronger privacy amplification. On the other hand, the higher the weight, the stronger the influence of the point on the output of the mechanism in the event that the point does get selected. We provide a general result that quantifies the trade-off between these two effects. We show that heterogeneous sampling probabilities can lead to both stronger privacy and better utility than uniform subsampling while retaining the subsample size. In particular, we formulate and solve the problem of privacy-optimal sampling, that is, finding
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340; QST &#26041;&#27861;&#65292;&#21487;&#25429;&#25417;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#32034;&#37327;&#23376;&#24577;&#30340;&#23494;&#24230;&#30697;&#38453;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21463;&#38480;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.05433</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#29992;&#20110;&#37327;&#23376;&#24577;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Attention-Based Transformer Networks for Quantum State Tomography. (arXiv:2305.05433v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340; QST &#26041;&#27861;&#65292;&#21487;&#25429;&#25417;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25104;&#21151;&#24212;&#29992;&#20110;&#26816;&#32034;&#37327;&#23376;&#24577;&#30340;&#23494;&#24230;&#30697;&#38453;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21463;&#38480;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#33391;&#22909;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#31070;&#32463;&#32593;&#32476;&#19968;&#30452;&#34987;&#29992;&#20110;&#37327;&#23376;&#24577;&#37325;&#26500;&#65288;QST&#65289;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#37325;&#26500;&#37327;&#23376;&#24577;&#30340;&#25928;&#29575;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#35821;&#35328;&#24314;&#27169;&#19982;&#37327;&#23376;&#24577;&#37325;&#26500;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340; QST &#26041;&#27861;&#65292;&#29992;&#20110;&#25429;&#25417;&#19981;&#21516;&#27979;&#37327;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30452;&#25509;&#20174;&#27979;&#37327;&#32479;&#35745;&#25968;&#25454;&#20013;&#26816;&#32034;&#37327;&#23376;&#24577;&#30340;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#36741;&#21161;&#20351;&#29992;&#32508;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#24110;&#21161;&#26368;&#23567;&#21270;&#23454;&#38469;&#24577;&#19982;&#26816;&#32034;&#24577;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#36319;&#36394;&#20102;&#28041;&#21450;&#21508;&#31181;&#21442;&#25968;&#35843;&#25972;&#30340;&#24120;&#35265;&#35757;&#32451;&#31574;&#30053;&#23545;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340; QST &#26041;&#27861;&#30340;&#19981;&#21516;&#24433;&#21709;&#12290;&#32467;&#21512;&#36825;&#20123;&#25216;&#26415;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#22522;&#20934;&#32447;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#37325;&#26500;&#32431;&#24577;&#21644;&#28151;&#21512;&#24577;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21463;&#38480;&#27979;&#37327;&#25968;&#25454;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been actively explored for quantum state tomography (QST) due to their favorable expressibility. To further enhance the efficiency of reconstructing quantum states, we explore the similarity between language modeling and quantum state tomography and propose an attention-based QST method that utilizes the Transformer network to capture the correlations between measured results from different measurements. Our method directly retrieves the density matrices of quantum states from measured statistics, with the assistance of an integrated loss function that helps minimize the difference between the actual states and the retrieved states. Then, we systematically trace different impacts within a bag of common training strategies involving various parameter adjustments on the attention-based QST method. Combining these techniques, we establish a robust baseline that can efficiently reconstruct pure and mixed quantum states. Furthermore, by comparing the performance of thre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#33021;&#28304;&#39640;&#25928;&#24615;&#21644;&#23454;&#26102;&#24615;&#30340;&#31038;&#20132;&#27675;&#22260;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;ERSAM&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#36866;&#21512;SAM&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#28385;&#36275;&#33021;&#28304;&#25928;&#29575;&#12289;&#23454;&#26102;&#22788;&#29702;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#35201;&#27714;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.10727</link><description>&lt;p&gt;
ERSAM: &#38754;&#21521;&#33021;&#28304;&#39640;&#25928;&#21644;&#23454;&#26102;&#31038;&#20132;&#27675;&#22260;&#27979;&#37327;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time Social Ambiance Measurement. (arXiv:2303.10727v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#33021;&#28304;&#39640;&#25928;&#24615;&#21644;&#23454;&#26102;&#24615;&#30340;&#31038;&#20132;&#27675;&#22260;&#27979;&#37327;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;ERSAM&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#36866;&#21512;SAM&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#28385;&#36275;&#33021;&#28304;&#25928;&#29575;&#12289;&#23454;&#26102;&#22788;&#29702;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#35201;&#27714;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35813;&#26694;&#26550;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#27675;&#22260;&#25551;&#36848;&#20102;&#31038;&#20132;&#20114;&#21160;&#21457;&#29983;&#30340;&#32972;&#26223;&#65292;&#21487;&#20197;&#20351;&#29992;&#35821;&#38899;&#38899;&#39057;&#36890;&#36807;&#35745;&#31639;&#21516;&#26102;&#21457;&#35328;&#32773;&#30340;&#25968;&#37327;&#26469;&#27979;&#37327;&#12290;&#36825;&#31181;&#27979;&#37327;&#24050;&#32463;&#23454;&#29616;&#20102;&#21508;&#31181;&#24515;&#29702;&#20581;&#24247;&#36319;&#36394;&#21644;&#38754;&#21521;&#20154;&#31867;&#30340;&#29289;&#32852;&#32593;&#24212;&#29992;&#12290;&#34429;&#28982;&#35774;&#22791;&#19978;&#30340;&#31038;&#20132;&#27675;&#22260;&#27979;&#37327; (SAM) &#38750;&#24120;&#29702;&#24819;&#65292;&#20197;&#30830;&#20445;&#29992;&#25143;&#38544;&#31169;&#24182;&#20419;&#36827;&#19978;&#36848;&#24212;&#29992;&#30340;&#24191;&#27867;&#37319;&#29992;&#65292;&#20294;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#39537;&#21160;&#30340;SAM&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#31227;&#21160;&#35774;&#22791;&#19978;&#30340;&#24120;&#35265;&#36164;&#28304;&#30456;&#30683;&#30462;&#12290;&#27492;&#22806;&#65292;&#22312;&#20020;&#24202;&#35774;&#32622;&#19979;&#65292;&#30001;&#20110;&#21508;&#31181;&#38544;&#31169;&#38480;&#21046;&#21644;&#25152;&#38656;&#30340;&#20154;&#21147;&#21171;&#21160;&#65292;&#21482;&#26377;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#25110;&#23454;&#38469;&#21487;&#34892;&#65292;&#36825;&#36827;&#19968;&#27493;&#25361;&#25112;&#20102;&#35774;&#22791;&#19978;SAM&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#23454;&#29616;&#20934;&#30830;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#38754;&#21521;&#33021;&#28304;&#39640;&#25928;&#21644;&#23454;&#26102;SAM&#30340;ERSAM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;ERSAM&#26694;&#26550;&#21487;&#20197;&#33258;&#21160;&#25628;&#32034;&#36866;&#21512;SAM&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#24182;&#28385;&#36275;&#33021;&#28304;&#25928;&#29575;&#12289;&#23454;&#26102;&#22788;&#29702;&#21644;&#26377;&#38480;&#26631;&#31614;&#25968;&#25454;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;ERSAM&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;SAM&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#39640;&#20102;&#31934;&#24230;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social ambiance describes the context in which social interactions happen, and can be measured using speech audio by counting the number of concurrent speakers. This measurement has enabled various mental health tracking and human-centric IoT applications. While on-device Socal Ambiance Measure (SAM) is highly desirable to ensure user privacy and thus facilitate wide adoption of the aforementioned applications, the required computational complexity of state-of-the-art deep neural networks (DNNs) powered SAM solutions stands at odds with the often constrained resources on mobile devices. Furthermore, only limited labeled data is available or practical when it comes to SAM under clinical settings due to various privacy constraints and the required human effort, further challenging the achievable accuracy of on-device SAM solutions. To this end, we propose a dedicated neural architecture search framework for Energy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM framework can
&lt;/p&gt;</description></item></channel></rss>