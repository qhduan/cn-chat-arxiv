<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2312.15101</link><description>&lt;p&gt;
&#20462;&#22797;-Con&#65306;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#30340;&#33258;&#21160;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
Fix-Con: Automatic Fault Localization and Repair of Deep Learning Model Conversions
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.15101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;Fix-Con&#65292;&#29992;&#20110;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36716;&#25442;&#36807;&#31243;&#20013;&#20462;&#22797;&#30001;&#36716;&#25442;&#24341;&#20837;&#30340;&#25925;&#38556;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#26041;&#38754;&#30340;&#25925;&#38556;&#65292;&#25552;&#39640;&#36716;&#25442;&#27169;&#22411;&#30340;&#37096;&#32626;&#21644;&#39044;&#27979;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#21516;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#27493;&#39588;&#65292;&#21487;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#22686;&#21152;&#27169;&#22411;&#22312;&#35774;&#22791;&#20043;&#38388;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#21033;&#29992;&#21487;&#33021;&#21482;&#22312;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20013;&#25552;&#20379;&#30340;&#20248;&#21270;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36716;&#25442;&#36807;&#31243;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#65292;&#23548;&#33268;&#36716;&#25442;&#21518;&#30340;&#27169;&#22411;&#26080;&#27861;&#37096;&#32626;&#25110;&#23384;&#22312;&#38382;&#39064;&#65292;&#20005;&#37325;&#38477;&#20302;&#20102;&#20854;&#39044;&#27979;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#21270;&#30340;&#25925;&#38556;&#23450;&#20301;&#21644;&#20462;&#22797;&#26041;&#27861;&#65292;Fix-Con&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#20043;&#38388;&#36827;&#34892;&#27169;&#22411;&#36716;&#25442;&#26102;&#20351;&#29992;&#12290;Fix-Con&#33021;&#22815;&#26816;&#27979;&#21644;&#20462;&#22797;&#22312;&#36716;&#25442;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#27169;&#22411;&#36755;&#20837;&#12289;&#21442;&#25968;&#12289;&#36229;&#21442;&#25968;&#21644;&#27169;&#22411;&#22270;&#30340;&#25925;&#38556;&#12290;Fix-Con&#20351;&#29992;&#20174;&#35843;&#26597;&#36716;&#25442;&#38382;&#39064;&#20013;&#25366;&#25496;&#20986;&#30340;&#19968;&#32452;&#25925;&#38556;&#31867;&#22411;&#26469;&#23450;&#20301;&#36716;&#25442;&#27169;&#22411;&#20013;&#28508;&#22312;&#30340;&#36716;&#25442;&#25925;&#38556;&#65292;&#24182;&#36866;&#24403;&#20462;&#22797;&#23427;&#20204;&#65292;&#20363;&#22914;&#20351;&#29992;&#28304;&#27169;&#22411;&#30340;&#21442;&#25968;&#26367;&#25442;&#30446;&#26631;&#27169;&#22411;&#30340;&#21442;&#25968;&#12290;&#36825;&#19968;&#36807;&#31243;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#22270;&#20687;&#19978;&#36827;&#34892;&#36845;&#20195;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
Converting deep learning models between frameworks is a common step to maximize model compatibility across devices and leverage optimization features that may be exclusively provided in one deep learning framework. However, this conversion process may be riddled with bugs, making the converted models either undeployable or problematic, considerably degrading their prediction correctness.   We propose an automated approach for fault localization and repair, Fix-Con, during model conversion between deep learning frameworks. Fix-Con is capable of detecting and fixing faults introduced in model input, parameters, hyperparameters, and the model graph during conversion.   Fix-Con uses a set of fault types mined from surveying conversion issues raised to localize potential conversion faults in the converted target model, and then repairs them appropriately, e.g. replacing the parameters of the target model with those from the source model. This is done iteratively for every image in the datas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.09066</link><description>&lt;p&gt;
Continual Learning&#20013;&#30340;&#36229;&#21442;&#25968;&#65306;&#29616;&#23454;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Hyperparameters in Continual Learning: a Reality Check
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09066
&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#23545;&#20110;&#36830;&#32493;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#34987;&#24378;&#35843;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#35780;&#20272;&#38454;&#27573;&#30340;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;&#30340;&#36830;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#31639;&#27861;&#26088;&#22312;&#22312;CL&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#32531;&#35299;&#31283;&#23450;&#24615;&#21644;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35843;&#25972;&#27599;&#31181;&#31639;&#27861;&#30340;&#36866;&#24403;&#36229;&#21442;&#25968;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#26412;&#25991;&#20027;&#24352;&#29616;&#34892;&#30340;&#35780;&#20272;&#21327;&#35758;&#26082;&#19981;&#20999;&#23454;&#38469;&#65292;&#20063;&#26080;&#27861;&#26377;&#25928;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09066v1 Announce Type: new  Abstract: Various algorithms for continual learning (CL) have been designed with the goal of effectively alleviating the trade-off between stability and plasticity during the CL process. To achieve this goal, tuning appropriate hyperparameters for each algorithm is essential. As an evaluation protocol, it has been common practice to train a CL algorithm using diverse hyperparameter values on a CL scenario constructed with a benchmark dataset. Subsequently, the best performance attained with the optimal hyperparameter value serves as the criterion for evaluating the CL algorithm. In this paper, we contend that this evaluation protocol is not only impractical but also incapable of effectively assessing the CL capability of a CL algorithm. Returning to the fundamental principles of model evaluation in machine learning, we propose an evaluation protocol that involves Hyperparameter Tuning and Evaluation phases. Those phases consist of different datase
&lt;/p&gt;</description></item><item><title>&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.08674</link><description>&lt;p&gt;
&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20986;&#29616;&#20154;&#31867;&#35838;&#31243;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Human Curriculum Effects Emerge with In-Context Learning in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08674
&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#31034;&#20363;&#35838;&#31243;&#21644;&#20219;&#21153;&#32467;&#26500;&#26377;&#25935;&#24863;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#21644;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;&#33719;&#24471;&#20998;&#32452;&#21644;&#20132;&#38169;&#35757;&#32451;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#23398;&#20064;&#23545;&#35268;&#21017;&#32467;&#26500;&#21644;&#35757;&#32451;&#20013;&#25152;&#20351;&#29992;&#30340;&#31034;&#20363;&#35838;&#31243;&#38750;&#24120;&#25935;&#24863;&#12290;&#22312;&#30001;&#31616;&#27905;&#35268;&#21017;&#25511;&#21046;&#30340;&#20219;&#21153;&#20013;&#65292;&#24403;&#30456;&#20851;&#31034;&#20363;&#22312;&#22810;&#27425;&#35797;&#39564;&#20013;&#34987;&#20998;&#32452;&#26102;&#65292;&#23398;&#20064;&#26356;&#21152;&#31283;&#20581;&#65307;&#20294;&#22312;&#32570;&#20047;&#36825;&#26679;&#30340;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#65292;&#20132;&#38169;&#35757;&#32451;&#26356;&#21152;&#26377;&#25928;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#27809;&#26377;&#31070;&#32463;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#25429;&#25417;&#21040;&#36825;&#20123;&#30475;&#20284;&#30683;&#30462;&#30340;&#25928;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#19978;&#19979;&#25991;&#23398;&#20064;&#8221;&#65288;ICL&#65289;&#22312;&#20351;&#29992;&#20803;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#33258;&#21457;&#20135;&#29983;&#20102;&#21516;&#26679;&#30340;&#26435;&#34913;&#12290;ICL&#26159;&#36890;&#36807;&#20869;&#23618;&#24490;&#29615;&#31639;&#27861;&#22312;&#28608;&#27963;&#21160;&#21147;&#23398;&#20013;&#23454;&#29616;&#30340;&#19968;&#31181;&#8220;&#19978;&#19979;&#25991;&#20869;&#23398;&#20064;&#8221;&#65288;in-context learning&#65289;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#26435;&#37325;&#26356;&#25913;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#23545;&#39044;&#35757;&#32451;&#30340;LLMs&#21644;&#20803;&#23398;&#20064;&#21464;&#21387;&#22120;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;ICL&#22312;&#28041;&#21450;&#35268;&#21017;&#32467;&#26500;&#30340;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#20154;&#31867;&#25152;&#31034;&#30340;&#20998;&#32452;&#20248;&#21183;&#65292;&#32780;&#21516;&#26102;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#21017;&#22797;&#21046;&#20102;&#20154;&#31867;&#22312;&#32570;&#23569;&#36825;&#26679;&#32467;&#26500;&#30340;&#20219;&#21153;&#19978;&#25152;&#35266;&#23519;&#21040;&#30340;&#20132;&#38169;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with "in-context learning" (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks "in context" - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29616;&#20195;&#22823;&#20247;&#24066;&#22330;&#24212;&#29992;&#20013;&#65292;&#38543;&#26102;&#38388;&#26356;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36890;&#36807;&#30740;&#31350;&#39044;&#27979;&#24615;&#22810;&#26679;&#24615;&#65292;&#37327;&#21270;&#20102;&#36825;&#31181;&#39044;&#27979;&#24615;&#27969;&#22833;&#65292;&#24182;&#36890;&#36807;Rashomon&#38598;&#21512;&#26469;&#20998;&#26512;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#39044;&#26399;&#27969;&#22833;&#12290;</title><link>https://arxiv.org/abs/2402.07745</link><description>&lt;p&gt;
&#20351;&#29992;&#19968;&#32452;&#22909;&#27169;&#22411;&#36827;&#34892;&#39044;&#27979;&#24615;&#23458;&#25143;&#27969;&#22833;
&lt;/p&gt;
&lt;p&gt;
Predictive Churn with the Set of Good Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29616;&#20195;&#22823;&#20247;&#24066;&#22330;&#24212;&#29992;&#20013;&#65292;&#38543;&#26102;&#38388;&#26356;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#19981;&#31283;&#23450;&#30340;&#39044;&#27979;&#32467;&#26524;&#65292;&#36890;&#36807;&#30740;&#31350;&#39044;&#27979;&#24615;&#22810;&#26679;&#24615;&#65292;&#37327;&#21270;&#20102;&#36825;&#31181;&#39044;&#27979;&#24615;&#27969;&#22833;&#65292;&#24182;&#36890;&#36807;Rashomon&#38598;&#21512;&#26469;&#20998;&#26512;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#39044;&#26399;&#27969;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22823;&#20247;&#24066;&#22330;&#24212;&#29992;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32463;&#24120;&#20250;&#38543;&#26102;&#38388;&#36827;&#34892;&#26356;&#26032;&#12290;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#65292;&#23613;&#31649;&#25972;&#20307;&#24615;&#33021;&#22312;&#25552;&#21319;&#65292;&#20294;&#36825;&#20123;&#26356;&#26032;&#21487;&#33021;&#20250;&#20197;&#19981;&#21487;&#39044;&#27979;&#30340;&#26041;&#24335;&#25913;&#21464;&#29305;&#23450;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#37327;&#21270;&#27169;&#22411;&#26356;&#26032;&#21069;&#21518;&#30340;&#19981;&#31283;&#23450;&#39044;&#27979;&#25968;&#37327;&#26469;&#34913;&#37327;&#39044;&#27979;&#24615;&#27969;&#22833;&#12290;&#26412;&#25991;&#36890;&#36807;&#39044;&#27979;&#24615;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#36825;&#31181;&#25928;&#24212;&#65292;&#21363;&#22312;&#19968;&#32452;&#25509;&#36817;&#26368;&#20248;&#27169;&#22411;&#65288;Rashomon&#38598;&#21512;&#65289;&#20013;&#23384;&#22312;&#20914;&#31361;&#39044;&#27979;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#20256;&#32479;&#30340;&#39044;&#27979;&#24615;&#22810;&#26679;&#24615;&#24230;&#37327;&#26469;&#30740;&#31350;&#36825;&#32452;&#28508;&#22312;&#27169;&#22411;&#30340;&#39044;&#26399;&#27969;&#22833;&#65292;&#21363;&#21487;&#33021;&#29992;&#20110;&#26367;&#25442;&#22522;&#32447;&#27169;&#22411;&#22312;&#37096;&#32626;&#20013;&#30340;&#27169;&#22411;&#38598;&#21512;&#12290;&#25105;&#20204;&#20174;&#19981;&#21516;&#30340;&#35282;&#24230;&#32473;&#20986;&#20102;&#27169;&#22411;&#38598;&#20869;Rashomon&#38598;&#21512;&#20043;&#38388;&#39044;&#26399;&#27969;&#22833;&#30340;&#29702;&#35770;&#32467;&#26524;&#65292;&#24182;&#36890;&#36807;Rashomon&#38598;&#21512;&#34920;&#24449;&#20102;&#27169;&#22411;&#26356;&#26032;&#20013;&#30340;&#39044;&#26399;&#27969;&#22833;&#65292;&#32467;&#21512;&#25105;&#20204;&#30340;&#20998;&#26512;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models in modern mass-market applications are often updated over time. One of the foremost challenges faced is that, despite increasing overall performance, these updates may flip specific model predictions in unpredictable ways. In practice, researchers quantify the number of unstable predictions between models pre and post update -- i.e., predictive churn. In this paper, we study this effect through the lens of predictive multiplicity -- i.e., the prevalence of conflicting predictions over the set of near-optimal models (the Rashomon set). We show how traditional measures of predictive multiplicity can be used to examine expected churn over this set of prospective models -- i.e., the set of models that may be used to replace a baseline model in deployment. We present theoretical results on the expected churn between models within the Rashomon set from different perspectives. And we characterize expected churn over model updates via the Rashomon set, pairing our analy
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;</title><link>http://arxiv.org/abs/2310.16945</link><description>&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection&#65288;CATE&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#22240;&#26524;Q&#38598;&#25104;&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Q-Aggregation for CATE Model Selection. (arXiv:2310.16945v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16945
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q&#38598;&#25104;&#30340;CATE&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#65292;&#20854;&#36890;&#36807;&#20351;&#29992;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#23454;&#29616;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20339;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#36951;&#25022;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#26465;&#20214;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;CATE&#65289;&#26159;&#20010;&#24615;&#21270;&#20915;&#31574;&#30340;&#26680;&#24515;&#12290;&#23613;&#31649;&#26377;&#22823;&#37327;&#29992;&#20110;CATE&#20272;&#35745;&#30340;&#27169;&#22411;&#65292;&#20294;&#30001;&#20110;&#22240;&#26524;&#25512;&#26029;&#30340;&#22522;&#26412;&#38382;&#39064;&#65292;&#27169;&#22411;&#36873;&#25321;&#26159;&#19968;&#39033;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#30340;&#23454;&#35777;&#24037;&#20316;&#25552;&#20379;&#20102;&#26377;&#21033;&#20110;&#20855;&#26377;&#21452;&#37325;&#40065;&#26834;&#24615;&#36136;&#30340;&#20195;&#29702;&#25439;&#22833;&#24230;&#37327;&#21644;&#27169;&#22411;&#38598;&#25104;&#30340;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#29702;&#35770;&#29702;&#35299;&#36824;&#19981;&#22815;&#12290;&#30452;&#25509;&#24212;&#29992;&#20808;&#21069;&#30340;&#29702;&#35770;&#24037;&#20316;&#20250;&#30001;&#20110;&#27169;&#22411;&#36873;&#25321;&#38382;&#39064;&#30340;&#38750;&#20984;&#24615;&#32780;&#23548;&#33268;&#27425;&#20248;&#30340;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#29575;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29616;&#26377;&#20027;&#35201;CATE&#38598;&#25104;&#26041;&#27861;&#30340;&#36951;&#25022;&#29575;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#40065;&#26834;&#25439;&#22833;&#30340;Q&#38598;&#25104;&#30340;&#26032;&#30340;CATE&#27169;&#22411;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#34920;&#26126;&#65292;&#22240;&#26524;Q&#38598;&#25104;&#22312;&#39044;&#27979;&#27169;&#22411;&#36873;&#25321;&#30340;&#36951;&#25022;&#29575;&#19978;&#36798;&#21040;&#20102;&#32479;&#35745;&#19978;&#30340;&#26368;&#20248;&#20540;&#20026;$\frac{\log(M)}{n}$&#65288;&#20854;&#20013;$M$&#20026;&#27169;&#22411;&#25968;&#65292;$n$&#20026;&#26679;&#26412;&#25968;&#65289;&#65292;&#21152;&#19978;&#39640;&#38454;&#20272;&#35745;&#35823;&#24046;&#39033;
&lt;/p&gt;
&lt;p&gt;
Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error term
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.16326</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#22522;&#20934;&#12289;&#22522;&#32447;&#21644;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations. (arXiv:2305.16326v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;GPT-3&#21644;GPT-4&#22312;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#34920;&#29616;&#65292;&#20998;&#26512;&#20102;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#30340;&#38169;&#35823;&#31867;&#22411;&#65292;&#24182;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#25163;&#21160;&#31579;&#36873;&#21644;&#25552;&#21462;&#30693;&#35782;&#21464;&#24471;&#22256;&#38590;&#12290;&#33258;&#21160;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;BioNLP&#65289;&#25216;&#26415;&#26377;&#21161;&#20110;&#20943;&#36731;&#36825;&#31181;&#36127;&#25285;&#12290;&#36817;&#24180;&#26469;&#65292;&#22914;GPT-3&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#21331;&#36234;&#30340;&#24615;&#33021;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#22312;BioNLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#23545;&#26041;&#27861;&#24320;&#21457;&#21644;&#19979;&#28216;&#29992;&#25143;&#30340;&#24433;&#21709;&#20173;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#65288;1&#65289;&#22312;&#22235;&#20010;&#24212;&#29992;&#31243;&#24207;&#20013;&#22312;&#20843;&#20010;BioNLP&#25968;&#25454;&#38598;&#20013;&#24314;&#31435;&#20102;GPT-3&#21644;GPT-4&#22312;&#38646;-shot&#21644;&#19968;-shot&#35774;&#32622;&#19979;&#30340;&#22522;&#20934;&#34920;&#29616;&#65292;&#21253;&#25324;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65292;&#20851;&#31995;&#25552;&#21462;&#65292;&#22810;&#26631;&#31614;&#25991;&#26723;&#20998;&#31867;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#21644;&#25512;&#29702;&#65307;&#65288;2&#65289;&#23457;&#26597;&#20102;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#23558;&#38169;&#35823;&#20998;&#20026;&#19977;&#31181;&#31867;&#22411;&#65306;&#32570;&#22833;&#65292;&#19981;&#19968;&#33268;&#21644;&#19981;&#38656;&#35201;&#30340;&#20154;&#24037;&#20869;&#23481;&#65307;&#65288;3&#65289;&#25552;&#20986;&#20102;&#20351;&#29992;LLMs&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biomedical literature is growing rapidly, making it challenging to curate and extract knowledge manually. Biomedical natural language processing (BioNLP) techniques that can automatically extract information from biomedical literature help alleviate this burden. Recently, large Language Models (LLMs), such as GPT-3 and GPT-4, have gained significant attention for their impressive performance. However, their effectiveness in BioNLP tasks and impact on method development and downstream users remain understudied. This pilot study (1) establishes the baseline performance of GPT-3 and GPT-4 at both zero-shot and one-shot settings in eight BioNLP datasets across four applications: named entity recognition, relation extraction, multi-label document classification, and semantic similarity and reasoning, (2) examines the errors produced by the LLMs and categorized the errors into three types: missingness, inconsistencies, and unwanted artificial content, and (3) provides suggestions for using L
&lt;/p&gt;</description></item><item><title>FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;</title><link>http://arxiv.org/abs/2305.14392</link><description>&lt;p&gt;
FEDORA&#65306;&#29992;&#20110;&#21453;&#24212;&#34892;&#20026;&#30340;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FEDORA: Flying Event Dataset fOr Reactive behAvior. (arXiv:2305.14392v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14392
&lt;/p&gt;
&lt;p&gt;
FEDORA&#26159;&#19968;&#20010;&#39134;&#34892;&#20107;&#20214;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#32570;&#23569;&#23436;&#25972;&#25968;&#25454;&#21644;&#26102;&#38388;&#20998;&#36776;&#29575;&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#24110;&#21161;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#19979;&#23454;&#29616;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20307;&#22312;&#39134;&#34892;&#20013;&#20351;&#29992;&#26497;&#23569;&#25968;&#30340;&#31070;&#32463;&#20803;&#21644;&#26497;&#20302;&#30340;&#22833;&#35823;&#29575;&#25191;&#34892;&#22797;&#26434;&#30340;&#39640;&#36895;&#26426;&#21160;&#65292;&#31361;&#26174;&#20102;&#36825;&#20123;&#36164;&#28304;&#21463;&#38480;&#21046;&#30340;&#29983;&#29289;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#20107;&#20214;&#39537;&#21160;&#30828;&#20214;&#36880;&#28176;&#25104;&#20026;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#23454;&#29616;&#22797;&#26434;&#35270;&#35273;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#35270;&#35273;&#30340;&#33258;&#20027;&#23548;&#33322;&#21644;&#36991;&#38556;&#21253;&#25324;&#20960;&#20010;&#29420;&#31435;&#20294;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#22914;&#20809;&#27969;&#20272;&#35745;&#12289;&#28145;&#24230;&#20272;&#35745;&#12289;&#21516;&#26102;&#23450;&#20301;&#19982;&#24314;&#22270;&#65288;SLAM&#65289;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#35782;&#21035;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#20219;&#21153;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#65292;&#20182;&#20204;&#24517;&#39035;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#25968;&#25454;&#38598;&#21482;&#25552;&#20379;&#25152;&#38656;&#25968;&#25454;&#30340;&#36873;&#23450;&#23376;&#38598;&#65292;&#36825;&#20351;&#24471;&#32593;&#32476;&#38388;&#30340;&#19968;&#33268;&#24615;&#38590;&#20197;&#23454;&#29616;&#12290;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#21478;&#19968;&#20010;&#38480;&#21046;&#26159;&#25552;&#20379;&#30340;&#26377;&#38480;&#26102;&#38388;&#20998;&#36776;&#29575;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FEDORA&#65292;
&lt;/p&gt;
&lt;p&gt;
The ability of living organisms to perform complex high speed manoeuvers in flight with a very small number of neurons and an incredibly low failure rate highlights the efficacy of these resource-constrained biological systems. Event-driven hardware has emerged, in recent years, as a promising avenue for implementing complex vision tasks in resource-constrained environments. Vision-based autonomous navigation and obstacle avoidance consists of several independent but related tasks such as optical flow estimation, depth estimation, Simultaneous Localization and Mapping (SLAM), object detection, and recognition. To ensure coherence between these tasks, it is imperative that they be trained on a single dataset. However, most existing datasets provide only a selected subset of the required data. This makes inter-network coherence difficult to achieve. Another limitation of existing datasets is the limited temporal resolution they provide. To address these limitations, we present FEDORA, a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2303.10430</link><description>&lt;p&gt;
NoisyHate&#65306;&#22312;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#19979;&#23545;&#20869;&#23481;&#23457;&#26680;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
NoisyHate: Benchmarking Content Moderation Machine Learning Models with Human-Written Perturbations Online. (arXiv:2303.10430v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#65292;&#20855;&#26377;&#26377;&#23475;&#20869;&#23481;&#30340;&#22312;&#32447;&#25991;&#26412;&#26159;&#19968;&#31181;&#23041;&#32961;&#65292;&#21487;&#33021;&#20250;&#24341;&#36215;&#32593;&#32476;&#39578;&#25200;&#12290;&#23613;&#31649;&#35768;&#22810;&#24179;&#21488;&#37319;&#21462;&#20102;&#25514;&#26045;&#65292;&#20363;&#22914;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#26469;&#20943;&#23569;&#20854;&#24433;&#21709;&#65292;&#20294;&#37027;&#20123;&#26377;&#23475;&#20869;&#23481;&#21457;&#24067;&#32773;&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20462;&#25913;&#26377;&#23475;&#35789;&#27719;&#30340;&#25340;&#20889;&#26469;&#36867;&#36991;&#31995;&#32479;&#12290;&#36825;&#20123;&#20462;&#25913;&#21518;&#30340;&#21333;&#35789;&#20063;&#31216;&#20026;&#20154;&#31867;&#32534;&#20889;&#30340;&#25991;&#26412;&#25200;&#21160;&#12290;&#35768;&#22810;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#23450;&#30340;&#25216;&#26415;&#26469;&#29983;&#25104;&#23545;&#25239;&#26679;&#26412;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33719;&#24471;&#35782;&#21035;&#36825;&#20123;&#25200;&#21160;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25200;&#21160;&#19982;&#20154;&#31867;&#32534;&#20889;&#30340;&#25200;&#21160;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#31867;&#32534;&#20889;&#30340;&#22312;&#32447;&#25200;&#21160;&#30340;&#22522;&#20934;&#27979;&#35797;&#38598;&#65292;&#29992;&#20110;&#27602;&#24615;&#35328;&#35770;&#26816;&#27979;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25307;&#21215;&#20102;&#19968;&#32452;&#24037;&#20154;&#26469;&#35780;&#20272;&#27492;&#27979;&#35797;&#38598;&#30340;&#36136;&#37327;&#24182;&#21024;&#38500;&#20302;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#21516;&#26102;&#65292;&#20026;&#20102;&#26816;&#26597;&#25105;&#20204;&#30340;&#25200;&#21160;&#26159;&#21542;&#21487;&#20197;&#24402;&#19968;&#21270;&#20026;&#20854;&#24178;&#20928;&#29256;&#26412;&#65292;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#19968;&#20010;&#30456;&#20851;&#30340;&#27979;&#35797;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online texts with toxic content are a threat in social media that might cause cyber harassment. Although many platforms applied measures, such as machine learning-based hate-speech detection systems, to diminish their effect, those toxic content publishers can still evade the system by modifying the spelling of toxic words. Those modified words are also known as human-written text perturbations. Many research works developed certain techniques to generate adversarial samples to help the machine learning models obtain the ability to recognize those perturbations. However, there is still a gap between those machine-generated perturbations and human-written perturbations. In this paper, we introduce a benchmark test set containing human-written perturbations online for toxic speech detection models. We also recruited a group of workers to evaluate the quality of this test set and dropped low-quality samples. Meanwhile, to check if our perturbation can be normalized to its clean version, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;oracle&#31995;&#32479;&#65292;&#33021;&#22815;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;&#65292;&#35813;&#31639;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#26356;&#22909;&#12290;m-MARCO&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;</title><link>http://arxiv.org/abs/2303.09271</link><description>&lt;p&gt;
&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Finding Minimum-Cost Explanations for Predictions made by Tree Ensembles. (arXiv:2303.09271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;oracle&#31995;&#32479;&#65292;&#33021;&#22815;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#26368;&#23567;&#20195;&#20215;&#35299;&#37322;&#65292;&#35813;&#31639;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#26356;&#22909;&#12290;m-MARCO&#31639;&#27861;&#21487;&#20197;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20316;&#20026;&#20851;&#38190;&#31995;&#32479;&#30340;&#20915;&#31574;&#25903;&#25345;&#26102;&#65292;&#33021;&#22815;&#35299;&#37322;&#20026;&#20309;&#27169;&#22411;&#20570;&#20986;&#29305;&#23450;&#39044;&#27979;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#25552;&#20379;&#30340;&#35299;&#37322;&#24517;&#39035;&#26159;&#21487;&#35777;&#26126;&#30340;&#65292;&#24182;&#19988;&#26368;&#22909;&#19981;&#21253;&#21547;&#20887;&#20313;&#20449;&#24687;&#65292;&#21363;&#26368;&#23567;&#35299;&#37322;&#12290;&#26412;&#25991;&#26088;&#22312;&#23547;&#25214;&#26641;&#38598;&#25104;&#27169;&#22411;&#39044;&#27979;&#30340;&#35299;&#37322;&#65292;&#36825;&#20123;&#35299;&#37322;&#19981;&#20165;&#26159;&#26368;&#23567;&#30340;&#65292;&#32780;&#19988;&#22312;&#25104;&#26412;&#20989;&#25968;&#26041;&#38754;&#20063;&#26159;&#26368;&#23567;&#30340;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#8220;&#31070;&#35861;&#8221;&#31995;&#32479;&#65292;&#21487;&#20197;&#30830;&#23450;&#35299;&#37322;&#30340;&#27491;&#30830;&#24615;&#65292;&#22312;&#35745;&#31639;&#26368;&#23567;&#35299;&#37322;&#26102;&#36229;&#36234;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26367;&#20195;&#26041;&#26696;&#30340;&#36816;&#34892;&#34920;&#29616;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25913;&#32534;&#20102;&#26469;&#33258;&#30456;&#20851;&#24037;&#20316;&#30340;&#21483;&#20570;MARCO&#30340;&#31639;&#27861;&#65288;&#23558;&#20854;&#31216;&#20026;m-MARCO&#65289;&#65292;&#30446;&#30340;&#26159;&#35745;&#31639;&#27599;&#20010;&#39044;&#27979;&#30340;&#21333;&#20010;&#26368;&#23567;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#30456;&#23545;&#20110;&#26522;&#20030;&#25152;&#26377;&#26368;&#23567;&#35299;&#37322;&#30340;MARCO&#31639;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20004;&#20493;&#30340;&#24635;&#20307;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to explain why a machine learning model arrives at a particular prediction is crucial when used as decision support by human operators of critical systems. The provided explanations must be provably correct, and preferably without redundant information, called minimal explanations. In this paper, we aim at finding explanations for predictions made by tree ensembles that are not only minimal, but also minimum with respect to a cost function.  To this end, we first present a highly efficient oracle that can determine the correctness of explanations, surpassing the runtime performance of current state-of-the-art alternatives by several orders of magnitude when computing minimal explanations.  Secondly, we adapt an algorithm called MARCO from related works (calling it m-MARCO) for the purpose of computing a single minimum explanation per prediction, and demonstrate an overall speedup factor of two compared to the MARCO algorithm which enumerates all minimal explanations.  Final
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#36827;&#34892;&#20102;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2010.11750</link><description>&lt;p&gt;
&#37327;&#21270;&#24322;&#26500;&#36716;&#31227;&#30340;&#31934;&#30830;&#39640;&#32500;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Precise High-Dimensional Asymptotics for Quantifying Heterogeneous Transfers. (arXiv:2010.11750v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.11750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#36827;&#34892;&#20102;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26102;&#20351;&#29992;&#26469;&#33258;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#26679;&#26412;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#20160;&#20040;&#26102;&#20505;&#23558;&#26469;&#33258;&#20004;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#21512;&#24182;&#27604;&#21333;&#29420;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26356;&#22909;&#65311;&#30452;&#35266;&#19978;&#65292;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#36716;&#31227;&#25928;&#24212;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#65292;&#22914;&#26679;&#26412;&#22823;&#23567;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#36825;&#31181;&#36716;&#31227;&#25928;&#24212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#27604;&#36739;&#32852;&#21512;&#23398;&#20064;&#21644;&#21333;&#20219;&#21153;&#23398;&#20064;&#20043;&#38388;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#19968;&#20010;&#20219;&#21153;&#26159;&#21542;&#27604;&#21478;&#19968;&#20010;&#20219;&#21153;&#20855;&#26377;&#27604;&#36739;&#20248;&#21183;&#21462;&#20915;&#20110;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30830;&#20999;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#31867;&#22411;&#12290;&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#19968;&#20123;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#30340;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#65292;&#24403;&#26679;&#26412;&#22823;&#23567;&#19982;&#29305;&#24449;&#32500;&#24230;&#25104;&#27604;&#20363;&#22686;&#21152;&#26102;&#65292;&#22266;&#23450;&#27604;&#20363;&#12290;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#20197;&#26679;&#26412;&#22823;&#23567;&#30340;&#20989;&#25968;&#24418;&#24335;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of learning one task with samples from another task has received much interest recently. In this paper, we ask a fundamental question: when is combining data from two tasks better than learning one task alone? Intuitively, the transfer effect from one task to another task depends on dataset shifts such as sample sizes and covariance matrices. However, quantifying such a transfer effect is challenging since we need to compare the risks between joint learning and single-task learning, and the comparative advantage of one over the other depends on the exact kind of dataset shift between both tasks. This paper uses random matrix theory to tackle this challenge in a linear regression setting with two tasks. We give precise asymptotics about the excess risks of some commonly used estimators in the high-dimensional regime, when the sample sizes increase proportionally with the feature dimension at fixed ratios. The precise asymptotics is provided as a function of the sample sizes 
&lt;/p&gt;</description></item></channel></rss>