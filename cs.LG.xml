<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2404.02138</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#39064;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#27700;&#21360;
&lt;/p&gt;
&lt;p&gt;
Topic-based Watermarks for LLM-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02138
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#20026;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#25552;&#20379;&#20102;&#26032;&#30340;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#20102;&#29983;&#25104;&#30340;&#25991;&#26412;&#36755;&#20986;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#30456;&#20284;&#24230;&#38590;&#20197;&#20998;&#36776;&#12290;&#27700;&#21360;&#31639;&#27861;&#26159;&#28508;&#22312;&#24037;&#20855;&#65292;&#36890;&#36807;&#22312;LLM&#29983;&#25104;&#30340;&#36755;&#20986;&#20013;&#23884;&#20837;&#21487;&#26816;&#27979;&#30340;&#31614;&#21517;&#65292;&#21487;&#20197;&#21306;&#20998;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#21644;&#20154;&#31867;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#27700;&#21360;&#26041;&#26696;&#22312;&#24050;&#30693;&#25915;&#20987;&#19979;&#32570;&#20047;&#20581;&#22766;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;LLM&#27599;&#22825;&#29983;&#25104;&#25968;&#19975;&#20010;&#25991;&#26412;&#36755;&#20986;&#65292;&#27700;&#21360;&#31639;&#27861;&#38656;&#35201;&#35760;&#24518;&#27599;&#20010;&#36755;&#20986;&#25165;&#33021;&#35753;&#26816;&#27979;&#27491;&#24120;&#24037;&#20316;&#65292;&#36825;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26412;&#25991;&#38024;&#23545;&#24403;&#21069;&#27700;&#21360;&#26041;&#26696;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;LLMs&#30340;&#8220;&#22522;&#20110;&#20027;&#39064;&#30340;&#27700;&#21360;&#31639;&#27861;&#8221;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02138v1 Announce Type: cross  Abstract: Recent advancements of large language models (LLMs) have resulted in indistinguishable text outputs comparable to human-generated text. Watermarking algorithms are potential tools that offer a way to differentiate between LLM- and human-generated text by embedding detectable signatures within LLM-generated output. However, current watermarking schemes lack robustness against known attacks against watermarking algorithms. In addition, they are impractical considering an LLM generates tens of thousands of text outputs per day and the watermarking algorithm needs to memorize each output it generates for the detection to work. In this work, focusing on the limitations of current watermarking schemes, we propose the concept of a "topic-based watermarking algorithm" for LLMs. The proposed algorithm determines how to generate tokens for the watermarked LLM output based on extracted topics of an input prompt or the output of a non-watermarked 
&lt;/p&gt;</description></item><item><title>CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.16218</link><description>&lt;p&gt;
CoverUp&#65306;&#22522;&#20110;&#35206;&#30422;&#29575;&#24341;&#23548;&#30340;LLM&#27979;&#35797;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoverUp: Coverage-Guided LLM-Based Test Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16218
&lt;/p&gt;
&lt;p&gt;
CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CoverUp&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32467;&#21512;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#12290;CoverUp&#36890;&#36807;&#36845;&#20195;&#25913;&#21892;&#35206;&#30422;&#29575;&#65292;&#23558;&#35206;&#30422;&#29575;&#20998;&#26512;&#19982;LLM&#23545;&#35805;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#20415;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#23578;&#26410;&#28085;&#30422;&#30340;&#20195;&#30721;&#34892;&#21644;&#20998;&#25903;&#19978;&#12290;&#26368;&#32456;&#30340;&#27979;&#35797;&#22871;&#20214;&#30456;&#27604;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#26174;&#33879;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#65306;&#19982;CodaMosa&#30456;&#27604;&#65292;&#19968;&#31181;&#28151;&#21512;LLM / &#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#31995;&#32479;&#65292;CoverUp&#22312;&#21508;&#26041;&#38754;&#37117;&#22823;&#24133;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#12290;&#20197;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;CoverUp&#23454;&#29616;&#20102;81%&#30340;&#20013;&#20301;&#32447;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;62%&#65289;&#12289;53%&#30340;&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;35%&#65289;&#21644;78%&#30340;&#32447;+&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;55%&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoverUp&#30340;&#36845;&#20195;&#12289;&#35206;&#30422;&#29575;&#24341;&#23548;&#26041;&#27861;&#23545;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#20854;&#25104;&#21151;&#30340;&#36817;&#19968;&#21322;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16218v1 Announce Type: cross  Abstract: This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11345</link><description>&lt;p&gt;
&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#20316;&#31454;&#20105;Agent&#65306;&#22343;&#22330;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Independent RL for Cooperative-Competitive Agents: A Mean-Field Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22343;&#22330;&#35270;&#35282;&#30740;&#31350;&#20102;&#29420;&#31435;&#24378;&#21270;&#23398;&#20064;&#22312;&#21512;&#20316;&#31454;&#20105;&#20195;&#29702;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;RL&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#32771;&#34385;&#26080;&#38480;&#20195;&#29702;&#25968;&#37327;&#30340;&#24773;&#20917;&#26469;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30340;&#38750;&#31283;&#24577;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20998;&#25104;&#22242;&#38431;&#30340;&#20195;&#29702;&#20043;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#27599;&#20010;&#22242;&#38431;&#20869;&#37096;&#23384;&#22312;&#21512;&#20316;&#65292;&#20294;&#19981;&#21516;&#22242;&#38431;&#20043;&#38388;&#23384;&#22312;&#38750;&#38646;&#21644;&#30340;&#31454;&#20105;&#12290;&#20026;&#20102;&#24320;&#21457;&#19968;&#31181;&#21487;&#20197;&#26126;&#30830;&#23454;&#29616;&#32435;&#20160;&#22343;&#34913;&#30340;RL&#26041;&#27861;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#32447;&#24615;&#20108;&#27425;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#26377;&#38480;&#20154;&#21475;&#29615;&#22659;&#20013;&#30001;&#22810;&#26234;&#33021;&#20307;&#20132;&#20114;&#24341;&#36215;&#30340;&#38750;&#31283;&#24577;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#27599;&#20010;&#22242;&#38431;&#20869;&#20195;&#29702;&#25968;&#37327;&#26080;&#38480;&#30340;&#24773;&#20917;&#65292;&#21363;&#22343;&#22330;&#35774;&#32622;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#24191;&#20041;&#21644;&#30340;LQ&#22343;&#22330;&#31867;&#22411;&#21338;&#24328;&#65288;GS-MFTGs&#65289;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#36870;&#21487;&#36870;&#26465;&#20214;&#19979;&#34920;&#24449;&#20102;GS-MFTG&#30340;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#12290;&#28982;&#21518;&#35777;&#26126;&#20102;&#36825;&#20010;MFTG NE&#22312;&#26377;&#38480;&#20154;&#21475;&#21338;&#24328;&#20013;&#20026;$\mathcal{O}(1/M)$-NE&#65292;&#20854;&#20013;$M$&#26159;&#27599;&#20010;&#22242;&#38431;&#20013;&#20195;&#29702;&#25968;&#37327;&#30340;&#19979;&#30028;&#12290;&#36825;&#20123;&#32467;&#26500;&#24615;&#32467;&#26524;&#25512;&#21160;&#20102;&#19968;&#20010;&#21517;&#20026;&#22810;&#29609;&#23478;&#36882;&#36827;&#24335;&#33258;&#28982;Pol&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11345v1 Announce Type: cross  Abstract: We address in this paper Reinforcement Learning (RL) among agents that are grouped into teams such that there is cooperation within each team but general-sum (non-zero sum) competition across different teams. To develop an RL method that provably achieves a Nash equilibrium, we focus on a linear-quadratic structure. Moreover, to tackle the non-stationarity induced by multi-agent interactions in the finite population setting, we consider the case where the number of agents within each team is infinite, i.e., the mean-field setting. This results in a General-Sum LQ Mean-Field Type Game (GS-MFTGs). We characterize the Nash equilibrium (NE) of the GS-MFTG, under a standard invertibility condition. This MFTG NE is then shown to be $\mathcal{O}(1/M)$-NE for the finite population game where $M$ is a lower bound on the number of agents in each team. These structural results motivate an algorithm called Multi-player Receding-horizon Natural Pol
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39640;&#25928;&#21644;&#33258;&#36866;&#24212;&#39044;&#27979;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#36807;&#31243;&#20998;&#35299;&#20026;&#20174;&#26597;&#35810;&#20013;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#24182;&#22312;&#35813;&#21333;&#20010;&#12289;&#26597;&#35810;&#30456;&#20851;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#30340;&#20004;&#20010;&#27493;&#39588;&#65292;&#21033;&#29992;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.10231</link><description>&lt;p&gt;
&#23569;&#21363;&#26159;&#22810;&#65306;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Less is More: One-shot Subgraph Reasoning on Large-scale Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22823;&#35268;&#27169;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39640;&#25928;&#21644;&#33258;&#36866;&#24212;&#39044;&#27979;&#30340;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#36807;&#31243;&#20998;&#35299;&#20026;&#20174;&#26597;&#35810;&#20013;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#24182;&#22312;&#35813;&#21333;&#20010;&#12289;&#26597;&#35810;&#30456;&#20851;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#30340;&#20004;&#20010;&#27493;&#39588;&#65292;&#21033;&#29992;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#22312;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#19978;&#25512;&#23548;&#26032;&#30340;&#20107;&#23454;&#65292;&#38142;&#25509;&#39044;&#27979;&#22120;&#20174;&#22270;&#32467;&#26500;&#20013;&#23398;&#20064;&#65292;&#24182;&#25910;&#38598;&#23616;&#37096;&#35777;&#25454;&#20197;&#25214;&#21040;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#21033;&#29992;&#25972;&#20010;KG&#36827;&#34892;&#39044;&#27979;&#32780;&#23384;&#22312;&#20005;&#37325;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22823;&#35268;&#27169;KG&#19978;&#30340;&#24212;&#29992;&#65292;&#24182;&#19988;&#26080;&#27861;&#30452;&#25509;&#36890;&#36807;&#24120;&#35268;&#25277;&#26679;&#26041;&#27861;&#35299;&#20915;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#27425;&#24615;&#23376;&#22270;&#38142;&#25509;&#39044;&#27979;&#20197;&#23454;&#29616;&#39640;&#25928;&#19988;&#33258;&#36866;&#24212;&#30340;&#39044;&#27979;&#12290; &#35774;&#35745;&#21407;&#21017;&#26159;&#65292;&#39044;&#27979;&#36807;&#31243;&#19981;&#30452;&#25509;&#20316;&#29992;&#20110;&#25972;&#20010;KG&#65292;&#32780;&#26159;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#65292;&#21363;&#65288;i&#65289;&#26681;&#25454;&#26597;&#35810;&#20165;&#25552;&#21462;&#19968;&#20010;&#23376;&#22270;&#21644;&#65288;ii&#65289;&#22312;&#36825;&#20010;&#21333;&#19968;&#30340;&#12289;&#26597;&#35810;&#30456;&#20851;&#30340;&#23376;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290; &#25105;&#20204;&#21457;&#29616;&#65292;&#38750;&#21442;&#25968;&#21270;&#21644;&#35745;&#31639;&#39640;&#25928;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#20010;&#24615;&#21270;PageRank&#65288;PPR&#65289;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#28508;&#22312;&#31572;&#26696;&#21644;&#25903;&#25345;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10231v1 Announce Type: cross  Abstract: To deduce new facts on a knowledge graph (KG), a link predictor learns from the graph structure and collects local evidence to find the answer to a given query. However, existing methods suffer from a severe scalability problem due to the utilization of the whole KG for prediction, which hinders their promise on large scale KGs and cannot be directly addressed by vanilla sampling methods. In this work, we propose the one-shot-subgraph link prediction to achieve efficient and adaptive prediction. The design principle is that, instead of directly acting on the whole KG, the prediction procedure is decoupled into two steps, i.e., (i) extracting only one subgraph according to the query and (ii) predicting on this single, query dependent subgraph. We reveal that the non-parametric and computation-efficient heuristics Personalized PageRank (PPR) can effectively identify the potential answers and supporting evidence. With efficient subgraph-b
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02107</link><description>&lt;p&gt;
&#36845;&#20195;$Q$-&#32593;&#32476;&#65306;&#36229;&#36234;&#21333;&#27493;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Iterated $Q$-Network: Beyond the One-Step Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02107
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#23376;&#38656;&#35201;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#21253;&#25324;&#20132;&#26367;&#24212;&#29992;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#38543;&#21518;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#30340;&#36845;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31995;&#21015;$Q$&#20989;&#25968;&#36924;&#36817;&#65292;&#20854;&#20013;&#27599;&#20010;$Q$&#20989;&#25968;&#37117;&#20316;&#20026;&#19979;&#19968;&#20010;&#20989;&#25968;&#38142;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;iQN&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#21487;&#20197;&#26080;&#32541;&#22320;&#29992;&#20110;&#20540;&#22522;&#21644;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari$2600$&#28216;&#25103;&#21644;&#36830;&#32493;&#25511;&#21046;MuJoCo&#29615;&#22659;&#20013;&#22312;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.01875</link><description>&lt;p&gt;
ICLN&#65306;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#29992;&#20110;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ICLN: Input Convex Loss Network for Decision Focused Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01875
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#20026;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#25552;&#20379;&#20102;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#20013;&#65292;&#39044;&#27979;&#26410;&#30693;&#21442;&#25968;&#36890;&#24120;&#34987;&#35748;&#20026;&#19982;&#20248;&#21270;&#37096;&#20998;&#26080;&#20851;&#12290;&#20915;&#31574;&#38598;&#20013;&#23398;&#20064;&#65288;DFL&#65289;&#26159;&#19968;&#20010;&#38754;&#21521;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35843;&#25972;&#39044;&#27979;&#27169;&#22411;&#20197;&#20026;&#30456;&#24212;&#20219;&#21153;&#25552;&#20379;&#26356;&#22909;&#30340;&#20915;&#31574;&#26469;&#25972;&#21512;&#39044;&#27979;&#21644;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#36755;&#20837;&#20984;&#25439;&#22833;&#32593;&#32476;&#65288;ICLN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#23616;&#26367;&#20195;&#25439;&#22833;&#65292;&#21487;&#20197;&#22312;&#19968;&#33324;&#30340;DFL&#33539;&#24335;&#20013;&#23454;&#29616;&#12290;ICLN&#36890;&#36807;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20219;&#21153;&#25439;&#22833;&#65292;&#24050;&#32463;&#34987;&#20445;&#35777;&#20026;&#26576;&#20123;&#24773;&#20917;&#19979;&#26159;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01875v1 Announce Type: cross  Abstract: In decision-making problem under uncertainty, predicting unknown parameters is often considered independent of the optimization part. Decision-focused Learning (DFL) is a task-oriented framework to integrate prediction and optimization by adapting predictive model to give better decision for the corresponding task. Here, an inevitable challenge arises when computing gradients of the optimal decision with respect to the parameters. Existing researches cope this issue by smoothly reforming surrogate optimization or construct surrogate loss function that mimic task loss. However, they are applied to restricted optimization domain or build functions in a local manner leading a large computational time. In this paper, we propose Input Convex Loss Network (ICLN), a novel global surrogate loss which can be implemented in a general DFL paradigm. ICLN learns task loss via Input Convex Neural Networks which is guaranteed to be convex for some in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.15589</link><description>&lt;p&gt;
&#20174;&#23398;&#26415;&#25163;&#31295;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#20013;&#35201;&#27714;LLMs&#25776;&#20889;&#20803;&#35780;&#35770;&#33609;&#26696;
&lt;/p&gt;
&lt;p&gt;
Prompting LLMs to Compose Meta-Review Drafts from Peer-Review Narratives of Scholarly Manuscripts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15589
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#26469;&#28608;&#21457;&#19977;&#31181;&#27969;&#34892;LLM&#65292;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#65292;&#24182;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#36807;&#31243;&#20013;&#26368;&#37325;&#35201;&#20294;&#20063;&#26368;&#32321;&#37325;&#30340;&#20219;&#21153;&#20043;&#19968;&#26159;&#25776;&#20889;&#20803;&#35780;&#35770;&#65292;&#36825;&#28041;&#21450;&#26681;&#25454;&#22810;&#20301;&#19987;&#23478;&#30340;&#21516;&#34892;&#35780;&#23457;&#21465;&#20107;&#29702;&#35299;&#23398;&#26415;&#25163;&#31295;&#30340;&#26680;&#24515;&#36129;&#29486;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#19987;&#23478;&#22810;&#35270;&#35282;&#30340;&#30475;&#27861;&#24635;&#32467;&#20026;&#31616;&#27905;&#30340;&#25972;&#20307;&#27010;&#36848;&#12290;&#37492;&#20110;&#29983;&#25104;&#22411;AI&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#37325;&#22823;&#21457;&#23637;&#65292;&#25105;&#20204;&#26377;&#20805;&#20998;&#30340;&#29702;&#30001;&#28145;&#20837;&#30740;&#31350;LLMs&#22312;&#23398;&#26415;&#21516;&#34892;&#35780;&#23457;&#29615;&#22659;&#20013;&#29983;&#25104;&#36825;&#31181;&#20803;&#35780;&#35770;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#19977;&#31181;&#27969;&#34892;&#30340;LLM&#65292;&#21363;GPT-3.5&#12289;LLaMA2&#21644;PaLM2&#65292;&#25191;&#34892;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#22522;&#20110;&#26368;&#36817;&#25552;&#20986;&#30340;TELeR&#20998;&#31867;&#27861;&#20197;&#19981;&#21516;&#31867;&#22411;/&#32423;&#21035;&#30340;&#25552;&#31034;&#20419;&#20351;&#23427;&#20204;&#33258;&#21160;&#29983;&#25104;&#20803;&#35780;&#35770;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20803;&#35780;&#35770;&#36827;&#34892;&#20102;&#35814;&#32454;&#30340;&#23450;&#24615;&#30740;&#31350;&#65292;&#24182;&#24635;&#32467;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15589v1 Announce Type: cross  Abstract: One of the most important yet onerous tasks in the academic peer-reviewing process is composing meta-reviews, which involves understanding the core contributions, strengths, and weaknesses of a scholarly manuscript based on peer-review narratives from multiple experts and then summarizing those multiple experts' perspectives into a concise holistic overview. Given the latest major developments in generative AI, especially Large Language Models (LLMs), it is very compelling to rigorously study the utility of LLMs in generating such meta-reviews in an academic peer-review setting. In this paper, we perform a case study with three popular LLMs, i.e., GPT-3.5, LLaMA2, and PaLM2, to automatically generate meta-reviews by prompting them with different types/levels of prompts based on the recently proposed TELeR taxonomy. Finally, we perform a detailed qualitative study of the meta-reviews generated by the LLMs and summarize our findings and 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;</title><link>https://arxiv.org/abs/2402.14029</link><description>&lt;p&gt;
&#20923;&#32467;&#32593;&#32476;&#20013;&#30340;&#37096;&#20998;&#25628;&#32034;&#36275;&#20197;&#25214;&#21040;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;
&lt;/p&gt;
&lt;p&gt;
Partial Search in a Frozen Network is Enough to Find a Strong Lottery Ticket
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14029
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20923;&#32467;&#38543;&#26426;&#23376;&#38598;&#30340;&#21021;&#22987;&#26435;&#37325;&#26469;&#20943;&#23569;&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLT&#65289;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#38477;&#20302;&#20102;SLT&#25628;&#32034;&#31354;&#38388;&#65292;&#20445;&#35777;&#20102;SLT&#22312;&#36825;&#31181;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#20013;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25688;&#35201;&#65306;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#31264;&#23494;&#32593;&#32476;&#21253;&#21547;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#26435;&#37325;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#39640;&#20934;&#30830;&#24230;&#30340;&#23376;&#32593;&#32476;--&#24378;&#22823;&#30340;&#24425;&#31080;&#31080;&#35777;&#65288;SLTs&#65289;&#12290;&#26368;&#36817;&#65292;Gadhikar&#31561;&#20154;&#65288;2023&#24180;&#65289;&#22312;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#26126;&#65292;SLTs&#20063;&#21487;&#20197;&#22312;&#38543;&#26426;&#20462;&#21098;&#30340;&#28304;&#32593;&#32476;&#20013;&#25214;&#21040;&#65292;&#20174;&#32780;&#20943;&#23569;SLT&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#38480;&#21046;&#20102;&#23545;&#29978;&#33267;&#27604;&#28304;&#32593;&#32476;&#26356;&#31232;&#30095;&#30340;SLTs&#30340;&#25628;&#32034;&#65292;&#23548;&#33268;&#30001;&#20110;&#24847;&#22806;&#30340;&#39640;&#31232;&#30095;&#24615;&#32780;&#20934;&#30830;&#24230;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29420;&#31435;&#20110;&#25152;&#38656;SLT&#31232;&#30095;&#24615;&#30340;&#20219;&#24847;&#27604;&#29575;&#20943;&#23569;SLT&#25628;&#32034;&#31354;&#38388;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20923;&#32467;&#19968;&#37096;&#20998;&#21021;&#22987;&#26435;&#37325;&#30340;&#38543;&#26426;&#23376;&#38598;&#65292;&#23558;&#20854;&#25490;&#38500;&#22312;&#25628;&#32034;&#31354;&#38388;&#20043;&#22806;--&#21363;&#65292;&#36890;&#36807;&#27704;&#20037;&#20462;&#21098;&#23427;&#20204;&#25110;&#23558;&#23427;&#20204;&#38145;&#23450;&#20026;SLT&#30340;&#22266;&#23450;&#37096;&#20998;&#12290;&#20107;&#23454;&#19978;&#65292;&#36890;&#36807;&#25105;&#20204;&#19982;&#38543;&#26426;&#20923;&#32467;&#21464;&#37327;&#30340;&#23376;&#38598;&#21644;&#36924;&#36817;&#65292;&#22312;&#36825;&#31181;&#20943;&#23569;&#30340;&#25628;&#32034;&#31354;&#38388;&#20013;&#65292;SLT&#30340;&#23384;&#22312;&#22312;&#29702;&#35770;&#19978;&#26159;&#24471;&#21040;&#20445;&#35777;&#30340;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36824;&#21487;&#20197;&#20943;&#23569;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14029v1 Announce Type: cross  Abstract: Randomly initialized dense networks contain subnetworks that achieve high accuracy without weight learning -- strong lottery tickets (SLTs). Recently, Gadhikar et al. (2023) demonstrated theoretically and experimentally that SLTs can also be found within a randomly pruned source network, thus reducing the SLT search space. However, this limits the search to SLTs that are even sparser than the source, leading to worse accuracy due to unintentionally high sparsity. This paper proposes a method that reduces the SLT search space by an arbitrary ratio that is independent of the desired SLT sparsity. A random subset of the initial weights is excluded from the search space by freezing it -- i.e., by either permanently pruning them or locking them as a fixed part of the SLT. Indeed, the SLT existence in such a reduced search space is theoretically guaranteed by our subset-sum approximation with randomly frozen variables. In addition to reducin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2401.17417</link><description>&lt;p&gt;
&#22522;&#20110;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#30340;&#31359;&#22681;&#25104;&#20687;
&lt;/p&gt;
&lt;p&gt;
Through-Wall Imaging based on WiFi Channel State Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#23454;&#29616;&#31359;&#22681;&#25104;&#20687;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#23460;&#20869;&#29615;&#22659;&#21487;&#35270;&#21270;&#30417;&#27979;&#21040;&#25151;&#38388;&#36793;&#30028;&#20043;&#22806;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;WiFi&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#22312;&#31359;&#22681;&#22330;&#26223;&#20013;&#21512;&#25104;&#22270;&#20687;&#12290;&#21033;&#29992;WiFi&#30340;&#20248;&#21183;&#65292;&#22914;&#25104;&#26412;&#25928;&#30410;&#65292;&#20809;&#29031;&#19981;&#21464;&#24615;&#21644;&#31359;&#22681;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#23460;&#20869;&#29615;&#22659;&#30340;&#21487;&#35270;&#21270;&#30417;&#27979;&#65292;&#36234;&#36807;&#25151;&#38388;&#36793;&#30028;&#65292;&#26080;&#38656;&#25668;&#20687;&#26426;&#12290;&#26356;&#19968;&#33324;&#22320;&#65292;&#23427;&#36890;&#36807;&#35299;&#38145;&#25191;&#34892;&#22522;&#20110;&#22270;&#20687;&#30340;&#19979;&#28216;&#20219;&#21153;&#65288;&#20363;&#22914;&#65292;&#35270;&#35273;&#27963;&#21160;&#35782;&#21035;&#65289;&#30340;&#36873;&#39033;&#65292;&#25552;&#39640;&#20102;WiFi CSI&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#23454;&#29616;&#20174;WiFi CSI&#21040;&#22270;&#20687;&#30340;&#36328;&#27169;&#24577;&#36716;&#25442;&#65292;&#25105;&#20204;&#20381;&#36182;&#20110;&#19968;&#20010;&#36866;&#24212;&#25105;&#20204;&#38382;&#39064;&#29305;&#23450;&#30340;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#26550;&#26500;&#37197;&#32622;&#30340;&#21076;&#38500;&#30740;&#31350;&#21644;&#37325;&#24314;&#22270;&#20687;&#30340;&#23450;&#37327;/&#23450;&#24615;&#35780;&#20272;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a seminal approach for synthesizing images from WiFi Channel State Information (CSI) in through-wall scenarios. Leveraging the strengths of WiFi, such as cost-effectiveness, illumination invariance, and wall-penetrating capabilities, our approach enables visual monitoring of indoor environments beyond room boundaries and without the need for cameras. More generally, it improves the interpretability of WiFi CSI by unlocking the option to perform image-based downstream tasks, e.g., visual activity recognition. In order to achieve this crossmodal translation from WiFi CSI to images, we rely on a multimodal Variational Autoencoder (VAE) adapted to our problem specifics. We extensively evaluate our proposed methodology through an ablation study on architecture configuration and a quantitative/qualitative assessment of reconstructed images. Our results demonstrate the viability of our method and highlight its potential for practical applications.
&lt;/p&gt;</description></item><item><title>XtalNet&#26159;&#39318;&#20010;&#29992;&#20110;&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#23454;&#29616;&#31471;&#21040;&#31471;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#30340;&#31561;&#21464;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#36798;400&#20010;&#21407;&#23376;&#30340;&#26377;&#26426;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2401.03862</link><description>&lt;p&gt;
&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
End-to-End Crystal Structure Prediction from Powder X-Ray Diffraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03862
&lt;/p&gt;
&lt;p&gt;
XtalNet&#26159;&#39318;&#20010;&#29992;&#20110;&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#23454;&#29616;&#31471;&#21040;&#31471;&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#30340;&#31561;&#21464;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#36798;400&#20010;&#21407;&#23376;&#30340;&#26377;&#26426;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26230;&#20307;&#32467;&#26500;&#39044;&#27979;&#65288;CSP&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#38598;&#20013;&#22312;&#26080;&#26465;&#20214;&#22320;&#29983;&#25104;&#20855;&#26377;&#26377;&#38480;&#21407;&#23376;&#30340;&#26080;&#26426;&#26230;&#20307;&#12290; &#26412;&#30740;&#31350;&#24341;&#20837;&#20102;XtalNet&#65292;&#36825;&#26159;&#39318;&#20010;&#29992;&#20110;&#20174;&#31881;&#26411;X&#23556;&#32447;&#34893;&#23556;&#65288;PXRD&#65289;&#23454;&#29616;&#31471;&#21040;&#31471;CSP&#30340;&#31561;&#21464;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#12290; &#19982;&#20808;&#21069;&#20165;&#20381;&#36182;&#25104;&#20998;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;XtalNet&#21033;&#29992;PXRD&#20316;&#20026;&#39069;&#22806;&#26465;&#20214;&#65292;&#28040;&#38500;&#20102;&#27169;&#31946;&#24615;&#65292;&#24182;&#20351;&#24471;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#22810;&#36798;400&#20010;&#21407;&#23376;&#30340;&#21333;&#20803;&#32990;&#30340;&#22797;&#26434;&#26377;&#26426;&#32467;&#26500;&#12290; XtalNet&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;&#23545;&#27604;PXRD-&#26230;&#20307;&#39044;&#35757;&#32451;&#65288;CPCP&#65289;&#27169;&#22359;&#65292;&#23558;PXRD&#31354;&#38388;&#19982;&#26230;&#20307;&#32467;&#26500;&#31354;&#38388;&#23545;&#40784;&#65292;&#20197;&#21450;&#26465;&#20214;&#26230;&#20307;&#32467;&#26500;&#29983;&#25104;&#65288;CCSG&#65289;&#27169;&#22359;&#65292;&#26681;&#25454;PXRD&#27169;&#24335;&#29983;&#25104;&#20505;&#36873;&#26230;&#20307;&#32467;&#26500;&#12290; &#22312;&#20004;&#20010;MOF&#25968;&#25454;&#38598;&#65288;hMOF-100&#21644;hMOF-400&#65289;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#20102;XtalNet&#30340;&#26377;&#25928;&#24615;&#12290; XtalNet&#23454;&#29616;&#20102;9&#30340;&#21069;&#21313;&#21305;&#37197;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03862v2 Announce Type: replace-cross  Abstract: Crystal structure prediction (CSP) has made significant progress, but most methods focus on unconditional generations of inorganic crystal with limited atoms in the unit cell. This study introduces XtalNet, the first equivariant deep generative model for end-to-end CSP from Powder X-ray Diffraction (PXRD). Unlike previous methods that rely solely on composition, XtalNet leverages PXRD as an additional condition, eliminating ambiguity and enabling the generation of complex organic structures with up to 400 atoms in the unit cell. XtalNet comprises two modules: a Contrastive PXRD-Crystal Pretraining (CPCP) module that aligns PXRD space with crystal structure space, and a Conditional Crystal Structure Generation (CCSG) module that generates candidate crystal structures conditioned on PXRD patterns. Evaluation on two MOF datasets (hMOF-100 and hMOF-400) demonstrates XtalNet's effectiveness. XtalNet achieves a top-10 Match Rate of 9
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#39118;&#38505;&#36866;&#24212;&#24615;&#35843;&#25972;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#35748;&#30693;&#39118;&#38505;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2310.05179</link><description>&lt;p&gt;
&#20855;&#26377;&#22312;&#32447;&#39118;&#38505;&#24863;&#30693;&#36866;&#24212;&#24615;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning with Online Risk-awareness Adaption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#39118;&#38505;&#36866;&#24212;&#24615;&#35843;&#25972;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#35748;&#30693;&#39118;&#38505;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38656;&#35201;&#32771;&#34385;&#27425;&#20248;&#32467;&#26524;&#65292;&#36825;&#21462;&#20915;&#20110;&#20195;&#29702;&#20154;&#23545;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#29087;&#24713;&#31243;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;Distributional RL with Online Risk Adaption&#65288;DRL-ORA&#65289;&#65292;&#21487;&#20197;&#32508;&#21512;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#24182;&#21160;&#24577;&#36873;&#25321;&#35748;&#30693;&#39118;&#38505;&#27700;&#24179;&#65292;&#36890;&#36807;&#22312;&#32447;&#35299;&#20915;&#24635;&#21464;&#24046;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#39118;&#38505;&#27700;&#24179;&#36873;&#25321;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;Follow-The-Leader&#31867;&#22411;&#31639;&#27861;&#36827;&#34892;&#32593;&#26684;&#25628;&#32034;&#26469;&#26377;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05179v2 Announce Type: replace  Abstract: The use of reinforcement learning (RL) in practical applications requires considering sub-optimal outcomes, which depend on the agent's familiarity with the uncertain environment. Dynamically adjusting the level of epistemic risk over the course of learning can tactically achieve reliable optimal policy in safety-critical environments and tackle the sub-optimality of a static risk level. In this work, we introduce a novel framework, Distributional RL with Online Risk Adaption (DRL-ORA), which can quantify the aleatory and epistemic uncertainties compositely and dynamically select the epistemic risk levels via solving a total variation minimization problem online. The risk level selection can be efficiently achieved through grid search using a Follow-The-Leader type algorithm, and its offline oracle is related to "satisficing measure" (in the decision analysis community) under a special modification of the loss function. We show multi
&lt;/p&gt;</description></item><item><title>UniAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2307.16375</link><description>&lt;p&gt;
UniAP: &#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#33258;&#21160;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
UniAP: Unifying Inter- and Intra-Layer Automatic Parallelism by Mixed Integer Quadratic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2307.16375
&lt;/p&gt;
&lt;p&gt;
UniAP&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#24120;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#27169;&#22411;&#12290;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#65292;&#25163;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#20154;&#21147;&#65292;&#24182;&#19988;&#28789;&#27963;&#24615;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#24182;&#34892;&#31574;&#30053;&#20248;&#21270;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#23384;&#22312;&#27425;&#20248;&#35299;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20250;&#21516;&#26102;&#20248;&#21270;&#36328;&#23618;&#24182;&#34892;&#21270;&#21644;&#20869;&#23618;&#24182;&#34892;&#21270;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#24182;&#34892;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UniAP&#30340;&#26032;&#22411;&#33258;&#21160;&#24182;&#34892;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20108;&#27425;&#35268;&#21010;&#32479;&#19968;&#36328;&#23618;&#21644;&#20869;&#23618;&#30340;&#33258;&#21160;&#24182;&#34892;&#21270;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;UniAP&#26159;&#31532;&#19968;&#31181;&#33021;&#22815;&#21516;&#26102;&#20248;&#21270;&#36825;&#20004;&#20010;&#31867;&#21035;&#30340;&#24182;&#34892;&#31574;&#30053;&#20197;&#27714;&#24471;&#26368;&#20248;&#35299;&#30340;&#24182;&#34892;&#21270;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniAP&#22312;&#21534;&#21520;&#37327;&#26041;&#38754;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#26368;&#22810;1.71&#20493;&#65292;&#24182;&#20943;&#23569;&#20102;&#31574;&#30053;&#20248;&#21270;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed learning is commonly used for training deep learning models, especially large models. In distributed learning, manual parallelism (MP) methods demand considerable human effort and have limited flexibility. Hence, automatic parallelism (AP) methods have recently been proposed for automating the parallel strategy optimization process. Existing AP methods suffer from sub-optimal solutions because they do not jointly optimize the two categories of parallel strategies (i.e., inter-layer parallelism and intra-layer parallelism). In this paper, we propose a novel AP method called UniAP, which unifies inter- and intra-layer automatic parallelism by mixed integer quadratic programming. To the best of our knowledge, UniAP is the first parallel method that can jointly optimize the two categories of parallel strategies to find an optimal solution. Experimental results show that UniAP outperforms state-of-the-art methods by up to 1.71$\times$ in throughput and reduces strategy optimizat
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2401.14295</link><description>&lt;p&gt;
&#25512;&#29702;&#30340;&#25299;&#25169;&#23398;&#65306;&#25581;&#31192;&#24605;&#32500;&#38142;&#12289;&#26641;&#21644;&#22270;
&lt;/p&gt;
&lt;p&gt;
Topologies of Reasoning: Demystifying Chains, Trees, and Graphs of Thoughts. (arXiv:2401.14295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14295
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#32467;&#21512;&#32467;&#26500;&#30340;&#25552;&#31034;&#24037;&#31243;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#29702;&#24615;&#33021;&#26041;&#38754;&#30340;&#21069;&#26223;&#65292;&#36890;&#36807;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#30340;&#35774;&#35745;&#26469;&#24341;&#23548;&#25972;&#20307;&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;&#27169;&#22411;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#34013;&#22270;&#65292;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#29305;&#21035;&#26159;&#22312;&#36890;&#36807;&#21019;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24615;&#33021;&#26041;&#38754;&#12290;&#20854;&#20013;&#65292;&#19982;&#32467;&#26500;&#30456;&#32467;&#21512;&#30340;&#25552;&#31034;&#24037;&#31243;&#34987;&#35270;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20854;&#35774;&#35745;&#22914;&#24605;&#32500;&#38142;&#12289;&#24605;&#32500;&#26641;&#25110;&#24605;&#32500;&#22270;&#31561;&#65292;&#36890;&#36807;&#32467;&#26500;&#25351;&#23548;&#25972;&#20307;LLM&#25512;&#29702;&#36807;&#31243;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#20363;&#30340;&#35828;&#26126;&#65292;&#36825;&#31181;&#33539;&#24335;&#26174;&#33879;&#22686;&#24378;&#20102;LLM&#22312;&#36923;&#36753;&#25110;&#25968;&#23398;&#25512;&#29702;&#12289;&#35268;&#21010;&#25110;&#21019;&#36896;&#24615;&#20889;&#20316;&#31561;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#26041;&#20415;&#29702;&#35299;&#36825;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#24182;&#20026;&#26410;&#26469;&#30340;&#21457;&#23637;&#38138;&#24179;&#36947;&#36335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;LLM&#25512;&#29702;&#26041;&#26696;&#30340;&#36890;&#29992;&#34013;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23545;&#25552;&#31034;&#25191;&#34892;&#27969;&#31243;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#28548;&#28165;&#24182;&#26126;&#30830;&#23450;&#20041;&#20102;&#19981;&#21516;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#25105;&#20204;&#24314;&#31435;&#31532;&#19968;&#20010;&#20998;&#31867;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
The field of natural language processing (NLP) has witnessed significant progress in recent years, with a notable focus on improving large language models' (LLM) performance through innovative prompting techniques. Among these, prompt engineering coupled with structures has emerged as a promising paradigm, with designs such as Chain-of-Thought, Tree of Thoughts, or Graph of Thoughts, in which the overall LLM reasoning is guided by a structure such as a graph. As illustrated with numerous examples, this paradigm significantly enhances the LLM's capability to solve numerous tasks, ranging from logical or mathematical reasoning to planning or creative writing. To facilitate the understanding of this growing field and pave the way for future developments, we devise a general blueprint for effective and efficient LLM reasoning schemes. For this, we conduct an in-depth analysis of the prompt execution pipeline, clarifying and clearly defining different concepts. We then build the first taxon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20844;&#24179;GNN&#30340;&#23545;&#25239;&#24615;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20844;&#24179;GNN&#30340;&#20551;&#35774;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01591</link><description>&lt;p&gt;
&#26356;&#22909;&#30340;&#20844;&#24179;&#24615;&#32988;&#20110;&#36951;&#25022;&#65306;&#38024;&#23545;&#20844;&#24179;GNN&#30340;&#23545;&#25239;&#24615;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
Better Fair than Sorry: Adversarial Missing Data Imputation for Fair GNNs. (arXiv:2311.01591v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01591
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20844;&#24179;GNN&#30340;&#23545;&#25239;&#24615;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#20844;&#24179;GNN&#30340;&#20551;&#35774;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#27492;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#32570;&#22833;&#20445;&#25252;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#20844;&#24179;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#38382;&#39064;&#12290;&#22312;&#35768;&#22810;&#30456;&#20851;&#20219;&#21153;&#20013;&#65292;&#20915;&#31574;&#21487;&#33021;&#20250;&#23545;&#29305;&#23450;&#31038;&#21306;&#20135;&#29983;&#19981;&#25104;&#27604;&#20363;&#30340;&#24433;&#21709;&#65292;&#32780;GNNs&#24050;&#32463;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;GNNs&#24037;&#20316;&#35201;&#20040;&#20551;&#35774;&#20445;&#25252;&#23646;&#24615;&#26159;&#23436;&#20840;&#34987;&#35266;&#23519;&#21040;&#30340;&#65292;&#35201;&#20040;&#20551;&#35774;&#32570;&#22833;&#25968;&#25454;&#30340;&#22635;&#20805;&#26159;&#20844;&#24179;&#30340;&#12290;&#23454;&#38469;&#19978;&#65292;&#22635;&#20805;&#20013;&#30340;&#20559;&#24046;&#20250;&#20256;&#25773;&#21040;&#27169;&#22411;&#30340;&#32467;&#26524;&#20013;&#65292;&#23548;&#33268;&#23427;&#20204;&#36807;&#39640;&#22320;&#20272;&#35745;&#20102;&#20854;&#39044;&#27979;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;Better Fair than Sorry&#65288;BFtS&#65289;&#65292;&#20026;&#20844;&#24179;GNNs&#20351;&#29992;&#30340;&#20445;&#25252;&#23646;&#24615;&#30340;&#20844;&#24179;&#32570;&#22833;&#25968;&#25454;&#22635;&#20805;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;BFtS&#32972;&#21518;&#30340;&#20851;&#38190;&#35774;&#35745;&#21407;&#21017;&#26159;&#22635;&#20805;&#24212;&#35813;&#36817;&#20284;&#20110;&#20844;&#24179;GNN&#30340;&#26368;&#22256;&#38590;&#24773;&#20917;&#65292;&#21363;&#22312;&#26368;&#20248;&#21270;&#20844;&#24179;&#24615;&#26368;&#22256;&#38590;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#19977;&#26041;&#23545;&#25239;&#26041;&#26696;&#26469;&#23454;&#29616;&#36825;&#20010;&#24819;&#27861;&#65292;&#22312;&#36825;&#20010;&#26041;&#26696;&#20013;&#65292;&#20004;&#20010;&#23545;&#25163;&#20849;&#21516;&#23545;&#25239;&#20844;&#24179;GNN&#12290;&#36890;&#36807;&#20351;&#29992;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;BFtS&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the problem of learning fair Graph Neural Networks (GNNs) under missing protected attributes. GNNs have achieved state-of-the-art results in many relevant tasks where decisions might disproportionately impact specific communities. However, existing work on fair GNNs assumes that either protected attributes are fully-observed or that the missing data imputation is fair. In practice, biases in the imputation will be propagated to the model outcomes, leading them to overestimate the fairness of their predictions. We address this challenge by proposing Better Fair than Sorry (BFtS), a fair missing data imputation model for protected attributes used by fair GNNs. The key design principle behind BFtS is that imputations should approximate the worst-case scenario for the fair GNN -- i.e. when optimizing fairness is the hardest. We implement this idea using a 3-player adversarial scheme where two adversaries collaborate against the fair GNN. Experiments using synthetic and
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08224</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#20013;&#28508;&#22312;&#20108;&#36827;&#21046;&#32534;&#30721;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Latent Binary Encoding in Deep Neural Network Classifiers. (arXiv:2310.08224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08224
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35266;&#23519;&#21040;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#65292;&#36825;&#31181;&#32534;&#30721;&#36890;&#36807;&#24341;&#20837;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#21644;&#25351;&#25968;&#22686;&#38271;&#30340;&#25439;&#22833;&#20989;&#25968;&#20135;&#29983;&#65292;&#24182;&#19988;&#21152;&#36895;&#20102;&#25910;&#25947;&#21644;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35266;&#23519;&#21040;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20986;&#29616;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#32447;&#24615;&#20498;&#25968;&#31532;&#20108;&#23618;&#65292;&#24182;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37197;&#22791;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#38543;&#30528;&#28508;&#22312;&#31354;&#38388;&#20013;&#22352;&#26631;$\vec{x}$&#30340;&#24179;&#26041;&#25351;&#25968;&#22686;&#38271;&#65292;&#35825;&#23548;&#20986;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#12290;&#25105;&#20204;&#25551;&#36848;&#30340;&#29616;&#35937;&#26159;&#24050;&#30693;&#30340;&#19968;&#31181;&#34987;&#31216;&#20026;"&#31070;&#32463;&#23849;&#28291;"&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#23427;&#22312;&#35757;&#32451;&#30340;&#26368;&#21518;&#38454;&#27573;&#20986;&#29616;&#65292;&#24182;&#23548;&#33268;&#28508;&#22312;&#31867;&#22343;&#20540;&#23849;&#28291;&#20026;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#65288;ETF&#65289;&#30340;&#39030;&#28857;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20108;&#36827;&#21046;&#32534;&#30721;&#21152;&#36895;&#20102;&#25910;&#25947;&#21040;&#31616;&#21333;&#31561;&#35282;&#32039;&#26694;&#26550;&#30340;&#36807;&#31243;&#65292;&#24182;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
We observe the emergence of binary encoding within the latent space of deep-neural-network classifiers. Such binary encoding is induced by introducing a linear penultimate layer, which is equipped during training with a loss function that grows as $\exp(\vec{x}^2)$, where $\vec{x}$ are the coordinates in the latent space. The phenomenon we describe represents a specific instance of a well-documented occurrence known as \textit{neural collapse}, which arises in the terminal phase of training and entails the collapse of latent class means to the vertices of a simplex equiangular tight frame (ETF). We show that binary encoding accelerates convergence toward the simplex ETF and enhances classification accuracy.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20197;Learn From Model (LFM)&#20026;&#21517;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#21644;&#35774;&#35745;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2310.08184</link><description>&lt;p&gt;
&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Learn From Model Beyond Fine-Tuning: A Survey. (arXiv:2310.08184v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20197;Learn From Model (LFM)&#20026;&#21517;&#65292;&#25506;&#32034;&#20102;&#36229;&#36234;&#24494;&#35843;&#30340;&#27169;&#22411;&#23398;&#20064;&#25216;&#26415;&#65292;&#26088;&#22312;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#21644;&#35774;&#35745;&#65292;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#25512;&#24191;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27169;&#22411;&#30340;&#23398;&#20064;&#65288;LFM&#65289;&#26159;&#19968;&#31181;&#26032;&#30340;&#30740;&#31350;&#36235;&#21183;&#65292;&#23427;&#19987;&#27880;&#20110;&#36890;&#36807;&#23545;&#27169;&#22411;&#25509;&#21475;&#36827;&#34892;&#30740;&#31350;&#12289;&#20462;&#25913;&#21644;&#35774;&#35745;&#26469;&#26356;&#22909;&#22320;&#29702;&#35299;&#27169;&#22411;&#30340;&#32467;&#26500;&#21644;&#26435;&#37325;&#65288;&#22312;&#40657;&#21283;&#23376;&#29615;&#22659;&#20013;&#65289;&#65292;&#24182;&#23558;&#27169;&#22411;&#27867;&#21270;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#23558;LFM&#25216;&#26415;&#30340;&#30740;&#31350;&#20998;&#20026;&#20116;&#20010;&#20027;&#35201;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Foundation models (FM) have demonstrated remarkable performance across a wide range of tasks (especially in the fields of natural language processing and computer vision), primarily attributed to their ability to comprehend instructions and access extensive, high-quality data. This not only showcases their current effectiveness but also sets a promising trajectory towards the development of artificial general intelligence. Unfortunately, due to multiple constraints, the raw data of the model used for large model training are often inaccessible, so the use of end-to-end models for downstream tasks has become a new research trend, which we call Learn From Model (LFM) in this article. LFM focuses on the research, modification, and design of FM based on the model interface, so as to better understand the model structure and weights (in a black box environment), and to generalize the model to downstream tasks. The study of LFM techniques can be broadly categorized into five major areas: mod
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#39640;&#25928;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2309.02711</link><description>&lt;p&gt;
&#35299;&#20915;&#19981;&#23436;&#20840;&#23545;&#31216;&#24615;&#65306;&#19968;&#31181;&#26032;&#30340;&#23545;&#31216;&#23398;&#20064;&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Addressing Imperfect Symmetry: a Novel Symmetry-Learning Actor-Critic Extension. (arXiv:2309.02711v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#33021;&#39640;&#25928;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#26159;&#29702;&#35299;&#25105;&#20204;&#30340;&#29615;&#22659;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20294;&#24448;&#24448;&#20174;&#25968;&#23398;&#30340;&#35282;&#24230;&#36807;&#20110;&#31616;&#21270;&#20102;&#29616;&#23454;&#12290;&#20154;&#31867;&#26159;&#20010;&#24456;&#22909;&#30340;&#20363;&#23376;&#65292;&#22806;&#35980;&#21644;&#35748;&#30693;&#20559;&#35265;&#65288;&#20363;&#22914;&#26377;&#19968;&#21482;&#21344;&#20027;&#23548;&#22320;&#20301;&#30340;&#25163;&#65289;&#37117;&#19981;&#23436;&#32654;&#22320;&#20559;&#31163;&#20102;&#23545;&#31216;&#24615;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#30340;&#22823;&#33041;&#24456;&#23481;&#26131;&#20811;&#26381;&#36825;&#20123;&#19981;&#23436;&#32654;&#24182;&#39640;&#25928;&#22320;&#36866;&#24212;&#23545;&#31216;&#24615;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#30340;&#39537;&#21160;&#21160;&#26426;&#22312;&#20110;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25429;&#25417;&#36825;&#31181;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33258;&#36866;&#24212;&#23545;&#31216;&#23398;&#20064;&#65288;ASL&#65289;-&#19968;&#31181;&#27169;&#22411;&#26368;&#23567;&#21270;&#30340;&#28436;&#21592;-&#35780;&#35770;&#32773;&#25193;&#23637;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#35299;&#20915;&#19981;&#23436;&#20840;&#25110;&#19981;&#31934;&#30830;&#30340;&#23545;&#31216;&#25551;&#36848;&#12290;ASL&#21253;&#25324;&#19968;&#20010;&#23545;&#31216;&#25311;&#21512;&#32452;&#20214;&#21644;&#19968;&#20010;&#27169;&#22359;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#23427;&#22312;&#25152;&#26377;&#29366;&#24577;&#20013;&#24378;&#21046;&#23454;&#26045;&#20849;&#21516;&#30340;&#23545;&#31216;&#20851;&#31995;&#65292;&#24182;&#36866;&#24212;&#20102;&#25152;&#23398;&#31574;&#30053;&#12290;&#23558;ASL&#30340;&#24615;&#33021;&#19982;&#29616;&#26377;&#30340;&#23545;&#31216;&#22686;&#24378;&#26041;&#27861;&#22312;&#19968;&#20010;&#28041;&#21450;&#22235;&#36275;&#34434;&#34433;&#27169;&#22411;&#30340;&#26696;&#20363;&#30740;&#31350;&#20013;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry, a fundamental concept to understand our environment, often oversimplifies reality from a mathematical perspective. Humans are a prime example, deviating from perfect symmetry in terms of appearance and cognitive biases (e.g. having a dominant hand). Nevertheless, our brain can easily overcome these imperfections and efficiently adapt to symmetrical tasks. The driving motivation behind this work lies in capturing this ability through reinforcement learning. To this end, we introduce Adaptive Symmetry Learning (ASL) $\unicode{x2013}$ a model-minimization actor-critic extension that addresses incomplete or inexact symmetry descriptions by adapting itself during the learning process. ASL consists of a symmetry fitting component and a modular loss function that enforces a common symmetric relation across all states while adapting to the learned policy. The performance of ASL is compared to existing symmetry-enhanced methods in a case study involving a four-legged ant model for mul
&lt;/p&gt;</description></item><item><title>SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2307.05591</link><description>&lt;p&gt;
SITTA: &#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SITTA: A Semantic Image-Text Alignment for Image Captioning. (arXiv:2307.05591v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05591
&lt;/p&gt;
&lt;p&gt;
SITTA&#26159;&#19968;&#31181;&#29992;&#20110;&#22270;&#20687;&#25551;&#36848;&#30340;&#35821;&#20041;&#22270;&#20687;&#25991;&#26412;&#23545;&#40784;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#25104;&#21151;&#22320;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#21644;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22270;&#20687;&#30340;&#25991;&#26412;&#21644;&#35821;&#20041;&#29702;&#35299;&#23545;&#20110;&#29983;&#25104;&#36866;&#24403;&#30340;&#25551;&#36848;&#38750;&#24120;&#37325;&#35201;&#12290;&#36825;&#38656;&#35201;&#26816;&#27979;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#24314;&#27169;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#35780;&#20272;&#22330;&#26223;&#30340;&#35821;&#20041;&#65292;&#24182;&#23558;&#25552;&#21462;&#30340;&#30693;&#35782;&#34920;&#31034;&#22312;&#35821;&#35328;&#31354;&#38388;&#20013;&#12290;&#20026;&#20102;&#22312;&#20445;&#35777;&#33391;&#22909;&#30340;&#22270;&#20687;-&#35821;&#35328;&#26144;&#23556;&#30340;&#21516;&#26102;&#23454;&#29616;&#20016;&#23500;&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#34987;&#26465;&#20214;&#21270;&#20026;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;-&#25991;&#26412;&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#20351;&#29992;&#22270;&#20687;&#36755;&#20837;&#12290;&#36825;&#35201;&#27714;&#23558;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#20013;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#35821;&#35328;&#34920;&#31034;&#36827;&#34892;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#26368;&#22909;&#22320;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#26816;&#27979;&#21040;&#30340;&#35821;&#20041;&#20256;&#36882;&#32473;LM&#36824;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#31181;&#26500;&#24314;&#32447;&#24615;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#23558;&#20004;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23884;&#20837;&#31354;&#38388;&#20043;&#38388;&#30340;&#35821;&#20041;&#36716;&#31227;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#23558;&#22810;&#27169;&#24577;&#35821;&#35328;&#32534;&#30721;&#22120;&#30340;&#23884;&#20837;&#31354;&#38388;&#19982;&#29983;&#25104;&#24615;LM&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual and semantic comprehension of images is essential for generating proper captions. The comprehension requires detection of objects, modeling of relations between them, an assessment of the semantics of the scene and, finally, representing the extracted knowledge in a language space. To achieve rich language capabilities while ensuring good image-language mappings, pretrained language models (LMs) were conditioned on pretrained multi-modal (image-text) models that allow for image inputs. This requires an alignment of the image representation of the multi-modal model with the language representations of a generative LM. However, it is not clear how to best transfer semantics detected by the vision encoder of the multi-modal model to the LM. We introduce two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models. The first aligns the embedding space of the multi-modal language encoder with the embe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28608;&#21169;&#20998;&#37197;&#30340;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.03515</link><description>&lt;p&gt;
&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#28608;&#21169;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
Incentive Allocation in Vertical Federated Learning Based on Bankruptcy Problem. (arXiv:2307.03515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30772;&#20135;&#38382;&#39064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#20013;&#28608;&#21169;&#20998;&#37197;&#30340;&#25361;&#25112;&#65292;&#20197;&#30830;&#20445;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#21512;&#20316;&#35757;&#32451;&#22312;&#19981;&#21516;&#21442;&#19982;&#26041;&#20043;&#38388;&#22402;&#30452;&#21010;&#20998;&#30340;&#31169;&#26377;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;VFL&#35774;&#32622;&#20013;&#65292;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#26041;&#65288;&#25317;&#26377;&#24102;&#26631;&#31614;&#26679;&#26412;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#65289;&#36890;&#36807;&#19982;&#26576;&#20123;&#34987;&#21160;&#26041;&#65288;&#25317;&#26377;&#30456;&#21516;&#26679;&#26412;&#20294;&#27809;&#26377;&#26631;&#31614;&#30340;&#39069;&#22806;&#29305;&#24449;&#30340;&#21442;&#19982;&#26041;&#65289;&#21512;&#20316;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#25913;&#36827;&#20854;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#28608;&#21169;&#34987;&#21160;&#26041;&#21442;&#19982;VFL&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22522;&#20110;&#34987;&#21160;&#26041;&#22312;VFL&#36807;&#31243;&#20013;&#30340;&#36129;&#29486;&#26469;&#20026;&#20182;&#20204;&#20998;&#37197;&#28608;&#21169;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#23450;&#20041;&#20026;&#26680;&#24515;&#28216;&#25103;&#35770;&#27010;&#24565;&#30340;&#19968;&#31181;&#21464;&#20307;&#8212;&#8212;&#30772;&#20135;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#22612;&#26408;&#24503;&#21010;&#20998;&#35268;&#21017;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#23427;&#30830;&#20445;&#20102;&#28608;&#21169;&#30340;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) is a promising approach for collaboratively training machine learning models using private data partitioned vertically across different parties. Ideally in a VFL setting, the active party (party possessing features of samples with labels) benefits by improving its machine learning model through collaboration with some passive parties (parties possessing additional features of the same samples without labels) in a privacy preserving manner. However, motivating passive parties to participate in VFL can be challenging. In this paper, we focus on the problem of allocating incentives to the passive parties by the active party based on their contributions to the VFL process. We formulate this problem as a variant of the Nucleolus game theory concept, known as the Bankruptcy Problem, and solve it using the Talmud's division rule. We evaluate our proposed method on synthetic and real-world datasets and show that it ensures fairness and stability in incentive a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#30340;&#24179;&#28369;&#27169;&#22411;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#20026;&#20160;&#20040;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;</title><link>http://arxiv.org/abs/2306.10947</link><description>&lt;p&gt;
&#20351;&#29992;&#36895;&#29575;&#20989;&#25968;&#29702;&#35299;&#25554;&#20540;&#21306;&#38388;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Generalization in the Interpolation Regime using the Rate Function. (arXiv:2306.10947v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10947
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22823;&#20559;&#24046;&#29702;&#35770;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#30340;&#24179;&#28369;&#27169;&#22411;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#65292;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#20026;&#20160;&#20040;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22823;&#20559;&#24046;&#29702;&#35770;&#30340;&#22522;&#26412;&#21407;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#24179;&#28369;&#24230;&#30340;&#26032;&#29305;&#24449;&#25551;&#36848;&#26041;&#27861;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#24120;&#29992;&#23454;&#25968;&#20540;&#65288;&#22914;&#26435;&#37325;&#33539;&#25968;&#65289;&#26469;&#34920;&#24449;&#27169;&#22411;&#30340;&#24179;&#28369;&#24230;&#65292;&#25105;&#20204;&#34920;&#26126;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#23454;&#20540;&#20989;&#25968;&#26469;&#25551;&#36848;&#24179;&#28369;&#24230;&#12290;&#22522;&#20110;&#27169;&#22411;&#24179;&#28369;&#24230;&#30340;&#36825;&#19968;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#35299;&#37322;&#65292;&#20026;&#20160;&#20040;&#19968;&#20123;&#25554;&#20540;&#22120;&#34920;&#29616;&#20986;&#38750;&#24120;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#21450;&#20026;&#20160;&#20040;&#24191;&#27867;&#20351;&#29992;&#30340;&#29616;&#20195;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65292;$\ell_2$-&#35268;&#33539;&#21270;&#65292;&#25968;&#25454;&#22686;&#24378;&#65292;&#19981;&#21464;&#30340;&#26550;&#26500;&#21644;&#36229;&#21442;&#25968;&#21270;&#65289;&#33021;&#22815;&#25214;&#21040;&#23427;&#20204;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#25552;&#20379;&#20102;&#20114;&#34917;&#30340;&#36807;&#31243;&#65292;&#36825;&#20123;&#36807;&#31243;&#20351;&#20248;&#21270;&#22120;&#20559;&#21521;&#20110;&#26356;&#24179;&#28369;&#30340;&#25554;&#20540;&#22120;&#65292;&#32780;&#26681;&#25454;&#36825;&#31181;&#29702;&#35770;&#20998;&#26512;&#65292;&#26356;&#24179;&#28369;&#30340;&#25554;&#20540;&#22120;&#26159;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#35823;&#24046;&#30340;&#25554;&#20540;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel characterization of the smoothness of a model based on basic principles of Large Deviation Theory. In contrast to prior work, where the smoothness of a model is normally characterized by a real value (e.g., the weights' norm), we show that smoothness can be described by a simple real-valued function. Based on this concept of smoothness, we propose an unifying theoretical explanation of why some interpolators generalize remarkably well and why a wide range of modern learning techniques (i.e., stochastic gradient descent, $\ell_2$-norm regularization, data augmentation, invariant architectures, and overparameterization) are able to find them. The emergent conclusion is that all these methods provide complimentary procedures that bias the optimizer to smoother interpolators, which, according to this theoretical analysis, are the ones with better generalization error.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#31454;&#20105;&#23545;&#25163;&#30340;&#19981;&#35802;&#23454;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.16272</link><description>&lt;p&gt;
&#22312;&#21327;&#21516;&#23398;&#20064;&#21644;&#20248;&#21270;&#20013;&#28608;&#21169;&#31454;&#20105;&#23545;&#25163;&#35802;&#23454;&#34892;&#20026;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Incentivizing Honesty among Competitors in Collaborative Learning and Optimization. (arXiv:2305.16272v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#22312;&#21327;&#20316;&#23398;&#20064;&#20013;&#31454;&#20105;&#23545;&#25163;&#30340;&#19981;&#35802;&#23454;&#34892;&#20026;&#65292;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#35753;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#27604;&#20165;&#21033;&#29992;&#21333;&#19968;&#25968;&#25454;&#28304;&#30340;&#27169;&#22411;&#25928;&#26524;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#28508;&#22312;&#30340;&#21442;&#19982;&#32773;&#26159;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31454;&#20105;&#23545;&#25163;&#65292;&#22914;&#27599;&#20010;&#37117;&#24076;&#26395;&#36890;&#36807;&#25552;&#20379;&#26368;&#20339;&#25512;&#33616;&#26469;&#21560;&#24341;&#23458;&#25143;&#30340;&#20844;&#21496;&#12290;&#36825;&#21487;&#33021;&#20250;&#28608;&#21169;&#19981;&#35802;&#23454;&#30340;&#26356;&#26032;&#65292;&#25439;&#23475;&#20854;&#20182;&#21442;&#19982;&#32773;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#21487;&#33021;&#30772;&#22351;&#21327;&#20316;&#30340;&#22909;&#22788;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#25551;&#36848;&#36825;&#31181;&#20132;&#20114;&#65292;&#24182;&#22312;&#35813;&#26694;&#26550;&#20869;&#30740;&#31350;&#20102;&#20004;&#20010;&#23398;&#20064;&#20219;&#21153;&#65306;&#21333;&#36718;&#22343;&#20540;&#20272;&#35745;&#21644;&#24378;&#20984;&#30446;&#26631;&#30340;&#22810;&#36718; SGD&#12290;&#23545;&#20110;&#19968;&#31867;&#33258;&#28982;&#30340;&#21442;&#19982;&#32773;&#34892;&#20026;&#65292;&#25105;&#20204;&#21457;&#29616;&#29702;&#24615;&#30340;&#23458;&#25143;&#20250;&#34987;&#28608;&#21169;&#24378;&#28872;&#22320;&#25805;&#32437;&#20182;&#20204;&#30340;&#26356;&#26032;&#65292;&#20174;&#32780;&#38450;&#27490;&#23398;&#20064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#21046;&#26469;&#28608;&#21169;&#35802;&#23454;&#27807;&#36890;&#65292;&#24182;&#30830;&#20445;&#23398;&#20064;&#36136;&#37327;&#19982;&#20840;&#38754;&#21512;&#20316;&#30456;&#24403;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative learning techniques have the potential to enable training machine learning models that are superior to models trained on a single entity's data. However, in many cases, potential participants in such collaborative schemes are competitors on a downstream task, such as firms that each aim to attract customers by providing the best recommendations. This can incentivize dishonest updates that damage other participants' models, potentially undermining the benefits of collaboration. In this work, we formulate a game that models such interactions and study two learning tasks within this framework: single-round mean estimation and multi-round SGD on strongly-convex objectives. For a natural class of player actions, we show that rational clients are incentivized to strongly manipulate their updates, preventing learning. We then propose mechanisms that incentivize honest communication and ensure learning quality comparable to full cooperation. Lastly, we empirically demonstrate the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04979</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Label-free segmentation from cardiac ultrasound using self-supervised learning. (arXiv:2210.04979v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#30340;&#20998;&#21106;&#21644;&#27979;&#37327;&#23545;&#20110;&#24515;&#33039;&#36229;&#22768;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#36825;&#20123;&#20219;&#21153;&#32791;&#26102;&#19988;&#38590;&#20197;&#37325;&#29616;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#36741;&#21161;&#65292;&#20294;&#26159;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#32467;&#21512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20020;&#24202;&#39046;&#22495;&#30693;&#35782;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;450&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;93000&#24352;&#22270;&#29255;&#65289;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;8393&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;4476266&#24352;&#22270;&#29255;&#65292;&#24179;&#22343;&#24180;&#40836;61&#23681;&#65292;&#22899;&#24615;&#21344;51%&#65289;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21033;&#29992;&#20998;&#21106;&#32467;&#26524;&#36827;&#34892;&#29983;&#29289;&#27979;&#37327;&#12290;&#25105;&#20204;&#36824;&#23545;&#26469;&#33258;&#39069;&#22806;10030&#21517;&#24739;&#32773;&#30340;&#22806;&#37096;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#25163;&#21160;&#25551;&#36857;&#30340;&#24038;&#23460;&#20449;&#24687;&#12290;&#22312;&#20960;&#31181;&#19981;&#21516;&#30340;&#27979;&#37327;&#25351;&#26631;&#65288;r2 0.56-0.84&#65289;&#19978;&#65292;&#20020;&#24202;&#27979;&#37327;&#21644;&#25105;&#20204;&#30340;&#27969;&#31243;&#39044;&#27979;&#20043;&#38388;&#30340;r2&#20540;&#19982;&#24050;&#25253;&#36947;&#30340;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#21464;&#24322;&#31243;&#24230;&#30456;&#20284;&#65292;&#24182;&#19988;&#19982;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20026;0.85&#65288;&#33539;&#22260;0.71-0.97&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation combining computer vision, clinical domain knowledge, and deep learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean 61 years, 51% female), using the resulting segmentations to calculate biometrics. We also tested against external images from an additional 10,030 patients with available manual tracings of the left ventricle. r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation and comparable to supervised learning across several different measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber size and function was 0.85 (range 0.71-0.97) compar
&lt;/p&gt;</description></item><item><title>&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23376;&#31354;&#38388;&#35757;&#32451;&#21644;&#26435;&#37325;&#24179;&#22343;&#20540;&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#26524;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>http://arxiv.org/abs/2205.13104</link><description>&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#65306;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;&#19968;&#33324;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Trainable Weight Averaging: A General Approach for Subspace Training. (arXiv:2205.13104v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13104
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35757;&#32451;&#30340;&#26435;&#37325;&#24179;&#22343;&#20540;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#36830;&#25509;&#23376;&#31354;&#38388;&#35757;&#32451;&#21644;&#26435;&#37325;&#24179;&#22343;&#20540;&#65292;&#25552;&#20379;&#39640;&#25928;&#30340;&#35757;&#32451;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#25928;&#26524;&#21644;&#38477;&#20302;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#20013;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#26159;&#23454;&#29616;&#39640;&#25928;&#35757;&#32451;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#26041;&#21521;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#25110;&#22312;&#35757;&#32451;&#36712;&#36857;&#19978;&#25191;&#34892;&#38477;&#32500;&#26041;&#27861;&#26469;&#25552;&#21462;&#23376;&#31354;&#38388;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#32500;&#24230;&#21644;&#25968;&#20540;&#36816;&#31639;&#26041;&#38754;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#25110;&#19981;&#31283;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23376;&#31354;&#38388;&#35757;&#32451;&#19982;&#26435;&#37325;&#24179;&#22343;&#20540;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#35757;&#32451;&#26435;&#37325;&#24179;&#22343;&#20540;(TWA)&#65292;&#36825;&#26159;&#19968;&#31181;&#27867;&#21270;&#20197;&#21069;&#21162;&#21147;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;TWA&#22312;&#32500;&#24230;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#65292;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#26041;&#26696;&#26469;&#24212;&#23545;&#22823;&#35268;&#27169;&#38382;&#39064;&#30340;&#23376;&#31354;&#38388;&#35757;&#32451;&#65292;&#23427;&#20801;&#35768;&#22810;&#20010;&#33410;&#28857;&#19978;&#30340;&#24182;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#20869;&#23384;&#21644;&#35745;&#31639;&#36127;&#25285;&#22343;&#21248;&#20998;&#37197;&#32473;&#27599;&#20010;&#33410;&#28857;&#12290;&#25105;&#20204;&#23558;TWA&#24212;&#29992;&#20110;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Training deep neural networks (DNNs) in low-dimensional subspaces is a promising direction for achieving efficient training and better generalization performance. Previous works extract the subspaces by using random projection or performing dimensionality reduction method on the training trajectory, but these methods can be inefficient or unstable in terms of dimensionality and numerical operations. In this paper, we connect subspace training to weight averaging and propose Trainable Weight Averaging (TWA), a general approach for subspace training that generalizes the previous efforts. TWA is efficient in terms of dimensionality and also easy to use, making it a promising new method for subspace training. We further design an efficient scheme for subspace training to cope with large-scale problems, which allows parallel training across multiple nodes and evenly distributing the memory and computation burden to each node. We apply TWA to efficient neural network training and improving f
&lt;/p&gt;</description></item></channel></rss>