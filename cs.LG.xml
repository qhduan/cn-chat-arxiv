<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13522</link><description>&lt;p&gt;
REAL&#65306;&#29992;&#20110;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#33539;&#20363;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;(EFCIL)&#26088;&#22312;&#20943;&#36731;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#27809;&#26377;&#21487;&#29992;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#19982;&#23384;&#20648;&#21382;&#21490;&#26679;&#26412;&#30340;&#22238;&#25918;&#24335;CIL&#30456;&#27604;&#65292;EFCIL&#22312;&#26080;&#33539;&#20363;&#32422;&#26463;&#19979;&#26356;&#23481;&#26131;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#20998;&#26512;&#23398;&#20064;(AL)&#30340;CIL&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;EFCIL&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;(REAL)&#12290;REAL&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;(DS-BPT)&#21644;&#19968;&#20010;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;(RED)&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#12290;DS-BPT&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;(SSCL)&#20004;&#20010;&#27969;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#30784;&#30693;&#35782;&#25552;&#21462;&#12290;RED&#36807;&#31243;&#23558;&#30417;&#30563;&#30693;&#35782;&#25552;&#28860;&#21040;SSCL&#39044;&#35757;&#32451;&#39592;&#24178;&#37096;&#20998;&#65292;&#20419;&#36827;&#21518;&#32493;&#30340;&#22522;&#20110;AL&#30340;CIL&#65292;&#23558;CIL&#36716;&#25442;&#20026;&#36882;&#24402;&#26368;&#23567;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13522v1 Announce Type: new  Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning without available historical data. Compared with its counterpart (replay-based CIL) that stores historical samples, the EFCIL suffers more from forgetting issues under the exemplar-free constraint. In this paper, inspired by the recently developed analytic learning (AL) based CIL, we propose a representation enhanced analytic learning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining (DS-BPT) and a representation enhancing distillation (RED) process to enhance the representation of the extractor. The DS-BPT pretrains model in streams of both supervised learning and self-supervised contrastive learning (SSCL) for base knowledge extraction. The RED process distills the supervised knowledge to the SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that converts the CIL to a recursive least
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;VCoTTA&#65292;&#19968;&#31181;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#27979;&#37327;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#37319;&#29992;&#21464;&#20998;&#39044;&#28909;&#31574;&#30053;&#23558;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#22343;&#20540;&#25945;&#24072;&#26356;&#26032;&#31574;&#30053;&#26469;&#26356;&#26032;&#23398;&#29983;&#27169;&#22411;&#65292;&#32467;&#21512;&#28304;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#30340;&#20808;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;&#20808;&#39564;&#20559;&#31227;&#26041;&#38754;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.08182</link><description>&lt;p&gt;
&#21464;&#20998;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Variational Continual Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08182
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;VCoTTA&#65292;&#19968;&#31181;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#29992;&#20110;&#27979;&#37327;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#37319;&#29992;&#21464;&#20998;&#39044;&#28909;&#31574;&#30053;&#23558;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#22343;&#20540;&#25945;&#24072;&#26356;&#26032;&#31574;&#30053;&#26469;&#26356;&#26032;&#23398;&#29983;&#27169;&#22411;&#65292;&#32467;&#21512;&#28304;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#30340;&#20808;&#39564;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;&#20808;&#39564;&#20559;&#31227;&#26041;&#38754;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#39564;&#20559;&#31227;&#22312;&#21482;&#20351;&#29992;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;&#65288;CTTA&#65289;&#26041;&#27861;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#35823;&#24046;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;VCoTTA&#65292;&#19968;&#31181;&#29992;&#20110;&#27979;&#37327;CTTA&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#21464;&#20998;&#36125;&#21494;&#26031;&#26041;&#27861;&#12290;&#22312;&#28304;&#38454;&#27573;&#65292;&#25105;&#20204;&#36890;&#36807;&#21464;&#20998;&#39044;&#28909;&#31574;&#30053;&#23558;&#39044;&#35757;&#32451;&#30340;&#30830;&#23450;&#24615;&#27169;&#22411;&#36716;&#21270;&#20026;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNN&#65289;&#65292;&#23558;&#19981;&#30830;&#23450;&#24615;&#27880;&#20837;&#27169;&#22411;&#20013;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#25105;&#20204;&#37319;&#29992;&#21464;&#20998;&#25512;&#26029;&#30340;&#22343;&#20540;&#25945;&#24072;&#26356;&#26032;&#31574;&#30053;&#65292;&#23558;&#23398;&#29983;&#27169;&#22411;&#21644;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#27861;&#29992;&#20110;&#25945;&#24072;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#28304;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#30340;&#20808;&#39564;&#26469;&#26356;&#26032;&#23398;&#29983;&#27169;&#22411;&#12290;&#35777;&#25454;&#19979;&#30028;&#34987;&#21046;&#23450;&#20026;&#23398;&#29983;&#27169;&#22411;&#21644;&#25945;&#24072;&#27169;&#22411;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#65292;&#20197;&#21450;&#20808;&#39564;&#28151;&#21512;&#30340;Kullback-Leibler&#65288;KL&#65289;&#25955;&#24230;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20943;&#36731;&#22312;CTTA&#20013;&#30340;&#20808;&#39564;&#20559;&#31227;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prior drift is crucial in Continual Test-Time Adaptation (CTTA) methods that only use unlabeled test data, as it can cause significant error propagation. In this paper, we introduce VCoTTA, a variational Bayesian approach to measure uncertainties in CTTA. At the source stage, we transform a pre-trained deterministic model into a Bayesian Neural Network (BNN) via a variational warm-up strategy, injecting uncertainties into the model. During the testing time, we employ a mean-teacher update strategy using variational inference for the student model and exponential moving average for the teacher model. Our novel approach updates the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating prior drift within th
&lt;/p&gt;</description></item><item><title>SketchOGD&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.16424</link><description>&lt;p&gt;
SketchOGD&#65306;&#20869;&#23384;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SketchOGD: Memory-Efficient Continual Learning. (arXiv:2305.16424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16424
&lt;/p&gt;
&lt;p&gt;
SketchOGD&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#25345;&#32493;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#23481;&#26131;&#24536;&#35760;&#20808;&#21069;&#20219;&#21153;&#19978;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#24448;&#24448;&#28041;&#21450;&#23384;&#20648;&#36807;&#21435;&#20219;&#21153;&#30340;&#20449;&#24687;&#65292;&#36825;&#24847;&#21619;&#30528;&#20869;&#23384;&#20351;&#29992;&#26159;&#30830;&#23450;&#23454;&#29992;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#19968;&#31181;&#24050;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;OGD&#21033;&#29992;&#20808;&#21069;&#27169;&#22411;&#26799;&#24230;&#26469;&#25214;&#21040;&#32500;&#25345;&#20808;&#21069;&#25968;&#25454;&#28857;&#24615;&#33021;&#30340;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#26799;&#24230;&#30340;&#20869;&#23384;&#25104;&#26412;&#38543;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#22686;&#38271;&#32780;&#22686;&#21152;&#65292;&#22240;&#27492;OGD&#19981;&#36866;&#29992;&#20110;&#20219;&#24847;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SketchOGD&#12290;SketchOGD&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
When machine learning models are trained continually on a sequence of tasks, they are liable to forget what they learned on previous tasks -- a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper proposes a memory-efficient solution to catastrophic forgetting, improving upon an established algorithm known as orthogonal gradient descent (OGD). OGD utilizes prior model gradients to find weight updates that preserve performance on prior datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over arbitrarily long time horizons. To address this problem, this paper proposes SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fix
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#22312;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#26102;&#65292;&#26368;&#20248;&#26399;&#26395;&#38169;&#35823;&#36793;&#30028;&#31561;&#20110;&#20854;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#12290;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#19982;&#26368;&#20339;&#20989;&#25968;&#30340;&#38169;&#35823;&#27425;&#25968;&#20043;&#38388;&#23384;&#22312;&#29305;&#23450;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.13849</link><description>&lt;p&gt;
&#20351;&#29992;&#19987;&#23478;&#24314;&#35758;&#21644;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#36827;&#34892;&#26368;&#20248;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension. (arXiv:2302.13849v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13849
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#22312;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035;&#26102;&#65292;&#26368;&#20248;&#26399;&#26395;&#38169;&#35823;&#36793;&#30028;&#31561;&#20110;&#20854;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#12290;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#19982;&#26368;&#20339;&#20989;&#25968;&#30340;&#38169;&#35823;&#27425;&#25968;&#20043;&#38388;&#23384;&#22312;&#29305;&#23450;&#20851;&#31995;&#12290;&#27492;&#22806;&#65292;&#35813;&#30740;&#31350;&#36824;&#35299;&#20915;&#20102;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#65292;&#32463;&#20856;&#30340;&#32467;&#26524;&#34920;&#26126;&#20351;&#29992;&#30830;&#23450;&#24615;&#23398;&#20064;&#22120;&#30340;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#21487;&#20197;&#36890;&#36807;Littlestone&#32500;&#24230;&#26469;&#23454;&#29616;&#65288;Littlestone '88&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#38543;&#26426;&#23398;&#20064;&#22120;&#30340;&#31867;&#20284;&#32467;&#26524;&#65306;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#23398;&#20064;&#19968;&#20010;&#31867;&#21035; $\mathcal{H}$&#26102;&#65292;&#26368;&#20248;&#26399;&#26395;&#38169;&#35823;&#36793;&#30028;&#31561;&#20110;&#20854;&#38543;&#26426;&#21270;&#30340;Littlestone&#32500;&#24230;&#65292;&#21363;&#23384;&#22312;&#19968;&#20010;&#30001; $\mathcal{H}$ &#25171;&#30862;&#30340;&#26641;&#65292;&#20854;&#24179;&#22343;&#28145;&#24230;&#20026; $2d$&#65292;&#32780; $d$ &#26159;&#26368;&#22823;&#30340;&#32500;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#19982; $\mathcal{H}$ &#20013;&#26368;&#20339;&#20989;&#25968;&#30340;&#38169;&#35823;&#27425;&#25968; $k$ &#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;Littlestone&#32500;&#24230; $d$ &#30340;&#31867;&#21035;&#23398;&#20064;&#30340;&#26368;&#20248;&#38543;&#26426;&#21270;&#38169;&#35823;&#36793;&#30028;&#26159; $k + \Theta (\sqrt{k d} + d )$&#12290;&#36825;&#20063;&#24847;&#21619;&#30528;&#30830;&#23450;&#24615;&#23398;&#20064;&#30340;&#26368;&#20248;&#38169;&#35823;&#36793;&#30028;&#26159; $2k + O (\sqrt{k d} + d )$&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;Auer&#21644;Long ['99]&#30740;&#31350;&#30340;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#20316;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#32463;&#20856;&#38382;&#39064;&#30340;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
A classical result in online learning characterizes the optimal mistake bound achievable by deterministic learners using the Littlestone dimension (Littlestone '88). We prove an analogous result for randomized learners: we show that the optimal expected mistake bound in learning a class $\mathcal{H}$ equals its randomized Littlestone dimension, which is the largest $d$ for which there exists a tree shattered by $\mathcal{H}$ whose average depth is $2d$. We further study optimal mistake bounds in the agnostic case, as a function of the number of mistakes made by the best function in $\mathcal{H}$, denoted by $k$. We show that the optimal randomized mistake bound for learning a class with Littlestone dimension $d$ is $k + \Theta (\sqrt{k d} + d )$. This also implies an optimal deterministic mistake bound of $2k + O (\sqrt{k d} + d )$, thus resolving an open question which was studied by Auer and Long ['99].  As an application of our theory, we revisit the classical problem of prediction 
&lt;/p&gt;</description></item></channel></rss>