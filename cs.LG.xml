<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;</title><link>https://rss.arxiv.org/abs/2312.09196</link><description>&lt;p&gt;
DIRECT: &#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIRECT: Deep Active Learning under Imbalance and Label Noise
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.09196
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DIRECT&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#28145;&#24230;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#12290;&#36890;&#36807;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#32597;&#35265;&#31867;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#31867;&#21035;&#19981;&#24179;&#34913;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#32597;&#35265;&#21644;&#23569;&#25968;&#31867;&#30340;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#20027;&#21160;&#23398;&#20064;&#21487;&#33021;&#26159;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#26368;&#26377;&#25928;&#25216;&#26415;&#65292;&#23427;&#20174;&#26681;&#26412;&#19978;&#37319;&#38598;&#26356;&#24179;&#34913;&#21644;&#20855;&#26377;&#20449;&#24687;&#37327;&#30340;&#26631;&#35760;&#31034;&#20363;&#36827;&#34892;&#27880;&#37322;&#12290;&#26631;&#31614;&#22122;&#22768;&#26159;&#25968;&#25454;&#27880;&#37322;&#20219;&#21153;&#20013;&#21478;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#65292;&#23545;&#20110;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#26469;&#35828;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#21644;&#26631;&#31614;&#22122;&#22768;&#19979;&#30340;&#20027;&#21160;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#31283;&#20581;&#22320;&#30830;&#23450;&#31867;&#21035;&#20998;&#21106;&#38408;&#20540;&#24182;&#26631;&#35760;&#26368;&#19981;&#30830;&#23450;&#19988;&#31163;&#20854;&#26368;&#36817;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#31616;&#21270;&#20026;&#19968;&#32500;&#20027;&#21160;&#23398;&#20064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;DIRECT&#33021;&#22815;&#21033;&#29992;&#32463;&#20856;&#30340;&#20027;&#21160;&#23398;&#20064;&#25991;&#29486;&#26469;&#35299;&#20915;&#25209;&#27425;&#26631;&#35760;&#21644;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#23481;&#24525;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22823;&#37327;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class imbalance is a prevalent issue in real world machine learning applications, often leading to poor performance in rare and minority classes. With an abundance of wild unlabeled data, active learning is perhaps the most effective technique in solving the problem at its root -- collecting a more balanced and informative set of labeled examples during annotation. Label noise is another common issue in data annotation jobs, which is especially challenging for active learning methods. In this work, we conduct the first study of active learning under both class imbalance and label noise. We propose a novel algorithm that robustly identifies the class separation threshold and annotates the most uncertain examples that are closest from it. Through a novel reduction to one-dimensional active learning, our algorithm DIRECT is able to leverage the classic active learning literature to address issues such as batch labeling and tolerance towards label noise. We present extensive experiments on
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.12503</link><description>&lt;p&gt;
&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#23041;&#32961;&#12289;&#28431;&#27934;&#21644;&#36127;&#36131;&#20219;&#30340;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Securing Large Language Models: Threats, Vulnerabilities and Responsible Practices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12503
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26174;&#33879;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#26684;&#23616;&#12290;&#23427;&#20204;&#23545;&#21508;&#31181;&#20219;&#21153;&#20135;&#29983;&#20102;&#24433;&#21709;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#20102;&#25105;&#20204;&#22788;&#29702;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#23427;&#20204;&#24341;&#20154;&#27880;&#30446;&#30340;&#23454;&#29992;&#24615;&#22806;&#65292;LLMs&#36824;&#24102;&#26469;&#20102;&#37325;&#35201;&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#32771;&#34385;&#12290;&#36825;&#20123;&#25361;&#25112;&#38656;&#35201;&#20180;&#32454;&#30740;&#31350;&#65292;&#20197;&#30830;&#20445;&#36127;&#36131;&#20219;&#30340;&#37096;&#32626;&#65292;&#24182;&#38450;&#33539;&#28508;&#22312;&#30340;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#20840;&#38754;&#35843;&#26597;&#20102;&#19982;LLMs&#30456;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20174;&#20116;&#20010;&#20027;&#39064;&#35282;&#24230;&#36827;&#34892;&#65306;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12289;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#28431;&#27934;&#12289;LLMs&#35823;&#29992;&#21487;&#33021;&#36896;&#25104;&#30340;&#28508;&#22312;&#21361;&#23475;&#12289;&#32531;&#35299;&#31574;&#30053;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#21516;&#26102;&#35782;&#21035;&#24403;&#21069;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#24314;&#35758;&#26410;&#26469;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#23433;&#20840;&#21644;&#39118;&#38505;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12503v1 Announce Type: cross  Abstract: Large language models (LLMs) have significantly transformed the landscape of Natural Language Processing (NLP). Their impact extends across a diverse spectrum of tasks, revolutionizing how we approach language understanding and generations. Nevertheless, alongside their remarkable utility, LLMs introduce critical security and risk considerations. These challenges warrant careful examination to ensure responsible deployment and safeguard against potential vulnerabilities. This research paper thoroughly investigates security and privacy concerns related to LLMs from five thematic perspectives: security and privacy concerns, vulnerabilities against adversarial attacks, potential harms caused by misuses of LLMs, mitigation strategies to address these challenges while identifying limitations of current strategies. Lastly, the paper recommends promising avenues for future research to enhance the security and risk management of LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.08680</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#36731;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#29289;&#20307;&#24187;&#35273;
&lt;/p&gt;
&lt;p&gt;
Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;MARINE&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#20943;&#23569;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#20307;&#24187;&#35273;&#12290;&#35813;&#26694;&#26550;&#26080;&#38656;&#35757;&#32451;&#25110;API&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#35270;&#35273;&#27169;&#22411;&#21644;&#24341;&#20837;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#29983;&#25104;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLM&#65289;&#30340;&#36827;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#23427;&#20204;&#22312;&#22270;&#20687;&#20013;&#20135;&#29983;&#34394;&#20551;&#29289;&#20307;&#30340;&#20005;&#37325;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#30528;&#37325;&#20110;&#20351;&#29992;&#29305;&#27530;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#25110;&#24378;&#22823;&#30340;LLM&#65288;&#20363;&#22914;GPT-3.5&#65289;&#26469;&#32416;&#27491;LVLM&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#35201;&#27714;&#26114;&#36149;&#30340;&#35757;&#32451;/&#24494;&#35843;&#25110;API&#35775;&#38382;&#20808;&#36827;&#30340;LLM&#26469;&#22312;&#29983;&#25104;&#21518;&#32416;&#27491;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#21517;&#20026;&#36890;&#36807;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#32531;&#35299;&#24187;&#35273;&#30340;&#26694;&#26550;&#65288;MARINE&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#26694;&#26550;&#26082;&#26080;&#38656;&#35757;&#32451;&#20063;&#26080;&#38656;API&#35775;&#38382;&#65292;&#21487;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#26377;&#25928;&#22320;&#20943;&#23569;&#29289;&#20307;&#24187;&#35273;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;MARINE&#36890;&#36807;&#38598;&#25104;&#29616;&#26377;&#30340;&#24320;&#28304;&#35270;&#35273;&#27169;&#22411;&#20016;&#23500;LVLM&#30340;&#35270;&#35273;&#35821;&#22659;&#65292;&#24182;&#20351;&#29992;&#26080;&#20998;&#31867;&#22120;&#24341;&#23548;&#26469;&#25972;&#21512;&#39069;&#22806;&#30340;&#29289;&#20307;&#22522;&#30784;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;LVLM&#29983;&#25104;&#30340;&#31934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advancement of Large Vision-Language Models (LVLMs) has increasingly highlighted the critical issue of their tendency to hallucinate non-existing objects in the images. To address this issue, previous works focused on using specially curated datasets or powerful LLMs (e.g., GPT-3.5) to rectify the outputs of LVLMs. However, these approaches require either expensive training/fine-tuning or API access to advanced LLMs to correct the model's output post-generation. In this paper, we tackle this challenge by introducing a framework called Mitigating hallucinAtion via classifieR-Free guIdaNcE (MARINE), which is both training-free and API-free, and can effectively and efficiently reduce object hallucinations during the generation process. Specifically, MARINE enriches the visual context of LVLMs by integrating existing open-source vision models, and employs classifier-free guidance to incorporate the additional object grounding features to improve the precision of LVLMs' generations. Thr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.06535</link><description>&lt;p&gt;
Bandit Convex Optimisation&#65288;&#24378;&#30423;&#20984;&#20248;&#21270;&#65289;
&lt;/p&gt;
&lt;p&gt;
Bandit Convex Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06535
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#24378;&#30423;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#21644;&#29992;&#20110;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#22810;&#31181;&#24037;&#20855;&#12290;&#34429;&#28982;&#27809;&#26377;&#22826;&#22810;&#21019;&#26032;&#65292;&#20294;&#36890;&#36807;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#29616;&#26377;&#24037;&#20855;&#65292;&#33719;&#24471;&#20102;&#26032;&#30340;&#31639;&#27861;&#21644;&#25913;&#36827;&#20102;&#19968;&#20123;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#30423;&#20984;&#20248;&#21270;&#26159;&#30740;&#31350;&#38646;&#38454;&#20984;&#20248;&#21270;&#30340;&#22522;&#26412;&#26694;&#26550;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35768;&#22810;&#24037;&#20855;&#65292;&#21253;&#25324;&#20999;&#24179;&#38754;&#26041;&#27861;&#12289;&#20869;&#28857;&#26041;&#27861;&#12289;&#36830;&#32493;&#25351;&#25968;&#26435;&#37325;&#12289;&#26799;&#24230;&#19979;&#38477;&#21644;&#22312;&#32447;&#29275;&#39039;&#27493;&#39588;&#12290;&#35299;&#37322;&#20102;&#35768;&#22810;&#20551;&#35774;&#21644;&#35774;&#32622;&#20043;&#38388;&#30340;&#32454;&#24494;&#24046;&#21035;&#12290;&#23613;&#31649;&#22312;&#36825;&#37324;&#27809;&#26377;&#22826;&#22810;&#30495;&#27491;&#26032;&#30340;&#19996;&#35199;&#65292;&#20294;&#19968;&#20123;&#29616;&#26377;&#24037;&#20855;&#20197;&#26032;&#39062;&#30340;&#26041;&#24335;&#24212;&#29992;&#20110;&#33719;&#24471;&#26032;&#31639;&#27861;&#12290;&#19968;&#20123;&#30028;&#38480;&#31245;&#24494;&#25913;&#36827;&#20102;&#19968;&#20123;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bandit convex optimisation is a fundamental framework for studying zeroth-order convex optimisation. These notes cover the many tools used for this problem, including cutting plane methods, interior point methods, continuous exponential weights, gradient descent and online Newton step. The nuances between the many assumptions and setups are explained. Although there is not much truly new here, some existing tools are applied in novel ways to obtain new algorithms. A few bounds are improved in minor ways.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36125;&#21494;&#26031;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36870;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24314;&#27169;&#21644;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16612</link><description>&lt;p&gt;
&#22312;&#36870;&#38382;&#39064;&#20013;&#23398;&#20064;&#39640;&#26031;&#28151;&#21512;&#29289;&#36827;&#34892;&#31232;&#30095;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Learning a Gaussian Mixture for Sparsity Regularization in Inverse Problems. (arXiv:2401.16612v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36125;&#21494;&#26031;&#20272;&#35745;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#36870;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24314;&#27169;&#21644;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36870;&#38382;&#39064;&#20013;&#65292;&#24191;&#27867;&#35748;&#20026;&#24341;&#20837;&#31232;&#30095;&#20808;&#39564;&#23545;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#27491;&#21017;&#21270;&#25928;&#26524;&#12290;&#36825;&#31181;&#26041;&#27861;&#26159;&#22522;&#20110;&#19968;&#20010;&#20808;&#39564;&#20551;&#35774;&#65292;&#21363;&#26410;&#30693;&#37327;&#21487;&#20197;&#22312;&#19968;&#20010;&#26377;&#38480;&#25968;&#37327;&#30340;&#26174;&#33879;&#25104;&#20998;&#30340;&#22522;&#30784;&#19978;&#36866;&#24403;&#34920;&#31034;&#65292;&#32780;&#22823;&#22810;&#25968;&#31995;&#25968;&#25509;&#36817;&#20110;&#38646;&#12290;&#36825;&#31181;&#24773;&#20917;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#32463;&#24120;&#20986;&#29616;&#65292;&#27604;&#22914;&#20998;&#27573;&#24179;&#28369;&#20449;&#21495;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#39640;&#26031;&#36864;&#21270;&#28151;&#21512;&#29289;&#24418;&#24335;&#34920;&#36848;&#30340;&#27010;&#29575;&#31232;&#30095;&#20808;&#39564;&#65292;&#33021;&#22815;&#23545;&#20110;&#20219;&#24847;&#22522;&#36827;&#34892;&#31232;&#30095;&#24314;&#27169;&#12290;&#22312;&#36825;&#20010;&#21069;&#25552;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21487;&#20197;&#35299;&#37322;&#20026;&#32447;&#24615;&#36870;&#38382;&#39064;&#30340;&#36125;&#21494;&#26031;&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#20272;&#35745;&#36825;&#20010;&#32593;&#32476;&#30340;&#21442;&#25968;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19982;&#24120;&#29992;&#30340;&#31232;&#30095;&#27491;&#21017;&#21270;&#26041;&#27861;&#30340;&#25968;&#20540;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
In inverse problems, it is widely recognized that the incorporation of a sparsity prior yields a regularization effect on the solution. This approach is grounded on the a priori assumption that the unknown can be appropriately represented in a basis with a limited number of significant components, while most coefficients are close to zero. This occurrence is frequently observed in real-world scenarios, such as with piecewise smooth signals. In this study, we propose a probabilistic sparsity prior formulated as a mixture of degenerate Gaussians, capable of modeling sparsity with respect to a generic basis. Under this premise, we design a neural network that can be interpreted as the Bayes estimator for linear inverse problems. Additionally, we put forth both a supervised and an unsupervised training strategy to estimate the parameters of this network. To evaluate the effectiveness of our approach, we conduct a numerical comparison with commonly employed sparsity-promoting regularization
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.12533</link><description>&lt;p&gt;
&#26377;&#25928;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;k&#20013;&#24515;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;k&#20013;&#24515;&#32858;&#31867;&#19978;&#21033;&#29992;&#32972;&#26223;&#30693;&#35782;&#30340;&#32422;&#26463;&#32858;&#31867;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#24471;&#21040;&#20102;&#25928;&#29575;&#39640;&#19988;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20013;&#24515;&#20026;&#22522;&#30784;&#30340;&#32858;&#31867;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#20013;&#37117;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20852;&#36259;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36755;&#20837;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#32858;&#31867;&#32467;&#26524;&#30340;&#32972;&#26223;&#30693;&#35782;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;k&#20013;&#24515;&#32858;&#31867;&#65292;&#24182;&#23558;&#20854;&#36755;&#20837;&#30340;&#32972;&#26223;&#30693;&#35782;&#24314;&#27169;&#20026;&#24517;&#36830;&#65288;ML&#65289;&#21644;&#19981;&#36830;&#65288;CL&#65289;&#32422;&#26463;&#38598;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#21253;&#25324;k&#20013;&#24515;&#22312;&#20869;&#30340;&#32858;&#31867;&#38382;&#39064;&#26412;&#36136;&#19978;&#37117;&#26159;NP&#22256;&#38590;&#30340;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#21463;&#32422;&#26463;&#21464;&#20307;&#34987;&#35748;&#20026;&#21463;&#21040;&#26356;&#20005;&#37325;&#30340;&#36817;&#20284;&#21644;&#35745;&#31639;&#38556;&#30861;&#30340;&#38480;&#21046;&#65292;&#26497;&#22823;&#22320;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#37319;&#29992;&#19968;&#31995;&#21015;&#25216;&#26415;&#65292;&#21253;&#25324;&#21453;&#25903;&#37197;&#38598;&#65292;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#25972;&#25968;&#24179;&#38754;&#21644;LP&#23545;&#20598;&#24615;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#26368;&#20339;&#36817;&#20284;&#27604;&#20363;2&#30340;&#32422;&#26463;k&#20013;&#24515;&#30340;&#39640;&#25928;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#26500;&#24314;&#20102;&#31454;&#20105;&#22522;&#20934;&#31639;&#27861;&#65292;&#24182;&#23545;&#25105;&#20204;&#30340;&#36817;&#20284;&#31639;&#27861;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Center-based clustering has attracted significant research interest from both theory and practice. In many practical applications, input data often contain background knowledge that can be used to improve clustering results. In this work, we build on widely adopted $k$-center clustering and model its input background knowledge as must-link (ML) and cannot-link (CL) constraint sets. However, most clustering problems including $k$-center are inherently $\mathcal{NP}$-hard, while the more complex constrained variants are known to suffer severer approximation and computation barriers that significantly limit their applicability. By employing a suite of techniques including reverse dominating sets, linear programming (LP) integral polyhedron, and LP duality, we arrive at the first efficient approximation algorithm for constrained $k$-center with the best possible ratio of 2. We also construct competitive baseline algorithms and empirically evaluate our approximation algorithm against them o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.01343</link><description>&lt;p&gt;
IoTGeM: &#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#30340;&#36890;&#29992;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IoTGeM: Generalizable Models for Behaviour-Based IoT Attack Detection. (arXiv:2401.01343v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#20110;&#34892;&#20026;&#30340;&#29289;&#32852;&#32593;&#25915;&#20987;&#26816;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#25913;&#36827;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#12289;&#24341;&#20837;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#12289;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#20197;&#21450;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#65292;&#24182;&#19988;&#32463;&#36807;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20851;&#20110;&#29289;&#32852;&#32593;&#35774;&#22791;&#32593;&#32476;&#30340;&#22522;&#20110;&#34892;&#20026;&#30340;&#25915;&#20987;&#26816;&#27979;&#30740;&#31350;&#65292;&#25152;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#26377;&#38480;&#65292;&#24182;&#19988;&#24448;&#24448;&#27809;&#26377;&#24471;&#21040;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#24314;&#27169;&#29289;&#32852;&#32593;&#32593;&#32476;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#20110;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#20063;&#33021;&#25552;&#39640;&#26816;&#27979;&#21644;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#28378;&#21160;&#31383;&#21475;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#27493;&#39588;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#26469;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#38548;&#31163;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#38598;&#26469;&#26500;&#24314;&#21644;&#27979;&#35797;&#27169;&#22411;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#20808;&#21069;&#27169;&#22411;&#22312;&#36890;&#29992;&#24615;&#26041;&#38754;&#30340;&#24120;&#35265;&#25968;&#25454;&#27844;&#28431;&#38382;&#39064;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26469;&#22686;&#21152;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#20174;&#32780;&#33021;&#22815;&#35782;&#21035;&#20986;&#25903;&#25745;&#25915;&#20987;&#20934;&#30830;&#26816;&#27979;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Previous research on behaviour-based attack detection on networks of IoT devices has resulted in machine learning models whose ability to adapt to unseen data is limited, and often not demonstrated. In this paper we present an approach for modelling IoT network attacks that focuses on generalizability, yet also leads to better detection and performance. First, we present an improved rolling window approach for feature extraction, and introduce a multi-step feature selection process that reduces overfitting. Second, we build and test models using isolated train and test datasets, thereby avoiding common data leaks that have limited the generalizability of previous models. Third, we rigorously evaluate our methodology using a diverse portfolio of machine learning models, evaluation metrics and datasets. Finally, we build confidence in the models by using explainable AI techniques, allowing us to identify the features that underlie accurate detection of attacks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#23558;&#23545;&#31216;&#24615;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;1. &#24378;&#21046;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;2. &#21457;&#29616;&#26410;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;3. &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#23545;&#31216;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00212</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#24378;&#21046;&#12289;&#21457;&#29616;&#21644;&#25512;&#21160;&#23545;&#31216;&#24615;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework to Enforce, Discover, and Promote Symmetry in Machine Learning. (arXiv:2311.00212v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00212
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19977;&#31181;&#26041;&#24335;&#23558;&#23545;&#31216;&#24615;&#34701;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65306;1. &#24378;&#21046;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;2. &#21457;&#29616;&#26410;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;3. &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20419;&#36827;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#23384;&#22312;&#20110;&#33258;&#28982;&#30028;&#20013;&#65292;&#24182;&#22312;&#29289;&#29702;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#25198;&#28436;&#30528;&#36234;&#26469;&#36234;&#26680;&#24515;&#30340;&#35282;&#33394;&#12290;&#22522;&#26412;&#23545;&#31216;&#24615;&#65292;&#22914;&#24222;&#21152;&#33713;&#19981;&#21464;&#24615;&#65292;&#20351;&#22312;&#22320;&#29699;&#19978;&#23454;&#39564;&#23460;&#20013;&#21457;&#29616;&#30340;&#29289;&#29702;&#23450;&#24459;&#33021;&#22815;&#25512;&#24191;&#21040;&#23431;&#23449;&#30340;&#26368;&#36828;&#22788;&#12290;&#23545;&#31216;&#24615;&#23545;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#23454;&#29616;&#36825;&#31181;&#25512;&#24191;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24179;&#31227;&#19981;&#21464;&#24615;&#20801;&#35768;&#20351;&#29992;&#21442;&#25968;&#26356;&#23569;&#30340;&#27169;&#22411;&#65288;&#22914;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#22312;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#20197;&#19977;&#31181;&#26041;&#24335;&#34701;&#20837;&#23545;&#31216;&#24615;&#65306;1. &#22312;&#35757;&#32451;&#27169;&#22411;&#26102;&#24378;&#21046;&#24050;&#30693;&#30340;&#23545;&#31216;&#24615;&#65307;2. &#21457;&#29616;&#32473;&#23450;&#27169;&#22411;&#25110;&#25968;&#25454;&#38598;&#30340;&#26410;&#30693;&#23545;&#31216;&#24615;&#65307;3. &#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36890;&#36807;&#23398;&#20064;&#25171;&#30772;&#29992;&#25143;&#25351;&#23450;&#30340;&#20505;&#36873;&#32676;&#20307;&#20869;&#30340;&#23545;&#31216;&#24615;&#26469;&#20419;&#36827;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry is present throughout nature and continues to play an increasingly central role in physics and machine learning. Fundamental symmetries, such as Poincar\'{e} invariance, allow physical laws discovered in laboratories on Earth to be extrapolated to the farthest reaches of the universe. Symmetry is essential to achieving this extrapolatory power in machine learning applications. For example, translation invariance in image classification allows models with fewer parameters, such as convolutional neural networks, to be trained on smaller data sets and achieve state-of-the-art performance. In this paper, we provide a unifying theoretical and methodological framework for incorporating symmetry into machine learning models in three ways: 1. enforcing known symmetry when training a model; 2. discovering unknown symmetries of a given model or data set; and 3. promoting symmetry during training by learning a model that breaks symmetries within a user-specified group of candidates when 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#20250;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#31283;&#24577;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2308.06671</link><description>&lt;p&gt;
&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#30340;&#24179;&#34913;&#27861;&#21017;&#19982;&#31283;&#24577;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
Law of Balance and Stationary Distribution of Stochastic Gradient Descent. (arXiv:2308.06671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#20013;&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#20250;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#31283;&#24577;&#20998;&#24067;&#65292;&#35813;&#20998;&#24067;&#23637;&#31034;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#12290;&#36825;&#20123;&#21457;&#29616;&#25581;&#31034;&#20102;&#21160;&#24577;&#26799;&#24230;&#19979;&#38477;&#27861;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#26159;&#25105;&#20204;&#29992;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24456;&#38590;&#29702;&#35299;SGD&#22914;&#20309;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#21644;&#36864;&#21270;&#30340;&#25439;&#22833;&#26354;&#38754;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SGD&#30340;&#23567;&#25209;&#37327;&#22122;&#38899;&#21487;&#20197;&#20351;&#35299;&#20915;&#26041;&#26696;&#21521;&#24179;&#34913;&#35299;&#38752;&#36817;&#65292;&#21482;&#35201;&#25439;&#22833;&#20989;&#25968;&#21253;&#21547;&#19968;&#20010;&#37325;&#26032;&#32553;&#25918;&#23545;&#31216;&#24615;&#12290;&#30001;&#20110;&#31616;&#21333;&#25193;&#25955;&#36807;&#31243;&#21644;SGD&#21160;&#21147;&#23398;&#30340;&#24046;&#24322;&#22312;&#23545;&#31216;&#24615;&#23384;&#22312;&#26102;&#26368;&#37325;&#35201;&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#25439;&#22833;&#20989;&#25968;&#30340;&#23545;&#31216;&#24615;&#26159;&#20102;&#35299;SGD&#24037;&#20316;&#26041;&#24335;&#30340;&#37325;&#35201;&#32447;&#32034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#20010;&#32467;&#26524;&#24212;&#29992;&#20110;&#23548;&#20986;&#20855;&#26377;&#20219;&#24847;&#28145;&#24230;&#21644;&#23485;&#24230;&#30340;&#23545;&#35282;&#32447;&#24615;&#32593;&#32476;&#30340;&#38543;&#26426;&#26799;&#24230;&#27969;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#31283;&#24577;&#20998;&#24067;&#23637;&#29616;&#20102;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#29616;&#35937;&#65292;&#22914;&#30456;&#21464;&#12289;&#30772;&#22351;&#30340;&#36941;&#21382;&#24615;&#21644;&#27874;&#21160;&#21453;&#36716;&#12290;&#36825;&#20123;&#29616;&#35937;&#20165;&#22312;&#28145;&#23618;&#32593;&#32476;&#20013;&#23384;&#22312;&#65292;&#34920;&#26126;&#20102;&#19968;&#31181;&#22522;&#26412;&#30340;&#26032;&#30340;&#21152;&#28145;&#35757;&#32451;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
The stochastic gradient descent (SGD) algorithm is the algorithm we use to train neural networks. However, it remains poorly understood how the SGD navigates the highly nonlinear and degenerate loss landscape of a neural network. In this work, we prove that the minibatch noise of SGD regularizes the solution towards a balanced solution whenever the loss function contains a rescaling symmetry. Because the difference between a simple diffusion process and SGD dynamics is the most significant when symmetries are present, our theory implies that the loss function symmetries constitute an essential probe of how SGD works. We then apply this result to derive the stationary distribution of stochastic gradient flow for a diagonal linear network with arbitrary depth and width. The stationary distribution exhibits complicated nonlinear phenomena such as phase transitions, broken ergodicity, and fluctuation inversion. These phenomena are shown to exist uniquely in deep networks, implying a fundam
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#26680;&#33539;&#22260;&#31354;&#38388;&#65292;&#24341;&#20837;&#20102;&#949;-&#35206;&#30422;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#30830;&#23450;&#25110;&#19981;&#31934;&#30830;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.16516</link><description>&lt;p&gt;
&#23545;&#20110;&#26680;&#33539;&#22260;&#31354;&#38388;&#65292;&#21482;&#38656;&#35201;&#22266;&#23450;&#25968;&#37327;&#30340;&#26597;&#35810;&#23601;&#36275;&#22815;&#20102;
&lt;/p&gt;
&lt;p&gt;
For Kernel Range Spaces a Constant Number of Queries Are Sufficient. (arXiv:2306.16516v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16516
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26680;&#33539;&#22260;&#31354;&#38388;&#65292;&#24341;&#20837;&#20102;&#949;-&#35206;&#30422;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#22788;&#29702;&#19981;&#30830;&#23450;&#25110;&#19981;&#31934;&#30830;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#33539;&#22260;&#31354;&#38388;&#30340;&#949;-&#35206;&#30422;&#27010;&#24565;&#12290;&#26680;&#33539;&#22260;&#31354;&#38388;&#28041;&#21450;&#19968;&#20010;&#28857;&#38598;X&#8834;R^d&#21644;&#30001;&#22266;&#23450;&#26680;&#20989;&#25968;&#65288;&#20363;&#22914;&#39640;&#26031;&#26680;&#20989;&#25968;K(p,&#183;)=exp(-||p-&#183;||^2)&#65289;&#23450;&#20041;&#30340;&#26597;&#35810;&#31354;&#38388;&#12290;&#23545;&#20110;&#22823;&#23567;&#20026;n&#30340;&#28857;&#38598;X&#65292;&#26597;&#35810;&#36820;&#22238;&#19968;&#20010;&#20540;&#21521;&#37327;Rp&#8712;R^n&#65292;&#20854;&#20013;&#31532;i&#20010;&#22352;&#26631;(Rp)_i=K(p,x_i)&#65292;&#20854;&#20013;x_i&#8712;X&#12290;&#949;-&#35206;&#30422;&#26159;&#28857;&#38598;Q&#8834;R^d&#30340;&#23376;&#38598;&#65292;&#23545;&#20110;&#20219;&#24847;p&#8712;R^d&#65292;&#23384;&#22312;q&#8712;Q&#20351;&#24471;||(Rp-Rq)/n||_1&#8804;&#949;&#12290;&#36825;&#26159;Haussler&#22312;&#32452;&#21512;&#33539;&#22260;&#31354;&#38388;&#65288;&#20363;&#22914;&#30001;&#29699;&#26597;&#35810;&#23450;&#20041;&#30340;&#28857;&#38598;&#23376;&#38598;&#65289;&#20013;&#949;-&#35206;&#30422;&#27010;&#24565;&#30340;&#24179;&#28369;&#27169;&#25311;&#65292;&#20854;&#20013;&#24471;&#21040;&#30340;&#21521;&#37327;Rp&#26159;{0,1}^n&#32780;&#19981;&#26159;[0,1]^n&#12290;&#36825;&#20123;&#33539;&#22260;&#31354;&#38388;&#30340;&#26680;&#29256;&#26412;&#20986;&#29616;&#22312;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#20854;&#20013;&#22352;&#26631;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#25110;&#19981;&#31934;&#30830;&#30340;&#65292;&#22240;&#27492;&#24076;&#26395;&#22312;&#33539;&#22260;&#26597;&#35810;&#20013;&#28155;&#21152;&#19968;&#20123;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the notion of an $\varepsilon$-cover for a kernel range space. A kernel range space concerns a set of points $X \subset \mathbb{R}^d$ and the space of all queries by a fixed kernel (e.g., a Gaussian kernel $K(p,\cdot) = \exp(-\|p-\cdot\|^2)$). For a point set $X$ of size $n$, a query returns a vector of values $R_p \in \mathbb{R}^n$, where the $i$th coordinate $(R_p)_i = K(p,x_i)$ for $x_i \in X$. An $\varepsilon$-cover is a subset of points $Q \subset \mathbb{R}^d$ so for any $p \in \mathbb{R}^d$ that $\frac{1}{n} \|R_p R_q\|_1\leq \varepsilon$ for some $q \in Q$. This is a smooth analog of Haussler's notion of $\varepsilon$-covers for combinatorial range spaces (e.g., defined by subsets of points within a ball query) where the resulting vectors $R_p$ are in $\{0,1\}^n$ instead of $[0,1]^n$. The kernel versions of these range spaces show up in data analysis tasks where the coordinates may be uncertain or imprecise, and hence one wishes to add some flexibility in the not
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;WL&#27979;&#35797;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#19977;&#27425;&#36845;&#20195;&#30340;$(d-1)$-WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#19988;&#21482;&#38656;&#35201;&#19968;&#27425;&#36845;&#20195;&#30340;$d$-WL&#27979;&#35797;&#23601;&#21487;&#20197;&#36798;&#21040;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.12853</link><description>&lt;p&gt;
&#19977;&#27425;&#36845;&#20195;&#30340;$(1-d)$-WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#28857;&#20113;&#30340;&#38750;&#31561;&#36317;&#21464;&#25442;. (arXiv:2303.12853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Three iterations of $(1-d)$-WL test distinguish non isometric clouds of $d$-dimensional points. (arXiv:2303.12853v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;WL&#27979;&#35797;&#22312;&#28857;&#20113;&#20013;&#30340;&#24212;&#29992;&#65292;&#32467;&#26524;&#21457;&#29616;&#19977;&#27425;&#36845;&#20195;&#30340;$(d-1)$-WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#19988;&#21482;&#38656;&#35201;&#19968;&#27425;&#36845;&#20195;&#30340;$d$-WL&#27979;&#35797;&#23601;&#21487;&#20197;&#36798;&#21040;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Weisfeiler-Lehman (WL)&#27979;&#35797;&#26159;&#19968;&#20010;&#26816;&#26597;&#22270;&#21516;&#26500;&#30340;&#22522;&#26412;&#36845;&#20195;&#31639;&#27861;&#12290;&#23427;&#34987;&#35266;&#23519;&#21040;&#26159;&#20960;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#35774;&#35745;&#30340;&#22522;&#30784;&#65292;&#36825;&#20123;&#32593;&#32476;&#30340;&#33021;&#21147;&#21644;&#24615;&#33021;&#21487;&#20197;&#29992;&#36825;&#20010;&#27979;&#35797;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#29702;&#35299;&#12290;&#21463;&#26368;&#36817;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#28041;&#21450;&#19977;&#32500;&#29289;&#20307;&#30340;&#25968;&#25454;&#38598;&#30340;&#21457;&#23637;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;WL&#27979;&#35797;&#23545;&#23436;&#25972;&#30340;&#36317;&#31163;&#22270;&#34920;&#31034;&#30340;&#27431;&#20960;&#37324;&#24471;&#28857;&#20113;&#26159;&#8220;&#23436;&#25972;&#30340;&#8221;&#26102;&#65292;&#23427;&#20309;&#26102;&#33021;&#22815;&#35782;&#21035;&#20986;&#20219;&#24847;&#19968;&#20010;&#20219;&#24847;&#28857;&#20113;.&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#65292;$(d-1)$-&#32500;WL&#27979;&#35797;&#21487;&#20197;&#21306;&#20998;$d$&#32500;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#20219;&#20309;$d\ge 2$&#37117;&#21487;&#20197;&#65292;&#32780;&#19988;&#21482;&#38656;&#35201;&#36827;&#34892;&#19977;&#27425;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#20110;$d=2,3$&#26159;&#32039;&#30340;&#12290;&#25105;&#20204;&#36824;&#35266;&#23519;&#21040;$d$&#32500;WL&#27979;&#35797;&#21482;&#38656;&#35201;&#36827;&#34892;&#19968;&#27425;&#36845;&#20195;&#23601;&#21487;&#20197;&#36798;&#21040;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Weisfeiler--Lehman (WL) test is a fundamental iterative algorithm for checking isomorphism of graphs. It has also been observed that it underlies the design of several graph neural network architectures, whose capabilities and performance can be understood in terms of the expressive power of this test. Motivated by recent developments in machine learning applications to datasets involving three-dimensional objects, we study when the WL test is {\em complete} for clouds of euclidean points represented by complete distance graphs, i.e., when it can distinguish, up to isometry, any arbitrary such cloud.  Our main result states that the $(d-1)$-dimensional WL test is complete for point clouds in $d$-dimensional Euclidean space, for any $d\ge 2$, and that only three iterations of the test suffice. Our result is tight for $d = 2, 3$. We also observe that the $d$-dimensional WL test only requires one iteration to achieve completeness.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#38454;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;&#65288;SOCGS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#32422;&#26463;&#20108;&#27425;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#27425;&#32447;&#24615;&#25910;&#25947;&#36845;&#20195;&#21518;&#20108;&#27425;&#25910;&#25947;&#20110;&#21407;&#22987;&#38388;&#38553;&#12290;</title><link>http://arxiv.org/abs/2002.08907</link><description>&lt;p&gt;
&#20108;&#38454;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;
&lt;/p&gt;
&lt;p&gt;
Second-order Conditional Gradient Sliding. (arXiv:2002.08907v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2002.08907
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#38454;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;&#65288;SOCGS&#65289;&#31639;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#35299;&#20915;&#32422;&#26463;&#20108;&#27425;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#26377;&#38480;&#27425;&#32447;&#24615;&#25910;&#25947;&#36845;&#20195;&#21518;&#20108;&#27425;&#25910;&#25947;&#20110;&#21407;&#22987;&#38388;&#38553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38656;&#35201;&#39640;&#31934;&#24230;&#35299;&#20915;&#38382;&#39064;&#26102;&#65292;&#32422;&#26463;&#20108;&#38454;&#20984;&#20248;&#21270;&#31639;&#27861;&#26159;&#39318;&#36873;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#23616;&#37096;&#20108;&#27425;&#25910;&#25947;&#24615;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#27599;&#27425;&#36845;&#20195;&#26102;&#38656;&#35201;&#35299;&#20915;&#19968;&#20010;&#32422;&#26463;&#20108;&#27425;&#23376;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;\emph{&#20108;&#38454;&#26465;&#20214;&#26799;&#24230;&#28369;&#21160;}&#65288;SOCGS&#65289;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#26080;&#25237;&#24433;&#31639;&#27861;&#26469;&#36817;&#20284;&#35299;&#20915;&#32422;&#26463;&#20108;&#27425;&#23376;&#38382;&#39064;&#12290;&#24403;&#21487;&#34892;&#22495;&#26159;&#19968;&#20010;&#22810;&#38754;&#20307;&#26102;&#65292;&#35813;&#31639;&#27861;&#22312;&#26377;&#38480;&#27425;&#32447;&#24615;&#25910;&#25947;&#36845;&#20195;&#21518;&#20108;&#27425;&#25910;&#25947;&#20110;&#21407;&#22987;&#38388;&#38553;&#12290;&#36827;&#20837;&#20108;&#27425;&#25910;&#25947;&#38454;&#27573;&#21518;&#65292;SOCGS&#31639;&#27861;&#38656;&#36890;&#36807;$\mathcal{O}(\log(\log 1/\varepsilon))$&#27425;&#19968;&#38454;&#21644;Hessian&#27491;&#20132;&#35843;&#29992;&#20197;&#21450;$\mathcal{O}(\log (1/\varepsilon) \log(\log1/\varepsilon))$&#27425;&#32447;&#24615;&#26368;&#23567;&#21270;&#27491;&#20132;&#35843;&#29992;&#26469;&#23454;&#29616;$\varepsilon$-&#26368;&#20248;&#35299;&#12290;&#24403;&#21487;&#34892;&#22495;&#21482;&#33021;&#36890;&#36807;&#32447;&#24615;&#20248;&#21270;&#27491;&#20132;&#35843;&#29992;&#39640;&#25928;&#35775;&#38382;&#26102;&#65292;&#27492;&#31639;&#27861;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constrained second-order convex optimization algorithms are the method of choice when a high accuracy solution to a problem is needed, due to their local quadratic convergence. These algorithms require the solution of a constrained quadratic subproblem at every iteration. We present the \emph{Second-Order Conditional Gradient Sliding} (SOCGS) algorithm, which uses a projection-free algorithm to solve the constrained quadratic subproblems inexactly. When the feasible region is a polytope the algorithm converges quadratically in primal gap after a finite number of linearly convergent iterations. Once in the quadratic regime the SOCGS algorithm requires $\mathcal{O}(\log(\log 1/\varepsilon))$ first-order and Hessian oracle calls and $\mathcal{O}(\log (1/\varepsilon) \log(\log1/\varepsilon))$ linear minimization oracle calls to achieve an $\varepsilon$-optimal solution. This algorithm is useful when the feasible region can only be accessed efficiently through a linear optimization oracle, 
&lt;/p&gt;</description></item></channel></rss>