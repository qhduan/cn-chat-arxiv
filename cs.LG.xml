<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#24341;&#20837;&#20102;PPA-Game&#27169;&#22411;&#26469;&#34920;&#24449;&#31867;&#20284;YouTube&#21644;TikTok&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#31454;&#20105;&#21160;&#24577;&#65292;&#20998;&#26512;&#26174;&#31034;&#32431;&#32435;&#20160;&#22343;&#34913;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#24120;&#35265;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#27599;&#20010;&#20195;&#29702;&#32773;&#30340;&#32047;&#31215;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2403.15524</link><description>&lt;p&gt;
PPA-Game&#65306;&#34920;&#24449;&#21644;&#23398;&#20064;&#22312;&#32447;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#30340;&#31454;&#20105;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
PPA-Game: Characterizing and Learning Competitive Dynamics Among Online Content Creators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15524
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;PPA-Game&#27169;&#22411;&#26469;&#34920;&#24449;&#31867;&#20284;YouTube&#21644;TikTok&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#20043;&#38388;&#31454;&#20105;&#21160;&#24577;&#65292;&#20998;&#26512;&#26174;&#31034;&#32431;&#32435;&#20160;&#22343;&#34913;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26159;&#24120;&#35265;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#29992;&#20110;&#26368;&#22823;&#21270;&#27599;&#20010;&#20195;&#29702;&#32773;&#30340;&#32047;&#31215;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27604;&#20363;&#24615;&#25910;&#30410;&#20998;&#37197;&#28216;&#25103;&#65288;PPA-Game&#65289;&#26469;&#27169;&#25311;&#20195;&#29702;&#32773;&#22914;&#20309;&#31454;&#20105;&#21487;&#20998;&#37197;&#36164;&#28304;&#21644;&#28040;&#36153;&#32773;&#30340;&#27880;&#24847;&#21147;&#65292;&#31867;&#20284;&#20110;YouTube&#21644;TikTok&#31561;&#24179;&#21488;&#19978;&#30340;&#20869;&#23481;&#21019;&#20316;&#32773;&#12290;&#26681;&#25454;&#24322;&#36136;&#26435;&#37325;&#20026;&#20195;&#29702;&#32773;&#20998;&#37197;&#25910;&#30410;&#65292;&#21453;&#26144;&#20102;&#21019;&#20316;&#32773;&#20043;&#38388;&#20869;&#23481;&#36136;&#37327;&#30340;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#32431;&#32435;&#20160;&#22343;&#34913;&#65288;PNE&#65289;&#24182;&#19981;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#37117;&#26377;&#20445;&#35777;&#65292;&#20294;&#22312;&#25105;&#20204;&#30340;&#27169;&#25311;&#20013;&#65292;&#36890;&#24120;&#20250;&#35266;&#23519;&#21040;&#65292;&#20854;&#32570;&#20047;&#24773;&#20917;&#26159;&#32597;&#35265;&#30340;&#12290;&#38500;&#20102;&#20998;&#26512;&#38745;&#24577;&#25910;&#30410;&#22806;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#20195;&#29702;&#32773;&#20851;&#20110;&#36164;&#28304;&#25910;&#30410;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#23558;&#22810;&#29609;&#23478;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#25972;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#22312;$T$&#36718;&#20013;&#20419;&#36827;&#27599;&#20010;&#20195;&#29702;&#32773;&#32047;&#31215;&#25910;&#30410;&#30340;&#26368;&#22823;&#21270;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#20219;&#20309;&#20195;&#29702;&#32773;&#30340;&#36951;&#25022;&#22312;&#20219;&#20309;$\eta &gt; 0$&#19979;&#37117;&#21463;&#21040;$O(\log^{1 + \eta} T)$&#30340;&#38480;&#21046;&#12290;&#32463;&#39564;&#32467;&#26524;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15524v1 Announce Type: cross  Abstract: We introduce the Proportional Payoff Allocation Game (PPA-Game) to model how agents, akin to content creators on platforms like YouTube and TikTok, compete for divisible resources and consumers' attention. Payoffs are allocated to agents based on heterogeneous weights, reflecting the diversity in content quality among creators. Our analysis reveals that although a pure Nash equilibrium (PNE) is not guaranteed in every scenario, it is commonly observed, with its absence being rare in our simulations. Beyond analyzing static payoffs, we further discuss the agents' online learning about resource payoffs by integrating a multi-player multi-armed bandit framework. We propose an online algorithm facilitating each agent's maximization of cumulative payoffs over $T$ rounds. Theoretically, we establish that the regret of any agent is bounded by $O(\log^{1 + \eta} T)$ for any $\eta &gt; 0$. Empirical results further validate the effectiveness of ou
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;PGPE&#31639;&#27861;&#21644;&#19981;&#21516;&#20984;&#25918;&#23485;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#32531;&#35299;&#32039;&#20945;&#20984;&#26494;&#24347;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2403.07095</link><description>&lt;p&gt;
&#29992;&#39640;&#26031;&#24179;&#28369;&#20811;&#26381;&#35748;&#35777;&#22521;&#35757;&#30340;&#24726;&#35770;
&lt;/p&gt;
&lt;p&gt;
Overcoming the Paradox of Certified Training with Gaussian Smoothing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07095
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#26041;&#27861;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;PGPE&#31639;&#27861;&#21644;&#19981;&#21516;&#20984;&#25918;&#23485;&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26102;&#32531;&#35299;&#32039;&#20945;&#20984;&#26494;&#24347;&#24102;&#26469;&#30340;&#38382;&#39064;&#65292;&#24182;&#33719;&#24471;&#26356;&#22909;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20294;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#20197;&#39640;&#35748;&#35777;&#20934;&#30830;&#24230;&#23545;&#25239;&#23545;&#25239;&#24615;&#31034;&#20363;&#20173;&#28982;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#35757;&#32451;&#20013;&#65292;&#23613;&#31649;&#35748;&#35777;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#21033;&#29992;&#32039;&#20945;&#30340;&#20984;&#26494;&#24347;&#36827;&#34892;&#30028;&#35745;&#31639;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#34920;&#29616;&#19981;&#22914;&#36739;&#26494;&#30340;&#26494;&#24347;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20551;&#35774;&#36825;&#26159;&#30001;&#36825;&#20123;&#26356;&#32039;&#30340;&#26494;&#24347;&#23548;&#33268;&#30340;&#25439;&#22833;&#34920;&#38754;&#30340;&#19981;&#36830;&#32493;&#24615;&#21644;&#25200;&#21160;&#25935;&#24863;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#29702;&#35770;&#19978;&#23637;&#31034;&#20102;&#39640;&#26031;&#25439;&#22833;&#24179;&#28369;&#21487;&#20197;&#32531;&#35299;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#32467;&#21512;PGPE&#30340;&#35748;&#35777;&#35757;&#32451;&#26041;&#27861;&#65292;&#35813;&#31639;&#27861;&#35745;&#31639;&#24179;&#28369;&#25439;&#22833;&#30340;&#26799;&#24230;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#20984;&#25918;&#23485;&#26469;&#30830;&#35748;&#36825;&#19968;&#28857;&#12290;&#22312;&#20351;&#29992;&#36825;&#31181;&#35757;&#32451;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#26356;&#32039;&#23494;&#30340;&#30028;&#38480;&#30830;&#23454;&#23548;&#33268;&#26356;&#22909;&#30340;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#30456;&#21516;&#32593;&#32476;&#19978;&#32988;&#36807;&#21516;&#31867;&#25216;&#26415;&#12290;&#23613;&#31649;&#25193;&#23637;&#22522;&#20110;PGPE&#30340;&#35757;&#32451;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07095v1 Announce Type: new  Abstract: Training neural networks with high certified accuracy against adversarial examples remains an open problem despite significant efforts. While certification methods can effectively leverage tight convex relaxations for bound computation, in training, these methods perform worse than looser relaxations. Prior work hypothesized that this is caused by the discontinuity and perturbation sensitivity of the loss surface induced by these tighter relaxations. In this work, we show theoretically that Gaussian Loss Smoothing can alleviate both of these issues. We confirm this empirically by proposing a certified training method combining PGPE, an algorithm computing gradients of a smoothed loss, with different convex relaxations. When using this training method, we observe that tighter bounds indeed lead to strictly better networks that can outperform state-of-the-art methods on the same network. While scaling PGPE-based training remains challengin
&lt;/p&gt;</description></item><item><title>FairTargetSim&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;&#65292;&#23637;&#31034;&#20102;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#36866;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;</title><link>https://arxiv.org/abs/2403.06031</link><description>&lt;p&gt;
FairTargetSim&#65306;&#29992;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#20844;&#24179;&#24615;&#24433;&#21709;&#30340;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
FairTargetSim: An Interactive Simulator for Understanding and Explaining the Fairness Effects of Target Variable Definition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06031
&lt;/p&gt;
&lt;p&gt;
FairTargetSim&#25552;&#20379;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;&#65292;&#23637;&#31034;&#20102;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#23545;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;&#65292;&#36866;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#38656;&#35201;&#20026;&#39044;&#27979;&#25110;&#20915;&#31574;&#23450;&#20041;&#30446;&#26631;&#21464;&#37327;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#23545;&#20844;&#24179;&#24615;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#65306;&#20559;&#35265;&#36890;&#24120;&#24050;&#32463;&#34987;&#32534;&#30721;&#22312;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#26412;&#36523;&#20013;&#65292;&#32780;&#19981;&#26159;&#22312;&#20219;&#20309;&#25968;&#25454;&#25910;&#38598;&#25110;&#35757;&#32451;&#20043;&#21069;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20132;&#20114;&#24335;&#27169;&#25311;&#22120;&#65292;FairTargetSim (FTS)&#65292;&#23637;&#31034;&#20102;&#30446;&#26631;&#21464;&#37327;&#23450;&#20041;&#22914;&#20309;&#24433;&#21709;&#20844;&#24179;&#24615;&#12290;FTS&#26159;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#24037;&#20855;&#65292;&#36866;&#29992;&#20110;&#31639;&#27861;&#24320;&#21457;&#32773;&#12289;&#30740;&#31350;&#20154;&#21592;&#21644;&#38750;&#25216;&#26415;&#21033;&#30410;&#30456;&#20851;&#32773;&#12290;FTS&#20351;&#29992;&#20102;&#31639;&#27861;&#25307;&#32856;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#21644;&#29992;&#25143;&#23450;&#20041;&#30340;&#30446;&#26631;&#21464;&#37327;&#12290;FTS&#26159;&#24320;&#28304;&#30340;&#65292;&#21487;&#22312;&#20197;&#19979;&#32593;&#22336;&#25214;&#21040;&#65306;http://tinyurl.com/ftsinterface&#12290;&#26412;&#25991;&#38468;&#24102;&#30340;&#35270;&#39057;&#32593;&#22336;&#20026;&#65306;http://tinyurl.com/ijcaifts&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06031v1 Announce Type: cross  Abstract: Machine learning requires defining one's target variable for predictions or decisions, a process that can have profound implications on fairness: biases are often encoded in target variable definition itself, before any data collection or training. We present an interactive simulator, FairTargetSim (FTS), that illustrates how target variable definition impacts fairness. FTS is a valuable tool for algorithm developers, researchers, and non-technical stakeholders. FTS uses a case study of algorithmic hiring, using real-world data and user-defined target variables. FTS is open-source and available at: http://tinyurl.com/ftsinterface. The video accompanying this paper is here: http://tinyurl.com/ijcaifts.
&lt;/p&gt;</description></item><item><title>TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.</title><link>https://arxiv.org/abs/2402.12683</link><description>&lt;p&gt;
TorchCP&#65306;&#22522;&#20110;PyTorch&#30340;&#19968;&#31181;&#36866;&#29992;&#20110;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30340;&#24211;
&lt;/p&gt;
&lt;p&gt;
TorchCP: A Library for Conformal Prediction based on PyTorch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12683
&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;Python&#24037;&#20855;&#21253;&#65292;&#20026;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#25552;&#20379;&#20102;&#23454;&#29616;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#22810;&#31181;&#24037;&#20855;&#65292;&#21253;&#25324;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#12290;En_Tdlr: TorchCP is a Python toolbox built on PyTorch for conformal prediction research on deep learning models, providing various implementations for posthoc and training methods for classification and regression tasks, including multi-dimension output.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TorchCP&#26159;&#19968;&#20010;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#19978;&#30340;&#21512;&#25311;&#24120;&#35268;&#39044;&#27979;&#30740;&#31350;&#30340;Python&#24037;&#20855;&#21253;&#12290;&#23427;&#21253;&#21547;&#20102;&#29992;&#20110;&#21518;&#39564;&#21644;&#35757;&#32451;&#26041;&#27861;&#30340;&#21508;&#31181;&#23454;&#29616;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#65288;&#21253;&#25324;&#22810;&#32500;&#36755;&#20986;&#65289;&#12290;TorchCP&#24314;&#31435;&#22312;PyTorch&#20043;&#19978;&#65292;&#24182;&#21033;&#29992;&#30697;&#38453;&#35745;&#31639;&#30340;&#20248;&#21183;&#65292;&#25552;&#20379;&#31616;&#27905;&#39640;&#25928;&#30340;&#25512;&#29702;&#23454;&#29616;&#12290;&#35813;&#20195;&#30721;&#37319;&#29992;LGPL&#35768;&#21487;&#35777;&#65292;&#24182;&#22312;$\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$&#24320;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12683v1 Announce Type: new  Abstract: TorchCP is a Python toolbox for conformal prediction research on deep learning models. It contains various implementations for posthoc and training methods for classification and regression tasks (including multi-dimension output). TorchCP is built on PyTorch (Paszke et al., 2019) and leverages the advantages of matrix computation to provide concise and efficient inference implementations. The code is licensed under the LGPL license and is open-sourced at $\href{https://github.com/ml-stat-Sustech/TorchCP}{\text{this https URL}}$.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;X&#40657;&#23458;&#30340;&#27010;&#24565;&#65292;&#21363;&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26469;&#25805;&#32437;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#25351;&#26631;&#65292;&#20174;&#32780;&#20135;&#29983;&#25152;&#38656;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#30740;&#31350;&#32773;&#24635;&#32467;&#20102;X&#40657;&#23458;&#29616;&#35937;&#30340;&#20005;&#37325;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#23545;XAI&#30740;&#31350;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.08513</link><description>&lt;p&gt;
X&#40657;&#23458;&#65306;&#35823;&#23548;&#30340;&#33258;&#21160;&#26426;&#22120;&#23398;&#20064;&#30340;&#23041;&#32961;
&lt;/p&gt;
&lt;p&gt;
X Hacking: The Threat of Misguided AutoML
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.08513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;X&#40657;&#23458;&#30340;&#27010;&#24565;&#65292;&#21363;&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26469;&#25805;&#32437;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#25351;&#26631;&#65292;&#20174;&#32780;&#20135;&#29983;&#25152;&#38656;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#38477;&#20302;&#20854;&#39044;&#27979;&#24615;&#33021;&#12290;&#30740;&#31350;&#32773;&#24635;&#32467;&#20102;X&#40657;&#23458;&#29616;&#35937;&#30340;&#20005;&#37325;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#35752;&#20102;&#23545;XAI&#30740;&#31350;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26377;&#21161;&#20110;&#24314;&#31435;&#23545;&#27169;&#22411;&#39044;&#27979;&#21644;&#27966;&#29983;&#35265;&#35299;&#30340;&#20449;&#20219;&#65292;&#20294;&#20063;&#20026;&#20998;&#26512;&#24072;&#25552;&#20379;&#20102;&#19968;&#31181;&#25197;&#26354;&#30340;&#21160;&#26426;&#65292;&#21363;&#25805;&#32437;XAI&#25351;&#26631;&#20197;&#25903;&#25345;&#39044;&#20808;&#35268;&#23450;&#30340;&#32467;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;X&#40657;&#23458;&#30340;&#27010;&#24565;&#65292;&#21363;&#23558;p-hacking&#24212;&#29992;&#20110;&#35832;&#22914;Shap&#20540;&#20043;&#31867;&#30340;XAI&#25351;&#26631;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#33258;&#21160;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#26469;&#23547;&#25214;&#8220;&#21487;&#36777;&#25252;&#8221;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#20135;&#29983;&#25152;&#38656;&#30340;&#35299;&#37322;&#24182;&#22312;&#32500;&#25345;&#20248;&#36234;&#30340;&#39044;&#27979;&#24615;&#33021;&#26102;&#12290;&#25105;&#20204;&#23558;&#35299;&#37322;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#34920;&#36848;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29087;&#24713;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#22312;&#32463;&#39564;&#19978;&#23637;&#31034;&#20102;X&#40657;&#23458;&#30340;&#21487;&#34892;&#24615;&#21644;&#20005;&#37325;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;XAI&#30740;&#31350;&#30340;&#21487;&#20449;&#24230;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#20262;&#29702;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explainable AI (XAI) and interpretable machine learning methods help to build trust in model predictions and derived insights, yet also present a perverse incentive for analysts to manipulate XAI metrics to support pre-specified conclusions. This paper introduces the concept of X-hacking, a form of p-hacking applied to XAI metrics such as Shap values. We show how an automated machine learning pipeline can be used to search for 'defensible' models that produce a desired explanation while maintaining superior predictive performance to a common baseline. We formulate the trade-off between explanation and accuracy as a multi-objective optimization problem and illustrate the feasibility and severity of X-hacking empirically on familiar real-world datasets. Finally, we suggest possible methods for detection and prevention, and discuss ethical implications for the credibility and reproducibility of XAI research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;</title><link>https://arxiv.org/abs/2311.09386</link><description>&lt;p&gt;
&#36229;&#36234;PCA&#65306;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#30340;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Beyond PCA: A Probabilistic Gram-Schmidt Approach to Feature Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#24615;Gram-Schmidt&#26041;&#27861;&#26469;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#20174;&#32780;&#25552;&#21462;&#25968;&#25454;&#20013;&#30340;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#32447;&#24615;&#29305;&#24449;&#25552;&#21462;&#22312;&#25968;&#25454;&#20013;&#23384;&#22312;&#38750;&#32447;&#24615;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#26159;&#19968;&#20010;&#22522;&#26412;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#27010;&#29575;&#24615;Gram-Schmidt (GS)&#31867;&#22411;&#30340;&#27491;&#20132;&#21270;&#36807;&#31243;&#26469;&#26816;&#27979;&#21644;&#26144;&#23556;&#20986;&#20887;&#20313;&#32500;&#24230;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#19968;&#26063;&#20989;&#25968;&#19978;&#24212;&#29992;GS&#36807;&#31243;&#65292;&#35813;&#26063;&#20989;&#25968;&#39044;&#35745;&#25429;&#25417;&#21040;&#25968;&#25454;&#20013;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31995;&#21015;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#26032;&#30340;&#22823;&#26041;&#24046;&#26041;&#21521;&#65292;&#25110;&#32773;&#23558;&#36825;&#20123;&#20381;&#36182;&#24615;&#20174;&#20027;&#25104;&#20998;&#20013;&#21435;&#38500;&#12290;&#22312;&#21069;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29109;&#20943;&#23569;&#30340;&#20449;&#24687;&#29702;&#35770;&#20445;&#35777;&#12290;&#22312;&#21518;&#19968;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26576;&#20123;&#20551;&#35774;&#19979;&#65292;&#25152;&#24471;&#31639;&#27861;&#22312;&#25152;&#36873;&#25321;&#20989;&#25968;&#26063;&#30340;&#32447;&#24615;&#24352;&#25104;&#31354;&#38388;&#20013;&#21487;&#20197;&#26816;&#27979;&#21644;&#21435;&#38500;&#38750;&#32447;&#24615;&#20381;&#36182;&#24615;&#12290;&#20004;&#31181;&#25552;&#20986;&#30340;&#26041;&#27861;&#37117;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#32447;&#24615;&#29305;&#24449;&#24182;&#21435;&#38500;&#38750;&#32447;&#24615;&#20887;&#20313;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear feature extraction at the presence of nonlinear dependencies among the data is a fundamental challenge in unsupervised learning. We propose using a probabilistic Gram-Schmidt (GS) type orthogonalization process in order to detect and map out redundant dimensions. Specifically, by applying the GS process over a family of functions which presumably captures the nonlinear dependencies in the data, we construct a series of covariance matrices that can either be used to identify new large-variance directions, or to remove those dependencies from the principal components. In the former case, we provide information-theoretic guarantees in terms of entropy reduction. In the latter, we prove that under certain assumptions the resulting algorithms detect and remove nonlinear dependencies whenever those dependencies lie in the linear span of the chosen function family. Both proposed methods extract linear features from the data while removing nonlinear redundancies. We provide simulation r
&lt;/p&gt;</description></item><item><title>VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.13367</link><description>&lt;p&gt;
VFedMH: &#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#29992;&#20110;&#35757;&#32451;&#22810;&#21442;&#19982;&#26041;&#24322;&#26500;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
VFedMH: Vertical Federated Learning for Training Multi-party Heterogeneous Models. (arXiv:2310.13367v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13367
&lt;/p&gt;
&lt;p&gt;
VFedMH&#26159;&#19968;&#31181;&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#32858;&#21512;&#21442;&#19982;&#32773;&#30340;&#23884;&#20837;&#26469;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#24322;&#26500;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;VFL&#26041;&#27861;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#21512;&#23398;&#20064;&#65288;VFL&#65289;&#20316;&#20026;&#19968;&#31181;&#38598;&#25104;&#26679;&#26412;&#23545;&#40784;&#21644;&#29305;&#24449;&#21512;&#24182;&#30340;&#26032;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;VFL&#26041;&#27861;&#22312;&#22788;&#29702;&#21442;&#19982;&#32773;&#20043;&#38388;&#23384;&#22312;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#26102;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#24433;&#21709;&#20102;&#20248;&#21270;&#25910;&#25947;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VFedMH&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22810;&#26041;&#24322;&#26500;&#27169;&#22411;&#12290;VFedMH&#30340;&#37325;&#28857;&#26159;&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#32858;&#21512;&#27599;&#20010;&#21442;&#19982;&#32773;&#30693;&#35782;&#30340;&#23884;&#20837;&#65292;&#32780;&#19981;&#26159;&#20013;&#38388;&#32467;&#26524;&#12290;&#20027;&#21160;&#26041;&#65292;&#25317;&#26377;&#26679;&#26412;&#30340;&#26631;&#31614;&#21644;&#29305;&#24449;&#65292;&#22312;VFedMH&#20013;&#23433;&#20840;&#22320;&#32858;&#21512;&#26412;&#22320;&#23884;&#20837;&#20197;&#33719;&#24471;&#20840;&#23616;&#30693;&#35782;&#23884;&#20837;&#65292;&#24182;&#23558;&#20854;&#21457;&#36865;&#32473;&#34987;&#21160;&#26041;&#12290;&#34987;&#21160;&#26041;&#20165;&#25317;&#26377;&#26679;&#26412;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#21033;&#29992;&#20840;&#23616;&#23884;&#20837;&#22312;&#20854;&#26412;&#22320;&#24322;&#26500;&#32593;&#32476;&#19978;&#36827;&#34892;&#21069;&#21521;&#20256;&#25773;&#12290;&#28982;&#32780;&#65292;&#34987;&#21160;&#26041;&#19981;&#25317;&#26377;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) has gained increasing attention as a novel training paradigm that integrates sample alignment and feature union. However, existing VFL methods face challenges when dealing with heterogeneous local models among participants, which affects optimization convergence and generalization. To address this issue, this paper proposes a novel approach called Vertical Federated learning for training Multi-parties Heterogeneous models (VFedMH). VFedMH focuses on aggregating the embeddings of each participant's knowledge instead of intermediate results during forward propagation. The active party, who possesses labels and features of the sample, in VFedMH securely aggregates local embeddings to obtain global knowledge embeddings, and sends them to passive parties. The passive parties, who own only features of the sample, then utilize the global embeddings to propagate forward on their local heterogeneous networks. However, the passive party does not own the labels, 
&lt;/p&gt;</description></item><item><title>GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2310.03399</link><description>&lt;p&gt;
GRAPES: &#23398;&#20064;&#29992;&#20110;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22270;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
GRAPES: Learning to Sample Graphs for Scalable Graph Neural Networks. (arXiv:2310.03399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03399
&lt;/p&gt;
&lt;p&gt;
GRAPES&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#35782;&#21035;&#22312;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#26102;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#38598;&#21512;&#65292;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#36807;&#20197;&#19981;&#21516;&#26041;&#24335;&#32858;&#21512;&#21608;&#22260;&#20449;&#24687;&#26469;&#23398;&#20064;&#22270;&#20013;&#33410;&#28857;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#36825;&#20123;&#32593;&#32476;&#30340;&#21152;&#28145;&#65292;&#30001;&#20110;&#37051;&#22495;&#23610;&#23544;&#30340;&#22686;&#21152;&#65292;&#23427;&#20204;&#30340;&#24863;&#21463;&#37326;&#21576;&#25351;&#25968;&#22686;&#38271;&#65292;&#23548;&#33268;&#39640;&#20869;&#23384;&#28040;&#32791;&#12290;&#22270;&#37319;&#26679;&#36890;&#36807;&#23545;&#22270;&#20013;&#33410;&#28857;&#36827;&#34892;&#25277;&#26679;&#26469;&#35299;&#20915;GNNs&#20013;&#30340;&#20869;&#23384;&#38382;&#39064;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;GNNs&#21487;&#20197;&#25193;&#23637;&#21040;&#26356;&#22823;&#30340;&#22270;&#12290;&#22823;&#22810;&#25968;&#37319;&#26679;&#26041;&#27861;&#19987;&#27880;&#20110;&#22266;&#23450;&#30340;&#37319;&#26679;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#32467;&#26500;&#25110;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;GRAPES&#65292;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#22270;&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23398;&#20064;&#35782;&#21035;&#29992;&#20110;&#35757;&#32451;GNN&#20998;&#31867;&#22120;&#30340;&#19968;&#32452;&#20855;&#26377;&#24433;&#21709;&#21147;&#30340;&#33410;&#28857;&#12290;GRAPES&#20351;&#29992;GFlowNet&#26469;&#23398;&#20064;&#32473;&#23450;&#20998;&#31867;&#30446;&#26631;&#30340;&#33410;&#28857;&#37319;&#26679;&#27010;&#29575;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#23567;&#35268;&#27169;&#21644;&#22823;&#35268;&#27169;&#22270;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;GRAPES&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;GRAPES&#21363;&#20351;&#22312;&#37319;&#26679;&#27604;&#20363;&#36739;&#20302;&#30340;&#24773;&#20917;&#19979;&#20173;&#20445;&#25345;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) learn the representation of nodes in a graph by aggregating the neighborhood information in various ways. As these networks grow in depth, their receptive field grows exponentially due to the increase in neighborhood sizes, resulting in high memory costs. Graph sampling solves memory issues in GNNs by sampling a small ratio of the nodes in the graph. This way, GNNs can scale to much larger graphs. Most sampling methods focus on fixed sampling heuristics, which may not generalize to different structures or tasks. We introduce GRAPES, an adaptive graph sampling method that learns to identify sets of influential nodes for training a GNN classifier. GRAPES uses a GFlowNet to learn node sampling probabilities given the classification objectives. We evaluate GRAPES across several small- and large-scale graph benchmarks and demonstrate its effectiveness in accuracy and scalability. In contrast to existing sampling methods, GRAPES maintains high accuracy even with 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#24615;&#30340;&#20020;&#24202;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;COVID-19&#35786;&#26029;&#30340;AI&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#20110;AI&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#26368;&#39640;&#19979;&#38477;20&#65285;&#12290;</title><link>http://arxiv.org/abs/2308.09730</link><description>&lt;p&gt;
&#22522;&#20110;COVID-19&#30340;&#25968;&#25454;&#22810;&#26679;&#24615;&#21644;&#34394;&#25311;&#25104;&#20687;&#30340;AI&#35786;&#26029;&#65306;&#20197;&#30149;&#20363;&#30740;&#31350;&#20026;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Data diversity and virtual imaging in AI-based diagnosis: A case study based on COVID-19. (arXiv:2308.09730v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.09730
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#22810;&#26679;&#24615;&#30340;&#20020;&#24202;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;&#20102;COVID-19&#35786;&#26029;&#30340;AI&#27169;&#22411;&#65292;&#21457;&#29616;&#25968;&#25454;&#38598;&#29305;&#24449;&#23545;&#20110;AI&#24615;&#33021;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#23481;&#26131;&#23548;&#33268;&#27867;&#21270;&#33021;&#21147;&#36739;&#24046;&#65292;&#26368;&#39640;&#19979;&#38477;20&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#35843;&#26597;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#22312;&#26032;&#22411;&#20896;&#29366;&#30149;&#27602;&#65288;COVID-19&#65289;&#30340;&#21307;&#23398;&#24433;&#20687;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65292;&#35768;&#22810;&#25253;&#36947;&#31216;&#20854;&#24615;&#33021;&#20960;&#20046;&#23436;&#32654;&#12290;&#28982;&#32780;&#65292;&#24615;&#33021;&#30340;&#21464;&#24322;&#24615;&#21644;&#28508;&#22312;&#30340;&#25968;&#25454;&#20559;&#24046;&#24341;&#21457;&#20102;&#23545;&#20020;&#24202;&#36866;&#29992;&#24615;&#30340;&#25285;&#24551;&#12290;&#26412;&#22238;&#39038;&#24615;&#30740;&#31350;&#28041;&#21450;&#20351;&#29992;&#20020;&#24202;&#22810;&#26679;&#24615;&#21644;&#34394;&#25311;&#29983;&#25104;&#30340;&#21307;&#23398;&#22270;&#20687;&#24320;&#21457;&#21644;&#35780;&#20272;COVID-19&#35786;&#26029;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#27425;&#34394;&#25311;&#25104;&#20687;&#35797;&#39564;&#65292;&#20197;&#35780;&#20272;AI&#24615;&#33021;&#21463;&#30142;&#30149;&#33539;&#22260;&#12289;&#36752;&#23556;&#21058;&#37327;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#21644;&#33016;&#37096;&#25918;&#23556;&#25668;&#24433;&#65288;CXR&#65289;&#25104;&#20687;&#27169;&#24577;&#31561;&#20960;&#20010;&#24739;&#32773;&#21644;&#29289;&#29702;&#24615;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25968;&#25454;&#38598;&#29305;&#24449;&#65288;&#21253;&#25324;&#25968;&#37327;&#12289;&#22810;&#26679;&#24615;&#21644;&#24739;&#30149;&#29575;&#65289;&#24378;&#28872;&#24433;&#21709;&#20102;AI&#30340;&#24615;&#33021;&#65292;&#23548;&#33268;&#25509;&#25910;&#32773;&#25805;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#38754;&#31215;&#19979;&#38477;&#20102;&#39640;&#36798;20&#65285;&#65292;&#19988;&#27867;&#21270;&#33021;&#21147;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many studies have investigated deep-learning-based artificial intelligence (AI) models for medical imaging diagnosis of the novel coronavirus (COVID-19), with many reports of near-perfect performance. However, variability in performance and underlying data biases raise concerns about clinical generalizability. This retrospective study involved the development and evaluation of artificial intelligence (AI) models for COVID-19 diagnosis using both diverse clinical and virtually generated medical images. In addition, we conducted a virtual imaging trial to assess how AI performance is affected by several patient- and physics-based factors, including the extent of disease, radiation dose, and imaging modality of computed tomography (CT) and chest radiography (CXR). AI performance was strongly influenced by dataset characteristics including quantity, diversity, and prevalence, leading to poor generalization with up to 20% drop in receiver operating characteristic area under the curve. Model
&lt;/p&gt;</description></item></channel></rss>