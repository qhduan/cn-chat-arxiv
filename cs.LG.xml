<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;</title><link>https://arxiv.org/abs/2403.16542</link><description>&lt;p&gt;
&#20855;&#26377;&#30456;&#20851;&#22122;&#22768;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Online Federated Learning with Correlated Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16542
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#30456;&#20851;&#22122;&#22768;&#25552;&#39640;&#25928;&#29992;&#24182;&#30830;&#20445;&#38544;&#31169;&#30340;&#24046;&#20998;&#38544;&#31169;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#24314;&#31435;&#20102;&#21160;&#24577;&#36951;&#25022;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#32852;&#37030;&#23398;&#20064;&#65292;&#21033;&#29992;&#26102;&#38388;&#30456;&#20851;&#30340;&#22122;&#22768;&#26469;&#25552;&#39640;&#25928;&#29992;&#21516;&#26102;&#30830;&#20445;&#36830;&#32493;&#21457;&#24067;&#30340;&#27169;&#22411;&#30340;&#38544;&#31169;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#33258;DP&#22122;&#22768;&#21644;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#27969;&#24335;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#25200;&#21160;&#36845;&#20195;&#20998;&#26512;&#26469;&#25511;&#21046;DP&#22122;&#22768;&#23545;&#25928;&#29992;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20934;&#24378;&#20984;&#26465;&#20214;&#19979;&#22914;&#20309;&#26377;&#25928;&#31649;&#29702;&#26469;&#33258;&#26412;&#22320;&#26356;&#26032;&#30340;&#28418;&#31227;&#35823;&#24046;&#12290;&#22312;$(\epsilon, \delta)$-DP&#39044;&#31639;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#25972;&#20010;&#26102;&#38388;&#27573;&#19978;&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#37327;&#21270;&#20102;&#20851;&#38190;&#21442;&#25968;&#30340;&#24433;&#21709;&#20197;&#21450;&#21160;&#24577;&#29615;&#22659;&#21464;&#21270;&#30340;&#24378;&#24230;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16542v1 Announce Type: new  Abstract: We propose a novel differentially private algorithm for online federated learning that employs temporally correlated noise to improve the utility while ensuring the privacy of the continuously released models. To address challenges stemming from DP noise and local updates with streaming noniid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon that quantifies the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments validate the efficacy of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22797;&#26434;&#25968;&#25454;&#32858;&#31867;&#26694;&#26550;&#65288;CDC&#65289;&#65292;&#33021;&#22815;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#39640;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#22270;&#28388;&#27874;&#22120;&#21644;&#39640;&#36136;&#37327;&#38170;&#28857;&#26469;&#34701;&#21512;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#20449;&#24687;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#38598;&#32676;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03670</link><description>&lt;p&gt;
CDC&#65306;&#22797;&#26434;&#25968;&#25454;&#32858;&#31867;&#30340;&#31616;&#21333;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CDC: A Simple Framework for Complex Data Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03670
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22797;&#26434;&#25968;&#25454;&#32858;&#31867;&#26694;&#26550;&#65288;CDC&#65289;&#65292;&#33021;&#22815;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#39640;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#22270;&#28388;&#27874;&#22120;&#21644;&#39640;&#36136;&#37327;&#38170;&#28857;&#26469;&#34701;&#21512;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#20449;&#24687;&#65292;&#20855;&#26377;&#24456;&#39640;&#30340;&#38598;&#32676;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#20170;&#25968;&#25454;&#39537;&#21160;&#30340;&#25968;&#23383;&#26102;&#20195;&#65292;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#37327;&#20197;&#21450;&#22797;&#26434;&#24230;&#65288;&#22914;&#22810;&#35270;&#22270;&#12289;&#38750;&#27431;&#20960;&#37324;&#24471;&#21644;&#22810;&#20851;&#32852;&#24615;&#65289;&#27491;&#22312;&#21576;&#25351;&#25968;&#29978;&#33267;&#26356;&#24555;&#22320;&#22686;&#38271;&#12290;&#32858;&#31867;&#26080;&#30417;&#30563;&#22320;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#25928;&#30693;&#35782;&#65292;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#29420;&#31435;&#24320;&#21457;&#65292;&#22788;&#29702;&#19968;&#20010;&#29305;&#23450;&#25361;&#25112;&#65292;&#29306;&#29298;&#20854;&#20182;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22797;&#26434;&#25968;&#25454;&#32858;&#31867;&#65288;CDC&#65289;&#26694;&#26550;&#65292;&#21487;&#20197;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#39640;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#22270;&#28388;&#27874;&#22120;&#34701;&#21512;&#20960;&#20309;&#32467;&#26500;&#21644;&#23646;&#24615;&#20449;&#24687;&#12290;&#28982;&#21518;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#20445;&#23384;&#30456;&#20284;&#24615;&#30340;&#27491;&#21017;&#21270;&#22120;&#33258;&#36866;&#24212;&#23398;&#20064;&#39640;&#36136;&#37327;&#38170;&#28857;&#26469;&#38477;&#20302;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#19978;&#35828;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#38598;&#32676;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23558;CDC&#37096;&#32626;&#21040;&#35268;&#27169;&#20026;111M&#30340;&#22270;&#25968;&#25454;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03670v1 Announce Type: new  Abstract: In today's data-driven digital era, the amount as well as complexity, such as multi-view, non-Euclidean, and multi-relational, of the collected data are growing exponentially or even faster. Clustering, which unsupervisely extracts valid knowledge from data, is extremely useful in practice. However, existing methods are independently developed to handle one particular challenge at the expense of the others. In this work, we propose a simple but effective framework for complex data clustering (CDC) that can efficiently process different types of data with linear complexity. We first utilize graph filtering to fuse geometry structure and attribute information. We then reduce the complexity with high-quality anchors that are adaptively learned via a novel similarity-preserving regularizer. We illustrate the cluster-ability of our proposed method theoretically and experimentally. In particular, we deploy CDC to graph data of size 111M.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22635;&#34917;&#20102;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#22312;&#21508;&#24322;&#25968;&#25454;&#19978;&#25910;&#25947;&#20998;&#26512;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#24378;&#20984;&#21644;&#19968;&#33324;&#20984;&#30446;&#26631;&#30340;SFL&#25910;&#25947;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O(1/T)$&#21644;$O(1/\sqrt[3]{T})&#12290;</title><link>https://arxiv.org/abs/2402.15166</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#24322;&#26500;&#25968;&#25454;&#19978;&#30340;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#30340;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis of Split Federated Learning on Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22635;&#34917;&#20102;&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#22312;&#21508;&#24322;&#25968;&#25454;&#19978;&#25910;&#25947;&#20998;&#26512;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#20102;&#38024;&#23545;&#24378;&#20984;&#21644;&#19968;&#33324;&#20984;&#30446;&#26631;&#30340;SFL&#25910;&#25947;&#20998;&#26512;&#65292;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O(1/T)$&#21644;$O(1/\sqrt[3]{T})&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#35010;&#32852;&#37030;&#23398;&#20064;&#65288;SFL&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#30340;&#20998;&#24067;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#36827;&#34892;&#21327;&#20316;&#27169;&#22411;&#35757;&#32451;&#12290;&#22312;SFL&#20013;&#65292;&#20840;&#23616;&#27169;&#22411;&#36890;&#24120;&#34987;&#20998;&#20026;&#20004;&#37096;&#20998;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#20197;&#24182;&#34892;&#32852;&#37030;&#26041;&#24335;&#35757;&#32451;&#19968;&#37096;&#20998;&#65292;&#20027;&#26381;&#21153;&#22120;&#35757;&#32451;&#21478;&#19968;&#37096;&#20998;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;SFL&#31639;&#27861;&#21457;&#23637;&#30340;&#30740;&#31350;&#24456;&#22810;&#65292;&#20294;SFL&#30340;&#25910;&#25947;&#20998;&#26512;&#22312;&#25991;&#29486;&#20013;&#36824;&#26410;&#26377;&#25552;&#21450;&#65292;&#26412;&#25991;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#23545;SFL&#36827;&#34892;&#20998;&#26512;&#21487;&#33021;&#27604;&#23545;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20998;&#26512;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#36825;&#26159;&#30001;&#20110;&#23458;&#25143;&#31471;&#21644;&#20027;&#26381;&#21153;&#22120;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#21452;&#36895;&#26356;&#26032;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#38024;&#23545;&#24322;&#26500;&#25968;&#25454;&#19978;&#24378;&#20984;&#21644;&#19968;&#33324;&#20984;&#30446;&#26631;&#30340;SFL&#25910;&#25947;&#20998;&#26512;&#12290;&#25910;&#25947;&#36895;&#29575;&#20998;&#21035;&#20026;$O(1/T)$&#21644;$O(1/\sqrt[3]{T})$&#65292;&#20854;&#20013;$T$&#34920;&#31034;SFL&#35757;&#32451;&#30340;&#24635;&#36718;&#25968;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#38750;&#20984;&#30446;&#26631;&#21644;&#19968;&#20123;&#23458;&#25143;&#31471;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#21487;&#29992;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15166v1 Announce Type: cross  Abstract: Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients. In SFL, a global model is typically split into two parts, where clients train one part in a parallel federated manner, and a main server trains the other. Despite the recent research on SFL algorithm development, the convergence analysis of SFL is missing in the literature, and this paper aims to fill this gap. The analysis of SFL can be more challenging than that of federated learning (FL), due to the potential dual-paced updates at the clients and the main server. We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data. The convergence rates are $O(1/T)$ and $O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and where some clients may be unavailable during trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#27969;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#24182;&#38454;&#26799;&#23646;&#24615;&#30340;&#26080;&#22522;&#30784;&#25512;&#24191;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31245;&#24378;&#30340;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#25439;&#22833;&#20989;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#33267;&#38646;&#12290;</title><link>https://arxiv.org/abs/2402.08948</link><description>&lt;p&gt;
&#20351;&#29992;&#39640;&#26031;&#36755;&#20837;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Mean-Field Analysis for Learning Subspace-Sparse Polynomials with Gaussian Input
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#27969;&#21160;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21512;&#24182;&#38454;&#26799;&#23646;&#24615;&#30340;&#26080;&#22522;&#30784;&#25512;&#24191;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31245;&#24378;&#30340;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#25439;&#22833;&#20989;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#33267;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#21644;&#21452;&#23618;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#23376;&#31354;&#38388;&#31232;&#30095;&#22810;&#39033;&#24335;&#30340;&#22343;&#22330;&#27969;&#21160;&#65292;&#20854;&#20013;&#36755;&#20837;&#20998;&#24067;&#26159;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#65292;&#36755;&#20986;&#20165;&#20381;&#36182;&#20110;&#36755;&#20837;&#22312;&#20302;&#32500;&#23376;&#31354;&#38388;&#19978;&#30340;&#25237;&#24433;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Abbe&#31561;&#20154;(2022&#24180;)&#20013;&#21512;&#24182;&#38454;&#26799;&#23646;&#24615;&#30340;&#26080;&#22522;&#30784;&#25512;&#24191;&#65292;&#24182;&#24314;&#31435;&#20102;SGD&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#27492;&#26465;&#20214;&#20960;&#20046;&#26159;&#20805;&#20998;&#30340;&#65292;&#21363;&#27604;&#24517;&#35201;&#26465;&#20214;&#31245;&#24378;&#30340;&#26465;&#20214;&#21487;&#20197;&#20445;&#35777;&#25439;&#22833;&#20989;&#25968;&#30340;&#25351;&#25968;&#34928;&#20943;&#33267;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08948v1 Announce Type: new Abstract: In this work, we study the mean-field flow for learning subspace-sparse polynomials using stochastic gradient descent and two-layer neural networks, where the input distribution is standard Gaussian and the output only depends on the projection of the input onto a low-dimensional subspace. We propose a basis-free generalization of the merged-staircase property in Abbe et al. (2022) and establish a necessary condition for the SGD-learnability. In addition, we prove that the condition is almost sufficient, in the sense that a condition slightly stronger than the necessary condition can guarantee the exponential decay of the loss functional to zero.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.07204</link><description>&lt;p&gt;
&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#22478;&#24066;&#34892;&#31243;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Synergizing Spatial Optimization with Large Language Models for Open-Domain Urban Itinerary Planning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#65292;&#36890;&#36807;&#32467;&#21512;&#31354;&#38388;&#20248;&#21270;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;Open-domain Urban Itinerary Planning (OUIP)&#20219;&#21153;&#65292;&#29992;&#20110;&#26681;&#25454;&#29992;&#25143;&#20197;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#35831;&#27714;&#30452;&#25509;&#29983;&#25104;&#34892;&#31243;&#12290;OUIP&#19982;&#20256;&#32479;&#34892;&#31243;&#35268;&#21010;&#19981;&#21516;&#65292;&#20256;&#32479;&#35268;&#21010;&#38480;&#21046;&#20102;&#29992;&#25143;&#34920;&#36798;&#26356;&#35814;&#32454;&#30340;&#38656;&#27714;&#65292;&#38459;&#30861;&#20102;&#30495;&#27491;&#30340;&#20010;&#24615;&#21270;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#22788;&#29702;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38750;&#23454;&#26102;&#20449;&#24687;&#12289;&#19981;&#23436;&#25972;&#30340;&#30693;&#35782;&#21644;&#19981;&#36275;&#30340;&#31354;&#38388;&#24847;&#35782;&#65292;&#23427;&#20204;&#26080;&#27861;&#29420;&#31435;&#22320;&#25552;&#20379;&#28385;&#24847;&#30340;&#29992;&#25143;&#20307;&#39564;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ItiNera&#30340;OUIP&#31995;&#32479;&#65292;&#23558;&#31354;&#38388;&#20248;&#21270;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30456;&#32467;&#21512;&#65292;&#26681;&#25454;&#29992;&#25143;&#38656;&#27714;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#22478;&#24066;&#34892;&#31243;&#23450;&#21046;&#26381;&#21153;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;LLM&#30340;&#27969;&#27700;&#32447;&#65292;&#29992;&#20110;&#25552;&#21462;&#21644;&#26356;&#26032;&#20852;&#36259;&#28857;&#29305;&#24449;&#65292;&#20197;&#21019;&#24314;&#29992;&#25143;&#33258;&#24049;&#30340;&#20010;&#24615;&#21270;&#20852;&#36259;&#28857;&#25968;&#25454;&#24211;&#12290;&#23545;&#20110;&#27599;&#20010;&#29992;&#25143;&#35831;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;LLM&#36827;&#34892;&#21327;&#21516;&#23454;&#29616;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we for the first time propose the task of Open-domain Urban Itinerary Planning (OUIP) for citywalk, which directly generates itineraries based on users' requests described in natural language. OUIP is different from conventional itinerary planning, which limits users from expressing more detailed needs and hinders true personalization. Recently, large language models (LLMs) have shown potential in handling diverse tasks. However, due to non-real-time information, incomplete knowledge, and insufficient spatial awareness, they are unable to independently deliver a satisfactory user experience in OUIP. Given this, we present ItiNera, an OUIP system that synergizes spatial optimization with Large Language Models (LLMs) to provide services that customize urban itineraries based on users' needs. Specifically, we develop an LLM-based pipeline for extracting and updating POI features to create a user-owned personalized POI database. For each user request, we leverage LLM in coop
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#27169;&#22411;&#30340;&#26032;&#23481;&#37327;&#27979;&#37327;2sED&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38480;&#21046;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#35823;&#24046;&#20855;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36880;&#23618;&#36845;&#20195;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;2sED&#65292;&#20174;&#32780;&#22788;&#29702;&#22823;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.09184</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#20004;&#23610;&#24230;&#22797;&#26434;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Two-Scale Complexity Measure for Deep Learning Models. (arXiv:2401.09184v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09184
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#32479;&#35745;&#27169;&#22411;&#30340;&#26032;&#23481;&#37327;&#27979;&#37327;2sED&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38480;&#21046;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#19988;&#19982;&#35757;&#32451;&#35823;&#24046;&#20855;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36880;&#23618;&#36845;&#20195;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#36817;&#20284;2sED&#65292;&#20174;&#32780;&#22788;&#29702;&#22823;&#37327;&#21442;&#25968;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#32500;&#24230;&#30340;&#32479;&#35745;&#27169;&#22411;&#26032;&#23481;&#37327;&#27979;&#37327;2sED&#12290;&#36825;&#20010;&#26032;&#30340;&#25968;&#37327;&#22312;&#23545;&#27169;&#22411;&#36827;&#34892;&#28201;&#21644;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#21487;&#38752;&#22320;&#38480;&#21046;&#27867;&#21270;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26631;&#20934;&#25968;&#25454;&#38598;&#21644;&#27969;&#34892;&#30340;&#27169;&#22411;&#26550;&#26500;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;2sED&#19982;&#35757;&#32451;&#35823;&#24046;&#20855;&#26377;&#24456;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#23545;&#20110;&#39532;&#23572;&#21487;&#22827;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36880;&#23618;&#36845;&#20195;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20174;&#19979;&#26041;&#36817;&#20284;2sED&#65292;&#20174;&#32780;&#35299;&#20915;&#20855;&#26377;&#22823;&#37327;&#21442;&#25968;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#36817;&#20284;&#23545;&#19981;&#21516;&#30340;&#31361;&#20986;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#37117;&#24456;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel capacity measure 2sED for statistical models based on the effective dimension. The new quantity provably bounds the generalization error under mild assumptions on the model. Furthermore, simulations on standard data sets and popular model architectures show that 2sED correlates well with the training error. For Markovian models, we show how to efficiently approximate 2sED from below through a layerwise iterative approach, which allows us to tackle deep learning models with a large number of parameters. Simulation results suggest that the approximation is good for different prominent models and data sets.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20154;&#31867;&#22996;&#25176;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22996;&#25176;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20449;&#24687;&#19979;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#36825;&#39033;&#30740;&#31350;&#25512;&#36827;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#22996;&#25176;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.04729</link><description>&lt;p&gt;
&#20851;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20154;&#31867;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#30340;&#22996;&#25176;&#34892;&#20026;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effect of Contextual Information on Human Delegation Behavior in Human-AI collaboration. (arXiv:2401.04729v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04729
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#20013;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20154;&#31867;&#22996;&#25176;&#34892;&#20026;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#22996;&#25176;&#34892;&#20026;&#22312;&#19981;&#21516;&#19978;&#19979;&#25991;&#20449;&#24687;&#19979;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#36825;&#39033;&#30740;&#31350;&#25512;&#36827;&#20102;&#23545;&#20154;&#24037;&#26234;&#33021;&#22996;&#25176;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#31995;&#32479;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#26029;&#22686;&#24378;&#33021;&#21147;&#20026;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#20316;&#24102;&#26469;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;&#21033;&#29992;&#29616;&#26377;&#30340;&#20114;&#34917;&#33021;&#21147;&#65292;&#35753;&#20154;&#20204;&#23558;&#20010;&#21035;&#23454;&#20363;&#22996;&#25176;&#32473;&#20154;&#24037;&#26234;&#33021;&#26159;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20351;&#20154;&#20204;&#26377;&#25928;&#22320;&#22996;&#25176;&#23454;&#20363;&#38656;&#35201;&#20182;&#20204;&#35780;&#20272;&#33258;&#24049;&#21644;&#20154;&#24037;&#26234;&#33021;&#22312;&#32473;&#23450;&#20219;&#21153;&#30340;&#32972;&#26223;&#19979;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#20154;&#31867;&#20915;&#23450;&#23558;&#23454;&#20363;&#22996;&#25176;&#32473;&#20154;&#24037;&#26234;&#33021;&#26102;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25552;&#20379;&#19978;&#19979;&#25991;&#20449;&#24687;&#26174;&#33879;&#25552;&#39640;&#20102;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#24403;&#21442;&#19982;&#32773;&#25509;&#25910;&#21040;&#19981;&#21516;&#31867;&#22411;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#26102;&#65292;&#22996;&#25176;&#34892;&#20026;&#20250;&#21457;&#29983;&#26174;&#33879;&#21464;&#21270;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#36825;&#39033;&#30740;&#31350;&#25512;&#36827;&#20102;&#20154;&#24037;&#26234;&#33021;&#22996;&#25176;&#20013;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#29702;&#35299;&#65292;&#24182;&#20026;&#35774;&#35745;&#26356;&#26377;&#25928;&#30340;&#21327;&#20316;&#31995;&#32479;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The constantly increasing capabilities of artificial intelligence (AI) open new possibilities for human-AI collaboration. One promising approach to leverage existing complementary capabilities is allowing humans to delegate individual instances to the AI. However, enabling humans to delegate instances effectively requires them to assess both their own and the AI's capabilities in the context of the given task. In this work, we explore the effects of providing contextual information on human decisions to delegate instances to an AI. We find that providing participants with contextual information significantly improves the human-AI team performance. Additionally, we show that the delegation behavior changes significantly when participants receive varying types of contextual information. Overall, this research advances the understanding of human-AI interaction in human delegation and provides actionable insights for designing more effective collaborative systems.
&lt;/p&gt;</description></item><item><title>HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.01145</link><description>&lt;p&gt;
HAAQI-Net: &#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net: A non-intrusive neural music quality assessment model for hearing aids. (arXiv:2401.01145v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01145
&lt;/p&gt;
&lt;p&gt;
HAAQI-Net&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#21161;&#21548;&#22120;&#29992;&#25143;&#30340;&#38750;&#20405;&#20837;&#24615;&#31070;&#32463;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;BLSTM&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#20197;&#21450;&#39044;&#35757;&#32451;&#30340;BEATs&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#65292;&#33021;&#22815;&#24555;&#36895;&#19988;&#20934;&#30830;&#22320;&#39044;&#27979;&#38899;&#20048;&#30340;HAAQI&#24471;&#20998;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#24615;&#33021;&#21644;&#26356;&#20302;&#30340;&#25512;&#29702;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HAAQI-Net&#65292;&#19968;&#31181;&#38024;&#23545;&#21161;&#21548;&#22120;&#29992;&#25143;&#23450;&#21046;&#30340;&#38750;&#20405;&#20837;&#24615;&#28145;&#24230;&#23398;&#20064;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#22914;Hearing Aid Audio Quality Index (HAAQI) &#19981;&#21516;&#65292;HAAQI-Net&#37319;&#29992;&#20102;&#24102;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#21452;&#21521;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;(BLSTM)&#12290;&#35813;&#27169;&#22411;&#20197;&#35780;&#20272;&#30340;&#38899;&#20048;&#26679;&#26412;&#21644;&#21548;&#21147;&#25439;&#22833;&#27169;&#24335;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#39044;&#27979;&#30340;HAAQI&#24471;&#20998;&#12290;&#27169;&#22411;&#37319;&#29992;&#20102;&#39044;&#35757;&#32451;&#30340;&#26469;&#33258;&#38899;&#39057;&#21464;&#25442;&#22120;(BEATs)&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#36827;&#34892;&#22768;&#23398;&#29305;&#24449;&#25552;&#21462;&#12290;&#36890;&#36807;&#23558;&#39044;&#27979;&#20998;&#25968;&#19982;&#30495;&#23454;&#20998;&#25968;&#36827;&#34892;&#27604;&#36739;&#65292;HAAQI-Net&#36798;&#21040;&#20102;0.9257&#30340;&#38271;&#26399;&#19968;&#33268;&#24615;&#30456;&#20851;(LCC)&#65292;0.9394&#30340;&#26031;&#30382;&#23572;&#26364;&#31561;&#32423;&#30456;&#20851;&#31995;&#25968;(SRCC)&#65292;&#21644;0.0080&#30340;&#22343;&#26041;&#35823;&#24046;(MSE)&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#31181;&#39640;&#24615;&#33021;&#20276;&#38543;&#30528;&#25512;&#29702;&#26102;&#38388;&#30340;&#22823;&#24133;&#20943;&#23569;&#65306;&#20174;62.52&#31186;(HAAQI)&#20943;&#23569;&#21040;2.71&#31186;(HAAQI-Net)&#65292;&#20026;&#21161;&#21548;&#22120;&#29992;&#25143;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#38899;&#36136;&#35780;&#20272;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces HAAQI-Net, a non-intrusive deep learning model for music quality assessment tailored to hearing aid users. In contrast to traditional methods like the Hearing Aid Audio Quality Index (HAAQI), HAAQI-Net utilizes a Bidirectional Long Short-Term Memory (BLSTM) with attention. It takes an assessed music sample and a hearing loss pattern as input, generating a predicted HAAQI score. The model employs the pre-trained Bidirectional Encoder representation from Audio Transformers (BEATs) for acoustic feature extraction. Comparing predicted scores with ground truth, HAAQI-Net achieves a Longitudinal Concordance Correlation (LCC) of 0.9257, Spearman's Rank Correlation Coefficient (SRCC) of 0.9394, and Mean Squared Error (MSE) of 0.0080. Notably, this high performance comes with a substantial reduction in inference time: from 62.52 seconds (by HAAQI) to 2.71 seconds (by HAAQI-Net), serving as an efficient music quality assessment model for hearing aid users.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#20803;&#32467;&#26524;&#25552;&#21319;&#24314;&#27169;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#38646;&#32467;&#26524;&#26679;&#26412;&#30340;&#20449;&#24687;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290; (arXiv:2310.05549v1 [stat.ML])</title><link>http://arxiv.org/abs/2310.05549</link><description>&lt;p&gt;
&#19968;&#20010;&#26032;&#30340;&#20108;&#20803;&#32467;&#26524;&#25552;&#21319;&#24314;&#27169;&#36716;&#25442;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Transformation Approach for Uplift Modeling with Binary Outcome. (arXiv:2310.05549v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20108;&#20803;&#32467;&#26524;&#25552;&#21319;&#24314;&#27169;&#36716;&#25442;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#38646;&#32467;&#26524;&#26679;&#26412;&#30340;&#20449;&#24687;&#24182;&#19988;&#26131;&#20110;&#20351;&#29992;&#12290; (arXiv:2310.05549v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#21319;&#24314;&#27169;&#22312;&#24066;&#22330;&#33829;&#38144;&#21644;&#23458;&#25143;&#20445;&#30041;&#31561;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#26377;&#25928;&#24212;&#29992;&#65292;&#29992;&#20110;&#38024;&#23545;&#37027;&#20123;&#30001;&#20110;&#27963;&#21160;&#25110;&#27835;&#30103;&#26356;&#26377;&#21487;&#33021;&#20135;&#29983;&#21453;&#24212;&#30340;&#23458;&#25143;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20108;&#20803;&#32467;&#26524;&#36716;&#25442;&#26041;&#27861;&#65292;&#35299;&#38145;&#20102;&#38646;&#32467;&#26524;&#26679;&#26412;&#30340;&#20840;&#37096;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uplift modeling has been used effectively in fields such as marketing and customer retention, to target those customers who are more likely to respond due to the campaign or treatment. Essentially, it is a machine learning technique that predicts the gain from performing some action with respect to not taking it. A popular class of uplift models is the transformation approach that redefines the target variable with the original treatment indicator. These transformation approaches only need to train and predict the difference in outcomes directly. The main drawback of these approaches is that in general it does not use the information in the treatment indicator beyond the construction of the transformed outcome and usually is not efficient. In this paper, we design a novel transformed outcome for the case of the binary target variable and unlock the full value of the samples with zero outcome. From a practical perspective, our new approach is flexible and easy to use. Experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#31232;&#30095;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26159;&#28176;&#36817;&#26412;&#22320;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#29702;&#35770;&#19978;&#19982;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2305.10391</link><description>&lt;p&gt;
&#31232;&#30095;&#22270;&#30340;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#30340;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;&#23558;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#20110;&#31232;&#30095;&#22270;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#26159;&#28176;&#36817;&#26412;&#22320;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29616;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#31639;&#27861;&#65292;&#24182;&#23558;&#26368;&#20248;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#29702;&#35770;&#19978;&#19982;&#29616;&#26377;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29305;&#24449;&#35013;&#39280;&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#65292;&#22312;&#31232;&#30095;&#35774;&#32622;&#19979;&#65292;&#21363;&#33410;&#28857;&#30340;&#39044;&#26399;&#24230;&#25968;&#20026;&#33410;&#28857;&#25968;&#30340;O(1)&#26102;&#12290;&#36825;&#26679;&#30340;&#22270;&#36890;&#24120;&#34987;&#31216;&#20026;&#26412;&#22320;&#26641;&#29366;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21483;&#20570;&#28176;&#36817;&#26412;&#22320;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#27010;&#24565;&#65292;&#24182;&#26681;&#25454;&#36825;&#20010;&#26631;&#20934;&#35745;&#31639;&#20102;&#20855;&#26377;&#20219;&#24847;&#33410;&#28857;&#29305;&#24449;&#21644;&#36793;&#36830;&#25509;&#20998;&#24067;&#30340;&#30456;&#24403;&#19968;&#33324;&#30340;&#32479;&#35745;&#25968;&#25454;&#27169;&#22411;&#30340;&#26368;&#20248;&#20998;&#31867;&#22120;&#12290;&#35813;&#26368;&#20248;&#20998;&#31867;&#22120;&#21487;&#20197;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#23454;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#35745;&#31639;&#20102;&#35813;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#35823;&#24046;&#65292;&#24182;&#22312;&#19968;&#20010;&#24050;&#32463;&#30740;&#31350;&#20805;&#20998;&#30340;&#32479;&#35745;&#27169;&#22411;&#19978;&#20174;&#29702;&#35770;&#19978;&#19982;&#29616;&#26377;&#30340;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#20302;&#22270;&#20449;&#21495;&#30340;&#24773;&#20917;&#19979;&#65292;&#26368;&#20339;&#28040;&#24687;&#20256;&#36882;&#26550;&#26500;&#25554;&#20540;&#20110;&#26631;&#20934;MLP&#21644;&#19968;&#31181;&#20856;&#22411;&#30340;c&#26550;&#26500;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the node classification problem on feature-decorated graphs in the sparse setting, i.e., when the expected degree of a node is $O(1)$ in the number of nodes. Such graphs are typically known to be locally tree-like. We introduce a notion of Bayes optimality for node classification tasks, called asymptotic local Bayes optimality, and compute the optimal classifier according to this criterion for a fairly general statistical data model with arbitrary distributions of the node features and edge connectivity. The optimal classifier is implementable using a message-passing graph neural network architecture. We then compute the generalization error of this classifier and compare its performance against existing learning methods theoretically on a well-studied statistical model with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find that the optimal message-passing architecture interpolates between a standard MLP in the regime of low graph signal and a typical c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#31639;&#27861;&#34892;&#20026;&#30340;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#22270;&#34920;&#31034;&#26469;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#21363;&#35299;&#37322;&#21333;&#20803;&#26356;&#21152;&#21487;&#35299;&#37322;&#19988;&#32771;&#34385;&#20102;&#23439;&#35266;&#32423;&#29305;&#24449;&#21644;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#12290;</title><link>http://arxiv.org/abs/2006.02482</link><description>&lt;p&gt;
&#29992;&#22240;&#26524;&#23398;&#20064;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#31639;&#27861;&#30340;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Explaining the Behavior of Black-Box Prediction Algorithms with Causal Learning. (arXiv:2006.02482v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.02482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#31639;&#27861;&#34892;&#20026;&#30340;&#22240;&#26524;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#22240;&#26524;&#22270;&#34920;&#31034;&#26469;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#21363;&#35299;&#37322;&#21333;&#20803;&#26356;&#21152;&#21487;&#35299;&#37322;&#19988;&#32771;&#34385;&#20102;&#23439;&#35266;&#32423;&#29305;&#24449;&#21644;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#23398;&#26041;&#27861;&#22312;&#35299;&#37322;&#40657;&#31665;&#39044;&#27979;&#27169;&#22411;&#65288;&#20363;&#22914;&#22522;&#20110;&#22270;&#20687;&#20687;&#32032;&#25968;&#25454;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65289;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#37325;&#35201;&#32570;&#28857;&#65306;&#65288;i&#65289;&#8220;&#35299;&#37322;&#21333;&#20803;&#8221;&#26159;&#30456;&#20851;&#39044;&#27979;&#27169;&#22411;&#30340;&#24494;&#35266;&#32423;&#36755;&#20837;&#65292;&#20363;&#22914;&#22270;&#20687;&#20687;&#32032;&#65292;&#32780;&#19981;&#26159;&#26356;&#26377;&#29992;&#20110;&#29702;&#35299;&#22914;&#20309;&#21487;&#33021;&#25913;&#21464;&#31639;&#27861;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#30340;&#23439;&#35266;&#32423;&#29305;&#24449;&#65307;&#65288;ii&#65289;&#29616;&#26377;&#26041;&#27861;&#20551;&#35774;&#29305;&#24449;&#19982;&#30446;&#26631;&#27169;&#22411;&#39044;&#27979;&#20043;&#38388;&#19981;&#23384;&#22312;&#26410;&#27979;&#37327;&#30340;&#28151;&#28102;&#65292;&#36825;&#22312;&#35299;&#37322;&#21333;&#20803;&#26159;&#23439;&#35266;&#32423;&#21464;&#37327;&#26102;&#19981;&#25104;&#31435;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#22312;&#20998;&#26512;&#20154;&#21592;&#26080;&#27861;&#35775;&#38382;&#30446;&#26631;&#39044;&#27979;&#31639;&#27861;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#30340;&#37325;&#35201;&#24773;&#20917;&#65292;&#32780;&#21482;&#33021;&#26681;&#25454;&#29305;&#23450;&#36755;&#20837;&#26597;&#35810;&#27169;&#22411;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25552;&#20379;&#22240;&#26524;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#22240;&#26524;&#22270;&#34920;&#31034;&#65292;&#20801;&#35768;&#26356;&#22909;&#22320;&#29702;&#35299;&#31639;&#27861;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal approaches to post-hoc explainability for black-box prediction models (e.g., deep neural networks trained on image pixel data) have become increasingly popular. However, existing approaches have two important shortcomings: (i) the "explanatory units" are micro-level inputs into the relevant prediction model, e.g., image pixels, rather than interpretable macro-level features that are more useful for understanding how to possibly change the algorithm's behavior, and (ii) existing approaches assume there exists no unmeasured confounding between features and target model predictions, which fails to hold when the explanatory units are macro-level variables. Our focus is on the important setting where the analyst has no access to the inner workings of the target prediction algorithm, rather only the ability to query the output of the model in response to a particular input. To provide causal explanations in such a setting, we propose to learn causal graphical representations that allo
&lt;/p&gt;</description></item></channel></rss>