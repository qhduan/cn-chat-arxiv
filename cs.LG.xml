<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2404.02127</link><description>&lt;p&gt;
FLawN-T5: &#26377;&#25928;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#28151;&#21512;&#22312;&#27861;&#24459;&#25512;&#29702;&#20013;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
FLawN-T5: An Empirical Examination of Effective Instruction-Tuning Data Mixtures for Legal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#35777;&#26126;&#20102;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#21487;&#20197;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#20026;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02127v1  &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#25351;&#23548;&#35843;&#25972;&#26159;&#20351;&#35821;&#35328;&#27169;&#22411;&#23545;&#30452;&#25509;&#29992;&#25143;&#20132;&#20114;&#26377;&#25928;&#30340;&#37325;&#35201;&#27493;&#39588;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#27861;&#24459;&#20219;&#21153;&#20173;&#28982;&#36229;&#20986;&#20102;&#22823;&#22810;&#25968;&#24320;&#25918;&#24335;LLMs&#30340;&#33539;&#22260;&#65292;&#32780;&#19988;&#30446;&#21069;&#35813;&#39046;&#22495;&#36824;&#27809;&#26377;&#20219;&#20309;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#35813;&#24212;&#29992;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21517;&#20026;LawInstruct&#30340;&#22823;&#22411;&#27861;&#24459;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;&#20102;17&#20010;&#21496;&#27861;&#31649;&#36758;&#21306;&#12289;24&#31181;&#35821;&#35328;&#65292;&#24635;&#35745;1200&#19975;&#20010;&#31034;&#20363;&#12290;&#25105;&#20204;&#21576;&#29616;&#35777;&#25454;&#34920;&#26126;&#65292;&#39046;&#22495;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#35843;&#25972;&#33021;&#22815;&#25913;&#21892;&#22312;LegalBench&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#23558;Flan-T5 XL&#22312;&#22522;&#20934;&#32447;&#19978;&#25552;&#39640;8&#20010;&#28857;&#25110;16%&#12290;&#28982;&#32780;&#65292;&#35813;&#25928;&#24212;&#24182;&#19981;&#36866;&#29992;&#20110;&#25152;&#26377;&#20219;&#21153;&#12289;&#35757;&#32451;&#27169;&#24335;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;LawInstruct&#26159;&#19968;&#20010;&#36164;&#28304;&#65292;&#21487;&#20197;&#21152;&#36895;&#22312;&#27861;&#24459;&#39046;&#22495;&#24320;&#21457;&#20855;&#26377;&#26356;&#24378;&#20449;&#24687;&#22788;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02127v1 Announce Type: cross  Abstract: Instruction tuning is an important step in making language models useful for direct user interaction. However, many legal tasks remain out of reach for most open LLMs and there do not yet exist any large scale instruction datasets for the domain. This critically limits research in this application area. In this work, we curate LawInstruct, a large legal instruction dataset, covering 17 jurisdictions, 24 languages and a total of 12M examples. We present evidence that domain-specific pretraining and instruction tuning improve performance on LegalBench, including improving Flan-T5 XL by 8 points or 16\% over the baseline. However, the effect does not generalize across all tasks, training regimes, model sizes, and other factors. LawInstruct is a resource for accelerating the development of models with stronger information processing and decision making capabilities in the legal domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;CDR&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#65292;&#36991;&#20813;&#20256;&#32479;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.20298</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#23884;&#20837;&#21644;&#23618;&#27425;&#24863;&#30693;&#22495;&#35299;&#32806;&#30340;&#22522;&#20110;&#35780;&#35770;&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;CDR&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#65292;&#36991;&#20813;&#20256;&#32479;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#23545;&#25512;&#33616;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#30340;&#31639;&#27861;&#65292;&#20197;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#21560;&#24341;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#25429;&#25417;&#21487;&#22312;&#39046;&#22495;&#38388;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20174;&#26356;&#20016;&#23500;&#30340;&#39046;&#22495;&#65288;&#28304;&#39046;&#22495;&#65289;&#36716;&#31227;&#21040;&#26356;&#31232;&#30095;&#30340;&#39046;&#22495;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#27431;&#20960;&#37324;&#24503;&#23884;&#20837;&#31354;&#38388;&#65292;&#22312;&#20934;&#30830;&#34920;&#31034;&#26356;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22788;&#29702;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#20513;&#23548;&#19968;&#31181;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#30340;&#21452;&#26354;CDR&#26041;&#27861;&#26469;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#12290;&#39318;&#20808;&#24378;&#35843;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#21452;&#26354;&#20960;&#20309;&#20013;&#23545;&#23567;&#20462;&#25913;&#36896;&#25104;&#30340;&#24178;&#25200;&#20250;&#34987;&#25918;&#22823;&#65292;&#26368;&#32456;&#23548;&#33268;&#23618;&#27425;&#24615;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20298v1 Announce Type: cross  Abstract: The issue of data sparsity poses a significant challenge to recommender systems. In response to this, algorithms that leverage side information such as review texts have been proposed. Furthermore, Cross-Domain Recommendation (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention. Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items. This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships. We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic geometry result in magnified perturbations, ultimately leading to the collapse of hierarchical 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#24212;&#29992;&#30340;&#28145;&#24230;&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#65292;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35745;&#31639;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#20013;&#23637;&#29616;&#20986;&#27604;&#26631;&#20934;&#32447;&#24615;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20854;&#29305;&#23450;&#26550;&#26500;&#20351;&#24471;&#23454;&#29616;&#26356;&#39640;&#38454;&#32423;&#25968;&#23637;&#24320;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#36127;&#25285;&#12290;</title><link>https://arxiv.org/abs/2403.18044</link><description>&lt;p&gt;
&#28145;&#24230;&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#20302;&#32500;&#32447;&#24615;&#21442;&#25968;&#21464;&#21270;&#36924;&#36817;&#21644;&#38750;&#32447;&#24615;&#21453;&#39304;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Deep polytopic autoencoders for low-dimensional linear parameter-varying approximations and nonlinear feedback design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18044
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#24212;&#29992;&#30340;&#28145;&#24230;&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#65292;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35745;&#31639;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#20013;&#23637;&#29616;&#20986;&#27604;&#26631;&#20934;&#32447;&#24615;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#20854;&#29305;&#23450;&#26550;&#26500;&#20351;&#24471;&#23454;&#29616;&#26356;&#39640;&#38454;&#32423;&#25968;&#23637;&#24320;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#25552;&#20379;&#20102;&#22810;&#38754;&#20307;&#20013;&#29366;&#24577;&#30340;&#20302;&#32500;&#21442;&#25968;&#21270;&#12290;&#23545;&#20110;&#38750;&#32447;&#24615;PDE&#65292;&#36825;&#24456;&#23481;&#26131;&#24212;&#29992;&#20110;&#20302;&#32500;&#32447;&#24615;&#21442;&#25968;&#21464;&#21270;(LPV)&#36924;&#36817;&#65292;&#22240;&#20026;&#23427;&#20204;&#24050;&#34987;&#29992;&#20110;&#36890;&#36807;&#29366;&#24577;&#30456;&#20851;Riccati&#26041;&#31243;&#30340;&#32423;&#25968;&#23637;&#24320;&#23454;&#29616;&#26377;&#25928;&#30340;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#24212;&#29992;&#30340;&#22810;&#38754;&#20307;&#33258;&#32534;&#30721;&#22120;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#22312;&#35270;&#22270;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;LPV&#36924;&#36817;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#32447;&#24615;&#26041;&#27861;&#65292;&#20197;&#21450;&#29305;&#23450;&#26550;&#26500;&#22914;&#20309;&#22312;&#20960;&#20046;&#27809;&#26377;&#39069;&#22806;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#26356;&#39640;&#38454;&#32423;&#25968;&#23637;&#24320;&#12290;&#25105;&#20204;&#36890;&#36807;&#24443;&#24213;&#30340;&#25968;&#20540;&#30740;&#31350;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#31995;&#32479;&#30340;&#35745;&#31639;&#38750;&#32447;&#24615;&#25511;&#21046;&#22120;&#35774;&#35745;&#20013;&#30340;&#24615;&#36136;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18044v1 Announce Type: cross  Abstract: Polytopic autoencoders provide low-dimensional parametrizations of states in a polytope. For nonlinear PDEs, this is readily applied to low-dimensional linear parameter-varying (LPV) approximations as they have been exploited for efficient nonlinear controller design via series expansions of the solution to the state-dependent Riccati equation. In this work, we develop a polytopic autoencoder for control applications and show how it outperforms standard linear approaches in view of LPV approximations of nonlinear systems and how the particular architecture enables higher order series expansions at little extra computational effort. We illustrate the properties and potentials of this approach to computational nonlinear controller design for large-scale systems with a thorough numerical study.
&lt;/p&gt;</description></item><item><title>TT-BLIP&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;BLIP&#21644;Tri-Transformer&#25216;&#26415;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#65292;&#37319;&#29992;Multimodal Tri-Transformer&#34701;&#21512;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#32508;&#21512;&#34920;&#24449;&#21644;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.12481</link><description>&lt;p&gt;
TT-BLIP&#65306;&#20351;&#29992;BLIP&#21644;Tri-Transformer&#22686;&#24378;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
TT-BLIP: Enhancing Fake News Detection Using BLIP and Tri-Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12481
&lt;/p&gt;
&lt;p&gt;
TT-BLIP&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;BLIP&#21644;Tri-Transformer&#25216;&#26415;&#65292;&#32467;&#21512;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#20449;&#24687;&#25552;&#21462;&#65292;&#37319;&#29992;Multimodal Tri-Transformer&#34701;&#21512;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22686;&#24378;&#30340;&#32508;&#21512;&#34920;&#24449;&#21644;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12481v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;   &#25688;&#35201;&#65306;&#26816;&#27979;&#20551;&#26032;&#38395;&#21463;&#21040;&#20102;&#26497;&#22823;&#20851;&#27880;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#26041;&#27861;&#23558;&#29420;&#31435;&#32534;&#30721;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#20018;&#32852;&#65292;&#24573;&#30053;&#20102;&#32508;&#21512;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#22909;&#22788;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#32570;&#20047;&#19987;&#38376;&#30340;&#29305;&#24449;&#25552;&#21462;&#36827;&#19968;&#27493;&#38480;&#21046;&#20102;&#36825;&#20123;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;TT-BLIP&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#20449;&#24687;&#24212;&#29992;&#20102;&#24341;&#23548;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#29992;&#20110;&#32479;&#19968;&#30340;&#35270;&#35273;-&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#65288;BLIP&#65289;&#65306;BERT &#21644; BLIP\textsubscript{Txt} &#29992;&#20110;&#25991;&#26412;&#65292;ResNet &#21644; BLIP\textsubscript{Img} &#29992;&#20110;&#22270;&#20687;&#65292;&#20197;&#21450;&#29992;&#20110;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#21452;&#21521; BLIP &#32534;&#30721;&#22120;&#12290;&#22810;&#27169;&#24577;&#19977;&#35282;&#21464;&#25442;&#22120;&#20351;&#29992;&#19977;&#31181;&#31867;&#22411;&#30340;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#34701;&#21512;&#19977;&#27169;&#24577;&#29305;&#24449;&#65292;&#30830;&#20445;&#20102;&#22686;&#24378;&#34920;&#31034;&#21644;&#25913;&#36827;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#20998;&#26512;&#12290;&#23454;&#39564;&#20351;&#29992;&#20102;&#20004;&#20010;&#20551;&#26032;&#38395;&#25968;&#25454;&#38598;&#65292;&#24494;&#21338;&#21644;Gossipcop&#12290; &#32467;&#26524;&#34920;&#26126;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12481v1 Announce Type: new  Abstract: Detecting fake news has received a lot of attention. Many previous methods concatenate independently encoded unimodal data, ignoring the benefits of integrated multimodal information. Also, the absence of specialized feature extraction for text and images further limits these methods. This paper introduces an end-to-end model called TT-BLIP that applies the bootstrapping language-image pretraining for unified vision-language understanding and generation (BLIP) for three types of information: BERT and BLIP\textsubscript{Txt} for text, ResNet and BLIP\textsubscript{Img} for images, and bidirectional BLIP encoders for multimodal information. The Multimodal Tri-Transformer fuses tri-modal features using three types of multi-head attention mechanisms, ensuring integrated modalities for enhanced representations and improved multimodal data analysis. The experiments are performed using two fake news datasets, Weibo and Gossipcop. The results in
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.14648</link><description>&lt;p&gt;
&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#37325;&#26032;&#24605;&#32771;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#20197;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Rethinking Invariance Regularization in Adversarial Training to Improve Robustness-Accuracy Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14648
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#20102;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;Asymmetrically Representation-regularized Adversarial Training (AR-AT)&#26469;&#35299;&#20915;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#21644;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#65292;&#25913;&#21892;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#19968;&#30452;&#26159;&#25269;&#25239;&#23545;&#25239;&#24615;&#26679;&#26412;&#65288;AEs&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#23384;&#22312;&#40065;&#26834;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#22522;&#20110;&#34920;&#31034;&#30340;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#65292;&#23398;&#20064;&#20855;&#26377;&#36776;&#21035;&#24615;&#21364;&#23545;&#25239;&#24615;&#19981;&#21464;&#30340;&#34920;&#31034;&#65292;&#26088;&#22312;&#32531;&#35299;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#22312;&#32463;&#39564;&#19978;&#30830;&#23450;&#20102;&#22952;&#30861;&#19981;&#21464;&#24615;&#27491;&#21017;&#21270;&#30340;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19981;&#21464;&#24615;&#25439;&#22833;&#21644;&#20998;&#31867;&#30446;&#26631;&#20043;&#38388;&#30340;&#8220;&#26799;&#24230;&#20914;&#31361;&#8221;&#65292;&#34920;&#26126;&#23384;&#22312;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#20197;&#21450;&#65288;2&#65289;&#30001;&#20110;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#36755;&#20837;&#30340;&#20998;&#24067;&#21457;&#25955;&#32780;&#20986;&#29616;&#30340;&#28151;&#21512;&#20998;&#24067;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#23545;&#31216;&#34920;&#31034;&#27491;&#21017;&#21270;&#30340;&#23545;&#25239;&#35757;&#32451;&#65288;AR-AT&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#19968;&#20010;&#20572;&#27490;&#26799;&#24230;&#25805;&#20316;&#21644;&#19968;&#20010;&#39044;&#27979;&#22120;&#26469;&#36991;&#20813;&#8220;&#23849;&#28291;&#35299;&#8221;&#65292;&#28789;&#24863;&#26469;&#33258;&#26368;&#36817;&#30340;&#38750;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14648v1 Announce Type: cross  Abstract: Although adversarial training has been the state-of-the-art approach to defend against adversarial examples (AEs), they suffer from a robustness-accuracy trade-off. In this work, we revisit representation-based invariance regularization to learn discriminative yet adversarially invariant representations, aiming to mitigate this trade-off. We empirically identify two key issues hindering invariance regularization: (1) a "gradient conflict" between invariance loss and classification objectives, indicating the existence of "collapsing solutions," and (2) the mixture distribution problem arising from diverged distributions of clean and adversarial inputs. To address these issues, we propose Asymmetrically Representation-regularized Adversarial Training (AR-AT), which incorporates a stop-gradient operation and a pre-dictor in the invariance loss to avoid "collapsing solutions," inspired by a recent non-contrastive self-supervised learning a
&lt;/p&gt;</description></item><item><title>&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;</title><link>https://arxiv.org/abs/2402.08062</link><description>&lt;p&gt;
&#36991;&#20813;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#28798;&#38590;&#65306;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;
&lt;/p&gt;
&lt;p&gt;
Avoiding Catastrophe in Continuous Spaces by Asking for Help
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08062
&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#23547;&#27714;&#24110;&#21161;&#26469;&#36991;&#20813;&#28798;&#38590;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#28798;&#38590;&#21457;&#29983;&#30340;&#27010;&#29575;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#36830;&#32493;1D&#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;&#31616;&#21333;&#30340;&#22238;&#25253;&#20989;&#25968;&#19979;&#65292;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110;0&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20855;&#26377;&#27491;&#24335;&#36951;&#25022;&#20445;&#35777;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20551;&#35774;&#25152;&#26377;&#38169;&#35823;&#37117;&#26159;&#21487;&#36870;&#30340;&#65292;&#24182;&#20381;&#36182;&#20110;&#23581;&#35797;&#25152;&#26377;&#21487;&#33021;&#30340;&#36873;&#39033;&#12290;&#24403;&#19968;&#20123;&#38169;&#35823;&#26159;&#26080;&#27861;&#20462;&#22797;&#29978;&#33267;&#26159;&#28798;&#38590;&#24615;&#30340;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#20250;&#23548;&#33268;&#31967;&#31957;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#19979;&#25991;&#22810;&#33218;&#36172;&#21338;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#21457;&#29983;&#28798;&#38590;&#30340;&#27010;&#29575;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#36718;&#30340;&#22238;&#25253;&#20195;&#34920;&#20102;&#22312;&#35813;&#36718;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65292;&#24182;&#23581;&#35797;&#26368;&#22823;&#21270;&#22238;&#25253;&#30340;&#20056;&#31215;&#65288;&#24635;&#20307;&#36991;&#20813;&#28798;&#38590;&#30340;&#27010;&#29575;&#65289;&#12290;&#20026;&#20102;&#32473; agent &#19968;&#20123;&#25104;&#21151;&#30340;&#26426;&#20250;&#65292;&#25105;&#20204;&#20801;&#35768;&#26377;&#38480;&#27425;&#21521;&#23548;&#24072;&#25552;&#38382;&#65292;&#24182;&#20551;&#35774;&#22238;&#25253;&#20989;&#25968;&#20026; Lipschitz &#36830;&#32493;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#24403;&#26102;&#38388;&#36328;&#24230;&#22686;&#38271;&#26102;&#65292;&#23427;&#30340;&#36951;&#25022;&#21644;&#21521;&#23548;&#24072;&#26597;&#35810;&#29575;&#37117;&#36235;&#36817;&#20110; 0&#65292;&#20551;&#35774;&#26159;&#19968;&#20010;&#36830;&#32493;&#30340; 1D &#29366;&#24577;&#31354;&#38388;&#21644;&#30456;&#23545;"&#31616;&#21333;"&#30340;&#22238;&#25253;&#20989;&#25968;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#21305;&#37197;&#30340;&#19979;&#30028;&#65306;&#22312;&#27809;&#26377;&#31616;&#21333;&#24615;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#20309;&#31639;&#27861;&#35201;&#20040;&#19981;&#26029;&#26597;&#35810;&#24322;&#24120;&#30340;&#34892;&#20026;&#65292;&#35201;&#20040;&#27599;&#27425;&#26597;&#35810;&#23436;&#20840;&#30456;&#21516;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most reinforcement learning algorithms with formal regret guarantees assume all mistakes are reversible and rely on essentially trying all possible options. This approach leads to poor outcomes when some mistakes are irreparable or even catastrophic. We propose a variant of the contextual bandit problem where the goal is to minimize the chance of catastrophe. Specifically, we assume that the payoff each round represents the chance of avoiding catastrophe that round, and try to maximize the product of payoffs (the overall chance of avoiding catastrophe). To give the agent some chance of success, we allow a limited number of queries to a mentor and assume a Lipschitz continuous payoff function. We present an algorithm whose regret and rate of querying the mentor both approach 0 as the time horizon grows, assuming a continuous 1D state space and a relatively "simple" payoff function. We also provide a matching lower bound: without the simplicity assumption: any algorithm either constantly
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2402.03923</link><description>&lt;p&gt;
&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer
&lt;/p&gt;
&lt;p&gt;
Return-Aligned Decision Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#36890;&#36807;&#20998;&#31163;&#22238;&#25253;&#19982;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#65292;&#23454;&#29616;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#23398;&#20064;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65288;&#21363;&#22238;&#25253;&#65289;&#30340;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24212;&#29992;&#33539;&#22260;&#30340;&#25193;&#22823;&#65292;&#35757;&#32451;&#33021;&#22815;&#26368;&#22823;&#21270;&#22238;&#25253;&#24182;&#20351;&#23454;&#38469;&#22238;&#25253;&#19982;&#25351;&#23450;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#30340;&#26234;&#33021;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20174;&#32780;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#24615;&#33021;&#12290;&#20915;&#31574;Transformer&#65288;DT&#65289;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#20248;&#21270;&#29983;&#25104;&#20197;&#30446;&#26631;&#22238;&#25253;&#20026;&#26465;&#20214;&#30340;&#21160;&#20316;&#30340;&#31574;&#30053;&#65292;&#24182;&#37197;&#22791;&#20102;&#20351;&#29992;&#30446;&#26631;&#22238;&#25253;&#25511;&#21046;&#26234;&#33021;&#20307;&#30340;&#26426;&#21046;&#12290;&#23613;&#31649;DT&#26088;&#22312;&#23545;&#40784;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#65292;&#20294;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#21457;&#29616;&#20102;DT&#20013;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36820;&#22238;&#23545;&#40784;&#30340;&#20915;&#31574;Transformer&#65288;RADT&#65289;&#65292;&#26088;&#22312;&#26377;&#25928;&#22320;&#23558;&#23454;&#38469;&#22238;&#25253;&#19982;&#30446;&#26631;&#22238;&#25253;&#23545;&#40784;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#23558;&#22238;&#25253;&#20174;&#20256;&#32479;&#30340;&#36755;&#20837;&#24207;&#21015;&#20013;&#20998;&#31163;&#20986;&#26469;&#65292;&#20256;&#32479;&#36755;&#20837;&#24207;&#21015;&#36890;&#24120;&#21253;&#21547;&#22238;&#25253;&#12289;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional approaches in offline reinforcement learning aim to learn the optimal policy that maximizes the cumulative reward, also known as return. However, as applications broaden, it becomes increasingly crucial to train agents that not only maximize the returns, but align the actual return with a specified target return, giving control over the agent's performance. Decision Transformer (DT) optimizes a policy that generates actions conditioned on the target return through supervised learning and is equipped with a mechanism to control the agent using the target return. Despite being designed to align the actual return with the target return, we have empirically identified a discrepancy between the actual return and the target return in DT. In this paper, we propose Return-Aligned Decision Transformer (RADT), designed to effectively align the actual return with the target return. Our model decouples returns from the conventional input sequence, which typically consists of returns, s
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#35745;&#31639;&#39640;&#38454;&#23548;&#25968;&#65292;&#23558;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.03885</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#38454;&#23548;&#25968;&#24635;&#32467;&#65292;&#23558;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Adapting Newton's Method to Neural Networks through a Summary of Higher-Order Derivatives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03885
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#35745;&#31639;&#39640;&#38454;&#23548;&#25968;&#65292;&#23558;&#29275;&#39039;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#24212;&#29992;&#20110;&#21521;&#37327;&#21464;&#37327;$\boldsymbol{\theta}$&#19978;&#30340;&#20989;&#25968;$\mathcal{L}$&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;$\boldsymbol{\theta}$&#34987;&#34920;&#31034;&#20026;&#20803;&#32452;$(\mathbf{T}_1, \cdots, \mathbf{T}_S)$&#30340;&#24352;&#37327;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#35768;&#22810;&#24120;&#35265;&#30340;&#29992;&#20363;&#65292;&#20363;&#22914;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#20302;&#24265;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#33258;&#21160;&#24494;&#20998;&#21644;&#35745;&#31639;&#25216;&#24039;&#65292;&#25552;&#20379;&#20851;&#20110;$\mathcal{L}$&#21450;&#20854;&#24352;&#37327;$\mathbf{T}_s$&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#39640;&#38454;&#20449;&#24687;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#25216;&#26415;&#26469;&#24314;&#31435;&#19968;&#20010;&#20108;&#38454;&#20248;&#21270;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#35757;&#32451;&#21508;&#31181;&#26550;&#26500;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#20108;&#38454;&#26041;&#27861;&#21033;&#29992;&#20102;$\boldsymbol{\theta}$&#34987;&#20998;&#21106;&#20026;&#24352;&#37327;$(\mathbf{T}_1, \cdots, \mathbf{T}_S)$&#30340;&#20998;&#21306;&#32467;&#26500;&#65292;&#22240;&#27492;&#19981;&#38656;&#35201;&#35745;&#31639;$\mathcal{L}$&#30340;Hessian&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a gradient-based optimization method applied to a function $\mathcal{L}$ of a vector of variables $\boldsymbol{\theta}$, in the case where $\boldsymbol{\theta}$ is represented as a tuple of tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$. This framework encompasses many common use-cases, such as training neural networks by gradient descent. First, we propose a computationally inexpensive technique providing higher-order information on $\mathcal{L}$, especially about the interactions between the tensors $\mathbf{T}_s$, based on automatic differentiation and computational tricks. Second, we use this technique at order 2 to build a second-order optimization method which is suitable, among other things, for training deep neural networks of various architectures. This second-order method leverages the partition structure of $\boldsymbol{\theta}$ into tensors $(\mathbf{T}_1, \cdots, \mathbf{T}_S)$, in such a way that it requires neither the computation of the Hessian of $\mathcal{
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#37325;&#21472;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.10148</link><description>&lt;p&gt;
&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Explicitly Disentangled Representations in Object-Centric Learning. (arXiv:2401.10148v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10148
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#29289;&#20307;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#26126;&#30830;&#35299;&#24320;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#31354;&#38388;&#21010;&#20998;&#20026;&#20004;&#20010;&#19981;&#37325;&#21472;&#30340;&#23376;&#38598;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#21152;&#31283;&#23450;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#35270;&#35273;&#25968;&#25454;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#34920;&#31034;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#26080;&#30417;&#30563;&#23398;&#20064;&#29289;&#20307;&#20013;&#24515;&#21270;&#34920;&#31034;&#30340;&#25216;&#26415;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22686;&#24378;&#28508;&#22312;&#29305;&#24449;&#30340;&#31283;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#35757;&#32451;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#27493;&#39588;&#26159;&#35299;&#24320;&#23548;&#33268;&#25968;&#25454;&#21464;&#21270;&#30340;&#22240;&#32032;&#12290;&#20808;&#21069;&#65292;&#19981;&#21464;&#21345;&#27133;&#27880;&#24847;&#23454;&#29616;&#20102;&#20174;&#20854;&#20182;&#29305;&#24449;&#20013;&#35299;&#24320;&#20301;&#32622;&#12289;&#23610;&#24230;&#21644;&#26041;&#21521;&#12290;&#25193;&#23637;&#36825;&#19968;&#26041;&#27861;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#20998;&#31163;&#24418;&#29366;&#21644;&#32441;&#29702;&#32452;&#25104;&#37096;&#20998;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#23558;&#29289;&#20307;&#20013;&#24515;&#21270;&#27169;&#22411;&#20013;&#30340;&#24418;&#29366;&#21644;&#32441;&#29702;&#25104;&#20998;&#20559;&#32622;&#20026;&#28508;&#22312;&#31354;&#38388;&#32500;&#24230;&#30340;&#20004;&#20010;&#19981;&#37325;&#21472;&#23376;&#38598;&#12290;&#36825;&#20123;&#23376;&#38598;&#26159;&#20808;&#39564;&#24050;&#30693;&#30340;&#65292;&#22240;&#27492;&#22312;&#35757;&#32451;&#36807;&#31243;&#20043;&#21069;&#12290;&#22312;&#19968;&#31995;&#21015;&#29289;&#20307;&#20013;&#24515;&#21270;&#27979;&#35797;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#25581;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Extracting structured representations from raw visual data is an important and long-standing challenge in machine learning. Recently, techniques for unsupervised learning of object-centric representations have raised growing interest. In this context, enhancing the robustness of the latent features can improve the efficiency and effectiveness of the training of downstream tasks. A promising step in this direction is to disentangle the factors that cause variation in the data. Previously, Invariant Slot Attention disentangled position, scale, and orientation from the remaining features. Extending this approach, we focus on separating the shape and texture components. In particular, we propose a novel architecture that biases object-centric models toward disentangling shape and texture components into two non-overlapping subsets of the latent space dimensions. These subsets are known a priori, hence before the training process. Experiments on a range of object-centric benchmarks reveal t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35299;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#30340;&#31283;&#20581;&#26550;&#26500;&#65292;&#24182;&#22312;&#30561;&#30496;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2310.06715</link><description>&lt;p&gt;
S4Sleep: &#35299;&#26512;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
S4Sleep: Elucidating the design space of deep-learning-based sleep stage classification models. (arXiv:2310.06715v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#26512;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#30561;&#30496;&#38454;&#27573;&#20998;&#31867;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#19981;&#21516;&#36755;&#20837;&#34920;&#31034;&#30340;&#31283;&#20581;&#26550;&#26500;&#65292;&#24182;&#22312;&#30561;&#30496;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#36890;&#36947;&#30561;&#30496;&#33041;&#30005;&#22270;&#35760;&#24405;&#36827;&#34892;&#30561;&#30496;&#38454;&#27573;&#25171;&#20998;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#23384;&#22312;&#26174;&#33879;&#30340;&#35780;&#20998;&#20154;&#21592;&#20043;&#38388;&#24046;&#24322;&#30340;&#20219;&#21153;&#12290;&#22240;&#27492;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#24102;&#26469;&#24456;&#22823;&#30340;&#30410;&#22788;&#12290;&#34429;&#28982;&#24050;&#32463;&#20026;&#27492;&#25552;&#20986;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#20294;&#26576;&#20123;&#20851;&#38190;&#30340;&#26550;&#26500;&#20915;&#31574;&#24182;&#26410;&#24471;&#21040;&#31995;&#32479;&#24615;&#30340;&#25506;&#32034;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35814;&#32454;&#35843;&#26597;&#20102;&#24191;&#27867;&#30340;&#32534;&#30721;&#22120;-&#39044;&#27979;&#22120;&#26550;&#26500;&#33539;&#30068;&#20869;&#30340;&#36825;&#20123;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#25214;&#21040;&#20102;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#21644;&#22768;&#35889;&#22270;&#36755;&#20837;&#34920;&#31034;&#30340;&#31283;&#20581;&#26550;&#26500;&#12290;&#36825;&#20123;&#26550;&#26500;&#23558;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#20316;&#20026;&#32452;&#25104;&#37096;&#20998;&#65292;&#23545;&#24191;&#27867;&#30340;SHHS&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#32479;&#35745;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;&#36825;&#20123;&#25913;&#36827;&#36890;&#36807;&#32479;&#35745;&#21644;&#31995;&#32479;&#35823;&#24046;&#20272;&#35745;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#39044;&#35745;&#65292;&#20174;&#26412;&#30740;&#31350;&#20013;&#33719;&#24471;&#30340;&#26550;&#26500;&#27934;&#23519;&#19981;&#20165;&#23545;&#26410;&#26469;&#30340;&#30561;&#30496;&#20998;&#26399;&#30740;&#31350;&#26377;&#20215;&#20540;&#65292;&#32780;&#19988;&#23545;&#25972;&#20307;&#30561;&#30496;&#30740;&#31350;&#37117;&#26377;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scoring sleep stages in polysomnography recordings is a time-consuming task plagued by significant inter-rater variability. Therefore, it stands to benefit from the application of machine learning algorithms. While many algorithms have been proposed for this purpose, certain critical architectural decisions have not received systematic exploration. In this study, we meticulously investigate these design choices within the broad category of encoder-predictor architectures. We identify robust architectures applicable to both time series and spectrogram input representations. These architectures incorporate structured state space models as integral components, leading to statistically significant advancements in performance on the extensive SHHS dataset. These improvements are assessed through both statistical and systematic error estimations. We anticipate that the architectural insights gained from this study will not only prove valuable for future research in sleep staging but also hol
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#30340;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#65292;&#24182;&#20855;&#26377;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.07870</link><description>&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#23454;&#29616;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Brain-Inspired Computational Intelligence via Predictive Coding. (arXiv:2308.07870v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07870
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#30340;&#33041;&#21551;&#21457;&#24335;&#35745;&#31639;&#26234;&#33021;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#29616;&#26377;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20123;&#37325;&#35201;&#38480;&#21046;&#65292;&#24182;&#20855;&#26377;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26377;&#24076;&#26395;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27491;&#22312;&#36805;&#36895;&#25104;&#20026;&#26412;&#19990;&#32426;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#22312;AI&#39046;&#22495;&#21462;&#24471;&#30340;&#22823;&#37096;&#20998;&#25104;&#26524;&#37117;&#26159;&#20351;&#29992;&#35823;&#24046;&#21453;&#21521;&#20256;&#25773;&#23398;&#20064;&#31639;&#27861;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25152;&#23454;&#29616;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#26222;&#21450;&#24212;&#29992;&#24050;&#32463;&#20984;&#26174;&#20986;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#38590;&#20197;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12289;&#32570;&#20047;&#40065;&#26834;&#24615;&#12289;&#19981;&#21487;&#38752;&#24615;&#21644;&#29983;&#29289;&#23398;&#19978;&#30340;&#19981;&#21512;&#29702;&#24615;&#12290;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#38656;&#35201;&#21463;&#21040;&#31070;&#32463;&#31185;&#23398;&#29702;&#35770;&#30340;&#21551;&#21457;&#21644;&#25351;&#23548;&#30340;&#26041;&#26696;&#12290;&#20854;&#20013;&#19968;&#31181;&#29702;&#35770;&#31216;&#20026;&#39044;&#27979;&#32534;&#30721;&#65288;PC&#65289;&#65292;&#22312;&#26426;&#22120;&#26234;&#33021;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20855;&#26377;&#20196;&#20154;&#20852;&#22859;&#30340;&#29305;&#24615;&#65292;&#20351;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#20013;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#65306;PC&#21487;&#20197;&#27169;&#25311;&#19981;&#21516;&#33041;&#21306;&#30340;&#20449;&#24687;&#22788;&#29702;&#65292;&#21487;&#20197;&#29992;&#20110;&#35748;&#30693;&#25511;&#21046;&#21644;&#26426;&#22120;&#20154;&#25216;&#26415;&#65292;&#24182;&#22312;&#21464;&#20998;&#25512;&#29702;&#26041;&#38754;&#20855;&#26377;&#22362;&#23454;&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) is rapidly becoming one of the key technologies of this century. The majority of results in AI thus far have been achieved using deep neural networks trained with the error backpropagation learning algorithm. However, the ubiquitous adoption of this approach has highlighted some important limitations such as substantial computational cost, difficulty in quantifying uncertainty, lack of robustness, unreliability, and biological implausibility. It is possible that addressing these limitations may require schemes that are inspired and guided by neuroscience theories. One such theory, called predictive coding (PC), has shown promising performance in machine intelligence tasks, exhibiting exciting properties that make it potentially valuable for the machine learning community: PC can model information processing in different brain areas, can be used in cognitive control and robotics, and has a solid mathematical grounding in variational inference, offering a pow
&lt;/p&gt;</description></item><item><title>AutoML&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#21270;&#26500;&#24314;&#31471;&#21040;&#31471;AI/ML&#27969;&#27700;&#32447;&#30340;&#35299;&#20915;&#26041;&#26696;&#34987;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#20854;&#22312;&#24320;&#21457;AI/ML&#31995;&#32479;&#30340;&#22242;&#38431;&#20013;&#30340;&#37319;&#29992;&#31243;&#24230;&#21644;&#24863;&#30693;&#31243;&#24230;&#32570;&#20047;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.10774</link><description>&lt;p&gt;
&#35780;&#20272;AutoML&#22312;&#25968;&#25454;&#39537;&#21160;&#36719;&#20214;&#24037;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Assessing the Use of AutoML for Data-Driven Software Engineering. (arXiv:2307.10774v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10774
&lt;/p&gt;
&lt;p&gt;
AutoML&#20316;&#20026;&#19968;&#31181;&#33258;&#21160;&#21270;&#26500;&#24314;&#31471;&#21040;&#31471;AI/ML&#27969;&#27700;&#32447;&#30340;&#35299;&#20915;&#26041;&#26696;&#34987;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#30446;&#21069;&#23545;&#20854;&#22312;&#24320;&#21457;AI/ML&#31995;&#32479;&#30340;&#22242;&#38431;&#20013;&#30340;&#37319;&#29992;&#31243;&#24230;&#21644;&#24863;&#30693;&#31243;&#24230;&#32570;&#20047;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#30001;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#26500;&#24314;&#36719;&#20214;&#24212;&#29992;&#26041;&#38754;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20844;&#21496;&#27491;&#21162;&#21147;&#25307;&#32856;&#20855;&#26377;&#28145;&#20837;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#30340;&#21592;&#24037;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;AutoML&#20316;&#20026;&#22635;&#34917;AI / ML&#25216;&#33021;&#32570;&#21475;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#22823;&#21463;&#27426;&#36814;&#65292;&#22240;&#20026;&#23427;&#25215;&#35834;&#33258;&#21160;&#21270;&#26500;&#24314;&#31471;&#21040;&#31471;AI / ML&#27969;&#27700;&#32447;&#65292;&#36825;&#20123;&#27969;&#27700;&#32447;&#36890;&#24120;&#30001;&#19987;&#38376;&#30340;&#22242;&#38431;&#25104;&#21592;&#35774;&#35745;&#12290;&#30446;&#26631;&#65306;&#23613;&#31649;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#21644;&#39640;&#26399;&#26395;&#65292;&#20294;&#30446;&#21069;&#23545;&#20110;&#24320;&#21457;AI / ML&#31995;&#32479;&#30340;&#22242;&#38431;&#24403;&#21069;&#37319;&#29992;AutoML&#30340;&#31243;&#24230;&#20197;&#21450;&#20174;&#23454;&#36341;&#32773;&#21644;&#30740;&#31350;&#32773;&#30340;&#35270;&#35282;&#26469;&#30475;&#23427;&#30340;&#24863;&#30693;&#31243;&#24230;&#32570;&#20047;&#20449;&#24687;&#12290;&#26041;&#27861;&#65306;&#20026;&#20102;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#65292;&#21253;&#25324;&#23545;&#20004;&#20010;&#36719;&#20214;&#24037;&#31243;&#25968;&#25454;&#38598;&#20013;12&#20010;&#31471;&#21040;&#31471;AutoML&#24037;&#20855;&#30340;&#22522;&#20934;&#27979;&#35797;&#20197;&#21450;&#29992;&#25143;&#35843;&#26597;&#21644;&#21518;&#32493;&#35775;&#35848;&#65292;&#20197;&#36827;&#19968;&#27493;&#20102;&#35299;AutoML&#30340;&#37319;&#29992;&#21644;&#24863;&#30693;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#21457;&#29616;AutoML&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
Background. Due to the widespread adoption of Artificial Intelligence (AI) and Machine Learning (ML) for building software applications, companies are struggling to recruit employees with a deep understanding of such technologies. In this scenario, AutoML is soaring as a promising solution to fill the AI/ML skills gap since it promises to automate the building of end-to-end AI/ML pipelines that would normally be engineered by specialized team members. Aims. Despite the growing interest and high expectations, there is a dearth of information about the extent to which AutoML is currently adopted by teams developing AI/ML-enabled systems and how it is perceived by practitioners and researchers. Method. To fill these gaps, in this paper, we present a mixed-method study comprising a benchmark of 12 end-to-end AutoML tools on two SE datasets and a user survey with follow-up interviews to further our understanding of AutoML adoption and perception. Results. We found that AutoML solutions can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#25345;&#32493;&#20877;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.05494</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#23545;&#25239;&#24615;&#28431;&#27934;&#25915;&#20987;&#30340;&#23454;&#29992;&#24615;&#27979;&#35797;&#65306;&#21160;&#24577;&#23398;&#20064;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Adversarial Evasion Attacks Practicality in Networks: Testing the Impact of Dynamic Learning. (arXiv:2306.05494v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#21516;&#26102;&#25506;&#31350;&#20102;&#25345;&#32493;&#20877;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(NIDS)&#20013;&#65292;&#30001;&#20110;&#20854;&#33258;&#21160;&#21270;&#30340;&#29305;&#24615;&#21644;&#22312;&#22788;&#29702;&#21644;&#20998;&#31867;&#22823;&#37327;&#25968;&#25454;&#19978;&#30340;&#39640;&#31934;&#24230;&#12290;&#20294;&#26426;&#22120;&#23398;&#20064;&#23384;&#22312;&#32570;&#38519;&#65292;&#20854;&#20013;&#26368;&#22823;&#30340;&#38382;&#39064;&#20043;&#19968;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#20854;&#30446;&#30340;&#26159;&#20351;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20135;&#29983;&#38169;&#35823;&#30340;&#39044;&#27979;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#36129;&#29486;&#65306;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NIDS&#23454;&#29992;&#24615;&#38382;&#39064;&#30340;&#20998;&#31867;&#21644;&#23545;&#25345;&#32493;&#35757;&#32451;&#23545;NIDS&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21363;&#20351;&#27809;&#26377;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#25345;&#32493;&#20877;&#35757;&#32451;&#20063;&#21487;&#20197;&#20943;&#23569;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#23545;&#25239;&#24615;&#25915;&#20987;&#21487;&#33021;&#20250;&#21361;&#21450;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;NIDS&#65292;&#20294;&#25345;&#32493;&#20877;&#35757;&#32451;&#21487;&#24102;&#26469;&#19968;&#23450;&#30340;&#32531;&#35299;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has become ubiquitous, and its deployment in Network Intrusion Detection Systems (NIDS) is inevitable due to its automated nature and high accuracy in processing and classifying large volumes of data. However, ML has been found to have several flaws, on top of them are adversarial attacks, which aim to trick ML models into producing faulty predictions. While most adversarial attack research focuses on computer vision datasets, recent studies have explored the practicality of such attacks against ML-based network security entities, especially NIDS.  This paper presents two distinct contributions: a taxonomy of practicality issues associated with adversarial attacks against ML-based NIDS and an investigation of the impact of continuous training on adversarial attacks against NIDS. Our experiments indicate that continuous re-training, even without adversarial training, can reduce the effect of adversarial attacks. While adversarial attacks can harm ML-based NIDSs, ou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.06375</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#30340;Transformer&#65306;&#34920;&#31034;&#21644;&#35757;&#32451;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20197;&#22797;&#21046;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22312;&#36825;&#19968;&#32467;&#26500;&#21270;&#39046;&#22495;&#30340;&#25104;&#21151;&#12290;&#29305;&#21035;&#26377;&#36259;&#30340;&#26159;&#65292;&#34920;&#26684;&#25968;&#25454;&#20855;&#26377;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#37329;&#34701;&#20132;&#26131;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#20540;&#30340;&#24322;&#36136;&#24615;&#65292;&#20854;&#20013;&#31867;&#21035;&#20803;&#32032;&#19982;&#25968;&#20540;&#39033;&#28151;&#21512;&#65292;&#20351;&#24471;&#36825;&#31181;&#36866;&#24212;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#26469;&#34920;&#31034;&#24322;&#26500;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#25968;&#20540;&#29305;&#24449;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#34920;&#31034;&#65292;&#24182;&#19988;&#25972;&#20010;&#32593;&#32476;&#20351;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#21033;&#29992;&#27874;&#12289;&#36335;&#24452;&#34892;&#31243;&#26102;&#38388;&#21644;eikonal&#26041;&#31243;&#26469;&#25551;&#36848;&#20449;&#24687;&#30340;&#20256;&#25773;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#27874;&#21644;eikonal&#27169;&#22411;&#30340;&#32467;&#21512;&#12290;&#20316;&#32773;&#22312;&#38543;&#26426;&#22270;&#24418;&#12289;&#23567;&#19990;&#30028;&#22270;&#21644;&#23454;&#38469;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;</title><link>http://arxiv.org/abs/2201.07577</link><description>&lt;p&gt;
&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Models for information propagation on graphs. (arXiv:2201.07577v3 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2201.07577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32479;&#19968;&#30340;&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#19977;&#31181;&#19981;&#21516;&#30340;&#31867;&#21035;&#65292;&#21033;&#29992;&#27874;&#12289;&#36335;&#24452;&#34892;&#31243;&#26102;&#38388;&#21644;eikonal&#26041;&#31243;&#26469;&#25551;&#36848;&#20449;&#24687;&#30340;&#20256;&#25773;&#65292;&#24182;&#32473;&#20986;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#27874;&#21644;eikonal&#27169;&#22411;&#30340;&#32467;&#21512;&#12290;&#20316;&#32773;&#22312;&#38543;&#26426;&#22270;&#24418;&#12289;&#23567;&#19990;&#30028;&#22270;&#21644;&#23454;&#38469;&#32593;&#32476;&#19978;&#36827;&#34892;&#20102;&#25968;&#20540;&#27169;&#25311;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#21644;&#32479;&#19968;&#20102;&#19981;&#21516;&#30340;&#22270;&#19978;&#20449;&#24687;&#20256;&#25773;&#27169;&#22411;&#12290;&#31532;&#19968;&#31867;&#27169;&#22411;&#23558;&#20256;&#25773;&#24314;&#27169;&#20026;&#19968;&#31181;&#27874;&#65292;&#23427;&#20174;&#19968;&#32452;&#24050;&#30693;&#33410;&#28857;&#22312;&#21021;&#22987;&#26102;&#38388;&#24320;&#22987;&#21521;&#25152;&#26377;&#20854;&#20182;&#26410;&#30693;&#33410;&#28857;&#20256;&#25773;&#65292;&#20256;&#25773;&#30340;&#39034;&#24207;&#30001;&#20449;&#24687;&#27874;&#21069;&#30340;&#21040;&#36798;&#26102;&#38388;&#30830;&#23450;&#12290;&#31532;&#20108;&#31867;&#27169;&#22411;&#22522;&#20110;&#36335;&#24452;&#19978;&#30340;&#34892;&#31243;&#26102;&#38388;&#30340;&#27010;&#24565;&#12290;&#20174;&#19968;&#32452;&#21021;&#22987;&#24050;&#30693;&#33410;&#28857;&#21040;&#19968;&#20010;&#33410;&#28857;&#30340;&#20449;&#24687;&#20256;&#25773;&#26102;&#38388;&#34987;&#23450;&#20041;&#20026;&#25152;&#26377;&#21487;&#20197;&#21040;&#36798;&#35813;&#33410;&#28857;&#30340;&#36335;&#24452;&#30340;&#23376;&#38598;&#19978;&#30340;&#24191;&#20041;&#26053;&#34892;&#26102;&#38388;&#30340;&#26368;&#23567;&#20540;&#12290;&#26368;&#21518;&#19968;&#20010;&#27169;&#22411;&#31867;&#26159;&#36890;&#36807;&#22312;&#27599;&#20010;&#26410;&#30693;&#33410;&#28857;&#19978;&#26045;&#21152;&#19968;&#20010;eikonal&#24418;&#24335;&#30340;&#23616;&#37096;&#26041;&#31243;&#65292;&#24182;&#22312;&#24050;&#30693;&#33410;&#28857;&#22788;&#26045;&#21152;&#36793;&#30028;&#26465;&#20214;&#26469;&#32473;&#20986;&#30340;&#12290;&#22312;&#19968;&#20010;&#33410;&#28857;&#30340;&#35299;&#30340;&#20540;&#19982;&#20855;&#26377;&#36739;&#20302;&#20540;&#30340;&#30456;&#37051;&#33410;&#28857;&#30340;&#35299;&#30340;&#20540;&#32806;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27169;&#22411;&#31867;&#30340;&#31934;&#30830;&#20844;&#24335;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#12290;&#21463;&#21040;&#31532;&#19968;&#21040;&#36798;&#26102;&#38388;&#27169;&#22411;&#21644;eikonal&#26041;&#31243;&#20043;&#38388;&#30340;&#32852;&#31995;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28151;&#21512;&#24418;&#24335;&#65292;&#32467;&#21512;&#20102;&#27874;&#21644;eikonal&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22270;&#24418;&#19978;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25968;&#20540;&#27169;&#25311;&#65292;&#21253;&#25324;&#38543;&#26426;&#22270;&#24418;&#12289;&#23567;&#19990;&#30028;&#22270;&#21644;&#23454;&#38469;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose and unify classes of different models for information propagation over graphs. In a first class, propagation is modelled as a wave which emanates from a set of known nodes at an initial time, to all other unknown nodes at later times with an ordering determined by the arrival time of the information wave front. A second class of models is based on the notion of a travel time along paths between nodes. The time of information propagation from an initial known set of nodes to a node is defined as the minimum of a generalised travel time over subsets of all admissible paths. A final class is given by imposing a local equation of an eikonal form at each unknown node, with boundary conditions at the known nodes. The solution value of the local equation at a node is coupled to those of neighbouring nodes with lower values. We provide precise formulations of the model classes and prove equivalences between them. Motivated by the connection between first arrival time model and the e
&lt;/p&gt;</description></item></channel></rss>