<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#20195;&#26367;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#12289;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;&#24615;&#33021;&#23849;&#28291;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.12086</link><description>&lt;p&gt;
&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#26159;&#40065;&#26834;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders Are Robust Neural Architecture Search Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.12086
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26080;&#38656;&#26631;&#35760;&#25968;&#25454;&#65292;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#20351;&#29992;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#20195;&#26367;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#12289;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;&#24615;&#33021;&#23849;&#28291;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30446;&#21069;&#20005;&#37325;&#20381;&#36182;&#26631;&#35760;&#25968;&#25454;&#65292;&#32780;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#26082;&#26114;&#36149;&#21448;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#30340;&#26032;&#22411;NAS&#26694;&#26550;&#65292;&#23427;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#28040;&#38500;&#20102;&#23545;&#26631;&#35760;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#23558;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#26367;&#25442;&#20026;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;&#33021;&#22815;&#22312;&#19981;&#25439;&#23475;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#24773;&#20917;&#19979;&#40065;&#26834;&#22320;&#21457;&#29616;&#32593;&#32476;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#22810;&#23610;&#24230;&#35299;&#30721;&#22120;&#35299;&#20915;&#20102;&#22312;&#26080;&#30417;&#30563;&#33539;&#24335;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#21487;&#24494;&#26550;&#26500;&#25628;&#32034;&#65288;DARTS&#65289;&#26041;&#27861;&#36935;&#21040;&#30340;&#24615;&#33021;&#23849;&#28291;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#21508;&#31181;&#25628;&#32034;&#31354;&#38388;&#21644;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#20026;&#20854;&#32988;&#36807;&#22522;&#20934;&#26041;&#27861;&#25552;&#20379;&#20102;&#23454;&#35777;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.12086v2 Announce Type: replace  Abstract: Neural Architecture Search (NAS) currently relies heavily on labeled data, which is both expensive and time-consuming to acquire. In this paper, we propose a novel NAS framework based on Masked Autoencoders (MAE) that eliminates the need for labeled data during the search process. By replacing the supervised learning objective with an image reconstruction task, our approach enables the robust discovery of network architectures without compromising performance and generalization ability. Additionally, we address the problem of performance collapse encountered in the widely-used Differentiable Architecture Search (DARTS) method in the unsupervised paradigm by introducing a multi-scale decoder. Through extensive experiments conducted on various search spaces and datasets, we demonstrate the effectiveness and robustness of the proposed method, providing empirical evidence of its superiority over baseline approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#29486;&#32508;&#36848;&#27604;&#36739;&#20102;&#22312;&#39044;&#27979;&#38750;&#27954;COVID-19&#30149;&#20363;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.03606</link><description>&lt;p&gt;
&#23558;&#29992;&#20110;&#30740;&#31350;&#35770;&#25991;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#19982;&#39044;&#27979;&#38750;&#27954;COVID-19&#30149;&#20363;&#30340;&#27604;&#36739;&#65306;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Comparing Time-Series Analysis Approaches Utilized in Research Papers to Forecast COVID-19 Cases in Africa: A Literature Review. (arXiv:2310.03606v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29486;&#32508;&#36848;&#27604;&#36739;&#20102;&#22312;&#39044;&#27979;&#38750;&#27954;COVID-19&#30149;&#20363;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#65292;&#31361;&#20986;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#29486;&#32508;&#36848;&#26088;&#22312;&#27604;&#36739;&#22312;&#39044;&#27979;&#38750;&#27954;COVID-19&#30149;&#20363;&#20013;&#20351;&#29992;&#30340;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#23545;2020&#24180;1&#26376;&#33267;2023&#24180;7&#26376;&#21457;&#34920;&#30340;&#33521;&#25991;&#30740;&#31350;&#35770;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#25628;&#32034;&#65292;&#37325;&#28857;&#20851;&#27880;&#22312;&#38750;&#27954;COVID-19&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26041;&#27861;&#30340;&#35770;&#25991;&#12290;&#35813;&#36807;&#31243;&#20351;&#29992;&#20102;&#21253;&#25324;PubMed&#12289;&#35895;&#27468;&#23398;&#26415;&#12289;Scopus&#21644;&#31185;&#23398;&#24341;&#25991;&#32034;&#24341;&#31561;&#22810;&#31181;&#25968;&#25454;&#24211;&#12290;&#30740;&#31350;&#35770;&#25991;&#32463;&#36807;&#35780;&#20272;&#36807;&#31243;&#65292;&#25552;&#21462;&#20102;&#20851;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#30340;&#23454;&#26045;&#21644;&#24615;&#33021;&#30340;&#30456;&#20851;&#20449;&#24687;&#12290;&#35813;&#30740;&#31350;&#31361;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#23398;&#65292;&#24182;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#39044;&#27979;&#30149;&#27602;&#20256;&#25773;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#12290;&#26412;&#32508;&#36848;&#30340;&#32467;&#26524;&#21487;&#20197;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#26356;&#28145;&#20837;&#30340;&#35265;&#35299;&#65292;&#26410;&#26469;&#30340;&#30740;&#31350;&#24212;&#32771;&#34385;&#36825;&#20123;&#35265;&#35299;&#65292;&#20197;&#25913;&#36827;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#27169;&#22411;&#24182;&#25506;&#32034;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This literature review aimed to compare various time-series analysis approaches utilized in forecasting COVID-19 cases in Africa. The study involved a methodical search for English-language research papers published between January 2020 and July 2023, focusing specifically on papers that utilized time-series analysis approaches on COVID-19 datasets in Africa. A variety of databases including PubMed, Google Scholar, Scopus, and Web of Science were utilized for this process. The research papers underwent an evaluation process to extract relevant information regarding the implementation and performance of the time-series analysis models. The study highlighted the different methodologies employed, evaluating their effectiveness and limitations in forecasting the spread of the virus. The result of this review could contribute deeper insights into the field, and future research should consider these insights to improve time series analysis models and explore the integration of different appr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#30340;&#21078;&#26512;&#65292;&#25581;&#31034;&#20102;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#38750;&#20856;&#22411;&#25968;&#25454;&#28857;&#19978;&#23454;&#29616;&#30340;&#27867;&#21270;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#20063;&#21457;&#29616;&#20102;&#19982;&#27492;&#31639;&#27861;&#30456;&#20851;&#30340;&#26356;&#39640;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#36798;&#21040;&#26356;&#29702;&#24819;&#30340;&#20934;&#30830;&#24230;&#19982;&#38544;&#31169;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2310.00488</link><description>&lt;p&gt;
&#20851;&#20110;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#30340;&#35760;&#24518;&#21644;&#38544;&#31169;&#39118;&#38505;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Memorization and Privacy Risks of Sharpness Aware Minimization. (arXiv:2310.00488v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#30340;&#21078;&#26512;&#65292;&#25581;&#31034;&#20102;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#31639;&#27861;&#22312;&#38750;&#20856;&#22411;&#25968;&#25454;&#28857;&#19978;&#23454;&#29616;&#30340;&#27867;&#21270;&#25910;&#30410;&#12290;&#21516;&#26102;&#65292;&#20063;&#21457;&#29616;&#20102;&#19982;&#27492;&#31639;&#27861;&#30456;&#20851;&#30340;&#26356;&#39640;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#36798;&#21040;&#26356;&#29702;&#24819;&#30340;&#20934;&#30830;&#24230;&#19982;&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#35774;&#35745;&#23547;&#27714;&#31070;&#32463;&#32593;&#32476;&#25439;&#22833;&#20248;&#21270;&#20013;&#26356;&#24179;&#22374;&#30340;&#26497;&#20540;&#30340;&#31639;&#27861;&#25104;&#20026;&#28966;&#28857;&#65292;&#22240;&#20026;&#26377;&#32463;&#39564;&#35777;&#25454;&#34920;&#26126;&#36825;&#20250;&#22312;&#35768;&#22810;&#25968;&#25454;&#38598;&#19978;&#23548;&#33268;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36807;&#24230;&#21442;&#25968;&#21270;&#27169;&#22411;&#20013;&#30340;&#25968;&#25454;&#35760;&#24518;&#35270;&#35282;&#26469;&#21078;&#26512;&#36825;&#20123;&#24615;&#33021;&#25910;&#30410;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#24110;&#21161;&#25105;&#20204;&#30830;&#23450;&#30456;&#23545;&#20110;&#26222;&#36890;SGD&#65292;&#23547;&#27714;&#26356;&#24179;&#22374;&#26497;&#20540;&#30340;&#31639;&#27861;&#22312;&#21738;&#20123;&#25968;&#25454;&#28857;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23574;&#38160;&#24847;&#35782;&#26368;&#23567;&#21270;&#65288;SAM&#65289;&#25152;&#23454;&#29616;&#30340;&#27867;&#21270;&#25910;&#30410;&#22312;&#38750;&#20856;&#22411;&#25968;&#25454;&#28857;&#19978;&#29305;&#21035;&#26174;&#33879;&#65292;&#36825;&#38656;&#35201;&#35760;&#24518;&#12290;&#36825;&#19968;&#35748;&#35782;&#24110;&#21161;&#25105;&#20204;&#25581;&#31034;&#19982;SAM&#30456;&#20851;&#30340;&#26356;&#39640;&#30340;&#38544;&#31169;&#39118;&#38505;&#65292;&#24182;&#36890;&#36807;&#35814;&#23613;&#30340;&#23454;&#35777;&#35780;&#20272;&#36827;&#34892;&#39564;&#35777;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#32531;&#35299;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#29702;&#24819;&#30340;&#20934;&#30830;&#24230;&#19982;&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many recent works, there is an increased focus on designing algorithms that seek flatter optima for neural network loss optimization as there is empirical evidence that it leads to better generalization performance in many datasets. In this work, we dissect these performance gains through the lens of data memorization in overparameterized models. We define a new metric that helps us identify which data points specifically do algorithms seeking flatter optima do better when compared to vanilla SGD. We find that the generalization gains achieved by Sharpness Aware Minimization (SAM) are particularly pronounced for atypical data points, which necessitate memorization. This insight helps us unearth higher privacy risks associated with SAM, which we verify through exhaustive empirical evaluations. Finally, we propose mitigation strategies to achieve a more desirable accuracy vs privacy tradeoff.
&lt;/p&gt;</description></item></channel></rss>