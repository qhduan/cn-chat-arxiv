<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;</title><link>https://rss.arxiv.org/abs/2312.07950</link><description>&lt;p&gt;
&#36328;&#22359;&#37327;&#21270;&#65306;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37327;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CBQ: Cross-Block Quantization for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2312.07950
&lt;/p&gt;
&lt;p&gt;
CBQ&#26159;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36328;&#22359;&#37325;&#26500;&#22411;&#21518;&#35757;&#32451;&#37327;&#21270;&#26041;&#27861;&#12290;CBQ&#36890;&#36807;&#20351;&#29992;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#24314;&#31435;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;CBQ&#36824;&#37319;&#29992;&#20102;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#21644;&#33258;&#36866;&#24212;&#30340;&#21462;&#25972;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#24182;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#22312;&#20197;&#26497;&#20302;&#25104;&#26412;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PTQ&#26041;&#27861;&#21482;&#20851;&#27880;&#22788;&#29702;&#21333;&#20010;&#23618;&#25110;&#21333;&#20010;&#22359;&#20869;&#30340;&#24322;&#24120;&#20540;&#65292;&#24573;&#30053;&#20102;&#22359;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#20302;&#20301;&#35774;&#32622;&#20013;&#23548;&#33268;&#20005;&#37325;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22359;&#38388;&#37325;&#26500;&#30340;&#36328;&#22359;PTQ&#26041;&#27861;CBQ&#12290;CBQ&#37319;&#29992;&#20102;&#19968;&#31181;&#21516;&#28304;&#37325;&#26500;&#26041;&#26696;&#26469;&#23454;&#29616;&#22359;&#38388;&#30340;&#38271;&#31243;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#26368;&#23567;&#21270;&#35823;&#24046;&#31215;&#32047;&#12290;&#27492;&#22806;&#65292;CBQ&#36824;&#32467;&#21512;&#20102;&#19968;&#31181;&#31895;&#21040;&#31934;&#30340;&#39044;&#22788;&#29702;&#31574;&#30053;&#65288;CFP&#65289;&#26469;&#25233;&#21046;&#26435;&#37325;&#21644;&#28608;&#27963;&#20540;&#30340;&#24322;&#24120;&#20540;&#65292;&#24182;&#37197;&#21512;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;LoRA&#21462;&#25972;&#25216;&#26415;&#23454;&#29616;&#31934;&#30830;&#30340;&#26435;&#37325;&#37327;&#21270;&#12290;&#36825;&#20123;&#21019;&#26032;&#20351;CBQ&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26497;&#31471;&#24322;&#24120;&#20540;&#65292;&#36824;&#33021;&#25552;&#39640;&#25972;&#20307;&#37327;&#21270;&#31934;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;CBQ&#22312;&#20302;&#20301;&#37327;&#21270;&#65288;W4A4&#65292;W4A8&#31561;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) has played a key role in compressing large language models (LLMs) with ultra-low costs. However, existing PTQ methods only focus on handling the outliers within one layer or one block, which ignores the dependency of blocks and leads to severe performance degradation in low-bit settings. In this paper, we propose CBQ, a cross-block reconstruction-based PTQ method for LLMs. CBQ employs a cross-block dependency using a homologous reconstruction scheme, establishing long-range dependencies across multiple blocks to minimize error accumulation. Furthermore, CBQ incorporates a coarse-to-fine preprocessing (CFP) strategy for suppressing weight and activation outliers, coupled with an adaptive LoRA-Rounding technique for precise weight quantization. These innovations enable CBQ to not only handle extreme outliers effectively but also improve overall quantization accuracy. Extensive experiments show that CBQ achieves superior low-bit quantization (W4A4, W4A8, W
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.18998</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#36328;&#31995;&#32479;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#65288;MSS&#65289;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29305;&#24615;&#21487;&#33021;&#22312;&#21508;&#31181;&#25925;&#38556;&#31867;&#21035;&#20013;&#20986;&#29616;&#25925;&#38556;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;&#25925;&#38556;&#65292;AIOps&#24037;&#20855;&#21033;&#29992;&#22522;&#20110;&#36319;&#36394;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#65288;2&#65289;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MSS&#65292;Trainticket&#21644;OnlineBoutique&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35843;&#25972;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20197;&#23545;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#26032;&#39062;&#25925;&#38556;&#31867;&#21035;&#30340;&#24322;&#24120;&#36319;&#36394;&#36827;&#34892;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#26368;&#21021;&#35757;&#32451;&#30340;&#21516;&#19968;&#31995;&#32479;&#20869;&#65292;&#36824;&#26159;&#22312;&#20854;&#20182;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
&lt;/p&gt;</description></item><item><title>ELLEN&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#19982;&#35821;&#35328;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#22312;&#26497;&#20854;&#36731;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#38750;&#24120;&#24378;&#21170;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.17385</link><description>&lt;p&gt;
ELLEN: &#38750;&#24120;&#36731;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#39640;&#25928;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ELLEN: Extremely Lightly Supervised Learning For Efficient Named Entity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17385
&lt;/p&gt;
&lt;p&gt;
ELLEN&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23558;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#19982;&#35821;&#35328;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#22312;&#26497;&#20854;&#36731;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#20102;&#38750;&#24120;&#24378;&#21170;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21322;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#38382;&#39064;&#65292;&#20391;&#37325;&#20110;&#26497;&#20854;&#36731;&#37327;&#32423;&#30340;&#30417;&#30563;&#65292;&#21253;&#25324;&#20165;&#21253;&#21547;&#27599;&#31867;&#21035;10&#20010;&#31034;&#20363;&#30340;&#35789;&#27719;&#34920;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ELLEN&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#12289;&#23436;&#20840;&#27169;&#22359;&#21270;&#30340;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#23427;&#23558;&#32463;&#36807;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#19982;&#35821;&#35328;&#35268;&#21017;&#30456;&#32467;&#21512;&#12290;&#36825;&#20123;&#35268;&#21017;&#21253;&#25324;&#8220;&#19968;&#20010;&#35805;&#35821;&#19968;&#20010;&#24847;&#20041;&#8221;&#36825;&#26679;&#30340;&#35265;&#35299;&#65292;&#20351;&#29992;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#26080;&#30417;&#30563;NER&#65292;&#21033;&#29992;&#35789;&#24615;&#26631;&#31614;&#35782;&#21035;&#21644;&#28040;&#38500;&#26410;&#26631;&#35760;&#23454;&#20307;&#20316;&#20026;&#20551;&#36127;&#20363;&#65292;&#20197;&#21450;&#20851;&#20110;&#20998;&#31867;&#22120;&#32622;&#20449;&#24230;&#24471;&#20998;&#22312;&#23616;&#37096;&#21644;&#20840;&#23616;&#32972;&#26223;&#19979;&#30340;&#20854;&#20182;&#30452;&#35273;&#12290;&#22312;&#20351;&#29992;&#19978;&#36848;&#35789;&#27719;&#34920;&#26497;&#23567;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;ELLEN&#22312;CoNLL-2003&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#23427;&#36824;&#22312;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#30456;&#21516;&#30417;&#30563;&#35774;&#32622;&#65288;&#21363;&#65292;&#35757;&#32451;&#25968;&#25454;&#30340;5%&#65289;&#19979;&#65292;&#20248;&#20110;&#22823;&#22810;&#25968;&#29616;&#26377;&#65288;&#19988;&#26356;&#20026;&#22797;&#26434;&#65289;&#30340;&#21322;&#30417;&#30563;NER&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17385v1 Announce Type: cross  Abstract: In this work, we revisit the problem of semi-supervised named entity recognition (NER) focusing on extremely light supervision, consisting of a lexicon containing only 10 examples per class. We introduce ELLEN, a simple, fully modular, neuro-symbolic method that blends fine-tuned language models with linguistic rules. These rules include insights such as ''One Sense Per Discourse'', using a Masked Language Model as an unsupervised NER, leveraging part-of-speech tags to identify and eliminate unlabeled entities as false negatives, and other intuitions about classifier confidence scores in local and global context. ELLEN achieves very strong performance on the CoNLL-2003 dataset when using the minimal supervision from the lexicon above. It also outperforms most existing (and considerably more complex) semi-supervised NER methods under the same supervision settings commonly used in the literature (i.e., 5% of the training data). Further, 
&lt;/p&gt;</description></item><item><title>ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.16354</link><description>&lt;p&gt;
ChatDBG: &#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
ChatDBG: An AI-Powered Debugging Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16354
&lt;/p&gt;
&lt;p&gt;
ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ChatDBG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;&#12290;ChatDBG&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20256;&#32479;&#35843;&#35797;&#22120;&#30340;&#21151;&#33021;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;ChatDBG&#20801;&#35768;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#36827;&#34892;&#21327;&#20316;&#23545;&#35805;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#25552;&#20986;&#20851;&#20110;&#31243;&#24207;&#29366;&#24577;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#23545;&#23849;&#28291;&#25110;&#26029;&#35328;&#22833;&#36133;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#35832;&#22914;&#8220;&#20026;&#20160;&#20040;x&#20026;&#31354;&#65311;&#8221;&#20043;&#31867;&#30340;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;ChatDBG&#25480;&#20104;LLM&#33258;&#20027;&#26435;&#65292;&#36890;&#36807;&#21457;&#20986;&#21629;&#20196;&#26469;&#27983;&#35272;&#22534;&#26632;&#21644;&#26816;&#26597;&#31243;&#24207;&#29366;&#24577;&#36827;&#34892;&#35843;&#35797;&#65307;&#28982;&#21518;&#25253;&#21578;&#20854;&#21457;&#29616;&#24182;&#23558;&#25511;&#21046;&#26435;&#20132;&#36824;&#32473;&#31243;&#24207;&#21592;&#12290;&#25105;&#20204;&#30340;ChatDBG&#21407;&#22411;&#19982;&#26631;&#20934;&#35843;&#35797;&#22120;&#38598;&#25104;&#65292;&#21253;&#25324;LLDB&#12289;GDB&#21644;WinDBG&#29992;&#20110;&#26412;&#22320;&#20195;&#30721;&#20197;&#21450;&#29992;&#20110;Python&#30340;Pdb&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20195;&#30721;&#38598;&#21512;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20855;&#26377;&#24050;&#30693;&#38169;&#35823;&#30340;C/C++&#20195;&#30721;&#21644;&#19968;&#22871;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16354v1 Announce Type: cross  Abstract: This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code includi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#26032;&#27169;&#25311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;RS3L&#65292;&#36890;&#36807;&#20171;&#20837;&#27169;&#25311;&#36807;&#31243;&#24182;&#37325;&#26032;&#27169;&#25311;&#20107;&#20214;&#23454;&#29616;&#65292;&#29983;&#25104;&#19968;&#32452;&#28085;&#30422;&#25152;&#26377;&#29289;&#29702;&#39537;&#21160;&#21464;&#21270;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#20419;&#36827;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;R3SL&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07066</link><description>&lt;p&gt;
&#22522;&#20110;&#37325;&#26032;&#27169;&#25311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Re-Simulation-based Self-Supervised Learning for Pre-Training Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07066
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#37325;&#26032;&#27169;&#25311;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;RS3L&#65292;&#36890;&#36807;&#20171;&#20837;&#27169;&#25311;&#36807;&#31243;&#24182;&#37325;&#26032;&#27169;&#25311;&#20107;&#20214;&#23454;&#29616;&#65292;&#29983;&#25104;&#19968;&#32452;&#28085;&#30422;&#25152;&#26377;&#29289;&#29702;&#39537;&#21160;&#21464;&#21270;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#20174;&#32780;&#20419;&#36827;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#24182;&#23637;&#31034;&#20102;&#39044;&#35757;&#32451;R3SL&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26159;&#35757;&#32451;&#29616;&#20195;&#22823;&#22411;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26680;&#24515;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#23398;&#20064;&#24378;&#22823;&#34920;&#31034;&#30340;&#26041;&#26696;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;SSL&#31574;&#30053;&#24517;&#39035;&#36866;&#24212;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#31867;&#22411;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RS3L&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;SSL&#31574;&#30053;&#65292;&#37319;&#29992;&#37325;&#26032;&#27169;&#25311;&#30340;&#26041;&#27861;&#26469;&#39537;&#21160;&#23545;&#27604;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#20171;&#20837;&#27169;&#25311;&#36807;&#31243;&#30340;&#20013;&#38388;&#24182;&#37325;&#26032;&#36816;&#34892;&#20171;&#20837;&#20043;&#21518;&#30340;&#27169;&#25311;&#32452;&#20214;&#65292;&#25105;&#20204;&#29983;&#25104;&#19968;&#20010;&#20107;&#20214;&#30340;&#22810;&#20010;&#23454;&#29616;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#32452;&#28085;&#30422;&#27169;&#25311;&#22120;&#20013;&#25152;&#26377;&#29289;&#29702;&#39537;&#21160;&#21464;&#21270;&#30340;&#22686;&#24378;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#33021;&#29289;&#29702;&#23454;&#39564;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36825;&#31181;&#31574;&#30053;&#22914;&#20309;&#20419;&#36827;&#22522;&#30784;&#27169;&#22411;&#30340;&#21457;&#23637;&#65307;&#25105;&#20204;&#23637;&#31034;&#20102;R3SL&#39044;&#35757;&#32451;&#22914;&#20309;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21306;&#20998;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07066v1 Announce Type: cross  Abstract: Self-Supervised Learning (SSL) is at the core of training modern large machine learning models, providing a scheme for learning powerful representations that can be used in a variety of downstream tasks. However, SSL strategies must be adapted to the type of training data and downstream tasks required. We propose RS3L, a novel simulation-based SSL strategy that employs a method of re-simulation to drive data augmentation for contrastive learning. By intervening in the middle of the simulation process and re-running simulation components downstream of the intervention, we generate multiple realizations of an event, thus producing a set of augmentations covering all physics-driven variations available in the simulator. Using experiments from high-energy physics, we explore how this strategy may enable the development of a foundation model; we show how R3SL pre-training enables powerful performance in downstream tasks such as discriminati
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;"Signature Isolation Forest"&#65292;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#30340;&#31614;&#21517;&#21464;&#25442;&#21435;&#38500;&#20102;Functional Isolation Forest&#30340;&#32447;&#24615;&#20869;&#31215;&#21644;&#35789;&#20856;&#36873;&#25321;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.04405</link><description>&lt;p&gt;
Signature Isolation Forest
&lt;/p&gt;
&lt;p&gt;
Signature Isolation Forest
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04405
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;"Signature Isolation Forest"&#65292;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#30340;&#31614;&#21517;&#21464;&#25442;&#21435;&#38500;&#20102;Functional Isolation Forest&#30340;&#32447;&#24615;&#20869;&#31215;&#21644;&#35789;&#20856;&#36873;&#25321;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Functional Isolation Forest (FIF)&#26159;&#19968;&#31181;&#38024;&#23545;&#21151;&#33021;&#25968;&#25454;&#35774;&#35745;&#30340;&#26368;&#26032;&#19968;&#27969;&#24322;&#24120;&#26816;&#27979;(AD)&#31639;&#27861;&#12290;&#23427;&#20381;&#36182;&#20110;&#19968;&#31181;&#26641;&#20998;&#21306;&#36807;&#31243;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#26354;&#32447;&#35266;&#27979;&#25237;&#24433;&#21040;&#36890;&#36807;&#32447;&#24615;&#20869;&#31215;&#32472;&#21046;&#30340;&#35789;&#20856;&#19978;&#26469;&#35745;&#31639;&#24322;&#24120;&#24471;&#20998;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#8220;Signature Isolation Forest&#8221;&#65292;&#19968;&#31181;&#21033;&#29992;&#31895;&#36335;&#24452;&#29702;&#35770;&#31614;&#21517;&#21464;&#25442;&#30340;&#26032;&#39062;AD&#31639;&#27861;&#31867;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#25552;&#20986;&#20004;&#31181;&#31639;&#27861;&#26469;&#28040;&#38500;FIF&#26045;&#21152;&#30340;&#38480;&#21046;&#65292;&#36825;&#20004;&#31181;&#31639;&#27861;&#29305;&#21035;&#38024;&#23545;FIF&#20869;&#31215;&#30340;&#32447;&#24615;&#24615;&#21644;&#35789;&#20856;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04405v1 Announce Type: cross  Abstract: Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark sho
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20248;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#36870;&#36716;&#26410;&#30693;&#37327;&#23376;&#27604;&#29305;&#37193;&#25805;&#20316;&#65292;&#23558;&#36741;&#21161;&#27604;&#29305;&#24320;&#38144;&#20943;&#23569;&#21040;3&#65292;&#26174;&#31034;&#20102;&#37327;&#23376;&#26803;&#32467;&#26500;&#30340;&#23454;&#29992;&#24615;&#21644;PQComb&#22312;&#35299;&#20915;&#22797;&#26434;&#37327;&#23376;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.03761</link><description>&lt;p&gt;
&#21442;&#25968;&#21270;&#37327;&#23376;&#26803;&#21644;&#31616;&#21270;&#30005;&#36335;&#29992;&#20110;&#36870;&#36716;&#26410;&#30693;&#37327;&#23376;&#27604;&#29305;-&#37193;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Parameterized quantum comb and simpler circuits for reversing unknown qubit-unitary operations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03761
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20248;&#21270;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31616;&#21270;&#30340;&#21327;&#35758;&#65292;&#29992;&#20110;&#36870;&#36716;&#26410;&#30693;&#37327;&#23376;&#27604;&#29305;&#37193;&#25805;&#20316;&#65292;&#23558;&#36741;&#21161;&#27604;&#29305;&#24320;&#38144;&#20943;&#23569;&#21040;3&#65292;&#26174;&#31034;&#20102;&#37327;&#23376;&#26803;&#32467;&#26500;&#30340;&#23454;&#29992;&#24615;&#21644;PQComb&#22312;&#35299;&#20915;&#22797;&#26434;&#37327;&#23376;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Quantum comb&#26159;&#37327;&#23376;&#20449;&#24687;&#22788;&#29702;&#20013;&#34920;&#24449;&#22797;&#26434;&#37327;&#23376;&#21327;&#35758;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PQComb&#65292;&#19968;&#20010;&#21033;&#29992;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#25506;&#32034;&#37327;&#23376;&#26803;&#22312;&#19968;&#33324;&#37327;&#23376;&#36807;&#31243;&#36716;&#25442;&#20219;&#21153;&#21450;&#20854;&#20182;&#26041;&#38754;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;&#36890;&#36807;&#20248;&#21270;PQComb&#36827;&#34892;&#26410;&#30693;&#37193;&#28436;&#21270;&#30340;&#26102;&#38388;&#21453;&#28446;&#27169;&#25311;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26410;&#30693;&#37327;&#23376;&#27604;&#29305;&#37193;&#21453;&#28436;&#21327;&#35758;&#65292;&#23558;&#27604;&#29616;&#26377;&#26041;&#27861;[Yoshida, Soeda, Murao, PRL 131, 120602, 2023]&#30340;&#36741;&#21161;&#27604;&#29305;&#24320;&#38144;&#20174;6&#20943;&#23569;&#21040;3&#12290;&#36825;&#23637;&#31034;&#20102;&#37327;&#23376;&#26803;&#32467;&#26500;&#30340;&#23454;&#29992;&#24615;&#65292;&#23637;&#31034;&#20102;PQComb&#22312;&#35299;&#20915;&#22797;&#26434;&#37327;&#23376;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20026;PQComb&#22312;&#37327;&#23376;&#35745;&#31639;&#21644;&#37327;&#23376;&#20449;&#24687;&#20013;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24378;&#35843;&#20102;&#23427;&#22312;&#35299;&#20915;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#22810;&#26679;&#38382;&#39064;&#26102;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03761v1 Announce Type: cross  Abstract: Quantum comb is an essential tool for characterizing complex quantum protocols in quantum information processing. In this work, we introduce PQComb, a framework leveraging parameterized quantum circuits to explore the capabilities of quantum combs for general quantum process transformation tasks and beyond. By optimizing PQComb for time-reversal simulations of unknown unitary evolutions, we develop a simpler protocol for unknown qubit unitary inversion that reduces the ancilla qubit overhead from 6 to 3 compared to the existing method in [Yoshida, Soeda, Murao, PRL 131, 120602, 2023]. This demonstrates the utility of quantum comb structures and showcases PQComb's potential for solving complex quantum tasks. Our results pave the way for broader PQComb applications in quantum computing and quantum information, emphasizing its versatility for tackling diverse problems in quantum machine learning.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#24863;&#30693;Boltzmann&#20272;&#35745;&#22120;&#65288;StABlE&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#30417;&#30563;&#35757;&#32451;&#21644;&#21442;&#32771;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#65292;&#29992;&#20110;&#29983;&#25104;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#12290;</title><link>https://arxiv.org/abs/2402.13984</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#24494;Boltzmann&#20272;&#35745;&#22120;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#30340;&#31283;&#23450;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Stability-Aware Training of Neural Network Interatomic Potentials with Differentiable Boltzmann Estimators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13984
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#24863;&#30693;Boltzmann&#20272;&#35745;&#22120;&#65288;StABlE&#65289;&#35757;&#32451;&#26041;&#27861;&#65292;&#32467;&#21512;&#20256;&#32479;&#30417;&#30563;&#35757;&#32451;&#21644;&#21442;&#32771;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#65292;&#29992;&#20110;&#29983;&#25104;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21407;&#23376;&#38388;&#21183;&#65288;NNIPs&#65289;&#26159;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#20013;&#30340;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#19981;&#31283;&#23450;&#30340;&#27169;&#25311;&#65292;&#37319;&#26679;&#38750;&#29289;&#29702;&#29366;&#24577;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#22312;&#23545;&#27169;&#25311;&#38271;&#26102;&#38388;&#23610;&#24230;&#29616;&#35937;&#24314;&#27169;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31283;&#23450;&#24615;&#24863;&#30693;Boltzmann&#20272;&#35745;&#22120;&#65288;StABlE&#65289;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24335;&#35757;&#32451;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#20256;&#32479;&#30417;&#30563;&#35757;&#32451;&#21644;&#21442;&#32771;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#65292;&#20197;&#20135;&#29983;&#31283;&#23450;&#19988;&#20934;&#30830;&#30340;NNIPs&#12290;StABlE&#35757;&#32451;&#36890;&#36807;&#36845;&#20195;&#36816;&#34892;MD&#27169;&#25311;&#20197;&#23547;&#25214;&#19981;&#31283;&#23450;&#21306;&#22495;&#65292;&#24182;&#36890;&#36807;&#19982;&#21442;&#32771;&#21487;&#35266;&#23519;&#37327;&#30340;&#30417;&#30563;&#26469;&#32416;&#27491;&#36825;&#20123;&#19981;&#31283;&#23450;&#24615;&#12290;&#35813;&#35757;&#32451;&#36807;&#31243;&#30001;Boltzmann&#20272;&#35745;&#22120;&#25903;&#25345;&#65292;&#35813;&#20272;&#35745;&#22120;&#20801;&#35768;&#23545;&#31995;&#32479;&#21487;&#35266;&#23519;&#37327;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#25152;&#38656;&#30340;&#26799;&#24230;&#36827;&#34892;&#39640;&#25928;&#35745;&#31639;&#65292;&#24182;&#33021;&#26816;&#27979;&#20840;&#23616;&#21644;&#23616;&#37096;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13984v1 Announce Type: new  Abstract: Neural network interatomic potentials (NNIPs) are an attractive alternative to ab-initio methods for molecular dynamics (MD) simulations. However, they can produce unstable simulations which sample unphysical states, limiting their usefulness for modeling phenomena occurring over longer timescales. To address these challenges, we present Stability-Aware Boltzmann Estimator (StABlE) Training, a multi-modal training procedure which combines conventional supervised training from quantum-mechanical energies and forces with reference system observables, to produce stable and accurate NNIPs. StABlE Training iteratively runs MD simulations to seek out unstable regions, and corrects the instabilities via supervision with a reference observable. The training procedure is enabled by the Boltzmann Estimator, which allows efficient computation of gradients required to train neural networks to system observables, and can detect both global and local 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#21644;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#22270;&#25193;&#20805;&#25216;&#26415;&#20272;&#35745;&#26041;&#24046;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#39033;&#20943;&#36731;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#24179;&#34913;&#22330;&#26223;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#35299;&#20915;GNN&#20013;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2310.18765</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22522;&#20110;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30340;&#21322;&#30417;&#30563;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Semi-Supervised Imbalanced Node Classification from Bias-Variance Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18765
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#23558;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#21644;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#22270;&#25193;&#20805;&#25216;&#26415;&#20272;&#35745;&#26041;&#24046;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#39033;&#20943;&#36731;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#24179;&#34913;&#22330;&#26223;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#24182;&#20026;&#35299;&#20915;GNN&#20013;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20013;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#21644;&#20559;&#24046;-&#26041;&#24046;&#20998;&#35299;&#30456;&#32467;&#21512;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#23558;&#25968;&#25454;&#19981;&#24179;&#34913;&#19982;&#27169;&#22411;&#26041;&#24046;&#23494;&#20999;&#30456;&#20851;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#21033;&#29992;&#22270;&#25193;&#20805;&#25216;&#26415;&#26469;&#20272;&#35745;&#26041;&#24046;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#27491;&#21017;&#39033;&#26469;&#20943;&#36731;&#19981;&#24179;&#34913;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#27979;&#35797;&#65292;&#21253;&#25324;&#33258;&#28982;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#21644;&#20844;&#24320;&#21010;&#20998;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#19981;&#24179;&#34913;&#22330;&#26223;&#20013;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#35813;&#24037;&#20316;&#20026;&#35299;&#20915;GNN&#20013;&#30340;&#19981;&#24179;&#34913;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new approach to address the issue of class imbalance in graph neural networks (GNNs) for learning on graph-structured data. Our approach integrates imbalanced node classification and Bias-Variance Decomposition, establishing a theoretical framework that closely relates data imbalance to model variance. We also leverage graph augmentation technique to estimate the variance, and design a regularization term to alleviate the impact of imbalance. Exhaustive tests are conducted on multiple benchmarks, including naturally imbalanced datasets and public-split class-imbalanced datasets, demonstrating that our approach outperforms state-of-the-art methods in various imbalanced scenarios. This work provides a novel theoretical perspective for addressing the problem of imbalanced node classification in GNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#20855;&#26377;&#27969;&#24418;&#20540;&#29305;&#24449;&#30340;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#12290;&#36825;&#20123;&#23618;&#20855;&#26377;&#23545;&#33410;&#28857;&#25490;&#21015;&#21644;&#29305;&#24449;&#27969;&#24418;&#30340;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#30410;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2401.14381</link><description>&lt;p&gt;
&#38754;&#21521;&#27969;&#24418;&#20540;&#22270;&#30340;&#25193;&#25955;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65306;&#22810;&#37325;&#38590;&#39064;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;
&lt;/p&gt;
&lt;p&gt;
Manifold GCN: Diffusion-based Convolutional Neural Network for Manifold-valued Graphs. (arXiv:2401.14381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20004;&#20010;&#29992;&#20110;&#20855;&#26377;&#27969;&#24418;&#20540;&#29305;&#24449;&#30340;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#12290;&#36825;&#20123;&#23618;&#20855;&#26377;&#23545;&#33410;&#28857;&#25490;&#21015;&#21644;&#29305;&#24449;&#27969;&#24418;&#30340;&#31561;&#21464;&#24615;&#65292;&#24182;&#22312;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#30410;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20855;&#26377;Riemannian&#27969;&#24418;&#29305;&#24449;&#30340;&#22270;&#19978;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#12290;&#31532;&#19968;&#65292;&#22522;&#20110;&#27969;&#24418;&#20540;&#22270;&#30340;&#25193;&#25955;&#26041;&#31243;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#25193;&#25955;&#23618;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#24847;&#25968;&#37327;&#30340;&#33410;&#28857;&#21644;&#22270;&#36830;&#25509;&#27169;&#24335;&#12290;&#31532;&#20108;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#21521;&#37327;&#31070;&#32463;&#20803;&#26694;&#26550;&#30340;&#24605;&#24819;&#36716;&#21270;&#21040;&#25105;&#20204;&#30340;&#19968;&#33324;&#35774;&#32622;&#20013;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20999;&#32447;&#22810;&#23618;&#24863;&#30693;&#22120;&#12290;&#36825;&#20004;&#20010;&#23618;&#23545;&#33410;&#28857;&#25490;&#21015;&#21644;&#29305;&#24449;&#27969;&#24418;&#30340;&#31561;&#21464;&#20855;&#26377;&#21709;&#24212;&#65292;&#36825;&#20123;&#29305;&#24615;&#22312;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#24050;&#34987;&#35777;&#26126;&#20855;&#26377;&#26377;&#30410;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#20197;&#21450;&#22312;&#21491;&#20391;&#28023;&#39532;&#19977;&#35282;&#32593;&#26684;&#19978;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#30340;&#25968;&#20540;&#23454;&#20363;&#34920;&#26126;&#25105;&#20204;&#24314;&#31435;&#30340;&#23618;&#20855;&#26377;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose two graph neural network layers for graphs with features in a Riemannian manifold. First, based on a manifold-valued graph diffusion equation, we construct a diffusion layer that can be applied to an arbitrary number of nodes and graph connectivity patterns. Second, we model a tangent multilayer perceptron by transferring ideas from the vector neuron framework to our general setting. Both layers are equivariant with respect to node permutations and isometries of the feature manifold. These properties have been shown to lead to a beneficial inductive bias in many deep learning tasks. Numerical examples on synthetic data as well as on triangle meshes of the right hippocampus to classify Alzheimer's disease demonstrate the very good performance of our layers.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#21152;&#26435;&#30340;&#31639;&#27861;&#65292;&#22312;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#21487;&#20197;&#36755;&#20986;&#25509;&#36817;&#26368;&#20248;&#30340;&#21152;&#26435;&#65292;&#19988;&#31639;&#27861;&#31616;&#21333;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#24847;&#22320;&#24341;&#20837;&#20998;&#24067;&#20559;&#31227;&#36827;&#34892;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#20316;&#20026;&#24212;&#29992;&#23454;&#20363;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#23545;&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#30340;MAP&#28608;&#37238;&#20855;&#26377;&#38750;&#32467;&#21512;&#24615;&#30340;&#23567;&#20998;&#23376;&#32467;&#21512;&#29289;&#12290;</title><link>http://arxiv.org/abs/2401.11562</link><description>&lt;p&gt;
&#20351;&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#21152;&#26435;&#20197;&#22686;&#24378;&#36873;&#25321;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing selectivity using Wasserstein distance based reweighing. (arXiv:2401.11562v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11562
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20351;&#29992;Wasserstein&#36317;&#31163;&#36827;&#34892;&#21152;&#26435;&#30340;&#31639;&#27861;&#65292;&#22312;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#22312;&#20854;&#20182;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#21487;&#20197;&#36755;&#20986;&#25509;&#36817;&#26368;&#20248;&#30340;&#21152;&#26435;&#65292;&#19988;&#31639;&#27861;&#31616;&#21333;&#21487;&#25193;&#23637;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#24847;&#22320;&#24341;&#20837;&#20998;&#24067;&#20559;&#31227;&#36827;&#34892;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#20316;&#20026;&#24212;&#29992;&#23454;&#20363;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#23545;&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#30340;MAP&#28608;&#37238;&#20855;&#26377;&#38750;&#32467;&#21512;&#24615;&#30340;&#23567;&#20998;&#23376;&#32467;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32473;&#23450;&#20004;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;&#119982;&#21644;&#119983;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#36138;&#23146;&#31639;&#27861;&#26469;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#21152;&#26435;&#65292;&#20351;&#24471;&#22312;&#119982;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#30340;&#26497;&#38480;&#20998;&#24067;&#36924;&#36817;&#22312;&#119983;&#19978;&#35757;&#32451;&#24471;&#21040;&#30340;&#26497;&#38480;&#20998;&#24067;&#12290;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#36755;&#20837;&#25968;&#25454;&#38598;&#30340;&#24230;&#37327;&#29109;&#26377;&#30028;&#26102;&#65292;&#25105;&#20204;&#30340;&#36138;&#23146;&#31639;&#27861;&#36755;&#20986;&#25509;&#36817;&#26368;&#20248;&#30340;&#21152;&#26435;&#65292;&#21363;&#32593;&#32476;&#26435;&#37325;&#30340;&#20004;&#20010;&#19981;&#21464;&#20998;&#24067;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#19978;&#21487;&#20197;&#35777;&#26126;&#25509;&#36817;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#31616;&#21333;&#21487;&#25193;&#23637;&#65292;&#24182;&#19988;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#25928;&#29575;&#19978;&#30028;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#26377;&#24847;&#22320;&#24341;&#20837;&#20998;&#24067;&#20559;&#31227;&#20197;&#36827;&#34892;&#65288;&#36719;&#65289;&#22810;&#30446;&#26631;&#20248;&#21270;&#12290;&#20316;&#20026;&#19968;&#20010;&#21160;&#26426;&#24212;&#29992;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#26469;&#35782;&#21035;&#23545;MNK2&#65288;&#19968;&#31181;&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#30340;MAP&#28608;&#37238;&#65289;&#20855;&#26377;&#38750;&#32467;&#21512;&#24615;&#30340;&#23567;&#20998;&#23376;&#32467;&#21512;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given two labeled data-sets $\mathcal{S}$ and $\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\mathcal{T}$.  On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.  Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1
&lt;/p&gt;</description></item><item><title>D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.06150</link><description>&lt;p&gt;
D-STGCNT:&#19968;&#31181;&#22522;&#20110;transformer&#30340;&#23494;&#38598;&#26102;&#31354;&#22270;&#21367;&#31215;GRU&#32593;&#32476;&#29992;&#20110;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;
&lt;/p&gt;
&lt;p&gt;
D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation. (arXiv:2401.06150v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06150
&lt;/p&gt;
&lt;p&gt;
D-STGCNT&#26159;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;STGCN&#21644;transformer&#30340;&#26550;&#26500;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#24739;&#32773;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#12290;&#23427;&#36890;&#36807;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#20851;&#38190;&#20851;&#33410;&#65292;&#22312;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#26469;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#65292;&#26377;&#25928;&#24314;&#31435;&#26102;&#31354;&#21160;&#24577;&#27169;&#22411;&#12290;transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#23545;&#20110;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#33258;&#21160;&#35780;&#20272;&#26080;&#20020;&#24202;&#30417;&#30563;&#24773;&#20917;&#19979;&#24739;&#32773;&#36827;&#34892;&#36523;&#20307;&#24247;&#22797;&#38203;&#28860;&#30340;&#25361;&#25112;&#12290;&#20854;&#30446;&#26631;&#26159;&#25552;&#20379;&#36136;&#37327;&#35780;&#20998;&#20197;&#30830;&#20445;&#27491;&#30830;&#25191;&#34892;&#21644;&#33719;&#24471;&#26399;&#26395;&#32467;&#26524;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#22270;&#32467;&#26500;&#30340;&#27169;&#22411;&#65292;Dense Spatio-Temporal Graph Conv-GRU Network with Transformer&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#25913;&#36827;&#30340;STGCN&#21644;transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#39640;&#25928;&#22788;&#29702;&#26102;&#31354;&#25968;&#25454;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#39592;&#26550;&#25968;&#25454;&#35270;&#20026;&#22270;&#24418;&#65292;&#24182;&#26816;&#27979;&#27599;&#20010;&#24247;&#22797;&#38203;&#28860;&#20013;&#36215;&#20027;&#35201;&#20316;&#29992;&#30340;&#20851;&#33410;&#12290;&#23494;&#38598;&#36830;&#25509;&#21644;GRU&#26426;&#21046;&#29992;&#20110;&#24555;&#36895;&#22788;&#29702;&#22823;&#22411;3D&#39592;&#26550;&#36755;&#20837;&#24182;&#26377;&#25928;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#12290;transformer&#32534;&#30721;&#22120;&#30340;&#27880;&#24847;&#26426;&#21046;&#20391;&#37325;&#20110;&#36755;&#20837;&#24207;&#21015;&#30340;&#30456;&#20851;&#37096;&#20998;&#65292;&#20351;&#20854;&#22312;&#35780;&#20272;&#24247;&#22797;&#38203;&#28860;&#26041;&#38754;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the challenge of automatically assessing physical rehabilitation exercises for patients who perform the exercises without clinician supervision. The objective is to provide a quality score to ensure correct performance and achieve desired results. To achieve this goal, a new graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with Transformer, is introduced. This model combines a modified version of STGCN and transformer architectures for efficient handling of spatio-temporal data. The key idea is to consider skeleton data respecting its non-linear structure as a graph and detecting joints playing the main role in each rehabilitation exercise. Dense connections and GRU mechanisms are used to rapidly process large 3D skeleton inputs and effectively model temporal dynamics. The transformer encoder's attention mechanism focuses on relevant parts of the input sequence, making it useful for evaluating rehabilitation exercises. The evaluation of our propose
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#65292;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#20914;&#20987;&#22368;&#33853;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.05407</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#29305;&#24449;&#25490;&#24207;&#22312;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#30340;&#20914;&#20987;&#22368;&#33853;&#26816;&#27979;&#20107;&#20214;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Feature Ranking for Impact Fall Detection Event Using Multisensor Data. (arXiv:2401.05407v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#36827;&#34892;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#65292;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23454;&#29616;&#20102;&#20914;&#20987;&#22368;&#33853;&#26816;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#20154;&#30340;&#36300;&#20498;&#65292;&#29305;&#21035;&#26159;&#32769;&#24180;&#20154;&#65292;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#30340;&#20260;&#23475;&#21644;&#24182;&#21457;&#30151;&#12290;&#22312;&#36300;&#20498;&#20107;&#20214;&#20013;&#26816;&#27979;&#20914;&#20987;&#30636;&#38388;&#23545;&#20110;&#21450;&#26102;&#25552;&#20379;&#24110;&#21161;&#21644;&#20943;&#23569;&#36127;&#38754;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#38598;&#24212;&#29992;&#24443;&#24213;&#30340;&#39044;&#22788;&#29702;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#30446;&#30340;&#26159;&#28040;&#38500;&#22122;&#38899;&#24182;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#26469;&#35782;&#21035;&#22810;&#20256;&#24863;&#22120;UP-FALL&#25968;&#25454;&#38598;&#20013;&#26368;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#25928;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#32467;&#26524;&#25968;&#25454;&#20449;&#24687;&#35780;&#20272;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#26816;&#27979;&#20914;&#20987;&#30636;&#38388;&#26041;&#38754;&#30340;&#25928;&#29575;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#35780;&#20272;&#25351;&#26631;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#22312;&#20914;&#20987;&#26816;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#23637;&#31034;&#20102;&#21033;&#29992;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Falls among individuals, especially the elderly population, can lead to serious injuries and complications. Detecting impact moments within a fall event is crucial for providing timely assistance and minimizing the negative consequences. In this work, we aim to address this challenge by applying thorough preprocessing techniques to the multisensor dataset, the goal is to eliminate noise and improve data quality. Furthermore, we employ a feature selection process to identify the most relevant features derived from the multisensor UP-FALL dataset, which in turn will enhance the performance and efficiency of machine learning models. We then evaluate the efficiency of various machine learning models in detecting the impact moment using the resulting data information from multiple sensors. Through extensive experimentation, we assess the accuracy of our approach using various evaluation metrics. Our results achieve high accuracy rates in impact detection, showcasing the power of leveraging 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09762</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#28388;&#27874;&#21644;&#27169;&#24335;&#35782;&#21035;&#24378;&#21270;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Reinforcing POD based model reduction techniques in reaction-diffusion complex networks using stochastic filtering and pattern recognition. (arXiv:2307.09762v1 [cs.CE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#24378;&#21270;&#20102;&#22522;&#20110;POD&#30340;&#21453;&#24212;&#25193;&#25955;&#22797;&#26434;&#32593;&#32476;&#27169;&#22411;&#31616;&#21270;&#25216;&#26415;&#65292;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#32593;&#32476;&#34987;&#29992;&#20110;&#24314;&#27169;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#65292;&#28982;&#32780;&#36825;&#20123;&#31995;&#32479;&#30340;&#32500;&#24230;&#20351;&#24471;&#20854;&#20998;&#26512;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;POD&#31561;&#38477;&#32500;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23481;&#26131;&#21463;&#36755;&#20837;&#25968;&#25454;&#25200;&#21160;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#26694;&#26550;&#65292;&#23558;&#27169;&#24335;&#35782;&#21035;&#21644;&#38543;&#26426;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22686;&#24378;&#36825;&#20123;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#21463;&#25200;&#21160;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20195;&#29702;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#28982;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;(ODEs)&#22312;&#29305;&#23450;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#26694;&#26550;&#19982;&#22522;&#20110;&#31070;&#32463;ODE&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Complex networks are used to model many real-world systems. However, the dimensionality of these systems can make them challenging to analyze. Dimensionality reduction techniques like POD can be used in such cases. However, these models are susceptible to perturbations in the input data. We propose an algorithmic framework that combines techniques from pattern recognition (PR) and stochastic filtering theory to enhance the output of such models. The results of our study show that our method can improve the accuracy of the surrogate model under perturbed inputs. Deep Neural Networks (DNNs) are susceptible to adversarial attacks. However, recent research has revealed that neural Ordinary Differential Equations (ODEs) exhibit robustness in specific applications. We benchmark our algorithmic framework with a Neural ODE-based approach as a reference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2304.09310</link><description>&lt;p&gt;
&#33258;&#36866;&#24212; $\tau$-Lasso&#65306;&#20854;&#20581;&#22766;&#24615;&#21644;&#26368;&#20248;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09310
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#40065;&#26834;&#30340;&#33258;&#36866;&#24212; $\tau$-Lasso &#20272;&#35745;&#22120;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20197;&#38477;&#20302;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#23427;&#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#40065;&#26834; $\tau$-&#22238;&#24402;&#20272;&#35745;&#22120;&#65292;&#20197;&#24212;&#23545;&#21709;&#24212;&#21464;&#37327;&#21644;&#21327;&#21464;&#37327;&#30340;&#20005;&#37325;&#27745;&#26579;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20272;&#35745;&#22120;&#20026;&#33258;&#36866;&#24212; $\tau$-Lasso&#65292;&#23427;&#23545;&#24322;&#24120;&#20540;&#21644;&#39640;&#26464;&#26438;&#28857;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#37319;&#29992;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#26469;&#20943;&#23569;&#30495;&#23454;&#22238;&#24402;&#31995;&#25968;&#30340;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#33258;&#36866;&#24212; $\ell_1$-&#33539;&#25968;&#24809;&#32602;&#39033;&#20026;&#27599;&#20010;&#22238;&#24402;&#31995;&#25968;&#20998;&#37197;&#19968;&#20010;&#26435;&#37325;&#12290;&#23545;&#20110;&#22266;&#23450;&#25968;&#37327;&#30340;&#39044;&#27979;&#21464;&#37327; $p$&#65292;&#25105;&#20204;&#26174;&#31034;&#20986;&#33258;&#36866;&#24212; $\tau$-Lasso &#20855;&#26377;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#30495;&#23454;&#25903;&#25345;&#19979;&#22238;&#24402;&#21521;&#37327;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#30340;&#26368;&#20248;&#24615;&#36136;&#65292;&#20551;&#23450;&#24050;&#30693;&#30495;&#23454;&#22238;&#24402;&#21521;&#37327;&#30340;&#25903;&#25345;&#12290;&#28982;&#21518;&#25105;&#20204;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#26029;&#28857;&#21644;&#24433;&#21709;&#20989;&#25968;&#26469;&#34920;&#24449;&#20854;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27169;&#25311;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#20272;&#35745;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
&lt;/p&gt;</description></item></channel></rss>