<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>BG-HGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#29616;&#26377;HGNNs&#22312;&#22797;&#26434;&#24322;&#26500;&#22270;&#19978;&#38754;&#20020;&#30340;&#21442;&#25968;&#29190;&#28856;&#21644;&#20851;&#31995;&#22349;&#22604;&#31561;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.08207</link><description>&lt;p&gt;
BG-HGNN: &#26397;&#21521;&#21487;&#25193;&#23637;&#21644;&#39640;&#25928;&#30340;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BG-HGNN: Toward Scalable and Efficient Heterogeneous Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08207
&lt;/p&gt;
&lt;p&gt;
BG-HGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#29616;&#26377;HGNNs&#22312;&#22797;&#26434;&#24322;&#26500;&#22270;&#19978;&#38754;&#20020;&#30340;&#21442;&#25968;&#29190;&#28856;&#21644;&#20851;&#31995;&#22349;&#22604;&#31561;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#34987;&#24314;&#27169;&#20026;&#22312;&#24322;&#26500;&#22270;&#19978;&#30340;&#23398;&#20064;&#20219;&#21153;&#65292;&#20855;&#26377;&#26469;&#33258;&#19981;&#21516;&#31867;&#22411;&#33410;&#28857;&#21644;&#36793;&#30340;&#21508;&#31181;&#20851;&#31995;&#12290;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#26159;&#19968;&#31181;&#20026;&#24322;&#26500;&#22270;&#35774;&#35745;&#30340;&#26377;&#21069;&#36884;&#30340;&#31070;&#32463;&#27169;&#22411;&#31867;&#12290;&#29616;&#26377;HGNNs&#24314;&#31435;&#22312;&#20256;&#32479;GNNs&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#21442;&#25968;&#31354;&#38388;&#26469;&#24314;&#27169;&#19981;&#21516;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;HGNNs&#30340;&#23454;&#38469;&#26377;&#25928;&#24615;&#36890;&#24120;&#23616;&#38480;&#20110;&#31616;&#21333;&#30340;&#24322;&#26500;&#22270;&#65292;&#20855;&#26377;&#23569;&#37327;&#20851;&#31995;&#31867;&#22411;&#12290;&#26412;&#25991;&#39318;&#20808;&#31361;&#20986;&#21644;&#35777;&#26126;&#29616;&#26377;HGNNs&#20351;&#29992;&#30340;&#26631;&#20934;&#26041;&#27861;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#21442;&#25968;&#29190;&#28856;&#21644;&#20851;&#31995;&#22349;&#22604;&#65292;&#20351;&#24471;HGNNs&#23545;&#20855;&#26377;&#22823;&#37327;&#20851;&#31995;&#31867;&#22411;&#30340;&#22797;&#26434;&#24322;&#26500;&#22270; less&#26377;&#25928;&#25110;&#19981;&#23454;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;Blend&amp;Grind-HGNN (BG-HGNN)&#65292;&#36890;&#36807;&#20180;&#32454;&#22788;&#29702;&#25361;&#25112;&#26469;&#26377;&#25928;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08207v1 Announce Type: new  Abstract: Many computer vision and machine learning problems are modelled as learning tasks on heterogeneous graphs, featuring a wide array of relations from diverse types of nodes and edges. Heterogeneous graph neural networks (HGNNs) stand out as a promising neural model class designed for heterogeneous graphs. Built on traditional GNNs, existing HGNNs employ different parameter spaces to model the varied relationships. However, the practical effectiveness of existing HGNNs is often limited to simple heterogeneous graphs with few relation types. This paper first highlights and demonstrates that the standard approach employed by existing HGNNs inevitably leads to parameter explosion and relation collapse, making HGNNs less effective or impractical for complex heterogeneous graphs with numerous relation types. To overcome this issue, we introduce a novel framework, Blend&amp;Grind-HGNN (BG-HGNN), which effectively tackles the challenges by carefully i
&lt;/p&gt;</description></item><item><title>WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.07312</link><description>&lt;p&gt;
WiGenAI: &#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#23454;&#29616;&#26080;&#32447;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20132;&#32455;
&lt;/p&gt;
&lt;p&gt;
WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07312
&lt;/p&gt;
&lt;p&gt;
WiGenAI&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#65292;&#20026;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#24320;&#21457;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#26032;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;GPT-3&#21644;&#31283;&#23450;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#24050;&#32463;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#23454;&#29616;&#20102;&#33539;&#24335;&#36716;&#21464;&#65292;&#21521;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21457;&#23637;&#12290;&#20174;&#25968;&#25454;&#36890;&#20449;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39044;&#35745;&#23558;&#24191;&#27867;&#24212;&#29992;&#20110;&#26410;&#26469;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#30340;&#26032;&#19968;&#20195;&#20013;&#65292;&#24378;&#35843;&#20102;&#22312;&#26032;&#20852;&#36890;&#20449;&#22330;&#26223;&#20013;&#38656;&#35201;&#26032;&#39062;&#30340;AI&#26412;&#22320;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20171;&#32461;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#22880;&#23450;&#22522;&#30784;&#12290;&#20171;&#32461;&#20102;&#25193;&#25955;&#22411;&#29983;&#25104;&#27169;&#22411;&#20316;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#26032;&#33539;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#22312;&#26080;&#32447;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#12290;&#36824;&#25552;&#20379;&#20102;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#24320;&#21457;&#20855;&#26377;&#38887;&#24615;&#30340;AI&#26412;&#22320;&#36890;&#20449;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#23637;&#31034;&#20854;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#24212;&#29992;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innovative foundation models, such as GPT-3 and stable diffusion models, have made a paradigm shift in the realm of artificial intelligence (AI) towards generative AI-based systems. In unison, from data communication and networking perspective, AI and machine learning (AI/ML) algorithms are envisioned to be pervasively incorporated into the future generations of wireless communications systems, highlighting the need for novel AI-native solutions for the emergent communication scenarios. In this article, we outline the applications of generative AI in wireless communication systems to lay the foundations for research in this field. Diffusion-based generative models, as the new state-of-the-art paradigm of generative models, are introduced, and their applications in wireless communication systems are discussed. Two case studies are also presented to showcase how diffusion models can be exploited for the development of resilient AI-native communication systems. Specifically, we propose de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19600</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#19979;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Heterogeneous Data via Adaptive Self-Distillation. (arXiv:2305.19600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20351;&#24471;&#23458;&#25143;&#26426;&#21487;&#20197;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#20219;&#20309;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#20174;&#32780;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#20013;&#21457;&#29616;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35266;&#23519;&#21040;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#22343;&#21248;&#24615;&#65288;&#20363;&#22914;&#31867;&#21035;&#19981;&#24179;&#34913;&#65289;&#12290;&#22312;&#36825;&#31181;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#20250;&#20986;&#29616;&#8220;&#23458;&#25143;&#26426;&#28418;&#31227;&#8221;&#38382;&#39064;&#65292;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#25910;&#25947;&#21040;&#20854;&#33258;&#24049;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#22312;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#23458;&#25143;&#31471;&#25110;&#26381;&#21153;&#22120;&#20195;&#30721;&#36827;&#34892;&#20219;&#20309;&#26356;&#25913;&#65292;&#22240;&#27492;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#37096;&#32626;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train a global model by aggregating the locally trained models without sharing any local training data. In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the local data distributions observed by each of these clients. Under such non-iid data distributions across clients, FL suffers from the 'client-drift' problem where every client converges to its own local optimum. This results in slower convergence and poor performance of the aggregated model. To address this limitation, we propose a novel regularization technique based on adaptive self-distillation (ASD) for training models on the client side. Our regularization scheme adaptively adjusts to the client's training data based on: (1) the closeness of the local model's predictions with that of the global model and (2) the client's label distribution. The proposed regularization can be easily integrated atop exis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20923;&#32467;&#20877;&#21464;&#25442;(FTT)&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#21644;&#29305;&#24449;&#22122;&#22768;&#19979;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#20923;&#32467;&#29305;&#24449;&#23398;&#20064;&#22120;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#26680;&#24515;&#29305;&#24449;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.11075</link><description>&lt;p&gt;
&#20923;&#32467;&#20877;&#35757;&#32451;&#65306;&#22312;&#34394;&#20551;&#30456;&#20851;&#21644;&#29305;&#24449;&#22122;&#22768;&#19979;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Freeze then Train: Towards Provable Representation Learning under Spurious Correlations and Feature Noise. (arXiv:2210.11075v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.11075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20923;&#32467;&#20877;&#21464;&#25442;(FTT)&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#21644;&#29305;&#24449;&#22122;&#22768;&#19979;&#23454;&#29616;&#21487;&#35777;&#26126;&#30340;&#34920;&#31034;&#23398;&#20064;&#12290;&#35813;&#31639;&#27861;&#39318;&#20808;&#20923;&#32467;&#29305;&#24449;&#23398;&#20064;&#22120;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#26680;&#24515;&#29305;&#24449;&#65292;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#29615;&#22659;&#20013;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#65292;&#22914;&#22270;&#20687;&#32972;&#26223;&#65292;&#21487;&#33021;&#20351;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;(ERM)&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;Kirichenko&#31561;&#20154;(2022) &#23454;&#35777;&#21457;&#29616;&#65292;&#21363;&#20351;&#23384;&#22312;&#34394;&#20551;&#30456;&#20851;&#65292;&#19982;&#32467;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#29305;&#24449;&#20173;&#28982;&#21487;&#20197;&#24456;&#22909;&#22320;&#23398;&#20064;&#12290;&#36825;&#24320;&#21551;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#31574;&#30053;&#65292;&#21363;&#39318;&#20808;&#35757;&#32451;&#29305;&#24449;&#23398;&#20064;&#22120;&#32780;&#19981;&#26159;&#20998;&#31867;&#22120;&#65292;&#28982;&#21518;&#22312;&#27979;&#35797;&#29615;&#22659;&#20013;&#36827;&#34892;&#32447;&#24615;&#25506;&#27979;(&#37325;&#35757;&#32451;&#26368;&#21518;&#19968;&#23618;)&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#29702;&#35299;&#20309;&#26102;&#20197;&#21450;&#20026;&#20160;&#20040;&#36825;&#31181;&#26041;&#27861;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21482;&#26377;&#24403;&#19982;&#32467;&#26524;&#30456;&#20851;&#30340;&#26680;&#24515;&#29305;&#24449;&#20851;&#32852;&#30340;&#19981;&#21487;&#23454;&#29616;&#22122;&#22768;&#23567;&#20110;&#34394;&#20551;&#29305;&#24449;&#30340;&#22122;&#22768;&#26102;&#65292;&#25165;&#33021;&#24456;&#22909;&#22320;&#23398;&#20064;&#36825;&#20123;&#29305;&#24449;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#24182;&#19981;&#19968;&#23450;&#25104;&#31435;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#21644;&#23454;&#39564;&#35777;&#25454;&#25903;&#25345;&#36825;&#20010;&#21457;&#29616;&#65292;&#24182;&#38416;&#36848;&#19981;&#21487;&#23454;&#29616;&#22122;&#22768;&#30340;&#37325;&#35201;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20923;&#32467;&#20877;&#21464;&#25442;(FTT)&#30340;&#31639;&#27861;&#65292;&#39318;&#20808;&#20923;&#32467;&#29305;&#24449;&#23398;&#20064;&#22120;&#65292;&#28982;&#21518;&#22312;&#20854;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#26680;&#24515;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FTT&#22312;&#29305;&#24449;&#23398;&#20064;&#22120;&#19978;&#30340;&#19968;&#20010;&#28201;&#21644;&#26465;&#20214;&#19979;&#20445;&#35777;&#26377;&#30028;&#30340;&#27867;&#21270;&#35823;&#24046;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;FTT&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#34394;&#20551;&#30456;&#20851;&#20197;&#21450;&#29305;&#24449;&#22122;&#22768;&#35774;&#32622;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The existence of spurious correlations such as image backgrounds in the training environment can make empirical risk minimization (ERM) perform badly in the test environment. To address this problem, Kirichenko et al. (2022) empirically found that the core features that are related to the outcome can still be learned well even with the presence of spurious correlations. This opens a promising strategy to first train a feature learner rather than a classifier, and then perform linear probing (last layer retraining) in the test environment. However, a theoretical understanding of when and why this approach works is lacking. In this paper, we find that core features are only learned well when their associated non-realizable noise is smaller than that of spurious features, which is not necessarily true in practice. We provide both theories and experiments to support this finding and to illustrate the importance of non-realizable noise. Moreover, we propose an algorithm called Freeze then T
&lt;/p&gt;</description></item></channel></rss>