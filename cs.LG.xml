<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2404.02151</link><description>&lt;p&gt;
&#29992;&#31616;&#21333;&#33258;&#36866;&#24212;&#25915;&#20987;&#36234;&#29425;&#21151;&#33021;&#23545;&#40784;&#30340;LLM
&lt;/p&gt;
&lt;p&gt;
Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02151
&lt;/p&gt;
&lt;p&gt;
&#23637;&#31034;&#20102;&#23545;&#40784;&#30340;LLM&#23545;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#19981;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#27169;&#22411;&#19978;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#23545;&#20110;&#19981;&#20844;&#24320;logprobs&#30340;&#27169;&#22411;&#22914;&#20309;&#36827;&#34892;&#36234;&#29425;&#20197;&#21450;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26368;&#26032;&#30340;&#23433;&#20840;&#23545;&#40784;&#30340;LLM&#20063;&#19981;&#20855;&#26377;&#25269;&#25239;&#31616;&#21333;&#33258;&#36866;&#24212;&#36234;&#29425;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25104;&#21151;&#21033;&#29992;&#23545;logprobs&#30340;&#35775;&#38382;&#36827;&#34892;&#36234;&#29425;&#65306;&#25105;&#20204;&#26368;&#21021;&#35774;&#35745;&#20102;&#19968;&#20010;&#23545;&#25239;&#24615;&#25552;&#31034;&#27169;&#26495;&#65288;&#26377;&#26102;&#20250;&#36866;&#24212;&#30446;&#26631;LLM&#65289;&#65292;&#28982;&#21518;&#25105;&#20204;&#22312;&#21518;&#32512;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#20197;&#26368;&#22823;&#21270;&#30446;&#26631;logprob&#65288;&#20363;&#22914;token&#8220;Sure&#8221;&#65289;&#65292;&#21487;&#33021;&#20250;&#36827;&#34892;&#22810;&#27425;&#37325;&#21551;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;GPT-3.5/4&#12289;Llama-2-Chat-7B/13B/70B&#12289;Gemma-7B&#21644;&#38024;&#23545;GCG&#25915;&#20987;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#30340;HarmBench&#19978;&#30340;R2D2&#31561;&#20960;&#20046;100%&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;--&#26681;&#25454;GPT-4&#30340;&#35780;&#21028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36716;&#31227;&#25110;&#39044;&#22635;&#20805;&#25915;&#20987;&#20197;100%&#30340;&#25104;&#21151;&#29575;&#23545;&#25152;&#26377;&#19981;&#26292;&#38706;logprobs&#30340;Claude&#27169;&#22411;&#36827;&#34892;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#21463;&#27745;&#26579;&#30340;&#27169;&#22411;&#20013;&#20351;&#29992;&#23545;&#19968;&#32452;&#21463;&#38480;&#21046;&#30340;token&#25191;&#34892;&#38543;&#26426;&#25628;&#32034;&#20197;&#26597;&#25214;&#26408;&#39532;&#23383;&#31526;&#20018;&#30340;&#26041;&#27861;--&#36825;&#39033;&#20219;&#21153;&#19982;&#35768;&#22810;&#20854;&#20182;&#20219;&#21153;&#20849;&#20139;&#30456;&#21516;&#30340;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02151v1 Announce Type: cross  Abstract: We show that even the most recent safety-aligned LLMs are not robust to simple adaptive jailbreaking attacks. First, we demonstrate how to successfully leverage access to logprobs for jailbreaking: we initially design an adversarial prompt template (sometimes adapted to the target LLM), and then we apply random search on a suffix to maximize the target logprob (e.g., of the token "Sure"), potentially with multiple restarts. In this way, we achieve nearly 100\% attack success rate -- according to GPT-4 as a judge -- on GPT-3.5/4, Llama-2-Chat-7B/13B/70B, Gemma-7B, and R2D2 from HarmBench that was adversarially trained against the GCG attack. We also show how to jailbreak all Claude models -- that do not expose logprobs -- via either a transfer or prefilling attack with 100\% success rate. In addition, we show how to use random search on a restricted set of tokens for finding trojan strings in poisoned models -- a task that shares many s
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25311;&#33647;&#20998;&#23376;&#65292;&#30456;&#27604;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31454;&#20105;&#24615;&#65292;&#33021;&#22815;&#35299;&#20915;&#23545;&#26144;&#24322;&#26500;&#20307;&#38382;&#39064;&#65292;&#36827;&#32780;&#32771;&#34385;&#25152;&#26377;&#20998;&#23376;&#20960;&#20309;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2402.15864</link><description>&lt;p&gt;
&#22522;&#20110;&#22330;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Field-based Molecule Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15864
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22330;&#30340;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#25311;&#33647;&#20998;&#23376;&#65292;&#30456;&#27604;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#31454;&#20105;&#24615;&#65292;&#33021;&#22815;&#35299;&#20915;&#23545;&#26144;&#24322;&#26500;&#20307;&#38382;&#39064;&#65292;&#36827;&#32780;&#32771;&#34385;&#25152;&#26377;&#20998;&#23376;&#20960;&#20309;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;FMG&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#25311;&#33647;&#20998;&#23376;&#30340;&#22522;&#20110;&#22330;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#22914;&#20309;&#30456;&#27604;&#26222;&#36941;&#20351;&#29992;&#30340;&#22522;&#20110;&#28857;&#20113;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#37325;&#35201;&#20248;&#21183;&#65292;&#24182;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#20998;&#23376;&#31283;&#23450;&#24615;&#29983;&#25104;&#12290;&#25105;&#20204;&#35299;&#20915;&#20102;&#20809;&#23398;&#24322;&#26500;&#20307;&#65288;&#23545;&#26144;&#24322;&#26500;&#20307;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20808;&#21069;&#34987;&#24573;&#30053;&#30340;&#23545;&#20110;&#33647;&#29289;&#23433;&#20840;&#24615;&#21644;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#20998;&#23376;&#23646;&#24615;&#65292;&#24182;&#22240;&#27492;&#32771;&#34385;&#20102;&#25152;&#26377;&#20998;&#23376;&#20960;&#20309;&#26041;&#38754;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#26159;&#23545;&#19968;&#32452;&#21464;&#25442;&#19981;&#21464;&#30340;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#26144;&#24322;&#26500;&#20307;&#25104;&#23545;&#23384;&#22312;&#65292;&#23548;&#33268;&#23427;&#20204;&#23545;&#20998;&#23376;&#30340;R&#21644;S&#26500;&#22411;&#20445;&#25345;&#19981;&#21464;&#65292;&#32780;&#25105;&#20204;&#30340;&#22522;&#20110;&#22330;&#30340;&#29983;&#25104;&#27169;&#22411;&#25429;&#25417;&#20102;&#36825;&#19968;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15864v1 Announce Type: new  Abstract: This work introduces FMG, a field-based model for drug-like molecule generation. We show how the flexibility of this method provides crucial advantages over the prevalent, point-cloud based methods, and achieves competitive molecular stability generation. We tackle optical isomerism (enantiomers), a previously omitted molecular property that is crucial for drug safety and effectiveness, and thus account for all molecular geometry aspects. We demonstrate how previous methods are invariant to a group of transformations that includes enantiomer pairs, leading them invariant to the molecular R and S configurations, while our field-based generative model captures this property.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#22810;&#31181;&#21253;&#21547;&#20851;&#31995;&#23450;&#20041;&#12289;&#29369;&#35947;&#27169;&#31946;&#20449;&#24687;&#31995;&#32479;&#30340;&#22522;&#30784;&#21629;&#39064;&#21644;&#22522;&#20110;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#30340;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.04256</link><description>&lt;p&gt;
&#29369;&#35947;&#27169;&#31946;&#38598;&#21450;&#20854;&#24212;&#29992;&#20110;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#30340;&#22522;&#30784;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Foundational theories of hesitant fuzzy sets and hesitant fuzzy information systems and their applications for multi-strength intelligent classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.04256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#22810;&#31181;&#21253;&#21547;&#20851;&#31995;&#23450;&#20041;&#12289;&#29369;&#35947;&#27169;&#31946;&#20449;&#24687;&#31995;&#32479;&#30340;&#22522;&#30784;&#21629;&#39064;&#21644;&#22522;&#20110;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#30340;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29369;&#35947;&#27169;&#31946;&#38598;&#22312;&#26576;&#20123;&#19981;&#30830;&#23450;&#21644;&#29369;&#35947;&#30340;&#24773;&#20917;&#19979;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22312;&#38598;&#21512;&#20013;&#65292;&#21253;&#21547;&#20851;&#31995;&#26159;&#19968;&#20010;&#37325;&#35201;&#19988;&#22522;&#30784;&#30340;&#23450;&#20041;&#12290;&#22240;&#27492;&#65292;&#20316;&#20026;&#19968;&#31181;&#38598;&#21512;&#65292;&#29369;&#35947;&#27169;&#31946;&#38598;&#38656;&#35201;&#19968;&#20010;&#26126;&#30830;&#30340;&#21253;&#21547;&#20851;&#31995;&#23450;&#20041;&#12290;&#22522;&#20110;&#31163;&#25955;&#24418;&#24335;&#30340;&#29369;&#35947;&#27169;&#31946;&#38582;&#23646;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20960;&#31181;&#36866;&#29992;&#20110;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#21253;&#21547;&#20851;&#31995;&#12290;&#38543;&#21518;&#65292;&#20171;&#32461;&#20102;&#19968;&#20123;&#29369;&#35947;&#27169;&#31946;&#38598;&#30340;&#22522;&#30784;&#21629;&#39064;&#65292;&#20197;&#21450;&#29369;&#35947;&#27169;&#31946;&#38598;&#26063;&#30340;&#21629;&#39064;&#12290;&#38024;&#23545;&#21442;&#25968;&#20943;&#23569;&#65292;&#25552;&#20986;&#20102;&#29369;&#35947;&#27169;&#31946;&#20449;&#24687;&#31995;&#32479;&#30340;&#19968;&#20123;&#22522;&#30784;&#21629;&#39064;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#31034;&#20363;&#21644;&#31639;&#27861;&#26469;&#35828;&#26126;&#21442;&#25968;&#20943;&#23569;&#30340;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#24378;&#24230;&#26234;&#33021;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#23545;&#22797;&#26434;&#31995;&#32479;&#36827;&#34892;&#20581;&#24247;&#29366;&#24577;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.04256v3 Announce Type: replace  Abstract: Hesitant fuzzy sets are widely used in certain instances of uncertainty and hesitation. In sets, the inclusion relationship is an important and foundational definition. Thus, as a kind of set, hesitant fuzzy sets require an explicit definition of inclusion relationship. Based on the hesitant fuzzy membership degree of discrete form, several kinds of inclusion relationships for hesitant fuzzy sets are proposed in this work. Then, some foundational propositions of hesitant fuzzy sets are presented, along with propositions of families of hesitant fuzzy sets. Some foundational propositions of hesitant fuzzy information systems are proposed with respect to parameter reductions and an example and an algorithm are given to illustrate the processes of parameter reduction. Finally, a multi-strength intelligent classifier is proposed to make health state diagnoses for complex systems.
&lt;/p&gt;</description></item><item><title>DHQRN&#21487;&#20197;&#39044;&#27979;&#26356;&#19968;&#33324;&#30340;Huber&#20998;&#20301;&#25968;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#20998;&#24067;&#30340;&#23614;&#37096;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2306.10306</link><description>&lt;p&gt;
&#28145;&#24230;Huber&#20998;&#20301;&#25968;&#22238;&#24402;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Huber quantile regression networks. (arXiv:2306.10306v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10306
&lt;/p&gt;
&lt;p&gt;
DHQRN&#21487;&#20197;&#39044;&#27979;&#26356;&#19968;&#33324;&#30340;Huber&#20998;&#20301;&#25968;&#65292;&#24182;&#19988;&#22312;&#39044;&#27979;&#20998;&#24067;&#30340;&#23614;&#37096;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#22238;&#24402;&#24212;&#29992;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#24179;&#26041;&#35823;&#24046;&#25110;&#32477;&#23545;&#35823;&#24046;&#35780;&#20998;&#20989;&#25968;&#26469;&#25253;&#21578;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#22343;&#20540;&#25110;&#20013;&#20301;&#25968;&#12290;&#21457;&#20986;&#26356;&#22810;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#20989;&#25968;&#65288;&#20998;&#20301;&#25968;&#21644;&#26399;&#26395;&#20540;&#65289;&#30340;&#37325;&#35201;&#24615;&#24050;&#34987;&#35748;&#20026;&#26159;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#25163;&#27573;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#36890;&#36807;&#20998;&#20301;&#25968;&#21644;&#26399;&#26395;&#20540;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;QRNN&#21644;ERNN&#65289;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#28145;&#24230;Huber&#20998;&#20301;&#25968;&#22238;&#24402;&#32593;&#32476;&#65288;DHQRN&#65289;&#65292;&#23427;&#23558;QRNN&#21644;ERNN&#23884;&#22871;&#20026;&#36793;&#32536;&#24773;&#20917;&#12290; DHQRN&#21487;&#20197;&#39044;&#27979;Huber&#20998;&#20301;&#25968;&#65292;&#36825;&#26159;&#26356;&#19968;&#33324;&#30340;&#20989;&#25968;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#20998;&#20301;&#25968;&#21644;&#26399;&#26395;&#20540;&#20316;&#20026;&#26497;&#38480;&#24773;&#20917;&#23884;&#22871;&#36215;&#26469;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#20351;&#29992;Huber&#20998;&#20301;&#25968;&#22238;&#24402;&#20989;&#25968;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#36825;&#19982;Huber&#20998;&#20301;&#25968;&#21151;&#33021;&#19968;&#33268;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;DHQRN&#34987;&#24212;&#29992;&#20110;&#39044;&#27979;&#25151;&#20215;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#65292;&#24182;&#19982;&#20854;&#20182;&#22238;&#24402;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20960;&#20010;&#35823;&#24046;&#25351;&#26631;&#20013;&#65292;DHQRN&#32988;&#36807;&#20854;&#20182;&#25216;&#26415;&#65292;&#22312;&#39044;&#27979;&#20998;&#24067;&#30340;&#23614;&#37096;&#25552;&#20379;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typical machine learning regression applications aim to report the mean or the median of the predictive probability distribution, via training with a squared or an absolute error scoring function. The importance of issuing predictions of more functionals of the predictive probability distribution (quantiles and expectiles) has been recognized as a means to quantify the uncertainty of the prediction. In deep learning (DL) applications, that is possible through quantile and expectile regression neural networks (QRNN and ERNN respectively). Here we introduce deep Huber quantile regression networks (DHQRN) that nest QRNNs and ERNNs as edge cases. DHQRN can predict Huber quantiles, which are more general functionals in the sense that they nest quantiles and expectiles as limiting cases. The main idea is to train a deep learning algorithm with the Huber quantile regression function, which is consistent for the Huber quantile functional. As a proof of concept, DHQRN are applied to predict hou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#31216;&#24615;&#26126;&#30830;&#22320;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046;&#25152;&#23548;&#33268;&#30340;&#36924;&#36817;-&#27867;&#21270;&#26435;&#34913;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#31181;&#27169;&#22411;&#22312;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#23545;&#31216;&#24615;&#30340;&#21516;&#26102;&#20250;&#25913;&#36827;&#27867;&#21270;&#12290;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#24615;&#33021;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2305.17592</link><description>&lt;p&gt;
(&#36817;&#20284;)&#32676;&#31561;&#21464;&#24615;&#19979;&#30340;&#36924;&#36817;-&#27867;&#21270;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Approximation-Generalization Trade-offs under (Approximate) Group Equivariance. (arXiv:2305.17592v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35814;&#32454;&#30740;&#31350;&#20102;&#36890;&#36807;&#23545;&#31216;&#24615;&#26126;&#30830;&#22320;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046;&#25152;&#23548;&#33268;&#30340;&#36924;&#36817;-&#27867;&#21270;&#26435;&#34913;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#36825;&#31181;&#27169;&#22411;&#22312;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#23545;&#31216;&#24615;&#30340;&#21516;&#26102;&#20250;&#25913;&#36827;&#27867;&#21270;&#12290;&#36825;&#19968;&#32467;&#26524;&#23545;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#24615;&#33021;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#31216;&#24615;&#26126;&#30830;&#22320;&#24341;&#20837;&#20219;&#21153;&#29305;&#23450;&#30340;&#24402;&#32435;&#20559;&#24046;&#24050;&#25104;&#20026;&#39640;&#24615;&#33021;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24320;&#21457;&#20013;&#30340;&#24120;&#35268;&#35774;&#35745;&#20934;&#21017;&#12290;&#20363;&#22914;&#65292;&#32676;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#22312;&#34507;&#30333;&#36136;&#21644;&#33647;&#29289;&#35774;&#35745;&#31561;&#21508;&#20010;&#39046;&#22495;&#21644;&#24212;&#29992;&#20013;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#26222;&#36941;&#24863;&#35273;&#26159;&#65292;&#23558;&#30456;&#20851;&#23545;&#31216;&#24615;&#25972;&#21512;&#21040;&#27169;&#22411;&#20013;&#20250;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#26377;&#20154;&#35748;&#20026;&#65292;&#24403;&#25968;&#25454;&#21644;/&#25110;&#27169;&#22411;&#21482;&#33021;&#34920;&#29616;&#20986;$\textit{&#36817;&#20284;}$&#25110;$\textit{&#37096;&#20998;}$&#23545;&#31216;&#24615;&#26102;&#65292;&#26368;&#20248;&#25110;&#26368;&#22909;&#24615;&#33021;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#27169;&#22411;&#23545;&#40784;&#20110;&#25968;&#25454;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20123;&#30452;&#35273;&#36827;&#34892;&#20102;&#27491;&#24335;&#30340;&#32479;&#19968;&#30740;&#31350;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#33324;&#30340;&#25968;&#37327;&#30028;&#38480;&#65292;&#35777;&#26126;&#25429;&#33719;&#20219;&#21153;&#29305;&#23450;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#23558;&#23548;&#33268;&#25913;&#36827;&#30340;&#27867;&#21270;&#12290;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#35201;&#27714;&#21464;&#25442;&#26159;&#26377;&#38480;&#30340;&#65292;&#29978;&#33267;&#19981;&#38656;&#35201;&#24418;&#25104;&#23436;&#25972;&#30340;....
&lt;/p&gt;
&lt;p&gt;
The explicit incorporation of task-specific inductive biases through symmetry has emerged as a general design precept in the development of high-performance machine learning models. For example, group equivariant neural networks have demonstrated impressive performance across various domains and applications such as protein and drug design. A prevalent intuition about such models is that the integration of relevant symmetry results in enhanced generalization. Moreover, it is posited that when the data and/or the model may only exhibit $\textit{approximate}$ or $\textit{partial}$ symmetry, the optimal or best-performing model is one where the model symmetry aligns with the data symmetry. In this paper, we conduct a formal unified investigation of these intuitions. To begin, we present general quantitative bounds that demonstrate how models capturing task-specific symmetries lead to improved generalization. In fact, our results do not require the transformations to be finite or even form
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#30340;&#31639;&#27861;Cooperator&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#27604;Transformer&#31639;&#27861;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.10449</link><description>&lt;p&gt;
&#21512;&#20316;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#12290; &#65288;arXiv:2305.10449v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
Cooperation Is All You Need. (arXiv:2305.10449v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10449
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#30340;&#31639;&#27861;Cooperator&#65292;&#35813;&#31639;&#27861;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#34920;&#29616;&#27604;Transformer&#31639;&#27861;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36229;&#36234;&#8220;&#26641;&#31361;&#27665;&#20027;&#8221;&#20043;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;Cooperator&#30340;&#8220;&#26412;&#22320;&#22788;&#29702;&#22120;&#27665;&#20027;&#8221;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#19982;&#22522;&#20110;Transformers&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65288;&#20363;&#22914;ChatGPT&#65289;&#22312;&#32622;&#25442;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#30340;&#21151;&#33021;&#36827;&#34892;&#27604;&#36739;&#12290; Transformers&#22522;&#20110;&#38271;&#26399;&#20197;&#26469;&#30340;&#8220;&#31215;&#20998;-&#21457;&#23556;&#8221;&#8220;&#28857;&#8221;&#31070;&#32463;&#20803;&#30340;&#27010;&#24565;&#65292;&#32780;Cooperator&#21017;&#21463;&#21040;&#26368;&#36817;&#31070;&#32463;&#29983;&#29289;&#23398;&#31361;&#30772;&#30340;&#21551;&#31034;&#65292;&#36825;&#20123;&#31361;&#30772;&#34920;&#26126;&#65292;&#31934;&#31070;&#29983;&#27963;&#30340;&#32454;&#32990;&#22522;&#30784;&#21462;&#20915;&#20110;&#26032;&#30382;&#23618;&#20013;&#20855;&#26377;&#20004;&#20010;&#21151;&#33021;&#19978;&#19981;&#21516;&#28857;&#30340;&#19978;&#30382;&#31070;&#32463;&#20803;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#24403;&#29992;&#20110;RL&#26102;&#65292;&#22522;&#20110;Cooperator&#30340;&#31639;&#27861;&#23398;&#20064;&#36895;&#24230;&#27604;&#22522;&#20110;Transformer&#30340;&#31639;&#27861;&#24555;&#24471;&#22810;&#65292;&#21363;&#20351;&#23427;&#20204;&#20855;&#26377;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Going beyond 'dendritic democracy', we introduce a 'democracy of local processors', termed Cooperator. Here we compare their capabilities when used in permutation-invariant neural networks for reinforcement learning (RL), with machine learning algorithms based on Transformers, such as ChatGPT. Transformers are based on the long-standing conception of integrate-and-fire 'point' neurons, whereas Cooperator is inspired by recent neurobiological breakthroughs suggesting that the cellular foundations of mental life depend on context-sensitive pyramidal neurons in the neocortex which have two functionally distinct points. We show that when used for RL, an algorithm based on Cooperator learns far quicker than that based on Transformer, even while having the same number of parameters.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#38543;&#26426;&#24615;&#21487;&#20197;&#33258;&#28982;&#22320;&#36991;&#20813;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#20013;&#30340;&#20005;&#26684;&#38797;&#28857;&#38382;&#39064;&#65292;&#36825;&#19968;&#35748;&#35782;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36817;&#26399;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2210.06723</link><description>&lt;p&gt;
&#38543;&#26426;&#22122;&#22768;&#23545;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#26377;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic noise can be helpful for variational quantum algorithms. (arXiv:2210.06723v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#38543;&#26426;&#24615;&#21487;&#20197;&#33258;&#28982;&#22320;&#36991;&#20813;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#20013;&#30340;&#20005;&#26684;&#38797;&#28857;&#38382;&#39064;&#65292;&#36825;&#19968;&#35748;&#35782;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36817;&#26399;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38797;&#28857;&#26159;&#23545;&#20110;&#19968;&#38454;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#22312;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#30340;&#27010;&#24565;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36991;&#20813;&#38797;&#28857;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#35777;&#25454;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#24615;&#26469;&#33258;&#28982;&#22320;&#36991;&#20813;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#20013;&#30340;&#38797;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#22312;&#25968;&#20540;&#27169;&#25311;&#21644;&#37327;&#23376;&#30828;&#20214;&#19978;&#25552;&#20379;&#20102;&#23454;&#38469;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#21464;&#20998;&#31639;&#27861;&#30340;&#33258;&#28982;&#38543;&#26426;&#24615;&#21487;&#20197;&#26377;&#21161;&#20110;&#36991;&#20813;&#20005;&#26684;&#30340;&#38797;&#28857;&#65292;&#21363;&#33267;&#23569;&#20855;&#26377;&#19968;&#20010;&#36127;Hessian&#29305;&#24449;&#20540;&#30340;&#38797;&#28857;&#12290;&#36825;&#20010;&#35265;&#35299;&#34920;&#26126;&#19968;&#23450;&#31243;&#24230;&#30340;&#38543;&#26426;&#22122;&#22768;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26356;&#22909;&#22320;&#29702;&#35299;&#36817;&#26399;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Saddle points constitute a crucial challenge for first-order gradient descent algorithms. In notions of classical machine learning, they are avoided for example by means of stochastic gradient descent methods. In this work, we provide evidence that the saddle points problem can be naturally avoided in variational quantum algorithms by exploiting the presence of stochasticity. We prove convergence guarantees and present practical examples in numerical simulations and on quantum hardware. We argue that the natural stochasticity of variational algorithms can be beneficial for avoiding strict saddle points, i.e., those saddle points with at least one negative Hessian eigenvalue. This insight that some levels of shot noise could help is expected to add a new perspective to notions of near-term variational quantum algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28023;&#32501;&#27602;&#21270;&#8221;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#28023;&#32501;&#26679;&#26412;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27599;&#20010;&#36755;&#20837;&#19978;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#19988;&#21363;&#20351;&#25915;&#20987;&#32773;&#21482;&#25511;&#21046;&#20102;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20063;&#21487;&#20197;&#36827;&#34892;&#27492;&#25915;&#20987;&#65292;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2203.08147</link><description>&lt;p&gt;
&#22522;&#20110;&#28023;&#32501;&#27602;&#21270;&#30340;&#33021;&#32791;&#24310;&#36831;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy-Latency Attacks via Sponge Poisoning. (arXiv:2203.08147v4 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.08147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#28023;&#32501;&#27602;&#21270;&#8221;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#39318;&#27425;&#35777;&#26126;&#20102;&#22312;&#35757;&#32451;&#26102;&#27880;&#20837;&#28023;&#32501;&#26679;&#26412;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#27599;&#20010;&#36755;&#20837;&#19978;&#30340;&#33021;&#32791;&#21644;&#24310;&#36831;&#65292;&#24182;&#19988;&#21363;&#20351;&#25915;&#20987;&#32773;&#21482;&#25511;&#21046;&#20102;&#19968;&#20123;&#27169;&#22411;&#26356;&#26032;&#20063;&#21487;&#20197;&#36827;&#34892;&#27492;&#25915;&#20987;&#65292;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28023;&#32501;&#26679;&#26412;&#26159;&#22312;&#27979;&#35797;&#26102;&#31934;&#24515;&#20248;&#21270;&#30340;&#36755;&#20837;&#65292;&#21487;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#19978;&#37096;&#32626;&#26102;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#12290;&#26412;&#25991;&#39318;&#27425;&#35777;&#26126;&#20102;&#28023;&#32501;&#26679;&#26412;&#20063;&#21487;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;&#28023;&#32501;&#27602;&#21270;&#30340;&#25915;&#20987;&#27880;&#20837;&#21040;&#35757;&#32451;&#20013;&#12290;&#35813;&#25915;&#20987;&#20801;&#35768;&#22312;&#27599;&#20010;&#27979;&#35797;&#26102;&#36755;&#20837;&#20013;&#19981;&#21152;&#21306;&#20998;&#22320;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28023;&#32501;&#27602;&#21270;&#24418;&#24335;&#21270;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#19982;&#20248;&#21270;&#27979;&#35797;&#26102;&#28023;&#32501;&#26679;&#26412;&#30456;&#20851;&#30340;&#38480;&#21046;&#65292;&#24182;&#34920;&#26126;&#21363;&#20351;&#25915;&#20987;&#32773;&#20165;&#25511;&#21046;&#20960;&#20010;&#27169;&#22411;&#26356;&#26032;&#65292;&#20363;&#22914;&#27169;&#22411;&#35757;&#32451;&#34987;&#22806;&#21253;&#32473;&#19981;&#21463;&#20449;&#20219;&#30340;&#31532;&#19977;&#26041;&#25110;&#36890;&#36807;&#32852;&#37030;&#23398;&#20064;&#20998;&#24067;&#24335;&#36827;&#34892;&#65292;&#20063;&#21487;&#20197;&#36827;&#34892;&#36825;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#34920;&#26126;&#28023;&#32501;&#27602;&#21270;&#20960;&#20046;&#23436;&#20840;&#28040;&#38500;&#20102;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#25928;&#26524;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27602;&#21270;&#27169;&#22411;&#30340;&#28608;&#27963;&#65292;&#30830;&#23450;&#20102;&#21738;&#20123;&#35745;&#31639;&#23545;&#23548;&#33268;&#33021;&#37327;&#28040;&#32791;&#21644;&#24310;&#36831;&#22686;&#21152;&#36215;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Sponge examples are test-time inputs carefully optimized to increase energy consumption and latency of neural networks when deployed on hardware accelerators. In this work, we are the first to demonstrate that sponge examples can also be injected at training time, via an attack that we call sponge poisoning. This attack allows one to increase the energy consumption and latency of machine-learning models indiscriminately on each test-time input. We present a novel formalization for sponge poisoning, overcoming the limitations related to the optimization of test-time sponge examples, and show that this attack is possible even if the attacker only controls a few model updates; for instance, if model training is outsourced to an untrusted third-party or distributed via federated learning. Our extensive experimental analysis shows that sponge poisoning can almost completely vanish the effect of hardware accelerators. We also analyze the activations of poisoned models, identifying which comp
&lt;/p&gt;</description></item></channel></rss>