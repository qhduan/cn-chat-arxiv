<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#65292;&#24182;&#35774;&#35745;&#20102;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#20351;&#24471;&#22312;&#40657;&#21283;&#23376;&#35774;&#32622;&#19979;&#65292;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29305;&#23450;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;</title><link>https://arxiv.org/abs/2404.02314</link><description>&lt;p&gt;
&#20998;&#23376;&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#21542;&#30495;&#30340;&#38656;&#35201;&#20803;&#35757;&#32451;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Meta-training Really Necessary for Molecular Few-Shot Learning ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#65292;&#24182;&#35774;&#35745;&#20102;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#20351;&#24471;&#22312;&#40657;&#21283;&#23376;&#35774;&#32622;&#19979;&#65292;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#33719;&#24471;&#20102;&#31454;&#20105;&#24615;&#34920;&#29616;&#65292;&#21516;&#26102;&#28040;&#38500;&#20102;&#29305;&#23450;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23569;&#26679;&#26412;&#23398;&#20064;&#22312;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#32780;&#26368;&#36817;&#24555;&#36895;&#22686;&#38271;&#30340;&#25991;&#29486;&#22823;&#22810;&#28041;&#21450;&#22797;&#26434;&#30340;&#20803;&#23398;&#20064;&#31574;&#30053;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26356;&#20026;&#30452;&#25509;&#30340;&#20998;&#23376;&#25968;&#25454;&#24494;&#35843;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#39532;&#27663;&#36317;&#31163;&#30340;&#27491;&#21017;&#21270;&#20108;&#27425;&#25506;&#38024;&#25439;&#22833;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#19987;&#38376;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#20248;&#21270;&#22120;&#65292;&#36991;&#20813;&#20102;&#25105;&#20204;&#25439;&#22833;&#20989;&#25968;&#30340;&#36864;&#21270;&#35299;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#24494;&#35843;&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#27604;&#36739;&#20013;&#33719;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#40657;&#21283;&#23376;&#35774;&#32622;&#65292;&#24182;&#28040;&#38500;&#20102;&#29305;&#23450;&#24773;&#33410;&#39044;&#35757;&#32451;&#31574;&#30053;&#30340;&#38656;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;&#31454;&#20105;&#26041;&#27861;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30340;&#24494;&#35843;&#22522;&#32447;&#22987;&#32456;&#27604;&#20803;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02314v1 Announce Type: cross  Abstract: Few-shot learning has recently attracted significant interest in drug discovery, with a recent, fast-growing literature mostly involving convoluted meta-learning strategies. We revisit the more straightforward fine-tuning approach for molecular data, and propose a regularized quadratic-probe loss based on the the Mahalanobis distance. We design a dedicated block-coordinate descent optimizer, which avoid the degenerate solutions of our loss. Interestingly, our simple fine-tuning approach achieves highly competitive performances in comparison to state-of-the-art methods, while being applicable to black-box settings and removing the need for specific episodic pre-training strategies. Furthermore, we introduce a new benchmark to assess the robustness of the competing methods to domain shifts. In this setting, our fine-tuning baseline obtains consistently better results than meta-learning methods.
&lt;/p&gt;</description></item><item><title>TensorNet&#25193;&#23637;&#20102;&#20854;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#30005;&#20998;&#23376;&#21644;&#33258;&#26059;&#29366;&#24577;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#21270;&#23398;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15073</link><description>&lt;p&gt;
&#35770;&#31515;&#21345;&#23572;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#21183;&#20013;&#21253;&#21547;&#30005;&#33655;&#21644;&#33258;&#26059;&#29366;&#24577;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Inclusion of Charge and Spin States in Cartesian Tensor Neural Network Potentials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15073
&lt;/p&gt;
&lt;p&gt;
TensorNet&#25193;&#23637;&#20102;&#20854;&#33021;&#21147;&#65292;&#21487;&#20197;&#22788;&#29702;&#24102;&#30005;&#20998;&#23376;&#21644;&#33258;&#26059;&#29366;&#24577;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#21270;&#23398;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#23553;&#20449;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;TensorNet&#30340;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#31561;&#21464;&#31515;&#21345;&#23572;&#24352;&#37327;&#31070;&#32463;&#32593;&#32476;&#21183;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#24102;&#30005;&#20998;&#23376;&#21644;&#33258;&#26059;&#29366;&#24577;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#26550;&#26500;&#26356;&#25913;&#25110;&#22686;&#21152;&#25104;&#26412;&#12290;&#36890;&#36807;&#21512;&#24182;&#36825;&#20123;&#23646;&#24615;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36755;&#20837;&#36864;&#21270;&#38382;&#39064;&#65292;&#22686;&#24378;&#20102;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#21270;&#23398;&#31995;&#32479;&#20013;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#36825;&#19968;&#36827;&#23637;&#26174;&#33879;&#25299;&#23485;&#20102;TensorNet&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15073v1 Announce Type: new  Abstract: In this letter, we present an extension to TensorNet, a state-of-the-art equivariant Cartesian tensor neural network potential, allowing it to handle charged molecules and spin states without architectural changes or increased costs. By incorporating these attributes, we address input degeneracy issues, enhancing the model's predictive accuracy across diverse chemical systems. This advancement significantly broadens TensorNet's applicability, maintaining its efficiency and accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13196</link><description>&lt;p&gt;
&#20351;Prompt&#35843;&#20248;&#35270;&#35273;Transformer&#26356;&#20026;&#20581;&#22766;&#30340;ADAPT
&lt;/p&gt;
&lt;p&gt;
ADAPT to Robustify Prompt Tuning Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ADAPT&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#65292;&#22686;&#24378;&#35270;&#35273;Transformer&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#35270;&#35273;Transformer&#65292;&#24050;&#30693;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#35768;&#22810;&#29616;&#26377;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20381;&#36182;&#20110;&#23545;&#25972;&#20010;&#27169;&#22411;&#36827;&#34892;&#20840;&#38754;&#24494;&#35843;&#20197;&#22686;&#21152;&#27169;&#22411;&#30340;&#31283;&#20581;&#24615;&#12290;&#36825;&#20123;&#38450;&#24481;&#26041;&#27861;&#38656;&#35201;&#20026;&#27599;&#20010;&#20219;&#21153;&#23384;&#20648;&#25972;&#20010;&#27169;&#22411;&#30340;&#21103;&#26412;&#65292;&#32780;&#27169;&#22411;&#21487;&#33021;&#21253;&#21547;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;prompt&#35843;&#20248;&#34987;&#29992;&#26469;&#36866;&#24212;&#22823;&#22411;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#20445;&#23384;&#22823;&#22411;&#21103;&#26412;&#12290;&#26412;&#25991;&#20174;&#31283;&#20581;&#24615;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#23545;&#35270;&#35273;Transformer&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#30340;&#21442;&#25968;&#39640;&#25928;prompt&#35843;&#20248;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20043;&#21069;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#22312;&#24212;&#29992;&#21040;prompt&#35843;&#20248;&#33539;&#24335;&#26102;&#65292;&#23384;&#22312;&#26799;&#24230;&#27169;&#31946;&#24182;&#23481;&#26131;&#21463;&#21040;&#33258;&#36866;&#24212;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ADAPT&#65292;&#19968;&#31181;&#22312;prompt&#35843;&#20248;&#33539;&#24335;&#20013;&#25191;&#34892;&#33258;&#36866;&#24212;&#23545;&#25239;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13196v1 Announce Type: new  Abstract: The performance of deep models, including Vision Transformers, is known to be vulnerable to adversarial attacks. Many existing defenses against these attacks, such as adversarial training, rely on full-model fine-tuning to induce robustness in the models. These defenses require storing a copy of the entire model, that can have billions of parameters, for each task. At the same time, parameter-efficient prompt tuning is used to adapt large transformer-based models to downstream tasks without the need to save large copies. In this paper, we examine parameter-efficient prompt tuning of Vision Transformers for downstream tasks under the lens of robustness. We show that previous adversarial defense methods, when applied to the prompt tuning paradigm, suffer from gradient obfuscation and are vulnerable to adaptive attacks. We introduce ADAPT, a novel framework for performing adaptive adversarial training in the prompt tuning paradigm. Our meth
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#28041;&#21450;&#23545;&#25239;&#25439;&#22833;&#21644;&#30828;&#32422;&#26463;&#30340;CMDP&#65292;&#22312;&#20004;&#31181;&#19981;&#21516;&#24773;&#24418;&#19979;&#35774;&#35745;&#20102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#30340;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.03672</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#38543;&#26426;&#30828;&#32422;&#26463;&#30340;&#23545;&#25239;MDP&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Adversarial MDPs with Stochastic Hard Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#28041;&#21450;&#23545;&#25239;&#25439;&#22833;&#21644;&#30828;&#32422;&#26463;&#30340;CMDP&#65292;&#22312;&#20004;&#31181;&#19981;&#21516;&#24773;&#24418;&#19979;&#35774;&#35745;&#20102;&#20855;&#26377;&#27425;&#32447;&#24615;&#36951;&#25022;&#30340;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#20808;&#21069;&#30740;&#31350;&#20013;&#23545;&#36825;&#19968;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#24102;&#26377;&#23545;&#25239;&#25439;&#22833;&#21644;&#38543;&#26426;&#30828;&#32422;&#26463;&#30340;&#21463;&#38480;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#20013;&#30340;&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20004;&#31181;&#19981;&#21516;&#30340;&#24773;&#24418;&#12290;&#22312;&#31532;&#19968;&#31181;&#24773;&#24418;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#33324;CMDP&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#21644;&#32047;&#31215;&#27491;&#32422;&#26463;&#36829;&#21453;&#12290;&#22312;&#31532;&#20108;&#31181;&#24773;&#24418;&#20013;&#65292;&#22312;&#19968;&#20010;&#25919;&#31574;&#20005;&#26684;&#28385;&#36275;&#32422;&#26463;&#23384;&#22312;&#19988;&#20026;&#23398;&#20064;&#32773;&#25152;&#20102;&#35299;&#30340;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#21516;&#26102;&#30830;&#20445;&#22312;&#27599;&#19968;&#36718;&#20013;&#32422;&#26463;&#20197;&#39640;&#27010;&#29575;&#24471;&#21040;&#28385;&#36275;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#31532;&#19968;&#20010;&#30740;&#31350;&#26082;&#28041;&#21450;&#23545;&#25239;&#25439;&#22833;&#21448;&#28041;&#21450;&#30828;&#32422;&#26463;&#30340;CMDP&#30340;&#24037;&#20316;&#12290;&#23454;&#38469;&#19978;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35201;&#20040;&#38598;&#20013;&#22312;&#26356;&#24369;&#30340;&#36719;&#32422;&#26463;&#19978;--&#20801;&#35768;&#27491;&#36829;&#21453;&#26469;&#25269;&#28040;&#36127;&#36829;&#21453;--&#35201;&#20040;&#23616;&#38480;&#20110;&#38543;&#26426;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22788;&#29702;&#19968;&#33324;&#30340;&#38750;&#32479;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03672v1 Announce Type: new  Abstract: We study online learning problems in constrained Markov decision processes (CMDPs) with adversarial losses and stochastic hard constraints. We consider two different scenarios. In the first one, we address general CMDPs, where we design an algorithm that attains sublinear regret and cumulative positive constraints violation. In the second scenario, under the mild assumption that a policy strictly satisfying the constraints exists and is known to the learner, we design an algorithm that achieves sublinear regret while ensuring that the constraints are satisfied at every episode with high probability. To the best of our knowledge, our work is the first to study CMDPs involving both adversarial losses and hard constraints. Indeed, previous works either focus on much weaker soft constraints--allowing for positive violation to cancel out negative ones--or are restricted to stochastic losses. Thus, our algorithms can deal with general non-stat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#22266;&#23450;&#32479;&#35745;&#25968;&#25454;&#65292;&#20063;&#19981;&#38480;&#21046;&#20110;&#39044;&#27979;&#26550;&#26500;&#12290;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#36716;&#25442;&#21644;&#39044;&#27979;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.16777</link><description>&lt;p&gt;
&#36890;&#36807;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Distribution Shift in Time Series Forecasting with Instance Normalization Flows. (arXiv:2401.16777v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16777
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#35299;&#20915;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#22266;&#23450;&#32479;&#35745;&#25968;&#25454;&#65292;&#20063;&#19981;&#38480;&#21046;&#20110;&#39044;&#27979;&#26550;&#26500;&#12290;&#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#23454;&#29616;&#36716;&#25442;&#21644;&#39044;&#27979;&#30340;&#32852;&#21512;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#38750;&#24179;&#31283;&#24615;&#65292;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#24456;&#22823;&#31243;&#24230;&#19978;&#38459;&#30861;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#26080;&#27861;&#22788;&#29702;&#36229;&#20986;&#31616;&#21333;&#32479;&#35745;&#30340;&#20559;&#31227;&#65292;&#35201;&#20040;&#19982;&#39044;&#27979;&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#29992;&#35299;&#32806;&#20844;&#24335;&#65292;&#19981;&#20381;&#36182;&#20110;&#22266;&#23450;&#32479;&#35745;&#25968;&#25454;&#65292;&#20063;&#19981;&#38480;&#21046;&#20110;&#39044;&#27979;&#26550;&#26500;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20844;&#24335;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#21452;&#23618;&#20248;&#21270;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#36716;&#25442;&#65288;&#22806;&#24490;&#29615;&#65289;&#21644;&#39044;&#27979;&#65288;&#20869;&#24490;&#29615;&#65289;&#30340;&#32852;&#21512;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#36716;&#25442;&#32780;&#35328;&#65292;&#23545;&#34920;&#36798;&#33021;&#21147;&#21644;&#21452;&#21521;&#24615;&#30340;&#29305;&#27530;&#35201;&#27714;&#20419;&#20351;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#20363;&#35268;&#33539;&#21270;&#27969;&#65288;IN-Flow&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#32593;&#32476;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#36716;&#25442;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#22987;&#32456;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to non-stationarity of time series, the distribution shift problem largely hinders the performance of time series forecasting. Existing solutions either fail for the shifts beyond simple statistics or the limited compatibility with forecasting models. In this paper, we propose a general decoupled formulation for time series forecasting, with no reliance on fixed statistics and no restriction on forecasting architectures. Then, we make such a formulation formalized into a bi-level optimization problem, to enable the joint learning of the transformation (outer loop) and forecasting (inner loop). Moreover, the special requirements of expressiveness and bi-direction for the transformation motivate us to propose instance normalization flows (IN-Flow), a novel invertible network for time series transformation. Extensive experiments demonstrate our method consistently outperforms state-of-the-art baselines on both synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20445;&#35777;&#21453;&#20107;&#23454;&#29983;&#25104;&#24402;&#22240;&#30340;&#29305;&#24449;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#40784;&#12290;</title><link>http://arxiv.org/abs/2310.01766</link><description>&lt;p&gt;
&#25506;&#32034;&#38024;&#23545;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21453;&#20107;&#23454;&#23545;&#40784;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Exploring Counterfactual Alignment Loss towards Human-centered AI. (arXiv:2310.01766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#20445;&#35777;&#21453;&#20107;&#23454;&#29983;&#25104;&#24402;&#22240;&#30340;&#29305;&#24449;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#20013;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#20351;&#24471;&#20154;&#20204;&#38590;&#20197;&#20449;&#20219;&#23427;&#20204;&#30340;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;-&#25209;&#35780;&#39046;&#22495;&#22914;&#21307;&#30103;&#20445;&#20581;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#35299;&#37322;&#24341;&#23548;&#23398;&#20064;&#26041;&#27861;&#25552;&#20986;&#20102;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#19982;&#20154;&#31867;&#19987;&#23478;&#26631;&#27880;&#30340;&#22270;&#20687;&#21306;&#22495;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#33719;&#24471;&#19968;&#20010;&#26412;&#36136;&#19978;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#25152;&#22522;&#20110;&#30340;&#27880;&#24847;&#21147;&#26144;&#23556;&#21487;&#33021;&#26080;&#27861;&#22240;&#26524;&#22320;&#24402;&#22240;&#20110;&#27169;&#22411;&#39044;&#27979;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#23545;&#20854;&#23545;&#40784;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#26032;&#22411;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#20107;&#23454;&#29983;&#25104;&#30340;&#22240;&#26524;&#24402;&#22240;&#33021;&#21147;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#65292;&#31216;&#20026;&#21453;&#20107;&#23454;&#23545;&#40784;&#25439;&#22833;&#65288;CF-Align&#65289;&#12290;&#36825;&#20010;&#25439;&#22833;&#20445;&#35777;&#20102;&#20998;&#31867;&#22120;&#30001;&#21453;&#20107;&#23454;&#29983;&#25104;&#24402;&#22240;&#30340;&#29305;&#24449;&#19982;&#20154;&#31867;&#19987;&#23478;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks have demonstrated impressive accuracy in supervised learning tasks. However, their lack of transparency makes it hard for humans to trust their results, especially in safe-critic domains such as healthcare. To address this issue, recent explanation-guided learning approaches proposed to align the gradient-based attention map to image regions annotated by human experts, thereby obtaining an intrinsically human-centered model. However, the attention map these methods are based on may fail to causally attribute the model predictions, thus compromising their validity for alignment. To address this issue, we propose a novel human-centered framework based on counterfactual generation. In particular, we utilize the counterfactual generation's ability for causal attribution to introduce a novel loss called the CounterFactual Alignment (CF-Align) loss. This loss guarantees that the features attributed by the counterfactual generation for the classifier align with the human 
&lt;/p&gt;</description></item><item><title>MoleculeSDE&#26159;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20960;&#20309;&#19982;2D&#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2305.18407</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Group Symmetric Stochastic Differential Equation Model for Molecule Multi-modal Pretraining. (arXiv:2305.18407v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18407
&lt;/p&gt;
&lt;p&gt;
MoleculeSDE&#26159;&#29992;&#20110;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#30340;&#32676;&#23545;&#31216;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104;3D&#20960;&#20309;&#19982;2D&#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#65292;&#23427;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#25552;&#39640;&#22522;&#20110; AI &#30340;&#33647;&#29289;&#21457;&#29616;&#24615;&#33021;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#37096;&#20998;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#20851;&#27880;&#21333;&#19968;&#30340;&#27169;&#24577;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#22823;&#21270;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65288;MI&#65289;&#21487;&#20197;&#22686;&#24378;&#20998;&#23376;&#34920;&#31034;&#33021;&#21147;&#12290;&#32780;&#29616;&#26377;&#30340;&#20998;&#23376;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#22522;&#20110;&#20174;&#25299;&#25169;&#21644;&#20960;&#20309;&#32534;&#30721;&#30340;&#34920;&#31034;&#31354;&#38388;&#26469;&#20272;&#35745; MI&#65292;&#22240;&#27492;&#20002;&#22833;&#20102;&#20998;&#23376;&#30340;&#20851;&#38190;&#32467;&#26500;&#20449;&#24687;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MoleculeSDE&#12290;MoleculeSDE&#21033;&#29992;&#32676;&#23545;&#31216;&#65288;&#22914; SE&#65288;3&#65289;-&#31561;&#21464;&#21644;&#21453;&#23556;-&#21453;&#23545;&#31216;&#65289;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30452;&#25509;&#29983;&#25104; 3D &#20960;&#20309;&#24418;&#29366;&#19982; 2D &#25299;&#25169;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#23427;&#19981;&#20165;&#33719;&#24471;&#26356;&#32039;&#30340;MI&#30028;&#38480;&#65292;&#32780;&#19988;&#36824;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#23384;&#20998;&#23376;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecule pretraining has quickly become the go-to schema to boost the performance of AI-based drug discovery. Naturally, molecules can be represented as 2D topological graphs or 3D geometric point clouds. Although most existing pertaining methods focus on merely the single modality, recent research has shown that maximizing the mutual information (MI) between such two modalities enhances the molecule representation ability. Meanwhile, existing molecule multi-modal pretraining approaches approximate MI based on the representation space encoded from the topology and geometry, thus resulting in the loss of critical structural information of molecules. To address this issue, we propose MoleculeSDE. MoleculeSDE leverages group symmetric (e.g., SE(3)-equivariant and reflection-antisymmetric) stochastic differential equation models to generate the 3D geometries from 2D topologies, and vice versa, directly in the input space. It not only obtains tighter MI bound but also enables prosperous dow
&lt;/p&gt;</description></item></channel></rss>