<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01632</link><description>&lt;p&gt;
&#36229;&#36234;&#23610;&#24230;&#65306;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#26080;&#36951;&#25022;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Beyond Lengthscales: No-regret Bayesian Optimisation With Unknown Hyperparameters Of Any Type
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01632
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#20219;&#24847;&#31867;&#22411;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#65292;&#24182;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#38656;&#35201;&#25311;&#21512;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#65292;&#32780;&#25311;&#21512;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#38656;&#35201;&#25351;&#23450;&#36229;&#21442;&#25968; - &#22823;&#37096;&#20998;&#29702;&#35770;&#25991;&#29486;&#20551;&#35774;&#36825;&#20123;&#36229;&#21442;&#25968;&#26159;&#24050;&#30693;&#30340;&#12290;&#20043;&#21069;&#30340;&#29702;&#35770;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#25968;&#25454;&#22312;&#31354;&#38388;&#20013;&#22343;&#21248;&#22635;&#20805;&#65292;&#32780;&#24120;&#29992;&#30340;&#39640;&#26031;&#36807;&#31243;&#36229;&#21442;&#25968;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#22120;&#21482;&#26377;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#25165;&#26159;&#19968;&#33268;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#65292;&#25968;&#25454;&#19981;&#19968;&#23450;&#28385;&#36275;&#36825;&#31181;&#22343;&#21248;&#22635;&#20805;&#30340;&#26465;&#20214;&#12290;&#30001;&#20110;&#26080;&#27861;&#20445;&#35777;&#36229;&#21442;&#25968;&#20272;&#35745;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#19988;&#36825;&#20123;&#36229;&#21442;&#25968;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#39640;&#26031;&#36807;&#31243;&#25311;&#21512;&#65292;&#22240;&#27492;&#23545;&#20855;&#26377;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20043;&#21069;&#25552;&#20986;&#30340;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#30340;&#31639;&#27861;&#20165;&#33021;&#22788;&#29702;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#26410;&#30693;&#38271;&#24230;&#23610;&#24230;&#12289;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#33539;&#25968;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#39057;&#29575;&#27966;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21629;&#21517;&#20026;HE-GP-UCB&#65292;&#23427;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#26080;&#36951;&#25022;&#29305;&#24615;&#30340;&#31639;&#27861;&#65292;&#22312;&#20855;&#26377;&#26410;&#30693;&#36229;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimisation requires fitting a Gaussian process model, which in turn requires specifying hyperparameters - most of the theoretical literature assumes those hyperparameters are known. The commonly used maximum likelihood estimator for hyperparameters of the Gaussian process is consistent only if the data fills the space uniformly, which does not have to be the case in Bayesian optimisation. Since no guarantees exist regarding the correctness of hyperparameter estimation, and those hyperparameters can significantly affect the Gaussian process fit, theoretical analysis of Bayesian optimisation with unknown hyperparameters is very challenging. Previously proposed algorithms with the no-regret property were only able to handle the special case of unknown lengthscales, reproducing kernel Hilbert space norm and applied only to the frequentist case. We propose a novel algorithm, HE-GP-UCB, which is the first algorithm enjoying the no-regret property in the case of unknown hyperparame
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#36866;&#24212;&#35266;&#27979;&#29305;&#24449;&#38598;&#65292;&#24182;&#23558;&#22635;&#20805;&#35268;&#21017;&#21644;&#22238;&#24402;&#27169;&#22411;&#21516;&#26102;&#23398;&#20064;&#65292;&#30456;&#27604;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#38750;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#24773;&#20917;&#19979;&#65292;&#26041;&#27861;&#23454;&#29616;&#20102;2-10%&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01543</link><description>&lt;p&gt;
&#38024;&#23545;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive Optimization for Prediction with Missing Data
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#36866;&#24212;&#35266;&#27979;&#29305;&#24449;&#38598;&#65292;&#24182;&#23558;&#22635;&#20805;&#35268;&#21017;&#21644;&#22238;&#24402;&#27169;&#22411;&#21516;&#26102;&#23398;&#20064;&#65292;&#30456;&#27604;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#25968;&#25454;&#38750;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#24773;&#20917;&#19979;&#65292;&#26041;&#27861;&#23454;&#29616;&#20102;2-10%&#30340;&#20934;&#30830;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20855;&#26377;&#32570;&#22833;&#26465;&#30446;&#30340;&#39044;&#27979;&#27169;&#22411;&#26102;&#65292;&#26368;&#24120;&#29992;&#21644;&#22810;&#21151;&#33021;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#27969;&#27700;&#32447;&#25216;&#26415;&#65292;&#39318;&#20808;&#22635;&#20805;&#32570;&#22833;&#26465;&#30446;&#65292;&#28982;&#21518;&#35745;&#31639;&#39044;&#27979;&#32467;&#26524;&#12290;&#26412;&#25991;&#23558;&#32570;&#22833;&#25968;&#25454;&#39044;&#27979;&#35270;&#20026;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#33258;&#36866;&#24212;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#65292;&#20854;&#20013;&#22238;&#24402;&#31995;&#25968;&#33021;&#22815;&#36866;&#24212;&#35266;&#27979;&#29305;&#24449;&#38598;&#12290;&#25105;&#20204;&#34920;&#26126;&#19968;&#20123;&#33258;&#36866;&#24212;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#31561;&#21516;&#20110;&#21516;&#26102;&#23398;&#20064;&#22635;&#20805;&#35268;&#21017;&#21644;&#19979;&#28216;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#32780;&#19981;&#26159;&#39034;&#24207;&#23398;&#20064;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32852;&#21512;&#22635;&#20805;-&#22238;&#24402;&#30340;&#35299;&#37322;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#25512;&#24191;&#21040;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#22312;&#25968;&#25454;&#38750;&#23436;&#20840;&#38543;&#26426;&#32570;&#22833;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26679;&#22806;&#20934;&#30830;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;2-10%&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
When training predictive models on data with missing entries, the most widely used and versatile approach is a pipeline technique where we first impute missing entries and then compute predictions. In this paper, we view prediction with missing data as a two-stage adaptive optimization problem and propose a new class of models, adaptive linear regression models, where the regression coefficients adapt to the set of observed features. We show that some adaptive linear regression models are equivalent to learning an imputation rule and a downstream linear regression model simultaneously instead of sequentially. We leverage this joint-impute-then-regress interpretation to generalize our framework to non-linear models. In settings where data is strongly not missing at random, our methods achieve a 2-10% improvement in out-of-sample accuracy.
&lt;/p&gt;</description></item><item><title>FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.20261</link><description>&lt;p&gt;
FABind+: &#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#22686;&#24378;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20261
&lt;/p&gt;
&lt;p&gt;
FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#25216;&#26415;&#20381;&#36182;&#20110;&#21463;&#29289;&#29702;&#21407;&#29702;&#25903;&#37197;&#30340;&#24191;&#27867;&#37319;&#26679;&#21644;&#27169;&#25311;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21069;&#26223;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#22686;&#38271;&#12290;&#24314;&#31435;&#22312;FABind&#30340;&#22522;&#30784;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind+&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22823;&#25552;&#21319;&#20854;&#21069;&#36523;&#24615;&#33021;&#30340;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30830;&#23450;&#21475;&#34955;&#39044;&#27979;&#26159;&#20998;&#23376;&#23545;&#25509;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23545;&#25509;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23545;&#25509;&#27169;&#22359;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#20854;&#23039;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#20256;&#32479;&#37319;&#26679;/&#29983;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#32479;&#35745;&#25968;&#25454;&#22256;&#38590;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16707</link><description>&lt;p&gt;
&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Domain Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#32479;&#35745;&#25968;&#25454;&#22256;&#38590;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#20851;&#20110;&#29992;&#20110;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#35752;&#35770;&#20102;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65288;DIL&#65289;&#12290;&#22312;DIL&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#35266;&#23519;&#26032;&#39046;&#22495;&#19978;&#30340;&#26679;&#26412;&#12290;&#27169;&#22411;&#24517;&#39035;&#23545;&#25152;&#26377;&#39046;&#22495;&#19978;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#25105;&#20204;&#38656;&#35201;&#22312;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#20165;&#38388;&#27463;&#24615;&#22320;&#34987;&#35266;&#23519;&#30340;&#32422;&#26463;&#19979;&#25191;&#34892;DIL&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26497;&#31471;&#24773;&#20917;&#65292;&#21363;&#25105;&#20204;&#21482;&#26377;&#19968;&#20221;&#26469;&#33258;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21333;&#27425;DIL&#12290;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;&#29616;&#26377;&#30340;DIL&#26041;&#27861;&#22312;&#21333;&#27425;DIL&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#21508;&#31181;&#35843;&#26597;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26126;&#30830;&#20102;&#21333;&#27425;DIL&#30340;&#22256;&#38590;&#26159;&#30001;&#25209;&#24402;&#19968;&#21270;&#23618;&#20013;&#30340;&#32479;&#35745;&#25968;&#25454;&#24341;&#36215;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16707v1 Announce Type: cross  Abstract: Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our tech
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#33258;&#36866;&#24212;LPD&#38647;&#36798;&#27874;&#24418;&#65292;&#20197;&#22312;&#19981;&#34987;&#21457;&#29616;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36827;&#34892;&#27979;&#36317;&#21644;&#24863;&#30693;</title><link>https://arxiv.org/abs/2403.12254</link><description>&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#33258;&#36866;&#24212;LPD&#38647;&#36798;&#27874;&#24418;
&lt;/p&gt;
&lt;p&gt;
Adaptive LPD Radar Waveform Design with Generative Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12254
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#24335;&#28145;&#24230;&#23398;&#20064;&#35774;&#35745;&#33258;&#36866;&#24212;LPD&#38647;&#36798;&#27874;&#24418;&#65292;&#20197;&#22312;&#19981;&#34987;&#21457;&#29616;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#36827;&#34892;&#27979;&#36317;&#21644;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#36866;&#24212;&#29983;&#25104;&#20302;&#27010;&#29575;&#26816;&#27979;&#65288;LPD&#65289;&#38647;&#36798;&#27874;&#24418;&#65292;&#20351;&#20854;&#19982;&#20854;&#25805;&#20316;&#29615;&#22659;&#34701;&#20026;&#19968;&#20307;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#27874;&#24418;&#26088;&#22312;&#36981;&#24490;&#19968;&#20010;&#19982;&#29615;&#22659;&#20013;&#30340;&#26080;&#32447;&#30005;&#39057;&#29575;&#65288;RF&#65289;&#32972;&#26223;&#26080;&#27861;&#21306;&#20998;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20173;&#28982;&#26377;&#25928;&#29992;&#20110;&#27979;&#36317;&#21644;&#24863;&#30693;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#23545;&#25239;&#24615;&#23398;&#20064;&#26694;&#26550;&#65307;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#32593;&#32476;&#29983;&#25104;&#26088;&#22312;&#22256;&#24785;&#35780;&#35770;&#23478;&#32593;&#32476;&#30340;&#27874;&#24418;&#65292;&#35780;&#35770;&#23478;&#32593;&#32476;&#34987;&#20248;&#21270;&#20026;&#21306;&#20998;&#29983;&#25104;&#30340;&#27874;&#24418;&#19982;&#32972;&#26223;&#12290;&#20026;&#20102;&#30830;&#20445;&#25105;&#20204;&#29983;&#25104;&#30340;&#27874;&#24418;&#23545;&#20110;&#24863;&#30693;&#20173;&#28982;&#26377;&#25928;&#65292;&#25105;&#20204;&#24341;&#20837;&#24182;&#26368;&#23567;&#21270;&#20102;&#19968;&#20010;&#22522;&#20110;&#27169;&#31946;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#21040;&#29983;&#25104;&#30340;&#27874;&#24418;&#19978;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#29983;&#25104;&#30340;&#27874;&#24418;&#30340;&#21333;&#33033;&#20914;&#21487;&#26816;&#27979;&#24615;&#19982;&#20351;&#29992;&#21333;&#29420;&#35757;&#32451;&#30340;&#26816;&#27979;&#31070;&#32463;&#32593;&#32476;&#26469;&#27604;&#36739;&#20256;&#32479;LPD&#27874;&#24418;&#30340;&#24615;&#33021;&#26469;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;ge
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12254v1 Announce Type: cross  Abstract: We propose a novel, learning-based method for adaptively generating low probability of detection (LPD) radar waveforms that blend into their operating environment. Our waveforms are designed to follow a distribution that is indistinguishable from the ambient radio frequency (RF) background -- while still being effective at ranging and sensing. To do so, we use an unsupervised, adversarial learning framework; our generator network produces waveforms designed to confuse a critic network, which is optimized to differentiate generated waveforms from the background. To ensure our generated waveforms are still effective for sensing, we introduce and minimize an ambiguity function-based loss on the generated waveforms. We evaluate the performance of our method by comparing the single-pulse detectability of our generated waveforms with traditional LPD waveforms using a separately trained detection neural network. We find that our method can ge
&lt;/p&gt;</description></item><item><title>KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03101</link><description>&lt;p&gt;
KnowAgent: &#30693;&#35782;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22522;&#20110;LLM&#30340;Agent
&lt;/p&gt;
&lt;p&gt;
KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03101
&lt;/p&gt;
&lt;p&gt;
KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#26102;&#20173;&#26377;&#25152;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#19982;&#29615;&#22659;&#20114;&#21160;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#21160;&#20316;&#26102;&#12290;&#36825;&#31181;&#19981;&#36275;&#20027;&#35201;&#26469;&#33258;&#20110;&#35821;&#35328;Agent&#20013;&#32570;&#20047;&#20869;&#32622;&#21160;&#20316;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;&#35268;&#21010;&#36712;&#36857;&#65292;&#20174;&#32780;&#23548;&#33268;&#35268;&#21010;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KnowAgent&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#26469;&#22686;&#24378;LLM&#35268;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KnowAgent&#37319;&#29992;&#20102;&#19968;&#20010;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#19968;&#20010;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#38480;&#21046;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#34892;&#21160;&#36335;&#24452;&#65292;&#23454;&#29616;&#26356;&#21512;&#29702;&#30340;&#36712;&#36857;&#21512;&#25104;&#65292;&#36827;&#32780;&#25552;&#39640;&#35821;&#35328;Agent&#30340;&#35745;&#21010;&#24615;&#33021;&#12290;&#22522;&#20110;HotpotQA&#21644;ALFWorld&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19981;&#21516;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;Hamiltonian&#30340;&#26412;&#22320;&#24615;&#27979;&#35797;&#20316;&#20026;&#23646;&#24615;&#27979;&#35797;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#30830;&#23450;&#26410;&#30693;&#30340;$n$&#27604;&#29305;Hamiltonian&#26159;&#21542;&#26159;$k$&#23616;&#37096;&#30340;&#65292;&#36890;&#36807;&#23545;$H$&#30340;&#26102;&#38388;&#28436;&#21270;&#36827;&#34892;&#35775;&#38382;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.02968</link><description>&lt;p&gt;
Hamiltonian&#24615;&#36136;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Hamiltonian Property Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hamiltonian&#30340;&#26412;&#22320;&#24615;&#27979;&#35797;&#20316;&#20026;&#23646;&#24615;&#27979;&#35797;&#38382;&#39064;&#65292;&#37325;&#28857;&#22312;&#20110;&#30830;&#23450;&#26410;&#30693;&#30340;$n$&#27604;&#29305;Hamiltonian&#26159;&#21542;&#26159;$k$&#23616;&#37096;&#30340;&#65292;&#36890;&#36807;&#23545;$H$&#30340;&#26102;&#38388;&#28436;&#21270;&#36827;&#34892;&#35775;&#38382;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;Hamiltonian&#26412;&#22320;&#24615;&#27979;&#35797;&#20316;&#20026;&#19968;&#20010;&#23646;&#24615;&#27979;&#35797;&#38382;&#39064;&#65292;&#21363;&#30830;&#23450;&#19968;&#20010;&#26410;&#30693;&#30340;$n$&#27604;&#29305;Hamiltonian $H$&#26159;&#21542;&#26159;$k$&#23616;&#37096;&#30340;&#65292;&#25110;&#32773;&#19982;&#25152;&#26377;$k$&#23616;&#37096;Hamiltonian&#37117;&#30456;&#36317;$\varepsilon$&#65292;&#24182;&#36890;&#36807;&#23545;$H$&#30340;&#26102;&#38388;&#28436;&#21270;&#36827;&#34892;&#35775;&#38382;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02968v1 Announce Type: cross  Abstract: Locality is a fundamental feature of many physical time evolutions. Assumptions on locality and related structural properties also underlie recently proposed procedures for learning an unknown Hamiltonian from access to the induced time evolution. However, no protocols to rigorously test whether an unknown Hamiltonian is local were known. We investigate Hamiltonian locality testing as a property testing problem, where the task is to determine whether an unknown $n$-qubit Hamiltonian $H$ is $k$-local or $\varepsilon$-far from all $k$-local Hamiltonians, given access to the time evolution along $H$. First, we emphasize the importance of the chosen distance measure: With respect to the operator norm, a worst-case distance measure, incoherent quantum locality testers require $\tilde{\Omega}(2^n)$ many time evolution queries and an expected total evolution time of $\tilde{\Omega}(2^n / \varepsilon)$, and even coherent testers need $\Omega(2
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#21644;&#24212;&#29992;&#65292;&#36890;&#36807;&#25552;&#20986;&#20855;&#22791;&#19981;&#21464;&#24615;/&#31561;&#21464;&#24615;&#23646;&#24615;&#30340;&#20960;&#20309;GNN&#26469;&#26356;&#22909;&#22320;&#34920;&#24449;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#25299;&#25169;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.00485</link><description>&lt;p&gt;
&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#32508;&#36848;&#65306;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#21644;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
A Survey of Geometric Graph Neural Networks: Data Structures, Models and Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00485
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#21644;&#24212;&#29992;&#65292;&#36890;&#36807;&#25552;&#20986;&#20855;&#22791;&#19981;&#21464;&#24615;/&#31561;&#21464;&#24615;&#23646;&#24615;&#30340;&#20960;&#20309;GNN&#26469;&#26356;&#22909;&#22320;&#34920;&#24449;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#25299;&#25169;&#65292;&#24182;&#25552;&#20379;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#22270;&#26159;&#19968;&#31181;&#20855;&#26377;&#20960;&#20309;&#29305;&#24449;&#30340;&#29305;&#27530;&#22270;&#24418;&#65292;&#23545;&#20110;&#24314;&#27169;&#35768;&#22810;&#31185;&#23398;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#19968;&#33324;&#22270;&#19981;&#21516;&#65292;&#20960;&#20309;&#22270;&#36890;&#24120;&#20855;&#26377;&#24179;&#31227;&#12289;&#26059;&#36716;&#21644;&#21453;&#23556;&#31561;&#29289;&#29702;&#23545;&#31216;&#24615;&#65292;&#36825;&#20351;&#23427;&#20204;&#38590;&#20197;&#34987;&#24403;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26377;&#25928;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#20855;&#22791;&#19981;&#21464;&#24615;/&#31561;&#21464;&#24615;&#23646;&#24615;&#30340;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#20960;&#20309;&#22270;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#25299;&#25169;&#12290;&#37492;&#20110;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#23545;&#19982;&#20960;&#20309;GNN&#30456;&#20851;&#30340;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#21644;&#24212;&#29992;&#36827;&#34892;&#20840;&#38754;&#35843;&#26597;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#24517;&#35201;&#20294;&#31616;&#27905;&#30340;&#25968;&#23398;&#22522;&#30784;&#30693;&#35782;&#65292;&#25105;&#20204;&#20174;&#20960;&#20309;&#28040;&#24687;&#20256;&#36882;&#30340;&#35282;&#24230;&#25552;&#20379;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#24212;&#29992;&#31243;&#24207;&#20197;&#21450;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#20197;&#20419;&#36827;&#20197;&#21518;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00485v1 Announce Type: new  Abstract: Geometric graph is a special kind of graph with geometric features, which is vital to model many scientific problems. Unlike generic graphs, geometric graphs often exhibit physical symmetries of translations, rotations, and reflections, making them ineffectively processed by current Graph Neural Networks (GNNs). To tackle this issue, researchers proposed a variety of Geometric Graph Neural Networks equipped with invariant/equivariant properties to better characterize the geometry and topology of geometric graphs. Given the current progress in this field, it is imperative to conduct a comprehensive survey of data structures, models, and applications related to geometric GNNs. In this paper, based on the necessary but concise mathematical preliminaries, we provide a unified view of existing models from the geometric message passing perspective. Additionally, we summarize the applications as well as the related datasets to facilitate later 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#32422;$O(1/\epsilon)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#20808;&#21069;&#25991;&#29486;&#20013;&#24050;&#26377;&#30340;$O(1/\epsilon^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#26377;&#25152;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.16324</link><description>&lt;p&gt;
&#23454;&#29616;&#32422;$O(1/\epsilon)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#29992;&#20110;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Achieving $\tilde{O}(1/\epsilon)$ Sample Complexity for Constrained Markov Decision Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16324
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#23454;&#29616;&#20102;&#32422;$O(1/\epsilon)$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#30456;&#27604;&#20808;&#21069;&#25991;&#29486;&#20013;&#24050;&#26377;&#30340;$O(1/\epsilon^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#26377;&#25152;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#32422;&#26463;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;CMDP&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#22312;&#39034;&#24207;&#23398;&#20064;&#21644;&#20915;&#31574;&#20013;&#28385;&#36275;&#23433;&#20840;&#24615;&#25110;&#36164;&#28304;&#32422;&#26463;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#25317;&#26377;&#26377;&#38480;&#36164;&#28304;&#21644;&#26410;&#30693;&#36716;&#31227;&#27010;&#29575;&#30340;MDP&#12290;&#22312;&#27599;&#20010;&#38454;&#27573;&#65292;&#25105;&#20204;&#37319;&#21462;&#19968;&#20010;&#34892;&#21160;&#65292;&#25910;&#38598;&#22870;&#21169;&#24182;&#28040;&#32791;&#19968;&#20123;&#36164;&#28304;&#65292;&#25152;&#26377;&#20551;&#35774;&#37117;&#26159;&#26410;&#30693;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#38543;&#30528;&#26102;&#38388;&#23398;&#20064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36808;&#20986;&#20102;&#20026;CMDP&#38382;&#39064;&#25512;&#23548;&#20986;&#26368;&#20248;&#30340;&#38382;&#39064;&#30456;&#20851;&#20445;&#35777;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#23545;&#25968;&#36951;&#25022;&#30028;&#38480;&#65292;&#36825;&#36716;&#21270;&#20026;$O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#65292;&#20854;&#20013;$\kappa$&#26159;&#19968;&#20010;&#19982;&#38382;&#39064;&#30456;&#20851;&#30340;&#21442;&#25968;&#65292;&#20294;&#19982;$\epsilon$&#26080;&#20851;&#12290;&#25105;&#20204;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30028;&#38480;&#25913;&#36827;&#20102;&#20808;&#21069;&#25991;&#29486;&#20013;&#38024;&#23545;CMDP&#38382;&#39064;&#24314;&#31435;&#30340;$O(1/\epsilon^2)$&#26679;&#26412;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16324v1 Announce Type: new  Abstract: We consider the reinforcement learning problem for the constrained Markov decision process (CMDP), which plays a central role in satisfying safety or resource constraints in sequential learning and decision-making. In this problem, we are given finite resources and a MDP with unknown transition probabilities. At each stage, we take an action, collecting a reward and consuming some resources, all assumed to be unknown and need to be learned over time. In this work, we take the first step towards deriving optimal problem-dependent guarantees for the CMDP problems. We derive a logarithmic regret bound, which translates into a $O(\frac{\kappa}{\epsilon}\cdot\log^2(1/\epsilon))$ sample complexity bound, with $\kappa$ being a problem-dependent parameter, yet independent of $\epsilon$. Our sample complexity bound improves upon the state-of-art $O(1/\epsilon^2)$ sample complexity for CMDP problems established in the previous literature, in terms
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13148</link><description>&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Defending Jailbreak Prompts via In-Context Adversarial Game
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13148
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#38450;&#24481;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#29616;&#20986;&#22312;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#20013;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23545;&#20854;&#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#23545;&#36234;&#29425;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20173;&#28982;&#23384;&#22312;&#12290;&#21463;&#21040;&#28145;&#24230;&#23398;&#20064;&#20013;&#23545;&#25239;&#35757;&#32451;&#21644;LLM&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19978;&#19979;&#25991;&#23545;&#25239;&#28216;&#25103;(ICAG)&#26469;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#65292;&#26080;&#38656;&#36827;&#34892;&#24494;&#35843;&#12290;ICAG&#21033;&#29992;&#20195;&#29702;&#23398;&#20064;&#36827;&#34892;&#23545;&#25239;&#28216;&#25103;&#65292;&#26088;&#22312;&#21160;&#24577;&#25193;&#23637;&#30693;&#35782;&#20197;&#38450;&#24481;&#36234;&#29425;&#25915;&#20987;&#12290;&#19982;&#20381;&#36182;&#38745;&#24577;&#25968;&#25454;&#38598;&#30340;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ICAG&#37319;&#29992;&#36845;&#20195;&#36807;&#31243;&#26469;&#22686;&#24378;&#38450;&#24481;&#21644;&#25915;&#20987;&#20195;&#29702;&#12290;&#36825;&#19968;&#25345;&#32493;&#25913;&#36827;&#36807;&#31243;&#21152;&#24378;&#20102;&#23545;&#26032;&#29983;&#25104;&#30340;&#36234;&#29425;&#25552;&#31034;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#30740;&#31350;&#35777;&#23454;&#20102;ICAG&#30340;&#26377;&#25928;&#24615;&#65292;&#32463;&#30001;ICAG&#20445;&#25252;&#30340;LLMs&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#26174;&#33879;&#38477;&#20302;&#20102;&#36234;&#29425;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13148v1 Announce Type: new  Abstract: Large Language Models (LLMs) demonstrate remarkable capabilities across diverse applications. However, concerns regarding their security, particularly the vulnerability to jailbreak attacks, persist. Drawing inspiration from adversarial training in deep learning and LLM agent learning processes, we introduce the In-Context Adversarial Game (ICAG) for defending against jailbreaks without the need for fine-tuning. ICAG leverages agent learning to conduct an adversarial game, aiming to dynamically extend knowledge to defend against jailbreaks. Unlike traditional methods that rely on static datasets, ICAG employs an iterative process to enhance both the defense and attack agents. This continuous improvement process strengthens defenses against newly generated jailbreak prompts. Our empirical studies affirm ICAG's efficacy, where LLMs safeguarded by ICAG exhibit significantly reduced jailbreak success rates across various attack scenarios. Mo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#39044;&#35757;&#32451;Transformer(EPT)&#26694;&#26550;&#65292;&#33021;&#22815;&#32479;&#19968;&#22810;&#39046;&#22495;&#20998;&#23376;&#30340;&#20960;&#20309;&#23398;&#20064;&#65292;&#36890;&#36807;&#22359;&#22686;&#24378;&#34920;&#31034;&#21644;E(3)&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;3D&#32467;&#26500;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2402.12714</link><description>&lt;p&gt;
&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#39044;&#35757;&#32451;Transformer&#29992;&#20110;&#22810;&#22495;3D&#20998;&#23376;&#30340;&#32479;&#19968;&#20960;&#20309;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equivariant Pretrained Transformer for Unified Geometric Learning on Multi-Domain 3D Molecules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12714
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20855;&#26377;&#31561;&#21464;&#24615;&#30340;&#39044;&#35757;&#32451;Transformer(EPT)&#26694;&#26550;&#65292;&#33021;&#22815;&#32479;&#19968;&#22810;&#39046;&#22495;&#20998;&#23376;&#30340;&#20960;&#20309;&#23398;&#20064;&#65292;&#36890;&#36807;&#22359;&#22686;&#24378;&#34920;&#31034;&#21644;E(3)&#31561;&#21464;&#24615;&#23454;&#29616;&#26356;&#20934;&#30830;&#30340;3D&#32467;&#26500;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;3D&#20998;&#23376;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#31185;&#23398;&#24212;&#29992;&#20013;&#20855;&#26377;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#21162;&#21147;&#36890;&#24120;&#38598;&#20013;&#22312;&#29305;&#23450;&#39046;&#22495;&#65288;&#34507;&#30333;&#36136;&#25110;&#23567;&#20998;&#23376;&#65289;&#30340;&#27169;&#22411;&#39044;&#35757;&#32451;&#19978;&#65292;&#38169;&#22833;&#20102;&#21033;&#29992;&#36328;&#39046;&#22495;&#30693;&#35782;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31561;&#21464;&#39044;&#35757;&#32451;Transformer&#65288;EPT&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#21327;&#35843;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#30340;&#20960;&#20309;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;EPT&#36890;&#36807;&#22359;&#22686;&#24378;&#34920;&#31034;&#32479;&#19968;&#20102;&#22810;&#39046;&#22495;&#20998;&#23376;&#30340;&#20960;&#20309;&#24314;&#27169;&#65292;&#33021;&#22815;&#20851;&#27880;&#27599;&#20010;&#21407;&#23376;&#26356;&#24191;&#27867;&#30340;&#19978;&#19979;&#25991;&#12290;&#22312;Transformer&#26694;&#26550;&#19978;&#65292;EPT&#36827;&#19968;&#27493;&#36890;&#36807;E(3)&#31561;&#21464;&#24615;&#36827;&#34892;&#22686;&#24378;&#65292;&#20197;&#20419;&#36827;&#20934;&#30830;&#34920;&#31034;3D&#32467;&#26500;&#12290;EPT&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#21019;&#26032;&#26159;&#20854;&#22359;&#32423;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#36825;&#20801;&#35768;&#22312;&#21253;&#21547;&#23567;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#32852;&#21512;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12714v1 Announce Type: new  Abstract: Pretraining on a large number of unlabeled 3D molecules has showcased superiority in various scientific applications. However, prior efforts typically focus on pretraining models on a specific domain, either proteins or small molecules, missing the opportunity to leverage the cross-domain knowledge. To mitigate this gap, we introduce Equivariant Pretrained Transformer (EPT), a novel pretraining framework designed to harmonize the geometric learning of small molecules and proteins. To be specific, EPT unifies the geometric modeling of multi-domain molecules via the block-enhanced representation that can attend a broader context of each atom. Upon transformer framework, EPT is further enhanced with E(3) equivariance to facilitate the accurate representation of 3D structures. Another key innovation of EPT is its block-level pretraining task, which allows for joint pretraining on datasets comprising both small molecules and proteins. Experim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23485;&#26494;&#20551;&#35774;&#19979;&#30340;&#38543;&#26426;&#20248;&#21270;&#20013;Adam&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#65292;Adam&#31639;&#27861;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#27010;&#29575;&#39640;&#25928;&#22320;&#23547;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#19982;&#20854;&#20182;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#30456;&#27604;&#65292;Adam&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#65292;&#26080;&#38656;&#35843;&#25972;&#27493;&#38271;&#21644;&#38382;&#39064;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2402.03982</link><description>&lt;p&gt;
&#22312;&#23485;&#26494;&#20551;&#35774;&#19979;&#20851;&#20110;&#38543;&#26426;&#20248;&#21270;&#20013;Adam&#25910;&#25947;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Convergence of Adam for Stochastic Optimization under Relaxed Assumptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03982
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#23485;&#26494;&#20551;&#35774;&#19979;&#30340;&#38543;&#26426;&#20248;&#21270;&#20013;Adam&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#36825;&#20010;&#27169;&#22411;&#19979;&#65292;Adam&#31639;&#27861;&#21487;&#20197;&#20197;&#36739;&#39640;&#30340;&#27010;&#29575;&#39640;&#25928;&#22320;&#23547;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#19982;&#20854;&#20182;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#30456;&#27604;&#65292;Adam&#31639;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#65292;&#26080;&#38656;&#35843;&#25972;&#27493;&#38271;&#21644;&#38382;&#39064;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#24615;&#21160;&#37327;&#35780;&#20272;&#65288;Adam&#65289;&#31639;&#27861;&#22312;&#35757;&#32451;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#22312;&#38750;&#20984;&#20809;&#28369;&#22330;&#26223;&#19979;&#65292;&#29305;&#21035;&#26159;&#22312;&#21487;&#33021;&#23384;&#22312;&#26080;&#30028;&#26799;&#24230;&#21644;&#20223;&#23556;&#26041;&#24046;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;Adam&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#26465;&#20214;&#19979;&#30340;&#26222;&#36890;Adam&#31639;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#22122;&#22768;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#25511;&#21046;&#30528;&#20223;&#23556;&#26041;&#24046;&#22122;&#22768;&#12289;&#26377;&#30028;&#22122;&#22768;&#21644;&#27425;&#39640;&#26031;&#22122;&#22768;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36825;&#20010;&#36890;&#29992;&#22122;&#22768;&#27169;&#22411;&#19979;&#65292;Adam&#31639;&#27861;&#21487;&#20197;&#20197;$\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$&#30340;&#27010;&#29575;&#39640;&#25928;&#22320;&#23547;&#25214;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#65292;&#20854;&#20013;$T$&#34920;&#31034;&#24635;&#36845;&#20195;&#27425;&#25968;&#65292;&#19982;&#38543;&#26426;&#19968;&#38454;&#31639;&#27861;&#30340;&#26356;&#24213;&#25928;&#29575;&#30456;&#21305;&#37197;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#22312;&#30456;&#21516;&#26465;&#20214;&#19979;&#65292;Adam&#31639;&#27861;&#26080;&#38656;&#35843;&#25972;&#27493;&#38271;&#21644;&#20219;&#20309;&#38382;&#39064;&#21442;&#25968;&#65292;&#20855;&#26377;&#27604;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#22909;&#30340;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Adaptive Momentum Estimation (Adam) algorithm is highly effective in training various deep learning tasks. Despite this, there's limited theoretical understanding for Adam, especially when focusing on its vanilla form in non-convex smooth scenarios with potential unbounded gradients and affine variance noise. In this paper, we study vanilla Adam under these challenging conditions. We introduce a comprehensive noise model which governs affine variance noise, bounded noise and sub-Gaussian noise. We show that Adam can find a stationary point with a $\mathcal{O}(\text{poly}(\log T)/\sqrt{T})$ rate in high probability under this general noise model where $T$ denotes total number iterations, matching the lower rate of stochastic first-order algorithms up to logarithm factors. More importantly, we reveal that Adam is free of tuning step-sizes with any problem-parameters, yielding a better adaptation property than the Stochastic Gradient Descent under the same conditions. We also provide 
&lt;/p&gt;</description></item><item><title>&#22312;&#37327;&#21270;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;&#27169;&#25311;&#37327;&#21270;&#35823;&#24046;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30828;&#20214;&#23454;&#29616;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.02593</link><description>&lt;p&gt;
&#22312;&#37327;&#21270;&#22122;&#22768;&#29615;&#22659;&#20013;&#21033;&#29992;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02593
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;&#27169;&#25311;&#37327;&#21270;&#35823;&#24046;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30828;&#20214;&#23454;&#29616;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#27169;&#25311;&#31995;&#32479;&#22266;&#26377;&#22320;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20687;GELU&#21644;SiLU&#36825;&#26679;&#30340;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#31283;&#20581;&#22320;&#20256;&#25773;&#26799;&#24230;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#26222;&#36941;&#23384;&#22312;&#20110;&#25152;&#26377;&#27169;&#25311;&#31995;&#32479;&#20013;&#30340;&#27169;&#25311;&#37327;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#22312;&#37327;&#21270;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#21367;&#31215;&#12289;&#32447;&#24615;&#21644;Transformer&#32593;&#32476;&#30340;&#20998;&#26512;&#21644;&#35757;&#32451;&#12290;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#20462;&#27491;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#65292;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#22312;&#25239;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#19982;ReLU&#30456;&#27604;&#65292;&#22312;&#25509;&#36817;&#38646;&#26102;&#26799;&#24230;&#35823;&#24046;&#39640;&#20986;100&#20493;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20855;&#26377;&#39640;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#30340;&#30828;&#20214;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models. We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems. We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise. Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations. As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero. Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02316</link><description>&lt;p&gt;
&#20320;&#30340;&#25193;&#25955;&#27169;&#22411;&#23454;&#38469;&#19978;&#26159;&#19968;&#20010;&#21487;&#35777;&#26126;&#40065;&#26834;&#30340;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
Your Diffusion Model is Secretly a Certifiably Robust Classifier
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02316
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#31216;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20102;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#36825;&#20123;NDCs&#26174;&#31034;&#20986;&#21331;&#36234;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#25193;&#25955;&#27169;&#22411;&#34987;&#20316;&#20026;&#40065;&#26834;&#20998;&#31867;&#30340;&#29983;&#25104;&#22120;&#20998;&#31867;&#22120;&#25152;&#37319;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25193;&#25955;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#32570;&#20047;&#65292;&#36825;&#35753;&#25105;&#20204;&#24576;&#30097;&#23427;&#20204;&#26159;&#21542;&#20250;&#23481;&#26131;&#21463;&#21040;&#26410;&#26469;&#26356;&#24378;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#23478;&#26063;&#65292;&#21629;&#21517;&#20026;&#22122;&#22768;&#25193;&#25955;&#20998;&#31867;&#22120;&#65288;NDCs&#65289;&#65292;&#20854;&#20855;&#26377;&#26368;&#26032;&#30340;&#21487;&#35777;&#26126;&#30340;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#36825;&#20123;&#20998;&#24067;&#30340;&#35777;&#25454;&#19979;&#30028;&#65288;ELBOs&#65289;&#65292;&#21033;&#29992;ELBO&#36817;&#20284;&#20284;&#28982;&#24230;&#37327;&#65292;&#24182;&#20351;&#29992;&#36125;&#21494;&#26031;&#23450;&#29702;&#35745;&#31639;&#20998;&#31867;&#27010;&#29575;&#65292;&#23558;&#25193;&#25955;&#20998;&#31867;&#22120;&#25512;&#24191;&#21040;&#20998;&#31867;&#39640;&#26031;&#21463;&#25439;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#25512;&#24191;&#30340;&#25193;&#25955;&#20998;&#31867;&#22120;&#19982;&#38543;&#26426;&#24179;&#28369;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#26500;&#24314;&#20855;&#26377;&#38750;&#24120;&#37327;Lipschitzness&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;NDCs&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20855;&#26377;&#21331;&#36234;&#30340;&#35748;&#35777;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#36798;&#21040;80%&#30340;...
&lt;/p&gt;
&lt;p&gt;
Diffusion models are recently employed as generative classifiers for robust classification. However, a comprehensive theoretical understanding of the robustness of diffusion classifiers is still lacking, leading us to question whether they will be vulnerable to future stronger attacks. In this study, we propose a new family of diffusion classifiers, named Noised Diffusion Classifiers~(NDCs), that possess state-of-the-art certified robustness. Specifically, we generalize the diffusion classifiers to classify Gaussian-corrupted data by deriving the evidence lower bounds (ELBOs) for these distributions, approximating the likelihood using the ELBO, and calculating classification probabilities via Bayes' theorem. We integrate these generalized diffusion classifiers with randomized smoothing to construct smoothed classifiers possessing non-constant Lipschitzness. Experimental results demonstrate the superior certified robustness of our proposed NDCs. Notably, we are the first to achieve 80\%
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.00849</link><description>&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65306;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;
&lt;/p&gt;
&lt;p&gt;
Score-based Causal Representation Learning: Linear and General Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00849
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#31639;&#27861;&#31867;&#65292;&#29992;&#20110;&#24178;&#39044;&#33539;&#22260;&#20869;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65292;&#28085;&#30422;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#36716;&#21270;&#12290;&#31639;&#27861;&#20445;&#35777;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#21019;&#36896;&#24615;&#22320;&#23558;&#24471;&#20998;&#20989;&#25968;&#19982;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#38024;&#23545;&#19968;&#33324;&#38750;&#21442;&#25968;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#21644;&#23558;&#28508;&#22312;&#21464;&#37327;&#26144;&#23556;&#21040;&#35266;&#27979;&#21464;&#37327;&#30340;&#26410;&#30693;&#36716;&#21270;&#65292;&#30740;&#31350;&#20102;&#22522;&#20110;&#24178;&#39044;&#30340;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#65288;CRL&#65289;&#12290;&#30740;&#31350;&#20102;&#32447;&#24615;&#21644;&#19968;&#33324;&#30340;&#36716;&#21270;&#12290;&#36825;&#31687;&#35770;&#25991;&#21516;&#26102;&#35752;&#35770;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#21487;&#35782;&#21035;&#24615;&#26159;&#25351;&#30830;&#23450;&#31639;&#27861;&#19981;&#30456;&#20851;&#30340;&#26465;&#20214;&#65292;&#20197;&#30830;&#20445;&#24674;&#22797;&#30495;&#23454;&#30340;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#21644;&#28508;&#22312;&#22240;&#26524;&#22270;&#12290;&#23454;&#29616;&#24615;&#26159;&#25351;&#31639;&#27861;&#26041;&#38754;&#65292;&#35299;&#20915;&#35774;&#35745;&#31639;&#27861;&#26469;&#23454;&#29616;&#21487;&#35782;&#21035;&#20445;&#35777;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23558;&#24471;&#20998;&#20989;&#25968;&#65288;&#21363;&#23494;&#24230;&#20989;&#25968;&#23545;&#25968;&#30340;&#26799;&#24230;&#65289;&#19982;CRL&#20043;&#38388;&#24314;&#31435;&#26032;&#32852;&#31995;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#24471;&#20998;&#20026;&#22522;&#30784;&#30340;&#31639;&#27861;&#31867;&#65292;&#30830;&#20445;&#20102;&#21487;&#35782;&#21035;&#24615;&#21644;&#23454;&#29616;&#24615;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#19987;&#27880;&#20110;&#32447;&#24615;&#36716;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#27599;&#20010;n&#20010;&#38543;&#26426;&#30828;&#24178;&#39044;&#19979;&#35813;&#36716;&#21270;&#30340;&#22240;&#26524;&#34920;&#31034;&#21487;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses intervention-based causal representation learning (CRL) under a general nonparametric latent causal model and an unknown transformation that maps the latent variables to the observed variables. Linear and general transformations are investigated. The paper addresses both the \emph{identifiability} and \emph{achievability} aspects. Identifiability refers to determining algorithm-agnostic conditions that ensure recovering the true latent causal variables and the latent causal graph underlying them. Achievability refers to the algorithmic aspects and addresses designing algorithms that achieve identifiability guarantees. By drawing novel connections between \emph{score functions} (i.e., the gradients of the logarithm of density functions) and CRL, this paper designs a \emph{score-based class of algorithms} that ensures both identifiability and achievability. First, the paper focuses on \emph{linear} transformations and shows that one stochastic hard intervention per n
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20351;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#25670;&#33073;&#37325;&#22797;&#29305;&#24449;&#20540;&#36755;&#20837;&#30340;&#38480;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#29305;&#24449;&#20540;&#30340;&#22343;&#21248;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2401.15603</link><description>&lt;p&gt;
&#29992;&#29305;&#24449;&#20540;&#20462;&#27491;&#25552;&#21319;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Expressive Power of Spectral Graph Neural Networks with Eigenvalue Correction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15603
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#21319;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20351;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#25670;&#33073;&#37325;&#22797;&#29305;&#24449;&#20540;&#36755;&#20837;&#30340;&#38480;&#21046;&#65292;&#24182;&#22686;&#24378;&#20102;&#29305;&#24449;&#20540;&#30340;&#22343;&#21248;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#20960;&#24180;&#20013;&#65292;&#29305;&#24449;&#20026;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#34920;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20551;&#35774;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#24444;&#27492;&#19981;&#21516;&#65292;&#22240;&#27492;&#26399;&#26395;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#20855;&#26377;&#24456;&#39640;&#30340;&#25311;&#21512;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#22312;&#23454;&#35777;&#19978;&#35266;&#23519;&#21040;&#35268;&#33539;&#21270;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#32463;&#24120;&#20855;&#26377;&#37325;&#22797;&#30340;&#29305;&#24449;&#20540;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#21487;&#36776;&#35748;&#29305;&#24449;&#20540;&#30340;&#25968;&#37327;&#22312;&#30830;&#23450;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#37492;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#65292;&#21487;&#20197;&#20351;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#25670;&#33073;&#37325;&#22797;&#29305;&#24449;&#20540;&#36755;&#20837;&#30340;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#29305;&#24449;&#20540;&#20462;&#27491;&#31574;&#30053;&#22686;&#24378;&#20102;&#29305;&#24449;&#20540;&#30340;&#22343;&#21248;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#36731;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#21463;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15603v2 Announce Type: replace  Abstract: In recent years, spectral graph neural networks, characterized by polynomial filters, have garnered increasing attention and have achieved remarkable performance in tasks such as node classification. These models typically assume that eigenvalues for the normalized Laplacian matrix are distinct from each other, thus expecting a polynomial filter to have a high fitting ability. However, this paper empirically observes that normalized Laplacian matrices frequently possess repeated eigenvalues. Moreover, we theoretically establish that the number of distinguishable eigenvalues plays a pivotal role in determining the expressive power of spectral graph neural networks. In light of this observation, we propose an eigenvalue correction strategy that can free polynomial filters from the constraints of repeated eigenvalue inputs. Concretely, the proposed eigenvalue correction strategy enhances the uniform distribution of eigenvalues, thus mit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2302.05614</link><description>&lt;p&gt;
&#20855;&#26377;&#21407;&#22411;&#30340;&#36328;&#39046;&#22495;&#38543;&#26426;&#39044;&#35757;&#32451;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cross-domain Random Pre-training with Prototypes for Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.05614
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CRPTpro&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#36827;&#34892;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#65292;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#65292;&#24182;&#23454;&#29616;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27492;&#24037;&#20316;&#24050;&#25552;&#20132;&#32473;IEEE&#36827;&#34892;&#21487;&#33021;&#30340;&#20986;&#29256;&#12290; CRPTpro&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22522;&#20110;&#22270;&#20687;&#30340;RL&#30340;&#36328;&#39046;&#22495;&#33258;&#30417;&#30563;&#38543;&#26426;&#39044;&#35757;&#32451;&#26694;&#26550;&#65292;&#21033;&#29992;&#21407;&#22411;&#12290; CRPTpro&#37319;&#29992;&#20102;&#36328;&#39046;&#22495;&#38543;&#26426;&#31574;&#30053;&#65292;&#21487;&#20197;&#36731;&#26494;&#24555;&#36895;&#22320;&#20174;&#22810;&#20010;&#39046;&#22495;&#20013;&#25277;&#26679;&#22810;&#26679;&#21270;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;&#39044;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#22312;&#25439;&#22833;&#36827;&#34892;&#21407;&#22411;&#34920;&#31034;&#23398;&#20064;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#39044;&#35757;&#32451;&#26377;&#25928;&#19988;&#36890;&#29992;&#30340;&#32534;&#30721;&#22120;&#12290;&#22312;&#27809;&#26377;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#36328;&#39046;&#22495;&#32534;&#30721;&#22120;&#21487;&#20197;&#39640;&#25928;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20013;&#23450;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19979;&#28216;&#35270;&#35273;&#25511;&#21046;RL&#20219;&#21153;&#12290; &#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#22914;APT&#21644;Proto-RL&#30456;&#27604;&#65292;CRP
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.05614v2 Announce Type: replace-cross  Abstract: This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. Task-agnostic cross-domain pre-training shows great potential in image-based Reinforcement Learning (RL) but poses a big challenge. In this paper, we propose CRPTpro, a Cross-domain self-supervised Random Pre-Training framework with prototypes for image-based RL. CRPTpro employs cross-domain random policy to easily and quickly sample diverse data from multiple domains, to improve pre-training efficiency. Moreover, prototypical representation learning with a novel intrinsic loss is proposed to pre-train an effective and generic encoder across different domains. Without finetuning, the cross-domain encoder can be implemented for challenging downstream visual-control RL tasks defined in different domains efficiently. Compared with prior arts like APT and Proto-RL, CRP
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;</title><link>http://arxiv.org/abs/2401.16412</link><description>&lt;p&gt;
&#23398;&#20064;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#36827;&#34892;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
Learning to Manipulate under Limited Information. (arXiv:2401.16412v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#26377;&#38480;&#20449;&#24687;&#26465;&#20214;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#65292;&#21457;&#29616;&#26576;&#20123;&#25237;&#31080;&#26041;&#27861;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#23481;&#26131;&#34987;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#19981;&#23481;&#26131;&#34987;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#32463;&#20856;&#32467;&#26524;&#65292;&#20219;&#20309;&#21512;&#29702;&#30340;&#20559;&#22909;&#25237;&#31080;&#26041;&#27861;&#26377;&#26102;&#20250;&#32473;&#20010;&#20307;&#25552;&#20379;&#25253;&#21578;&#19981;&#30495;&#23454;&#20559;&#22909;&#30340;&#28608;&#21169;&#12290;&#23545;&#20110;&#27604;&#36739;&#25237;&#31080;&#26041;&#27861;&#26469;&#35828;&#65292;&#19981;&#21516;&#25237;&#31080;&#26041;&#27861;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26356;&#25110;&#32773;&#26356;&#23569;&#25269;&#25239;&#36825;&#31181;&#31574;&#30053;&#24615;&#25805;&#32437;&#24050;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#22312;&#19981;&#21516;&#35268;&#27169;&#19979;&#23545;&#38480;&#21046;&#20449;&#24687;&#19979;&#23398;&#20064;&#22914;&#20309;&#21033;&#29992;&#32473;&#23450;&#25237;&#31080;&#26041;&#27861;&#36827;&#34892;&#25805;&#32437;&#30340;&#25104;&#21151;&#31243;&#24230;&#26469;&#34913;&#37327;&#25805;&#32437;&#30340;&#25269;&#25239;&#21147;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#23558;&#36817;40,000&#20010;&#19981;&#21516;&#35268;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23545;&#25239;8&#31181;&#19981;&#21516;&#30340;&#25237;&#31080;&#26041;&#27861;&#65292;&#22312;6&#31181;&#38480;&#21046;&#20449;&#24687;&#24773;&#20917;&#19979;&#65292;&#36827;&#34892;&#21253;&#21547;5-21&#21517;&#36873;&#27665;&#21644;3-6&#21517;&#20505;&#36873;&#20154;&#30340;&#22996;&#21592;&#20250;&#35268;&#27169;&#36873;&#20030;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#25237;&#31080;&#26041;&#27861;&#65292;&#22914;Borda&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#21487;&#20197;&#34987;&#31070;&#32463;&#32593;&#32476;&#39640;&#24230;&#25805;&#32437;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#65292;&#22914;Instant Runoff&#26041;&#27861;&#65292;&#34429;&#28982;&#34987;&#19968;&#20010;&#29702;&#24819;&#30340;&#25805;&#32437;&#32773;&#21033;&#28070;&#21270;&#25805;&#32437;&#65292;&#20294;&#22312;&#26377;&#38480;&#20449;&#24687;&#19979;&#19981;&#20250;&#21463;&#21040;&#25805;&#32437;&#12290;
&lt;/p&gt;
&lt;p&gt;
By classic results in social choice theory, any reasonable preferential voting method sometimes gives individuals an incentive to report an insincere preference. The extent to which different voting methods are more or less resistant to such strategic manipulation has become a key consideration for comparing voting methods. Here we measure resistance to manipulation by whether neural networks of varying sizes can learn to profitably manipulate a given voting method in expectation, given different types of limited information about how other voters will vote. We trained nearly 40,000 neural networks of 26 sizes to manipulate against 8 different voting methods, under 6 types of limited information, in committee-sized elections with 5-21 voters and 3-6 candidates. We find that some voting methods, such as Borda, are highly manipulable by networks with limited information, while others, such as Instant Runoff, are not, despite being quite profitably manipulated by an ideal manipulator with
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.04364</link><description>&lt;p&gt;
SoK&#65306;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
SoK: Facial Deepfake Detectors. (arXiv:2401.04364v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#36805;&#36895;&#25104;&#20026;&#23545;&#31038;&#20250;&#26500;&#25104;&#28145;&#36828;&#21644;&#20005;&#37325;&#23041;&#32961;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20027;&#35201;&#30001;&#20110;&#20854;&#26131;&#20110;&#21046;&#20316;&#21644;&#20256;&#25773;&#12290;&#36825;&#31181;&#24773;&#20917;&#21152;&#36895;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#39564;&#35777;&#26102; heavily &#20381;&#36182;&#23454;&#39564;&#23460;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#22320;&#35753;&#23427;&#20204;&#24212;&#23545;&#26032;&#39062;&#12289;&#26032;&#20852;&#21644;&#23454;&#38469;&#30340;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#12290;&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#24191;&#27867;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#26681;&#25454;&#20960;&#20010;&#20851;&#38190;&#26631;&#20934;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#26631;&#20934;&#23558;&#36825;&#20123;&#26816;&#27979;&#22120;&#20998;&#20026; 4 &#20010;&#39640;&#32423;&#32452;&#21035;&#21644; 13 &#20010;&#32454;&#31890;&#24230;&#23376;&#32452;&#21035;&#65292;&#37117;&#36981;&#24490;&#19968;&#20010;&#32479;&#19968;&#30340;&#26631;&#20934;&#27010;&#24565;&#26694;&#26550;&#12290;&#36825;&#31181;&#20998;&#31867;&#21644;&#26694;&#26550;&#25552;&#20379;&#20102;&#23545;&#24433;&#21709;&#26816;&#27979;&#22120;&#21151;&#25928;&#30340;&#22240;&#32032;&#30340;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#23545; 16 &#20010;&#20027;&#35201;&#30340;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26222;&#36866;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#40657;&#30418;&#25915;&#20987;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#20998;&#25955;&#32593;&#32476;&#30340;&#36890;&#20449;&#39640;&#25928;&#31639;&#27861;PISCO, &#36890;&#36807;&#27010;&#29575;&#24615;&#30340;&#20195;&#29702;&#38388;&#21644;&#20195;&#29702;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25928;&#29575;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#25240;&#34935;&#12290;</title><link>http://arxiv.org/abs/2311.18787</link><description>&lt;p&gt;
&#36890;&#20449;&#39640;&#25928;&#30340;&#21322;&#20998;&#25955;&#32593;&#32476;&#32852;&#37030;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Optimization over Semi-Decentralized Networks. (arXiv:2311.18787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.18787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21322;&#20998;&#25955;&#32593;&#32476;&#30340;&#36890;&#20449;&#39640;&#25928;&#31639;&#27861;PISCO, &#36890;&#36807;&#27010;&#29575;&#24615;&#30340;&#20195;&#29702;&#38388;&#21644;&#20195;&#29702;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25928;&#29575;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#25240;&#34935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#30340;&#32852;&#37030;&#21644;&#20998;&#25955;&#24335;&#23398;&#20064;&#20013;&#65292;&#36890;&#20449;&#25928;&#29575;&#26159;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#29942;&#39048;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#20998;&#25955;&#36890;&#20449;&#21327;&#35758;&#19979;&#30340;&#36890;&#20449;&#39640;&#25928;&#31639;&#27861;PISCO&#65292;&#36890;&#36807;&#27010;&#29575;&#24615;&#30340;&#20195;&#29702;&#38388;&#21644;&#20195;&#29702;&#19982;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#36890;&#20449;&#65292;&#23454;&#29616;&#20102;&#36890;&#20449;&#25928;&#29575;&#19982;&#25910;&#25947;&#36895;&#24230;&#30340;&#25240;&#34935;&#12290;PISCO&#31639;&#27861;&#36890;&#36807;&#26799;&#24230;&#36861;&#36394;&#21644;&#22810;&#20010;&#26412;&#22320;&#26356;&#26032;&#20445;&#35777;&#20102;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;PISCO&#31639;&#27861;&#22312;&#38750;&#20984;&#38382;&#39064;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#25968;&#37327;&#26041;&#38754;&#65292;PISCO&#31639;&#27861;&#20855;&#26377;&#32447;&#24615;&#21152;&#36895;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
In large-scale federated and decentralized learning, communication efficiency is one of the most challenging bottlenecks. While gossip communication -- where agents can exchange information with their connected neighbors -- is more cost-effective than communicating with the remote server, it often requires a greater number of communication rounds, especially for large and sparse networks. To tackle the trade-off, we examine the communication efficiency under a semi-decentralized communication protocol, in which agents can perform both agent-to-agent and agent-to-server communication in a probabilistic manner. We design a tailored communication-efficient algorithm over semi-decentralized networks, referred to as PISCO, which inherits the robustness to data heterogeneity thanks to gradient tracking and allows multiple local updates for saving communication. We establish the convergence rate of PISCO for nonconvex problems and show that PISCO enjoys a linear speedup in terms of the number
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.19786</link><description>&lt;p&gt;
&#20174;&#22806;&#37096;&#21040;Swap&#36951;&#25022;2.0&#65306;&#38024;&#23545;&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#25928;&#32422;&#21270;&#21644;&#26080;&#30693;&#23545;&#25163;
&lt;/p&gt;
&lt;p&gt;
From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19786
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#32463;&#20856;&#30340;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#22312;&#36739;&#23569;&#30340;&#36718;&#25968;&#21644;&#26356;&#20302;&#30340;&#22797;&#26434;&#24230;&#19979;&#36798;&#21040;&#30456;&#21516;&#30340;Swap&#36951;&#25022;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20174;Swap&#36951;&#25022;&#26368;&#23567;&#21270;&#21040;&#22806;&#37096;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#32422;&#21270;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#34892;&#20026;&#31354;&#38388;&#30340;&#26377;&#38480;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#23384;&#22312;&#26576;&#20010;&#20551;&#35774;&#31867;&#30340;&#26080;&#22806;&#37096;&#36951;&#25022;&#31639;&#27861;&#65292;&#23601;&#24517;&#28982;&#23384;&#22312;&#30456;&#21516;&#31867;&#21035;&#30340;&#26080;Swap&#36951;&#25022;&#31639;&#27861;&#12290;&#23545;&#20110;&#23398;&#20064;&#19987;&#23478;&#24314;&#35758;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24847;&#21619;&#30528;&#21487;&#20197;&#20445;&#35777;&#22312;$\log(N)^{O(1/\epsilon)}$&#36718;&#21518;&#65292;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#20026;$O(N)$&#30340;&#24773;&#20917;&#19979;&#65292;Swap&#36951;&#25022;&#34987;&#38480;&#23450;&#20026;$\epsilon$&#65292;&#32780;Blum-Mansour&#21644;Stolz-Lugosi&#30340;&#32463;&#20856;&#32422;&#21270;&#26041;&#27861;&#38656;&#35201;$O(N/\epsilon^2)$&#36718;&#21644;&#33267;&#23569;$\Omega(N^2)$&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#20276;&#38543;&#30528;&#19968;&#20010;&#30456;&#20851;&#30340;&#19979;&#30028;&#65292;&#19982;[BM07]&#19981;&#21516;&#65292;&#36825;&#20010;&#19979;&#30028;&#36866;&#29992;&#20110;&#26080;&#30693;&#21644;$\ell_1$-&#21463;&#38480;&#30340;&#23545;&#25163;&#21644;&#21487;&#20197;&#21033;&#29992;&#36825;&#20010;&#19979;&#30028;&#30340;&#23398;&#20064;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a novel reduction from swap-regret minimization to external-regret minimization, which improves upon the classical reductions of Blum-Mansour [BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the space of actions. We show that, whenever there exists a no-external-regret algorithm for some hypothesis class, there must also exist a no-swap-regret algorithm for that same class. For the problem of learning with expert advice, our result implies that it is possible to guarantee that the swap regret is bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$ per iteration complexity, where $N$ is the number of experts, while the classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$ rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes with an associated lower bound, which -- in contrast to that in [BM07] -- holds for oblivious and $\ell_1$-constrained adversaries and learners that can emplo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#40784;&#24230;&#37327;&#21644;&#22343;&#21248;&#24615;&#24230;&#37327;&#26469;&#35299;&#20915;&#22270;&#39046;&#22495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18209</link><description>&lt;p&gt;
&#36229;&#21367;&#26354;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#23545;&#40784;&#21644;&#22806;&#22771;&#21516;&#26500;&#24615;
&lt;/p&gt;
&lt;p&gt;
Alignment and Outer Shell Isotropy for Hyperbolic Graph Contrastive Learning. (arXiv:2310.18209v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#65292;&#24182;&#35774;&#35745;&#20102;&#23545;&#40784;&#24230;&#37327;&#21644;&#22343;&#21248;&#24615;&#24230;&#37327;&#26469;&#35299;&#20915;&#22270;&#39046;&#22495;&#30340;&#38750;&#27431;&#20960;&#37324;&#24503;&#20960;&#20309;&#32467;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#30340;&#33258;&#30417;&#30563;&#22270;&#34920;&#31034;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#23545;&#27604;&#23398;&#20064;&#30340;&#23884;&#20837;&#34987;&#25490;&#21015;&#22312;&#19968;&#20010;&#36229;&#29699;&#38754;&#19978;&#65292;&#20174;&#32780;&#20351;&#24471;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#20313;&#24358;&#36317;&#31163;&#27979;&#37327;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#39046;&#22495;&#30340;&#28508;&#22312;&#20960;&#20309;&#32467;&#26500;&#65292;&#22914;&#22270;&#24418;&#65292;&#23637;&#29616;&#20102;&#39640;&#24230;&#38750;&#27431;&#20960;&#37324;&#24503;&#30340;&#29305;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#22270;&#23884;&#20837;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26377;&#25928;&#25429;&#25417;&#23618;&#27425;&#25968;&#25454;&#19981;&#21464;&#24615;&#20449;&#24687;&#30340;&#23545;&#40784;&#24230;&#37327;&#65292;&#21516;&#26102;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#22343;&#21248;&#24230;&#37327;&#26469;&#38450;&#27490;&#25152;&#35859;&#30340;&#32500;&#24230;&#22604;&#38519;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#65292;&#24517;&#39035;&#35299;&#20915;&#19982;&#26641;&#30340;&#23646;&#24615;&#30456;&#20851;&#30340;&#21494;&#23376;&#21644;&#39640;&#24230;&#23618;&#38754;&#30340;&#22343;&#21248;&#24615;&#65292;&#32780;&#22312;&#21452;&#26354;&#27969;&#24418;&#30340;&#29615;&#22659;&#31354;&#38388;&#20013;&#65292;&#36825;&#20123;&#27010;&#24565;&#36716;&#21270;&#20026;&#23545;&#21516;&#26500;&#24615;&#30340;&#26045;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning good self-supervised graph representations that are beneficial to downstream tasks is challenging. Among a variety of methods, contrastive learning enjoys competitive performance. The embeddings of contrastive learning are arranged on a hypersphere that enables the Cosine distance measurement in the Euclidean space. However, the underlying structure of many domains such as graphs exhibits highly non-Euclidean latent geometry. To this end, we propose a novel contrastive learning framework to learn high-quality graph embedding. Specifically, we design the alignment metric that effectively captures the hierarchical data-invariant information, as well as we propose a substitute of uniformity metric to prevent the so-called dimensional collapse. We show that in the hyperbolic space one has to address the leaf- and height-level uniformity which are related to properties of trees, whereas in the ambient space of the hyperbolic manifold, these notions translate into imposing an isotro
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#21152;&#26435;&#22240;&#26524;&#35268;&#21017;&#26469;&#20272;&#35745;&#21644;&#21152;&#24378;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.06746</link><description>&lt;p&gt;
&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65306;&#36890;&#36807;&#21152;&#26435;&#22240;&#26524;&#35268;&#21017;&#22686;&#24378;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules. (arXiv:2310.06746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06746
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#21152;&#26435;&#22240;&#26524;&#35268;&#21017;&#26469;&#20272;&#35745;&#21644;&#21152;&#24378;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#26159;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#65292;&#24120;&#24120;&#38656;&#35201;&#20570;&#20986;&#39640;&#39118;&#38505;&#20915;&#31574;&#12290;&#21463;&#21040;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#24615;&#12289;&#25551;&#36848;&#24615;&#12289;&#30456;&#20851;&#24615;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25214;&#21040;&#25551;&#36848;&#28508;&#22312;&#23376;&#32676;&#30340;&#31934;&#32454;&#22240;&#26524;&#35268;&#21017;&#38598;&#26469;&#20272;&#35745;&#21644;&#22686;&#24378;&#25105;&#20204;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;&#12290;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#35268;&#21017;&#21457;&#29616;&#12289;&#35268;&#21017;&#36873;&#25321;&#21644;&#35268;&#21017;&#20998;&#26512;&#12290;&#22312;&#35268;&#21017;&#21457;&#29616;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#26862;&#26519;&#29983;&#25104;&#19968;&#32452;&#20855;&#26377;&#30456;&#24212;&#23376;&#32676;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#30340;&#22240;&#26524;&#35268;&#21017;&#27744;&#12290;&#36873;&#25321;&#38454;&#27573;&#20351;&#29992;D-&#23398;&#20064;&#26041;&#27861;&#20174;&#36825;&#20123;&#35268;&#21017;&#20013;&#36873;&#25321;&#23376;&#38598;&#65292;&#23558;&#20010;&#20307;&#27700;&#24179;&#30340;&#27835;&#30103;&#25928;&#24212;&#20316;&#20026;&#23376;&#32676;&#27700;&#24179;&#25928;&#24212;&#30340;&#32447;&#24615;&#32452;&#21512;&#36827;&#34892;&#35299;&#26500;&#12290;&#36825;&#26377;&#21161;&#20110;&#22238;&#31572;&#20043;&#21069;&#25991;&#29486;&#24573;&#35270;&#30340;&#38382;&#39064;&#65306;&#22914;&#26524;&#19968;&#20010;&#20010;&#20307;&#21516;&#26102;&#23646;&#20110;&#22810;&#20010;&#19981;&#21516;&#30340;&#27835;&#30103;&#23376;&#32676;&#65292;&#20250;&#24590;&#20040;&#26679;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
Interpretability is a key concern in estimating heterogeneous treatment effects using machine learning methods, especially for healthcare applications where high-stake decisions are often made. Inspired by the Predictive, Descriptive, Relevant framework of interpretability, we propose causal rule learning which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. Causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously bel
&lt;/p&gt;</description></item><item><title>ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17203</link><description>&lt;p&gt;
ComSD: &#22312;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#24179;&#34913;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
ComSD: Balancing Behavioral Quality and Diversity in Unsupervised Skill Discovery. (arXiv:2309.17203v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17203
&lt;/p&gt;
&lt;p&gt;
ComSD&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#24179;&#34913;&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#20013;&#30340;&#34892;&#20026;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#25216;&#33021;&#21457;&#29616;&#30340;&#29702;&#24819;&#26041;&#27861;&#33021;&#22815;&#22312;&#27809;&#26377;&#22806;&#37096;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#20135;&#29983;&#22810;&#26679;&#19988;&#21512;&#26684;&#30340;&#25216;&#33021;&#65292;&#21516;&#26102;&#21457;&#29616;&#30340;&#25216;&#33021;&#38598;&#33021;&#22815;&#20197;&#21508;&#31181;&#26041;&#24335;&#39640;&#25928;&#22320;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Contrastive multi-objectives Skill Discovery (ComSD)&#65292;&#36890;&#36807;&#26356;&#21512;&#29702;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#21644;&#21160;&#24577;&#21152;&#26435;&#30340;&#20869;&#22312;&#22870;&#21169;&#26469;&#20943;&#36731;&#21457;&#29616;&#30340;&#34892;&#20026;&#22312;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning diverse and qualified behaviors for utilization and adaptation without supervision is a key ability of intelligent creatures. Ideal unsupervised skill discovery methods are able to produce diverse and qualified skills in the absence of extrinsic reward, while the discovered skill set can efficiently adapt to downstream tasks in various ways. Maximizing the Mutual Information (MI) between skills and visited states can achieve ideal skill-conditioned behavior distillation in theory. However, it's difficult for recent advanced methods to well balance behavioral quality (exploration) and diversity (exploitation) in practice, which may be attributed to the unreasonable MI estimation by their rigid intrinsic reward design. In this paper, we propose Contrastive multi-objectives Skill Discovery (ComSD) which tries to mitigate the quality-versus-diversity conflict of discovered behaviors through a more reasonable MI estimation and a dynamically weighted intrinsic reward. ComSD proposes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;MIML&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#26080;&#26631;&#35760;&#32454;&#32990;&#22270;&#20687;&#19982;&#29983;&#29289;&#21147;&#23398;&#23646;&#24615;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#32454;&#32990;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#24418;&#24577;&#20449;&#24687;&#65292;&#23558;&#32454;&#32990;&#23646;&#24615;&#29702;&#35299;&#24471;&#26356;&#20840;&#38754;&#65292;&#30456;&#36739;&#20110;&#20165;&#32771;&#34385;&#21333;&#19968;&#25968;&#25454;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;98.3&#65285;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#30333;&#32454;&#32990;&#21644;&#32959;&#30244;&#32454;&#32990;&#20998;&#31867;&#20013;&#24471;&#21040;&#35777;&#26126;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.08421</link><description>&lt;p&gt;
MIML: &#36890;&#36807;&#24494;&#27969;&#25511;&#31995;&#32479;&#20869;&#30340;&#26426;&#26800;&#29305;&#24615;&#23545;&#39640;&#31934;&#24230;&#32454;&#32990;&#20998;&#31867;&#36827;&#34892;&#22810;&#37325;&#22270;&#20687;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MIML: Multiplex Image Machine Learning for High Precision Cell Classification via Mechanical Traits within Microfluidic Systems. (arXiv:2309.08421v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;MIML&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#23558;&#26080;&#26631;&#35760;&#32454;&#32990;&#22270;&#20687;&#19982;&#29983;&#29289;&#21147;&#23398;&#23646;&#24615;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#32454;&#32990;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#20102;&#24418;&#24577;&#20449;&#24687;&#65292;&#23558;&#32454;&#32990;&#23646;&#24615;&#29702;&#35299;&#24471;&#26356;&#20840;&#38754;&#65292;&#30456;&#36739;&#20110;&#20165;&#32771;&#34385;&#21333;&#19968;&#25968;&#25454;&#31867;&#22411;&#30340;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;98.3&#65285;&#30340;&#20998;&#31867;&#31934;&#24230;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#30333;&#32454;&#32990;&#21644;&#32959;&#30244;&#32454;&#32990;&#20998;&#31867;&#20013;&#24471;&#21040;&#35777;&#26126;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#26631;&#35760;&#32454;&#32990;&#20998;&#31867;&#26377;&#21161;&#20110;&#20026;&#36827;&#19968;&#27493;&#20351;&#29992;&#25110;&#26816;&#26597;&#25552;&#20379;&#21407;&#22987;&#32454;&#32990;&#65292;&#28982;&#32780;&#29616;&#26377;&#25216;&#26415;&#22312;&#29305;&#24322;&#24615;&#21644;&#36895;&#24230;&#26041;&#38754;&#24448;&#24448;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;MIML&#26469;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#35813;&#26550;&#26500;&#23558;&#26080;&#26631;&#35760;&#32454;&#32990;&#22270;&#20687;&#19982;&#29983;&#29289;&#21147;&#23398;&#23646;&#24615;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#27599;&#20010;&#32454;&#32990;&#22266;&#26377;&#30340;&#24191;&#38420;&#19988;&#24120;&#24120;&#34987;&#20302;&#20272;&#30340;&#24418;&#24577;&#20449;&#24687;&#12290;&#36890;&#36807;&#25972;&#21512;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#23545;&#32454;&#32990;&#23646;&#24615;&#26356;&#20840;&#38754;&#30340;&#29702;&#35299;&#65292;&#21033;&#29992;&#20102;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36890;&#24120;&#34987;&#20002;&#24323;&#30340;&#24418;&#24577;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#32454;&#32990;&#20998;&#31867;&#31934;&#24230;&#36798;&#21040;&#20102;&#24778;&#20154;&#30340;98.3&#65285;&#65292;&#22823;&#22823;&#20248;&#20110;&#20165;&#32771;&#34385;&#21333;&#19968;&#25968;&#25454;&#31867;&#22411;&#30340;&#27169;&#22411;&#12290;MIML&#24050;&#34987;&#35777;&#26126;&#22312;&#30333;&#32454;&#32990;&#21644;&#32959;&#30244;&#32454;&#32990;&#20998;&#31867;&#20013;&#26377;&#25928;&#65292;&#24182;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Label-free cell classification is advantageous for supplying pristine cells for further use or examination, yet existing techniques frequently fall short in terms of specificity and speed. In this study, we address these limitations through the development of a novel machine learning framework, Multiplex Image Machine Learning (MIML). This architecture uniquely combines label-free cell images with biomechanical property data, harnessing the vast, often underutilized morphological information intrinsic to each cell. By integrating both types of data, our model offers a more holistic understanding of the cellular properties, utilizing morphological information typically discarded in traditional machine learning models. This approach has led to a remarkable 98.3\% accuracy in cell classification, a substantial improvement over models that only consider a single data type. MIML has been proven effective in classifying white blood cells and tumor cells, with potential for broader applicatio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#23637;&#31034;&#20102;&#20854;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.14328</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Generative AI: A Survey. (arXiv:2308.14328v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14328
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#24378;&#21270;&#23398;&#20064;&#23637;&#31034;&#20102;&#20854;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#26159;&#26426;&#22120;&#23398;&#20064;&#30028;&#20013;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#37325;&#35201;&#20027;&#39064;&#65292;&#21487;&#20197;&#24433;&#21709;&#21040;&#35832;&#22810;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#25991;&#26412;&#29983;&#25104;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#12290;&#35757;&#32451;&#29983;&#25104;&#27169;&#22411;&#30340;&#20027;&#35201;&#33539;&#24335;&#26159;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#65292;&#36890;&#36807;&#20943;&#23567;&#27169;&#22411;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#25512;&#21160;&#23398;&#20064;&#22120;&#25429;&#25417;&#24182;&#36924;&#36817;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#31181;&#20844;&#24335;&#25104;&#21151;&#22320;&#24314;&#31435;&#20102;&#29983;&#25104;&#20219;&#21153;&#30340;&#30446;&#26631;&#65292;&#28982;&#32780;&#21364;&#26080;&#27861;&#28385;&#36275;&#20351;&#29992;&#32773;&#23545;&#29983;&#25104;&#27169;&#22411;&#30340;&#25152;&#26377;&#38656;&#27714;&#12290;&#24378;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#31454;&#20105;&#24615;&#36873;&#25321;&#65292;&#36890;&#36807;&#21019;&#24314;&#26032;&#30340;&#30446;&#26631;&#26469;&#27880;&#20837;&#26032;&#30340;&#35757;&#32451;&#20449;&#21495;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#24378;&#22823;&#21644;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#24341;&#20837;&#20154;&#31867;&#24402;&#32435;&#20559;&#22909;&#65292;&#22914;&#23545;&#25239;&#23398;&#20064;&#12289;&#25163;&#21160;&#35774;&#35745;&#35268;&#21017;&#21644;&#23398;&#20064;&#22870;&#21169;&#27169;&#22411;&#65292;&#20197;&#24314;&#31435;&#19968;&#20010;&#24615;&#33021;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Generative AI has been a long-standing essential topic in the machine learning community, which can impact a number of application areas like text generation and computer vision. The major paradigm to train a generative model is maximum likelihood estimation, which pushes the learner to capture and approximate the target data distribution by decreasing the divergence between the model distribution and the target distribution. This formulation successfully establishes the objective of generative tasks, while it is incapable of satisfying all the requirements that a user might expect from a generative model. Reinforcement learning, serving as a competitive option to inject new training signals by creating new objectives that exploit novel signals, has demonstrated its power and flexibility to incorporate human inductive bias from multiple angles, such as adversarial learning, hand-designed rules and learned reward model to build a performant model. Thereby, reinforcement learning ha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.12219</link><description>&lt;p&gt;
&#25193;&#23637;&#24615;&#21644;&#25351;&#23548;&#35843;&#20248;&#30340;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23436;&#25104;&#22810;&#31181;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Diffusion Language Models Can Perform Many Tasks with Scaling and Instruction-Finetuning. (arXiv:2308.12219v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#65292;&#21487;&#20197;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#33021;&#22815;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#20852;&#36215;&#24471;&#30410;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#19982;&#33258;&#22238;&#24402;&#27169;&#22411;&#30456;&#23218;&#32654;&#30340;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#20173;&#28982;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#22312;&#25968;&#25454;&#12289;&#35268;&#27169;&#21644;&#20219;&#21153;&#26041;&#38754;&#25193;&#23637;&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#26377;&#25928;&#20351;&#20854;&#25104;&#20026;&#24378;&#22823;&#30340;&#35821;&#35328;&#23398;&#20064;&#32773;&#12290;&#25105;&#20204;&#36890;&#36807;&#20808;&#36890;&#36807;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#39044;&#35757;&#32451;&#20174;&#22823;&#35268;&#27169;&#25968;&#25454;&#20013;&#33719;&#21462;&#30693;&#35782;&#65292;&#20877;&#36890;&#36807;&#25193;&#25955;&#36866;&#24212;&#23558;&#39044;&#35757;&#32451;&#30340;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#20026;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#24494;&#35843;&#21644;&#25351;&#23548;&#35843;&#20248;&#26469;&#21457;&#25496;&#20854;&#22312;&#35299;&#20915;&#36890;&#29992;&#35821;&#35328;&#20219;&#21153;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25193;&#23637;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#19979;&#28216;&#35821;&#35328;&#20219;&#21153;&#20013;&#25345;&#32493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent surge of generative AI has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream langua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#19968;&#20010;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#23458;&#25143;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.04996</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;&#24378;&#21270;&#23398;&#20064;&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning. (arXiv:2307.04996v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04996
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26041;&#27861;&#65292;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#21478;&#19968;&#31181;&#20351;&#29992;XGBoost&#31639;&#27861;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#12290;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#24182;&#22312;&#19968;&#20010;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#23458;&#25143;&#20013;&#36827;&#34892;&#20102;&#23454;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#25512;&#33616;&#22312;&#30452;&#25509;&#33829;&#38144;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#28608;&#21457;&#20102;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#24212;&#29992;&#26469;&#25552;&#21319;&#23458;&#25143;&#20307;&#39564;&#30340;&#30740;&#31350;&#21160;&#26426;&#12290;&#20363;&#22914;&#65292;&#22312;&#37329;&#34701;&#26381;&#21153;&#39046;&#22495;&#65292;&#20844;&#21496;&#21487;&#20197;&#36890;&#36807;&#21521;&#23458;&#25143;&#25552;&#20379;&#30456;&#20851;&#37329;&#34701;&#25991;&#31456;&#26469;&#22521;&#20859;&#20851;&#31995;&#65292;&#20419;&#36827;&#23458;&#25143;&#21442;&#19982;&#21644;&#20419;&#36827;&#30693;&#24773;&#30340;&#37329;&#34701;&#20915;&#31574;&#12290;&#23613;&#31649;&#19968;&#20123;&#26041;&#27861;&#19987;&#27880;&#20110;&#22522;&#20110;KG&#30340;&#25512;&#33616;&#31995;&#32479;&#20197;&#25913;&#36827;&#20869;&#23481;&#65292;&#20294;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;KG&#30340;&#25512;&#33616;&#31995;&#32479;&#26469;&#36827;&#34892;&#20915;&#31574;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#20010;&#24615;&#21270;&#25991;&#31456;&#25512;&#33616;&#26041;&#27861;&#65292;&#29992;&#20110;&#19968;&#23478;&#22823;&#22411;&#36328;&#22269;&#37329;&#34701;&#26381;&#21153;&#20844;&#21496;&#30340;&#19968;&#32452;&#23458;&#25143;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#20351;&#29992;XGBoost&#31639;&#27861;&#26469;&#21521;&#23458;&#25143;&#25512;&#33616;&#25991;&#31456;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21033;&#29992;&#20174;&#32467;&#26500;&#21270;&#65288;&#34920;&#26684;&#25968;&#25454;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65289;&#29983;&#25104;&#30340;KG&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized recommendations have a growing importance in direct marketing, which motivates research to enhance customer experiences by knowledge graph (KG) applications. For example, in financial services, companies may benefit from providing relevant financial articles to their customers to cultivate relationships, foster client engagement and promote informed financial decisions. While several approaches center on KG-based recommender systems for improved content, in this study we focus on interpretable KG-based recommender systems for decision making.To this end, we present two knowledge graph-based approaches for personalized article recommendations for a set of customers of a large multinational financial services company. The first approach employs Reinforcement Learning and the second approach uses the XGBoost algorithm for recommending articles to the customers. Both approaches make use of a KG generated from both structured (tabular data) and unstructured data (a large body o
&lt;/p&gt;</description></item><item><title>FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.11650</link><description>&lt;p&gt;
FedNoisy: &#20998;&#24067;&#24335;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
FedNoisy: Federated Noisy Label Learning Benchmark. (arXiv:2306.11650v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11650
&lt;/p&gt;
&lt;p&gt;
FedNoisy&#26159;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#32852;&#21512;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;20&#20010;&#22522;&#26412;&#35774;&#32622;&#21644;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#65292;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#25506;&#32034;&#32852;&#21512;&#23398;&#20064;&#20013;&#22122;&#22768;&#26631;&#31614;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23398;&#20064;&#24050;&#32463;&#22240;&#20026;&#26080;&#38656;&#23545;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25935;&#24863;&#25968;&#25454;&#36827;&#34892;&#32858;&#21512;&#32780;&#21464;&#24471;&#21463;&#27426;&#36814;&#12290;&#20294;&#26159;&#65292;&#25968;&#25454;&#38548;&#31163;&#30340;&#20998;&#24067;&#24335;&#21644;&#23396;&#31435;&#24615;&#21487;&#33021;&#20250;&#21463;&#21040;&#25968;&#25454;&#36136;&#37327;&#30340;&#22797;&#26434;&#24615;&#30340;&#24433;&#21709;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#21463;&#21040;&#22122;&#22768;&#26631;&#31614;&#30340;&#24178;&#25200;&#12290;&#35768;&#22810;&#21162;&#21147;&#37117;&#33268;&#21147;&#20110;&#22312;&#38598;&#20013;&#24335;&#25110;&#32852;&#21512;&#24335;&#29615;&#22659;&#20013;&#38450;&#24481;&#22122;&#22768;&#26631;&#31614;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#19968;&#20010;&#20840;&#38754;&#32771;&#34385;&#21508;&#31181;&#20856;&#22411;&#32852;&#21512;&#23398;&#20064;&#22330;&#26223;&#20013;&#22122;&#22768;&#26631;&#31614;&#24433;&#21709;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#20934;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20805;&#20998;&#25506;&#32034;&#28508;&#22312;&#30340;&#32852;&#21512;&#22122;&#22768;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#25506;&#32034;&#36825;&#20123;&#25968;&#25454;&#35774;&#32622;&#30340;&#29305;&#24615;&#65292;&#24182;&#25581;&#31034;&#20102;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#65292;&#36825;&#21487;&#33021;&#25351;&#23548;&#26410;&#26469;&#30340;&#26041;&#27861;&#24320;&#21457;&#12290;&#25105;&#20204;&#24378;&#35843;&#25105;&#20204;&#22522;&#20934;&#27979;&#35797;&#20013;&#25552;&#20986;&#30340;20&#20010;&#22522;&#26412;&#35774;&#32622;&#65292;&#36866;&#29992;&#20110;5&#20010;&#20197;&#19978;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#30340;&#20223;&#30495;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning has gained popularity for distributed learning without aggregating sensitive data from clients. But meanwhile, the distributed and isolated nature of data isolation may be complicated by data quality, making it more vulnerable to noisy labels. Many efforts exist to defend against the negative impacts of noisy labels in centralized or federated settings. However, there is a lack of a benchmark that comprehensively considers the impact of noisy labels in a wide variety of typical FL settings. In this work, we serve the first standardized benchmark that can help researchers fully explore potential federated noisy settings. Also, we conduct comprehensive experiments to explore the characteristics of these data settings and unravel challenging scenarios on the federated noisy label learning, which may guide method development in the future. We highlight the 20 basic settings for more than 5 datasets proposed in our benchmark and standardized simulation pipeline for federa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2306.07566</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#26631;&#31614;&#19979;&#30340;&#24322;&#36136;&#20915;&#31574;&#32773;&#65306;&#19968;&#31181;&#24037;&#20855;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach. (arXiv:2306.07566v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#36825;&#31181;&#38382;&#39064;&#22312;&#21382;&#21490;&#20915;&#31574;&#23548;&#33268;&#32467;&#26524;&#20165;&#37096;&#20998;&#26631;&#35760;&#26102;&#20986;&#29616;&#12290;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#19982;&#25972;&#20307;&#20154;&#32676;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#24403;&#21382;&#21490;&#20915;&#31574;&#21644;&#30446;&#26631;&#32467;&#26524;&#21487;&#20197;&#21516;&#26102;&#21463;&#26576;&#20123;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#24433;&#21709;&#26102;&#12290;&#22240;&#27492;&#65292;&#20165;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#25972;&#20307;&#20154;&#32676;&#20013;&#30340;&#20005;&#37325;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#24212;&#29992;&#20013;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#26469;&#35299;&#20915;&#27492;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#19979;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#32622;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#28385;&#36275;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#26102;&#20219;&#20309;&#32473;&#23450;&#39044;&#27979;&#35268;&#21017;&#30340;&#20840;&#20307;&#39118;&#38505;&#30340;&#28857;&#35782;&#21035;&#26465;&#20214;&#65292;&#24182;&#22312;&#28857;&#35782;&#21035;&#22833;&#36133;&#26102;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#39118;&#38505;&#30028;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning with selectively labeled data, which arises when outcomes are only partially labeled due to historical decision-making. The labeled data distribution may substantially differ from the full population, especially when the historical decisions and the target outcome can be simultaneously affected by some unobserved factors. Consequently, learning with only the labeled data may lead to severely biased results when deployed to the full population. Our paper tackles this challenge by exploiting the fact that in many applications the historical decisions were made by a set of heterogeneous decision-makers. In particular, we analyze this setup in a principled instrumental variable (IV) framework. We establish conditions for the full-population risk of any given prediction rule to be point-identified from the observed data and provide sharp risk bounds when the point identification fails. We further propose a weighted learning approach that learns prediction ru
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#27604;&#20854;&#20182;&#32463;&#20856;&#31639;&#27861;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2306.04828</link><description>&lt;p&gt;
&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#19982;GNN&#30340;&#24555;&#36895;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Fast and Effective GNN Training with Linearized Random Spanning Trees. (arXiv:2306.04828v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32447;&#24615;&#21270;&#38543;&#26426;&#29983;&#25104;&#26641;&#30340;GNN&#35757;&#32451;&#26694;&#26550;&#65292;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#27604;&#20854;&#20182;&#32463;&#20856;&#31639;&#27861;&#26356;&#24555;&#19988;&#26356;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26377;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32473;&#23450;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#30340;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20013;&#35757;&#32451;GNN&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#32447;&#24615;&#21270;&#20174;&#36755;&#20837;&#32593;&#32476;&#20013;&#25552;&#21462;&#30340;&#38543;&#26426;&#29983;&#25104;&#26641;&#24471;&#21040;&#19968;&#31995;&#21015;&#36335;&#24452;&#22270;&#26469;&#36880;&#27493;&#31934;&#32454;&#21270;&#26435;&#37325;&#26356;&#26032;&#25805;&#20316;&#12290;&#36335;&#24452;&#22270;&#34987;&#35774;&#35745;&#20026;&#20445;&#30041;&#21407;&#22987;&#22270;&#30340;&#22522;&#26412;&#25299;&#25169;&#21644;&#33410;&#28857;&#20449;&#24687;&#12290;&#21516;&#26102;&#65292;&#36335;&#24452;&#22270;&#30340;&#31232;&#30095;&#24615;&#20351;&#24471;GNN&#35757;&#32451;&#26356;&#36731;&#20415;&#65292;&#38500;&#20102;&#21487;&#25193;&#23637;&#24615;&#22806;&#65292;&#36824;&#26377;&#21161;&#20110;&#32531;&#35299;&#36807;&#24230;&#21387;&#32553;&#21644;&#36807;&#24230;&#24179;&#28369;&#31561;&#32463;&#20856;&#35757;&#32451;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#24418;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65292;&#19982;&#20247;&#25152;&#21608;&#30693;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#35757;&#32451;&#36895;&#24230;&#21644;&#27979;&#35797;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new effective and scalable framework for training GNNs in supervised node classification tasks, given graph-structured data. Our approach increasingly refines the weight update operations on a sequence of path graphs obtained by linearizing random spanning trees extracted from the input network. The path graphs are designed to retain essential topological and node information of the original graph. At the same time, the sparsity of path graphs enables a much lighter GNN training which, besides scalability, helps in mitigating classical training issues, like over-squashing and over-smoothing. We carry out an extensive experimental investigation on a number of real-world graph benchmarks, where we apply our framework to graph convolutional networks, showing simultaneous improvement of both training speed and test accuracy, as compared to well-known baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.06587</link><description>&lt;p&gt;
&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How Expressive are Spectral-Temporal Graph Neural Networks for Time Series Forecasting?. (arXiv:2305.06587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06587
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#20854;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#12289;&#34920;&#29616;&#21147;&#21463;&#21040;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#23454;&#20363;TGC&#65292;&#20854;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35889;&#26102;&#22270;&#31070;&#32463;&#32593;&#32476;&#26159;&#22823;&#22810;&#25968;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#25277;&#35937;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#38656;&#35201;&#26356;&#22810;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#30693;&#35782;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#35889;&#26102;GNN&#30340;&#34920;&#29616;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20855;&#26377;&#32447;&#24615;&#35889;&#26102;GNN&#26159;&#26222;&#36866;&#30340;&#65292;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#21147;&#21463;&#21040;&#25105;&#20204;&#30340;&#31163;&#25955;&#26102;&#38388;&#21160;&#24577;&#22270;&#25193;&#23637;&#30340;&#31532;&#19968;&#38454;Weisfeiler-Leman&#31639;&#27861;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#21457;&#29616;&#22312;&#23454;&#36341;&#20013;&#26377;&#29992;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#30456;&#20851;&#38480;&#21046;&#65292;&#24182;&#27010;&#36848;&#20102;&#22312;&#35889;&#22495;&#20013;&#35774;&#35745;&#31354;&#38388;&#21644;&#26102;&#38388;&#27169;&#22359;&#30340;&#29702;&#35770;&#34013;&#22270;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#24182;&#20026;&#20102;&#23637;&#31034;&#22522;&#20110;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#35889;&#26102;GNN&#26377;&#22810;&#20040;&#24378;&#22823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; Temporal Graph GegenConv (TGC) &#30340;&#31616;&#21333;&#23454;&#20363;&#65292;&#26174;&#33879;&#20248;&#20110;&#22823;&#22810;&#25968;&#24050;&#26377;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph GegenConv (TGC), which significantly outperforms most e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#37327;&#21270;&#20132;&#26131;&#20013;&#30340;&#36816;&#29992;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110; RL &#30340;&#20132;&#26131;&#31639;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#26377;&#28508;&#21147;&#20248;&#20110;&#20256;&#32479;&#30340;&#20132;&#26131;&#31639;&#27861;&#12290;&#36825;&#19968;&#30740;&#31350;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24341;&#23548;&#26356;&#20026;&#22797;&#26434;&#21644;&#26377;&#25928;&#30340;&#20132;&#26131;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2304.06037</link><description>&lt;p&gt;
&#20351;&#29992;&#28145;&#24230; Q &#23398;&#20064;&#36827;&#34892;&#37327;&#21270;&#20132;&#26131;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantitative Trading using Deep Q Learning. (arXiv:2304.06037v1 [q-fin.TR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#22312;&#37327;&#21270;&#20132;&#26131;&#20013;&#30340;&#36816;&#29992;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110; RL &#30340;&#20132;&#26131;&#31639;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#26377;&#28508;&#21147;&#20248;&#20110;&#20256;&#32479;&#30340;&#20132;&#26131;&#31639;&#27861;&#12290;&#36825;&#19968;&#30740;&#31350;&#20195;&#34920;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24341;&#23548;&#26356;&#20026;&#22797;&#26434;&#21644;&#26377;&#25928;&#30340;&#20132;&#26131;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#24050;&#34987;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#28216;&#25103;&#29609;&#27861;&#21644;&#33258;&#20027;&#31995;&#32479;&#31561;&#22810;&#31181;&#24212;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#24378;&#21270;&#23398;&#20064;&#22312;&#37327;&#21270;&#20132;&#26131;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#26088;&#22312;&#22312;&#37329;&#34701;&#24066;&#22330;&#19978;&#36827;&#34892;&#30408;&#21033;&#20132;&#26131;&#12290;&#26412;&#35770;&#25991;&#25506;&#35752;&#20102; RL &#22312;&#37327;&#21270;&#20132;&#26131;&#20013;&#30340;&#20351;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110; RL &#30340;&#20132;&#26131;&#31639;&#27861;&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;RL &#21487;&#20197;&#25104;&#20026;&#37327;&#21270;&#20132;&#26131;&#30340;&#24378;&#26377;&#21147;&#24037;&#20855;&#65292;&#24182;&#19988;&#23427;&#26377;&#28508;&#21147;&#20248;&#20110;&#20256;&#32479;&#30340;&#20132;&#26131;&#31639;&#27861;&#12290;&#22312;&#37327;&#21270;&#20132;&#26131;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#20195;&#34920;&#30528;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#21487;&#20197;&#28508;&#22312;&#22320;&#24341;&#23548;&#26356;&#20026;&#22797;&#26434;&#21644;&#26377;&#25928;&#30340;&#20132;&#26131;&#31995;&#32479;&#30340;&#24320;&#21457;&#12290;&#26410;&#26469;&#30340;&#24037;&#20316;&#21487;&#20197;&#25506;&#32034;&#20351;&#29992;&#26367;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#25972;&#21512;&#20854;&#20182;&#25968;&#25454;&#26469;&#28304;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#36164;&#20135;&#31867;&#21035;&#19978;&#27979;&#35797;&#35813;&#31995;&#32479;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#23427;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is a branch of machine learning that has been used in a variety of applications such as robotics, game playing, and autonomous systems. In recent years, there has been growing interest in applying RL to quantitative trading, where the goal is to make profitable trades in financial markets. This paper explores the use of RL in quantitative trading and presents a case study of a RL-based trading algorithm. The results show that RL can be a powerful tool for quantitative trading, and that it has the potential to outperform traditional trading algorithms. The use of reinforcement learning in quantitative trading represents a promising area of research that can potentially lead to the development of more sophisticated and effective trading systems. Future work could explore the use of alternative reinforcement learning algorithms, incorporate additional data sources, and test the system on different asset classes. Overall, our research demonstrates the potential 
&lt;/p&gt;</description></item></channel></rss>