<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16459</link><description>&lt;p&gt;
&#20851;&#20110;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23398;&#20064;&#25910;&#25947;&#36895;&#29575;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the rates of convergence for learning with convolutional neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16459
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#19968;&#23450;&#26435;&#37325;&#32422;&#26463;&#30340;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#65292;&#20197;&#21450;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#20570;&#20102;&#26032;&#30340;&#20998;&#26512;&#65292;&#20026;&#22522;&#20110;CNNs&#30340;&#23398;&#20064;&#38382;&#39064;&#25512;&#23548;&#20102;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#22312;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#21644;&#20108;&#20803;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#36924;&#36817;&#21644;&#23398;&#20064;&#33021;&#21147;&#12290;&#31532;&#19968;&#20010;&#32467;&#26524;&#35777;&#26126;&#20102;&#22312;&#26435;&#37325;&#19978;&#26377;&#19968;&#23450;&#32422;&#26463;&#26465;&#20214;&#19979;CNNs&#30340;&#26032;&#36924;&#36817;&#19978;&#30028;&#12290;&#31532;&#20108;&#20010;&#32467;&#26524;&#32473;&#20986;&#20102;&#23545;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#35206;&#30422;&#25968;&#30340;&#26032;&#20998;&#26512;&#65292;&#20854;&#20013;CNNs&#26159;&#20854;&#29305;&#20363;&#12290;&#35813;&#20998;&#26512;&#35814;&#32454;&#32771;&#34385;&#20102;&#26435;&#37325;&#30340;&#22823;&#23567;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#32473;&#20986;&#20102;&#27604;&#29616;&#26377;&#25991;&#29486;&#26356;&#22909;&#30340;&#19978;&#30028;&#12290;&#21033;&#29992;&#36825;&#20004;&#20010;&#32467;&#26524;&#65292;&#25105;&#20204;&#33021;&#22815;&#25512;&#23548;&#22522;&#20110;CNNs&#30340;&#20272;&#35745;&#22120;&#22312;&#35768;&#22810;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#22312;&#38750;&#21442;&#25968;&#22238;&#24402;&#35774;&#32622;&#20013;&#20026;&#22522;&#20110;CNNs&#30340;&#26368;&#23567;&#20108;&#20056;&#23398;&#20064;&#24179;&#28369;&#20989;&#25968;&#24314;&#31435;&#20102;&#26497;&#23567;&#26368;&#20248;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#23545;&#20110;&#20108;&#20803;&#20998;&#31867;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20855;&#26377;&#38128;&#38142;&#25439;&#22833;&#21644;&#36923;&#36753;&#25439;&#22833;&#30340;CNN&#20998;&#31867;&#22120;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#21516;&#26102;&#36824;&#34920;&#26126;&#25152;&#24471;&#21040;&#30340;&#36895;&#29575;&#22312;&#20960;&#31181;&#24773;&#20917;&#19979;&#26159;&#26497;&#23567;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16459v1 Announce Type: new  Abstract: We study the approximation and learning capacities of convolutional neural networks (CNNs). Our first result proves a new approximation bound for CNNs with certain constraint on the weights. Our second result gives a new analysis on the covering number of feed-forward neural networks, which include CNNs as special cases. The analysis carefully takes into account the size of the weights and hence gives better bounds than existing literature in some situations. Using these two results, we are able to derive rates of convergence for estimators based on CNNs in many learning problems. In particular, we establish minimax optimal convergence rates of the least squares based on CNNs for learning smooth functions in the nonparametric regression setting. For binary classification, we derive convergence rates for CNN classifiers with hinge loss and logistic loss. It is also shown that the obtained rates are minimax optimal in several settings.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLC++&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32858;&#31867;&#20197;&#21450;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#23454;&#29616;&#20102;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#24050;&#30693;&#25968;&#25454;&#24182;&#23558;&#20854;&#20174;&#26410;&#30693;&#25968;&#25454;&#20013;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2403.14410</link><description>&lt;p&gt;
GLC++: &#20840;&#23616;&#23616;&#37096;&#32858;&#31867;&#21644;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#30340;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14410
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLC++&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32858;&#31867;&#20197;&#21450;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#23454;&#29616;&#20102;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#24050;&#30693;&#25968;&#25454;&#24182;&#23558;&#20854;&#20174;&#26410;&#30693;&#25968;&#25454;&#20013;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#22312;&#21327;&#21464;&#37327;&#21644;&#31867;&#21035;&#36716;&#31227;&#19979;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#20026;&#36825;&#19968;&#22256;&#22659;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;SFDA&#26041;&#27861;&#23616;&#38480;&#20110;&#23553;&#38381;&#38598;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26088;&#22312;&#20934;&#30830;&#20998;&#31867;&#23646;&#20110;&#24120;&#35265;&#31867;&#21035;&#30340;&#8220;&#24050;&#30693;&#8221;&#25968;&#25454;&#24182;&#23558;&#20854;&#19982;&#30446;&#26631;&#19987;&#26377;&#8220;&#26410;&#30693;&#8221;&#25968;&#25454;&#38548;&#31163;&#24320;&#26469;&#30340;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UniDA&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#29699;&#21644;&#23616;&#37096;&#32858;&#31867;&#65288;GLC&#65289;&#25216;&#26415;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#36866;&#24212;&#30340;&#19968;&#23545;&#20840;&#23616;&#32858;&#31867;&#31639;&#27861;&#26469;&#21306;&#20998;&#30446;&#26631;&#31867;&#21035;&#65292;&#36741;&#20197;&#26412;&#22320;k-NN&#32858;&#31867;&#31574;&#30053;&#20197;&#20943;&#36731;&#36127;&#38754;&#36716;&#31227;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#22266;&#26377;&#30340;&#23553;&#38381;&#28304;&#26550;&#26500;&#23548;&#33268;&#23545;&#8220;&#26410;&#30693;&#8221;&#25968;&#25454;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#38459;&#30861;&#20102;&#23545;&#19981;&#21516;&#8220;&#26410;&#30693;&#8221;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;GLC&#21457;&#23637;&#21040;GLC++&#65292;&#25972;&#21512;&#20102;&#23545;&#27604;&#20146;&#21644;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14410v1 Announce Type: cross  Abstract: Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify "known" data belonging to common categories and segregate them from target-private "unknown" data. We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of "unknown" data, impeding the identification of distinct "unknown" categories. To address this, we evolve GLC to GLC++, integrating a contrastive affini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08290</link><description>&lt;p&gt;
&#25968;&#25454;&#27745;&#26579;&#23545;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Effect of Data Poisoning on Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#25968;&#25454;&#27745;&#26579;&#26041;&#38754;&#30340;&#33030;&#24369;&#24615;&#65292;&#21457;&#29616;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23481;&#26131;&#21463;&#21040;&#25968;&#25454;&#27745;&#26579;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#20998;&#26512;&#40657;&#30418;&#31995;&#32479;&#39044;&#27979;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#24314;&#35758;&#25913;&#21464;&#36755;&#20837;&#20197;&#33719;&#24471;&#19981;&#21516;&#65288;&#26356;&#26377;&#21033;&#65289;&#31995;&#32479;&#36755;&#20986;&#30340;&#35745;&#31639;&#34917;&#25937;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#31361;&#26174;&#20102;&#23427;&#20204;&#23545;&#19981;&#21516;&#31867;&#22411;&#25805;&#32437;&#30340;&#33030;&#24369;&#24615;&#12290;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#25968;&#25454;&#27745;&#26579;&#30340;&#33030;&#24369;&#24615;&#12290;&#25105;&#20204;&#22312;&#22686;&#21152;&#19977;&#20010;&#19981;&#21516;&#23618;&#27425;&#30340;&#34917;&#25937;&#25104;&#26412;&#26041;&#38754;&#65292;&#24418;&#24335;&#21270;&#22320;&#30740;&#31350;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#21333;&#20010;&#23454;&#20363;&#12289;&#26576;&#20010;&#23376;&#32452;&#25110;&#25152;&#26377;&#23454;&#20363;&#19978;&#30340;&#25968;&#25454;&#27745;&#26579;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26368;&#20808;&#36827;&#30340;&#21453;&#20107;&#23454;&#29983;&#25104;&#26041;&#27861;&#21644;&#24037;&#20855;&#21253;&#23545;&#27492;&#31867;&#25968;&#25454;&#27745;&#26579;&#26159;&#33030;&#24369;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations provide a popular method for analyzing the predictions of black-box systems, and they can offer the opportunity for computational recourse by suggesting actionable changes on how to change the input to obtain a different (i.e. more favorable) system output. However, recent work highlighted their vulnerability to different types of manipulations. This work studies the vulnerability of counterfactual explanations to data poisoning. We formalize data poisoning in the context of counterfactual explanations for increasing the cost of recourse on three different levels: locally for a single instance, or a sub-group of instances, or globally for all instances. We demonstrate that state-of-the-art counterfactual generation methods \&amp; toolboxes are vulnerable to such data poisoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#65288;UAO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02339</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Testing-Time Optimization for 3D Human Pose Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02339
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270;&#65288;UAO&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;&#37327;&#21270;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#65292;&#25552;&#39640;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#22312;3D&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#21463;&#21040;&#22495;&#38388;&#24046;&#24322;&#30340;&#38480;&#21046;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#36827;&#34892;&#24494;&#35843;&#26041;&#38754;&#34920;&#29616;&#20248;&#31168;&#65292;&#20294;&#25972;&#20307;&#34920;&#29616;&#36890;&#24120;&#19981;&#22914;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#20808;&#21069;&#30340;&#22522;&#20110;&#20248;&#21270;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#25237;&#24433;&#32422;&#26463;&#65292;&#36825;&#20165;&#20165;&#30830;&#20445;&#20102;&#22312;2D&#31354;&#38388;&#20013;&#30340;&#23545;&#40784;&#65292;&#21487;&#33021;&#23548;&#33268;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#27979;&#35797;&#26102;&#38388;&#20248;&#21270; (UAO) &#26694;&#26550;&#65292;&#23427;&#20445;&#30041;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#24182;&#21033;&#29992;&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#32531;&#35299;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#35757;&#32451;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;2D&#21040;3D&#32593;&#32476;&#65292;&#29992;&#20110;&#20272;&#35745;&#30456;&#24212;&#30340;3D&#23039;&#21183;&#65292;&#24182;&#37327;&#21270;&#27599;&#20010;3D&#20851;&#33410;&#28857;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#23545;&#20110;&#27979;&#35797;&#26102;&#30340;&#20248;&#21270;&#65292;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#20923;&#32467;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#20165;&#20248;&#21270;&#23569;&#37327;&#20851;&#38190;&#21442;&#25968;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although data-driven methods have achieved success in 3D human pose estimation, they often suffer from domain gaps and exhibit limited generalization. In contrast, optimization-based methods excel in fine-tuning for specific cases but are generally inferior to data-driven methods in overall performance. We observe that previous optimization-based methods commonly rely on projection constraint, which only ensures alignment in 2D space, potentially leading to the overfitting problem. To address this, we propose an Uncertainty-Aware testing-time Optimization (UAO) framework, which keeps the prior information of pre-trained model and alleviates the overfitting problem using the uncertainty of joints. Specifically, during the training phase, we design an effective 2D-to-3D network for estimating the corresponding 3D pose while quantifying the uncertainty of each 3D joint. For optimization during testing, the proposed optimization framework freezes the pre-trained model and optimizes only a 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REDS&#30340;&#36164;&#28304;&#39640;&#25928;&#28145;&#24230;&#23376;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#26032;&#39062;&#30340;&#36845;&#20195;&#32972;&#21253;&#20248;&#21270;&#22120;&#26469;&#23454;&#29616;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#33258;&#36866;&#24212;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#35745;&#31639;&#22359;&#21644;&#37325;&#26032;&#23433;&#25490;&#25805;&#20316;&#39034;&#24207;&#31561;&#26041;&#27861;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2311.13349</link><description>&lt;p&gt;
REDS: &#36164;&#28304;&#39640;&#25928;&#30340;&#28145;&#24230;&#23376;&#32593;&#32476;&#29992;&#20110;&#21160;&#24577;&#36164;&#28304;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REDS&#30340;&#36164;&#28304;&#39640;&#25928;&#28145;&#24230;&#23376;&#32593;&#32476;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#21644;&#26032;&#39062;&#30340;&#36845;&#20195;&#32972;&#21253;&#20248;&#21270;&#22120;&#26469;&#23454;&#29616;&#27169;&#22411;&#22312;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#19979;&#30340;&#33258;&#36866;&#24212;&#24615;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#35745;&#31639;&#22359;&#21644;&#37325;&#26032;&#23433;&#25490;&#25805;&#20316;&#39034;&#24207;&#31561;&#26041;&#27861;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#28145;&#24230;&#27169;&#22411;&#32463;&#24120;&#36935;&#21040;&#36164;&#28304;&#21464;&#21270;&#65292;&#36825;&#28304;&#20110;&#33021;&#37327;&#27700;&#24179;&#27874;&#21160;&#12289;&#26102;&#38388;&#32422;&#26463;&#25110;&#31995;&#32479;&#20013;&#20854;&#20182;&#20851;&#38190;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#29983;&#25104;&#30340;&#26159;&#36164;&#28304;&#19981;&#21487;&#30693;&#30340;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#22312;&#36816;&#34892;&#26102;&#36827;&#34892;&#35843;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Resource-Efficient Deep Subnetworks (REDS)&#26469;&#24212;&#23545;&#21487;&#21464;&#36164;&#28304;&#19979;&#30340;&#27169;&#22411;&#36866;&#24212;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#27604;&#65292;REDS&#21033;&#29992;&#32467;&#26500;&#21270;&#31232;&#30095;&#24615;&#65292;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#20803;&#30340;&#25490;&#21015;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#20801;&#35768;&#30828;&#20214;&#29305;&#23450;&#30340;&#20248;&#21270;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;REDS&#36890;&#36807;&#65288;1&#65289;&#36339;&#36807;&#30001;&#26032;&#39062;&#30340;&#36845;&#20195;&#32972;&#21253;&#20248;&#21270;&#22120;&#35782;&#21035;&#30340;&#39034;&#24207;&#35745;&#31639;&#22359;&#65292;&#20197;&#21450;&#65288;2&#65289;&#21033;&#29992;&#31616;&#21333;&#30340;&#25968;&#23398;&#37325;&#26032;&#23433;&#25490;REDS&#35745;&#31639;&#22270;&#20013;&#25805;&#20316;&#30340;&#39034;&#24207;&#65292;&#20197;&#21033;&#29992;&#25968;&#25454;&#32531;&#23384;&#32780;&#23454;&#29616;&#35745;&#31639;&#25928;&#29575;&#12290;REDS&#25903;&#25345;&#20256;&#32479;&#30340;&#28145;&#24230;&#32593;&#32476;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13349v2 Announce Type: replace  Abstract: Deep models deployed on edge devices frequently encounter resource variability, which arises from fluctuating energy levels, timing constraints, or prioritization of other critical tasks within the system. State-of-the-art machine learning pipelines generate resource-agnostic models, not capable to adapt at runtime. In this work we introduce Resource-Efficient Deep Subnetworks (REDS) to tackle model adaptation to variable resources. In contrast to the state-of-the-art, REDS use structured sparsity constructively by exploiting permutation invariance of neurons, which allows for hardware-specific optimizations. Specifically, REDS achieve computational efficiency by (1) skipping sequential computational blocks identified by a novel iterative knapsack optimizer, and (2) leveraging simple math to re-arrange the order of operations in REDS computational graph to take advantage of the data cache. REDS support conventional deep networks freq
&lt;/p&gt;</description></item><item><title>&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#30740;&#31350;&#20102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#24433;&#21709;&#65292;&#19988;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2310.09336</link><description>&lt;p&gt;
&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#25506;&#32034;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task. (arXiv:2310.09336v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09336
&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#33021;&#21147;&#20197;&#20056;&#27861;&#26041;&#24335;&#20986;&#29616;&#65306;&#30740;&#31350;&#20102;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#22312;&#21512;&#25104;&#20219;&#21153;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#31181;&#33021;&#21147;&#21463;&#21040;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#24433;&#21709;&#65292;&#19988;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#27169;&#22411;&#23637;&#31034;&#20986;&#20102;&#20135;&#29983;&#26497;&#20026;&#36924;&#30495;&#25968;&#25454;&#30340;&#21069;&#25152;&#26410;&#26377;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#33258;&#28982;&#32452;&#21512;&#24615;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21487;&#38752;&#20351;&#29992;&#38656;&#35201;&#23637;&#31034;&#20986;&#33021;&#22815;&#32452;&#21512;&#26032;&#30340;&#27010;&#24565;&#38598;&#21512;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26410;&#35265;&#30340;&#36755;&#20986;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#26368;&#36817;&#30340;&#25193;&#25955;&#27169;&#22411;&#30830;&#23454;&#34920;&#29616;&#20986;&#20102;&#26377;&#36259;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#20063;&#20250;&#20986;&#29616;&#26080;&#27861;&#39044;&#27979;&#30340;&#22833;&#36133;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#26377;&#25511;&#21046;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#20102;&#35299;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#25105;&#20204;&#21464;&#21270;&#20102;&#35757;&#32451;&#25968;&#25454;&#30340;&#19981;&#21516;&#23646;&#24615;&#24182;&#27979;&#37327;&#20102;&#27169;&#22411;&#29983;&#25104;&#36234;&#30028;&#26679;&#26412;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65306;&#65288;i&#65289;&#20174;&#19968;&#20010;&#27010;&#24565;&#29983;&#25104;&#26679;&#26412;&#30340;&#33021;&#21147;&#21644;&#23558;&#23427;&#20204;&#32452;&#21512;&#36215;&#26469;&#30340;&#33021;&#21147;&#30340;&#20986;&#29616;&#39034;&#24207;&#21463;&#21040;&#20102;&#24213;&#23618;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#32467;&#26500;&#30340;&#24433;&#21709;&#65307;&#65288;ii&#65289;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#34920;&#26126;&#27169;&#22411;&#22312;&#23398;&#20064;&#21040;&#26356;&#39640;&#32423;&#30340;&#32452;&#21512;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhib
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2309.10370</link><description>&lt;p&gt;
&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#21644;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometric structure of shallow neural networks and constructive ${\mathcal L}^2$ cost minimization. (arXiv:2309.10370v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10370
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32467;&#26500;&#35299;&#37322;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;${\mathcal L}^2$&#20195;&#20215;&#26368;&#23567;&#21270;&#30340;&#26500;&#36896;&#26041;&#27861;&#33719;&#24471;&#20102;&#19968;&#20010;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32473;&#20986;&#20102;&#19968;&#20010;&#20960;&#20309;&#35299;&#37322;&#65306;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#30001;&#19968;&#20010;&#38544;&#34255;&#23618;&#12289;&#19968;&#20010;&#26012;&#22369;&#28608;&#27963;&#20989;&#25968;&#12289;&#19968;&#20010;${\mathcal L}^2$&#35889;&#33539;&#31867;&#65288;&#25110;&#32773;Hilbert-Schmidt&#65289;&#30340;&#20195;&#20215;&#20989;&#25968;&#12289;&#36755;&#20837;&#31354;&#38388;${\mathbb R}^M$&#12289;&#36755;&#20986;&#31354;&#38388;${\mathbb R}^Q$&#65288;&#20854;&#20013;$Q\leq M$&#65289;&#65292;&#20197;&#21450;&#35757;&#32451;&#36755;&#20837;&#26679;&#26412;&#25968;&#37327;$N&gt;QM$&#25152;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#26368;&#23567;&#20540;&#20855;&#26377;$O(\delta_P)$&#30340;&#19978;&#30028;&#65292;&#20854;&#20013;$\delta_P$&#34913;&#37327;&#20102;&#35757;&#32451;&#36755;&#20837;&#30340;&#20449;&#22122;&#27604;&#12290;&#25105;&#20204;&#20351;&#29992;&#36866;&#24212;&#20110;&#23646;&#20110;&#21516;&#19968;&#36755;&#20986;&#21521;&#37327;$y_j$&#30340;&#35757;&#32451;&#36755;&#20837;&#21521;&#37327;$\overline{x_{0,j}}$&#30340;&#25237;&#24433;&#26469;&#33719;&#24471;&#36817;&#20284;&#30340;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;$j=1,\dots,Q$&#12290;&#22312;&#29305;&#27530;&#24773;&#20917;$M=Q$&#19979;&#65292;&#25105;&#20204;&#26126;&#30830;&#30830;&#23450;&#20102;&#20195;&#20215;&#20989;&#25968;&#30340;&#19968;&#20010;&#30830;&#20999;&#36864;&#21270;&#23616;&#37096;&#26368;&#23567;&#20540;&#65307;&#36825;&#20010;&#23574;&#38160;&#30340;&#20540;&#19982;&#23545;&#20110;$Q\leq M$&#25152;&#33719;&#24471;&#30340;&#19978;&#30028;&#20043;&#38388;&#26377;&#19968;&#20010;&#30456;&#23545;&#35823;&#24046;$O(\delta_P^2)$&#12290;&#19978;&#30028;&#35777;&#26126;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#20010;&#26500;&#36896;&#24615;&#35757;&#32451;&#30340;&#32593;&#32476;&#65307;&#25105;&#20204;&#35777;&#26126;&#23427;&#27979;&#24230;&#20102;$Q$&#32500;&#31354;&#38388;&#20013;&#30340;&#32473;&#23450;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we provide a geometric interpretation of the structure of shallow neural networks characterized by one hidden layer, a ramp activation function, an ${\mathcal L}^2$ Schatten class (or Hilbert-Schmidt) cost function, input space ${\mathbb R}^M$, output space ${\mathbb R}^Q$ with $Q\leq M$, and training input sample size $N&gt;QM$. We prove an upper bound on the minimum of the cost function of order $O(\delta_P$ where $\delta_P$ measures the signal to noise ratio of training inputs. We obtain an approximate optimizer using projections adapted to the averages $\overline{x_{0,j}}$ of training input vectors belonging to the same output vector $y_j$, $j=1,\dots,Q$. In the special case $M=Q$, we explicitly determine an exact degenerate local minimum of the cost function; the sharp value differs from the upper bound obtained for $Q\leq M$ by a relative error $O(\delta_P^2)$. The proof of the upper bound yields a constructively trained network; we show that it metrizes the $Q$-dimen
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#26174;&#24335;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10189</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#23398;&#20064;&#39640;&#32500;&#38750;&#21442;&#25968;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning High-Dimensional Nonparametric Differential Equations via Multivariate Occupation Kernel Functions. (arXiv:2306.10189v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32447;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#22312;&#39640;&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;ODE&#31995;&#32479;&#65292;&#21487;&#20197;&#35299;&#20915;&#26174;&#24335;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#25968;&#25454;&#21644;&#22270;&#20687;&#25968;&#25454;&#20013;&#37117;&#20855;&#26377;&#36890;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;$d$&#32500;&#29366;&#24577;&#31354;&#38388;&#20013;$n$&#20010;&#36712;&#36857;&#24555;&#29031;&#20013;&#23398;&#20064;&#38750;&#21442;&#25968;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#31995;&#32479;&#38656;&#35201;&#23398;&#20064;$d$&#20010;&#20989;&#25968;&#12290;&#38500;&#38750;&#20855;&#26377;&#39069;&#22806;&#30340;&#31995;&#32479;&#23646;&#24615;&#30693;&#35782;&#65292;&#20363;&#22914;&#31232;&#30095;&#24615;&#21644;&#23545;&#31216;&#24615;&#65292;&#21542;&#21017;&#26174;&#24335;&#30340;&#20844;&#24335;&#25353;&#20108;&#27425;&#26041;&#32553;&#25918;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21521;&#37327;&#20540;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#25552;&#20379;&#30340;&#38544;&#24335;&#20844;&#24335;&#23398;&#20064;&#30340;&#32447;&#24615;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;ODE&#37325;&#20889;&#20026;&#26356;&#24369;&#30340;&#31215;&#20998;&#24418;&#24335;&#65292;&#25105;&#20204;&#38543;&#21518;&#36827;&#34892;&#26368;&#23567;&#21270;&#24182;&#25512;&#23548;&#20986;&#25105;&#20204;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#21521;&#37327;&#22330;&#20381;&#36182;&#20110;&#19982;&#35299;&#36712;&#36857;&#30456;&#20851;&#30340;&#22810;&#20803;&#21344;&#20301;&#26680;&#20989;&#25968;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;$d$&#21487;&#33021;&#36229;&#36807;100&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#20174;&#22270;&#20687;&#25968;&#25454;&#23398;&#20064;&#38750;&#21442;&#25968;&#19968;&#38454;&#25311;&#32447;&#24615;&#20559;&#24494;&#20998;&#26041;&#31243;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning a nonparametric system of ordinary differential equations (ODEs) from $n$ trajectory snapshots in a $d$-dimensional state space requires learning $d$ functions of $d$ variables. Explicit formulations scale quadratically in $d$ unless additional knowledge about system properties, such as sparsity and symmetries, is available. In this work, we propose a linear approach to learning using the implicit formulation provided by vector-valued Reproducing Kernel Hilbert Spaces. By rewriting the ODEs in a weaker integral form, which we subsequently minimize, we derive our learning algorithm. The minimization problem's solution for the vector field relies on multivariate occupation kernel functions associated with the solution trajectories. We validate our approach through experiments on highly nonlinear simulated and real data, where $d$ may exceed 100. We further demonstrate the versatility of the proposed method by learning a nonparametric first order quasilinear partial differential 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15612</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning. (arXiv:2305.15612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15612
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#20195;&#26367;&#23494;&#24230;&#27604;&#26469;&#20272;&#35745;&#20840;&#23616;&#26368;&#20248;&#35299;&#30340;&#20004;&#32452;&#25968;&#25454;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#36991;&#20813;&#20102;&#20998;&#31867;&#22120;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#22312;&#31185;&#23398;&#19982;&#24037;&#31243;&#30340;&#22810;&#20010;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#39640;&#25928;&#22320;&#25214;&#21040;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#27010;&#29575;&#22238;&#24402;&#27169;&#22411;&#65292;&#22914;&#39640;&#26031;&#36807;&#31243;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65292;&#34987;&#24191;&#27867;&#29992;&#20316;&#26367;&#20195;&#20989;&#25968;&#65292;&#29992;&#20110;&#27169;&#25311;&#22312;&#32473;&#23450;&#36755;&#20837;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#20989;&#25968;&#35780;&#20272;&#30340;&#26174;&#24335;&#20998;&#24067;&#12290;&#38500;&#20102;&#22522;&#20110;&#27010;&#29575;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#22522;&#20110;&#23494;&#24230;&#27604;&#20272;&#35745;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#24050;&#34987;&#25552;&#20986;&#26469;&#20272;&#35745;&#30456;&#23545;&#20110;&#20840;&#23616;&#26368;&#20248;&#35299;&#30456;&#23545;&#25509;&#36817;&#21644;&#30456;&#23545;&#36828;&#31163;&#30340;&#20004;&#32452;&#23494;&#24230;&#27604;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#21457;&#23637;&#36825;&#19968;&#30740;&#31350;&#65292;&#21487;&#20197;&#20351;&#29992;&#30417;&#30563;&#20998;&#31867;&#22120;&#26469;&#20272;&#35745;&#36825;&#20004;&#32452;&#30340;&#31867;&#21035;&#27010;&#29575;&#65292;&#32780;&#19981;&#26159;&#23494;&#24230;&#27604;&#12290;&#28982;&#32780;&#65292;&#27492;&#31574;&#30053;&#20013;&#20351;&#29992;&#30340;&#30417;&#30563;&#20998;&#31867;&#22120;&#20542;&#21521;&#20110;&#23545;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#36807;&#20110;&#33258;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization has attracted huge attention from diverse research areas in science and engineering, since it is capable of finding a global optimum of an expensive-to-evaluate black-box function efficiently. In general, a probabilistic regression model, e.g., Gaussian processes, random forests, and Bayesian neural networks, is widely used as a surrogate function to model an explicit distribution over function evaluations given an input to estimate and a training dataset. Beyond the probabilistic regression-based Bayesian optimization, density ratio estimation-based Bayesian optimization has been suggested in order to estimate a density ratio of the groups relatively close and relatively far to a global optimum. Developing this line of research further, a supervised classifier can be employed to estimate a class probability for the two groups instead of a density ratio. However, the supervised classifiers used in this strategy tend to be overconfident for a global solution candid
&lt;/p&gt;</description></item></channel></rss>