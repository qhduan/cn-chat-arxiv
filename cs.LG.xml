<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#29615;&#22659;&#38543;&#26426;&#24615;&#65292;&#22312;&#39118;&#38505;&#25935;&#24863;&#21644;&#35268;&#36991;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.17646</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Distributional Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17646
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#35299;&#20915;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#29615;&#22659;&#38543;&#26426;&#24615;&#65292;&#22312;&#39118;&#38505;&#25935;&#24863;&#21644;&#35268;&#36991;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#65292;&#22240;&#20854;&#20165;&#20381;&#36182;&#20110;&#35266;&#27979;&#25968;&#25454;&#12290;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#20013;&#24515;&#20851;&#27880;&#28857;&#26159;&#36890;&#36807;&#37327;&#21270;&#19982;&#21508;&#31181;&#34892;&#21160;&#21644;&#29615;&#22659;&#38543;&#26426;&#24615;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#30830;&#20445;&#25152;&#23398;&#31574;&#30053;&#30340;&#23433;&#20840;&#24615;&#12290;&#20256;&#32479;&#26041;&#27861;&#20027;&#35201;&#24378;&#35843;&#36890;&#36807;&#23398;&#20064;&#39118;&#38505;&#35268;&#36991;&#31574;&#30053;&#26469;&#32531;&#35299;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#24448;&#24448;&#24573;&#35270;&#29615;&#22659;&#38543;&#26426;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#30340;&#20998;&#24067;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21516;&#26102;&#22788;&#29702;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#21644;&#29615;&#22659;&#38543;&#26426;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#23398;&#20064;&#39118;&#38505;&#35268;&#36991;&#31574;&#30053;&#24182;&#34920;&#24449;&#25240;&#29616;&#32047;&#31215;&#22870;&#21169;&#30340;&#25972;&#20010;&#20998;&#24067;&#30340;&#26080;&#27169;&#22411;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#26368;&#22823;&#21270;&#32047;&#31215;&#25240;&#29616;&#22238;&#25253;&#30340;&#26399;&#26395;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#22312;&#39118;&#38505;&#25935;&#24863;&#21644;&#39118;&#38505;&#35268;&#36991;&#35774;&#32622;&#19979;&#30340;&#20840;&#38754;&#23454;&#39564;&#24471;&#21040;&#20005;&#26684;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17646v1 Announce Type: new  Abstract: Offline reinforcement learning (RL) presents distinct challenges as it relies solely on observational data. A central concern in this context is ensuring the safety of the learned policy by quantifying uncertainties associated with various actions and environmental stochasticity. Traditional approaches primarily emphasize mitigating epistemic uncertainty by learning risk-averse policies, often overlooking environmental stochasticity. In this study, we propose an uncertainty-aware distributional offline RL method to simultaneously address both epistemic uncertainty and environmental stochasticity. We propose a model-free offline RL algorithm capable of learning risk-averse policies and characterizing the entire distribution of discounted cumulative rewards, as opposed to merely maximizing the expected value of accumulated discounted returns. Our method is rigorously evaluated through comprehensive experiments in both risk-sensitive and ri
&lt;/p&gt;</description></item><item><title>&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#21608;&#22260;&#29615;&#22659;&#35270;&#20026;&#20849;&#21516;&#28436;&#21270;&#30340;&#31995;&#32479;&#65292;&#25552;&#20986;&#26234;&#33021;&#20307;-&#29615;&#22659;&#21327;&#21516;&#20248;&#21270;&#38382;&#39064;&#24182;&#24320;&#21457;&#21327;&#35843;&#31639;&#27861;&#65292;&#20197;&#25913;&#36827;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.14583</link><description>&lt;p&gt;
&#20026;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#30340;&#29615;&#22659;&#21644;&#25919;&#31574;&#36827;&#34892;&#21327;&#21516;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Co-Optimization of Environment and Policies for Decentralized Multi-Agent Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14583
&lt;/p&gt;
&lt;p&gt;
&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21644;&#21608;&#22260;&#29615;&#22659;&#35270;&#20026;&#20849;&#21516;&#28436;&#21270;&#30340;&#31995;&#32479;&#65292;&#25552;&#20986;&#26234;&#33021;&#20307;-&#29615;&#22659;&#21327;&#21516;&#20248;&#21270;&#38382;&#39064;&#24182;&#24320;&#21457;&#21327;&#35843;&#31639;&#27861;&#65292;&#20197;&#25913;&#36827;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21450;&#20854;&#21608;&#22260;&#29615;&#22659;&#35270;&#20026;&#19968;&#20010;&#20849;&#21516;&#28436;&#21270;&#30340;&#31995;&#32479;&#65292;&#20854;&#20013;&#19968;&#20010;&#30340;&#34892;&#20026;&#20250;&#24433;&#21709;&#21478;&#19968;&#20010;&#12290;&#20854;&#30446;&#26631;&#26159;&#23558;&#26234;&#33021;&#20307;&#34892;&#20026;&#21644;&#29615;&#22659;&#37197;&#32622;&#37117;&#35270;&#20026;&#20915;&#31574;&#21464;&#37327;&#65292;&#24182;&#20197;&#21327;&#35843;&#30340;&#26041;&#24335;&#20248;&#21270;&#36825;&#20004;&#20010;&#32452;&#20214;&#65292;&#20197;&#25913;&#36827;&#26576;&#20123;&#24863;&#20852;&#36259;&#30340;&#24230;&#37327;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#25317;&#25380;&#29615;&#22659;&#20013;&#30340;&#21435;&#20013;&#24515;&#21270;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#22810;&#26234;&#33021;&#20307;&#23548;&#33322;&#21644;&#29615;&#22659;&#20248;&#21270;&#30340;&#20004;&#20010;&#23376;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;&#26234;&#33021;&#20307;-&#29615;&#22659;&#21327;&#21516;&#20248;&#21270;&#8221;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#8220;&#21327;&#35843;&#31639;&#27861;&#8221;&#65292;&#22312;&#36825;&#20004;&#20010;&#23376;&#30446;&#26631;&#20043;&#38388;&#20132;&#26367;&#20197;&#23547;&#25214;&#26234;&#33021;&#20307;&#34892;&#20026;&#21644;&#38556;&#30861;&#29289;&#29615;&#22659;&#37197;&#32622;&#30340;&#26368;&#20339;&#32508;&#21512;&#65307;&#26368;&#32456;&#25552;&#39640;&#20102;&#23548;&#33322;&#24615;&#33021;&#12290;&#30001;&#20110;&#26126;&#30830;&#24314;&#27169;&#26234;&#33021;&#20307;&#12289;&#29615;&#22659;&#21644;&#24615;&#33021;&#20043;&#38388;&#20851;&#31995;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14583v1 Announce Type: cross  Abstract: This work views the multi-agent system and its surrounding environment as a co-evolving system, where the behavior of one affects the other. The goal is to take both agent actions and environment configurations as decision variables, and optimize these two components in a coordinated manner to improve some measure of interest. Towards this end, we consider the problem of decentralized multi-agent navigation in cluttered environments. By introducing two sub-objectives of multi-agent navigation and environment optimization, we propose an $\textit{agent-environment co-optimization}$ problem and develop a $\textit{coordinated algorithm}$ that alternates between these sub-objectives to search for an optimal synthesis of agent actions and obstacle configurations in the environment; ultimately, improving the navigation performance. Due to the challenge of explicitly modeling the relation between agents, environment and performance, we leverag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#36924;&#30495;&#30340;&#39640;&#36136;&#37327;&#26631;&#20934;&#24179;&#38754;&#65292;&#23545;&#25552;&#39640;&#20020;&#24202;&#21307;&#29983;&#30340;&#22521;&#35757;&#12289;&#25913;&#21892;&#22270;&#20687;&#36136;&#37327;&#20197;&#21450;&#25552;&#21319;&#19979;&#28216;&#35786;&#26029;&#21644;&#30417;&#27979;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2403.08700</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#21453;&#20107;&#23454;&#35299;&#37322;&#29992;&#20110;&#32974;&#20799;&#36229;&#22768;&#22270;&#20687;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Iterative Counterfactual Explanations for Fetal Ultrasound Image Quality Assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08700
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#25193;&#25955;&#30340;&#36845;&#20195;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#36924;&#30495;&#30340;&#39640;&#36136;&#37327;&#26631;&#20934;&#24179;&#38754;&#65292;&#23545;&#25552;&#39640;&#20020;&#24202;&#21307;&#29983;&#30340;&#22521;&#35757;&#12289;&#25913;&#21892;&#22270;&#20687;&#36136;&#37327;&#20197;&#21450;&#25552;&#21319;&#19979;&#28216;&#35786;&#26029;&#21644;&#30417;&#27979;&#20855;&#26377;&#28508;&#22312;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24576;&#23381;&#26399;&#36229;&#22768;&#22270;&#20687;&#36136;&#37327;&#23545;&#20934;&#30830;&#35786;&#26029;&#21644;&#30417;&#27979;&#32974;&#20799;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26631;&#20934;&#24179;&#38754;&#24456;&#22256;&#38590;&#65292;&#21463;&#21040;&#36229;&#22768;&#27874;&#25216;&#26415;&#20154;&#21592;&#30340;&#19987;&#19994;&#30693;&#35782;&#20197;&#21450;&#20687;&#23381;&#22919;BMI&#25110;&#32974;&#20799;&#21160;&#24577;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#21453;&#20107;&#23454;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65292;&#20174;&#20302;&#36136;&#37327;&#30340;&#38750;&#26631;&#20934;&#24179;&#38754;&#29983;&#25104;&#36924;&#30495;&#30340;&#39640;&#36136;&#37327;&#26631;&#20934;&#24179;&#38754;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29983;&#25104;&#36136;&#37327;&#22686;&#21152;&#30340;&#21487;&#20449;&#21453;&#20107;&#23454;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20026;&#36890;&#36807;&#25552;&#20379;&#35270;&#35273;&#21453;&#39304;&#21152;&#24378;&#20020;&#24202;&#21307;&#29983;&#22521;&#35757;&#20197;&#21450;&#25913;&#36827;&#22270;&#20687;&#36136;&#37327;&#65292;&#20174;&#32780;&#25913;&#21892;&#19979;&#28216;&#35786;&#26029;&#21644;&#30417;&#27979;&#25552;&#20379;&#20102;&#26410;&#26469;&#30340;&#24076;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08700v1 Announce Type: cross  Abstract: Obstetric ultrasound image quality is crucial for accurate diagnosis and monitoring of fetal health. However, producing high-quality standard planes is difficult, influenced by the sonographer's expertise and factors like the maternal BMI or the fetus dynamics. In this work, we propose using diffusion-based counterfactual explainable AI to generate realistic high-quality standard planes from low-quality non-standard ones. Through quantitative and qualitative evaluation, we demonstrate the effectiveness of our method in producing plausible counterfactuals of increased quality. This shows future promise both for enhancing training of clinicians by providing visual feedback, as well as for improving image quality and, consequently, downstream diagnosis and monitoring.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.06759</link><description>&lt;p&gt;
&#24179;&#22343;&#26657;&#20934;&#35823;&#24046;&#65306;&#19968;&#31181;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#20135;&#29983;&#19982;&#32463;&#39564;&#35266;&#23519;&#19981;&#19968;&#33268;&#30340;&#36807;&#20110;&#33258;&#20449;&#30340;&#32467;&#26524;&#65292;&#36825;&#31181;&#26657;&#20934;&#38169;&#35823;&#25361;&#25112;&#30528;&#23427;&#20204;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25913;&#21892;&#20687;&#32032;&#32423;&#26657;&#20934;&#32780;&#19981;&#20250;&#25439;&#23475;&#20998;&#21106;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;&#20351;&#29992;&#30828;&#20998;&#31665;&#65292;&#36825;&#31181;&#25439;&#22833;&#26159;&#30452;&#25509;&#21487;&#24494;&#30340;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#36817;&#20284;&#20294;&#21487;&#24494;&#30340;&#26367;&#20195;&#25110;&#36719;&#20998;&#31665;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#30340;&#27010;&#24565;&#65292;&#36825;&#19968;&#27010;&#24565;&#25512;&#24191;&#20102;&#26631;&#20934;&#30340;&#21487;&#38752;&#24615;&#22270;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#32423;&#21035;&#32858;&#21512;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#32454;&#21270;&#26657;&#20934;&#30340;&#35270;&#35273;&#35780;&#20272;&#12290;&#20351;&#29992;mL1-ACE&#65292;&#25105;&#20204;&#23558;&#24179;&#22343;&#21644;&#26368;&#22823;&#26657;&#20934;&#35823;&#24046;&#20998;&#21035;&#38477;&#20302;&#20102;45%&#21644;55%&#65292;&#21516;&#26102;&#22312;BraTS 2021&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;87%&#30340;Dice&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20998;&#20139;&#25105;&#20204;&#30340;&#20195;&#30721;: https://github
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06759v1 Announce Type: cross  Abstract: Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: https://github
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#36710;&#36742;&#32452;&#20316;&#20026;&#20998;&#26512;&#23545;&#35937;&#65292;&#25506;&#35752;&#20102;&#32771;&#34385;&#36710;&#36742;&#32452;&#21644;&#36947;&#36335;&#27573;&#29305;&#24449;&#30340;&#39118;&#38505;&#24418;&#25104;&#21644;&#20256;&#25773;&#26426;&#21046;&#65292;&#35782;&#21035;&#20986;&#24433;&#21709;&#30896;&#25758;&#39118;&#38505;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.12415</link><description>&lt;p&gt;
&#22522;&#20110;&#36710;&#36742;&#32452;&#30340;&#39640;&#36895;&#20844;&#36335;&#30896;&#25758;&#39118;&#38505;&#24418;&#25104;&#21644;&#20256;&#25773;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Vehicle-group-based Crash Risk Formation and Propagation Analysis for Expressways
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#36710;&#36742;&#32452;&#20316;&#20026;&#20998;&#26512;&#23545;&#35937;&#65292;&#25506;&#35752;&#20102;&#32771;&#34385;&#36710;&#36742;&#32452;&#21644;&#36947;&#36335;&#27573;&#29305;&#24449;&#30340;&#39118;&#38505;&#24418;&#25104;&#21644;&#20256;&#25773;&#26426;&#21046;&#65292;&#35782;&#21035;&#20986;&#24433;&#21709;&#30896;&#25758;&#39118;&#38505;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;&#36335;&#27573;&#19978;&#30340;&#30896;&#25758;&#25968;&#37327;&#25110;&#21487;&#33021;&#24615;&#19982;&#20132;&#36890;&#21442;&#25968;&#25110;&#36335;&#27573;&#30340;&#20960;&#20309;&#29305;&#24449;&#32852;&#31995;&#36215;&#26469;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#36710;&#36742;&#36830;&#32493;&#36816;&#21160;&#21644;&#19982;&#38468;&#36817;&#36710;&#36742;&#30340;&#20114;&#21160;&#23545;&#20854;&#24433;&#21709;&#12290;&#36890;&#20449;&#25216;&#26415;&#30340;&#36827;&#27493;&#36171;&#20104;&#20102;&#20174;&#21608;&#22260;&#36710;&#36742;&#25910;&#38598;&#39550;&#39542;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#30740;&#31350;&#22522;&#20110;&#36710;&#36742;&#32452;&#30340;&#30896;&#25758;&#39118;&#38505;&#25104;&#20026;&#21487;&#33021;&#12290;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#20197;&#36710;&#36742;&#32452;&#20316;&#20026;&#20998;&#26512;&#23545;&#35937;&#65292;&#25506;&#35752;&#20102;&#32771;&#34385;&#36710;&#36742;&#32452;&#21644;&#36947;&#36335;&#27573;&#29305;&#24449;&#30340;&#39118;&#38505;&#24418;&#25104;&#21644;&#20256;&#25773;&#26426;&#21046;&#12290;&#30830;&#23450;&#20102;&#20960;&#20010;&#24433;&#21709;&#30896;&#25758;&#39118;&#38505;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#36807;&#21435;&#30340;&#39640;&#39118;&#38505;&#36710;&#36742;&#32452;&#29366;&#24577;&#12289;&#22797;&#26434;&#30340;&#36710;&#36742;&#34892;&#20026;&#12289;&#22823;&#22411;&#36710;&#36742;&#30340;&#39640;&#30334;&#20998;&#27604;&#12289;&#36710;&#36742;&#32452;&#20869;&#39057;&#32321;&#21464;&#36947;&#20197;&#21450;&#29305;&#23450;&#30340;&#36947;&#36335;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12415v1 Announce Type: new  Abstract: Previous studies in predicting crash risk primarily associated the number or likelihood of crashes on a road segment with traffic parameters or geometric characteristics of the segment, usually neglecting the impact of vehicles' continuous movement and interactions with nearby vehicles. Advancements in communication technologies have empowered driving information collected from surrounding vehicles, enabling the study of group-based crash risks. Based on high-resolution vehicle trajectory data, this research focused on vehicle groups as the subject of analysis and explored risk formation and propagation mechanisms considering features of vehicle groups and road segments. Several key factors contributing to crash risks were identified, including past high-risk vehicle-group states, complex vehicle behaviors, high percentage of large vehicles, frequent lane changes within a vehicle group, and specific road geometries. A multinomial logisti
&lt;/p&gt;</description></item><item><title>SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.11322</link><description>&lt;p&gt;
SpikeNAS: &#19968;&#31181;&#38754;&#21521;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#30340;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SpikeNAS: A Fast Memory-Aware Neural Architecture Search Framework for Spiking Neural Network Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11322
&lt;/p&gt;
&lt;p&gt;
SpikeNAS&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#24110;&#21161;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#24555;&#36895;&#25214;&#21040;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#20026;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25552;&#20379;&#20102;&#23454;&#29616;&#36229;&#20302;&#21151;&#32791;&#35745;&#31639;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;SNN&#26550;&#26500;&#37117;&#28304;&#33258;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31070;&#32463;&#20803;&#30340;&#26550;&#26500;&#21644;&#25805;&#20316;&#19982;SNN&#19981;&#21516;&#65292;&#25110;&#32773;&#22312;&#19981;&#32771;&#34385;&#26469;&#33258;&#24213;&#23618;&#22788;&#29702;&#30828;&#20214;&#30340;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#24320;&#21457;&#12290;&#36825;&#20123;&#38480;&#21046;&#38459;&#30861;&#20102;SNN&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20805;&#20998;&#21457;&#25381;&#28508;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SpikeNAS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20869;&#23384;&#24863;&#30693;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#21487;&#22312;&#32473;&#23450;&#20869;&#23384;&#39044;&#31639;&#19979;&#24555;&#36895;&#25214;&#21040;&#19968;&#20010;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#36866;&#24403;SNN&#26550;&#26500;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;SpikeNAS&#37319;&#29992;&#20102;&#20960;&#20010;&#20851;&#38190;&#27493;&#39588;&#65306;&#20998;&#26512;&#32593;&#32476;&#25805;&#20316;&#23545;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#22686;&#24378;&#32593;&#32476;&#26550;&#26500;&#20197;&#25552;&#39640;&#23398;&#20064;&#36136;&#37327;&#65292;&#24182;&#24320;&#21457;&#24555;&#36895;&#20869;&#23384;&#24863;&#30693;&#25628;&#32034;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11322v1 Announce Type: cross  Abstract: Spiking Neural Networks (SNNs) offer a promising solution to achieve ultra low-power/energy computation for solving machine learning tasks. Currently, most of the SNN architectures are derived from Artificial Neural Networks whose neurons' architectures and operations are different from SNNs, or developed without considering memory budgets from the underlying processing hardware. These limitations hinder the SNNs from reaching their full potential in accuracy and efficiency. Towards this, we propose SpikeNAS, a novel memory-aware neural architecture search (NAS) framework for SNNs that can quickly find an appropriate SNN architecture with high accuracy under the given memory budgets. To do this, our SpikeNAS employs several key steps: analyzing the impacts of network operations on the accuracy, enhancing the network architecture to improve the learning quality, and developing a fast memory-aware search algorithm. The experimental resul
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2402.10787</link><description>&lt;p&gt;
EdgeQAT: &#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65292;&#29992;&#20110;&#21152;&#36895;&#36731;&#37327;&#32423;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EdgeQAT: Entropy and Distribution Guided Quantization-Aware Training for the Acceleration of Lightweight LLMs on the Edge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#20351;&#29992;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#26041;&#27861;&#26469;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#65292;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25512;&#29702;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#21644;&#35745;&#31639;&#37327;&#65292;LLMs&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36890;&#24120;&#37319;&#29992;&#37327;&#21270;&#26041;&#27861;&#29983;&#25104;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#24555;&#36895;&#25512;&#29702;&#30340;&#36731;&#37327;&#32423;LLMs&#12290;&#28982;&#32780;&#65292;&#21518;&#35757;&#32451;&#37327;&#21270;&#65288;PTQ&#65289;&#26041;&#27861;&#22312;&#23558;&#26435;&#37325;&#12289;&#28608;&#27963;&#21644;KV&#32531;&#23384;&#19968;&#36215;&#37327;&#21270;&#33267;8&#20301;&#20197;&#19979;&#26102;&#65292;&#36136;&#37327;&#20250;&#24613;&#21095;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#65288;QAT&#65289;&#24037;&#20316;&#23545;&#27169;&#22411;&#26435;&#37325;&#36827;&#34892;&#37327;&#21270;&#65292;&#32780;&#28608;&#27963;&#26410;&#34987;&#35302;&#21450;&#65292;&#36825;&#19981;&#33021;&#20805;&#20998;&#21457;&#25381;&#37327;&#21270;&#23545;&#36793;&#32536;&#31471;&#25512;&#29702;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EdgeQAT&#65292;&#21363;&#29109;&#21644;&#20998;&#24067;&#24341;&#23548;&#30340;QAT&#65292;&#29992;&#20110;&#20248;&#21270;&#36731;&#37327;&#32423;LLMs&#20197;&#23454;&#29616;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25512;&#29702;&#21152;&#36895;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#37327;&#21270;&#24615;&#33021;&#19979;&#38477;&#20027;&#35201;&#28304;&#33258;&#20449;&#24687;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10787v1 Announce Type: cross  Abstract: Despite the remarkable strides of Large Language Models (LLMs) in various fields, the wide applications of LLMs on edge devices are limited due to their massive parameters and computations. To address this, quantization is commonly adopted to generate lightweight LLMs with efficient computations and fast inference. However, Post-Training Quantization (PTQ) methods dramatically degrade in quality when quantizing weights, activations, and KV cache together to below 8 bits. Besides, many Quantization-Aware Training (QAT) works quantize model weights, leaving the activations untouched, which do not fully exploit the potential of quantization for inference acceleration on the edge. In this paper, we propose EdgeQAT, the Entropy and Distribution Guided QAT for the optimization of lightweight LLMs to achieve inference acceleration on Edge devices. We first identify that the performance drop of quantization primarily stems from the information
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.02325</link><description>&lt;p&gt;
&#21160;&#37327;&#22312;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#20013;&#23545;&#30446;&#26631;&#20989;&#25968;&#30340;&#24179;&#28369;&#20316;&#29992;&#30340;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Role of Momentum in Smoothing Objective Function in Implicit Graduated Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02325
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25581;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#24433;&#21709;&#31243;&#24230;&#30001;&#22810;&#20010;&#36229;&#21442;&#25968;&#20915;&#23450;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#23545;&#21160;&#37327;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;&#30340;&#29702;&#35770;&#35299;&#37322;&#21644;&#26032;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20855;&#26377;&#21160;&#37327;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#21644;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#23545;&#27492;&#32570;&#20047;&#29702;&#35770;&#35299;&#37322;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20855;&#26377;&#21160;&#37327;&#30340;SGD&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#65292;&#20854;&#31243;&#24230;&#30001;&#23398;&#20064;&#29575;&#12289;&#25209;&#22823;&#23567;&#12289;&#21160;&#37327;&#22240;&#23376;&#12289;&#38543;&#26426;&#26799;&#24230;&#30340;&#26041;&#24046;&#20197;&#21450;&#26799;&#24230;&#33539;&#25968;&#30340;&#19978;&#30028;&#30830;&#23450;&#12290;&#36825;&#19968;&#29702;&#35770;&#21457;&#29616;&#25581;&#31034;&#20102;&#20026;&#20160;&#20040;&#21160;&#37327;&#25913;&#21892;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#21160;&#37327;&#22240;&#23376;&#31561;&#36229;&#21442;&#25968;&#20316;&#29992;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;SGD&#21160;&#37327;&#24179;&#28369;&#29305;&#24615;&#30340;&#38544;&#24335;&#36880;&#27493;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#25903;&#25345;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#21363;SGD&#21160;&#37327;&#24179;&#28369;&#20102;&#30446;&#26631;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
While stochastic gradient descent (SGD) with momentum has fast convergence and excellent generalizability, a theoretical explanation for this is lacking. In this paper, we show that SGD with momentum smooths the objective function, the degree of which is determined by the learning rate, the batch size, the momentum factor, the variance of the stochastic gradient, and the upper bound of the gradient norm. This theoretical finding reveals why momentum improves generalizability and provides new insights into the role of the hyperparameters, including momentum factor. We also present an implicit graduated optimization algorithm that exploits the smoothing properties of SGD with momentum and provide experimental results supporting our assertion that SGD with momentum smooths the objective function.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;Wasserstein&#36317;&#31163;&#21450;&#20854;&#37325;&#24515;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20998;&#24067;&#21305;&#37197;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18531</link><description>&lt;p&gt;
&#36890;&#36807;Wasserstein&#24230;&#37327;&#36827;&#34892;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Dataset Distillation via the Wasserstein Metric
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18531
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;Wasserstein&#36317;&#31163;&#21450;&#20854;&#37325;&#24515;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#31934;&#28860;&#26041;&#27861;&#65292;&#21033;&#29992;&#20808;&#39564;&#30693;&#35782;&#25552;&#39640;&#20998;&#24067;&#21305;&#37197;&#25928;&#26524;&#65292;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#31574;&#30053;&#65292;&#23558;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#20449;&#24687;&#23553;&#35013;&#20026;&#26126;&#26174;&#26356;&#23567;&#30340;&#21512;&#25104;&#31561;&#20215;&#29289;&#65292;&#20174;&#32780;&#22312;&#20943;&#23569;&#35745;&#31639;&#24320;&#38144;&#30340;&#21516;&#26102;&#20445;&#30041;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#36317;&#31163;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#29702;&#35770;&#30340;&#24230;&#37327;&#65292;&#29992;&#20110;&#22686;&#24378;DD&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;Wasserstein&#37325;&#24515;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#37327;&#21270;&#20998;&#24067;&#24046;&#24322;&#21644;&#39640;&#25928;&#25429;&#33719;&#20998;&#24067;&#38598;&#21512;&#20013;&#24515;&#30340;&#20960;&#20309;&#24847;&#20041;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#20998;&#31867;&#27169;&#22411;&#30340;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#20419;&#36827;&#20102;&#26377;&#25928;&#30340;&#20998;&#24067;&#21305;&#37197;&#65292;&#21033;&#29992;&#36825;&#20123;&#27169;&#22411;&#22266;&#26377;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#20445;&#25345;&#20102;&#22522;&#20110;&#20998;&#24067;&#21305;&#37197;&#30340;&#25216;&#26415;&#30340;&#35745;&#31639;&#20248;&#21183;&#65292;&#32780;&#19988;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18531v2 Announce Type: replace-cross  Abstract: Dataset Distillation (DD) emerges as a powerful strategy to encapsulate the expansive information of large datasets into significantly smaller, synthetic equivalents, thereby preserving model performance with reduced computational overhead. Pursuing this objective, we introduce the Wasserstein distance, a metric grounded in optimal transport theory, to enhance distribution matching in DD. Our approach employs the Wasserstein barycenter to provide a geometrically meaningful method for quantifying distribution differences and capturing the centroid of distribution sets efficiently. By embedding synthetic data in the feature spaces of pretrained classification models, we facilitate effective distribution matching that leverages prior knowledge inherent in these models. Our method not only maintains the computational advantages of distribution matching-based techniques but also achieves new state-of-the-art performance across a ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;</title><link>http://arxiv.org/abs/2311.01356</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;
&lt;/p&gt;
&lt;p&gt;
On the Lipschitz constant of random neural networks. (arXiv:2311.01356v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;Lipschitz&#24120;&#25968;&#30340;&#31934;&#30830;&#21051;&#30011;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#19978;&#19979;&#30028;&#65292;&#24182;&#21305;&#37197;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#35777;&#30740;&#31350;&#24191;&#27867;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#23545;&#36755;&#20837;&#30340;&#24494;&#23567;&#23545;&#25239;&#24615;&#25200;&#21160;&#38750;&#24120;&#25935;&#24863;&#12290;&#36825;&#20123;&#25152;&#35859;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26368;&#22351;&#24773;&#20917;&#40065;&#26834;&#24615;&#21487;&#20197;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#26469;&#37327;&#21270;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#20010;&#37327;&#30340;&#29702;&#35770;&#32467;&#26524;&#22312;&#25991;&#29486;&#20013;&#20165;&#26377;&#23569;&#25968;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#22987;&#30740;&#31350;&#38543;&#26426;ReLU&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#65292;&#21363;&#36873;&#25321;&#38543;&#26426;&#26435;&#37325;&#24182;&#37319;&#29992;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#23545;&#20110;&#27973;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#23558;Lipschitz&#24120;&#25968;&#21051;&#30011;&#21040;&#19968;&#20010;&#32477;&#23545;&#25968;&#20540;&#24120;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#36275;&#22815;&#23485;&#24230;&#30340;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Lipschitz&#24120;&#25968;&#30340;&#19978;&#19979;&#30028;&#12290;&#36825;&#20123;&#30028;&#21305;&#37197;&#21040;&#19968;&#20010;&#20381;&#36182;&#20110;&#28145;&#24230;&#30340;&#23545;&#25968;&#22240;&#23376;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical studies have widely demonstrated that neural networks are highly sensitive to small, adversarial perturbations of the input. The worst-case robustness against these so-called adversarial examples can be quantified by the Lipschitz constant of the neural network. However, only few theoretical results regarding this quantity exist in the literature. In this paper, we initiate the study of the Lipschitz constant of random ReLU neural networks, i.e., neural networks whose weights are chosen at random and which employ the ReLU activation function. For shallow neural networks, we characterize the Lipschitz constant up to an absolute numerical constant. Moreover, we extend our analysis to deep neural networks of sufficiently large width where we prove upper and lower bounds for the Lipschitz constant. These bounds match up to a logarithmic factor that depends on the depth.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#37325;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;EEG&#20449;&#21495;&#36827;&#34892;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#26102;&#23384;&#22312;&#30340;&#20302;&#20449;&#22122;&#27604;&#12289;&#38750;&#31283;&#24577;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#22797;&#26434;&#24615;&#31561;&#25361;&#25112;&#65292;&#36890;&#36807;&#38477;&#20302;&#22122;&#22768;&#21644;&#26080;&#20851;&#20449;&#24687;&#65292;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.02515</link><description>&lt;p&gt;
&#22522;&#20110;EEG&#30340;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#30340;&#29305;&#24449;&#37325;&#21152;&#26435;
&lt;/p&gt;
&lt;p&gt;
Feature Reweighting for EEG-based Motor Imagery Classification. (arXiv:2308.02515v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02515
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#24449;&#37325;&#21152;&#26435;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20351;&#29992;EEG&#20449;&#21495;&#36827;&#34892;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;&#26102;&#23384;&#22312;&#30340;&#20302;&#20449;&#22122;&#27604;&#12289;&#38750;&#31283;&#24577;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#22797;&#26434;&#24615;&#31561;&#25361;&#25112;&#65292;&#36890;&#36807;&#38477;&#20302;&#22122;&#22768;&#21644;&#26080;&#20851;&#20449;&#24687;&#65292;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#36827;&#34892;&#36816;&#21160;&#24819;&#35937;&#65288;MI&#65289;&#20998;&#31867;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#30446;&#26631;&#65292;&#22240;&#20026;&#23427;&#29992;&#20110;&#39044;&#27979;&#20027;&#20307;&#32930;&#20307;&#31227;&#21160;&#30340;&#24847;&#22270;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;MI-EEG&#20998;&#31867;&#12290;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;MI-EEG&#20449;&#21495;&#20998;&#31867;&#30340;&#25361;&#25112;&#21253;&#25324;&#20449;&#22122;&#27604;&#20302;&#12289;&#38750;&#31283;&#24577;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;EEG&#20449;&#21495;&#30340;&#22797;&#26434;&#24615;&#12290;&#22522;&#20110;CNN&#30340;&#32593;&#32476;&#35745;&#31639;&#24471;&#21040;&#30340;MI-EEG&#20449;&#21495;&#29305;&#24449;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#30001;&#22122;&#22768;&#21644;&#26080;&#20851;&#29305;&#24449;&#35745;&#31639;&#24471;&#21040;&#30340;CNN&#32593;&#32476;&#30340;&#29305;&#24449;&#22270;&#20063;&#21253;&#21547;&#26080;&#20851;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#26080;&#29992;&#30340;&#29305;&#24449;&#24120;&#24120;&#35823;&#23548;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#38477;&#20302;&#20998;&#31867;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#29305;&#24449;&#37325;&#21152;&#26435;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classification of motor imagery (MI) using non-invasive electroencephalographic (EEG) signals is a critical objective as it is used to predict the intention of limb movements of a subject. In recent research, convolutional neural network (CNN) based methods have been widely utilized for MI-EEG classification. The challenges of training neural networks for MI-EEG signals classification include low signal-to-noise ratio, non-stationarity, non-linearity, and high complexity of EEG signals. The features computed by CNN-based networks on the highly noisy MI-EEG signals contain irrelevant information. Subsequently, the feature maps of the CNN-based network computed from the noisy and irrelevant features contain irrelevant information. Thus, many non-contributing features often mislead the neural network training and degrade the classification performance. Hence, a novel feature reweighting approach is proposed to address this issue. The proposed method gives a noise reduction mechanism named
&lt;/p&gt;</description></item></channel></rss>