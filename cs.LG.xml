<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(CoPSL)&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#24085;&#32047;&#25176;&#38598;&#65292;&#36890;&#36807;&#20849;&#20139;&#21644;&#29305;&#23450;&#23618;&#30340;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;MOP&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2404.01224</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Collaborative Pareto Set Learning in Multiple Multi-Objective Optimization Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01224
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(CoPSL)&#26694;&#26550;&#65292;&#21487;&#20197;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#30340;&#24085;&#32047;&#25176;&#38598;&#65292;&#36890;&#36807;&#20849;&#20139;&#21644;&#29305;&#23450;&#23618;&#30340;&#32467;&#26500;&#65292;&#23454;&#29616;&#20102;&#19981;&#21516;MOP&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(PSL)&#26159;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#19968;&#20010;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20174;&#20559;&#22909;&#21521;&#37327;&#21040;&#24085;&#32047;&#25176;&#26368;&#20248;&#35299;&#30340;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;PSL&#26041;&#27861;&#20165;&#38480;&#20110;&#19968;&#27425;&#35299;&#20915;&#21333;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;(MOP)&#12290;&#38754;&#23545;&#22810;&#20010;MOP&#26102;&#65292;&#36825;&#31181;&#38480;&#21046;&#19981;&#20165;&#23548;&#33268;&#26174;&#33879;&#30340;&#20302;&#25928;&#65292;&#32780;&#19988;&#26410;&#33021;&#21033;&#29992;&#27178;&#36328;&#19981;&#21516;MOP&#30340;&#28508;&#22312;&#21327;&#21516;&#25928;&#24212;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#24085;&#32047;&#25176;&#38598;&#23398;&#20064;(CoPSL)&#26694;&#26550;&#65292;&#23427;&#20197;&#21327;&#21516;&#26041;&#24335;&#21516;&#26102;&#23398;&#20064;&#22810;&#20010;MOP&#30340;&#24085;&#32047;&#25176;&#38598;&#12290;CoPSL&#37319;&#29992;&#20102;&#19968;&#20010;&#26550;&#26500;&#65292;&#21253;&#25324;&#20849;&#20139;&#21644;MOP&#29305;&#23450;&#23618;&#65292;&#20854;&#20013;&#20849;&#20139;&#23618;&#26088;&#22312;&#21327;&#21516;&#25429;&#25417;MOP&#20043;&#38388;&#30340;&#20844;&#20849;&#20851;&#31995;&#65292;&#32780;MOP&#29305;&#23450;&#23618;&#22788;&#29702;&#36825;&#20123;&#20851;&#31995;&#20197;&#29983;&#25104;&#27599;&#20010;MOP&#30340;&#35299;&#38598;&#12290;&#36825;&#31181;&#21327;&#21516;&#26041;&#27861;&#20351;&#24471;CoPSL&#33021;&#22815;&#39640;&#25928;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01224v1 Announce Type: new  Abstract: Pareto Set Learning (PSL) is an emerging research area in multi-objective optimization, focusing on training neural networks to learn the mapping from preference vectors to Pareto optimal solutions. However, existing PSL methods are limited to addressing a single Multi-objective Optimization Problem (MOP) at a time. When faced with multiple MOPs, this limitation not only leads to significant inefficiencies but also fails to exploit the potential synergies across varying MOPs. In this paper, we propose a Collaborative Pareto Set Learning (CoPSL) framework, which simultaneously learns the Pareto sets of multiple MOPs in a collaborative manner. CoPSL employs an architecture consisting of shared and MOP-specific layers, where shared layers aim to capture common relationships among MOPs collaboratively, and MOP-specific layers process these relationships to generate solution sets for each MOP. This collaborative approach enables CoPSL to effi
&lt;/p&gt;</description></item><item><title>KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15304</link><description>&lt;p&gt;
KTbench&#65306;&#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15304
&lt;/p&gt;
&lt;p&gt;
KTbench&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#25968;&#25454;&#27844;&#28431;&#30340;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;KT&#27169;&#22411;&#20013;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#23398;&#20064;&#21487;&#33021;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#28041;&#21450;&#22312;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#39044;&#27979;&#23398;&#29983;&#23545;&#23398;&#20064;&#39033;&#30446;&#30340;&#26410;&#26469;&#34920;&#29616;&#12290;&#23398;&#20064;&#39033;&#30446;&#34987;&#26631;&#35760;&#20026;&#31216;&#20026;&#30693;&#35782;&#27010;&#24565;&#65288;KCs&#65289;&#30340;&#25216;&#33021;&#26631;&#31614;&#12290;&#35768;&#22810;KT&#27169;&#22411;&#36890;&#36807;&#29992;&#26500;&#25104;KC&#30340;&#23398;&#20064;&#39033;&#30446;&#21462;&#20195;&#23398;&#20064;&#39033;&#30446;&#26469;&#23558;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#25193;&#23637;&#20026;KC-&#23398;&#29983;&#20132;&#20114;&#24207;&#21015;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#31232;&#30095;&#30340;&#23398;&#20064;&#39033;&#30446;-&#23398;&#29983;&#20132;&#20114;&#38382;&#39064;&#24182;&#26368;&#23567;&#21270;&#20102;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#27169;&#22411;&#23398;&#20064;&#21516;&#19968;&#39033;&#30446;&#20869;&#30340;KC&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#22522;&#26412;&#20107;&#23454;&#26631;&#31614;&#30340;&#27844;&#28431;&#24182;&#38459;&#30861;&#27169;&#22411;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#29616;&#26377;&#30340;&#22522;&#20934;&#23454;&#29616;&#24573;&#30053;&#20102;&#35745;&#25968;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15304v1 Announce Type: cross  Abstract: Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.   The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounti
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLR&#30340;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;HyperVQ&#65292;&#32467;&#21512;&#20102;&#30690;&#37327;&#37327;&#21270;&#21644;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#23398;&#20064;&#25968;&#25454;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.13015</link><description>&lt;p&gt;
HyperVQ&#65306;&#22522;&#20110;MLR&#30340;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#21521;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
HyperVQ: MLR-based Vector Quantization in Hyperbolic Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13015
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;MLR&#30340;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;HyperVQ&#65292;&#32467;&#21512;&#20102;&#30690;&#37327;&#37327;&#21270;&#21644;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#30340;&#20248;&#21183;&#65292;&#29992;&#20110;&#26356;&#22909;&#22320;&#23398;&#20064;&#25968;&#25454;&#30340;&#32039;&#20945;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#22312;&#25506;&#35752;&#22522;&#20110;tokenized&#25968;&#25454;&#30340;&#27169;&#22411;&#21462;&#24471;&#25104;&#21151;&#20043;&#21518;&#65292;&#23545;&#26377;&#25928;&#30340;tokenization&#26041;&#27861;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#38750;&#31163;&#25955;&#25968;&#25454;&#30340;&#35270;&#35273;&#25110;&#21548;&#35273;&#20219;&#21153;&#20013;&#12290;&#20854;&#20013;&#65292;&#26368;&#27969;&#34892;&#30340;tokenization&#26041;&#27861;&#20043;&#19968;&#26159;&#30690;&#37327;&#37327;&#21270;&#65288;VQ&#65289;&#65292;&#23427;&#26159;&#21508;&#20010;&#39046;&#22495;&#26368;&#26032;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#20214;&#20043;&#19968;&#12290;&#36890;&#24120;&#65292;VQ&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQVAE&#65289;&#34987;&#35757;&#32451;&#29992;&#20110;&#23558;&#25968;&#25454;&#36716;&#25442;&#21040;&#20854;&#32463;&#36807;tokenization&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#28982;&#21518;&#20877;&#36716;&#25442;&#22238;&#21435;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;VQVAE&#26159;&#36890;&#36807;&#37325;&#26500;&#30446;&#26631;&#26469;&#35757;&#32451;&#30340;&#65292;&#23545;&#20110;&#23884;&#20837;&#26159;&#21542;&#34987;&#24456;&#22909;&#22320;&#20998;&#35299;&#20026;&#19981;&#21516;&#21442;&#25968;&#24182;&#27809;&#26377;&#32422;&#26463;&#65292;&#36825;&#23545;&#20110;&#23558;&#23427;&#20204;&#29992;&#20110;&#21306;&#20998;&#20219;&#21153;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#20316;&#21697;&#24050;&#32463;&#35777;&#26126;&#20102;&#21033;&#29992;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#22909;&#22788;&#12290;&#36229;&#21322;&#24179;&#38754;&#31354;&#38388;&#30001;&#20110;&#20854;&#25351;&#25968;&#32423;&#20307;&#31215;&#22686;&#38271;&#21644;&#22266;&#26377;&#30340;&#24314;&#27169;&#20998;&#23618;&#21644;&#32467;&#22270;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#20135;&#29983;&#20102;&#32039;&#20945;&#30340;&#28508;&#22312;&#34920;&#31034;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13015v1 Announce Type: cross  Abstract: The success of models operating on tokenized data has led to an increased demand for effective tokenization methods, particularly when applied to vision or auditory tasks, which inherently involve non-discrete data. One of the most popular tokenization methods is Vector Quantization (VQ), a key component of several recent state-of-the-art methods across various domains. Typically, a VQ Variational Autoencoder (VQVAE) is trained to transform data to and from its tokenized representation. However, since the VQVAE is trained with a reconstruction objective, there is no constraint for the embeddings to be well disentangled, a crucial aspect for using them in discriminative tasks. Recently, several works have demonstrated the benefits of utilizing hyperbolic spaces for representation learning. Hyperbolic spaces induce compact latent representations due to their exponential volume growth and inherent ability to model hierarchical and structu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#23618;&#27425;&#65288;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.12529</link><description>&lt;p&gt;
&#19978;&#19979;&#25991;&#21270;&#20449;&#24687;&#25552;&#21319;&#20102;&#22270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Contextualized Messages Boost Graph Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12529
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#23618;&#27425;&#65288;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22240;&#20854;&#22788;&#29702;&#20197;&#22270;&#34920;&#31034;&#30340;&#20219;&#24847;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#33021;&#21147;&#32780;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;GNN&#36890;&#24120;&#36981;&#24490;&#28040;&#24687;&#20256;&#36882;&#26041;&#26696;&#26469;&#26412;&#22320;&#26356;&#26032;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#21518;&#20351;&#29992;&#22270;&#35835;&#20986;&#20989;&#25968;&#21019;&#24314;&#25972;&#20010;&#22270;&#30340;&#34920;&#31034;&#12290;&#19968;&#20123;&#30740;&#31350;&#36890;&#36807;&#20462;&#25913;&#28040;&#24687;&#20256;&#36882;&#26694;&#26550;&#30340;&#32858;&#21512;&#21644;&#32452;&#21512;&#31574;&#30053;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;GNN&#65292;&#24120;&#24120;&#21463;&#21551;&#21457;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#30740;&#31350;&#24050;&#32463;&#24320;&#22987;&#20174;&#22522;&#20110;&#22270;&#21516;&#26500;&#38382;&#39064;&#30340;&#29702;&#35770;&#35282;&#24230;&#25506;&#32034;GNN&#65292;&#35813;&#38382;&#39064;&#22266;&#26377;&#22320;&#20551;&#35774;&#21487;&#25968;&#30340;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#29702;&#35770;&#24037;&#20316;&#25506;&#32034;&#20102;&#20855;&#26377;&#19981;&#21487;&#25968;&#33410;&#28857;&#29305;&#24449;&#34920;&#31034;&#30340;GNN&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20851;&#20110;GNN&#22312;&#33410;&#28857;&#32423;&#12289;&#37051;&#22495;&#32423;&#21644;&#22270;&#32423;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12529v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have gained significant interest in recent years due to their ability to handle arbitrarily structured data represented as graphs. GNNs generally follow the message-passing scheme to locally update node feature representations. A graph readout function is then employed to create a representation for the entire graph. Several studies proposed different GNNs by modifying the aggregation and combination strategies of the message-passing framework, often inspired by heuristics. Nevertheless, several studies have begun exploring GNNs from a theoretical perspective based on the graph isomorphism problem which inherently assumes countable node feature representations. Yet, there are only a few theoretical works exploring GNNs with uncountable node feature representations. This paper presents a new perspective on the representational capabilities of GNNs across all levels - node-level, neighborhood-level, and graph-l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20445;&#25345;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10045</link><description>&lt;p&gt;
&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#23454;&#29616;&#23545;&#25239;&#40065;&#26834;&#24615;&#25968;&#25454;&#38598;&#31934;&#28860;
&lt;/p&gt;
&lt;p&gt;
Towards Adversarially Robust Dataset Distillation by Curvature Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#26354;&#29575;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20445;&#25345;&#27169;&#22411;&#39640;&#20934;&#30830;&#24615;&#24182;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#31934;&#28860;&#65288;DD&#65289;&#20801;&#35768;&#23558;&#25968;&#25454;&#38598;&#31934;&#28860;&#20026;&#21407;&#22987;&#22823;&#23567;&#30340;&#20998;&#25968;&#65292;&#21516;&#26102;&#20445;&#30041;&#20016;&#23500;&#30340;&#20998;&#24067;&#20449;&#24687;&#65292;&#20351;&#24471;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#22312;&#33410;&#30465;&#26174;&#33879;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#36798;&#21040;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#25552;&#39640;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;DD&#30340;&#19968;&#31181;&#26032;&#35270;&#35282;&#12290;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#22312;&#31934;&#28860;&#25968;&#25454;&#38598;&#20013;&#23884;&#20837;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20197;&#20351;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20445;&#25345;&#39640;&#31934;&#24230;&#30340;&#21516;&#26102;&#33719;&#24471;&#26356;&#22909;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23558;&#26354;&#29575;&#27491;&#21017;&#21270;&#32435;&#20837;&#21040;&#31934;&#28860;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26032;&#26041;&#27861;&#65292;&#32780;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#24320;&#38144;&#27604;&#26631;&#20934;&#30340;&#23545;&#25239;&#35757;&#32451;&#35201;&#23569;&#24471;&#22810;&#12290;&#22823;&#37327;&#30340;&#23454;&#35777;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#20934;&#30830;&#24615;&#19978;&#20248;&#20110;&#26631;&#20934;&#23545;&#25239;&#35757;&#32451;&#65292;&#21516;&#26102;&#22312;&#23545;&#25239;&#24615;&#33021;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10045v1 Announce Type: new  Abstract: Dataset distillation (DD) allows datasets to be distilled to fractions of their original size while preserving the rich distributional information so that models trained on the distilled datasets can achieve a comparable accuracy while saving significant computational loads. Recent research in this area has been focusing on improving the accuracy of models trained on distilled datasets. In this paper, we aim to explore a new perspective of DD. We study how to embed adversarial robustness in distilled datasets, so that models trained on these datasets maintain the high accuracy and meanwhile acquire better adversarial robustness. We propose a new method that achieves this goal by incorporating curvature regularization into the distillation process with much less computational overhead than standard adversarial training. Extensive empirical experiments suggest that our method not only outperforms standard adversarial training on both accur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.08955</link><description>&lt;p&gt;
&#26397;&#21521;&#39640;&#25928;&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471;&#33258;&#20027;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#24179;&#34913;&#20102;&#26399;&#26395;&#22238;&#25253;&#21644;&#39118;&#38505;&#65292;&#20855;&#26377;&#20135;&#29983;&#27010;&#29575;&#40065;&#26834;&#31574;&#30053;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;REINFORCE&#31639;&#27861;&#24182;&#37319;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;$\mathcal{O}(\epsilon^{-2})$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20197;&#36798;&#21040;$\epsilon$-&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;FOSP&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#27604;&#39118;&#38505;&#20013;&#24615;&#31639;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08955v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#20316;&#32773;&#25991;&#20214;&#22312;&#20505;&#36873;&#20316;&#32773;&#35821;&#27861;&#27169;&#22411;&#19982;&#21442;&#32771;&#32676;&#20307;&#35821;&#27861;&#27169;&#22411;&#19979;&#30340;&#21487;&#33021;&#24615;&#27604;&#29575;&#30340;&#26041;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#31185;&#23398;&#35299;&#37322;&#19981;&#36275;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.08462</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#27861;&#27169;&#22411;&#20284;&#28982;&#27604;&#30340;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Authorship Verification based on the Likelihood Ratio of Grammar Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35745;&#31639;&#20316;&#32773;&#25991;&#20214;&#22312;&#20505;&#36873;&#20316;&#32773;&#35821;&#27861;&#27169;&#22411;&#19982;&#21442;&#32771;&#32676;&#20307;&#35821;&#27861;&#27169;&#22411;&#19979;&#30340;&#21487;&#33021;&#24615;&#27604;&#29575;&#30340;&#26041;&#27861;&#65292;&#29992;&#20197;&#35299;&#20915;&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#20013;&#23384;&#22312;&#30340;&#31185;&#23398;&#35299;&#37322;&#19981;&#36275;&#21644;&#38590;&#20197;&#35299;&#37322;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#32773;&#36523;&#20221;&#39564;&#35777;&#65288;AV&#65289;&#26159;&#20998;&#26512;&#19968;&#32452;&#25991;&#20214;&#20197;&#30830;&#23450;&#23427;&#20204;&#26159;&#21542;&#30001;&#29305;&#23450;&#20316;&#32773;&#25776;&#20889;&#30340;&#36807;&#31243;&#12290;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;AV&#26041;&#27861;&#20351;&#29992;&#35745;&#31639;&#35299;&#20915;&#26041;&#26696;&#65292;&#23545;&#20110;&#20854;&#21151;&#33021;&#27809;&#26377;&#21512;&#29702;&#30340;&#31185;&#23398;&#35299;&#37322;&#65292;&#24182;&#19988;&#24120;&#24120;&#38590;&#20197;&#35299;&#37322;&#32473;&#20998;&#26512;&#20154;&#21592;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20381;&#36182;&#20110;&#35745;&#31639;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026; $\lambda_G$&#65288;LambdaG&#65289;&#30340;&#37327;&#65306;&#20505;&#36873;&#20316;&#32773;&#30340;&#19978;&#19979;&#25991;&#35821;&#27861;&#27169;&#22411;&#32473;&#20986;&#30340;&#25991;&#26723;&#30340;&#21487;&#33021;&#24615;&#19982;&#21442;&#32771;&#32676;&#20307;&#30340;&#19978;&#19979;&#25991;&#35821;&#27861;&#27169;&#22411;&#32473;&#20986;&#30340;&#30456;&#21516;&#25991;&#26723;&#30340;&#21487;&#33021;&#24615;&#20043;&#38388;&#30340;&#27604;&#29575;&#12290;&#36825;&#20123;&#35821;&#27861;&#27169;&#22411;&#26159;&#20351;&#29992;&#20165;&#38024;&#23545;&#35821;&#27861;&#29305;&#24449;&#36827;&#34892;&#35757;&#32451;&#30340; $n$-gram&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20272;&#35745;&#30340;&#12290;&#23613;&#31649;&#19981;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;LambdaG...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08462v1 Announce Type: new  Abstract: Authorship Verification (AV) is the process of analyzing a set of documents to determine whether they were written by a specific author. This problem often arises in forensic scenarios, e.g., in cases where the documents in question constitute evidence for a crime. Existing state-of-the-art AV methods use computational solutions that are not supported by a plausible scientific explanation for their functioning and that are often difficult for analysts to interpret. To address this, we propose a method relying on calculating a quantity we call $\lambda_G$ (LambdaG): the ratio between the likelihood of a document given a model of the Grammar for the candidate author and the likelihood of the same document given a model of the Grammar for a reference population. These Grammar Models are estimated using $n$-gram language models that are trained solely on grammatical features. Despite not needing large amounts of data for training, LambdaG st
&lt;/p&gt;</description></item><item><title>XpertAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#29305;&#23450;&#33539;&#22260;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#27169;&#22411;&#30340;&#26597;&#35810;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.07486</link><description>&lt;p&gt;
XpertAI&#65306;&#25581;&#31034;&#23376;&#27969;&#24418;&#30340;&#27169;&#22411;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
XpertAI: uncovering model strategies for sub-manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07486
&lt;/p&gt;
&lt;p&gt;
XpertAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#29305;&#23450;&#33539;&#22260;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#27169;&#22411;&#30340;&#26597;&#35810;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#20419;&#36827;&#20102;&#28145;&#20837;&#39564;&#35777;&#21644;&#30693;&#35782;&#25552;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#38024;&#23545;&#20998;&#31867;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#26377;XAI&#35299;&#20915;&#26041;&#26696;&#35299;&#20915;&#20102;&#29305;&#23450;&#20110;&#22238;&#24402;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#22238;&#24402;&#20013;&#65292;&#35299;&#37322;&#38656;&#35201;&#31934;&#30830;&#21046;&#23450;&#20197;&#24212;&#23545;&#29305;&#23450;&#29992;&#25143;&#26597;&#35810;&#65288;&#20363;&#22914;&#21306;&#20998;&#8220;&#20026;&#20160;&#20040;&#36755;&#20986;&#22823;&#20110;0&#65311;&#8221;&#21644;&#8220;&#20026;&#20160;&#20040;&#36755;&#20986;&#22823;&#20110;50&#65311;&#8221;&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24212;&#21453;&#26144;&#27169;&#22411;&#22312;&#30456;&#20851;&#25968;&#25454;&#23376;&#27969;&#24418;&#19978;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;XpertAI&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#33539;&#22260;&#29305;&#23450;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#23545;&#27169;&#22411;&#30340;&#31934;&#20934;&#26597;&#35810;&#65288;&#8220;&#34987;&#35299;&#37322;&#29289;&#8221;&#65289;&#30340;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#30340;&#26694;&#26550;&#12290;XpertAI&#36890;&#24120;&#21046;&#23450;&#21487;&#20197;&#19982;&#22522;&#20110;&#36974;&#25377;&#12289;&#26799;&#24230;&#38598;&#25104;&#25110;&#21453;&#21521;&#20256;&#25773;&#30340;&#27969;&#34892;XAI&#24402;&#22240;&#25216;&#26415;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07486v1 Announce Type: new  Abstract: In recent years, Explainable AI (XAI) methods have facilitated profound validation and knowledge extraction from ML models. While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models. In regression, explanations need to be precisely formulated to address specific user queries (e.g.\ distinguishing between `Why is the output above 0?' and `Why is the output above 50?'). They should furthermore reflect the model's behavior on the relevant data sub-manifold. In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum') as a linear combination of those sub-strategies. XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation. Qualitat
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.02514</link><description>&lt;p&gt;
&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#26426;&#22120;&#20154;&#35774;&#23450;&#30446;&#30340;&#65306;&#19968;&#20010;&#35745;&#31639;&#20998;&#31867;&#12289;&#23450;&#20041;&#21644;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#20027;&#24320;&#25918;&#24335;&#23398;&#20064;(OEL)&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#30452;&#25509;&#20132;&#20114;&#32047;&#31215;&#33719;&#21462;&#26032;&#25216;&#33021;&#21644;&#30693;&#35782;&#65292;&#20363;&#22914;&#20381;&#38752;&#20869;&#22312;&#21160;&#26426;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#30446;&#26631;&#30340;&#25351;&#23548;&#12290;OEL&#26426;&#22120;&#20154;&#23545;&#24212;&#29992;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#20027;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#23436;&#25104;&#23545;&#20154;&#31867;&#29992;&#25143;&#26377;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;OEL&#26426;&#22120;&#20154;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#36825;&#21487;&#33021;&#23548;&#33268;&#33719;&#21462;&#30340;&#30693;&#35782;&#23545;&#23436;&#25104;&#29992;&#25143;&#20219;&#21153;&#24182;&#19981;&#37027;&#20040;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#22260;&#32469;&#8220;&#30446;&#30340;&#8221;&#36825;&#19968;&#26032;&#27010;&#24565;&#23637;&#24320;&#12290;&#30446;&#30340;&#34920;&#31034;&#35774;&#35745;&#32773;&#21644;/&#25110;&#29992;&#25143;&#24076;&#26395;&#26426;&#22120;&#20154;&#20174;&#20013;&#33719;&#24471;&#20160;&#20040;&#12290;&#26426;&#22120;&#20154;&#24212;&#20351;&#29992;&#30446;&#30340;&#30340;&#20869;&#37096;&#34920;&#24449;&#65292;&#36825;&#37324;&#31216;&#20026;&#8220;&#24895;&#26395;&#8221;&#65292;&#26469;&#23558;&#20854;&#24320;&#25918;&#24335;&#25506;&#32034;&#38598;&#20013;&#20110;&#33719;&#21462;&#19982;&#20854;&#23436;&#25104;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#21457;&#23637;&#19968;&#20010;&#20849;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 Announce Type: cross  Abstract: Autonomous open-ended learning (OEL) robots are able to cumulatively acquire new skills and knowledge through direct interaction with the environment, for example relying on the guidance of intrinsic motivations and self-generated goals. OEL robots have a high relevance for applications as they can use the autonomously acquired knowledge to accomplish tasks relevant for their human users. OEL robots, however, encounter an important limitation: this may lead to the acquisition of knowledge that is not so much relevant to accomplish the users' tasks. This work analyses a possible solution to this problem that pivots on the novel concept of `purpose'. Purposes indicate what the designers and/or users want from the robot. The robot should use internal representations of purposes, called here `desires', to focus its open-ended exploration towards the acquisition of knowledge relevant to accomplish them. This work contributes to develop a co
&lt;/p&gt;</description></item><item><title>&#35752;&#35770;&#20102;&#20855;&#26377;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#23618;&#31995;&#32479;&#30340;&#36924;&#36817;&#33021;&#21147;&#21644;&#32479;&#19968;&#24378;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#20018;&#32852;RNN&#20648;&#23618;&#26500;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#31995;&#32479;</title><link>https://arxiv.org/abs/2403.01900</link><description>&lt;p&gt;
&#20855;&#26377;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#23618;&#31995;&#32479;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
Universality of reservoir systems with recurrent neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01900
&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#20855;&#26377;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20648;&#23618;&#31995;&#32479;&#30340;&#36924;&#36817;&#33021;&#21147;&#21644;&#32479;&#19968;&#24378;&#26222;&#36866;&#24615;&#65292;&#21487;&#20197;&#36890;&#36807;&#24182;&#34892;&#20018;&#32852;RNN&#20648;&#23618;&#26500;&#24314;&#36825;&#31181;&#31867;&#22411;&#30340;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35752;&#35770;&#20102;&#20648;&#23618;&#31995;&#32479;&#30340;&#36924;&#36817;&#33021;&#21147;&#65292;&#20854;&#20013;&#20648;&#23618;&#26159;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#12290;&#22312;&#25105;&#20204;&#30340;&#38382;&#39064;&#35774;&#23450;&#20013;&#65292;&#20648;&#23618;&#31995;&#32479;&#36890;&#36807;&#35843;&#25972;&#20854;&#32447;&#24615;&#36755;&#20986;&#26469;&#36924;&#36817;&#19968;&#32452;&#20989;&#25968;&#65292;&#32780;&#20648;&#23618;&#20445;&#25345;&#19981;&#21464;&#12290;&#25105;&#20204;&#23558;&#23637;&#31034;&#25105;&#20204;&#25152;&#31216;&#30340;&#19968;&#31867;&#20989;&#25968;&#30340;RNN&#20648;&#23618;&#31995;&#32479;&#30340;&#32479;&#19968;&#24378;&#26222;&#36866;&#24615;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#23545;&#20110;&#20219;&#24847;&#27491;&#25968;&#65292;&#25105;&#20204;&#21487;&#20197;&#26500;&#24314;&#19968;&#20010;&#36275;&#22815;&#22823;&#30340;RNN&#20648;&#23618;&#31995;&#32479;&#65292;&#20854;&#23545;&#35813;&#31867;&#20989;&#25968;&#20013;&#27599;&#20010;&#20989;&#25968;&#30340;&#36924;&#36817;&#35823;&#24046;&#37117;&#34987;&#27491;&#25968;&#20174;&#19978;&#26041;&#38480;&#23450;&#12290;&#36825;&#26679;&#30340;RNN&#20648;&#23618;&#31995;&#32479;&#26159;&#36890;&#36807;&#24182;&#34892;&#20018;&#32852;RNN&#20648;&#23618;&#26500;&#24314;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01900v1 Announce Type: cross  Abstract: Approximation capability of reservoir systems whose reservoir is a recurrent neural network (RNN) is discussed. In our problem setting, a reservoir system approximates a set of functions just by adjusting its linear readout while the reservoir is fixed. We will show what we call uniform strong universality of a family of RNN reservoir systems for a certain class of functions to be approximated. This means that, for any positive number, we can construct a sufficiently large RNN reservoir system whose approximation error for each function in the class of functions to be approximated is bounded from above by the positive number. Such RNN reservoir systems are constructed via parallel concatenation of RNN reservoirs.
&lt;/p&gt;</description></item><item><title>&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;&#21644;&#27169;&#22411;&#32858;&#21512;&#65292;&#36890;&#36807;&#37327;&#21270;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#34920;&#29616;&#20986;&#23545;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01540</link><description>&lt;p&gt;
&#20998;&#23618;&#37327;&#21270;&#32852;&#37030;&#23398;&#20064;&#65306;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Quantized Hierarchical Federated Learning: A Robust Approach to Statistical Heterogeneity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01540
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#32858;&#21512;&#21644;&#27169;&#22411;&#32858;&#21512;&#65292;&#36890;&#36807;&#37327;&#21270;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#34920;&#29616;&#20986;&#23545;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#22810;&#20010;&#38598;&#21512;&#20013;&#32467;&#21512;&#20102;&#37327;&#21270;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#20110;&#32479;&#35745;&#24322;&#36136;&#24615;&#30340;&#24377;&#24615;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38598;&#21512;&#20869;&#36845;&#20195;&#20013;&#32467;&#21512;&#20102;&#26799;&#24230;&#32858;&#21512;&#21644;&#38598;&#21512;&#38388;&#36845;&#20195;&#20013;&#30340;&#27169;&#22411;&#32858;&#21512;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20998;&#26512;&#26694;&#26550;&#26469;&#35780;&#20272;&#20854;&#26368;&#20248;&#24615;&#24046;&#36317;&#21644;&#25910;&#25947;&#36895;&#24230;&#65292;&#23558;&#36825;&#20123;&#26041;&#38754;&#19982;&#20256;&#32479;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#34920;&#36848;&#65292;&#20197;&#23548;&#20986;&#23553;&#38381;&#24418;&#24335;&#30340;&#26368;&#20248;&#31995;&#32479;&#21442;&#25968;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#19968;&#31995;&#21015;&#21442;&#25968;&#19978;&#22987;&#32456;&#23454;&#29616;&#39640;&#23398;&#20064;&#31934;&#24230;&#65292;&#24182;&#19988;&#22312;&#20855;&#26377;&#24322;&#26500;&#25968;&#25454;&#20998;&#24067;&#30340;&#22330;&#26223;&#20013;&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#20998;&#23618;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01540v1 Announce Type: new  Abstract: This paper presents a novel hierarchical federated learning algorithm within multiple sets that incorporates quantization for communication-efficiency and demonstrates resilience to statistical heterogeneity. Unlike conventional hierarchical federated learning algorithms, our approach combines gradient aggregation in intra-set iterations with model aggregation in inter-set iterations. We offer a comprehensive analytical framework to evaluate its optimality gap and convergence rate, comparing these aspects with those of conventional algorithms. Additionally, we develop a problem formulation to derive optimal system parameters in a closed-form solution. Our findings reveal that our algorithm consistently achieves high learning accuracy over a range of parameters and significantly outperforms other hierarchical algorithms, particularly in scenarios with heterogeneous data distributions.
&lt;/p&gt;</description></item><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14802</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#19979;&#30340;&#38142;&#36335;&#39044;&#27979;: &#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14802
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#20854;&#22312;&#23545;&#22270;&#34920;&#31034;&#30340;&#30495;&#23454;&#19990;&#30028;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#30340;&#28789;&#27963;&#24615;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#22312;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#24322;&#36136;&#22270;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#32463;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#23616;&#38480;&#20110;&#38024;&#23545;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#23450;&#22522;&#20934;&#12290;&#36825;&#31181;&#29421;&#31364;&#30340;&#28966;&#28857;&#38480;&#21046;&#20102;&#38142;&#36335;&#39044;&#27979;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#20004;&#20010;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#26576;&#31181;&#28508;&#22312;&#21407;&#22240;&#32780;&#36830;&#25509;&#65292;&#36825;&#20351;&#24471;&#25552;&#21069;&#39044;&#27979;&#36825;&#31181;&#36830;&#25509;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNNs&#65288;&#22914;GRAFF&#65289;&#23545;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14802v1 Announce Type: new  Abstract: In the past years, Graph Neural Networks (GNNs) have become the `de facto' standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as graphs. However, the message-passing mechanism of GNNs faces challenges in learnability and expressivity, hindering high performance on heterophilic graphs, where adjacent nodes frequently have different labels. Most existing solutions addressing these challenges are primarily confined to specific benchmarks focused on node classification tasks. This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including recommender systems. For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance. Physics-Inspired GNNs such as GRAFF provided a significant contribution to enhance node classification perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#25237;&#24433;&#21040;&#21322;&#27491;&#23450;&#38181;&#65292;&#21462;&#24471;&#20102;&#25910;&#25947;&#20110;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#35299;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09081</link><description>&lt;p&gt;
&#20302;&#31209;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#21487;&#25193;&#23637;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Low-Rank Extragradient Methods for Scalable Semidefinite Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20302;&#31209;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#20351;&#29992;&#20302;&#31209;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#25237;&#24433;&#21040;&#21322;&#27491;&#23450;&#38181;&#65292;&#21462;&#24471;&#20102;&#25910;&#25947;&#20110;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#35299;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31867;&#38750;&#24120;&#37325;&#35201;&#30340;&#21322;&#23450;&#35268;&#21010;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#26082;&#21253;&#25324;&#20984;&#30446;&#26631;&#20989;&#25968;&#65288;&#24179;&#28369;&#25110;&#38750;&#24179;&#28369;&#65289;&#65292;&#21448;&#21253;&#25324;&#39069;&#22806;&#30340;&#32447;&#24615;&#25110;&#38750;&#32447;&#24615;&#24179;&#28369;&#20984;&#32422;&#26463;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#32479;&#35745;&#23398;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#32452;&#21512;&#20248;&#21270;&#31561;&#39046;&#22495;&#37117;&#24456;&#24120;&#35265;&#12290;&#25105;&#20204;&#20851;&#27880;&#39640;&#32500;&#21644;&#21487;&#33021;&#30340;&#24773;&#22659;&#65292;&#20854;&#20013;&#38382;&#39064;&#20855;&#26377;&#20302;&#31209;&#35299;&#65292;&#24182;&#28385;&#36275;&#20302;&#31209;&#20114;&#34917;&#26465;&#20214;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20960;&#20010;&#29702;&#35770;&#32467;&#26524;&#65292;&#35777;&#26126;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#65292;&#24050;&#30693;&#30340;&#22806;&#25512;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#25509;&#36817;&#26368;&#20248;&#21407;&#22987;&#23545;&#20598;&#35299;&#30340;&#24773;&#20917;&#19979;&#21021;&#22987;&#21270;&#26102;&#65292;&#25910;&#25947;&#20110;&#24102;&#26377;&#26631;&#20934;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#30340;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#65292;&#20165;&#20351;&#29992;&#20302;&#31209;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#26469;&#25237;&#24433;&#21040;&#21322;&#27491;&#23450;&#38181;&#65292;&#32780;&#19981;&#26159;&#35745;&#31639;&#19978;&#38480;&#30340;&#20840;&#31209;SVD&#25152;&#38656;&#30340;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09081v1 Announce Type: cross Abstract: We consider several classes of highly important semidefinite optimization problems that involve both a convex objective function (smooth or nonsmooth) and additional linear or nonlinear smooth and convex constraints, which are ubiquitous in statistics, machine learning, combinatorial optimization, and other domains. We focus on high-dimensional and plausible settings in which the problem admits a low-rank solution which also satisfies a low-rank complementarity condition. We provide several theoretical results proving that, under these circumstances, the well-known Extragradient method, when initialized in the proximity of an optimal primal-dual solution, converges to a solution of the constrained optimization problem with its standard convergence rates guarantees, using only low-rank singular value decompositions (SVD) to project onto the positive semidefinite cone, as opposed to computationally-prohibitive full-rank SVDs required in w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#21387;&#32553;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05675</link><description>&lt;p&gt;
&#21387;&#32553;&#25968;&#25454;&#38598;&#30340;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#26377;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Adversarial Training with Compressed Datasets Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#21387;&#32553;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#65288;DC&#65289;&#26159;&#25351;&#20174;&#36739;&#22823;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#19968;&#31867;&#26368;&#36817;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#20445;&#30041;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#20351;&#24471;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;&#22312;&#23436;&#25972;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#30340;DC&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#39044;&#31639;&#19979;&#23454;&#29616;&#39640;&#27979;&#35797;&#24615;&#33021;&#65292;&#24182;&#27809;&#26377;&#30452;&#25509;&#35299;&#20915;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;DC&#26041;&#27861;&#33719;&#24471;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#27809;&#26377;&#26377;&#25928;&#30340;&#20256;&#36882;&#24615;&#12290;&#20026;&#20102;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#21387;&#32553;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23547;&#25214;&#25968;&#25454;&#38598;&#30340;&#26368;&#23567;&#26377;&#38480;&#35206;&#30422;&#65288;MFC&#65289;&#30340;&#26032;&#22411;&#40065;&#26834;&#24615;&#24863;&#30693;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#23450;&#20301;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#31934;&#30830;&#24230;&#65292;&#29978;&#33267;&#22312;&#20005;&#37325;&#38750;&#30452;&#35270;&#35270;&#32447;&#26465;&#20214;&#19979;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.16856</link><description>&lt;p&gt;
&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#31283;&#20581;&#30340;&#22823;&#35268;&#27169;&#32593;&#32476;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Attentional Graph Neural Networks for Robust Massive Network Localization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#32593;&#32476;&#23450;&#20301;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20986;&#33394;&#30340;&#31934;&#30830;&#24230;&#65292;&#29978;&#33267;&#22312;&#20005;&#37325;&#38750;&#30452;&#35270;&#35270;&#32447;&#26465;&#20214;&#19979;&#20063;&#33021;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24050;&#25104;&#20026;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#20102;&#21457;&#25496;GNNs&#22312;&#22238;&#24402;&#20013;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#23558;GNNs&#19982;&#27880;&#24847;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20854;&#36866;&#24212;&#24615;&#21644;&#40065;&#26834;&#24615;&#24443;&#24213;&#25913;&#21464;&#20102;&#24207;&#21015;&#23398;&#20064;&#20219;&#21153;&#30340;&#25216;&#26415;&#65292;&#20197;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#32447;&#24615;&#22238;&#24402;&#38382;&#39064;&#65306;&#32593;&#32476;&#23450;&#20301;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#30340;&#26032;&#22411;&#32593;&#32476;&#23450;&#20301;&#26041;&#27861;&#65292;&#21363;&#20351;&#22312;&#20005;&#37325;&#38750;&#30452;&#35270;&#35270;&#32447;(NLOS)&#26465;&#20214;&#19979;&#20063;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#31934;&#24230;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#32321;&#29712;&#30340;&#31163;&#32447;&#26657;&#20934;&#25110;NLOS&#35782;&#21035;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#27880;&#22270;&#31070;&#32463;&#32593;&#32476;(AGNN)&#27169;&#22411;&#65292;&#26088;&#22312;&#25913;&#21892;&#22522;&#20110;GCN&#26041;&#27861;&#30340;&#26377;&#38480;&#28789;&#27963;&#24615;&#21644;&#23545;&#36229;&#21442;&#25968;&#30340;&#39640;&#25935;&#24863;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16856v2 Announce Type: replace Abstract: In recent years, Graph neural networks (GNNs) have emerged as a prominent tool for classification tasks in machine learning. However, their application in regression tasks remains underexplored. To tap the potential of GNNs in regression, this paper integrates GNNs with attention mechanism, a technique that revolutionized sequential learning tasks with its adaptability and robustness, to tackle a challenging nonlinear regression problem: network localization. We first introduce a novel network localization method based on graph convolutional network (GCN), which exhibits exceptional precision even under severe non-line-of-sight (NLOS) conditions, thereby diminishing the need for laborious offline calibration or NLOS identification. We further propose an attentional graph neural network (AGNN) model, aimed at improving the limited flexibility and mitigating the high sensitivity to the hyperparameter of the GCN-based method. The AGNN co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.08150</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#20999;&#29255;&#36870;&#22238;&#24402;: &#26497;&#23567;&#26497;&#22823;&#24615;&#21644;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Sliced Inverse Regression: Minimax Optimality and Algorithm. (arXiv:2401.08150v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08150
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#24182;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20123;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#39537;&#21160;&#24212;&#29992;&#30340;&#26222;&#21450;&#65292;&#38544;&#31169;&#20445;&#25252;&#24050;&#25104;&#20026;&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#20999;&#29255;&#36870;&#22238;&#24402;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#32479;&#35745;&#25216;&#26415;&#65292;&#36890;&#36807;&#38477;&#20302;&#21327;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#20445;&#25345;&#36275;&#22815;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#20805;&#36275;&#32500;&#24230;&#20943;&#23569;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#30340;&#26368;&#20339;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#19981;&#21516;ially private &#20999;&#29255;&#36870;&#22238;&#24402;&#30340;&#19979;&#30028;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#26497;&#23567;&#26497;&#22823;&#19979;&#30028;&#30340;&#35201;&#27714;&#65292;&#24182;&#22312;&#38477;&#32500;&#31354;&#38388;&#20013;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#21644;&#20445;&#23384;&#37325;&#35201;&#20449;&#24687;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#30340;&#20223;&#30495;&#23454;&#39564;&#21644;&#30495;&#23454;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#24046;&#20998;&#38544;&#31169;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy preservation has become a critical concern in high-dimensional data analysis due to the growing prevalence of data-driven applications. Proposed by Li (1991), sliced inverse regression has emerged as a widely utilized statistical technique for reducing covariate dimensionality while maintaining sufficient statistical information. In this paper, we propose optimally differentially private algorithms specifically designed to address privacy concerns in the context of sufficient dimension reduction. We proceed to establish lower bounds for differentially private sliced inverse regression in both the low and high-dimensional settings. Moreover, we develop differentially private algorithms that achieve the minimax lower bounds up to logarithmic factors. Through a combination of simulations and real data analysis, we illustrate the efficacy of these differentially private algorithms in safeguarding privacy while preserving vital information within the reduced dimension space. As a na
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.11439</link><description>&lt;p&gt;
&#36890;&#36807;&#38750;&#32447;&#24615;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding deep neural networks through the lens of their non-linearity. (arXiv:2310.11439v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20146;&#21644;&#24230;&#35780;&#20998;&#36861;&#36394;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38750;&#32447;&#24615;&#20256;&#25773;&#65292;&#23588;&#20854;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#12290;&#23454;&#39564;&#35777;&#23454;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#21644;&#23545;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#26174;&#33879;&#25104;&#21151;&#24120;&#24120;&#24402;&#22240;&#20110;&#23427;&#20204;&#30340;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#36817;&#20284;&#20219;&#24847;&#22797;&#26434;&#20989;&#25968;&#30340;&#33021;&#21147;&#12290;&#20107;&#23454;&#19978;&#65292;DNN&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#27169;&#22411;&#65292;&#20854;&#20013;&#24341;&#20837;&#30340;&#28608;&#27963;&#20989;&#25968;&#22312;&#20854;&#20013;&#36215;&#21040;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#36890;&#36807;&#36817;&#20284;&#33021;&#21147;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;DNN&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20294;&#37327;&#21270;DNN&#25110;&#20010;&#21035;&#28608;&#27963;&#20989;&#25968;&#30340;&#38750;&#32447;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#20855;&#20307;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#36861;&#36394;&#38750;&#32447;&#24615;&#20256;&#25773;&#30340;&#29702;&#35770;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#20801;&#35768;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;&#21508;&#31181;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#23398;&#20064;&#33539;&#24335;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#31361;&#20986;&#20102;&#25152;&#25552;&#20986;&#30340;&#20146;&#21644;&#24230;&#35780;&#20998;&#30340;&#23454;&#38469;&#25928;&#29992;&#21644;&#28508;&#22312;&#24212;&#29992;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable success of deep neural networks (DNN) is often attributed to their high expressive power and their ability to approximate functions of arbitrary complexity. Indeed, DNNs are highly non-linear models, and activation functions introduced into them are largely responsible for this. While many works studied the expressive power of DNNs through the lens of their approximation capabilities, quantifying the non-linearity of DNNs or of individual activation functions remains an open problem. In this paper, we propose the first theoretically sound solution to track non-linearity propagation in deep neural networks with a specific focus on computer vision applications. Our proposed affinity score allows us to gain insights into the inner workings of a wide range of different architectures and learning paradigms. We provide extensive experimental results that highlight the practical utility of the proposed affinity score and its potential for long-reaching applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Luce&#30340;&#36873;&#25321;&#20844;&#29702;&#20026;&#22522;&#30784;&#30340;&#19968;&#31867;&#36873;&#25321;&#21644;&#25490;&#21517;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#38382;&#39064;&#31561;&#25928;&#30340;&#32463;&#20856;&#30697;&#38453;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;Sinkhorn&#31639;&#27861;&#65292;&#23558;&#36873;&#25321;&#24314;&#27169;&#31639;&#27861;&#32479;&#19968;&#20026;&#30697;&#38453;&#24179;&#34913;&#31639;&#27861;&#30340;&#29305;&#20363;&#12290;&#35770;&#25991;&#36824;&#35299;&#20915;&#20102;Sinkhorn&#31639;&#27861;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#20110;&#38750;&#36127;&#30697;&#38453;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#21644;&#23574;&#38160;&#28176;&#36817;&#36895;&#24230;&#30340;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2310.00260</link><description>&lt;p&gt;
&#20851;&#20110;Sinkhorn&#31639;&#27861;&#21644;&#36873;&#25321;&#24314;&#27169;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
On Sinkhorn's Algorithm and Choice Modeling. (arXiv:2310.00260v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00260
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;Luce&#30340;&#36873;&#25321;&#20844;&#29702;&#20026;&#22522;&#30784;&#30340;&#19968;&#31867;&#36873;&#25321;&#21644;&#25490;&#21517;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#19982;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#38382;&#39064;&#31561;&#25928;&#30340;&#32463;&#20856;&#30697;&#38453;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;Sinkhorn&#31639;&#27861;&#65292;&#23558;&#36873;&#25321;&#24314;&#27169;&#31639;&#27861;&#32479;&#19968;&#20026;&#30697;&#38453;&#24179;&#34913;&#31639;&#27861;&#30340;&#29305;&#20363;&#12290;&#35770;&#25991;&#36824;&#35299;&#20915;&#20102;Sinkhorn&#31639;&#27861;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#23545;&#20110;&#38750;&#36127;&#30697;&#38453;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#21644;&#23574;&#38160;&#28176;&#36817;&#36895;&#24230;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22522;&#20110;Luce&#36873;&#25321;&#20844;&#29702;&#30340;&#24191;&#27867;&#36873;&#25321;&#21644;&#25490;&#21517;&#27169;&#22411;&#65292;&#21253;&#25324;Bradley-Terry-Luce&#21644;Plackett-Luce&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#30456;&#20851;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#38382;&#39064;&#31561;&#20215;&#20110;&#20855;&#26377;&#30446;&#26631;&#34892;&#21644;&#21015;&#21644;&#30340;&#32463;&#20856;&#30697;&#38453;&#24179;&#34913;&#38382;&#39064;&#12290;&#36825;&#20010;&#35266;&#28857;&#25171;&#24320;&#20102;&#20004;&#20010;&#30475;&#20284;&#19981;&#30456;&#20851;&#30340;&#30740;&#31350;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#20043;&#38376;&#65292;&#24182;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36873;&#25321;&#24314;&#27169;&#25991;&#29486;&#20013;&#30340;&#29616;&#26377;&#31639;&#27861;&#32479;&#19968;&#20026;Sinkhorn&#30697;&#38453;&#24179;&#34913;&#31639;&#27861;&#30340;&#29305;&#27530;&#23454;&#20363;&#25110;&#31867;&#20284;&#29289;&#12290;&#25105;&#20204;&#20174;&#36825;&#20123;&#32852;&#31995;&#20013;&#33719;&#24471;&#21551;&#21457;&#65292;&#24182;&#35299;&#20915;&#20102;Sinkhorn&#31639;&#27861;&#30740;&#31350;&#20013;&#30340;&#37325;&#35201;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#24403;&#30697;&#38453;&#24179;&#34913;&#38382;&#39064;&#23384;&#22312;&#26377;&#38480;&#35299;&#26102;&#65292;Sinkhorn&#31639;&#27861;&#23545;&#20110;&#38750;&#36127;&#30697;&#38453;&#30340;&#20840;&#23616;&#32447;&#24615;&#25910;&#25947;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#25454;&#26500;&#24314;&#30340;&#20108;&#20998;&#22270;&#30340;&#20195;&#25968;&#36830;&#36890;&#24615;&#26469;&#25551;&#36848;&#36825;&#31181;&#20840;&#23616;&#25910;&#25947;&#36895;&#24230;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36824;&#24471;&#20986;&#20102;&#32447;&#24615;&#25910;&#25947;&#30340;&#23574;&#38160;&#28176;&#36817;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a broad class of choice and ranking models based on Luce's choice axiom, including the Bradley--Terry--Luce and Plackett--Luce models, we show that the associated maximum likelihood estimation problems are equivalent to a classic matrix balancing problem with target row and column sums. This perspective opens doors between two seemingly unrelated research areas, and allows us to unify existing algorithms in the choice modeling literature as special instances or analogs of Sinkhorn's celebrated algorithm for matrix balancing. We draw inspirations from these connections and resolve important open problems on the study of Sinkhorn's algorithm. We first prove the global linear convergence of Sinkhorn's algorithm for non-negative matrices whenever finite solutions to the matrix balancing problem exist. We characterize this global rate of convergence in terms of the algebraic connectivity of the bipartite graph constructed from data. Next, we also derive the sharp asymptotic rate of line
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2309.02712</link><description>&lt;p&gt;
&#25581;&#31034;&#28145;&#24230;&#23398;&#20064;&#30340;&#21069;&#27839;&#65306;&#22609;&#36896;&#22810;&#20010;&#39046;&#22495;&#30340;&#21019;&#26032;
&lt;/p&gt;
&lt;p&gt;
Unveiling the frontiers of deep learning: innovations shaping diverse domains. (arXiv:2309.02712v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24191;&#27867;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#20248;&#21183;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#20351;&#24471;&#24320;&#21457;&#33021;&#22815;&#23398;&#20064;&#12289;&#21487;&#35270;&#21270;&#12289;&#20248;&#21270;&#12289;&#25913;&#36827;&#21644;&#39044;&#27979;&#25968;&#25454;&#30340;&#35745;&#31639;&#26426;&#27169;&#22411;&#25104;&#20026;&#21487;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;DL&#24050;&#32463;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#22788;&#29702;&#12289;&#20892;&#19994;&#12289;&#20132;&#36890;&#39044;&#27979;&#12289;&#33258;&#28982;&#35821;&#35328;&#12289;&#29983;&#29289;&#21307;&#23398;&#12289;&#28798;&#23475;&#31649;&#29702;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#12289;&#33647;&#29289;&#35774;&#35745;&#12289;&#22522;&#22240;&#32452;&#23398;&#12289;&#20154;&#33080;&#35782;&#21035;&#21644;&#29983;&#24577;&#23398;&#12290;&#20026;&#20102;&#25506;&#32034;&#28145;&#24230;&#23398;&#20064;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#26377;&#24517;&#35201;&#30740;&#31350;&#28145;&#24230;&#23398;&#20064;&#22312;&#36825;&#20123;&#23398;&#31185;&#20013;&#30340;&#26368;&#26032;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;&#28145;&#24230;&#23398;&#20064;&#22312;&#25152;&#26377;&#28508;&#22312;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24191;&#27867;&#35843;&#26597;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#25152;&#26377;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#20197;&#21450;&#30456;&#20851;&#30340;&#20248;&#21183;&#21644;&#25361;&#25112;&#12290;&#27491;&#22914;&#25991;&#29486;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#65292;DL&#22312;&#39044;&#27979;&#21644;&#20998;&#26512;&#26041;&#38754;&#34920;&#29616;&#20986;&#20934;&#30830;&#24615;&#65292;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#35745;&#31639;&#24037;&#20855;&#65292;&#24182;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning (DL) enables the development of computer models that are capable of learning, visualizing, optimizing, refining, and predicting data. In recent years, DL has been applied in a range of fields, including audio-visual data processing, agriculture, transportation prediction, natural language, biomedicine, disaster management, bioinformatics, drug design, genomics, face recognition, and ecology. To explore the current state of deep learning, it is necessary to investigate the latest developments and applications of deep learning in these disciplines. However, the literature is lacking in exploring the applications of deep learning in all potential sectors. This paper thus extensively investigates the potential applications of deep learning across all major fields of study as well as the associated benefits and challenges. As evidenced in the literature, DL exhibits accuracy in prediction and analysis, makes it a powerful computational tool, and has the ability to articulate i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#38431;&#21015;&#20013;&#30340;&#25104;&#26412; (CLQ)&#65292;&#29992;&#20110;&#37327;&#21270;&#30001;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#26102;&#38388;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#26368;&#22823;&#22686;&#21152;&#37327;&#12290;&#35813;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#23398;&#20064;&#38431;&#21015;&#31995;&#32479;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#19981;&#23616;&#38480;&#20110;&#28176;&#36817;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.07817</link><description>&lt;p&gt;
&#37327;&#21270;&#38431;&#21015;&#31995;&#32479;&#20013;&#30340;&#23398;&#20064;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Cost of Learning in Queueing Systems. (arXiv:2308.07817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21363;&#23398;&#20064;&#38431;&#21015;&#20013;&#30340;&#25104;&#26412; (CLQ)&#65292;&#29992;&#20110;&#37327;&#21270;&#30001;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#26102;&#38388;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#26368;&#22823;&#22686;&#21152;&#37327;&#12290;&#35813;&#24230;&#37327;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#23398;&#20064;&#38431;&#21015;&#31995;&#32479;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#19981;&#23616;&#38480;&#20110;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38431;&#21015;&#31995;&#32479;&#26159;&#24191;&#27867;&#24212;&#29992;&#30340;&#38543;&#26426;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#36890;&#20449;&#32593;&#32476;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#26381;&#21153;&#31995;&#32479;&#31561;&#31561;&#12290;&#34429;&#28982;&#23427;&#20204;&#30340;&#26368;&#20248;&#25511;&#21046;&#24050;&#32463;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#20551;&#35774;&#31995;&#32479;&#21442;&#25968;&#30340;&#23436;&#32654;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#24456;&#24120;&#35265;&#65292;&#22240;&#27492;&#26368;&#36817;&#19968;&#31995;&#21015;&#20851;&#20110;&#38431;&#21015;&#31995;&#32479;&#30340;&#23398;&#20064;&#30340;&#30740;&#31350;&#20135;&#29983;&#20102;&#12290;&#36825;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#20027;&#35201;&#20851;&#27880;&#25152;&#25552;&#31639;&#27861;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#28176;&#36817;&#24230;&#37327;&#65292;&#21363;&#30528;&#30524;&#20110;&#21518;&#26399;&#24615;&#33021;&#30340;&#24230;&#37327;&#65292;&#26080;&#27861;&#25429;&#25417;&#23398;&#20064;&#38431;&#21015;&#31995;&#32479;&#20013;&#22266;&#26377;&#30340;&#32479;&#35745;&#22797;&#26434;&#24615;&#65292;&#36825;&#31181;&#22797;&#26434;&#24615;&#36890;&#24120;&#20986;&#29616;&#22312;&#26089;&#26399;&#38454;&#27573;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#38431;&#21015;&#20013;&#30340;&#25104;&#26412; (CLQ)&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#34913;&#37327;&#30001;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#23548;&#33268;&#30340;&#26102;&#38388;&#24179;&#22343;&#38431;&#21015;&#38271;&#24230;&#30340;&#26368;&#22823;&#22686;&#21152;&#37327;&#12290;&#25105;&#20204;&#23545;&#21333;&#38431;&#21015;&#22810;&#26381;&#21153;&#22120;&#31995;&#32479;&#30340;CLQ&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Queueing systems are widely applicable stochastic models with use cases in communication networks, healthcare, service systems, etc. Although their optimal control has been extensively studied, most existing approaches assume perfect knowledge of system parameters. Of course, this assumption rarely holds in practice where there is parameter uncertainty, thus motivating a recent line of work on bandit learning for queueing systems. This nascent stream of research focuses on the asymptotic performance of the proposed algorithms.  In this paper, we argue that an asymptotic metric, which focuses on late-stage performance, is insufficient to capture the intrinsic statistical complexity of learning in queueing systems which typically occurs in the early stage. Instead, we propose the Cost of Learning in Queueing (CLQ), a new metric that quantifies the maximum increase in time-averaged queue length caused by parameter uncertainty. We characterize the CLQ of a single-queue multi-server system,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.15539</link><description>&lt;p&gt;
&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Backdoor Defense with Non-Adversarial Backdoor. (arXiv:2307.15539v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#34987;&#27745;&#26579;&#26679;&#26412;&#20013;&#27880;&#20837;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#65292;&#24403;&#35302;&#21457;&#26102;&#21487;&#20197;&#25233;&#21046;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#21516;&#26102;&#20445;&#25345;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#24182;&#19981;&#20250;&#24433;&#21709;&#32593;&#32476;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24615;&#33021;&#65292;&#20294;&#19968;&#26086;&#28155;&#21152;&#35302;&#21457;&#27169;&#24335;&#65292;&#23601;&#20250;&#25805;&#32437;&#32593;&#32476;&#34892;&#20026;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#22823;&#22823;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#20294;&#23427;&#20204;&#22312;&#24178;&#20928;&#25968;&#25454;&#19978;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#20173;&#28982;&#36828;&#36828;&#33853;&#21518;&#20110;&#24178;&#20928;&#27169;&#22411;&#12290;&#21463;&#21518;&#38376;&#25915;&#20987;&#30340;&#38544;&#34109;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#38450;&#24481;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#27880;&#20837;&#20102;&#38024;&#23545;&#34987;&#27745;&#26579;&#26679;&#26412;&#30340;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#12290;&#25353;&#29031;&#21518;&#38376;&#25915;&#20987;&#30340;&#19968;&#33324;&#27493;&#39588;&#65292;&#25105;&#20204;&#26816;&#27979;&#19968;&#23567;&#32452;&#21487;&#30097;&#26679;&#26412;&#65292;&#28982;&#21518;&#23545;&#23427;&#20204;&#24212;&#29992;&#27602;&#21270;&#31574;&#30053;&#12290;&#19968;&#26086;&#35302;&#21457;&#65292;&#38750;&#23545;&#25239;&#24615;&#21518;&#38376;&#25233;&#21046;&#20102;&#25915;&#20987;&#32773;&#23545;&#27745;&#26579;&#25968;&#25454;&#30340;&#21518;&#38376;&#25915;&#20987;&#65292;&#20294;&#23545;&#24178;&#20928;&#25968;&#25454;&#30340;&#24433;&#21709;&#26377;&#38480;&#12290;&#38450;&#24481;&#21487;&#20197;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#26399;&#38388;&#36827;&#34892;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26631;&#20934;&#30340;&#31471;&#21040;&#31471;&#35757;&#32451;&#27969;&#31243;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not affect the network's performance on clean data but would manipulate the network behavior once a trigger pattern is added. Existing defense methods have greatly reduced attack success rate, but their prediction accuracy on clean data still lags behind a clean model by a large margin. Inspired by the stealthiness and effectiveness of backdoor attack, we propose a simple but highly effective defense framework which injects non-adversarial backdoors targeting poisoned samples. Following the general steps in backdoor attack, we detect a small set of suspected samples and then apply a poisoning strategy to them. The non-adversarial backdoor, once triggered, suppresses the attacker's backdoor on poisoned data, but has limited influence on clean data. The defense can be carried out during data preprocessing, without any modification to the standard end-to-end training pipeline. We conduct extensive experiments on mul
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14474</link><description>&lt;p&gt;
&#27827;&#24029;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limits to Reservoir Learning. (arXiv:2307.14474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#26469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#65288;IPC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#20449;&#21495;&#38598;&#21512;&#21040;&#23436;&#25972;&#20989;&#25968;&#22522;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;IPC&#26469;&#34913;&#37327;&#22122;&#22768;&#19979;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#65288;&#19968;&#31181;&#29305;&#27530;&#30340;&#24490;&#29615;&#32593;&#32476;&#65289;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;IPC&#22312;&#31995;&#32479;&#23610;&#23544;n&#19978;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;n&#20010;&#36755;&#20986;&#20449;&#21495;&#30340;$2^n$&#20010;&#21487;&#33021;&#30340;&#36880;&#28857;&#20056;&#31215;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36864;&#21270;&#24847;&#21619;&#30528;&#22312;&#20648;&#27700;&#24211;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20648;&#27700;&#24211;&#25152;&#34920;&#31034;&#30340;&#20989;&#25968;&#26063;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#19968;&#38598;&#21512;&#30340;$2^n$&#20010;&#20989;&#25968;&#22312;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#26696;&#65292;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.06915</link><description>&lt;p&gt;
&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;: &#28176;&#36817;&#27491;&#24577;&#24615;&#21644;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Weighted Averaged Stochastic Gradient Descent: Asymptotic Normality and Optimality. (arXiv:2307.06915v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#21152;&#26435;&#24179;&#22343;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26041;&#26696;&#65292;&#24182;&#24314;&#31435;&#20102;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#20855;&#26377;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26159;&#29616;&#20195;&#32479;&#35745;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#31616;&#21333;&#21644;&#26368;&#27969;&#34892;&#30340;&#31639;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#32780;&#21463;&#21040;&#38738;&#30544;&#12290;&#22312;&#19981;&#21516;&#30340;&#24773;&#22659;&#19979;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#24179;&#22343;&#26041;&#26696;&#26469;&#21152;&#36895;SGD&#30340;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#29992;&#20110;SGD&#30340;&#36890;&#29992;&#24179;&#22343;&#26041;&#26696;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31867;&#21152;&#26435;&#24179;&#22343;SGD&#35299;&#30340;&#28176;&#36817;&#27491;&#24577;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#28176;&#36817;&#26377;&#25928;&#30340;&#22312;&#32447;&#25512;&#29702;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#24179;&#22343;&#26041;&#26696;&#65292;&#23637;&#29616;&#20986;&#26368;&#20248;&#30340;&#32479;&#35745;&#36895;&#24230;&#21644;&#26377;&#21033;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#65292;&#20511;&#37492;&#20102;&#32447;&#24615;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#30340;&#26368;&#20248;&#26435;&#37325;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent (SGD) is one of the simplest and most popular algorithms in modern statistical and machine learning due to its computational and memory efficiency. Various averaging schemes have been proposed to accelerate the convergence of SGD in different settings. In this paper, we explore a general averaging scheme for SGD. Specifically, we establish the asymptotic normality of a broad range of weighted averaged SGD solutions and provide asymptotically valid online inference approaches. Furthermore, we propose an adaptive averaging scheme that exhibits both optimal statistical rate and favorable non-asymptotic convergence, drawing insights from the optimal weight for the linear model in terms of non-asymptotic mean squared error (MSE).
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#38754;&#21521;&#24322;&#26500;HPC&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#21253;&#25324;GPU&#12289;TPU&#12289;FPGA&#12289;ASIC&#12289;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#21644;RISC-V&#31561;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#26032;&#20852;&#20869;&#23384;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2306.15552</link><description>&lt;p&gt;
&#38754;&#21521;&#24322;&#26500;HPC&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning Hardware Accelerators for Heterogeneous HPC Platforms. (arXiv:2306.15552v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#38754;&#21521;&#24322;&#26500;HPC&#24179;&#21488;&#30340;&#28145;&#24230;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#21253;&#25324;GPU&#12289;TPU&#12289;FPGA&#12289;ASIC&#12289;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#21644;RISC-V&#31561;&#65292;&#21516;&#26102;&#20063;&#28085;&#30422;&#20102;&#26032;&#20852;&#20869;&#23384;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#24212;&#29992;&#20013;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#35821;&#38899;&#35782;&#21035;&#20013;&#25104;&#20026;&#30828;&#20214;&#21152;&#36895;&#22120;&#26368;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#32508;&#36848;&#24635;&#32467;&#21644;&#20998;&#31867;&#20102;&#35774;&#35745;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20197;&#28385;&#36275;HPC&#24212;&#29992;&#30340;&#24615;&#33021;&#35201;&#27714;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#24378;&#35843;&#20102;&#25903;&#25345;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#21253;&#25324;&#19981;&#20165;&#38480;&#20110;&#22522;&#20110;GPU&#21644;TPU&#30340;&#21152;&#36895;&#22120;&#65292;&#36824;&#21253;&#25324;&#22522;&#20110;FPGA&#21644;ASIC&#30340;&#29305;&#23450;&#35774;&#35745;&#30340;&#30828;&#20214;&#21152;&#36895;&#22120;&#12289;&#31070;&#32463;&#22788;&#29702;&#21333;&#20803;&#12289;&#22522;&#20110;&#24320;&#25918;&#30828;&#20214;RISC-V&#30340;&#21152;&#36895;&#22120;&#21644;&#21327;&#22788;&#29702;&#22120;&#12290;&#26412;&#32508;&#36848;&#36824;&#25551;&#36848;&#20102;&#22522;&#20110;&#26032;&#20852;&#20869;&#23384;&#25216;&#26415;&#21644;&#35745;&#31639;&#33539;&#24335;&#30340;&#21152;&#36895;&#22120;&#65292;&#20363;&#22914;3D&#22534;&#21472;&#22788;&#29702;&#22120;&#20869;&#23384;&#12289;&#38750;&#26131;&#22833;&#24615;&#23384;&#20648;&#22120;&#65288;&#20027;&#35201;&#26159;&#30005;&#38459;&#24335;&#38543;&#26426;&#23384;&#21462;&#23384;&#20648;&#22120;&#21644;&#30456;&#21464;&#23384;&#20648;&#22120;&#65289;&#23454;&#29616;&#20869;&#23384;&#35745;&#31639;&#65292;&#31070;&#32463;&#24418;&#24577;&#23398;&#22788;&#29702;&#21333;&#20803;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent trends in deep learning (DL) imposed hardware accelerators as the most viable solution for several classes of high-performance computing (HPC) applications such as image classification, computer vision, and speech recognition. This survey summarizes and classifies the most recent advances in designing DL accelerators suitable to reach the performance requirements of HPC applications. In particular, it highlights the most advanced approaches to support deep learning accelerations including not only GPU and TPU-based accelerators but also design-specific hardware accelerators such as FPGA-based and ASIC-based accelerators, Neural Processing Units, open hardware RISC-V-based accelerators and co-processors. The survey also describes accelerators based on emerging memory technologies and computing paradigms, such as 3D-stacked Processor-In-Memory, non-volatile memories (mainly, Resistive RAM and Phase Change Memories) to implement in-memory computing, Neuromorphic Processing Units, a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16900</link><description>&lt;p&gt;
InceptionNeXt&#65306;&#24403;Inception&#36935;&#21040;ConvNeXt
&lt;/p&gt;
&lt;p&gt;
InceptionNeXt: When Inception Meets ConvNeXt. (arXiv:2303.16900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;ViTs&#38271;&#31243;&#24314;&#27169;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24191;&#27867;&#30740;&#31350;&#21644;&#37319;&#29992;&#20102;&#22823;&#20869;&#26680;&#21367;&#31215;&#26469;&#25193;&#22823;&#24863;&#21463;&#37326;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;ConvNeXt&#37319;&#29992;&#20102;7x7&#28145;&#24230;&#21367;&#31215;&#12290;&#34429;&#28982;&#36825;&#31181;&#28145;&#24230;&#25805;&#20316;&#20165;&#28040;&#32791;&#23569;&#37327;FLOPs&#65292;&#20294;&#30001;&#20110;&#39640;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#65292;&#36825;&#22312;&#21151;&#33021;&#24378;&#22823;&#30340;&#35745;&#31639;&#35774;&#22791;&#19978;&#22823;&#22823;&#25439;&#23475;&#20102;&#27169;&#22411;&#25928;&#29575;&#12290;&#23613;&#31649;&#32553;&#23567;ConvNeXt&#30340;&#20869;&#26680;&#22823;&#23567;&#33021;&#25552;&#39640;&#36895;&#24230;&#65292;&#20294;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#30528;&#19979;&#38477;&#12290;&#22914;&#20309;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;Inceptions&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22823;&#20869;&#26680;&#28145;&#24230;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#65292;&#21363;&#23567;&#26041;&#20869;&#26680;&#12289;&#20004;&#20010;&#27491;&#20132;&#24102;&#20869;&#26680;&#21644;&#19968;&#20010;&#20114;&#34917;&#20869;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an ide
&lt;/p&gt;</description></item><item><title>&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#26469;&#20419;&#36827;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#30693;&#35782;&#31934;&#39311;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2206.04661</link><description>&lt;p&gt;
&#31934;&#39311;&#20915;&#31574;&#26641;
&lt;/p&gt;
&lt;p&gt;
Distillation Decision Tree. (arXiv:2206.04661v2 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.04661
&lt;/p&gt;
&lt;p&gt;
&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#26159;&#19968;&#31181;&#36890;&#36807;&#23558;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#26469;&#20419;&#36827;&#35299;&#37322;&#24615;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#24314;&#31435;&#22312;&#30693;&#35782;&#31934;&#39311;&#30340;&#29702;&#35770;&#22522;&#30784;&#19978;&#65292;&#24182;&#19988;&#22312;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#26465;&#20214;&#19979;&#21487;&#20197;&#26377;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#22240;&#20854;&#20986;&#33394;&#30340;&#39044;&#27979;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#38738;&#30544;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#23427;&#20204;&#32463;&#24120;&#38754;&#20020;&#25209;&#35780;&#21644;&#25361;&#25112;&#12290;&#30683;&#30462;&#30340;&#26159;&#65292;&#23427;&#20204;&#24378;&#22823;&#30340;&#39044;&#27979;&#33021;&#21147;&#34920;&#26126;&#23545;&#24213;&#23618;&#25968;&#25454;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#20174;&#32780;&#24847;&#21619;&#30528;&#37325;&#35201;&#30340;&#35299;&#37322;&#28508;&#21147;&#12290;&#20511;&#21161;&#30693;&#35782;&#31934;&#39311;&#30340;&#26032;&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31934;&#39311;&#20915;&#31574;&#26641;&#65288;DDT&#65289;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#23558;&#20851;&#20110;&#25968;&#25454;&#30340;&#30693;&#35782;&#20174;&#40657;&#30418;&#27169;&#22411;&#31934;&#39311;&#21040;&#20915;&#31574;&#26641;&#20013;&#65292;&#20174;&#32780;&#20419;&#36827;&#20102;&#23545;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#30693;&#35782;&#31934;&#39311;&#36807;&#31243;&#26500;&#24314;&#30340;DDT&#30340;&#21487;&#35299;&#37322;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#20854;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#20026;DDT&#30340;&#32467;&#26500;&#31283;&#23450;&#24615;&#24314;&#31435;&#20102;&#29702;&#35770;&#22522;&#30784;&#65292;&#35777;&#26126;&#20854;&#22312;&#19968;&#20123;&#20551;&#35774;&#19979;&#21487;&#20197;&#23454;&#29616;&#32467;&#26500;&#31283;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#31639;&#27861;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly the black-box models, are widely favored for their outstanding predictive capabilities. However, they often face scrutiny and criticism due to the lack of interpretability. Paradoxically, their strong predictive capabilities suggest a deep understanding about the underlying data, implying significant potential for interpretation. Leveraging the emerging concept of knowledge distillation, we introduced the method of distillation decision tree (DDT). This method enables the distillation of knowledge about the data from a black-box model into a decision tree, thereby facilitating the interpretation of the black-box model. Constructed through the knowledge distillation process, the interpretability of DDT relies significantly on the stability of its structure. We establish the theoretical foundations for the structural stability of DDT, demonstrating that its structure can achieve stability under mild assumptions. Furthermore, we develop algorithms for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#24182;&#21462;&#24471;&#20102; ROAM &#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#21453;&#24212;&#29575;&#35758;&#35770;&#12290;</title><link>http://arxiv.org/abs/2108.11328</link><description>&lt;p&gt;
&#29992;&#31616;&#27905;&#21487;&#35299;&#37322;&#30340;&#21152;&#24615;&#27169;&#22411;&#21644;&#32467;&#26500;&#20132;&#20114;&#39044;&#27979;&#20154;&#21475;&#26222;&#26597;&#35843;&#26597;&#21453;&#24212;&#29575;
&lt;/p&gt;
&lt;p&gt;
Predicting Census Survey Response Rates With Parsimonious Additive Models and Structured Interactions. (arXiv:2108.11328v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.11328
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#23569;&#37327;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#24182;&#21462;&#24471;&#20102; ROAM &#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#21487;&#20197;&#25552;&#20379;&#25913;&#36827;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#21453;&#24212;&#29575;&#35758;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20351;&#29992;&#19968;&#31995;&#21015;&#28789;&#27963;&#19988;&#21487;&#35299;&#37322;&#30340;&#38750;&#21442;&#25968;&#27169;&#22411;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#12290;&#26412;&#30740;&#31350;&#21463;&#21040;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#33879;&#21517;&#30340; ROAM &#24212;&#29992;&#30340;&#21551;&#21457;&#65292;&#35813;&#24212;&#29992;&#20351;&#29992;&#22312;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#35268;&#21010;&#25968;&#25454;&#24211;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#26469;&#35782;&#21035;&#38590;&#20197;&#35843;&#26597;&#30340;&#21306;&#22495;&#12290;&#21313;&#24180;&#21069;&#32452;&#32455;&#30340;&#19968;&#22330;&#20247;&#21253;&#31454;&#36187;&#34920;&#26126;&#65292;&#22522;&#20110;&#22238;&#24402;&#26641;&#38598;&#25104;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#35843;&#26597;&#21453;&#24212;&#29575;&#26041;&#38754;&#34920;&#29616;&#26368;&#20339;&#65307;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#65292;&#30456;&#24212;&#30340;&#27169;&#22411;&#19981;&#33021;&#29992;&#20110;&#25311;&#23450;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992; $\ell_0$-based &#24809;&#32602;&#30340;&#38750;&#21442;&#25968;&#21152;&#24615;&#27169;&#22411;&#65292;&#23427;&#20855;&#26377;&#23569;&#25968;&#20027;&#35201;&#21644;&#25104;&#23545;&#20132;&#20114;&#25928;&#24212;&#12290;&#20174;&#26041;&#27861;&#35770;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#20272;&#35745;&#22120;&#30340;&#35745;&#31639;&#21644;&#32479;&#35745;&#26041;&#38754;&#65292;&#24182;&#35752;&#35770;&#20102;&#23558;&#24378;&#23618;&#27425;&#20132;&#20114;&#21512;&#24182;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#65288;&#22312;Github &#19978;&#24320;&#28304;&#65289;&#20801;&#35768;&#25105;&#20204;&#29983;&#25104;&#26131;&#20110;&#21487;&#35270;&#21270;&#21644;&#35299;&#37322;&#30340;&#39044;&#27979;&#38754;&#65292;&#20174;&#32780;&#33719;&#24471;&#26377;&#20851;&#35843;&#26597;&#21453;&#24212;&#29575;&#30340;&#21487;&#34892;&#35265;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312; ROAM &#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#20197;&#25552;&#20379;&#26377;&#20851;&#32654;&#22269;&#20154;&#21475;&#26222;&#26597;&#23616;&#21644;&#20854;&#20182;&#35843;&#26597;&#30340;&#25913;&#36827;&#35843;&#26597;&#21453;&#24212;&#29575;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the problem of predicting survey response rates using a family of flexible and interpretable nonparametric models. The study is motivated by the US Census Bureau's well-known ROAM application which uses a linear regression model trained on the US Census Planning Database data to identify hard-to-survey areas. A crowdsourcing competition organized around ten years ago revealed that machine learning methods based on ensembles of regression trees led to the best performance in predicting survey response rates; however, the corresponding models could not be adopted for the intended application due to their black-box nature. We consider nonparametric additive models with small number of main and pairwise interaction effects using $\ell_0$-based penalization. From a methodological viewpoint, we study both computational and statistical aspects of our estimator; and discuss variants that incorporate strong hierarchical interactions. Our algorithms (opensourced on gith
&lt;/p&gt;</description></item></channel></rss>