<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#31561;&#24314;&#27169;&#32452;&#20214;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#12289;&#26102;&#38388;&#22788;&#29702;&#21644;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.16674</link><description>&lt;p&gt;
&#29702;&#35299;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#24314;&#27169;&#32452;&#20214;&#30340;&#21151;&#33021;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Understanding the Functional Roles of Modelling Components in Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16674
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#31561;&#24314;&#27169;&#32452;&#20214;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#12289;&#26102;&#38388;&#22788;&#29702;&#21644;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#33041;&#31070;&#32463;&#22238;&#36335;&#21551;&#21457;&#65292;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#23454;&#29616;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#29983;&#29289;&#20445;&#30495;&#24230;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;SNNs&#30456;&#24403;&#22256;&#38590;&#65292;&#22240;&#20026;&#20854;&#24314;&#27169;&#32452;&#20214;&#30340;&#21151;&#33021;&#35282;&#33394;&#20173;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35780;&#20272;&#32463;&#20856;&#27169;&#22411;&#30340;&#20960;&#20010;&#21464;&#20307;&#65292;&#25105;&#20204;&#31995;&#32479;&#30740;&#31350;&#20102;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#36825;&#20123;&#20851;&#38190;&#24314;&#27169;&#32452;&#20214;&#22312;&#22522;&#20110;&#28431;&#31215;&#20998;&#25918;&#30005;&#65288;LIF&#65289;&#30340;SNNs&#20013;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#20123;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;SNNs&#30340;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#28388;&#27844;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#37325;&#32622;&#26426;&#21046;&#23545;&#20110;&#19981;&#38388;&#26029;&#30340;&#26102;&#38388;&#22788;&#29702;&#21644;&#35745;&#31639;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24490;&#29615;&#21017;&#20016;&#23500;&#20102;&#27169;&#22411;&#22797;&#26434;&#21160;&#24577;&#30340;&#33021;&#21147;&#65292;&#20294;&#20250;&#25439;&#23475;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16674v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity. Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear. By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs. Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degr
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#33014;&#22218;&#23618;&#30340;&#26550;&#26500;&#20462;&#25913;&#65292;&#37197;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23558;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#29305;&#24449;&#25972;&#21512;&#36827;&#20998;&#26512;&#65292;&#20026;&#39640;&#32423;&#23545;&#35937;&#26631;&#35760;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.11826</link><description>&lt;p&gt;
CapsLorentzNet: &#23558;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#29305;&#24449;&#19982;&#22270;&#21367;&#31215;&#30456;&#25972;&#21512;
&lt;/p&gt;
&lt;p&gt;
CapsLorentzNet: Integrating Physics Inspired Features with Graph Convolution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11826
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#33014;&#22218;&#23618;&#30340;&#26550;&#26500;&#20462;&#25913;&#65292;&#37197;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#23558;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#29305;&#24449;&#25972;&#21512;&#36827;&#20998;&#26512;&#65292;&#20026;&#39640;&#32423;&#23545;&#35937;&#26631;&#35760;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#20986;&#29616;&#65292;&#25552;&#21319;&#23545;&#35937;&#26631;&#35760;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19982;&#21508;&#31181;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#20860;&#23481;&#30340;&#26032;&#39062;&#26550;&#26500;&#20462;&#25913;&#65292;&#36827;&#19968;&#27493;&#25512;&#21160;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20027;&#24352;&#22312;&#26631;&#20934;GNN&#20013;&#26367;&#25442;&#20256;&#32479;&#35299;&#30721;&#22359;&#20197;&#38598;&#25104;&#33014;&#22218;&#23618;&#12290;&#36825;&#20123;&#33014;&#22218;&#26159;&#20855;&#26377;&#21521;&#37327;&#28608;&#27963;&#30340;&#31070;&#32463;&#20803;&#32452;&#12290;&#36825;&#20123;&#21521;&#37327;&#30340;&#26041;&#21521;&#34920;&#31034;&#34987;&#30740;&#31350;&#23545;&#35937;&#30340;&#37325;&#35201;&#23646;&#24615;&#65292;&#20854;&#22823;&#23567;&#34920;&#24449;&#34987;&#30740;&#31350;&#23545;&#35937;&#26159;&#21542;&#23646;&#20110;&#30001;&#33014;&#22218;&#20195;&#34920;&#30340;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#33014;&#22218;&#32593;&#32476;&#32467;&#21512;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#26500;&#26426;&#21046;&#36827;&#34892;&#27491;&#21017;&#21270;&#65292;&#20419;&#36827;&#20102;&#19987;&#23478;&#35774;&#35745;&#30340;&#39640;&#32423;&#29305;&#24449;&#26080;&#32541;&#34701;&#20837;&#20998;&#26512;&#12290;&#25105;&#20204;&#24050;&#32463;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#19982;LorentzNet&#26550;&#26500;&#22312;&#22840;&#20811;&#33014;&#23376;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11826v1 Announce Type: cross  Abstract: With the advent of advanced machine learning techniques, boosted object tagging has witnessed significant progress. In this article, we take this field further by introducing novel architectural modifications compatible with a wide array of Graph Neural Network (GNN) architectures. Our approach advocates for integrating capsule layers, replacing the conventional decoding blocks in standard GNNs. These capsules are a group of neurons with vector activations. The orientation of these vectors represents important properties of the objects under study, with their magnitude characterizing whether the object under study belongs to the class represented by the capsule. Moreover, capsule networks incorporate a regularization by reconstruction mechanism, facilitating the seamless integration of expert-designed high-level features into the analysis. We have studied the usefulness of our architecture with the LorentzNet architecture for quark-glu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;</title><link>https://arxiv.org/abs/2403.10650</link><description>&lt;p&gt;
PALM&#65306;&#25512;&#36827;&#29992;&#20110;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#30340;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#26041;&#27861;&#20013;&#30001;&#20110;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#22256;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#35270;&#35273;&#27169;&#22411;&#38754;&#20020;&#39046;&#22495;&#20998;&#24067;&#30340;&#24555;&#36895;&#36716;&#21464;&#65292;&#23548;&#33268;&#35782;&#21035;&#24615;&#33021;&#19979;&#38477;&#12290;&#25345;&#32493;&#27979;&#35797;&#26102;&#38388;&#33258;&#36866;&#24212;&#65288;CTTA&#65289;&#30452;&#25509;&#26681;&#25454;&#27979;&#35797;&#25968;&#25454;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;&#28304;&#21028;&#21035;&#27169;&#22411;&#20197;&#36866;&#24212;&#36825;&#20123;&#19981;&#26029;&#21464;&#21270;&#30340;&#39046;&#22495;&#12290;&#19968;&#31181;&#39640;&#24230;&#26377;&#25928;&#30340;CTTA&#26041;&#27861;&#28041;&#21450;&#24212;&#29992;&#36880;&#23618;&#33258;&#36866;&#24212;&#23398;&#20064;&#29575;&#65292;&#24182;&#36873;&#25321;&#24615;&#22320;&#35843;&#25972;&#39044;&#35757;&#32451;&#23618;&#12290;&#28982;&#32780;&#65292;&#23427;&#21463;&#21040;&#39046;&#22495;&#36716;&#31227;&#20272;&#35745;&#19981;&#20934;&#30830;&#21644;&#30001;&#20266;&#26631;&#31614;&#24341;&#36215;&#30340;&#19981;&#20934;&#30830;&#24615;&#25152;&#22256;&#25200;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#35782;&#21035;&#23618;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36890;&#36807;&#23545;&#27169;&#22411;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#37327;&#21270;&#26469;&#36873;&#25321;&#23618;&#65292;&#32780;&#26080;&#39035;&#20381;&#36182;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#21033;&#29992;&#26799;&#24230;&#30340;&#22823;&#23567;&#20316;&#20026;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;softmax&#36755;&#20986;&#19982;&#22343;&#21248;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#26469;&#35745;&#31639;&#65292;&#20197;&#36873;&#25321;&#38656;&#35201;&#36827;&#19968;&#27493;&#36866;&#24212;&#30340;&#23618;&#12290;&#38543;&#21518;&#65292;&#20165;&#23646;&#20110;&#36825;&#20123;&#23618;&#30340;&#21442;&#25968;&#23558;&#34987;&#36827;&#19968;&#27493;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10650v1 Announce Type: cross  Abstract: Real-world vision models in dynamic environments face rapid shifts in domain distributions, leading to decreased recognition performance. Continual test-time adaptation (CTTA) directly adjusts a pre-trained source discriminative model to these changing domains using test data. A highly effective CTTA method involves applying layer-wise adaptive learning rates, and selectively adapting pre-trained layers. However, it suffers from the poor estimation of domain shift and the inaccuracies arising from the pseudo-labels. In this work, we aim to overcome these limitations by identifying layers through the quantification of model prediction uncertainty without relying on pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by backpropagating the KL divergence between the softmax output and a uniform distribution, to select layers for further adaptation. Subsequently, for the parameters exclusively belonging to these se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#21517;&#20026;&#35748;&#30693;&#23433;&#20840;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#20998;&#26512;&#31070;&#32463;&#25216;&#26415;&#23545;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#30456;&#20851;&#38382;&#39064;&#25551;&#36848;&#21644;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.07945</link><description>&lt;p&gt;
&#19968;&#20010;&#35299;&#20915;&#31070;&#32463;&#25216;&#26415;&#35748;&#30693;&#23433;&#20840;&#38382;&#39064;&#30340;&#25968;&#23398;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Mathematical Framework for the Problem of Security for Cognition in Neurotechnology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#21517;&#20026;&#35748;&#30693;&#23433;&#20840;&#65292;&#29992;&#20110;&#25551;&#36848;&#21644;&#20998;&#26512;&#31070;&#32463;&#25216;&#26415;&#23545;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#35299;&#20915;&#20102;&#30456;&#20851;&#38382;&#39064;&#25551;&#36848;&#21644;&#20998;&#26512;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#31070;&#32463;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#31070;&#32463;&#25216;&#26415;&#21644;&#23433;&#20840;&#20043;&#38388;&#21019;&#36896;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20851;&#38190;&#20132;&#21449;&#28857;&#12290;&#26893;&#20837;&#24335;&#35774;&#22791;&#12289;&#38750;&#20405;&#20837;&#24335;&#30417;&#27979;&#21644;&#38750;&#20405;&#20837;&#24335;&#27835;&#30103;&#37117;&#24102;&#26469;&#20102;&#36829;&#21453;&#20010;&#20307;&#35748;&#30693;&#38544;&#31169;&#21644;&#33258;&#27835;&#30340;&#21069;&#26223;&#12290;&#36234;&#26469;&#36234;&#22810;&#30340;&#31185;&#23398;&#23478;&#21644;&#21307;&#29983;&#21628;&#21505;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064; -- &#25105;&#20204;&#31216;&#20043;&#20026;&#35748;&#30693;&#23433;&#20840; -- &#20294;&#24212;&#29992;&#24037;&#20316;&#21463;&#21040;&#38480;&#21046;&#12290;&#38459;&#30861;&#31185;&#23398;&#21644;&#24037;&#31243;&#21162;&#21147;&#35299;&#20915;&#35748;&#30693;&#23433;&#20840;&#38382;&#39064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#28165;&#26224;&#25551;&#36848;&#21644;&#20998;&#26512;&#30456;&#20851;&#38382;&#39064;&#30340;&#25163;&#27573;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#35748;&#30693;&#23433;&#20840;&#65292;&#36825;&#26159;&#19968;&#20010;&#25968;&#23398;&#26694;&#26550;&#65292;&#36890;&#36807;&#20511;&#37492;&#22810;&#20010;&#39046;&#22495;&#30340;&#26041;&#27861;&#21644;&#32467;&#26524;&#65292;&#23454;&#29616;&#36825;&#31181;&#25551;&#36848;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#23545;&#35748;&#30693;&#23433;&#20840;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#28982;&#21518;&#25552;&#20986;&#25551;&#36848;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07945v1 Announce Type: cross  Abstract: The rapid advancement in neurotechnology in recent years has created an emerging critical intersection between neurotechnology and security. Implantable devices, non-invasive monitoring, and non-invasive therapies all carry with them the prospect of violating the privacy and autonomy of individuals' cognition. A growing number of scientists and physicians have made calls to address this issue -- which we term Cognitive Security -- but applied efforts have been limited. A major barrier hampering scientific and engineering efforts to address Cognitive Security is the lack of a clear means of describing and analyzing relevant problems. In this paper we develop Cognitive Security, a mathematical framework which enables such description and analysis by drawing on methods and results from multiple fields. We demonstrate certain statistical properties which have significant implications for Cognitive Security, and then present descriptions of
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07921</link><description>&lt;p&gt;
Merino&#65306;&#22522;&#20110;&#29109;&#39537;&#21160;&#30340;IoT&#35774;&#22791;&#19978;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Merino: Entropy-driven Design for Generative Language Models on IoT Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07921
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#29616;&#20195;&#26102;&#20195;&#30340;&#38761;&#21629;&#24615;&#36827;&#27493;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#37096;&#32626;LLMs&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#65292;&#27604;&#22914;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#65292;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#32780;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35774;&#35745;&#33539;&#24335;&#26159;&#22312;&#32473;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#20869;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#12290;&#25972;&#20010;&#35774;&#35745;&#36807;&#31243;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#25968;&#23398;&#35268;&#21010;&#65288;MP&#65289;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#22312;CPU&#19978;&#23436;&#25104;&#65292;&#20351;&#20854;&#20960;&#20046;&#26159;&#38646;&#25104;&#26412;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#27169;&#22411;MeRino&#65292;&#22312;&#20061;&#20010;NLP&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MeRino&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#38646;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07921v1 Announce Type: cross  Abstract: Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost. In this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. Our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. The whole design procedure involves solving a mathematical programming (MP) problem, which can be done on the CPU within minutes, making it nearly zero-cost. We evaluate our designed models, termed MeRino, across nine NLP downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. Notably, MeRino achieves similar or better zero performan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#36801;&#31227;&#26694;&#26550;&#26469;&#23398;&#20064;&#26230;&#20307;&#22609;&#24615;&#29289;&#29702;&#24182;&#20174;&#21407;&#23376;&#27169;&#25311;&#20013;&#39044;&#27979;Peierls&#24212;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07526</link><description>&lt;p&gt;
&#29289;&#29702;&#23398;&#36801;&#31227;&#23398;&#20064;&#29992;&#20110;&#26448;&#26009;&#24378;&#24230;&#31579;&#36873;
&lt;/p&gt;
&lt;p&gt;
Physics-Transfer Learning for Material Strength Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07526
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#29289;&#29702;&#36801;&#31227;&#26694;&#26550;&#26469;&#23398;&#20064;&#26230;&#20307;&#22609;&#24615;&#29289;&#29702;&#24182;&#20174;&#21407;&#23376;&#27169;&#25311;&#20013;&#39044;&#27979;Peierls&#24212;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26448;&#26009;&#24378;&#24230;&#65292;&#20687;&#35768;&#22810;&#33258;&#28982;&#31185;&#23398;&#20013;&#30340;&#38382;&#39064;&#19968;&#26679;&#65292;&#28085;&#30422;&#22810;&#20010;&#38271;&#24230;&#21644;&#26102;&#38388;&#23610;&#24230;&#65292;&#24182;&#19988;&#35299;&#20915;&#26041;&#26696;&#24517;&#39035;&#22312;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#12290;Peierls&#24212;&#21147;&#26159;&#26230;&#20307;&#22609;&#24615;&#20013;&#30340;&#19968;&#20010;&#20013;&#24515;&#27010;&#24565;&#65292;&#36890;&#36807;&#20301;&#38169;&#23545;&#22609;&#24615;&#27969;&#21160;&#30340;&#38459;&#21147;&#26469;&#34913;&#37327;&#24378;&#24230;&#12290;&#30830;&#23450;Peierls&#24212;&#21147;&#28041;&#21450;&#21040;&#24377;&#24615;&#26230;&#26684;&#21709;&#24212;&#21644;&#26230;&#20307;&#28369;&#31227;&#33021;&#37327;&#26223;&#35266;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#12290;&#36890;&#36807;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#36890;&#36807;Peierls&#24212;&#21147;&#30340;&#26448;&#26009;&#24378;&#24230;&#31579;&#36873;&#23545;&#20110;&#20301;&#38169;&#30340;&#38750;&#23616;&#37096;&#29305;&#24615;&#32780;&#35328;&#22312;&#35745;&#31639;&#19978;&#24456;&#38590;&#65292;&#24182;&#19988;&#27809;&#26377;&#21253;&#21547;&#22312;&#26368;&#20808;&#36827;&#30340;&#35745;&#31639;&#26448;&#26009;&#25968;&#25454;&#24211;&#20013;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29289;&#29702;&#36801;&#31227;&#26694;&#26550;&#65292;&#20174;&#32463;&#39564;&#24615;&#21407;&#23376;&#27169;&#25311;&#20013;&#23398;&#20064;&#26230;&#20307;&#22609;&#24615;&#30340;&#29289;&#29702;&#35268;&#24459;&#65292;&#28982;&#21518;&#39044;&#27979;Peierls&#24212;&#21147;&#20174;&#21270;&#23398;&#20934;&#30830;&#30340;&#22522;&#20110;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#30340;&#35745;&#31639;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07526v1 Announce Type: cross  Abstract: The strength of materials, like many problems in the natural sciences, spans multiple length and time scales, and the solution has to balance accuracy and performance. Peierls stress is one of the central concepts in crystal plasticity that measures the strength through the resistance of a dislocation to plastic flow. The determination of Peierls stress involves a multiscale nature depending on both elastic lattice responses and the energy landscape of crystal slips. Material screening by strength via the Peierls stress from first-principles calculations is computationally intractable for the nonlocal characteristics of dislocations, and not included in the state-of-the-art computational material databases. In this work, we propose a physics-transfer framework to learn the physics of crystal plasticity from empirical atomistic simulations and then predict the Peierls stress from chemically accurate density functional theory-based calcu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#32593;&#30417;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#21644;&#25925;&#38556;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#20256;&#32479;&#30417;&#25511;&#31995;&#32479;&#30340;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.06942</link><description>&lt;p&gt;
&#20351;&#29992;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#30005;&#32593;&#30417;&#27979;&#21644;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Grid Monitoring and Protection with Continuous Point-on-Wave Measurements and Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06942
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#30005;&#32593;&#30417;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#25968;&#25454;&#21387;&#32553;&#21644;&#25925;&#38556;&#26816;&#27979;&#65292;&#23454;&#29616;&#20102;&#23545;&#20256;&#32479;&#30417;&#25511;&#31995;&#32479;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#19979;&#19968;&#20195;&#30005;&#32593;&#30417;&#27979;&#21644;&#25511;&#21046;&#31995;&#32479;&#30340;&#26696;&#20363;&#65292;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#12289;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#25512;&#26029;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#26102;&#24207;&#27979;&#37327;&#21644;AI&#25903;&#25345;&#30340;&#25968;&#25454;&#21387;&#32553;&#21644;&#25925;&#38556;&#26816;&#27979;&#30340;&#30417;&#27979;&#21644;&#25511;&#21046;&#26694;&#26550;&#65292;&#36229;&#36234;&#20102;&#20808;&#21069;&#22522;&#20110;SCADA&#21644;&#21516;&#27493;&#30456;&#37327;&#25216;&#26415;&#26500;&#24314;&#30340;&#24191;&#22495;&#30417;&#27979;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06942v1 Announce Type: cross  Abstract: Purpose This article presents a case for a next-generation grid monitoring and control system, leveraging recent advances in generative artificial intelligence (AI), machine learning, and statistical inference. Advancing beyond earlier generations of wide-area monitoring systems built upon supervisory control and data acquisition (SCADA) and synchrophasor technologies, we argue for a monitoring and control framework based on the streaming of continuous point-on-wave (CPOW) measurements with AI-powered data compression and fault detection.   Methods and Results: The architecture of the proposed design originates from the Wiener-Kallianpur innovation representation of a random process that transforms causally a stationary random process into an innovation sequence with independent and identically distributed random variables. This work presents a generative AI approach that (i) learns an innovation autoencoder that extracts innovation se
&lt;/p&gt;</description></item><item><title>KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.03101</link><description>&lt;p&gt;
KnowAgent: &#30693;&#35782;&#22686;&#24378;&#35268;&#21010;&#29992;&#20110;&#22522;&#20110;LLM&#30340;Agent
&lt;/p&gt;
&lt;p&gt;
KnowAgent: Knowledge-Augmented Planning for LLM-Based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03101
&lt;/p&gt;
&lt;p&gt;
KnowAgent&#24341;&#20837;&#20102;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#65292;&#36890;&#36807;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#22686;&#24378;LLM&#30340;&#35268;&#21010;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#35821;&#35328;Agent&#30340;&#35268;&#21010;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#22312;&#22788;&#29702;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#26102;&#20173;&#26377;&#25152;&#19981;&#36275;&#65292;&#29305;&#21035;&#26159;&#19982;&#29615;&#22659;&#20114;&#21160;&#36890;&#36807;&#29983;&#25104;&#21487;&#25191;&#34892;&#21160;&#20316;&#26102;&#12290;&#36825;&#31181;&#19981;&#36275;&#20027;&#35201;&#26469;&#33258;&#20110;&#35821;&#35328;Agent&#20013;&#32570;&#20047;&#20869;&#32622;&#21160;&#20316;&#30693;&#35782;&#65292;&#23548;&#33268;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#26080;&#27861;&#26377;&#25928;&#24341;&#23548;&#35268;&#21010;&#36712;&#36857;&#65292;&#20174;&#32780;&#23548;&#33268;&#35268;&#21010;&#24187;&#35273;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;KnowAgent&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#26174;&#24335;&#21160;&#20316;&#30693;&#35782;&#26469;&#22686;&#24378;LLM&#35268;&#21010;&#33021;&#21147;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;KnowAgent&#37319;&#29992;&#20102;&#19968;&#20010;&#21160;&#20316;&#30693;&#35782;&#24211;&#21644;&#19968;&#20010;&#30693;&#35782;&#22411;&#33258;&#23398;&#20064;&#31574;&#30053;&#26469;&#38480;&#21046;&#35268;&#21010;&#36807;&#31243;&#20013;&#30340;&#34892;&#21160;&#36335;&#24452;&#65292;&#23454;&#29616;&#26356;&#21512;&#29702;&#30340;&#36712;&#36857;&#21512;&#25104;&#65292;&#36827;&#32780;&#25552;&#39640;&#35821;&#35328;Agent&#30340;&#35745;&#21010;&#24615;&#33021;&#12290;&#22522;&#20110;HotpotQA&#21644;ALFWorld&#30340;&#23454;&#39564;&#32467;&#26524;&#22522;&#20110;&#19981;&#21516;&#30340;&#20027;&#24178;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03101v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated great potential in complex reasoning tasks, yet they fall short when tackling more sophisticated challenges, especially when interacting with environments through generating executable actions. This inadequacy primarily stems from the lack of built-in action knowledge in language agents, which fails to effectively guide the planning trajectories during task solving and results in planning hallucination. To address this issue, we introduce KnowAgent, a novel approach designed to enhance the planning capabilities of LLMs by incorporating explicit action knowledge. Specifically, KnowAgent employs an action knowledge base and a knowledgeable self-learning strategy to constrain the action path during planning, enabling more reasonable trajectory synthesis, and thereby enhancing the planning performance of language agents. Experimental results on HotpotQA and ALFWorld based on various backbone m
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#21644;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#33021;&#21147;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#26356;&#22909;&#30340;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.17217</link><description>&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Temporal Logic Specification-Conditioned Decision Transformer for Offline Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#26694;&#26550;&#65292;&#32467;&#21512;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#21644;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#33021;&#21147;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#22312;&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#20986;&#26356;&#22909;&#30340;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#22266;&#23450;&#25968;&#25454;&#38598;&#35757;&#32451;&#19968;&#20010;&#28385;&#36275;&#32422;&#26463;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21363;&#26102;&#38388;&#36923;&#36753;&#35268;&#33539;&#26465;&#20214;&#21270;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;SDT&#65289;&#65292;&#23427;&#21033;&#29992;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;STL&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#26469;&#25351;&#23450;&#20195;&#29702;&#24212;&#35813;&#36981;&#24490;&#30340;&#22797;&#26434;&#26102;&#38388;&#35268;&#21017;&#65292;&#20197;&#21450;&#20915;&#31574;&#36716;&#25442;&#22120;&#65288;DT&#65289;&#30340;&#39034;&#24207;&#24314;&#27169;&#33021;&#21147;&#12290;&#23545;DSRL&#22522;&#20934;&#27979;&#35797;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SDT&#22312;&#23398;&#20064;&#23433;&#20840;&#39640;&#22870;&#21169;&#31574;&#30053;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17217v1 Announce Type: cross  Abstract: Offline safe reinforcement learning (RL) aims to train a constraint satisfaction policy from a fixed dataset. Current state-of-the-art approaches are based on supervised learning with a conditioned policy. However, these approaches fall short in real-world applications that involve complex tasks with rich temporal and logical structures. In this paper, we propose temporal logic Specification-conditioned Decision Transformer (SDT), a novel framework that harnesses the expressive power of signal temporal logic (STL) to specify complex temporal rules that an agent should follow and the sequential modeling capability of Decision Transformer (DT). Empirical evaluations on the DSRL benchmarks demonstrate the better capacity of SDT in learning safe and high-reward policies compared with existing approaches. In addition, SDT shows good alignment with respect to different desired degrees of satisfaction of the STL specification that it is condi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15163</link><description>&lt;p&gt;
&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#20013;&#35780;&#20272;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Studying the Impact of Stochasticity on the Evaluation of Deep Neural Networks for Forest-Fire Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#22312;&#39640;&#24230;&#38543;&#26426;&#22330;&#26223;&#19979;&#30340;&#21487;&#38752;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;&#20551;&#35774;&#19979;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#29992;&#20110;&#31163;&#25955;&#21160;&#21147;&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#37326;&#28779;&#39044;&#27979;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#30740;&#31350;&#38543;&#26426;&#24615;&#23545;&#20004;&#31867;&#35780;&#20272;&#25351;&#26631;&#30340;&#24433;&#21709;&#65306;&#22522;&#20110;&#20998;&#31867;&#30340;&#25351;&#26631;&#65292;&#35780;&#20272;&#23545;&#35266;&#23519;&#22320;&#38754;&#30495;&#30456;&#65288;GT&#65289;&#30340;&#24544;&#23454;&#24230;&#65292;&#20197;&#21450;&#36866;&#24403;&#30340;&#24471;&#20998;&#35268;&#21017;&#65292;&#27979;&#35797;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#39640;&#24230;&#38543;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#35780;&#20272;&#23545;&#32479;&#35745;&#30340;&#24544;&#23454;&#24230;&#26159;&#19968;&#20010;&#21487;&#38752;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20998;&#26512;&#25193;&#23637;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#26862;&#26519;&#28779;&#28798;&#25968;&#25454;&#65292;&#31361;&#26174;&#20102;&#20256;&#32479;&#26862;&#26519;&#28779;&#28798;&#39044;&#27979;&#35780;&#20272;&#26041;&#27861;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24314;&#35758;&#21487;&#35299;&#37322;&#30340;&#36866;&#29992;&#20110;&#38543;&#26426;&#24615;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15163v1 Announce Type: cross  Abstract: This paper presents the first systematic study of the evaluation of Deep Neural Networks (DNNs) for discrete dynamical systems under stochastic assumptions, with a focus on wildfire prediction. We develop a framework to study the impact of stochasticity on two classes of evaluation metrics: classification-based metrics, which assess fidelity to observed ground truth (GT), and proper scoring rules, which test fidelity-to-statistic. Our findings reveal that evaluating for fidelity-to-statistic is a reliable alternative in highly stochastic scenarios. We extend our analysis to real-world wildfire data, highlighting limitations in traditional wildfire prediction evaluation methods, and suggest interpretable stochasticity-compatible alternatives.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#36710;&#36742;&#32452;&#20316;&#20026;&#20998;&#26512;&#23545;&#35937;&#65292;&#25506;&#35752;&#20102;&#32771;&#34385;&#36710;&#36742;&#32452;&#21644;&#36947;&#36335;&#27573;&#29305;&#24449;&#30340;&#39118;&#38505;&#24418;&#25104;&#21644;&#20256;&#25773;&#26426;&#21046;&#65292;&#35782;&#21035;&#20986;&#24433;&#21709;&#30896;&#25758;&#39118;&#38505;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.12415</link><description>&lt;p&gt;
&#22522;&#20110;&#36710;&#36742;&#32452;&#30340;&#39640;&#36895;&#20844;&#36335;&#30896;&#25758;&#39118;&#38505;&#24418;&#25104;&#21644;&#20256;&#25773;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Vehicle-group-based Crash Risk Formation and Propagation Analysis for Expressways
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#36710;&#36742;&#32452;&#20316;&#20026;&#20998;&#26512;&#23545;&#35937;&#65292;&#25506;&#35752;&#20102;&#32771;&#34385;&#36710;&#36742;&#32452;&#21644;&#36947;&#36335;&#27573;&#29305;&#24449;&#30340;&#39118;&#38505;&#24418;&#25104;&#21644;&#20256;&#25773;&#26426;&#21046;&#65292;&#35782;&#21035;&#20986;&#24433;&#21709;&#30896;&#25758;&#39118;&#38505;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#23558;&#36335;&#27573;&#19978;&#30340;&#30896;&#25758;&#25968;&#37327;&#25110;&#21487;&#33021;&#24615;&#19982;&#20132;&#36890;&#21442;&#25968;&#25110;&#36335;&#27573;&#30340;&#20960;&#20309;&#29305;&#24449;&#32852;&#31995;&#36215;&#26469;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#36710;&#36742;&#36830;&#32493;&#36816;&#21160;&#21644;&#19982;&#38468;&#36817;&#36710;&#36742;&#30340;&#20114;&#21160;&#23545;&#20854;&#24433;&#21709;&#12290;&#36890;&#20449;&#25216;&#26415;&#30340;&#36827;&#27493;&#36171;&#20104;&#20102;&#20174;&#21608;&#22260;&#36710;&#36742;&#25910;&#38598;&#39550;&#39542;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#20351;&#24471;&#30740;&#31350;&#22522;&#20110;&#36710;&#36742;&#32452;&#30340;&#30896;&#25758;&#39118;&#38505;&#25104;&#20026;&#21487;&#33021;&#12290;&#22522;&#20110;&#39640;&#20998;&#36776;&#29575;&#36710;&#36742;&#36712;&#36857;&#25968;&#25454;&#65292;&#26412;&#30740;&#31350;&#20197;&#36710;&#36742;&#32452;&#20316;&#20026;&#20998;&#26512;&#23545;&#35937;&#65292;&#25506;&#35752;&#20102;&#32771;&#34385;&#36710;&#36742;&#32452;&#21644;&#36947;&#36335;&#27573;&#29305;&#24449;&#30340;&#39118;&#38505;&#24418;&#25104;&#21644;&#20256;&#25773;&#26426;&#21046;&#12290;&#30830;&#23450;&#20102;&#20960;&#20010;&#24433;&#21709;&#30896;&#25758;&#39118;&#38505;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21253;&#25324;&#36807;&#21435;&#30340;&#39640;&#39118;&#38505;&#36710;&#36742;&#32452;&#29366;&#24577;&#12289;&#22797;&#26434;&#30340;&#36710;&#36742;&#34892;&#20026;&#12289;&#22823;&#22411;&#36710;&#36742;&#30340;&#39640;&#30334;&#20998;&#27604;&#12289;&#36710;&#36742;&#32452;&#20869;&#39057;&#32321;&#21464;&#36947;&#20197;&#21450;&#29305;&#23450;&#30340;&#36947;&#36335;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12415v1 Announce Type: new  Abstract: Previous studies in predicting crash risk primarily associated the number or likelihood of crashes on a road segment with traffic parameters or geometric characteristics of the segment, usually neglecting the impact of vehicles' continuous movement and interactions with nearby vehicles. Advancements in communication technologies have empowered driving information collected from surrounding vehicles, enabling the study of group-based crash risks. Based on high-resolution vehicle trajectory data, this research focused on vehicle groups as the subject of analysis and explored risk formation and propagation mechanisms considering features of vehicle groups and road segments. Several key factors contributing to crash risks were identified, including past high-risk vehicle-group states, complex vehicle behaviors, high percentage of large vehicles, frequent lane changes within a vehicle group, and specific road geometries. A multinomial logisti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03448</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65306;&#20855;&#26377;&#24191;&#20041;&#25910;&#25947;&#20445;&#35777;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36817;&#26469;&#21463;&#21040;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#26356;&#26032;&#21644;&#27169;&#22411;&#32858;&#21512;&#36825;&#20004;&#20010;&#20851;&#38190;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#37117;&#30001;&#23458;&#25143;&#31471;&#36827;&#34892;&#30340;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;DFL&#26041;&#27861;&#65292;&#23427;&#22312;&#36825;&#20004;&#20010;&#36807;&#31243;&#20013;&#24191;&#20041;&#21270;&#20102;&#38388;&#27463;&#24615;&#30340;&#27010;&#24565;&#65292;&#24314;&#27169;&#20102;&#22312;&#23454;&#38469;DFL&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19981;&#21516;&#24418;&#24335;&#30340;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;DSpodFL&#23558;&#35768;&#22810;&#30528;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#65292;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#65292;&#32479;&#19968;&#21040;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#19979;&#12290;&#25105;&#20204;&#23545;DSpodFL&#30340;&#25910;&#25947;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;&#21487;&#20197;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#65292;&#23558;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26377;&#38480;&#30340;&#26368;&#20339;&#24615;&#24046;&#36317;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works. Through experiments, we demonstra
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02803</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Distilling Medication Recommendation Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25913;&#36827;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;&#27169;&#22411;&#22312;&#21307;&#23398;&#25968;&#25454;&#35821;&#20041;&#29702;&#35299;&#21644;&#26032;&#24739;&#32773;&#22788;&#26041;&#25512;&#33616;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#25512;&#33616;&#26159;&#26234;&#33021;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#23427;&#26681;&#25454;&#24739;&#32773;&#29305;&#23450;&#30340;&#20581;&#24247;&#38656;&#27714;&#26469;&#25512;&#33616;&#26368;&#21512;&#36866;&#30340;&#33647;&#29289;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20351;&#29992;&#30340;&#35768;&#22810;&#22797;&#26434;&#27169;&#22411;&#24448;&#24448;&#24573;&#35270;&#21307;&#23398;&#25968;&#25454;&#30340;&#32454;&#24494;&#35821;&#20041;&#65292;&#32780;&#21482;&#26159;&#36807;&#24230;&#20381;&#36182;&#26631;&#35782;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#22788;&#29702;&#39318;&#27425;&#35775;&#38382;&#21307;&#38498;&#30340;&#24739;&#32773;&#30340;&#24773;&#20917;&#26102;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#20043;&#21069;&#30340;&#22788;&#26041;&#21382;&#21490;&#21487;&#20197;&#21442;&#32771;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24378;&#22823;&#30340;&#35821;&#20041;&#29702;&#35299;&#21644;&#36755;&#20837;&#19981;&#21487;&#30693;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;LLMs&#25913;&#36827;&#29616;&#26377;&#30340;&#33647;&#29289;&#25512;&#33616;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;&#33647;&#29289;&#25512;&#33616;&#65288;LEADER&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#21019;&#24314;&#21512;&#36866;&#30340;&#25552;&#31034;&#27169;&#26495;&#65292;&#20351;LLMs&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#33616;&#33647;&#29289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recommendation of medication is a vital aspect of intelligent healthcare systems, as it involves prescribing the most suitable drugs based on a patient's specific health needs. Unfortunately, many sophisticated models currently in use tend to overlook the nuanced semantics of medical data, while only relying heavily on identities. Furthermore, these models face significant challenges in handling cases involving patients who are visiting the hospital for the first time, as they lack prior prescription histories to draw upon. To tackle these issues, we harness the powerful semantic comprehension and input-agnostic characteristics of Large Language Models (LLMs). Our research aims to transform existing medication recommendation methodologies using LLMs. In this paper, we introduce a novel approach called Large Language Model Distilling Medication Recommendation (LEADER). We begin by creating appropriate prompt templates that enable LLMs to suggest medications effectively. However, the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;</title><link>https://arxiv.org/abs/2402.01878</link><description>&lt;p&gt;
LiPO: &#36890;&#36807;&#23398;&#20064;&#25490;&#24207;&#36827;&#34892;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
LiPO: Listwise Preference Optimization through Learning-to-Rank
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01878
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LiPO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#12290;&#36890;&#36807;&#20174;&#25490;&#21517;&#21015;&#34920;&#20013;&#23398;&#20064;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#20351;&#31574;&#30053;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21040;&#21487;&#34892;&#30340;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#24037;&#21453;&#39304;&#36827;&#34892;&#23545;&#40784;&#26159;&#25511;&#21046;&#20854;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34892;&#20026;&#30340;&#20851;&#38190;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;DPO&#21644;SLiC&#65292;&#25104;&#20026;&#20256;&#32479;&#30340;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#30340;&#22686;&#24378;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;&#23454;&#38469;&#19978;&#65292;&#20154;&#24037;&#21453;&#39304;&#36890;&#24120;&#20197;&#23545;&#22810;&#20010;&#21709;&#24212;&#36827;&#34892;&#25490;&#24207;&#30340;&#26684;&#24335;&#25552;&#20379;&#65292;&#20197;&#25674;&#38144;&#38405;&#35835;&#25552;&#31034;&#30340;&#25104;&#26412;&#12290;&#22810;&#20010;&#21709;&#24212;&#20063;&#21487;&#20197;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#25110;AI&#21453;&#39304;&#36827;&#34892;&#25490;&#24207;&#12290;&#32570;&#23569;&#20851;&#20110;&#30452;&#25509;&#36866;&#24212;&#21709;&#24212;&#21015;&#34920;&#30340;&#30740;&#31350;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#38382;&#39064;&#23450;&#20041;&#20026;&#19968;&#20010;&#21015;&#34920;&#22411;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#25551;&#36848;&#20102;&#21015;&#34920;&#22411;&#20559;&#22909;&#20248;&#21270;&#65288;LiPO&#65289;&#26694;&#26550;&#65292;&#22312;&#32473;&#23450;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#31574;&#30053;&#21487;&#20197;&#20174;&#19968;&#20010;&#25490;&#21517;&#21015;&#34920;&#20013;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#21487;&#34892;&#21709;&#24212;&#12290;&#36825;&#31181;&#35266;&#28857;&#19982;&#23398;&#20064;&#25490;&#24207;&#65288;LTR&#65289;&#24418;&#25104;&#26126;&#30830;&#30340;&#32852;&#31995;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20559;&#22909;&#20248;&#21270;&#24037;&#20316;&#21487;&#20197;&#26144;&#23556;&#21040;&#29616;&#26377;&#30340;&#25490;&#21517;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
Aligning language models (LMs) with curated human feedback is critical to control their behaviors in real-world applications. Several recent policy optimization methods, such as DPO and SLiC, serve as promising alternatives to the traditional Reinforcement Learning from Human Feedback (RLHF) approach. In practice, human feedback often comes in a format of a ranked list over multiple responses to amortize the cost of reading prompt. Multiple responses can also be ranked by reward models or AI feedback. There lacks such a study on directly fitting upon a list of responses. In this work, we formulate the LM alignment as a listwise ranking problem and describe the Listwise Preference Optimization (LiPO) framework, where the policy can potentially learn more effectively from a ranked list of plausible responses given the prompt. This view draws an explicit connection to Learning-to-Rank (LTR), where most existing preference optimization work can be mapped to existing ranking objectives, esp
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2311.18022</link><description>&lt;p&gt;
&#21033;&#29992;&#25351;&#25968;&#23610;&#24230;&#30340;&#28145;&#24230;&#24378;&#21270;ReLU&#32593;&#32476;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Compelling ReLU Network Initialization and Training to Leverage Exponential Scaling with Depth
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18022
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#36890;&#36807;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#31070;&#32463;&#32593;&#32476;&#30340;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#65292;&#20174;&#32780;&#24471;&#21040;&#36828;&#36828;&#36229;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ReLU&#28608;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#30475;&#20316;&#26159;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#32452;&#21512;&#12290;&#23545;&#20110;&#36825;&#26679;&#30340;&#32593;&#32476;&#65292;&#38543;&#30528;&#28145;&#24230;&#30340;&#22686;&#21152;&#65292;&#34920;&#36798;&#22312;&#36755;&#20837;&#22495;&#19978;&#30340;&#19981;&#21516;&#32447;&#24615;&#21306;&#22495;&#30340;&#25968;&#37327;&#26377;&#21487;&#33021;&#20197;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20294;&#24403;&#21021;&#22987;&#21442;&#25968;&#36873;&#25321;&#38543;&#26426;&#26102;&#65292;&#19981;&#22826;&#21487;&#33021;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#12290;&#36825;&#31181;&#19981;&#33391;&#30340;&#23610;&#24230;&#33021;&#22815;&#23548;&#33268;&#21363;&#20351;&#26159;&#31616;&#21333;&#20989;&#25968;&#20063;&#38656;&#35201;&#20351;&#29992;&#36807;&#22823;&#30340;&#27169;&#22411;&#26469;&#36817;&#20284;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#31574;&#30053;&#65306;&#39318;&#20808;&#20197;&#19968;&#31181;&#26041;&#24335;&#37325;&#26032;&#21442;&#25968;&#21270;&#32593;&#32476;&#26435;&#37325;&#65292;&#20351;&#24471;&#25351;&#25968;&#25968;&#37327;&#30340;&#28608;&#27963;&#27169;&#24335;&#24471;&#20197;&#23637;&#29616;&#12290;&#22312;&#36825;&#20123;&#26032;&#21442;&#25968;&#19978;&#36827;&#34892;&#35757;&#32451;&#21487;&#20197;&#24471;&#21040;&#19968;&#20010;&#21021;&#22987;&#35299;&#65292;&#31245;&#21518;&#36890;&#36807;&#26356;&#26032;&#24213;&#23618;&#27169;&#22411;&#26435;&#37325;&#26469;&#25913;&#36827;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#20135;&#29983;&#27604;&#38543;&#26426;&#21021;&#22987;&#21270;&#23545;&#24212;&#30340;&#20989;&#25968;&#36924;&#36817;&#22909;&#20960;&#20010;&#25968;&#37327;&#32423;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
A neural network with ReLU activations may be viewed as a composition of piecewise linear functions. For such networks, the number of distinct linear regions expressed over the input domain has the potential to scale exponentially with depth, but it is not expected to do so when the initial parameters are chosen randomly. This poor scaling can necessitate the use of overly large models to approximate even simple functions. To address this issue, we introduce a novel training strategy: we first reparameterize the network weights in a manner that forces an exponential number of activation patterns to manifest. Training first on these new parameters provides an initial solution that can later be refined by updating the underlying model weights. This approach allows us to produce function approximations that are several orders of magnitude better than their randomly initialized counterparts.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#25935;&#24863;&#20449;&#24687;&#27844;&#38706;&#30340;&#21435;&#20559;&#32622;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22270;&#32467;&#26500;&#20013;&#30340;&#25299;&#25169;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20559;&#32622;&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#19979;&#28216;&#35823;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.12824</link><description>&lt;p&gt;
MAPPING: &#20351;&#29992;&#26377;&#38480;&#25935;&#24863;&#20449;&#24687;&#27844;&#38706;&#30340;&#21435;&#20559;&#32622;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
MAPPING: Debiasing Graph Neural Networks for Fair Node Classification with Limited Sensitive Information Leakage. (arXiv:2401.12824v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26377;&#38480;&#25935;&#24863;&#20449;&#24687;&#27844;&#38706;&#30340;&#21435;&#20559;&#32622;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20844;&#24179;&#33410;&#28857;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22270;&#32467;&#26500;&#20013;&#30340;&#25299;&#25169;&#20381;&#36182;&#38382;&#39064;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20559;&#32622;&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#19979;&#28216;&#35823;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#21508;&#31181;&#22522;&#20110;&#32593;&#32476;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#32487;&#25215;&#24182;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#21382;&#21490;&#19978;&#30340;&#20559;&#35265;&#21644;&#31038;&#20250;&#21051;&#26495;&#21360;&#35937;&#65292;&#36825;&#20005;&#37325;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#22312;&#32447;&#20020;&#24202;&#35786;&#26029;&#12289;&#37329;&#34701;&#20449;&#36151;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#30340;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#20844;&#24179;&#24615;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19978;&#65292;&#24182;&#19981;&#33021;&#31616;&#21333;&#22320;&#22797;&#21046;&#21040;&#20855;&#26377;&#25299;&#25169;&#20381;&#36182;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#22270;&#32467;&#26500;&#20013;&#12290;&#29616;&#26377;&#30340;&#20844;&#24179;&#22270;&#23398;&#20064;&#36890;&#24120;&#20559;&#22909;&#20110;&#20351;&#29992;&#25104;&#23545;&#32422;&#26463;&#26469;&#23454;&#29616;&#20844;&#24179;&#24615;&#65292;&#20294;&#26080;&#27861;&#20811;&#26381;&#32500;&#24230;&#38480;&#21046;&#24182;&#23558;&#20854;&#25512;&#24191;&#21040;&#22810;&#20010;&#25935;&#24863;&#23646;&#24615;&#65307;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#38598;&#20013;&#22312;&#22788;&#29702;&#25216;&#26415;&#19978;&#26469;&#24378;&#21046;&#24182;&#35843;&#25972;&#20844;&#24179;&#24615;&#65292;&#22312;&#39044;&#22788;&#29702;&#38454;&#27573;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#21435;&#20559;&#32622;GNN&#26694;&#26550;&#65292;&#20197;&#38450;&#27490;&#19979;&#28216;&#35823;&#29992;&#24182;&#25552;&#39640;&#35757;&#32451;&#30340;&#21487;&#38752;&#24615;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;GNN&#24448;&#24448;&#20542;&#21521;&#20110;&#22686;&#24378;&#20844;&#24179;&#24615;&#25110;&#22686;&#21152;&#39044;&#27979;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;&#20108;&#32773;&#20043;&#38388;&#36827;&#34892;&#20840;&#38754;&#26435;&#34913;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite remarkable success in diverse web-based applications, Graph Neural Networks(GNNs) inherit and further exacerbate historical discrimination and social stereotypes, which critically hinder their deployments in high-stake domains such as online clinical diagnosis, financial crediting, etc. However, current fairness research that primarily craft on i.i.d data, cannot be trivially replicated to non-i.i.d. graph structures with topological dependence among samples. Existing fair graph learning typically favors pairwise constraints to achieve fairness but fails to cast off dimensional limitations and generalize them into multiple sensitive attributes; besides, most studies focus on in-processing techniques to enforce and calibrate fairness, constructing a model-agnostic debiasing GNN framework at the pre-processing stage to prevent downstream misuses and improve training reliability is still largely under-explored. Furthermore, previous work on GNNs tend to enhance either fairness or 
&lt;/p&gt;</description></item><item><title>CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2401.05043</link><description>&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
CreINNs: Credal-Set Interval Neural Networks for Uncertainty Estimation in Classification Tasks. (arXiv:2401.05043v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05043
&lt;/p&gt;
&lt;p&gt;
CreINNs&#26159;&#19968;&#31181;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;Credal-Set Interval Neural Networks&#65292;&#36890;&#36807;&#20445;&#30041;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#25429;&#25417;&#26435;&#37325;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CreINNs&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20248;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#38598;&#25104;&#65292;&#24182;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#23545;&#20110;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#36234;&#26469;&#36234;&#26377;&#21560;&#24341;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;Credal-Set Interval Neural Networks&#65288;CreINNs&#65289;&#65292;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#12290;CreINNs&#20445;&#30041;&#20102;&#20256;&#32479;&#30340;&#21306;&#38388;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#21306;&#38388;&#25429;&#25417;&#26435;&#37325;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#21516;&#26102;&#20351;&#29992;&#27010;&#29575;&#21306;&#38388;&#30340;&#25968;&#23398;&#26694;&#26550;&#39044;&#27979;&#21487;&#20449;&#21306;&#38388;&#12290;&#22312;&#19968;&#20010;&#36229;&#20986;&#20998;&#21457;&#26816;&#27979;&#22522;&#20934;&#65288;CIFAR10 vs SVHN&#65289;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20013;&#65292;CreINNs&#30456;&#27604;&#20110;&#21464;&#20998;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#21644;&#28145;&#24230;&#38598;&#25104;&#65288;DEs&#65289;&#65292;&#22312;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#19982;&#21464;&#20998;BNNs&#30456;&#27604;&#65292;CreINNs&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#26174;&#33879;&#38477;&#20302;&#65292;&#24182;&#19988;&#27604;DEs&#20855;&#26377;&#36739;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation is increasingly attractive for improving the reliability of neural networks. In this work, we present novel credal-set interval neural networks (CreINNs) designed for classification tasks. CreINNs preserve the traditional interval neural network structure, capturing weight uncertainty through deterministic intervals, while forecasting credal sets using the mathematical framework of probability intervals. Experimental validations on an out-of-distribution detection benchmark (CIFAR10 vs SVHN) showcase that CreINNs outperform epistemic uncertainty estimation when compared to variational Bayesian neural networks (BNNs) and deep ensembles (DEs). Furthermore, CreINNs exhibit a notable reduction in computational complexity compared to variational BNNs and demonstrate smaller model sizes than DEs.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04979</link><description>&lt;p&gt;
&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series. (arXiv:2401.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04979
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#38750;&#35268;&#21017;&#21644;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;NDE&#65289;&#30340;&#21487;&#36870;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#22522;&#20110;NDE&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#20445;&#35777;&#22312;&#20854;&#26631;&#20934;&#24418;&#24335;&#19979;&#36827;&#34892;&#21487;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#31070;&#32463;&#27969;&#30340;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Neural CDEs&#65289;&#30340;&#21464;&#31181;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#20302;&#30340;&#35745;&#31639;&#36127;&#25285;&#30340;&#21516;&#26102;&#30830;&#20445;&#20102;&#21487;&#36870;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#35757;&#32451;&#21452;&#37325;&#28508;&#22312;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#23545;&#21160;&#24577;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22686;&#24378;&#22411;&#21452;&#37325;&#28508;&#22312;&#29366;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#25552;&#39640;&#31934;&#24230;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method. While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form. Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden. Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics. Our research presents an advanced framework that excels in both classification and interpolation tasks. At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks. Empirical analysis demonstrates that our method significantly outperforms existing models. This work significan
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2311.01248</link><description>&lt;p&gt;
&#23558;&#20854;&#25512;&#21521;&#23637;&#31034;&#26497;&#38480;&#65306;&#22810;&#27169;&#24577;&#35270;&#35273;&#35302;&#35273;&#27169;&#20223;&#23398;&#20064;&#19982;&#21147;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Push it to the Demonstrated Limit: Multimodal Visuotactile Imitation Learning with Force Matching. (arXiv:2311.01248v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01248
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#22120;&#21644;&#27169;&#20223;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#37197;&#23545;&#20248;&#21270;&#35302;&#35273;&#21147;&#37327;&#26354;&#32447;&#21644;&#31616;&#21270;&#20256;&#24863;&#22120;&#24212;&#29992;&#65292;&#23545;&#25509;&#35302;&#20016;&#23500;&#30340;&#25805;&#20316;&#20219;&#21153;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#35302;&#35273;&#20256;&#24863;&#22120;&#24050;&#32463;&#25104;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#36807;&#31243;&#20013;&#33719;&#21462;&#23494;&#38598;&#25509;&#35302;&#20449;&#24687;&#30340;&#26377;&#25928;&#25163;&#27573;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#8220;&#36879;&#35270;&#20320;&#30340;&#30382;&#32932;&#8221;&#65288;STS&#65289;&#22411;&#20256;&#24863;&#22120;&#20855;&#26377;&#35270;&#35273;&#21644;&#35302;&#35273;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#21322;&#36879;&#26126;&#34920;&#38754;&#21644;&#21487;&#25511;&#29031;&#26126;&#23454;&#29616;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#35270;&#35273;&#35302;&#35273;&#20256;&#24863;&#19982;&#27169;&#20223;&#23398;&#20064;&#22312;&#23500;&#26377;&#25509;&#35302;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#22909;&#22788;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#35302;&#35273;&#21147;&#27979;&#37327;&#21644;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#36816;&#21160;&#31034;&#33539;&#20013;&#20135;&#29983;&#26356;&#22909;&#21305;&#37197;&#20154;&#20307;&#31034;&#33539;&#32773;&#30340;&#21147;&#26354;&#32447;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#35270;&#35273;/&#35302;&#35273;STS&#27169;&#24335;&#20999;&#25442;&#20316;&#20026;&#25511;&#21046;&#31574;&#30053;&#36755;&#20986;&#65292;&#31616;&#21270;&#20256;&#24863;&#22120;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#31181;&#35266;&#23519;&#37197;&#32622;&#65292;&#27604;&#36739;&#21644;&#23545;&#27604;&#20102;&#35270;&#35273;/&#35302;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#27169;&#24335;&#20999;&#25442;&#21644;&#19981;&#20999;&#25442;&#65289;&#19982;&#25163;&#33109;&#25346;&#36733;&#30340;&#30524;&#22312;&#25163;&#25668;&#20687;&#26426;&#30340;&#35270;&#35273;&#25968;&#25454;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#24191;&#27867;&#30340;&#23454;&#39564;&#31995;&#21015;&#19978;&#36827;&#34892;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical tactile sensors have emerged as an effective means to acquire dense contact information during robotic manipulation. A recently-introduced `see-through-your-skin' (STS) variant of this type of sensor has both visual and tactile modes, enabled by leveraging a semi-transparent surface and controllable lighting. In this work, we investigate the benefits of pairing visuotactile sensing with imitation learning for contact-rich manipulation tasks. First, we use tactile force measurements and a novel algorithm during kinesthetic teaching to yield a force profile that better matches that of the human demonstrator. Second, we add visual/tactile STS mode switching as a control policy output, simplifying the application of the sensor. Finally, we study multiple observation configurations to compare and contrast the value of visual/tactile data (both with and without mode switching) with visual data from a wrist-mounted eye-in-hand camera. We perform an extensive series of experiments on a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.00287</link><description>&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#27880;&#20837;&#65306;&#35780;&#20272;&#21644;&#25512;&#36827;&#20020;&#24202;&#25991;&#26412;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models. (arXiv:2311.00287v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#30340;&#21019;&#26032;&#26041;&#27861;ClinGen&#65292;&#35813;&#26041;&#27861;&#23558;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#39640;&#20102;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#24182;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#38656;&#35201;&#33021;&#22815;&#24212;&#23545;&#39046;&#22495;&#29305;&#23450;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#22797;&#26434;&#30340;&#21307;&#23398;&#26415;&#35821;&#21644;&#20020;&#24202;&#32972;&#26223;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#30452;&#25509;&#37096;&#32626;&#21487;&#33021;&#23548;&#33268;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#21463;&#21040;&#36164;&#28304;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#20020;&#24202;NLP&#20219;&#21153;&#30340;&#21512;&#25104;&#20020;&#24202;&#25991;&#26412;&#29983;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#12289;&#36164;&#28304;&#39640;&#25928;&#30340;&#26041;&#27861;ClinGen&#65292;&#23427;&#23558;&#30693;&#35782;&#27880;&#20837;&#21040;&#36825;&#20010;&#36807;&#31243;&#20013;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#28041;&#21450;&#20020;&#24202;&#30693;&#35782;&#25552;&#21462;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;LLM&#25552;&#31034;&#12290;&#20020;&#24202;&#20027;&#39064;&#21644;&#20889;&#20316;&#39118;&#26684;&#37117;&#26469;&#33258;&#22806;&#37096;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#22270;&#35889;&#21644;LLMs&#65292;&#20197;&#24341;&#23548;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#22312;7&#20010;&#20020;&#24202;NLP&#20219;&#21153;&#21644;16&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;ClinGen&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#22987;&#32456;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#26377;&#25928;&#22320;&#20351;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#20998;&#24067;&#23545;&#40784;&#65292;&#24182;&#26174;&#33879;&#20016;&#23500;&#20102;&#26679;&#26412;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the divers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65306;&#20998;&#23618;&#29420;&#31435;&#23376;&#27169;&#22411;&#35757;&#32451;&#65288;HIST&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20840;&#23616;&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#23376;&#27169;&#22411;&#65292;&#24182;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#36127;&#25285;&#65292;&#21516;&#26102;&#33410;&#32422;&#36164;&#28304;&#12290;</title><link>http://arxiv.org/abs/2310.17890</link><description>&lt;p&gt;
&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23376;&#27169;&#22411;&#21010;&#20998;&#65306;&#31639;&#27861;&#35774;&#35745;&#19982;&#25910;&#25947;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Submodel Partitioning in Hierarchical Federated Learning: Algorithm Design and Convergence Analysis. (arXiv:2310.17890v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65306;&#20998;&#23618;&#29420;&#31435;&#23376;&#27169;&#22411;&#35757;&#32451;&#65288;HIST&#65289;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#20840;&#23616;&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#23376;&#27169;&#22411;&#65292;&#24182;&#22312;&#20998;&#23618;&#32467;&#26500;&#20013;&#20998;&#24067;&#65292;&#20197;&#38477;&#20302;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#36127;&#25285;&#65292;&#21516;&#26102;&#33410;&#32422;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#30456;&#36739;&#20256;&#32479;&#30340;&#8220;&#26143;&#22411;&#25299;&#25169;&#8221;&#26550;&#26500;&#30340;&#32852;&#37030;&#23398;&#20064;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#19978;&#35757;&#32451;&#22823;&#35268;&#27169;&#27169;&#22411;&#26102;&#65292;HFL&#20173;&#28982;&#20250;&#23545;&#36793;&#32536;&#35774;&#22791;&#36896;&#25104;&#37325;&#22823;&#30340;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#23384;&#20648;&#36127;&#25285;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#20998;&#23618;&#29420;&#31435;&#23376;&#27169;&#22411;&#35757;&#32451;&#65288;HIST&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#20998;&#23618;&#22330;&#26223;&#19979;&#30340;&#36825;&#20123;&#38382;&#39064;&#12290;HIST&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20998;&#23618;&#29256;&#26412;&#30340;&#27169;&#22411;&#21010;&#20998;&#65292;&#21363;&#22312;&#27599;&#19968;&#36718;&#20013;&#23558;&#20840;&#23616;&#27169;&#22411;&#21010;&#20998;&#20026;&#19981;&#30456;&#20132;&#30340;&#23376;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#20998;&#24067;&#22312;&#19981;&#21516;&#30340;&#32454;&#32990;&#20013;&#65292;&#20351;&#24471;&#27599;&#20010;&#32454;&#32990;&#21482;&#36127;&#36131;&#35757;&#32451;&#20840;&#27169;&#22411;&#30340;&#19968;&#20010;&#21010;&#20998;&#12290;&#36825;&#26679;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#33410;&#30465;&#35745;&#31639;&#21644;&#23384;&#20648;&#25104;&#26412;&#65292;&#21516;&#26102;&#20943;&#36731;&#25972;&#20010;&#20998;&#23618;&#32467;&#26500;&#20013;&#30340;&#36890;&#20449;&#36127;&#36733;&#12290;&#25105;&#20204;&#23545;HIST&#22312;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#19979;&#30340;&#25910;&#25947;&#24615;&#34892;&#20026;&#36827;&#34892;&#20102;&#29305;&#24449;&#21270;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hierarchical federated learning (HFL) has demonstrated promising scalability advantages over the traditional "star-topology" architecture-based federated learning (FL). However, HFL still imposes significant computation, communication, and storage burdens on the edge, especially when training a large-scale model over resource-constrained Internet of Things (IoT) devices. In this paper, we propose hierarchical independent submodel training (HIST), a new FL methodology that aims to address these issues in hierarchical settings. The key idea behind HIST is a hierarchical version of model partitioning, where we partition the global model into disjoint submodels in each round, and distribute them across different cells, so that each cell is responsible for training only one partition of the full model. This enables each client to save computation/storage costs while alleviating the communication loads throughout the hierarchy. We characterize the convergence behavior of HIST for non-convex 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#25512;&#26029;&#20986;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#35266;&#27979;&#21463;&#21040;&#30340;&#31995;&#32479;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.15234</link><description>&lt;p&gt;
&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#65306;&#31995;&#32479;&#25928;&#24212;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Field-level simulation-based inference with galaxy catalogs: the impact of systematic effects. (arXiv:2310.15234v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26143;&#31995;&#30446;&#24405;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#22522;&#20110;&#27169;&#25311;&#30340;&#25512;&#26029;&#26041;&#27861;&#65292;&#33021;&#22815;&#40065;&#26834;&#22320;&#25512;&#26029;&#20986;&#23431;&#23449;&#23398;&#21442;&#25968;&#65292;&#35299;&#20915;&#20102;&#35266;&#27979;&#21463;&#21040;&#30340;&#31995;&#32479;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#31181;&#20174;&#26143;&#31995;&#32418;&#31227;&#35843;&#26597;&#20013;&#38480;&#21046;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#26377;&#25928;&#26041;&#27861;&#26159;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22330;&#32423;&#21035;&#30340;&#26080;&#20284;&#28982;&#25512;&#26029;&#65292;&#32780;&#19981;&#23545;&#23610;&#24230;&#36827;&#34892;&#21098;&#20999;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#24503;&#26705;&#33922;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24320;&#21457;&#20102;&#33021;&#22815;&#20934;&#30830;&#25512;&#26029;&#20986;&#36890;&#36807;&#20165;&#21253;&#21547;&#26143;&#31995;&#20301;&#32622;&#21644;&#24452;&#21521;&#36895;&#24230;&#30340;&#30446;&#24405;&#26469;&#30830;&#23450;$\Omega_{\rm m}$&#20540;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#23545;&#22825;&#20307;&#29289;&#29702;&#21644;&#20122;&#32593;&#26684;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#35266;&#27979;&#21463;&#21040;&#35768;&#22810;&#25928;&#24212;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;1&#65289;&#25513;&#34109;&#25928;&#24212;&#65292;2&#65289;&#29305;&#24322;&#36895;&#24230;&#21644;&#24452;&#21521;&#36317;&#31163;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;3&#65289;&#19981;&#21516;&#30340;&#26143;&#31995;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#35266;&#27979;&#21482;&#20801;&#35768;&#25105;&#20204;&#27979;&#37327;&#32418;&#31227;&#65292;&#32416;&#32544;&#20102;&#26143;&#31995;&#30340;&#24452;&#21521;&#20301;&#32622;&#21644;&#36895;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;CAMELS&#39033;&#30446;&#20013;&#19981;&#21516;&#20195;&#30721;&#36816;&#34892;&#30340;&#26368;&#26032;&#27700;&#21160;&#21147;&#23398;&#27169;&#25311;&#29983;&#25104;&#30340;&#26143;&#31995;&#30446;&#24405;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#25311;&#32771;&#34385;&#20102;&#36825;&#20123;&#35266;&#27979;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
It has been recently shown that a powerful way to constrain cosmological parameters from galaxy redshift surveys is to train graph neural networks to perform field-level likelihood-free inference without imposing cuts on scale. In particular, de Santi et al. (2023) developed models that could accurately infer the value of $\Omega_{\rm m}$ from catalogs that only contain the positions and radial velocities of galaxies that are robust to uncertainties in astrophysics and subgrid models. However, observations are affected by many effects, including 1) masking, 2) uncertainties in peculiar velocities and radial distances, and 3) different galaxy selections. Moreover, observations only allow us to measure redshift, intertwining galaxies' radial positions and velocities. In this paper we train and test our models on galaxy catalogs, created from thousands of state-of-the-art hydrodynamic simulations run with different codes from the CAMELS project, that incorporate these observational effect
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#24179;&#28369;&#30340;&#38381;&#24335;&#24471;&#20998;&#20989;&#25968;&#26469;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#19988;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#33021;&#22815;&#23454;&#29616;&#19982;&#31070;&#32463;SGMs&#30456;&#31454;&#20105;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12395</link><description>&lt;p&gt;
&#38381;&#24335;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Closed-Form Diffusion Models. (arXiv:2310.12395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38381;&#24335;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#26174;&#24335;&#24179;&#28369;&#30340;&#38381;&#24335;&#24471;&#20998;&#20989;&#25968;&#26469;&#29983;&#25104;&#26032;&#26679;&#26412;&#65292;&#26080;&#38656;&#35757;&#32451;&#65292;&#19988;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#33021;&#22815;&#23454;&#29616;&#19982;&#31070;&#32463;SGMs&#30456;&#31454;&#20105;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;(SGMs)&#36890;&#36807;&#36845;&#20195;&#22320;&#20351;&#29992;&#25200;&#21160;&#30446;&#26631;&#20989;&#25968;&#30340;&#24471;&#20998;&#20989;&#25968;&#26469;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#12290;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#35757;&#32451;&#38598;&#65292;&#21487;&#20197;&#38381;&#24335;&#22320;&#35780;&#20272;&#36825;&#20010;&#24471;&#20998;&#20989;&#25968;&#65292;&#20294;&#30001;&#27492;&#24471;&#21040;&#30340;SGMs&#20250;&#35760;&#24518;&#20854;&#35757;&#32451;&#25968;&#25454;&#65292;&#19981;&#33021;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#21487;&#20197;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26469;&#36817;&#20284;&#24471;&#20998;&#20989;&#25968;&#65292;&#20294;&#36825;&#31181;&#36817;&#20284;&#30340;&#35823;&#24046;&#26377;&#21161;&#20110;&#25512;&#24191;&#65292;&#28982;&#32780;&#31070;&#32463;SGMs&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#20195;&#20215;&#39640;&#65292;&#32780;&#19988;&#23545;&#20110;&#36825;&#31181;&#35823;&#24046;&#25552;&#20379;&#30340;&#26377;&#25928;&#27491;&#21017;&#21270;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#23578;&#19981;&#28165;&#26970;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#26174;&#24335;&#24179;&#28369;&#30340;&#38381;&#24335;&#24471;&#20998;&#26469;&#33719;&#24471;&#19968;&#20010;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;SGMs&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26368;&#36817;&#37051;&#30340;&#39640;&#25928;&#24471;&#20998;&#20989;&#25968;&#20272;&#35745;&#22120;&#12290;&#21033;&#29992;&#36825;&#20010;&#20272;&#35745;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28040;&#36153;&#32423;CPU&#19978;&#36816;&#34892;&#26102;&#33021;&#22815;&#36798;&#21040;&#19982;&#31070;&#32463;SGMs&#30456;&#31454;&#20105;&#30340;&#37319;&#26679;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative models (SGMs) sample from a target distribution by iteratively transforming noise using the score function of the perturbed target. For any finite training set, this score function can be evaluated in closed form, but the resulting SGM memorizes its training data and does not generate novel samples. In practice, one approximates the score by training a neural network via score-matching. The error in this approximation promotes generalization, but neural SGMs are costly to train and sample, and the effective regularization this error provides is not well-understood theoretically. In this work, we instead explicitly smooth the closed-form score to obtain an SGM that generates novel samples without training. We analyze our model and propose an efficient nearest-neighbor-based estimator of its score function. Using this estimator, our method achieves sampling times competitive with neural SGMs while running on consumer-grade CPUs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#36229;&#21442;&#25968;&#23545;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22312;&#39640;&#32500;&#32452;&#23398;&#30740;&#31350;&#20013;&#65292;&#36866;&#24403;&#35774;&#32622;RF&#36229;&#21442;&#25968;&#23545;&#36873;&#25321;&#37325;&#35201;&#21464;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2309.06943</link><description>&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#20013;&#36229;&#21442;&#25968;&#23545;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Effect of hyperparameters on variable selection in random forests. (arXiv:2309.06943v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06943
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#38543;&#26426;&#26862;&#26519;&#20013;&#36229;&#21442;&#25968;&#23545;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#65292;&#22312;&#39640;&#32500;&#32452;&#23398;&#30740;&#31350;&#20013;&#65292;&#36866;&#24403;&#35774;&#32622;RF&#36229;&#21442;&#25968;&#23545;&#36873;&#25321;&#37325;&#35201;&#21464;&#37327;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#26862;&#26519;&#65288;RF&#65289;&#22312;&#39640;&#32500;&#32452;&#23398;&#30740;&#31350;&#20013;&#36866;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#21644;&#21464;&#37327;&#36873;&#25321;&#12290;&#20808;&#21069;&#30740;&#31350;&#20102;RF&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#23545;&#39044;&#27979;&#24615;&#33021;&#21644;&#21464;&#37327;&#37325;&#35201;&#24615;&#20272;&#35745;&#30340;&#24433;&#21709;&#65292;&#20294;&#36229;&#21442;&#25968;&#23545;&#22522;&#20110;RF&#30340;&#21464;&#37327;&#36873;&#25321;&#30340;&#24433;&#21709;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#21033;&#29992;&#29702;&#35770;&#20998;&#24067;&#21644;&#23454;&#35777;&#22522;&#22240;&#34920;&#36798;&#25968;&#25454;&#36827;&#34892;&#20102;&#20004;&#20010;&#27169;&#25311;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;Vita&#21644;Boruta&#21464;&#37327;&#36873;&#25321; procedures &#22312;&#36873;&#25321;&#37325;&#35201;&#21464;&#37327;&#65288;&#25935;&#24863;&#24615;&#65289;&#30340;&#21516;&#26102;&#25511;&#21046;&#34394;&#35686;&#29575;&#65288;FDR&#65289;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#65292;&#35201;&#27604;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#25277;&#21462;&#31574;&#30053;&#21644;&#26368;&#23567;&#32456;&#31471;&#33410;&#28857;&#22823;&#23567;&#26356;&#33021;&#24433;&#21709;&#36873;&#25321; procedures&#12290;RF&#36229;&#21442;&#25968;&#30340;&#21512;&#36866;&#35774;&#32622;&#21462;&#20915;&#20110;
&lt;/p&gt;
&lt;p&gt;
Random forests (RFs) are well suited for prediction modeling and variable selection in high-dimensional omics studies. The effect of hyperparameters of the RF algorithm on prediction performance and variable importance estimation have previously been investigated. However, how hyperparameters impact RF-based variable selection remains unclear. We evaluate the effects on the Vita and the Boruta variable selection procedures based on two simulation studies utilizing theoretical distributions and empirical gene expression data. We assess the ability of the procedures to select important variables (sensitivity) while controlling the false discovery rate (FDR). Our results show that the proportion of splitting candidate variables (mtry.prop) and the sample fraction (sample.fraction) for the training dataset influence the selection procedures more than the drawing strategy of the training datasets and the minimal terminal node size. A suitable setting of the RF hyperparameters depends on the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2308.12215</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#26041;&#38754;&#30340;&#25361;&#25112;&#65306;&#19968;&#20010;&#38024;&#23545;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Challenges of Machine Learning for Trust and Safety: A Case Study on Misinformation Detection. (arXiv:2308.12215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12215
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20026;&#20363;&#65292;&#26816;&#26597;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#20013;&#23398;&#26415;&#19982;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#65292;&#24182;&#21457;&#29616;&#20102;&#25991;&#29486;&#20013;&#23384;&#22312;&#30340;&#20005;&#37325;&#19981;&#36275;&#20043;&#22788;&#65292;&#21253;&#25324;&#20219;&#21153;&#19981;&#31526;&#21512;&#22312;&#32447;&#26381;&#21153;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#19981;&#30495;&#23454;&#12289;&#35780;&#20272;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#31561;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20351;&#29992;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#26816;&#26597;&#20102;&#22312;&#23558;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#19978;&#23398;&#26415;&#21644;&#23454;&#36341;&#20043;&#38388;&#30340;&#33073;&#33410;&#12290;&#25105;&#20204;&#23545;&#35813;&#39046;&#22495;&#20013;270&#31687;&#24191;&#21463;&#24341;&#29992;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#33258;&#21160;&#26816;&#27979;&#34394;&#20551;&#20449;&#24687;&#30340;&#25991;&#29486;&#31995;&#32479;&#21270;&#65292;&#24182;&#23545;&#23376;&#38598;&#20013;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#12289;&#35774;&#35745;&#22833;&#35823;&#12289;&#21487;&#22797;&#29616;&#24615;&#21644;&#27867;&#21270;&#24615;&#31561;&#26041;&#38754;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#21457;&#29616;&#25991;&#29486;&#20013;&#23384;&#22312;&#20005;&#37325;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#36825;&#23545;&#25152;&#22768;&#31216;&#30340;&#24615;&#33021;&#21644;&#23454;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#19982;&#22312;&#32447;&#26381;&#21153;&#30495;&#27491;&#38754;&#20020;&#30340;&#25361;&#25112;&#26377;&#26412;&#36136;&#19978;&#30340;&#21306;&#21035;&#12290;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35780;&#20272;&#36890;&#24120;&#19981;&#20195;&#34920;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#26223;&#65292;&#32780;&#19988;&#35780;&#20272;&#24448;&#24448;&#19981;&#29420;&#31435;&#20110;&#27169;&#22411;&#35757;&#32451;&#12290;&#25968;&#25454;&#21644;&#20195;&#30721;&#30340;&#21487;&#29992;&#24615;&#24456;&#24046;&#12290;&#27169;&#22411;&#22312;&#39046;&#22495;&#22806;&#30340;&#25968;&#25454;&#19978;&#27867;&#21270;&#33021;&#21147;&#19981;&#24378;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35780;&#20272;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20110;&#20449;&#20219;&#19982;&#23433;&#20840;&#38382;&#39064;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We examine the disconnect between scholarship and practice in applying machine learning to trust and safety problems, using misinformation detection as a case study. We systematize literature on automated detection of misinformation across a corpus of 270 well-cited papers in the field. We then examine subsets of papers for data and code availability, design missteps, reproducibility, and generalizability. We find significant shortcomings in the literature that call into question claimed performance and practicality. Detection tasks are often meaningfully distinct from the challenges that online services actually face. Datasets and model evaluation are often non-representative of real-world contexts, and evaluation frequently is not independent of model training. Data and code availability is poor. Models do not generalize well to out-of-domain data. Based on these results, we offer recommendations for evaluating machine learning applications to trust and safety problems. Our aim is fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09254</link><description>&lt;p&gt;
&#29992;&#20110;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#22686;&#24378;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30001;&#20110;&#23545;&#29983;&#25104;&#34394;&#26500;&#20107;&#23454;&#30340;&#25285;&#24551;&#65292;&#26368;&#36817;&#20852;&#36215;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#29305;&#21035;&#24378;&#35843;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#39044;&#27979;&#38598;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#30340;&#26041;&#24335;&#37327;&#21270;GLM&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#39044;&#27979;&#38598;&#27169;&#22411;&#36890;&#36807;&#26631;&#37327;&#20540;&#21442;&#25968;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#39044;&#27979;&#38598;&#65292;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#20173;&#28385;&#36275;PAC&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#22235;&#31181;&#31867;&#22411;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#20845;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#19978;&#23637;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05772</link><description>&lt;p&gt;
&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38169;&#35823;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36825;&#31361;&#20986;&#20102;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#30830;&#23450;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25163;&#27573;&#65292;&#8220;&#30693;&#36947;&#19968;&#20010;&#27169;&#22411;&#19981;&#30693;&#36947;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#65292;&#20854;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20351;&#29992;&#38543;&#26426;&#38598;&#21512;&#30340;&#25968;&#23398;&#65292;&#21363;&#23545;&#26679;&#26412;&#31354;&#38388;&#30340;&#24130;&#38598;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38543;&#26426;&#38598;&#27169;&#22411;&#33021;&#22815;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#20013;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#8220;&#35748;&#35782;&#24615;&#8221;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#32622;&#20449;&#38598;&#30340;&#22823;&#23567;&#26469;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief func
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00677</link><description>&lt;p&gt;
SDC-HSDD-NDSA: &#20351;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#21644;&#24402;&#19968;&#21270;&#23494;&#24230;&#33258;&#36866;&#24212;&#30340;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#35782;&#21035;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#65292;&#21482;&#35201;&#19981;&#21516;&#30340;&#39640;&#23494;&#24230;&#32858;&#31867;&#20043;&#38388;&#26377;&#20302;&#23494;&#24230;&#21306;&#22495;&#20998;&#38548;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20302;&#23494;&#24230;&#21306;&#22495;&#23558;&#32858;&#31867;&#20998;&#38548;&#24320;&#30340;&#35201;&#27714;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#39640;&#23494;&#24230;&#21306;&#22495;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26500;&#65292;&#24212;&#35813;&#34987;&#32858;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;&#12290;&#36825;&#31181;&#24773;&#20917;&#35828;&#26126;&#20102;&#25105;&#20204;&#24050;&#30693;&#30340;&#25152;&#26377;&#20808;&#21069;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30340;&#20027;&#35201;&#32570;&#38519;--&#26080;&#27861;&#26816;&#27979;&#39640;&#23494;&#24230;&#32858;&#31867;&#20013;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#26696;&#65292;&#26082;&#20855;&#26377;&#20808;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21448;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#26410;&#34987;&#20302;&#23494;&#24230;&#21306;&#20998;&#24320;&#30340;&#32467;&#26500;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#12289;&#23618;&#27425;&#21270;&#12289;&#24402;&#19968;&#21270;&#23494;&#24230;&#20197;&#21450;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Direc
&lt;/p&gt;</description></item><item><title>GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09872</link><description>&lt;p&gt;
&#21487;&#27867;&#21270;&#30340;&#19968;&#27425;&#24615;&#32499;&#32034;&#25805;&#20316;&#31574;&#30053;&#21450;&#20854;&#21442;&#25968;&#24863;&#30693;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizable One-shot Rope Manipulation with Parameter-Aware Policy. (arXiv:2306.09872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09872
&lt;/p&gt;
&lt;p&gt;
GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#32499;&#32034;&#22312;&#36816;&#21160;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#20026;&#22240;&#32032;&#65292;&#20197;&#24448;&#32499;&#32034;&#25805;&#20316;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#25968;&#30334;&#27425;&#30495;&#23454;&#28436;&#31034;&#26469;&#20026;&#27599;&#20010;&#32499;&#32034;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#8220;&#21040;&#36798;&#30446;&#26631;&#8221;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25105;&#20204;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GenORM&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35753;&#25805;&#20316;&#31574;&#30053;&#36890;&#36807;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#30340;&#32499;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#31574;&#30053;&#19978;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#24182;&#20351;&#29992;&#21508;&#31181;&#27169;&#25311;&#21487;&#21464;&#24418;&#32499;&#32034;&#26469;&#35757;&#32451;&#23427;&#65292;&#20351;&#31574;&#30053;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#32499;&#32034;&#21442;&#25968;&#35843;&#25972;&#34892;&#21160;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;GenORM&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#28436;&#31034;&#21644;&#27169;&#25311;&#28857;&#20113;&#30340;&#32593;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#12290;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#24110;&#21161;&#65292;&#25105;&#20204;&#20165;&#38656;&#35201;&#19968;&#27425;&#28436;&#31034;&#25968;&#25454;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#30340;&#32499;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;</title><link>http://arxiv.org/abs/2306.05497</link><description>&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#25439;&#22833;&#20989;&#25968;&#65306;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Reevaluating Loss Functions: Enhancing Robustness to Label Noise in Deep Learning Models. (arXiv:2306.05497v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05497
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#22686;&#24378;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#26631;&#31614;&#22122;&#22768;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26032;&#25216;&#26415;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#26631;&#27880;&#30340;&#25968;&#25454;&#38598;&#20013;&#38590;&#20813;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#26631;&#31614;&#65292;&#36825;&#32473;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#24102;&#26469;&#20102;&#26497;&#22823;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#24456;&#23481;&#26131;&#36866;&#24212;&#36825;&#20123;&#38169;&#35823;&#30340;&#26631;&#31614;&#12290;&#21482;&#26377;&#20351;&#29992;&#19981;&#21463;&#22122;&#22768;&#24178;&#25200;&#30340;&#40065;&#26834;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#65292;&#25165;&#33021;&#33719;&#24471;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#21019;&#24314;&#22122;&#22768;&#40065;&#26834;&#27169;&#22411;&#30340;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#24335;&#26159;&#20351;&#29992;&#22122;&#22768;&#40065;&#26834;&#25439;&#22833;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#25968;&#37327;&#20247;&#22810;&#65292;&#23427;&#20204;&#36890;&#24120;&#20276;&#38543;&#30528;&#36229;&#21442;&#25968;&#65292;&#32780;&#19988;&#21487;&#33021;&#23398;&#20064;&#36895;&#24230;&#27604;&#24191;&#27867;&#20351;&#29992;&#20294;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#35201;&#24930;&#12290;&#36890;&#36807;&#21551;&#21457;&#24335;&#32771;&#34385;&#21644;&#24191;&#27867;&#30340;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21738;&#20123;&#24773;&#20917;&#19979;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#36866;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#22914;&#20309;&#36873;&#25321;&#21512;&#36866;&#30340;&#25439;&#22833;&#30340;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25216;&#26415;&#26469;&#22686;&#24378;&#24102;&#26377;&#26377;&#30028;&#25439;&#22833;&#20989;&#25968;&#30340;&#23398;&#20064;&#65306;&#21253;&#25324;&#36755;&#20986;&#20559;&#32622;&#65292;&#21363;&#30053;&#24494;&#22686;&#21152;&#19982;&#27491;&#30830;&#26631;&#31614;&#30456;&#23545;&#24212;&#30340;&#31070;&#32463;&#20803;&#39044;&#28608;&#27963;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#25216;&#26415;&#22312;&#26080;&#38656;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#31867;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large annotated datasets inevitably contain incorrect labels, which poses a major challenge for the training of deep neural networks as they easily fit the labels. Only when training with a robust model that is not easily distracted by the noise, a good generalization performance can be achieved. A simple yet effective way to create a noise robust model is to use a noise robust loss function. However, the number of proposed loss functions is large, they often come with hyperparameters, and may learn slower than the widely used but noise sensitive Cross Entropy loss. By heuristic considerations and extensive numerical experiments, we study in which situations the proposed loss functions are applicable and give suggestions on how to choose an appropriate loss. Additionally, we propose a novel technique to enhance learning with bounded loss functions: the inclusion of an output bias, i.e. a slight increase in the neuron pre-activation corresponding to the correct label. Surprisingly, we f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.16703</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304; -- &#19968;&#20010;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Sources of Uncertainty in Machine Learning -- A Statisticians' View. (arXiv:2305.16703v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22238;&#31572;&#20960;&#24180;&#21069;&#38590;&#20197;&#24819;&#35937;&#30340;&#38382;&#39064;&#12290;&#38500;&#20102;&#36825;&#20123;&#25104;&#21151;&#20043;&#22806;&#65292;&#36234;&#26469;&#36234;&#28165;&#26224;&#30340;&#26159;&#65292;&#22312;&#32431;&#39044;&#27979;&#20043;&#22806;&#65292;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#20063;&#26159;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#32463;&#20986;&#29616;&#20102;&#36825;&#26041;&#38754;&#30340;&#31532;&#19968;&#25209;&#27010;&#24565;&#21644;&#24605;&#24819;&#65292;&#20294;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#30340;&#35270;&#35282;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#26356;&#24120;&#35265;&#30456;&#20851;&#30340;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#26088;&#22312;&#35268;&#33539;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21508;&#24322;&#65292;&#24182;&#19988;&#19981;&#24635;&#26159;&#21487;&#20197;&#20998;&#35299;&#20026;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#31867;&#27604;&#65292;&#25105;&#20204;&#20063;&#23637;&#31034;&#20102;&#32479;&#35745;&#23398;&#27010;&#24565;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the rol
&lt;/p&gt;</description></item><item><title>DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.09479</link><description>&lt;p&gt;
DiFaReli: &#25193;&#25955;&#20154;&#33080;&#37325;&#29031;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
DiFaReli : Diffusion Face Relighting. (arXiv:2304.09479v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09479
&lt;/p&gt;
&lt;p&gt;
DiFaReli&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#65292;&#33021;&#22815;&#22788;&#29702;&#21333;&#35270;&#35282;&#30340;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#20154;&#33080;&#37325;&#29031;&#65292;&#26080;&#38656;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#65292;&#23454;&#39564;&#34920;&#26126;&#20854;&#25928;&#26524;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;&#21333;&#35270;&#35282;&#20154;&#33080;&#37325;&#29031;&#12290;&#22788;&#29702;&#20840;&#23616;&#29031;&#26126;&#25110;&#25237;&#24433;&#38452;&#24433;&#31561;&#38750;&#28459;&#21453;&#23556;&#25928;&#24212;&#19968;&#30452;&#26159;&#20154;&#33080;&#37325;&#29031;&#39046;&#22495;&#30340;&#38590;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#23450;&#20848;&#20271;&#29305;&#21453;&#23556;&#34920;&#38754;&#65292;&#31616;&#21270;&#20809;&#29031;&#27169;&#22411;&#65292;&#25110;&#32773;&#38656;&#35201;&#20272;&#35745;&#19977;&#32500;&#24418;&#29366;&#12289;&#21453;&#23556;&#29575;&#25110;&#38452;&#24433;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20272;&#35745;&#26159;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#38656;&#35201;&#35768;&#22810;&#20855;&#26377;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#30340;&#35757;&#32451;&#26679;&#26412;&#25165;&#33021;&#24456;&#22909;&#22320;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32469;&#36807;&#20102;&#20934;&#30830;&#20272;&#35745;&#22266;&#26377;&#32452;&#20214;&#30340;&#38656;&#35201;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;2D&#22270;&#20687;&#35757;&#32451;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#20809;&#32447;&#33310;&#21488;&#25968;&#25454;&#12289;&#22810;&#35270;&#22270;&#22270;&#20687;&#25110;&#20809;&#29031;&#22522;&#30784;&#20107;&#23454;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#21033;&#29992;&#26465;&#20214;&#25193;&#25955;&#38544;&#24335;&#27169;&#22411;&#65288;DDIM&#65289;&#35299;&#30721;&#35299;&#32806;&#30340;&#20809;&#32534;&#30721;&#20197;&#21450;&#20174;&#29616;&#25104;&#30340;&#20272;&#31639;&#22120;&#25512;&#26029;&#20986;&#30340;&#19982;3D&#24418;&#29366;&#21644;&#38754;&#37096;&#36523;&#20221;&#30456;&#20851;&#30340;&#20854;&#20182;&#32534;&#30721;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35843;&#33410;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#24402;&#19968;&#21270;&#26041;&#26696;&#65292;&#31616;&#21270;&#20809;&#19982;&#20960;&#20309;&#20043;&#38388;&#22797;&#26434;&#20114;&#21160;&#30340;&#24314;&#27169;&#12290;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to single-view face relighting in the wild. Handling non-diffuse effects, such as global illumination or cast shadows, has long been a challenge in face relighting. Prior work often assumes Lambertian surfaces, simplified lighting models or involves estimating 3D shape, albedo, or a shadow map. This estimation, however, is error-prone and requires many training examples with lighting ground truth to generalize well. Our work bypasses the need for accurate estimation of intrinsic components and can be trained solely on 2D images without any light stage data, multi-view images, or lighting ground truth. Our key idea is to leverage a conditional diffusion implicit model (DDIM) for decoding a disentangled light encoding along with other encodings related to 3D shape and facial identity inferred from off-the-shelf estimators. We also propose a novel conditioning technique that eases the modeling of the complex interaction between light and geometry by using a ren
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;LQR&#25511;&#21046;&#22120;&#65292;&#22312;&#20960;&#20046;&#24517;&#28982;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#21518;&#24724;&#19978;&#38480;&#35777;&#26126;&#65292;&#19988;&#20855;&#26377;&#26029;&#30005;&#26426;&#21046;&#20445;&#35777;&#23433;&#20840;&#24182;&#23545;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2301.05537</link><description>&lt;p&gt;
&#33258;&#36866;&#24212; LQR &#31639;&#27861;&#30340;&#36817;&#20046;&#24517;&#28982; $\sqrt{T}$ &#21518;&#24724;&#19978;&#38480;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Almost Surely $\sqrt{T}$ Regret Bound for Adaptive LQR. (arXiv:2301.05537v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05537
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;LQR&#25511;&#21046;&#22120;&#65292;&#22312;&#20960;&#20046;&#24517;&#28982;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#21518;&#24724;&#19978;&#38480;&#35777;&#26126;&#65292;&#19988;&#20855;&#26377;&#26029;&#30005;&#26426;&#21046;&#20445;&#35777;&#23433;&#20840;&#24182;&#23545;&#24615;&#33021;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26410;&#30693;&#31995;&#32479;&#21442;&#25968;&#30340;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#38382;&#39064;(LQR)&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#33267;&#20170;&#20173;&#19981;&#28165;&#26970;&#26159;&#21542;&#33021;&#20960;&#20046;&#24517;&#28982;&#22320;&#36798;&#21040; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#30340;&#21518;&#24724;&#19978;&#38480;&#65292;&#32780;&#26412;&#25991;&#21017;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;LQR&#25511;&#21046;&#22120;&#65292;&#22312;&#20960;&#20046;&#24517;&#28982;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377; $\tilde{ \mathcal{O}}(\sqrt{T})$ &#21518;&#24724;&#19978;&#38480;&#30340;&#35777;&#26126;&#12290;&#35813;&#25511;&#21046;&#22120;&#20855;&#26377;&#26029;&#30005;&#26426;&#21046;&#65292;&#21487;&#20197;&#32469;&#36807;&#28508;&#22312;&#30340;&#23433;&#20840;&#38544;&#24739;&#24182;&#30830;&#20445;&#31995;&#32479;&#21442;&#25968;&#20272;&#35745;&#30340;&#25910;&#25947;&#24615;&#65292;&#20294;&#34987;&#35777;&#26126;&#21482;&#20250;&#26377;&#26377;&#38480;&#27425;&#35302;&#21457;&#65292;&#24182;&#23545;&#25511;&#21046;&#22120;&#30340;&#28176;&#36817;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#12290;&#36890;&#36807;&#22312;&#30000;&#32435;&#35199;&#20234;&#22763;&#26364;(Tennessee Eastman)&#24037;&#33402;&#20013;&#36827;&#34892;&#20223;&#30495;&#39564;&#35777;&#20102;&#35813;&#25511;&#21046;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Linear-Quadratic Regulation (LQR) problem with unknown system parameters has been widely studied, but it has remained unclear whether $\tilde{ \mathcal{O}}(\sqrt{T})$ regret, which is the best known dependence on time, can be achieved almost surely. In this paper, we propose an adaptive LQR controller with almost surely $\tilde{ \mathcal{O}}(\sqrt{T})$ regret upper bound. The controller features a circuit-breaking mechanism, which circumvents potential safety breach and guarantees the convergence of the system parameter estimate, but is shown to be triggered only finitely often and hence has negligible effect on the asymptotic performance of the controller. The proposed controller is also validated via simulation on Tennessee Eastman Process~(TEP), a commonly used industrial process example.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15136</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;2D&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collective Intelligence for 2D Push Manipulation with Mobile Robots. (arXiv:2211.15136v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#31995;&#32479;&#36890;&#24120;&#34920;&#29616;&#20986;&#33021;&#22815;&#33258;&#25105;&#32452;&#32455;&#21644;&#36866;&#24212;&#21464;&#21270;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#20154;&#24037;&#31995;&#32479;&#32570;&#20047;&#36825;&#31181;&#31561;&#25928;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#36827;&#34892;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#30340;&#38598;&#20307;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20174;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#27966;&#29983;&#30340;&#35268;&#21010;&#22120;&#25552;&#28860;&#20026;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#65292;&#25105;&#20204;&#30340;&#22810;&#26426;&#22120;&#20154;&#25512;&#21160;&#25805;&#20316;&#31995;&#32479;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#36866;&#24212;&#22806;&#37096;&#25200;&#21160;&#21644;&#29615;&#22659;&#21464;&#21270;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While natural systems often present collective intelligence that allows them to self-organize and adapt to changes, the equivalent is missing in most artificial systems. We explore the possibility of such a system in the context of cooperative 2D push manipulations using mobile robots. Although conventional works demonstrate potential solutions for the problem in restricted settings, they have computational and learning difficulties. More importantly, these systems do not possess the ability to adapt when facing environmental changes. In this work, we show that by distilling a planner derived from a differentiable soft-body physics simulator into an attention-based neural network, our multi-robot push manipulation system achieves better performance than baselines. In addition, our system also generalizes to configurations not seen during training and is able to adapt toward task completions when external turbulence and environmental changes are applied. Supplementary videos can be foun
&lt;/p&gt;</description></item><item><title>R2C-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#21644;COVID-19&#20998;&#31867;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#22270;&#20687;&#24674;&#22797;&#25552;&#39640;X&#23556;&#32447;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2209.14770</link><description>&lt;p&gt;
R2C-GAN: &#29992;&#20110;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#21644;COVID-19&#20998;&#31867;&#30340;&#24674;&#22797;&#21040;&#20998;&#31867;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
R2C-GAN: Restore-to-Classify GANs for Blind X-Ray Restoration and COVID-19 Classification. (arXiv:2209.14770v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14770
&lt;/p&gt;
&lt;p&gt;
R2C-GAN&#26159;&#19968;&#31181;&#29992;&#20110;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#21644;COVID-19&#20998;&#31867;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#36890;&#36807;&#22270;&#20687;&#24674;&#22797;&#25552;&#39640;X&#23556;&#32447;&#22270;&#20687;&#36136;&#37327;&#65292;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#38024;&#23545;&#30450;&#30446;X&#23556;&#32447;&#24674;&#22797;&#30340;&#32852;&#21512;&#27169;&#22411;&#65306;&#24674;&#22797;&#21040;&#20998;&#31867;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(R2C-GANs)&#12290;&#35813;&#27169;&#22411;&#22312;&#20445;&#25345;&#30142;&#30149;&#23436;&#25972;&#24615;&#30340;&#21516;&#26102;&#36827;&#34892;&#22270;&#20687;&#24674;&#22797;&#65292;&#20174;&#32780;&#25552;&#39640;X&#23556;&#32447;&#22270;&#20687;&#30340;&#36136;&#37327;&#24182;&#23454;&#29616;&#26356;&#39640;&#30340;&#35786;&#26029;&#24615;&#33021;&#12290;&#23558;&#24674;&#22797;&#20219;&#21153;&#23450;&#20041;&#20026;&#20174;&#36136;&#37327;&#36739;&#24046;&#21253;&#21547;&#26377;&#22122;&#22768;&#12289;&#27169;&#31946;&#25110;&#36807;/&#27424;&#26333;&#22270;&#29255;&#21040;&#39640;&#36136;&#37327;&#22270;&#20687;&#39046;&#22495;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#32763;&#35793;&#38382;&#39064;&#12290;R2C-GAN&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#20004;&#20010;&#39046;&#22495;&#20043;&#38388;&#30340;&#27491;&#21521;&#21644;&#21453;&#21521;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restoration of poor quality images with a blended set of artifacts plays a vital role for a reliable diagnosis. Existing studies have focused on specific restoration problems such as image deblurring, denoising, and exposure correction where there is usually a strong assumption on the artifact type and severity. As a pioneer study in blind X-ray restoration, we propose a joint model for generic image restoration and classification: Restore-to-Classify Generative Adversarial Networks (R2C-GANs). Such a jointly optimized model keeps any disease intact after the restoration. Therefore, this will naturally lead to a higher diagnosis performance thanks to the improved X-ray image quality. To accomplish this crucial objective, we define the restoration task as an Image-to-Image translation problem from poor quality having noisy, blurry, or over/under-exposed images to high quality image domain. The proposed R2C-GAN model is able to learn forward and inverse transforms between the two domains
&lt;/p&gt;</description></item></channel></rss>