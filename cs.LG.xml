<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15576</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Data-centric Prediction Explanation via Kernelized Stein Discrepancy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15576
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#30340;&#25968;&#25454;&#20013;&#24515;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#24322;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#22522;&#20110;&#31034;&#20363;&#30340;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#28508;&#22312;&#34920;&#31034;&#26469;&#36830;&#25509;&#27979;&#35797;&#21644;&#35757;&#32451;&#25968;&#25454;&#28857;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#20851;&#27169;&#22411;&#39044;&#27979;&#21407;&#22240;&#30340;&#32447;&#32034;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34920;&#29616;&#20986;&#22266;&#26377;&#30340;&#32570;&#38519;&#65292;&#27604;&#22914;&#20135;&#29983;&#26174;&#30528;&#30340;&#35745;&#31639;&#24320;&#38144;&#25110;&#29983;&#25104;&#31895;&#31890;&#24230;&#30340;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#31934;&#24230;&#21644;&#25968;&#25454;&#20013;&#24515;&#30340;&#35299;&#37322;&#65288;HD-Explain&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#20869;&#26680;&#21270;&#26031;&#22374;&#19981;&#30456;&#23481;&#24615;&#65288;KSD&#65289;&#23646;&#24615;&#30340;&#31616;&#21333;&#39044;&#27979;&#35299;&#37322;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;KSD&#21807;&#19968;&#22320;&#20026;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#23450;&#20041;&#20102;&#19968;&#20010;&#21442;&#25968;&#21270;&#30340;&#20869;&#26680;&#20989;&#25968;&#65292;&#29992;&#20110;&#32534;&#30721;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#25968;&#25454;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#20869;&#26680;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#25552;&#20379;&#26368;&#20339;&#39044;&#27979;&#25903;&#25345;&#32473;&#27979;&#35797;&#28857;&#30340;&#35757;&#32451;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20998;&#31867;&#39046;&#22495;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;HD-Explain&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15576v1 Announce Type: new  Abstract: Existing example-based prediction explanation methods often bridge test and training data points through the model's parameters or latent representations. While these methods offer clues to the causes of model predictions, they often exhibit innate shortcomings, such as incurring significant computational overhead or producing coarse-grained explanations. This paper presents a Highly-precise and Data-centric Explanation (HD-Explain), a straightforward prediction explanation method exploiting properties of Kernelized Stein Discrepancy (KSD). Specifically, the KSD uniquely defines a parameterized kernel function for a trained model that encodes model-dependent data correlation. By leveraging the kernel function, one can identify training samples that provide the best predictive support to a test point efficiently. We conducted thorough analyses and experiments across multiple classification domains, where we show that HD-Explain outperform
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;</title><link>https://arxiv.org/abs/2403.15309</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Controlled Training Data Generation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#25511;&#21046;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#19968;&#26041;&#38754;&#20351;&#29992;&#30417;&#30563;&#27169;&#22411;&#21453;&#39304;&#25214;&#21040;&#23545;&#25239;&#24615;&#25552;&#31034;&#35789;&#23454;&#29616;&#22270;&#20687;&#29983;&#25104;&#65292;&#21478;&#19968;&#26041;&#38754;&#36890;&#36807;&#24341;&#23548;&#20351;&#29983;&#25104;&#36807;&#31243;&#26397;&#21521;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20197;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#65292;&#19987;&#38376;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;&#12290;&#19982;&#20043;&#21069;&#37027;&#20123;&#37319;&#29992;&#24320;&#29615;&#26041;&#27861;&#24182;&#39044;&#20808;&#23450;&#20041;&#25552;&#31034;&#35789;&#26469;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#29983;&#25104;&#26032;&#25968;&#25454;&#30340;&#20316;&#21697;&#19981;&#21516;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#21160;&#38381;&#29615;&#31995;&#32479;&#65292;&#20854;&#20013;&#21253;&#25324;&#20004;&#20010;&#21453;&#39304;&#26426;&#21046;&#12290;&#31532;&#19968;&#20010;&#26426;&#21046;&#20351;&#29992;&#26469;&#33258;&#32473;&#23450;&#30417;&#30563;&#27169;&#22411;&#30340;&#21453;&#39304;&#65292;&#24182;&#25214;&#21040;&#23548;&#33268;&#22270;&#20687;&#29983;&#25104;&#26368;&#22823;&#21270;&#27169;&#22411;&#25439;&#22833;&#30340;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#34429;&#28982;&#36825;&#20123;&#23545;&#25239;&#25552;&#31034;&#35789;&#23548;&#33268;&#20102;&#32463;&#36807;&#27169;&#22411;&#35757;&#32451;&#30340;&#22810;&#26679;&#21270;&#25968;&#25454;&#29983;&#25104;&#65292;&#20294;&#23427;&#20204;&#24182;&#19981;&#30693;&#36947;&#30446;&#26631;&#20998;&#24067;&#65292;&#36825;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#31532;&#20108;&#20010;&#21453;&#39304;&#26426;&#21046;&#65292;&#23558;&#29983;&#25104;&#36807;&#31243;&#24341;&#23548;&#21040;&#29305;&#23450;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#31216;&#23558;&#36825;&#20004;&#20010;&#26426;&#21046;&#32467;&#21512;&#36215;&#26469;&#30340;&#26041;&#27861;&#20026;&#24341;&#23548;&#23545;&#25239;&#25552;&#31034;&#35789;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15309v1 Announce Type: cross  Abstract: In this work, we present a method to control a text-to-image generative model to produce training data specifically "useful" for supervised learning. Unlike previous works that employ an open-loop approach and pre-define prompts to generate new data using either a language model or human expertise, we develop an automated closed-loop system which involves two feedback mechanisms. The first mechanism uses feedback from a given supervised model and finds adversarial prompts that result in image generations that maximize the model loss. While these adversarial prompts result in diverse data informed by the model, they are not informed of the target distribution, which can be inefficient. Therefore, we introduce the second feedback mechanism that guides the generation process towards a certain target distribution. We call the method combining these two mechanisms Guided Adversarial Prompts. We perform our evaluations on different tasks, da
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;</title><link>https://arxiv.org/abs/2403.14092</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#23454;&#26102;&#20943;&#23569;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
Carbon Footprint Reduction for Sustainable Data Centers in Real-Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14092
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#26174;&#33879;&#22686;&#21152;&#33021;&#28304;&#28040;&#32791;&#65292;&#30899;&#25490;&#25918;&#20302;&#30340;&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#27491;&#25104;&#20026;&#20840;&#29699;&#25919;&#24220;&#21644;&#20225;&#19994;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#38656;&#35201;&#22312;&#20919;&#21364;&#21644;IT&#36127;&#36733;&#20013;&#36827;&#34892;&#21151;&#32791;&#20248;&#21270;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#22522;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#32593;&#20013;&#30340;&#21487;&#29992;&#24615;&#26469;&#35843;&#25972;&#28789;&#27963;&#36127;&#36733;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#19981;&#38388;&#26029;&#30005;&#28304;&#20013;&#30340;&#30005;&#27744;&#23384;&#20648;&#65292;&#20351;&#29992;&#21327;&#20316;&#20195;&#29702;&#12290;&#36825;&#20123;&#20248;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#23545;&#21464;&#21270;&#30340;&#22806;&#37096;&#22240;&#32032;&#65288;&#22914;&#22825;&#27668;&#21644;&#30005;&#32593;&#30899;&#25490;&#25918;&#24378;&#24230;&#65289;&#30340;&#20381;&#36182;&#20351;&#24471;&#36825;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#22312;&#21160;&#24577;&#23454;&#38469;&#29615;&#22659;&#20013;&#21516;&#26102;&#20248;&#21270;&#25152;&#26377;&#36825;&#20123;&#30446;&#26631;&#30340;&#23454;&#26102;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30899;&#36275;&#36857;&#20943;&#23569;&#65288;DC-CFR&#65289;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#35282;&#24230;&#30340;&#25968;&#25454;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14092v1 Announce Type: cross  Abstract: As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.13797</link><description>&lt;p&gt;
&#24357;&#21512;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36873;&#25321;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#21644;&#33021;&#21147;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridge the Modality and Capacity Gaps in Vision-Language Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13797
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22312;&#35821;&#35328;-Only VLM&#36873;&#25321;&#20013;&#30340;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#21644;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#65292;&#24182;&#25552;&#20986;&#20102;VLM&#36873;&#25321;&#20013;&#24357;&#21512;&#36825;&#20004;&#20010;&#24046;&#36317;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#36890;&#36807;&#23558;&#22270;&#20687;&#19982;&#25991;&#26412;&#31867;&#21035;&#21517;&#31216;&#37197;&#23545;&#65292;&#22312;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#39044;&#35757;&#32451;&#30340;VLMs&#30340;&#19981;&#26029;&#22686;&#21152;&#20351;&#24471;&#29305;&#23450;&#20219;&#21153;&#30340;VLM&#36873;&#25321;&#26356;&#26377;&#21487;&#33021;&#26631;&#35782;&#20986;&#36866;&#21512;&#30340;VLM&#12290;&#22240;&#27492;&#65292;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#31574;&#30053;&#26159;&#20174;VLM&#21160;&#29289;&#22253;&#20013;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#39044;&#35757;&#32451;VLM&#65292;&#20165;&#20381;&#36182;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25991;&#26412;&#25968;&#25454;&#32780;&#26080;&#38656;&#35775;&#38382;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#31181;&#20165;&#35821;&#35328;VLM&#36873;&#25321;&#20013;&#20004;&#20010;&#22266;&#26377;&#25361;&#25112;&#65306;&#12300;&#27169;&#24577;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#22312;&#20004;&#20010;&#19981;&#21516;&#27169;&#24577;&#19979;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20351;&#24471;&#25991;&#26412;&#25104;&#20026;&#22270;&#20687;&#30340;&#19968;&#20010;&#19981;&#22826;&#21487;&#38752;&#30340;&#26367;&#20195;&#21697;&#65307;&#12300;&#33021;&#21147;&#24046;&#36317;&#12301;&#8212;&#8212;VLM&#30340;&#25972;&#20307;&#25490;&#21517;&#19982;&#20854;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#30340;&#25490;&#21517;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#65292;&#38459;&#30861;&#20102;&#30452;&#25509;&#20174;&#27169;&#22411;&#30340;&#25972;&#20307;&#34920;&#29616;&#26469;&#39044;&#27979;&#20854;&#25968;&#25454;&#38598;&#29305;&#23450;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;VLM&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13797v1 Announce Type: new  Abstract: Vision Language Models (VLMs) excel in zero-shot image classification by pairing images with textual category names. The expanding variety of Pre-Trained VLMs enhances the likelihood of identifying a suitable VLM for specific tasks. Thus, a promising zero-shot image classification strategy is selecting the most appropriate Pre-Trained VLM from the VLM Zoo, relying solely on the text data of the target dataset without access to the dataset's images. In this paper, we analyze two inherent challenges in assessing the ability of a VLM in this Language-Only VLM selection: the "Modality Gap" -- the disparity in VLM's embeddings across two different modalities, making text a less reliable substitute for images; and the "Capability Gap" -- the discrepancy between the VLM's overall ranking and its ranking for target dataset, hindering direct prediction of a model's dataset-specific performance from its general performance. We propose VLM Selectio
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;</title><link>https://arxiv.org/abs/2403.08946</link><description>&lt;p&gt;
&#21487;&#29992;&#30340;XAI&#65306;&#22312;LLM&#26102;&#20195;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#30340;10&#20010;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Usable XAI: 10 Strategies Towards Exploiting Explainability in the LLM Era
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08946
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#65292;&#20026;&#20102;&#36866;&#24212;&#20854;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#29992;&#30340;XAI&#27010;&#24565;&#65292;&#36890;&#36807;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#65292;&#23454;&#29616;XAI&#26041;&#27861;&#35770;&#30340;&#37325;&#22823;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25351;&#30340;&#26159;&#25552;&#20379;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27934;&#35265;&#65292;&#25581;&#31034;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#36816;&#20316;&#26041;&#24335;&#30340;&#25216;&#26415;&#12290;&#26368;&#36817;&#65292;XAI&#30340;&#37325;&#28857;&#27491;&#34987;&#25193;&#23637;&#21040;&#24120;&#24120;&#22240;&#20026;&#19981;&#36879;&#26126;&#32780;&#22791;&#21463;&#25209;&#35780;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#36825;&#19968;&#25299;&#23637;&#38656;&#35201;&#23545;XAI&#26041;&#27861;&#35770;&#36827;&#34892;&#26174;&#33879;&#36716;&#21464;&#65292;&#22240;&#20026;&#26377;&#20004;&#20010;&#21407;&#22240;&#12290;&#39318;&#20808;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;XAI&#26041;&#27861;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;LLMs&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#21644;&#20808;&#36827;&#33021;&#21147;&#12290;&#20854;&#27425;&#65292;&#38543;&#30528;LLMs&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#19981;&#21516;&#34892;&#19994;&#24212;&#29992;&#20013;&#65292;XAI&#30340;&#35282;&#33394;&#20174;&#20165;&#20165;&#25171;&#24320;&#8220;&#40657;&#21283;&#23376;&#8221;&#36716;&#21464;&#20026;&#31215;&#26497;&#22686;&#24378;LLMs&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#29983;&#20135;&#21147;&#21644;&#36866;&#29992;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20165;&#20316;&#20026;XAI&#27934;&#35265;&#30340;&#34987;&#21160;&#25509;&#21463;&#32773;&#65292;LLMs&#30340;&#29420;&#29305;&#33021;&#21147;&#33021;&#22815;&#30456;&#20114;&#22686;&#24378;XAI&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#65288;1&#65289;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08946v1 Announce Type: cross  Abstract: Explainable AI (XAI) refers to techniques that provide human-understandable insights into the workings of AI models. Recently, the focus of XAI is being extended towards Large Language Models (LLMs) which are often criticized for their lack of transparency. This extension calls for a significant transformation in XAI methodologies because of two reasons. First, many existing XAI methods cannot be directly applied to LLMs due to their complexity advanced capabilities. Second, as LLMs are increasingly deployed across diverse industry applications, the role of XAI shifts from merely opening the "black box" to actively enhancing the productivity and applicability of LLMs in real-world settings. Meanwhile, unlike traditional machine learning models that are passive recipients of XAI insights, the distinct abilities of LLMs can reciprocally enhance XAI. Therefore, in this paper, we introduce Usable XAI in the context of LLMs by analyzing (1)
&lt;/p&gt;</description></item><item><title>sDBSCAN&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#36827;&#34892;&#39640;&#32500;&#23494;&#24230;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15679</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#30340;&#21487;&#25193;&#23637;&#23494;&#24230;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Scalable Density-based Clustering with Random Projections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15679
&lt;/p&gt;
&lt;p&gt;
sDBSCAN&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#36827;&#34892;&#39640;&#32500;&#23494;&#24230;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#22312;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#19978;&#26174;&#33879;&#20248;&#20110;&#20854;&#20182;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;sDBSCAN&#30340;&#31639;&#27861;&#65292;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#20351;&#29992;&#20313;&#24358;&#36317;&#31163;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#23494;&#24230;&#32858;&#31867;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#25237;&#24433;&#30340;&#20445;&#37051;&#29305;&#24615;&#65292;sDBSCAN&#33021;&#22815;&#24555;&#36895;&#35782;&#21035;&#26680;&#24515;&#28857;&#21450;&#20854;&#37051;&#22495;&#65292;&#36825;&#26159;&#23494;&#24230;&#32858;&#31867;&#30340;&#20027;&#35201;&#38556;&#30861;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;sDBSCAN&#22312;&#36739;&#36731;&#30340;&#26465;&#20214;&#19979;&#20197;&#39640;&#27010;&#29575;&#36755;&#20986;&#31867;&#20284;&#20110;DBSCAN&#30340;&#32858;&#31867;&#32467;&#26500;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#20419;&#36827;sDBSCAN&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;sOPTICS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#20114;&#24335;&#25506;&#32034;&#20869;&#22312;&#32858;&#31867;&#32467;&#26500;&#30340;&#21487;&#25193;&#23637;OPTICS&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#38543;&#26426;&#26680;&#29305;&#24449;&#23558;sDBSCAN&#21644;sOPTICS&#25193;&#23637;&#21040;L2&#12289;L1&#12289;$\chi^2$&#21644;Jensen-Shannon&#36317;&#31163;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;sDBSCAN&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#19978;&#27604;&#35768;&#22810;&#20854;&#20182;&#32858;&#31867;&#31639;&#27861;&#26174;&#33879;&#26356;&#24555;&#65292;&#24182;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#65292;sDBSCAN&#21644;sOPTICS&#22312;&#20960;&#20998;&#38047;&#20869;&#36816;&#34892;&#65292;&#32780;scikit-learn&#30340;&#23545;&#24212;&#31639;&#27861;&#38656;&#35201;&#25968;&#23567;&#26102;&#25110;&#30001;&#20110;&#20869;&#23384;&#19981;&#36275;&#32780;&#26080;&#27861;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15679v1 Announce Type: new  Abstract: We present sDBSCAN, a scalable density-based clustering algorithm in high dimensions with cosine distance. Utilizing the neighborhood-preserving property of random projections, sDBSCAN can quickly identify core points and their neighborhoods, the primary hurdle of density-based clustering. Theoretically, sDBSCAN outputs a clustering structure similar to DBSCAN under mild conditions with high probability. To further facilitate sDBSCAN, we present sOPTICS, a scalable OPTICS for interactive exploration of the intrinsic clustering structure. We also extend sDBSCAN and sOPTICS to L2, L1, $\chi^2$, and Jensen-Shannon distances via random kernel features. Empirically, sDBSCAN is significantly faster and provides higher accuracy than many other clustering algorithms on real-world million-point data sets. On these data sets, sDBSCAN and sOPTICS run in a few minutes, while the scikit-learn's counterparts demand several hours or cannot run due to m
&lt;/p&gt;</description></item><item><title>&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.12819</link><description>&lt;p&gt;
&#24494;&#35843;&#12289;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25351;&#23548;&#24494;&#35843;&#65306;&#25105;&#20204;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#65311;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning, Prompting, In-Context Learning and Instruction-Tuning: How Many Labelled Samples Do We Need?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12819
&lt;/p&gt;
&lt;p&gt;
&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26631;&#35760;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#65292;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#20219;&#21153;&#26102;&#65292;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#36873;&#25321;&#20351;&#29992;&#36890;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#32780;&#19981;&#36827;&#34892;&#36827;&#19968;&#27493;&#26356;&#26032;&#65292;&#25110;&#32773;&#20351;&#29992;&#23569;&#37327;&#31034;&#20363;&#26469;&#35843;&#25972;&#19987;&#38376;&#30340;&#36739;&#23567;&#27169;&#22411;&#12290; &#24403;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#21487;&#29992;&#26102;&#65292;&#19987;&#38376;&#30340;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#36890;&#29992;&#27169;&#22411;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#35843;&#26597;&#19987;&#38376;&#27169;&#22411;&#38656;&#35201;&#22810;&#23569;&#26631;&#35760;&#26679;&#26412;&#25165;&#33021;&#23454;&#29616;&#36825;&#31181;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#32771;&#34385;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#35266;&#23519;&#25552;&#31034;&#12289;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#24494;&#35843;&#21644;&#25351;&#23548;&#24494;&#35843;&#30340;&#34892;&#20026;&#65292;&#35782;&#21035;&#23427;&#20204;&#22312;&#22686;&#21152;&#19981;&#21516;&#22797;&#26434;&#24615;&#20219;&#21153;&#30340;&#26631;&#35760;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#26102;&#30340;&#25910;&#25903;&#24179;&#34913;&#28857;&#65292;&#25105;&#20204;&#21457;&#29616;&#19987;&#38376;&#27169;&#22411;&#36890;&#24120;&#21482;&#38656;&#23569;&#37327;&#26679;&#26412;&#65288;100-1000&#20010;&#65289;&#23601;&#33021;&#19982;&#36890;&#29992;&#27169;&#22411;&#25345;&#24179;&#29978;&#33267;&#26356;&#22909;&#12290; &#21516;&#26102;&#65292;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#25454;&#37327;&#24378;&#28872;&#20381;&#36182;&#20110;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12819v1 Announce Type: cross  Abstract: When solving a task with limited labelled data, researchers can either use a general large language model without further update, or use the few examples to tune a specialised smaller model. When enough labels are available, the specialised models outperform the general ones on many NLP tasks. In this work, we aim to investigate how many labelled samples are required for the specialised models to achieve this superior performance, while taking the results variance into consideration. Observing the behaviour of prompting, in-context learning, fine-tuning and instruction-tuning, identifying their break-even points when increasing number of labelled training samples across three tasks of varying complexity, we find that the specialised models often need only few samples ($100-1000$) to be on par or better than the general ones. At the same time, the amount of required labelled data strongly depends on the task complexity and results varia
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#21253;&#25324;&#26641;&#32467;&#26500;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#35805;&#30340;&#30495;&#30456;&#12290;</title><link>https://arxiv.org/abs/2402.07281</link><description>&lt;p&gt;
&#12298;&#22522;&#20110;&#26641;&#32467;&#26500;&#26041;&#27861;&#30340;&#24322;&#24120;&#26816;&#27979;&#33021;&#21542;&#36229;&#36234;&#28145;&#24230;&#23398;&#20064;&#65311;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#12299;
&lt;/p&gt;
&lt;p&gt;
Can Tree Based Approaches Surpass Deep Learning in Anomaly Detection? A Benchmarking Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07281
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#19968;&#39033;&#22522;&#20934;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#21253;&#25324;&#26641;&#32467;&#26500;&#26041;&#27861;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#35805;&#30340;&#30495;&#30456;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30830;&#20445;&#26381;&#21153;&#36830;&#32493;&#24615;&#26102;&#65292;&#22797;&#26434;&#30340;&#20851;&#38190;&#20219;&#21153;&#31995;&#32479;&#20013;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#24322;&#24120;&#20107;&#20214;&#34987;&#35748;&#20026;&#26159;&#32597;&#35265;&#20107;&#20214;&#65292;&#22240;&#27492;&#20174;&#25805;&#20316;&#25968;&#25454;&#20013;&#26816;&#27979;&#24322;&#24120;&#24773;&#20917;&#38754;&#20020;&#30528;&#31867;&#21035;&#20998;&#24067;&#19981;&#24179;&#34913;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#20840;&#38754;&#30340;&#22522;&#20934;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#12290;&#35770;&#25991;&#36890;&#36807;&#23545;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#20844;&#27491;&#27604;&#36739;&#20570;&#20986;&#20102;&#37325;&#22823;&#36129;&#29486;&#65292;&#21253;&#25324;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#21508;&#31181;&#22522;&#20110;&#26641;&#32467;&#26500;&#30340;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#24322;&#24120;&#28857;&#26816;&#27979;&#26041;&#27861;&#12290;&#35770;&#25991;&#20351;&#29992;&#20102;104&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#21644;&#23569;&#25968;&#19987;&#26377;&#30340;&#24037;&#19994;&#31995;&#32479;&#25968;&#25454;&#38598;&#65292;&#22686;&#24378;&#20102;&#30740;&#31350;&#30340;&#22810;&#26679;&#24615;&#65292;&#20351;&#31639;&#27861;&#24615;&#33021;&#30340;&#35780;&#20272;&#26356;&#21152;&#30495;&#23454;&#65292;&#24182;&#24378;&#35843;&#20102;&#23545;&#23454;&#38469;&#22330;&#26223;&#30340;&#36866;&#24212;&#24615;&#30340;&#37325;&#35201;&#24615;&#12290;&#35770;&#25991;&#25581;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#31070;&#35805;&#30340;&#30495;&#30456;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detection of anomalous situations for complex mission-critical systems holds paramount importance when their service continuity needs to be ensured. A major challenge in detecting anomalies from the operational data arises due to the imbalanced class distribution problem since the anomalies are supposed to be rare events. This paper evaluates a diverse array of machine learning-based anomaly detection algorithms through a comprehensive benchmark study. The paper contributes significantly by conducting an unbiased comparison of various anomaly detection algorithms, spanning classical machine learning including various tree-based approaches to deep learning and outlier detection methods. The inclusion of 104 publicly available and a few proprietary industrial systems datasets enhances the diversity of the study, allowing for a more realistic evaluation of algorithm performance and emphasizing the importance of adaptability to real-world scenarios. The paper dispels the deep learning myth
&lt;/p&gt;</description></item><item><title>SVQ&#26159;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#22238;&#24402;&#23454;&#29616;&#31616;&#26126;&#34920;&#31034;&#30340;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20445;&#30041;&#20851;&#38190;&#32454;&#33410;&#21644;&#28388;&#38500;&#22122;&#22768;&#26469;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SVQ&#22312;&#20116;&#20010;&#31354;&#38388;-&#26102;&#38388;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.03406</link><description>&lt;p&gt;
SVQ: &#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#29992;&#20110;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SVQ: Sparse Vector Quantization for Spatiotemporal Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03406
&lt;/p&gt;
&lt;p&gt;
SVQ&#26159;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#22238;&#24402;&#23454;&#29616;&#31616;&#26126;&#34920;&#31034;&#30340;&#31232;&#30095;&#21521;&#37327;&#37327;&#21270;&#25216;&#26415;&#65292;&#36890;&#36807;&#20445;&#30041;&#20851;&#38190;&#32454;&#33410;&#21644;&#28388;&#38500;&#22122;&#22768;&#26469;&#25552;&#39640;&#26102;&#31354;&#39044;&#27979;&#24615;&#33021;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;SVQ&#22312;&#20116;&#20010;&#31354;&#38388;-&#26102;&#38388;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#39044;&#27979;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#20851;&#38190;&#65292;&#21462;&#24471;&#22909;&#30340;&#39044;&#27979;&#32467;&#26524;&#38656;&#35201;&#25214;&#21040;&#24494;&#22937;&#30340;&#27169;&#24335;&#24182;&#28388;&#38500;&#22122;&#22768;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#31232;&#30095;&#22238;&#24402;&#21521;&#37327;&#37327;&#21270;&#65288;SVQ&#65289;&#36825;&#19968;&#26032;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#31232;&#30095;&#22238;&#24402;&#26469;&#23454;&#29616;&#31616;&#26126;&#34920;&#31034;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#29702;&#35770;&#21644;&#23454;&#36341;&#19978;&#37117;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#32858;&#31867;&#30340;&#21521;&#37327;&#37327;&#21270;&#26041;&#27861;&#26356;&#26377;&#20248;&#21183;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#29992;&#22238;&#24402;&#27169;&#22411;&#20445;&#30041;&#21407;&#22987;&#21521;&#37327;&#30340;&#20851;&#38190;&#32454;&#33410;&#65292;&#21516;&#26102;&#36890;&#36807;&#31232;&#30095;&#35774;&#35745;&#28388;&#38500;&#22122;&#22768;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#23618;MLP&#21644;&#19968;&#20010;&#24191;&#27867;&#30340;&#30721;&#26412;&#26469;&#36817;&#20284;&#31232;&#30095;&#22238;&#24402;&#36807;&#31243;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#22823;&#22823;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#36824;&#20351;&#24471;SVQ&#20855;&#26377;&#21487;&#24494;&#24615;&#21644;&#35757;&#32451;&#31616;&#26131;&#24615;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#20116;&#20010;&#31354;&#38388;-&#26102;&#38388;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;SVQ&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;
&lt;/p&gt;
&lt;p&gt;
Spatio-temporal forecasting, pivotal in numerous fields, hinges on the delicate equilibrium between isolating nuanced patterns and sifting out noise. To tackle this, we introduce Sparse Regression-based Vector Quantization (SVQ), a novel technique that leverages sparse regression for succinct representation, an approach theoretically and practically favored over classical clustering-based vector quantization methods. This approach preserves critical details from the original vectors using a regression model while filtering out noise via sparse design. Moreover, we approximate the sparse regression process using a blend of a two-layer MLP and an extensive codebook. This approach not only substantially cuts down on computational costs but also grants SVQ differentiability and training simplicity, resulting in a notable enhancement of performance. Our empirical studies on five spatial-temporal benchmark datasets demonstrate that SVQ achieves state-of-the-art results. Specifically, on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;</title><link>http://arxiv.org/abs/2401.05502</link><description>&lt;p&gt;
&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#65306;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#36817;&#20284;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Diversity-aware clustering: Computational Complexity and Approximation Algorithms. (arXiv:2401.05502v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#22312;&#36873;&#25321;&#32858;&#31867;&#20013;&#24515;&#26102;&#35201;&#32771;&#34385;&#22810;&#20010;&#23646;&#24615;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#19981;&#21516;&#32858;&#31867;&#30446;&#26631;&#30340;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#20445;&#35777;&#32858;&#31867;&#36136;&#37327;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#32039;&#30830;&#30340;&#36817;&#20284;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#26679;&#24615;&#24863;&#30693;&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#19982;&#22810;&#20010;&#23646;&#24615;&#30456;&#20851;&#32852;&#65292;&#24418;&#25104;&#20132;&#21449;&#30340;&#32452;&#12290;&#32858;&#31867;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#30830;&#20445;&#20174;&#27599;&#20010;&#32452;&#20013;&#36873;&#25321;&#26368;&#23569;&#25968;&#37327;&#30340;&#32858;&#31867;&#20013;&#24515;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#32858;&#31867;&#30446;&#26631;&#65292;&#21487;&#20197;&#26159;$k$-&#20013;&#20301;&#25968;&#65292;$k$-&#22343;&#20540;&#25110;$k$-&#20379;&#24212;&#21830;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#65292;$1+\frac{8}{e}$&#21644;$3$&#65292;&#29992;&#20110;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20013;&#20301;&#25968;&#65292;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#22343;&#20540;&#21644;&#22810;&#26679;&#24615;&#24863;&#30693;$k$-&#20379;&#24212;&#21830;&#12290;&#36825;&#20123;&#36817;&#20284;&#27604;&#22312;&#20551;&#35774;Gap-ETH&#21644;FPT $\neq$ W[2]&#30340;&#24773;&#20917;&#19979;&#26159;&#32039;&#30830;&#30340;&#12290;&#23545;&#20110;&#20844;&#24179;$k$-&#20013;&#20301;&#25968;&#21644;&#20844;&#24179;$k$-&#22343;&#20540;&#30340;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21442;&#25968;&#21270;&#36817;&#20284;&#31639;&#27861;&#65292;&#36817;&#20284;&#27604;&#20998;&#21035;&#20026;$1+\frac{2}{e}$&#21644;$1+\frac{8}{e}$&#12290;&#23545;&#20110;&#20855;&#26377;&#19981;&#30456;&#20132;&#24037;&#21378;&#32452;&#30340;&#20844;&#24179;$k$-&#20379;&#24212;&#21830;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#36817;&#20284;&#31639;&#27861;&#65292;&#22240;&#23376;&#20026;$3$&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we study diversity-aware clustering problems where the data points are associated with multiple attributes resulting in intersecting groups. A clustering solution need to ensure that a minimum number of cluster centers are chosen from each group while simultaneously minimizing the clustering objective, which can be either $k$-median, $k$-means or $k$-supplier. We present parameterized approximation algorithms with approximation ratios $1+ \frac{2}{e}$, $1+\frac{8}{e}$ and $3$ for diversity-aware $k$-median, diversity-aware $k$-means and diversity-aware $k$-supplier, respectively. The approximation ratios are tight assuming Gap-ETH and FPT $\neq$ W[2]. For fair $k$-median and fair $k$-means with disjoint faicility groups, we present parameterized approximation algorithm with approximation ratios $1+\frac{2}{e}$ and $1+\frac{8}{e}$, respectively. For fair $k$-supplier with disjoint facility groups, we present a polynomial-time approximation algorithm with factor $3$, improv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CEFL&#30340;&#30899;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#25104;&#26412;&#24863;&#30693;&#31574;&#30053;&#26469;&#20248;&#21270;FL&#27169;&#22411;&#35757;&#32451;&#30340;&#20219;&#24847;&#25104;&#26412;&#24230;&#37327;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#30899;&#25490;&#25918;&#20943;&#23569;93&#65285;&#21644;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;50&#65285;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.17972</link><description>&lt;p&gt;
CEFL&#65306;&#30899;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CEFL: Carbon-Efficient Federated Learning. (arXiv:2310.17972v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17972
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;CEFL&#30340;&#30899;&#39640;&#25928;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#25104;&#26412;&#24863;&#30693;&#31574;&#30053;&#26469;&#20248;&#21270;FL&#27169;&#22411;&#35757;&#32451;&#30340;&#20219;&#24847;&#25104;&#26412;&#24230;&#37327;&#65292;&#24182;&#25104;&#21151;&#23454;&#29616;&#20102;&#30899;&#25490;&#25918;&#20943;&#23569;93&#65285;&#21644;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;50&#65285;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#23558;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35757;&#32451;&#20998;&#24067;&#22312;&#35768;&#22810;&#36793;&#32536;&#35774;&#22791;&#19978;&#65292;&#20197;&#20943;&#23569;&#25968;&#25454;&#20256;&#36755;&#24320;&#38144;&#21644;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#12290;&#30001;&#20110;FL&#27169;&#22411;&#35757;&#32451;&#21487;&#33021;&#28041;&#21450;&#25968;&#30334;&#19975;&#20010;&#35774;&#22791;&#65292;&#22240;&#27492;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#65292;&#22240;&#27492;&#20043;&#21069;&#30340;&#24037;&#20316;&#19968;&#30452;&#33268;&#21147;&#20110;&#25552;&#39640;&#20854;&#36164;&#28304;&#25928;&#29575;&#20197;&#20248;&#21270;&#26102;&#38388;&#33267;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#23558;&#25152;&#26377;&#36164;&#28304;&#35270;&#20026;&#30456;&#21516;&#65292;&#32780;&#23454;&#38469;&#19978;&#23427;&#20204;&#21487;&#33021;&#20135;&#29983;&#22823;&#19981;&#30456;&#21516;&#30340;&#25104;&#26412;&#65292;&#36825;&#21453;&#32780;&#28608;&#21457;&#20102;&#20248;&#21270;&#25104;&#26412;&#33267;&#20934;&#30830;&#24615;&#30340;&#21160;&#26426;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;CEFL&#65292;&#23427;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#25104;&#26412;&#24863;&#30693;&#23458;&#25143;&#36873;&#25321;&#31574;&#30053;&#65292;&#22312;&#35757;&#32451;FL&#27169;&#22411;&#26102;&#20248;&#21270;&#20219;&#24847;&#25104;&#26412;&#24230;&#37327;&#12290;&#25105;&#20204;&#30340;&#31574;&#30053;&#25193;&#23637;&#24182;&#32467;&#21512;&#20102;&#22522;&#20110;&#25928;&#29992;&#30340;&#23458;&#25143;&#36873;&#25321;&#21644;&#20851;&#38190;&#23398;&#20064;&#26399;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#20351;&#20854;&#20855;&#26377;&#25104;&#26412;&#24863;&#30693;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#35774;&#35745;&#30899;&#39640;&#25928;&#30340;FL&#26469;&#28436;&#31034;CEFL&#65292;&#22312;&#36825;&#37324;&#33021;&#28304;&#30340;&#30899;&#24378;&#24230;&#26159;&#25104;&#26412;&#65292;&#24182;&#19988;&#26174;&#31034;&#23427;&#21487;&#20197;&#23558;&#30899;&#25490;&#25918;&#20943;&#23569;93&#65285;&#65292;&#24182;&#23558;&#35757;&#32451;&#26102;&#38388;&#20943;&#23569;50&#65285;&#65292;&#19982;&#38543;&#26426;&#23458;&#25143;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) distributes machine learning (ML) training across many edge devices to reduce data transfer overhead and protect data privacy. Since FL model training may span millions of devices and is thus resource-intensive, prior work has focused on improving its resource efficiency to optimize time-to-accuracy. However, prior work generally treats all resources the same, while, in practice, they may incur widely different costs, which instead motivates optimizing cost-to-accuracy. To address the problem, we design CEFL, which uses adaptive cost-aware client selection policies to optimize an arbitrary cost metric when training FL models. Our policies extend and combine prior work on utility-based client selection and critical learning periods by making them cost-aware. We demonstrate CEFL by designing carbon-efficient FL, where energy's carbon-intensity is the cost, and show that it i) reduces carbon emissions by 93\% and reduces training time by 50% compared to random clie
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#31574;&#30053;&#36827;&#34892;&#20915;&#31574;&#25351;&#23548;&#30340;&#32534;&#25490;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35774;&#32622;&#20013;&#30340;&#21518;&#24724;&#36793;&#30028;&#32467;&#26524;&#36716;&#31227;&#21040;&#34920;&#26684;&#35774;&#32622;&#19979;&#30340;&#32534;&#25490;&#20013;&#65292;&#25512;&#24191;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#27934;&#23519;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#20854;&#36879;&#26126;&#30340;&#35777;&#26126;&#12290;&#22312;&#38543;&#26426;&#21305;&#37197;&#29609;&#20855;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;</title><link>http://arxiv.org/abs/2310.16473</link><description>&lt;p&gt;
&#19987;&#23478;&#30340;&#20132;&#21709;&#26354;&#65306;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36816;&#29992;&#23545;&#25239;&#24615;&#27934;&#23519;&#21147;&#30340;&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Symphony of experts: orchestration with adversarial insights in reinforcement learning. (arXiv:2310.16473v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16473
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#19987;&#23478;&#31574;&#30053;&#36827;&#34892;&#20915;&#31574;&#25351;&#23548;&#30340;&#32534;&#25490;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#23545;&#25239;&#24615;&#35774;&#32622;&#20013;&#30340;&#21518;&#24724;&#36793;&#30028;&#32467;&#26524;&#36716;&#31227;&#21040;&#34920;&#26684;&#35774;&#32622;&#19979;&#30340;&#32534;&#25490;&#20013;&#65292;&#25512;&#24191;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#27934;&#23519;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#20854;&#36879;&#26126;&#30340;&#35777;&#26126;&#12290;&#22312;&#38543;&#26426;&#21305;&#37197;&#29609;&#20855;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#24378;&#21270;&#23398;&#20064;&#21033;&#29992;&#20855;&#26377;&#20248;&#21183;&#29305;&#24615;&#30340;&#31574;&#30053;&#20197;&#36798;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#25506;&#32034;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#32534;&#25490;&#30340;&#27010;&#24565;&#26469;&#25506;&#32034;&#36825;&#19968;&#39046;&#22495;&#65292;&#20854;&#20013;&#19968;&#32452;&#65288;&#23569;&#37327;&#65289;&#19987;&#23478;&#31574;&#30053;&#25351;&#23548;&#20915;&#31574;&#65307;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#24314;&#31435;&#20102;&#27492;&#24314;&#27169;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#23545;&#25239;&#24615;&#35774;&#32622;&#20013;&#36716;&#31227;&#21518;&#24724;&#36793;&#30028;&#32467;&#26524;&#65292;&#22312;&#34920;&#26684;&#35774;&#32622;&#19979;&#24314;&#31435;&#20102;&#32534;&#25490;&#30340;&#20215;&#20540;&#20989;&#25968;&#21518;&#24724;&#36793;&#30028;&#12290;&#25105;&#20204;&#23558;&#23545; Agarwal &#31561;&#20154; [2021, &#31532;5.3&#33410;] &#20013;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#20998;&#26512;&#25512;&#24191;&#24182;&#25193;&#23637;&#21040;&#20219;&#24847;&#23545;&#25239;&#24615;&#32858;&#21512;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#23558;&#20854;&#25193;&#23637;&#21040;&#20272;&#35745;&#20248;&#21183;&#20989;&#25968;&#30340;&#24773;&#20917;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#26399;&#26395;&#20540;&#21644;&#39640;&#27010;&#29575;&#19979;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#27934;&#23519;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#28857;&#22312;&#20110;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#35328;&#35777;&#26126;&#36739;&#20026;&#36879;&#26126;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#38024;&#23545;&#38543;&#26426;&#21305;&#37197;&#29609;&#20855;&#27169;&#22411;&#36827;&#34892;&#20102;&#27169;&#25311;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured reinforcement learning leverages policies with advantageous properties to reach better performance, particularly in scenarios where exploration poses challenges. We explore this field through the concept of orchestration, where a (small) set of expert policies guides decision-making; the modeling thereof constitutes our first contribution. We then establish value-functions regret bounds for orchestration in the tabular setting by transferring regret-bound results from adversarial settings. We generalize and extend the analysis of natural policy gradient in Agarwal et al. [2021, Section 5.3] to arbitrary adversarial aggregation strategies. We also extend it to the case of estimated advantage functions, providing insights into sample complexity both in expectation and high probability. A key point of our approach lies in its arguably more transparent proofs compared to existing methods. Finally, we present simulations for a stochastic matching toy model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.10378</link><description>&lt;p&gt;
&#36328;&#35821;&#35328;&#22810;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models. (arXiv:2310.10378v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#35821;&#35328;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#27169;&#22411;&#22823;&#23567;&#12289;&#35821;&#35328;&#37197;&#23545;&#31561;&#22240;&#32032;&#21457;&#29616;&#20102;&#24433;&#21709;&#19968;&#33268;&#24615;&#30340;&#22240;&#32032;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#20250;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#35821;&#35328;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#26174;&#31034;&#23384;&#20648;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#30693;&#35782;&#65292;&#20294;&#22312;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#30830;&#20445;&#19981;&#21516;&#35821;&#35328;&#32972;&#26223;&#30340;&#29992;&#25143;&#20174;&#21516;&#19968;&#20010;&#27169;&#22411;&#20013;&#33719;&#24471;&#19968;&#33268;&#30340;&#21453;&#39304;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21508;&#31181;&#22810;&#35821;&#35328;PLM&#20013;&#20107;&#23454;&#30693;&#35782;&#30340;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#65288;CLC&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#19968;&#33268;&#24615;&#65288;RankC&#65289;&#24230;&#37327;&#65292;&#29992;&#20110;&#29420;&#31435;&#20110;&#20934;&#30830;&#24615;&#35780;&#20272;&#36328;&#35821;&#35328;&#38388;&#30340;&#30693;&#35782;&#19968;&#33268;&#24615;&#12290;&#21033;&#29992;&#36825;&#20010;&#24230;&#37327;&#26041;&#27861;&#65292;&#25105;&#20204;&#23545;&#20915;&#23450;CLC&#30340;&#22240;&#32032;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#21253;&#25324;&#27169;&#22411;&#23618;&#38754;&#21644;&#35821;&#35328;&#23545;&#23618;&#38754;&#12290;&#22312;&#20854;&#20182;&#32467;&#26524;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21487;&#20197;&#25552;&#39640;&#22823;&#22810;&#25968;&#35821;&#35328;&#20013;&#30340;&#20107;&#23454;&#25506;&#27979;&#20934;&#30830;&#24615;&#65292;&#20294;&#19981;&#33021;&#25913;&#21892;&#36328;&#35821;&#35328;&#19968;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#22411;&#32534;&#36753;&#22312;PLMs&#20013;&#25554;&#20837;&#26032;&#30340;&#20107;&#23454;&#20851;&#32852;&#36827;&#34892;&#20102;&#19968;&#20010;CLC&#30340;&#26696;&#20363;&#30740;&#31350;&#12290;&#23545;&#19968;&#23567;&#37096;&#20998;&#20107;&#23454;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts 
&lt;/p&gt;</description></item><item><title>OmniDrones&#26159;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#28789;&#27963;&#24179;&#21488;&#65292;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#21644;&#26080;&#20154;&#26426;&#23398;&#20064;&#24037;&#20855;&#12290;&#36825;&#20010;&#24179;&#21488;&#26377;&#21161;&#20110;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2309.12825</link><description>&lt;p&gt;
OmniDrones&#65306;&#19968;&#31181;&#29992;&#20110;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#30340;&#39640;&#25928;&#28789;&#27963;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
OmniDrones: An Efficient and Flexible Platform for Reinforcement Learning in Drone Control. (arXiv:2309.12825v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12825
&lt;/p&gt;
&lt;p&gt;
OmniDrones&#26159;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#28789;&#27963;&#24179;&#21488;&#65292;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#35774;&#35745;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#21644;&#26080;&#20154;&#26426;&#23398;&#20064;&#24037;&#20855;&#12290;&#36825;&#20010;&#24179;&#21488;&#26377;&#21161;&#20110;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;OmniDrones&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#20026;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#32780;&#35774;&#35745;&#30340;&#39640;&#25928;&#28789;&#27963;&#30340;&#24179;&#21488;&#65292;&#24314;&#31435;&#22312;Nvidia&#30340;Omniverse Isaac Sim&#19978;&#12290;&#23427;&#37319;&#29992;&#33258;&#19979;&#32780;&#19978;&#30340;&#35774;&#35745;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#22320;&#35774;&#35745;&#21644;&#23454;&#39564;&#21508;&#31181;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#22312;GPU&#24182;&#34892;&#21270;&#20223;&#30495;&#20043;&#19978;&#36827;&#34892;&#27169;&#25311;&#12290;&#23427;&#36824;&#25552;&#20379;&#19968;&#31995;&#21015;&#22522;&#20934;&#20219;&#21153;&#65292;&#28085;&#30422;&#21333;&#20010;&#26080;&#20154;&#26426;&#24748;&#20572;&#21040;&#22810;&#39537;&#21160;&#31995;&#32479;&#36319;&#36394;&#31561;&#21508;&#31181;&#25361;&#25112;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24320;&#28304;&#30340;&#26080;&#20154;&#26426;&#20223;&#30495;&#24179;&#21488;&#65292;&#37197;&#22791;&#20102;&#19968;&#22871;&#29992;&#20110;&#26080;&#20154;&#26426;&#23398;&#20064;&#30340;&#24037;&#20855;&#12290;&#23427;&#21253;&#25324;4&#20010;&#26080;&#20154;&#26426;&#27169;&#22411;&#65292;5&#31181;&#20256;&#24863;&#22120;&#27169;&#24335;&#65292;4&#31181;&#25511;&#21046;&#27169;&#24335;&#65292;10&#22810;&#20010;&#22522;&#20934;&#20219;&#21153;&#20197;&#21450;&#19968;&#20123;&#24120;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27169;&#22411;&#12290;&#20026;&#20102;&#23637;&#31034;OmniDrones&#30340;&#33021;&#21147;&#24182;&#25903;&#25345;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#36825;&#20123;&#22522;&#20934;&#20219;&#21153;&#30340;&#21021;&#27493;&#32467;&#26524;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#20010;&#24179;&#21488;&#33021;&#22815;&#20419;&#36827;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#23454;&#38469;&#26080;&#20154;&#26426;&#31995;&#32479;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we introduce OmniDrones, an efficient and flexible platform tailored for reinforcement learning in drone control, built on Nvidia's Omniverse Isaac Sim. It employs a bottom-up design approach that allows users to easily design and experiment with various application scenarios on top of GPU-parallelized simulations. It also offers a range of benchmark tasks, presenting challenges ranging from single-drone hovering to over-actuated system tracking. In summary, we propose an open-sourced drone simulation platform, equipped with an extensive suite of tools for drone learning. It includes 4 drone models, 5 sensor modalities, 4 control modes, over 10 benchmark tasks, and a selection of widely used RL baselines. To showcase the capabilities of OmniDrones and to support future research, we also provide preliminary results on these benchmark tasks. We hope this platform will encourage further studies on applying RL to practical drone systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#37325;&#20849;&#32447;&#24615;&#38382;&#39064;&#65292;&#38024;&#23545;&#22235;&#24029;&#30465;&#30340;&#30899;&#25490;&#25918;&#24773;&#20917;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#30830;&#23450;&#20102;&#34892;&#19994;&#20998;&#32452;&#65292;&#35780;&#20272;&#20102;&#25490;&#25918;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#30340;&#20943;&#25490;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.01115</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#37325;&#20849;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;&#65306;&#22235;&#24029;&#30465;&#30899;&#25490;&#25918;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions in Sichuan Province. (arXiv:2309.01115v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#37325;&#20849;&#32447;&#24615;&#38382;&#39064;&#65292;&#38024;&#23545;&#22235;&#24029;&#30465;&#30340;&#30899;&#25490;&#25918;&#24773;&#20917;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#30830;&#23450;&#20102;&#34892;&#19994;&#20998;&#32452;&#65292;&#35780;&#20272;&#20102;&#25490;&#25918;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#30340;&#20943;&#25490;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30697;&#38453;&#24402;&#19968;&#21270;&#23545;&#22235;&#24029;&#30465;46&#20010;&#20851;&#38190;&#20135;&#19994;2000-2019&#24180;&#30340;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;DBSCAN&#32858;&#31867;&#35782;&#21035;&#20102;16&#20010;&#29305;&#24449;&#31867;&#21035;&#20197;&#23458;&#35266;&#22320;&#20998;&#32452;&#34892;&#19994;&#12290;&#25509;&#19979;&#26469;&#65292;&#37319;&#29992;&#32602;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#36807;&#25311;&#21512;&#25511;&#21046;&#12289;&#39640;&#32500;&#25968;&#25454;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#22797;&#26434;&#33021;&#28304;&#25968;&#25454;&#22788;&#29702;&#30340;&#20248;&#21183;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29028;&#28845;&#21608;&#22260;&#30340;&#31532;&#20108;&#20010;&#32858;&#31867;&#22240;&#29983;&#20135;&#38656;&#27714;&#32780;&#20135;&#29983;&#30340;&#25490;&#25918;&#37327;&#26368;&#39640;&#12290;&#20197;&#27773;&#27833;&#21644;&#28966;&#28845;&#20026;&#20013;&#24515;&#30340;&#32858;&#31867;&#30340;&#25490;&#25918;&#37327;&#20063;&#24456;&#26174;&#33879;&#12290;&#22522;&#20110;&#27492;&#65292;&#20943;&#25490;&#24314;&#35758;&#21253;&#25324;&#28165;&#27905;&#29028;&#25216;&#26415;&#12289;&#20132;&#36890;&#31649;&#29702;&#12289;&#38050;&#38081;&#20013;&#30340;&#29028;&#30005;&#26367;&#20195;&#21644;&#34892;&#19994;&#26631;&#20934;&#21270;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23458;&#35266;&#36873;&#25321;&#22240;&#32032;&#65292;&#24182;&#26088;&#22312;&#25506;&#32034;&#26032;&#30340;&#20943;&#25490;&#36884;&#24452;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#34892;&#19994;&#20998;&#32452;&#65292;&#35780;&#20272;&#20102;&#25490;&#25918;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#30340;&#20943;&#25490;&#31574;&#30053;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study preprocessed 2000-2019 energy consumption data for 46 key Sichuan industries using matrix normalization. DBSCAN clustering identified 16 feature classes to objectively group industries. Penalized regression models were then applied for their advantages in overfitting control, high-dimensional data processing, and feature selection - well-suited for the complex energy data. Results showed the second cluster around coal had highest emissions due to production needs. Emissions from gasoline-focused and coke-focused clusters were also significant. Based on this, emission reduction suggestions included clean coal technologies, transportation management, coal-electricity replacement in steel, and industry standardization. The research introduced unsupervised learning to objectively select factors and aimed to explore new emission reduction avenues. In summary, the study identified industry groupings, assessed emissions drivers, and proposed scientific reduction strategies to bette
&lt;/p&gt;</description></item><item><title>CausalBench&#25361;&#25112;&#36187;&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31454;&#36187;&#65292;&#26088;&#22312;&#26500;&#24314;&#22522;&#22240;&#32593;&#32476;&#25512;&#26029;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#21033;&#29992;&#22823;&#35268;&#27169;&#36951;&#20256;&#24178;&#25200;&#25968;&#25454;&#25552;&#21319;&#20102;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.15395</link><description>&lt;p&gt;
CausalBench&#25361;&#25112;&#36187;&#65306;&#22522;&#20110;&#21333;&#32454;&#32990;&#24178;&#25200;&#25968;&#25454;&#30340;&#22522;&#22240;&#32593;&#32476;&#25512;&#26029;&#30340;&#26426;&#22120;&#23398;&#20064;&#31454;&#36187;
&lt;/p&gt;
&lt;p&gt;
The CausalBench challenge: A machine learning contest for gene network inference from single-cell perturbation data. (arXiv:2308.15395v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15395
&lt;/p&gt;
&lt;p&gt;
CausalBench&#25361;&#25112;&#36187;&#26159;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#31454;&#36187;&#65292;&#26088;&#22312;&#26500;&#24314;&#22522;&#22240;&#32593;&#32476;&#25512;&#26029;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;&#21442;&#19982;&#32773;&#21033;&#29992;&#22823;&#35268;&#27169;&#36951;&#20256;&#24178;&#25200;&#25968;&#25454;&#25552;&#21319;&#20102;&#20808;&#36827;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#32472;&#21046;&#32454;&#32990;&#31995;&#32479;&#20869;&#22522;&#22240;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20851;&#31995;&#26159;&#20851;&#38190;&#30340;&#26089;&#26399;&#27493;&#39588;&#12290;&#36825;&#26377;&#21161;&#20110;&#21046;&#23450;&#20851;&#20110;&#21487;&#33021;&#34987;&#26410;&#26469;&#33647;&#29289;&#38774;&#21521;&#30340;&#20998;&#23376;&#26426;&#21046;&#30340;&#20551;&#35774;&#12290;CausalBench&#25361;&#25112;&#26159;&#19968;&#39033;&#36992;&#35831;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#26469;&#25512;&#36827;&#26500;&#24314;&#22522;&#22240;-&#22522;&#22240;&#30456;&#20114;&#20316;&#29992;&#32593;&#32476;&#30340;&#26368;&#26032;&#25216;&#26415;&#30340;&#20513;&#35758;&#12290;&#36825;&#20123;&#32593;&#32476;&#26159;&#20174;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#30340;&#21333;&#32454;&#32990;&#25968;&#25454;&#38598;&#20013;&#25512;&#23548;&#20986;&#26469;&#30340;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#32463;&#36807;&#19981;&#21516;&#31867;&#22411;&#30340;&#24178;&#25200;&#12290;&#36825;&#20123;&#32593;&#32476;&#23545;&#20110;&#29702;&#35299;&#30142;&#30149;&#29983;&#29289;&#23398;&#30340;&#22240;&#26524;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;&#21442;&#19982;&#32773;&#21033;&#29992;CausalBench&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#20219;&#21153;&#26159;&#25552;&#21319;&#20808;&#36827;&#26041;&#27861;&#21033;&#29992;&#22823;&#35268;&#27169;&#36951;&#20256;&#24178;&#25200;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#25253;&#21578;&#20998;&#26512;&#21644;&#24635;&#32467;&#20102;&#25361;&#25112;&#36187;&#26399;&#38388;&#25552;&#20132;&#30340;&#26041;&#27861;&#65292;&#20197;&#25551;&#32472;&#31454;&#36187;&#26399;&#38388;&#25216;&#26415;&#21457;&#23637;&#30340;&#37096;&#20998;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, mapping interactions between genes within cellular systems is a crucial early step. This helps formulate hypotheses regarding molecular mechanisms that could potentially be targeted by future medicines. The CausalBench Challenge was an initiative to invite the machine learning community to advance the state of the art in constructing gene-gene interaction networks. These networks, derived from large-scale, real-world datasets of single cells under various perturbations, are crucial for understanding the causal mechanisms underlying disease biology. Using the framework provided by the CausalBench benchmark, participants were tasked with enhancing the capacity of the state of the art methods to leverage large-scale genetic perturbation data. This report provides an analysis and summary of the methods submitted during the challenge to give a partial image of the state of the art at the time of the challenge. The winning solutions significantly improved performance compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#31639;&#27861;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.14872</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#24179;&#34913;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#20960;&#20309;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits. (arXiv:2306.14872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#31639;&#27861;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#23454;&#35777;&#24615;&#33021;&#19982;&#24754;&#35266;&#29702;&#35770;&#21518;&#24724;&#30028;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#21551;&#21457;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#21253;&#25324;&#36138;&#24515;&#12289;OFUL&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#22312;&#20869;&#30340;&#24191;&#27867;&#31639;&#27861;&#31867;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#22312;&#20445;&#30041;&#22522;&#26412;&#31639;&#27861;&#22823;&#37096;&#20998;&#20248;&#33391;&#29305;&#24615;&#30340;&#21516;&#26102;&#8220;&#26657;&#27491;&#8221;&#22522;&#26412;&#31639;&#27861;&#22312;&#26576;&#20123;&#23454;&#20363;&#20013;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#28176;&#36817;&#26368;&#20248;&#21518;&#24724;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is motivated by recent developments in the linear bandit literature, which have revealed a discrepancy between the promising empirical performance of algorithms such as Thompson sampling and Greedy, when compared to their pessimistic theoretical regret bounds. The challenge arises from the fact that while these algorithms may perform poorly in certain problem instances, they generally excel in typical instances. To address this, we propose a new data-driven technique that tracks the geometry of the uncertainty ellipsoid, enabling us to establish an instance-dependent frequentist regret bound for a broad class of algorithms, including Greedy, OFUL, and Thompson sampling. This result empowers us to identify and ``course-correct" instances in which the base algorithms perform poorly. The course-corrected algorithms achieve the minimax optimal regret of order $\tilde{\mathcal{O}}(d\sqrt{T})$, while retaining most of the desirable properties of the base algorithms. We present sim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2306.07992</link><description>&lt;p&gt;
&#23433;&#20840;&#30340;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Securing Visually-Aware Recommender Systems: An Adversarial Image Reconstruction and Detection Framework. (arXiv:2306.07992v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65292;&#33021;&#22815;&#38450;&#24481;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#24182;&#19988;&#33021;&#22815;&#22312;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23500;&#21547;&#22270;&#29255;&#31561;&#35270;&#35273;&#25968;&#25454;&#19982;&#29289;&#21697;&#20851;&#32852;&#24230;&#22686;&#21152;&#65292;&#35270;&#35273;&#24863;&#30693;&#25512;&#33616;&#31995;&#32479;&#65288;VARS&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;VARS&#26131;&#21463;&#21040;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#30340;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21521;&#19982;&#36825;&#20123;&#29289;&#21697;&#20851;&#32852;&#30340;&#24178;&#20928;&#22270;&#20687;&#28155;&#21152;&#20154;&#31867;&#26080;&#27861;&#24863;&#30693;&#30340;&#25200;&#21160;&#12290;&#23545;VARS&#30340;&#25915;&#20987;&#20026;&#24191;&#27867;&#20351;&#29992;VARS&#30340;&#35768;&#22810;&#24212;&#29992;&#65288;&#22914;&#30005;&#23376;&#21830;&#21153;&#21644;&#31038;&#20132;&#32593;&#32476;&#65289;&#24102;&#26469;&#26032;&#30340;&#23433;&#20840;&#25361;&#25112;&#12290;&#22914;&#20309;&#20445;&#25252;VARS&#20813;&#21463;&#27492;&#31867;&#23545;&#25239;&#25915;&#20987;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#23578;&#32570;&#20047;&#31995;&#32479;&#22320;&#30740;&#31350;&#22914;&#20309;&#35774;&#35745;&#38024;&#23545;VARS&#35270;&#35273;&#25915;&#20987;&#30340;&#23433;&#20840;&#38450;&#24481;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#25239;&#22270;&#20687;&#37325;&#26500;&#21450;&#26816;&#27979;&#26694;&#26550;&#26469;&#20445;&#25252;VARS&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#21516;&#26102;(1)&#36890;&#36807;&#22522;&#20110;&#20840;&#23616;&#35270;&#35273;&#20256;&#36755;&#30340;&#22270;&#20687;&#37325;&#26500;&#26469;&#38450;&#24481;&#20197;&#23616;&#37096;&#25200;&#21160;&#20026;&#29305;&#24449;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;(2)&#20351;&#29992;&#22312;&#23569;&#37327;&#24178;&#20928;&#21644;&#23545;&#25239;&#24615;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#26816;&#27979;&#27169;&#22411;&#26469;&#26816;&#27979;&#23545;&#25239;&#24615;&#22270;&#20687;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#26377;&#25928;&#22320;&#38450;&#24481;&#21508;&#31181;&#29289;&#21697;-&#22270;&#20687;&#23545;&#25239;&#25915;&#20987;&#23545;VARS&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
With rich visual data, such as images, becoming readily associated with items, visually-aware recommendation systems (VARS) have been widely used in different applications. Recent studies have shown that VARS are vulnerable to item-image adversarial attacks, which add human-imperceptible perturbations to the clean images associated with those items. Attacks on VARS pose new security challenges to a wide range of applications such as e-Commerce and social networks where VARS are widely used. How to secure VARS from such adversarial attacks becomes a critical problem. Currently, there is still a lack of systematic study on how to design secure defense strategies against visual attacks on VARS. In this paper, we attempt to fill this gap by proposing an adversarial image reconstruction and detection framework to secure VARS. Our proposed method can simultaneously (1) secure VARS from adversarial attacks characterized by local perturbations by image reconstruction based on global vision tra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#26426;&#22120;&#20195;&#29702;&#26469;&#33258;&#20027;&#25191;&#34892;&#32929;&#31080;&#20132;&#26131;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#19981;&#21516;&#24066;&#22330;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2306.03985</link><description>&lt;p&gt;
&#26426;&#22120;&#20195;&#29702;&#22312;&#33391;&#24615;&#21644;&#24694;&#24615;&#24773;&#22659;&#19979;&#30340;&#33258;&#20027;&#32929;&#31080;&#20132;&#26131;
&lt;/p&gt;
&lt;p&gt;
Agent Performing Autonomous Stock Trading under Good and Bad Situations. (arXiv:2306.03985v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03985
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#35757;&#32451;&#26426;&#22120;&#20195;&#29702;&#26469;&#33258;&#20027;&#25191;&#34892;&#32929;&#31080;&#20132;&#26131;&#30340;&#30740;&#31350;&#65292;&#24182;&#22312;&#19981;&#21516;&#24066;&#22330;&#29615;&#22659;&#19979;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32929;&#31080;&#20132;&#26131;&#26159;&#36130;&#21153;&#31649;&#29702;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#24066;&#22330;&#21644;&#32463;&#27982;&#29615;&#22659;&#19981;&#31283;&#23450;&#65292;&#36890;&#24120;&#19981;&#33021;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20174;&#20107;&#32929;&#31080;&#20132;&#26131;&#38656;&#35201;&#26102;&#38388;&#21644;&#31934;&#21147;&#26469;&#20998;&#26512;&#12289;&#21046;&#23450;&#31574;&#30053;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#22914;&#26524;&#19968;&#20010;&#26426;&#22120;&#20195;&#29702;&#33021;&#22815;&#36741;&#21161;&#29978;&#33267;&#25191;&#34892;&#20998;&#26512;&#21644;&#24314;&#27169;&#36807;&#21435;&#25968;&#25454;&#65292;&#28982;&#21518;&#29983;&#25104;&#33258;&#20027;&#20132;&#26131;&#31574;&#30053;&#65292;&#37027;&#23558;&#26159;&#26041;&#20415;&#21644;&#26377;&#25928;&#30340;&#12290;&#36817;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#24050;&#34987;&#35777;&#26126;&#22312;&#28041;&#21450;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#36798;&#21040;&#30446;&#26631;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#22312;&#26412;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27169;&#25311;&#32929;&#31080;&#20132;&#26131;&#29615;&#22659;&#30340;&#31649;&#36947;&#65292;&#24182;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#28145;&#24230;Q&#23398;&#20064;&#12289;&#28145;&#24230;SARSA&#21644;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#26426;&#22120;&#20195;&#29702;&#26469;&#33258;&#21160;&#21270;&#32929;&#31080;&#20132;&#26131;&#36807;&#31243;&#12290;&#25105;&#20204;&#22312;&#30456;&#23545;&#33391;&#22909;&#65288;2021&#24180;&#20043;&#21069;&#65289;&#21644;&#24694;&#21155;&#65288;2021&#24180;-2022&#24180;&#65289;&#24773;&#20917;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stock trading is one of the popular ways for financial management. However, the market and the environment of economy is unstable and usually not predictable. Furthermore, engaging in stock trading requires time and effort to analyze, create strategies, and make decisions. It would be convenient and effective if an agent could assist or even do the task of analyzing and modeling the past data and then generate a strategy for autonomous trading. Recently, reinforcement learning has been shown to be robust in various tasks that involve achieving a goal with a decision making strategy based on time-series data. In this project, we have developed a pipeline that simulates the stock trading environment and have trained an agent to automate the stock trading process with deep reinforcement learning methods, including deep Q-learning, deep SARSA, and the policy gradient method. We evaluate our platform during relatively good (before 2021) and bad (2021 - 2022) situations. The stocks we've eva
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15560</link><description>&lt;p&gt;
&#22522;&#20110; Foundation Model APIs &#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#65306;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data via Foundation Model APIs 1: Images. (arXiv:2305.15560v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20943;&#36731;&#38544;&#31169;&#38382;&#39064;&#12290;&#19982;&#24403;&#21069;&#20026;&#27492;&#20219;&#21153;&#35757;&#32451;&#23450;&#21046;&#27169;&#22411;&#30340;&#20570;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;API&#29983;&#25104;DP&#21512;&#25104;&#25968;&#25454;&#65288;DPSDA&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#22522;&#30784;&#27169;&#22411;&#35270;&#20026;&#40657;&#30418;&#24182;&#21482;&#21033;&#29992;&#20854;&#25512;&#29702;API&#12290;&#36825;&#20123;&#22522;&#20110;API&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#26356;&#23481;&#26131;&#37096;&#32626;&#65292;&#22914;&#26368;&#36817; API &#24212;&#29992;&#31243;&#24207;&#30340;&#28608;&#22686;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#21033;&#29992;&#21487;&#36890;&#36807;&#20854;&#25512;&#29702;API&#35775;&#38382;&#20854;&#26435;&#37325;&#26410;&#21457;&#24067;&#30340;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#35775;&#38382;&#26356;&#21152;&#20005;&#26684;&#65292;&#36824;&#38656;&#20445;&#25252;API&#25552;&#20379;&#21830;&#30340;&#38544;&#31169;&#65292;&#36825;&#23558;&#24102;&#26469;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Private Evolution&#65288;PE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;API&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#21021;&#22987;&#23454;&#29616;&#12290;PE&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#22914;CIFAR-10&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29992;&#21644;&#38544;&#31169;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;DP&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating differentially private (DP) synthetic data that closely resembles the original private data without leaking sensitive user information is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are accessible via their inference APIs while the model weights are unreleased. However, this comes with greater challenges due to strictly more restrictive model access and the additional need to protect privacy from the API provider.  In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its ini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.13673</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#29289;&#29702;&#23398;&#65306;&#31532;&#19968;&#37096;&#20998;&#65292;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics of Language Models: Part 1, Context-Free Grammar. (arXiv:2305.13673v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;&#65292;&#24182;&#36890;&#36807;&#26500;&#36896;&#20154;&#36896;&#25968;&#25454;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;transformers&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#30740;&#31350;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65292;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36824;&#30740;&#31350;&#20102;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#35777;&#26126;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#30740;&#31350;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#22914;&#20309;&#23398;&#20064;&#19978;&#19979;&#25991;&#26080;&#20851;&#25991;&#27861;&#65288;CFG&#65289;-&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#31995;&#32479;&#65292;&#21487;&#25429;&#25417;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#65292;&#31243;&#24207;&#21644;&#20154;&#31867;&#36923;&#36753;&#30340;&#26041;&#38754;&#12290;CFG&#19982;&#19979;&#25512;&#33258;&#21160;&#26426;&#19968;&#26679;&#22256;&#38590;&#65292;&#21487;&#33021;&#26159;&#27169;&#26865;&#20004;&#21487;&#30340;&#65292;&#22240;&#27492;&#39564;&#35777;&#23383;&#31526;&#20018;&#26159;&#21542;&#28385;&#36275;&#35268;&#21017;&#38656;&#35201;&#21160;&#24577;&#35268;&#21010;&#12290;&#25105;&#20204;&#26500;&#36896;&#20102;&#20154;&#36896;&#25968;&#25454;&#65292;&#24182;&#35777;&#26126;&#21363;&#20351;&#23545;&#20110;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CFG&#65292;&#39044;&#35757;&#32451;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#29983;&#25104;&#20855;&#26377;&#25509;&#36817;&#23436;&#32654;&#20934;&#30830;&#24230;&#21644;&#26174;&#30528;&#22810;&#26679;&#24615;&#30340;&#21477;&#23376;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;transformers&#23398;&#20064;CFG&#32972;&#21518;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;&#25105;&#20204;&#21457;&#29616;transformer&#20869;&#37096;&#30340;&#38544;&#34255;&#29366;&#24577;&#38544;&#21547;&#32780;&#31934;&#30830;&#22320;&#32534;&#30721;&#20102;CFG&#32467;&#26500;&#65288;&#22914;&#22312;&#23376;&#26641;&#36793;&#30028;&#19978;&#31934;&#30830;&#23450;&#20301;&#26641;&#33410;&#28857;&#20449;&#24687;&#65289;&#65292;&#24182;&#23398;&#20250;&#24418;&#25104;&#31867;&#20284;&#21160;&#24577;&#35268;&#21010;&#30340;&#8220;&#36793;&#30028;&#21040;&#36793;&#30028;&#8221;&#30340;&#27880;&#24847;&#21147;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#19968;&#20123;&#26631;&#20934;CFG&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;&#27010;&#29575;CFG&#21644;&#32447;&#24615;CFG&#65292;&#24182;&#23637;&#31034;transformers&#20063;&#21487;&#20197;&#23398;&#20250;&#36825;&#20123;&#25193;&#23637;&#35821;&#27861;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#24182;&#20026;&#26410;&#26469;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;&#20998;&#26512;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design experiments to study $\textit{how}$ generative language models, like GPT, learn context-free grammars (CFGs) -- diverse language systems with a tree-like structure capturing many aspects of natural languages, programs, and human logics. CFGs are as hard as pushdown automata, and can be ambiguous so that verifying if a string satisfies the rules requires dynamic programming. We construct synthetic data and demonstrate that even for very challenging CFGs, pre-trained transformers can learn to generate sentences with near-perfect accuracy and remarkable $\textit{diversity}$.  More importantly, we delve into the $\textit{physical principles}$ behind how transformers learns CFGs. We discover that the hidden states within the transformer implicitly and $\textit{precisely}$ encode the CFG structure (such as putting tree node information exactly on the subtree boundary), and learn to form "boundary to boundary" attentions that resemble dynamic programming. We also cover some extensio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;AUTO&#30340;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#27979;&#35797;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22312;&#32447;&#25968;&#25454;&#30452;&#25509;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#24182;&#22312;&#32447;&#26816;&#27979;OOD&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.12267</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#31163;&#32676;&#20540;&#20248;&#21270;&#65306;&#29992;&#20110;&#22312;&#32447;&#27979;&#35797;&#26102;OOD&#26816;&#27979;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AUTO: Adaptive Outlier Optimization for Online Test-Time OOD Detection. (arXiv:2303.12267v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12267
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;AUTO&#30340;&#26041;&#27861;&#65292;&#22312;&#22312;&#32447;&#27979;&#35797;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22312;&#32447;&#25968;&#25454;&#30452;&#25509;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#33258;&#36866;&#24212;&#22320;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#24182;&#22312;&#32447;&#26816;&#27979;OOD&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;OOD&#65288;out-of-distribution&#65289;&#26816;&#27979;&#26159;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#12290;&#32463;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#36741;&#21161;&#31163;&#32676;&#20540;&#35757;&#32451;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31163;&#32676;&#20540;&#36890;&#24120;&#19982;&#27979;&#35797;OOD&#25968;&#25454;&#23384;&#22312;&#20998;&#24067;&#24046;&#36317;&#65292;&#24182;&#19988;&#19981;&#33021;&#35206;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#27979;&#35797;OOD&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#32467;&#21512;&#36825;&#20123;&#31163;&#32676;&#20540;&#36824;&#20250;&#22686;&#21152;&#35757;&#32451;&#30340;&#36127;&#25285;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#27979;&#35797;&#26102;OOD&#26816;&#27979;&#30340;&#26032;&#33539;&#24335;&#65292;&#35813;&#33539;&#24335;&#30452;&#25509;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#22312;&#32447;&#25968;&#25454;&#65292;&#20197;&#25552;&#39640;OOD&#26816;&#27979;&#24615;&#33021;&#12290;&#34429;&#28982;&#36825;&#31181;&#33539;&#24335;&#24456;&#39640;&#25928;&#65292;&#20294;&#23427;&#20063;&#38754;&#20020;&#30528;&#35832;&#22914;&#28798;&#38590;&#24615;&#36951;&#24536;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#31163;&#32676;&#20540;&#20248;&#21270;&#65288;AUTO&#65289;&#65292;&#23427;&#30001;&#20869;&#22806;&#24863;&#30693;&#28388;&#27874;&#22120;&#12289;ID&#23384;&#20648;&#22120;&#21644;&#35821;&#20041;&#19968;&#33268;&#30340;&#30446;&#26631;&#32452;&#25104;&#12290;AUTO&#33258;&#36866;&#24212;&#22320;&#20174;&#27979;&#35797;&#25968;&#25454;&#20013;&#25366;&#25496;&#20266;ID&#21644;&#20266;OOD&#26679;&#26412;&#65292;&#21033;&#29992;&#23427;&#20204;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#24182;&#22312;&#32447;&#26816;&#27979;OOD&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AUTO&#22312;&#21508;&#31181;&#22522;&#20934;&#21644;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution (OOD) detection is a crucial aspect of deploying machine learning models in open-world applications. Empirical evidence suggests that training with auxiliary outliers substantially improves OOD detection. However, such outliers typically exhibit a distribution gap compared to the test OOD data and do not cover all possible test OOD scenarios. Additionally, incorporating these outliers introduces additional training burdens. In this paper, we introduce a novel paradigm called test-time OOD detection, which utilizes unlabeled online data directly at test time to improve OOD detection performance. While this paradigm is efficient, it also presents challenges such as catastrophic forgetting. To address these challenges, we propose adaptive outlier optimization (AUTO), which consists of an in-out-aware filter, an ID memory bank, and a semantically-consistent objective. AUTO adaptively mines pseudo-ID and pseudo-OOD samples from test data, utilizing them to optimize netwo
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#21644;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#20013;&#21463;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#24212;&#29992;&#65292;&#25506;&#35752;&#23558;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#33267;&#27979;&#35797;&#24037;&#20855;&#20013;&#20197;&#25552;&#39640;&#25968;&#23383;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07546</link><description>&lt;p&gt;
&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#21450;&#20854;&#22312;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#32508;&#36848;&#65288;arXiv:2303.07546v1 [cs.SE]&#65289;
&lt;/p&gt;
&lt;p&gt;
Constrained Adversarial Learning and its applicability to Automated Software Testing: a systematic review. (arXiv:2303.07546v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#21644;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#20013;&#21463;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#24212;&#29992;&#65292;&#25506;&#35752;&#23558;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#33267;&#27979;&#35797;&#24037;&#20855;&#20013;&#20197;&#25552;&#39640;&#25968;&#23383;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#31181;&#26032;&#25216;&#26415;&#37117;&#20250;&#22686;&#21152;&#38544;&#21547;&#30340;&#28431;&#27934;&#65292;&#35753;&#36234;&#26469;&#36234;&#22810;&#30340;&#32593;&#32476;&#25915;&#20987;&#32773;&#21033;&#29992;&#12290;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#21487;&#20197;&#25104;&#20026;&#24555;&#36895;&#20998;&#26512;&#25968;&#21315;&#34892;&#20195;&#30721;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#30053;&#24494;&#20462;&#25913;&#21151;&#33021;&#29305;&#23450;&#30340;&#27979;&#35797;&#25968;&#25454;&#26469;&#36935;&#21040;&#22810;&#20010;&#28431;&#27934;&#21644;&#25915;&#20987;&#21521;&#37327;&#12290;&#36825;&#20010;&#36807;&#31243;&#19982;&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#21463;&#38480;&#24615;&#23545;&#25239;&#24615;&#31034;&#20363;&#30456;&#20284;&#65292;&#22240;&#27492;&#23558;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#21040;&#33258;&#21160;&#21270;&#27979;&#35797;&#24037;&#20855;&#20013;&#21487;&#33021;&#20250;&#26377;&#26174;&#30528;&#30340;&#22909;&#22788;&#12290;&#22240;&#27492;&#65292;&#26412;&#31995;&#32479;&#32508;&#36848;&#20391;&#37325;&#20110;&#38480;&#21046;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#22312;&#23545;&#25239;&#23398;&#20064;&#21644;&#36719;&#20214;&#27979;&#35797;&#20013;&#30340;&#24212;&#29992;&#30340;&#24403;&#21069;&#26368;&#26032;&#25216;&#26415;&#65292;&#26088;&#22312;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#27979;&#35797;&#24037;&#20855;&#65292;&#25552;&#39640;&#25968;&#23383;&#31995;&#32479;&#30340;&#24377;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23545;&#20110;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#29616;&#21463;&#38480;&#21046;&#30340;&#25968;&#25454;&#29983;&#25104;&#24212;&#29992;&#26159;&#31995;&#32479;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every novel technology adds hidden vulnerabilities ready to be exploited by a growing number of cyber-attacks. Automated software testing can be a promising solution to quickly analyze thousands of lines of code by generating and slightly modifying function-specific testing data to encounter a multitude of vulnerabilities and attack vectors. This process draws similarities to the constrained adversarial examples generated by adversarial learning methods, so there could be significant benefits to the integration of these methods in automated testing tools. Therefore, this systematic review is focused on the current state-of-the-art of constrained data generation methods applied for adversarial learning and software testing, aiming to guide researchers and developers to enhance testing tools with adversarial learning methods and improve the resilience and robustness of their digital systems. The found constrained data generation applications for adversarial machine learning were systemat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.10813</link><description>&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#20445;&#35777;&#25552;&#39640;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10813
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#65292;&#21517;&#20026;&#21028;&#21035;&#39118;&#38505;&#65292;&#26088;&#22312;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#32773;&#36824;&#35752;&#35770;&#20102;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#22312;&#36234;&#26469;&#36234;&#22810;&#30340;&#29616;&#23454;&#22330;&#26223;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#20110;&#38544;&#34255;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#27495;&#35270;&#30340;&#25285;&#24551;&#27491;&#22312;&#22686;&#21152;&#12290;&#35768;&#22810;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#20197;&#22686;&#24378;&#20844;&#24179;&#24615;&#65292;&#21253;&#25324;&#24120;&#29992;&#30340;&#32676;&#20307;&#20844;&#24179;&#24615;&#24230;&#37327;&#21644;&#20960;&#31181;&#32467;&#21512;&#38598;&#25104;&#23398;&#20064;&#30340;&#20844;&#24179;&#24863;&#30693;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20844;&#24179;&#24230;&#37327;&#21482;&#33021;&#20851;&#27880;&#20854;&#20013;&#20043;&#19968;&#65292;&#21363;&#32676;&#20307;&#20844;&#24179;&#24615;&#25110;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#23427;&#20204;&#20043;&#38388;&#30340;&#30828;&#24615;&#20860;&#23481;&#24615;&#26263;&#31034;&#20102;&#21363;&#20351;&#20854;&#20013;&#20043;&#19968;&#24471;&#21040;&#28385;&#36275;&#65292;&#20173;&#21487;&#33021;&#23384;&#22312;&#20559;&#35265;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#25552;&#21319;&#20844;&#24179;&#24615;&#30340;&#26426;&#21046;&#36890;&#24120;&#21482;&#25552;&#20379;&#32463;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#35770;&#25991;&#35752;&#35770;&#20844;&#24179;&#24615;&#26159;&#21542;&#21487;&#20197;&#22312;&#29702;&#35770;&#19978;&#24471;&#21040;&#20445;&#35777;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#36136;&#37327;&#24230;&#37327;&#26041;&#27861;&#8212;&#8212;&#21028;&#21035;&#39118;&#38505;&#65292;&#20197;&#21453;&#26144;&#20010;&#20307;&#21644;&#32676;&#20307;&#20844;&#24179;&#24615;&#20004;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;p...
&lt;/p&gt;
&lt;p&gt;
The concern about underlying discrimination hidden in ML models is increasing, as ML systems have been widely applied in more and more real-world scenarios and any discrimination hidden in them will directly affect human life. Many techniques have been developed to enhance fairness including commonly-used group fairness measures and several fairness-aware methods combining ensemble learning. However, existing fairness measures can only focus on one aspect -- either group or individual fairness, and the hard compatibility among them indicates a possibility of remaining biases even if one of them is satisfied. Moreover, existing mechanisms to boost fairness usually present empirical results to show validity, yet few of them discuss whether fairness can be boosted with certain theoretical guarantees. To address these issues, we propose a fairness quality measure named discriminative risk in this paper to reflect both individual and group fairness aspects. Furthermore, we investigate the p
&lt;/p&gt;</description></item><item><title>DPM-Solver++&#26159;&#19968;&#31181;&#24555;&#36895;&#27714;&#35299;&#22120;&#65292;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#24341;&#23548;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#21152;&#24555;&#26679;&#26412;&#29983;&#25104;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.01095</link><description>&lt;p&gt;
DPM-Solver++&#65306;&#29992;&#20110;&#24341;&#23548;&#37319;&#26679;&#30340;&#24555;&#36895;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
DPM-Solver++: Fast Solver for Guided Sampling of Diffusion Probabilistic Models. (arXiv:2211.01095v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01095
&lt;/p&gt;
&lt;p&gt;
DPM-Solver++&#26159;&#19968;&#31181;&#24555;&#36895;&#27714;&#35299;&#22120;&#65292;&#22312;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#24341;&#23548;&#37319;&#26679;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21487;&#21152;&#24555;&#26679;&#26412;&#29983;&#25104;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411; (DPMs) &#22312;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21512;&#25104;&#31561;&#39046;&#22495;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780; DPM &#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#25152;&#38656;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#24341;&#23548;&#37319;&#26679;&#12290;&#29616;&#26377;&#30340;&#24555;&#36895;&#37319;&#26679;&#22120; DDIM &#22312;&#24341;&#23548;&#37319;&#26679;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38656;&#35201; 100 &#33267; 250 &#27493;&#25165;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; DPM-Solver++&#65292;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#24341;&#23548;&#37319;&#26679;&#30340;&#39640;&#38454;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion probabilistic models (DPMs) have achieved impressive success in high-resolution image synthesis, especially in recent large-scale text-to-image generation applications. An essential technique for improving the sample quality of DPMs is guided sampling, which usually needs a large guidance scale to obtain the best sample quality. The commonly-used fast sampler for guided sampling is DDIM, a first-order diffusion ODE solver that generally needs 100 to 250 steps for high-quality samples. Although recent works propose dedicated high-order solvers and achieve a further speedup for sampling without guidance, their effectiveness for guided sampling has not been well-tested before. In this work, we demonstrate that previous high-order fast samplers suffer from instability issues, and they even become slower than DDIM when the guidance scale grows large. To further speed up guided sampling, we propose DPM-Solver++, a high-order solver for the guided sampling of DPMs. DPM-Solver++ solv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.12494</link><description>&lt;p&gt;
&#20851;&#20110;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#21644;&#19968;&#31867;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
On the Generalized Likelihood Ratio Test and One-Class Classifiers. (arXiv:2210.12494v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#19968;&#31867;&#20998;&#31867;&#22120;&#21644;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#22312;&#25910;&#25947;&#26102;&#20250;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#36824;&#23637;&#31034;&#20102;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#22312;&#25910;&#25947;&#26102;&#20063;&#33021;&#36798;&#21040;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31867;&#20998;&#31867;&#65288;OCC&#65289;&#26159;&#20915;&#23450;&#35266;&#23519;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30446;&#26631;&#31867;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#22312;&#21253;&#21547;&#30446;&#26631;&#31867;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#19968;&#20010;&#34920;&#29616;&#20026;&#24191;&#20041;&#20284;&#28982;&#27604;&#26816;&#39564;&#65288;GLRT&#65289;&#30340;OCC&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24403;&#30446;&#26631;&#31867;&#30340;&#32479;&#35745;&#20449;&#24687;&#21487;&#29992;&#26102;&#65292;GLRT&#35299;&#20915;&#20102;&#30456;&#21516;&#30340;&#38382;&#39064;&#12290;GLRT&#26159;&#19968;&#20010;&#20247;&#25152;&#21608;&#30693;&#19988;&#22312;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#35777;&#26126;&#26368;&#20339;&#30340;&#20998;&#31867;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22810;&#23618;&#24863;&#30693;&#22120;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#21644;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#27169;&#22411;&#12290;&#23427;&#20204;&#20351;&#29992;&#20154;&#24037;&#25968;&#25454;&#38598;&#35757;&#32451;&#20026;&#20004;&#31867;&#20998;&#31867;&#22120;&#65292;&#20854;&#20013;&#26367;&#20195;&#31867;&#20351;&#29992;&#22312;&#30446;&#26631;&#31867;&#25968;&#25454;&#38598;&#30340;&#23450;&#20041;&#22495;&#19978;&#22343;&#21248;&#29983;&#25104;&#30340;&#38543;&#26426;&#26679;&#26412;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#65292;&#27169;&#22411;&#22312;&#22823;&#25968;&#25454;&#38598;&#19978;&#25910;&#25947;&#21040;&#20102;GLRT&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20855;&#26377;&#36866;&#24403;&#26680;&#20989;&#25968;&#30340;&#19968;&#31867;&#26368;&#23567;&#20108;&#20056;SVM&#65288;OCLSSVM&#65289;&#22312;&#25910;&#25947;&#26102;&#34920;&#29616;&#20026;GLRT&#12290;
&lt;/p&gt;
&lt;p&gt;
One-class classification (OCC) is the problem of deciding whether an observed sample belongs to a target class. We consider the problem of learning an OCC model that performs as the generalized likelihood ratio test (GLRT), given a dataset containing samples of the target class. The GLRT solves the same problem when the statistics of the target class are available. The GLRT is a well-known and provably optimal (under specific assumptions) classifier. To this end, we consider both the multilayer perceptron neural network (NN) and the support vector machine (SVM) models. They are trained as two-class classifiers using an artificial dataset for the alternative class, obtained by generating random samples, uniformly over the domain of the target-class dataset. We prove that, under suitable assumptions, the models converge (with a large dataset) to the GLRT. Moreover, we show that the one-class least squares SVM (OCLSSVM) with suitable kernels at convergence performs as the GLRT. Lastly, we
&lt;/p&gt;</description></item></channel></rss>