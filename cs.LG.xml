<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.01671</link><description>&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#65306;&#32479;&#35745;&#26816;&#39564;&#12289;&#24230;&#37327;&#29109;&#20013;&#30340;&#38477;&#32500;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#20197;&#21033;&#29992;&#26469;&#31616;&#21270;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#24120;&#35265;&#30340;&#23545;&#31216;&#24615;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#20851;&#20110;&#26500;&#24314;&#25490;&#21015;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#30740;&#31350;&#27963;&#21160;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#30340;&#21464;&#37327;&#22914;&#20309;&#32479;&#35745;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#21364;&#40092;&#26377;&#30740;&#31350;&#65292;&#20854;&#20013;&#26679;&#26412;&#37327;&#20801;&#35768;&#38543;&#30528;&#32500;&#25968;&#30340;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#22312;&#32479;&#35745;&#29702;&#35770;&#26041;&#38754;&#65292;&#20851;&#20110;&#25490;&#21015;&#19981;&#21464;&#24615;&#22914;&#20309;&#24110;&#21161;&#20272;&#35745;&#20013;&#38477;&#32500;&#30340;&#30693;&#35782;&#29978;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20960;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22238;&#39038;&#24182;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27979;&#35797;&#22810;&#20803;&#20998;&#24067;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20551;&#35774;&#65307;&#65288;ii&#65289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#65307;&#65288;iii&#65289;&#20998;&#26512;&#20809;&#28369;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#24182;&#23558;&#20854;&#19982;&#26410;&#24378;&#21152;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#23545;&#24212;&#20989;&#25968;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01671v1 Announce Type: new  Abstract: Permutation invariance is among the most common symmetry that can be exploited to simplify complex problems in machine learning (ML). There has been a tremendous surge of research activities in building permutation invariant ML architectures. However, less attention is given to how to statistically test for permutation invariance of variables in a multivariate probability distribution where the dimension is allowed to grow with the sample size. Also, in terms of a statistical theory, little is known about how permutation invariance helps with estimation in reducing dimensions. In this paper, we take a step back and examine these questions in several fundamental problems: (i) testing the assumption of permutation invariance of multivariate distributions; (ii) estimating permutation invariant densities; (iii) analyzing the metric entropy of smooth permutation invariant function classes and compare them with their counterparts without impos
&lt;/p&gt;</description></item><item><title>&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14746</link><description>&lt;p&gt;
&#25193;&#23637;&#39640;&#25928;&#30340;LLM&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14746
&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#20026;&#20102;&#25552;&#39640;&#25928;&#29575;&#65292;&#30740;&#31350;&#20102;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#30340;&#39640;&#25928;LLM&#27169;&#22411;&#65292;&#24471;&#20986;&#20102;&#21442;&#25968;&#25968;&#37327;&#19982;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#26032;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#24471;&#21040;&#30340;LLM&#27169;&#22411;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#21363;&#22823;&#37096;&#20998;&#21442;&#25968;&#20026;&#38646;&#65292;&#36825;&#24341;&#21457;&#20102;&#20851;&#20110;&#25928;&#29575;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21363;&#37027;&#20123;&#22312;&#35757;&#32451;&#35821;&#26009;&#19978;&#36798;&#21040;&#25152;&#38656;&#20934;&#30830;&#24230;&#30340;&#21442;&#25968;&#26368;&#23569;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#24403;&#21069;&#35268;&#27169;&#19979;&#35757;&#32451;&#25439;&#22833;&#30340;&#29702;&#35770;&#21644;&#23454;&#35777;&#20272;&#35745;&#65292;&#20197;&#33719;&#24471;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#20013;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#19978;&#19979;&#30028;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26263;&#31034;&#65306;(1)&#35201;&#22312;&#35757;&#32451;&#35821;&#26009;&#20013;&#34920;&#31034;&#30340;&#25216;&#33021;&#25968;&#37327;&#32763;&#20493;&#65292;&#38656;&#35201;&#23558;&#35821;&#26009;&#35268;&#27169;&#22823;&#32422;&#25193;&#23637;&#19977;&#21040;&#20116;&#20493;&#65292;(2)&#23545;&#20110;&#39640;&#25928;&#30340;LLM&#27169;&#22411;&#65292;&#21442;&#25968;&#25968;&#37327;$N$&#21644;&#33258;&#28982;&#35757;&#32451;&#35821;&#26009;&#35268;&#27169;$D$&#28385;&#36275;$N \sim D^{0.58}$&#30340;&#20851;&#31995;&#65292;(3)&#22914;&#26524;&#19968;&#20010;LLM&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23567;&#20110;&#35757;&#32451;&#35821;&#26009;&#20013;&#30340;&#29420;&#29305;&#24207;&#21015;&#25968;&#37327;&#65292;&#25193;&#23637;&#21487;&#20197;&#25581;&#31034;&#20986;&#26032;&#30340;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14746v1 Announce Type: new  Abstract: Trained LLMs are typically sparse in that most of the parameters are zero, raising questions on efficiency. In response, we inquire into efficient LLMs, i.e. those with the fewest parameters that achieve the desired accuracy on a training corpus. Specifically, we compare theoretical and empirical estimates for training loss at current scale to obtain upper and lower bounds on the number of unique sequences in a natural training corpus as a function of its size. Our result implies (1) to double the number of skills represented in a training corpus, the corpus must scale roughly between three and five fold (2) for efficient LLMs, the number of parameters $N$ and the size $D$ of a natural training corpus scale as $N \sim D^{0.58}$ (3) if the number of parameters of an LLM is smaller than the number of unique sequences in the training corpus, scaling up can uncover emergent skills.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#12289;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#35797;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#25511;&#21046;&#26041;&#27861;&#26469;&#32416;&#27491;&#27979;&#35797;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.13196</link><description>&lt;p&gt;
&#23454;&#29992;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26680;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Practical Kernel Tests of Conditional Independence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13196
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#12289;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#35797;&#26465;&#20214;&#29420;&#31435;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#25511;&#21046;&#26041;&#27861;&#26469;&#32416;&#27491;&#27979;&#35797;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#12289;&#22522;&#20110;&#26680;&#30340;&#32479;&#35745;&#27979;&#35797;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#35797;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#30340;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#33719;&#24471;&#27491;&#30830;&#30340;&#27979;&#35797;&#27700;&#24179;&#65288;&#25351;&#23450;&#30340;&#38169;&#35823;&#38451;&#24615;&#29575;&#19978;&#38480;&#65289;&#65292;&#21516;&#26102;&#20173;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#27979;&#35797;&#33021;&#21147;&#12290;&#36807;&#22810;&#30340;&#20551;&#38451;&#24615;&#26159;&#30001;&#20110;&#27979;&#35797;&#32479;&#35745;&#37327;&#20013;&#30340;&#20559;&#24046;&#24341;&#36215;&#30340;&#65292;&#35813;&#32479;&#35745;&#37327;&#26159;&#20351;&#29992;&#38750;&#21442;&#25968;&#26680;&#23725;&#22238;&#24402;&#33719;&#24471;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#20559;&#24046;&#25511;&#21046;&#26041;&#27861;&#26469;&#20462;&#27491;&#27979;&#35797;&#27700;&#24179;&#65292;&#22522;&#20110;&#25968;&#25454;&#20998;&#21106;&#12289;&#36741;&#21161;&#25968;&#25454;&#65292;&#20197;&#21450;&#65288;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#65289;&#26356;&#31616;&#21333;&#30340;&#20989;&#25968;&#31867;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#32452;&#21512;&#31574;&#30053;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13196v1 Announce Type: new  Abstract: We describe a data-efficient, kernel-based approach to statistical testing of conditional independence. A major challenge of conditional independence testing, absent in tests of unconditional independence, is to obtain the correct test level (the specified upper bound on the rate of false positives), while still attaining competitive test power. Excess false positives arise due to bias in the test statistic, which is obtained using nonparametric kernel ridge regression. We propose three methods for bias control to correct the test level, based on data splitting, auxiliary data, and (where possible) simpler function classes. We show these combined strategies are effective both for synthetic and real-world data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2401.17177</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Discovery of PDEs via the Adjoint Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#26469;&#21457;&#29616;&#32473;&#23450;&#25968;&#25454;&#30340;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#20197;&#19968;&#33324;&#24418;&#24335;&#32771;&#34385;&#21442;&#25968;&#21270;&#30340;PDE&#65292;&#24182;&#21046;&#23450;&#26368;&#23567;&#21270;PDE&#35299;&#19982;&#25968;&#25454;&#35823;&#24046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#21464;&#20998;&#35745;&#31639;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65288;&#20276;&#38543;&#26041;&#31243;&#65289;&#30340;&#28436;&#21270;&#26041;&#31243;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#20986;&#19982;PDE&#21442;&#25968;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#19968;&#26063;&#21442;&#25968;&#21270;&#21644;&#38750;&#32447;&#24615;PDEs&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25512;&#23548;&#20986;&#30456;&#24212;&#30340;&#20276;&#38543;&#26041;&#31243;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#20276;&#38543;&#26041;&#27861;&#21487;&#20197;&#20197;&#26426;&#22120;&#31934;&#24230;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20276;&#38543;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;&#33879;&#21517;&#30340;PDE-FIND&#65288;Rudy et al., 2017&#65289;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form, and formulate the optimization problem that minimizes the error of PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, for a family of parameterized and nonlinear PDEs, we show how the corresponding adjoint equations can be derived. Here, we show that given smooth data set, the proposed adjoint method can recover the true PDE up to machine accuracy. However, in the presence of noise, the accuracy of the adjoint method becomes comparable to the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017). Even th
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2310.01259</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#20041;&#25512;&#29702;&#23454;&#29616;&#26356;&#24555;&#26356;&#20934;&#30830;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Faster and Accurate Neural Networks with Semantic Inference. (arXiv:2310.01259v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#32858;&#31867;&#35821;&#20041;&#30456;&#20284;&#30340;&#31867;&#26469;&#25552;&#21462;&#23376;&#22270;&#65292;&#20174;&#32780;&#20943;&#23569;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#36127;&#25285;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#21098;&#26525;&#21644;&#19987;&#38376;&#29992;&#20110;&#31227;&#21160;&#35774;&#22791;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20250;&#23548;&#33268;&#26126;&#26174;&#30340;&#20934;&#30830;&#29575;&#25439;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#20869;&#22312;&#20887;&#20313;&#26469;&#20943;&#23569;&#35745;&#31639;&#36127;&#36733;&#65292;&#24182;&#22312;&#24615;&#33021;&#19978;&#26377;&#38480;&#25439;&#22833;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#36755;&#20837;&#20849;&#20139;&#35768;&#22810;&#28388;&#27874;&#22120;&#65292;&#23588;&#20854;&#26159;&#22312;&#36739;&#26089;&#30340;&#23618;&#27425;&#19978;&#12290;&#22240;&#27492;&#65292;&#21487;&#20197;&#23545;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#31867;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#21019;&#24314;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#25512;&#29702;&#65288;SINF&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#31616;&#32780;&#35328;&#20043;&#65292;SINF&#65288;i&#65289;&#20351;&#29992;&#19968;&#20010;&#23567;&#30340;&#38468;&#21152;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#23545;&#35937;&#23646;&#20110;&#30340;&#35821;&#20041;&#32858;&#31867;&#65292;&#24182;&#65288;ii&#65289;&#25191;&#34892;&#19982;&#35813;&#35821;&#20041;&#32858;&#31867;&#30456;&#20851;&#30340;&#22522;&#26412;DNN&#25552;&#21462;&#30340;&#23376;&#22270;&#36827;&#34892;&#25512;&#29702;&#12290;&#20026;&#20102;&#25552;&#21462;&#27599;&#20010;&#29305;&#23450;&#20110;&#32858;&#31867;&#30340;&#23376;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#21306;&#20998;&#33021;&#21147;&#24471;&#20998;&#65288;DCS&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#20855;&#26377;&#21306;&#20998;&#33021;&#21147;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNN) usually come with a significant computational burden. While approaches such as structured pruning and mobile-specific DNNs have been proposed, they incur drastic accuracy loss. In this paper we leverage the intrinsic redundancy in latent representations to reduce the computational load with limited loss in performance. We show that semantically similar inputs share many filters, especially in the earlier layers. Thus, semantically similar classes can be clustered to create cluster-specific subgraphs. To this end, we propose a new framework called Semantic Inference (SINF). In short, SINF (i) identifies the semantic cluster the object belongs to using a small additional classifier and (ii) executes the subgraph extracted from the base DNN related to that semantic cluster for inference. To extract each cluster-specific subgraph, we propose a new approach named Discriminative Capability Score (DCS) that finds the subgraph with the capability to discriminate amon
&lt;/p&gt;</description></item><item><title>LieDetect&#26159;&#19968;&#31181;&#20174;&#32039;&#33268;Lie&#32676;&#30340;&#26377;&#38480;&#26679;&#26412;&#36712;&#36947;&#20013;&#20272;&#35745;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;&#19982;&#20854;&#20182;&#25216;&#26415;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26816;&#32034;&#31934;&#30830;&#30340;&#34920;&#31034;&#31867;&#22411;&#65292;&#24182;&#37325;&#24314;&#20854;&#36712;&#36947;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#35813;&#20316;&#29992;&#30340;Lie&#32676;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32039;&#33268;Lie&#32676;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03086</link><description>&lt;p&gt;
LieDetect: &#20174;&#28857;&#20113;&#20013;&#26816;&#27979;&#32039;&#33268;Lie&#32676;&#30340;&#34920;&#31034;&#36712;&#36947;
&lt;/p&gt;
&lt;p&gt;
LieDetect: Detection of representation orbits of compact Lie groups from point clouds. (arXiv:2309.03086v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03086
&lt;/p&gt;
&lt;p&gt;
LieDetect&#26159;&#19968;&#31181;&#20174;&#32039;&#33268;Lie&#32676;&#30340;&#26377;&#38480;&#26679;&#26412;&#36712;&#36947;&#20013;&#20272;&#35745;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;&#19982;&#20854;&#20182;&#25216;&#26415;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26816;&#32034;&#31934;&#30830;&#30340;&#34920;&#31034;&#31867;&#22411;&#65292;&#24182;&#37325;&#24314;&#20854;&#36712;&#36947;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#35813;&#20316;&#29992;&#30340;Lie&#32676;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32039;&#33268;Lie&#32676;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32039;&#33268;Lie&#32676;&#30340;&#26377;&#38480;&#26679;&#26412;&#36712;&#36947;&#20013;&#20272;&#35745;&#34920;&#31034;&#12290;&#19982;&#20854;&#20182;&#25253;&#36947;&#30340;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#26816;&#32034;&#31934;&#30830;&#30340;&#34920;&#31034;&#31867;&#22411;&#65292;&#20316;&#20026;&#19981;&#21487;&#32422;&#34920;&#31034;&#30340;&#30452;&#21644;&#12290;&#32780;&#19988;&#65292;&#23545;&#34920;&#31034;&#31867;&#22411;&#30340;&#20102;&#35299;&#21487;&#20197;&#37325;&#24314;&#20854;&#36712;&#36947;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#35813;&#20316;&#29992;&#30340;Lie&#32676;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32039;&#33268;Lie&#32676;&#65292;&#20294;&#21482;&#32771;&#34385;&#20102;SO(2), T^d, SU(2)&#21644;SO(3)&#30340;&#23454;&#20363;&#21270;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;Hausdorff&#21644;Wasserstein&#36317;&#31163;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#26469;&#33258;&#20110;&#20960;&#20309;&#27979;&#24230;&#29702;&#35770;&#65292;&#35745;&#31639;&#20960;&#20309;&#21644;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#12290;&#31639;&#27861;&#22312;&#39640;&#36798;16&#32500;&#30340;&#21512;&#25104;&#25968;&#25454;&#20197;&#21450;&#22270;&#20687;&#20998;&#26512;&#65292;&#35856;&#27874;&#20998;&#26512;&#21644;&#32463;&#20856;&#21147;&#23398;&#31995;&#32479;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21462;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We suggest a new algorithm to estimate representations of compact Lie groups from finite samples of their orbits. Different from other reported techniques, our method allows the retrieval of the precise representation type as a direct sum of irreducible representations. Moreover, the knowledge of the representation type permits the reconstruction of its orbit, which is useful to identify the Lie group that generates the action. Our algorithm is general for any compact Lie group, but only instantiations for SO(2), T^d, SU(2) and SO(3) are considered. Theoretical guarantees of robustness in terms of Hausdorff and Wasserstein distances are derived. Our tools are drawn from geometric measure theory, computational geometry, and optimization on matrix manifolds. The algorithm is tested for synthetic data up to dimension 16, as well as real-life applications in image analysis, harmonic analysis, and classical mechanics systems, achieving very accurate results.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312;MRO&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16189</link><description>&lt;p&gt;
&#28779;&#26143;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#65306;&#19968;&#31181;&#22810;&#23610;&#24230;&#23884;&#22871;&#26041;&#27861;&#20013;&#30340;&#22240;&#23376;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Martian time-series unraveled: A multi-scale nested approach with factorial variational autoencoders. (arXiv:2305.16189v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16189
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#29992;&#20110;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#65292;&#24182;&#22312;MRO&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#30340;&#28304;&#20998;&#31163;&#28041;&#21450;&#36890;&#36807;&#28151;&#21512;&#25805;&#20316;&#35760;&#24405;&#30340;&#26410;&#30693;&#28304;&#20449;&#21495;&#30340;&#20998;&#35299;&#65292;&#20854;&#20013;&#23545;&#28304;&#30340;&#20808;&#39564;&#30693;&#35782;&#26377;&#38480;&#65292;&#20165;&#21487;&#20197;&#35775;&#38382;&#20449;&#21495;&#28151;&#21512;&#25968;&#25454;&#38598;&#12290;&#36825;&#20010;&#38382;&#39064;&#26412;&#36136;&#19978;&#26159;&#19981;&#36866;&#29992;&#30340;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#21463;&#21040;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#28304;&#23637;&#29616;&#20986;&#30340;&#22810;&#31181;&#26102;&#38388;&#23610;&#24230;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#22810;&#23610;&#24230;&#32858;&#31867;&#21644;&#28304;&#20998;&#31163;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#23567;&#27874;&#25955;&#23556;&#21327;&#26041;&#24046;&#26469;&#25552;&#20379;&#38543;&#26426;&#36807;&#31243;&#30340;&#20302;&#32500;&#34920;&#31034;&#65292;&#33021;&#22815;&#21306;&#20998;&#19981;&#21516;&#30340;&#38750;&#39640;&#26031;&#38543;&#26426;&#36807;&#31243;&#12290;&#22312;&#36825;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22240;&#23376;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#23427;&#34987;&#35757;&#32451;&#29992;&#20110;(1)&#27010;&#29575;&#22320;&#23545;&#19981;&#21516;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#28304;&#36827;&#34892;&#32858;&#31867;&#21644;&#36880;&#23618;&#38750;&#30417;&#30563;&#28304;&#20998;&#31163;&#65292;(2)&#22312;&#27599;&#20010;&#26102;&#38388;&#23610;&#24230;&#19978;&#25552;&#21462;&#20302;&#32500;&#34920;&#31034;&#65292;(3)&#23398;&#20064;&#28304;&#20449;&#21495;&#30340;&#22240;&#23376;&#34920;&#31034;&#65292;(4)&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#20197;&#29983;&#25104;&#26410;&#30693;&#28304;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;MRO&#19978;&#30340;&#19977;&#20010;&#39057;&#36947;&#30340;&#21487;&#35265;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised source separation involves unraveling an unknown set of source signals recorded through a mixing operator, with limited prior knowledge about the sources, and only access to a dataset of signal mixtures. This problem is inherently ill-posed and is further challenged by the variety of time-scales exhibited by sources in time series data. Existing methods typically rely on a preselected window size that limits their capacity to handle multi-scale sources. To address this issue, instead of operating in the time domain, we propose an unsupervised multi-scale clustering and source separation framework by leveraging wavelet scattering covariances that provide a low-dimensional representation of stochastic processes, capable of distinguishing between different non-Gaussian stochastic processes. Nested within this representation space, we develop a factorial Gaussian-mixture variational autoencoder that is trained to (1) probabilistically cluster sources at different time-scales a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.10523</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35299;&#37322;&#24615;&#22522;&#30784;&#25277;&#21462;&#29992;&#20110;&#22522;&#20110;&#27010;&#24565;&#30340;&#35270;&#35273;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Interpretable Basis Extraction for Concept-Based Visual Explanations. (arXiv:2303.10523v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;CNN&#36827;&#34892;&#36716;&#25442;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#37322;&#20013;&#38388;&#23618;&#30340;&#34920;&#31034;&#65292;&#25552;&#21462;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#32593;&#32476;&#32467;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#37117;&#24456;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#29992;&#20154;&#31867;&#21487;&#20197;&#29702;&#35299;&#30340;&#27010;&#24565;&#26469;&#35299;&#37322;CNN&#22270;&#20687;&#20998;&#31867;&#22120;&#39044;&#27979;&#21644;&#20013;&#38388;&#23618;&#34920;&#31034;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#21518;&#22788;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#26597;&#25214;&#35299;&#37322;&#20687;&#32032;&#28608;&#27963;&#30340;&#31232;&#30095;&#20108;&#20540;&#21270;&#36716;&#25442;&#34920;&#31034;&#30340;&#29305;&#24449;&#31354;&#38388;&#26059;&#36716;&#26469;&#25552;&#21462;&#35299;&#37322;&#24615;&#27424;&#23436;&#22791;&#22522;&#30784;&#12290;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#27969;&#34892;CNN&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#32593;&#32476;&#26550;&#26500;&#21644;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#25552;&#21462;&#35299;&#37322;&#24615;&#22522;&#30784;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#25991;&#29486;&#20013;&#30340;&#22522;&#30784;&#21487;&#35299;&#37322;&#24615;&#24230;&#37327;&#65292;&#24182;&#34920;&#26126;&#65292;&#24403;&#20013;&#38388;&#23618;&#34920;&#31034;&#34987;&#36716;&#25442;&#20026;&#25105;&#20204;&#26041;&#27861;&#25552;&#21462;&#30340;&#22522;&#30784;&#26102;&#65292;&#23427;&#20204;&#21464;&#24471;&#26356;&#26131;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
An important line of research attempts to explain CNN image classifier predictions and intermediate layer representations in terms of human understandable concepts. In this work, we expand on previous works in the literature that use annotated concept datasets to extract interpretable feature space directions and propose an unsupervised post-hoc method to extract a disentangling interpretable basis by looking for the rotation of the feature space that explains sparse one-hot thresholded transformed representations of pixel activations. We do experimentation with existing popular CNNs and demonstrate the effectiveness of our method in extracting an interpretable basis across network architectures and training datasets. We make extensions to the existing basis interpretability metrics found in the literature and show that, intermediate layer representations become more interpretable when transformed to the bases extracted with our method. Finally, using the basis interpretability metrics
&lt;/p&gt;</description></item></channel></rss>