<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>CrystalFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21463;&#31354;&#38388;&#32676;&#25511;&#21046;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#65292;&#23427;&#36890;&#36807;&#39044;&#27979;&#21333;&#20301;&#32990;&#20013;&#23545;&#31216;&#19981;&#31561;&#20215;&#21407;&#23376;&#30340;&#31181;&#31867;&#21644;&#20301;&#32622;&#26469;&#29983;&#25104;&#26230;&#20307;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#31561;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2403.15734</link><description>&lt;p&gt;
&#22522;&#20110;&#31354;&#38388;&#32676;&#20449;&#24687;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Space Group Informed Transformer for Crystalline Materials Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15734
&lt;/p&gt;
&lt;p&gt;
CrystalFormer&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21463;&#31354;&#38388;&#32676;&#25511;&#21046;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#65292;&#23427;&#36890;&#36807;&#39044;&#27979;&#21333;&#20301;&#32990;&#20013;&#23545;&#31216;&#19981;&#31561;&#20215;&#21407;&#23376;&#30340;&#31181;&#31867;&#21644;&#20301;&#32622;&#26469;&#29983;&#25104;&#26230;&#20307;&#65292;&#24182;&#22312;&#26377;&#25928;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#31561;&#26041;&#38754;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#24615;&#33021;&#30456;&#21305;&#37197;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;CrystalFormer&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21463;&#31354;&#38388;&#32676;&#25511;&#21046;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#12290;&#31354;&#38388;&#32676;&#23545;&#31216;&#24615;&#26174;&#33879;&#31616;&#21270;&#20102;&#26230;&#20307;&#31354;&#38388;&#65292;&#36825;&#23545;&#20110;&#25968;&#25454;&#21644;&#35745;&#31639;&#26377;&#25928;&#30340;&#26230;&#20307;&#26448;&#26009;&#29983;&#25104;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#21033;&#29992;Wyckoff&#20301;&#32622;&#30340;&#26174;&#33879;&#31163;&#25955;&#21644;&#39034;&#24207;&#29305;&#24615;&#65292;CrystalFormer&#23398;&#20250;&#20102;&#36890;&#36807;&#30452;&#25509;&#39044;&#27979;&#21333;&#20301;&#32990;&#20013;&#23545;&#31216;&#19981;&#31561;&#20215;&#21407;&#23376;&#30340;&#31181;&#31867;&#21644;&#20301;&#32622;&#26469;&#29983;&#25104;&#26230;&#20307;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CrystalFormer&#22312;&#29983;&#25104;&#30340;&#26230;&#20307;&#26448;&#26009;&#30340;&#26377;&#25928;&#24615;&#12289;&#26032;&#39062;&#24615;&#21644;&#31283;&#23450;&#24615;&#26041;&#38754;&#19982;&#26631;&#20934;&#22522;&#20934;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#34920;&#26126;&#65292;CrystalFormer&#20174;&#25968;&#25454;&#20013;&#21560;&#25910;&#20102;&#21512;&#29702;&#30340;&#22266;&#20307;&#21270;&#23398;&#20449;&#24687;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#12290;CrystalFormer&#32479;&#19968;&#20102;&#22522;&#20110;&#23545;&#31216;&#24615;&#30340;&#32467;&#26500;&#25628;&#32034;&#21644;&#29983;&#25104;&#24615;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15734v1 Announce Type: cross  Abstract: We introduce CrystalFormer, a transformer-based autoregressive model specifically designed for space group-controlled generation of crystalline materials. The space group symmetry significantly simplifies the crystal space, which is crucial for data and compute efficient generative modeling of crystalline materials. Leveraging the prominent discrete and sequential nature of the Wyckoff positions, CrystalFormer learns to generate crystals by directly predicting the species and locations of symmetry-inequivalent atoms in the unit cell. Our results demonstrate that CrystalFormer matches state-of-the-art performance on standard benchmarks for both validity, novelty, and stability of the generated crystalline materials. Our analysis also shows that CrystalFormer ingests sensible solid-state chemistry information from data for generative modeling. The CrystalFormer unifies symmetry-based structure search and generative pre-training in the re
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2402.07586</link><description>&lt;p&gt;
&#25581;&#31034;&#29305;&#23450;&#32676;&#20307;&#30340;&#20998;&#24067;&#24335;&#27010;&#24565;&#28418;&#31227;: &#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#35201;&#27714;
&lt;/p&gt;
&lt;p&gt;
Unveiling Group-Specific Distributed Concept Drift: A Fairness Imperative in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07586
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#65292;&#39318;&#27425;&#25506;&#32034;&#20102;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#30340;&#25361;&#25112;&#21644;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#20013;&#65292;&#30830;&#20445;&#20844;&#24179;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#20851;&#27880;&#28857;&#65292;&#25512;&#21160;&#20102;&#24320;&#21457;&#26088;&#22312;&#20943;&#23569;&#20915;&#31574;&#36807;&#31243;&#20013;&#27495;&#35270;&#32467;&#26524;&#30340;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20844;&#24179;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#22312;&#36825;&#26041;&#38754;&#30340;&#24320;&#25299;&#24615;&#21162;&#21147;&#12290;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#26159;&#25351;&#19968;&#20010;&#32676;&#20307;&#38543;&#26102;&#38388;&#32463;&#21382;&#27010;&#24565;&#28418;&#31227;&#65292;&#32780;&#21478;&#19968;&#20010;&#32676;&#20307;&#21364;&#27809;&#26377;&#65292;&#23548;&#33268;&#20844;&#24179;&#24615;&#19979;&#38477;&#65292;&#21363;&#20351;&#20934;&#30830;&#24615;&#20445;&#25345;&#30456;&#23545;&#31283;&#23450;&#12290;&#22312;&#32852;&#37030;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#65292;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;&#27169;&#22411;&#65292;&#20854;&#20998;&#24067;&#24335;&#24615;&#36136;&#36827;&#19968;&#27493;&#25918;&#22823;&#20102;&#36825;&#20123;&#25361;&#25112;&#65292;&#22240;&#20026;&#27599;&#20010;&#23458;&#25143;&#31471;&#21487;&#20197;&#29420;&#31435;&#32463;&#21382;&#29305;&#23450;&#32676;&#20307;&#30340;&#27010;&#24565;&#28418;&#31227;&#65292;&#21516;&#26102;&#20173;&#20849;&#20139;&#30456;&#21516;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#20174;&#32780;&#21019;&#36896;&#20102;&#19968;&#20010;&#22797;&#26434;&#32780;&#21160;&#24577;&#30340;&#29615;&#22659;&#26469;&#32500;&#25345;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#20043;&#19968;&#26159;&#23545;&#32676;&#20307;&#29305;&#23450;&#30340;&#27010;&#24565;&#28418;&#31227;&#36827;&#34892;&#24418;&#24335;&#21270;&#21644;&#20869;&#37096;&#21270;&#30340;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the evolving field of machine learning, ensuring fairness has become a critical concern, prompting the development of algorithms designed to mitigate discriminatory outcomes in decision-making processes. However, achieving fairness in the presence of group-specific concept drift remains an unexplored frontier, and our research represents pioneering efforts in this regard. Group-specific concept drift refers to situations where one group experiences concept drift over time while another does not, leading to a decrease in fairness even if accuracy remains fairly stable. Within the framework of federated learning, where clients collaboratively train models, its distributed nature further amplifies these challenges since each client can experience group-specific concept drift independently while still sharing the same underlying concept, creating a complex and dynamic environment for maintaining fairness. One of the significant contributions of our research is the formalization and intr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#21387;&#32553;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05675</link><description>&lt;p&gt;
&#21387;&#32553;&#25968;&#25454;&#38598;&#30340;&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#26377;&#25928;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Adversarial Training with Compressed Datasets Effective?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05675
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#21387;&#32553;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#65288;DC&#65289;&#26159;&#25351;&#20174;&#36739;&#22823;&#25968;&#25454;&#38598;&#20013;&#29983;&#25104;&#36739;&#23567;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#19968;&#31867;&#26368;&#36817;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#36825;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#20445;&#30041;&#20102;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#22522;&#26412;&#20449;&#24687;&#65292;&#20351;&#24471;&#22312;&#20854;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;&#22312;&#23436;&#25972;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30446;&#21069;&#22823;&#22810;&#25968;&#30340;DC&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#22914;&#20309;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#39044;&#31639;&#19979;&#23454;&#29616;&#39640;&#27979;&#35797;&#24615;&#33021;&#65292;&#24182;&#27809;&#26377;&#30452;&#25509;&#35299;&#20915;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;DC&#26041;&#27861;&#33719;&#24471;&#30340;&#21387;&#32553;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#27809;&#26377;&#26377;&#25928;&#30340;&#20256;&#36882;&#24615;&#12290;&#20026;&#20102;&#21516;&#26102;&#25552;&#39640;&#25968;&#25454;&#38598;&#21387;&#32553;&#25928;&#29575;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23547;&#25214;&#25968;&#25454;&#38598;&#30340;&#26368;&#23567;&#26377;&#38480;&#35206;&#30422;&#65288;MFC&#65289;&#30340;&#26032;&#22411;&#40065;&#26834;&#24615;&#24863;&#30693;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dataset Condensation (DC) refers to the recent class of dataset compression methods that generate a smaller, synthetic, dataset from a larger dataset. This synthetic dataset retains the essential information of the original dataset, enabling models trained on it to achieve performance levels comparable to those trained on the full dataset. Most current DC methods have mainly concerned with achieving high test performance with limited data budget, and have not directly addressed the question of adversarial robustness. In this work, we investigate the impact of adversarial robustness on models trained with compressed datasets. We show that the compressed datasets obtained from DC methods are not effective in transferring adversarial robustness to models. As a solution to improve dataset compression efficiency and adversarial robustness simultaneously, we propose a novel robustness-aware dataset compression method based on finding the Minimal Finite Covering (MFC) of the dataset. The prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2210.13533</link><description>&lt;p&gt;
&#20998;&#24067;&#36716;&#31227;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sufficient Invariant Learning for Distribution Shift. (arXiv:2210.13533v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#20805;&#20998;&#19981;&#21464;&#23398;&#20064;&#65292;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#23398;&#20064;&#20102;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25351;&#20986;&#22312;&#20998;&#24067;&#36716;&#31227;&#26102;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#36866;&#29992;&#20110;&#27979;&#35797;&#38598;&#65292;&#38480;&#21046;&#20102;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#30340;&#20998;&#24067;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#65292;&#20445;&#35777;&#24615;&#33021;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25913;&#21892;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36328;&#32452;&#25110;&#39046;&#22495;&#30340;&#19981;&#21464;&#29305;&#24449;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20043;&#21069;&#30340;&#24037;&#20316;&#21482;&#37096;&#20998;&#22320;&#23398;&#20064;&#20102;&#19981;&#21464;&#29305;&#24449;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#24037;&#20316;&#20391;&#37325;&#20110;&#26377;&#38480;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#20294;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#20805;&#20998;&#19981;&#21464;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#30001;&#20110;&#21482;&#26377;&#35757;&#32451;&#38598;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#37096;&#20998;&#19981;&#21464;&#29305;&#24449;&#21487;&#33021;&#19981;&#23384;&#22312;&#20110;&#20998;&#24067;&#36716;&#31227;&#26102;&#30340;&#27979;&#35797;&#38598;&#20013;&#12290;&#22240;&#27492;&#65292;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#24615;&#33021;&#25552;&#39640;&#21487;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#35748;&#20026;&#20174;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#20805;&#20998;&#30340;&#19981;&#21464;&#29305;&#24449;&#23545;&#20110;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms have shown remarkable performance in diverse applications. However, it is still challenging to guarantee performance in distribution shifts when distributions of training and test datasets are different. There have been several approaches to improve the performance in distribution shift cases by learning invariant features across groups or domains. However, we observe that the previous works only learn invariant features partially. While the prior works focus on the limited invariant features, we first raise the importance of the sufficient invariant features. Since only training sets are given empirically, the learned partial invariant features from training sets might not be present in the test sets under distribution shift. Therefore, the performance improvement on distribution shifts might be limited. In this paper, we argue that learning sufficient invariant features from the training set is crucial for the distribution shift case. Concretely, we newly 
&lt;/p&gt;</description></item></channel></rss>