<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#40723;&#21169;&#22810;&#27169;&#24577;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21333;&#27169;&#24577;&#27169;&#22411;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02359</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33539;&#24335;&#30340;&#24402;&#22240;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Attribution Regularization for Multimodal Paradigms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#40723;&#21169;&#22810;&#27169;&#24577;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21333;&#27169;&#24577;&#27169;&#22411;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#25972;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#20197;&#22686;&#24378;&#23398;&#20064;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35266;&#23519;&#21040;&#21333;&#27169;&#24577;&#27169;&#22411;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#23613;&#31649;&#21518;&#32773;&#21487;&#20197;&#35775;&#38382;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#21333;&#20010;&#27169;&#24577;&#30340;&#24433;&#21709;&#24120;&#24120;&#20027;&#23548;&#20915;&#31574;&#36807;&#31243;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#20010;&#30740;&#31350;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#39033;&#40723;&#21169;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#20570;&#20986;&#20915;&#31574;&#26102;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#35813;&#39033;&#30446;&#30340;&#37325;&#28857;&#22312;&#20110;&#35270;&#39057;-&#38899;&#39057;&#39046;&#22495;&#65292;&#23613;&#31649;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#22312;&#28041;&#21450;&#22810;&#20010;&#27169;&#24577;&#30340;&#20307;&#29616;AI&#30740;&#31350;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02359v1 Announce Type: new  Abstract: Multimodal machine learning has gained significant attention in recent years due to its potential for integrating information from multiple modalities to enhance learning and decision-making processes. However, it is commonly observed that unimodal models outperform multimodal models, despite the latter having access to richer information. Additionally, the influence of a single modality often dominates the decision-making process, resulting in suboptimal performance. This research project aims to address these challenges by proposing a novel regularization term that encourages multimodal models to effectively utilize information from all modalities when making decisions. The focus of this project lies in the video-audio domain, although the proposed regularization technique holds promise for broader applications in embodied AI research, where multiple modalities are involved. By leveraging this regularization term, the proposed approach
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02353</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#35821;&#20041;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Semantic Augmentation in Images using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02353
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#32570;&#20047;&#36825;&#20123;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38480;&#21046;&#20854;&#27867;&#21270;&#21040;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21033;&#29992;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02353v1 Announce Type: cross  Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20043;&#38388;&#30340;&#39118;&#38505;&#36317;&#31163;&#27010;&#24565;&#65292;&#36890;&#36807;&#39118;&#38505;&#36317;&#31163;&#21487;&#20197;&#37327;&#21270;&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21464;&#21270;&#65292;&#24182;&#25506;&#32034;&#20102;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.01660</link><description>&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#20960;&#20309;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Geometry and Stability of Supervised Learning Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01660
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20043;&#38388;&#30340;&#39118;&#38505;&#36317;&#31163;&#27010;&#24565;&#65292;&#36890;&#36807;&#39118;&#38505;&#36317;&#31163;&#21487;&#20197;&#37327;&#21270;&#38382;&#39064;&#30340;&#31283;&#23450;&#24615;&#21464;&#21270;&#65292;&#24182;&#25506;&#32034;&#20102;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20043;&#38388;&#30340;&#36317;&#31163;&#27010;&#24565;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39118;&#38505;&#36317;&#31163;&#12290;&#36825;&#31181;&#20197;&#26368;&#20248;&#20256;&#36755;&#20026;&#28789;&#24863;&#30340;&#36317;&#31163;&#20419;&#36827;&#20102;&#31283;&#23450;&#24615;&#32467;&#26524;&#65307;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#38480;&#21046;&#36825;&#20123;&#20462;&#25913;&#21487;&#20197;&#23558;&#38382;&#39064;&#31227;&#21160;&#22810;&#23569;&#26469;&#37327;&#21270;&#35832;&#22914;&#37319;&#26679;&#20559;&#24046;&#12289;&#22122;&#22768;&#12289;&#26377;&#38480;&#25968;&#25454;&#21644;&#36924;&#36817;&#31561;&#38382;&#39064;&#22312;&#39118;&#38505;&#36317;&#31163;&#19979;&#22914;&#20309;&#25913;&#21464;&#32473;&#23450;&#38382;&#39064;&#12290;&#22312;&#24314;&#31435;&#20102;&#36317;&#31163;&#20043;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20135;&#29983;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#27979;&#22320;&#32447;&#24182;&#35777;&#26126;&#20998;&#31867;&#38382;&#39064;&#38598;&#22312;&#26356;&#22823;&#31867;&#30340;&#38382;&#39064;&#20013;&#26159;&#23494;&#38598;&#30340;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#39118;&#38505;&#36317;&#31163;&#30340;&#20004;&#20010;&#21464;&#20307;&#65306;&#19968;&#20010;&#22312;&#38382;&#39064;&#30340;&#39044;&#27979;&#21464;&#37327;&#19978;&#32467;&#21512;&#20102;&#25351;&#23450;&#30340;&#26435;&#37325;&#65292;&#21478;&#19968;&#20010;&#23545;&#38382;&#39064;&#30340;&#39118;&#38505;&#26223;&#35266;&#36718;&#24275;&#26356;&#20026;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01660v1 Announce Type: new  Abstract: We introduce a notion of distance between supervised learning problems, which we call the Risk distance. This optimal-transport-inspired distance facilitates stability results; one can quantify how seriously issues like sampling bias, noise, limited data, and approximations might change a given problem by bounding how much these modifications can move the problem under the Risk distance. With the distance established, we explore the geometry of the resulting space of supervised learning problems, providing explicit geodesics and proving that the set of classification problems is dense in a larger class of problems. We also provide two variants of the Risk distance: one that incorporates specified weights on a problem's predictors, and one that is more sensitive to the contours of a problem's risk landscape.
&lt;/p&gt;</description></item></channel></rss>