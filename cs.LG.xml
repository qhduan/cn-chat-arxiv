<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SMART&#21098;&#26525;&#22120;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#25513;&#30721;&#12289;&#21487;&#24494;&#30340;Top k&#36816;&#31639;&#31526;&#21644;&#21160;&#24577;&#28201;&#24230;&#21442;&#25968;&#25216;&#24039;&#65292;&#22312;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19969</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#30340;&#20998;&#31163;&#12289;&#21160;&#24577;&#21644;&#21487;&#24494;&#65288;SMART&#65289;&#21098;&#26525;&#22120;
&lt;/p&gt;
&lt;p&gt;
Separate, Dynamic and Differentiable (SMART) Pruner for Block/Output Channel Pruning on Computer Vision Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19969
&lt;/p&gt;
&lt;p&gt;
SMART&#21098;&#26525;&#22120;&#24341;&#20837;&#20102;&#29420;&#31435;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#25513;&#30721;&#12289;&#21487;&#24494;&#30340;Top k&#36816;&#31639;&#31526;&#21644;&#21160;&#24577;&#28201;&#24230;&#21442;&#25968;&#25216;&#24039;&#65292;&#22312;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21098;&#26525;&#24050;&#34987;&#35270;&#20026;&#20943;&#23567;&#27169;&#22411;&#22823;&#23567;&#12289;&#25913;&#21892;&#25512;&#29702;&#24310;&#36831;&#20197;&#21450;&#38477;&#20302;DNN&#21152;&#36895;&#22120;&#21151;&#32791;&#30340;&#20851;&#38190;&#31574;&#30053;&#12290;&#22312;&#21508;&#31181;&#21098;&#26525;&#25216;&#26415;&#20013;&#65292;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#22312;&#21152;&#36895;&#30828;&#20214;&#24615;&#33021;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20934;&#30830;&#24615;&#36890;&#24120;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#31163;&#12289;&#21160;&#24577;&#21644;&#21487;&#24494;&#65288;SMART&#65289;&#21098;&#26525;&#22120;&#12290;&#35813;&#21098;&#26525;&#22120;&#21033;&#29992;&#19968;&#20010;&#29420;&#31435;&#30340;&#12289;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#25513;&#30721;&#36827;&#34892;&#26435;&#37325;&#37325;&#35201;&#24615;&#25490;&#24207;&#65292;&#37319;&#29992;&#21487;&#24494;&#30340;Top k&#36816;&#31639;&#31526;&#26469;&#23454;&#29616;&#30446;&#26631;&#31232;&#30095;&#24615;&#65292;&#24182;&#21033;&#29992;&#21160;&#24577;&#28201;&#24230;&#21442;&#25968;&#25216;&#24039;&#26469;&#36867;&#31163;&#38750;&#31232;&#30095;&#23616;&#37096;&#26497;&#23567;&#20540;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;SMART&#21098;&#26525;&#22120;&#22312;&#22359;&#21644;&#36755;&#20986;&#36890;&#36947;&#21098;&#26525;&#30340;&#21508;&#31181;&#20219;&#21153;&#21644;&#27169;&#22411;&#19978;&#19968;&#30452;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19969v1 Announce Type: cross  Abstract: Deep Neural Network (DNN) pruning has emerged as a key strategy to reduce model size, improve inference latency, and lower power consumption on DNN accelerators. Among various pruning techniques, block and output channel pruning have shown significant potential in accelerating hardware performance. However, their accuracy often requires further improvement. In response to this challenge, we introduce a separate, dynamic and differentiable (SMART) pruner. This pruner stands out by utilizing a separate, learnable probability mask for weight importance ranking, employing a differentiable Top k operator to achieve target sparsity, and leveraging a dynamic temperature parameter trick to escape from non-sparse local minima. In our experiments, the SMART pruner consistently demonstrated its superiority over existing pruning methods across a wide range of tasks and models on block and output channel pruning. Additionally, we extend our testing
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;PDNGraph&#22270;&#32467;&#26500;&#21644;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;PDNNet&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;PDN&#32467;&#26500;&#21644;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;IR&#25481;&#30005;&#12290;</title><link>https://arxiv.org/abs/2403.18569</link><description>&lt;p&gt;
PDNNet&#65306;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
PDNNet: PDN-Aware GNN-CNN Heterogeneous Network for Dynamic IR Drop Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#21160;&#24577;IR&#25481;&#30005;&#39044;&#27979;&#30340;PDN&#24863;&#30693;GNN-CNN&#24322;&#26500;&#32593;&#32476;&#65292;&#24341;&#20837;&#20102;PDNGraph&#22270;&#32467;&#26500;&#21644;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;PDNNet&#65292;&#20197;&#21516;&#26102;&#32771;&#34385;PDN&#32467;&#26500;&#21644;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;IR&#25481;&#30005;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#28304;&#20379;&#24212;&#32593;&#32476;&#65288;PDN&#65289;&#19978;&#30340;IR&#25481;&#30005;&#19982;PDN&#30340;&#37197;&#32622;&#21644;&#30005;&#27969;&#28040;&#32791;&#23494;&#20999;&#30456;&#20851;&#12290;&#38543;&#30528;&#38598;&#25104;&#30005;&#36335;&#65288;IC&#65289;&#35774;&#35745;&#30340;&#19981;&#26029;&#22686;&#22823;&#65292;&#21160;&#24577;IR&#25481;&#30005;&#20223;&#30495;&#21464;&#24471;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;IR&#25481;&#30005;&#39044;&#27979;&#34987;&#25506;&#32034;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#32771;&#34385;&#19981;&#20165;&#22914;&#20309;&#27491;&#30830;&#34920;&#31034;&#21333;&#20803;-PDN&#20851;&#31995;&#65292;&#36824;&#32771;&#34385;&#22914;&#20309;&#22312;&#29305;&#24449;&#32858;&#21512;&#36807;&#31243;&#20013;&#27169;&#25311;IR&#25481;&#30005;&#36981;&#24490;&#20854;&#29289;&#29702;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#32467;&#26500;&#65292;PDNGraph&#65292;&#32479;&#19968;&#20102;PDN&#32467;&#26500;&#21644;&#32454;&#31890;&#24230;&#21333;&#20803;-PDN&#20851;&#31995;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#20998;&#25903;&#24322;&#26500;&#32593;&#32476;&#65292;PDNNet&#65292;&#23558;&#20004;&#20010;&#24182;&#34892;&#30340;GNN-CNN&#20998;&#25903;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26377;&#21033;&#20110;&#25429;&#25417;&#19978;&#36848;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18569v1 Announce Type: cross  Abstract: IR drop on the power delivery network (PDN) is closely related to PDN's configuration and cell current consumption. As the integrated circuit (IC) design is growing larger, dynamic IR drop simulation becomes computationally unaffordable and machine learning based IR drop prediction has been explored as a promising solution. Although CNN-based methods have been adapted to IR drop prediction task in several works, the shortcomings of overlooking PDN configuration is non-negligible. In this paper, we consider not only how to properly represent cell-PDN relation, but also how to model IR drop following its physical nature in the feature aggregation procedure. Thus, we propose a novel graph structure, PDNGraph, to unify the representations of the PDN structure and the fine-grained cell-PDN relation. We further propose a dual-branch heterogeneous network, PDNNet, incorporating two parallel GNN-CNN branches to favorably capture the above feat
&lt;/p&gt;</description></item><item><title>&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36328;&#36234;PDEs&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#29992;&#28508;&#22312;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.17728</link><description>&lt;p&gt;
&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#26159;PDE&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Masked Autoencoders are PDE Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17728
&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36328;&#36234;PDEs&#65292;&#21487;&#20197;&#23398;&#20064;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#26377;&#29992;&#28508;&#22312;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#27714;&#35299;&#22120;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDE&#65289;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#20294;&#23454;&#29992;&#24615;&#30446;&#21069;&#21463;&#21040;&#20854;&#27867;&#21270;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290; PDE&#22312;&#24191;&#27867;&#30340;&#23610;&#24230;&#19978;&#28436;&#21464;&#24182;&#23637;&#31034;&#20986;&#22810;&#26679;&#21270;&#30340;&#34892;&#20026;&#65307;&#39044;&#27979;&#36825;&#20123;&#29616;&#35937;&#23558;&#38656;&#35201;&#23398;&#20064;&#36328;&#36234;&#21508;&#31181;&#36755;&#20837;&#30340;&#34920;&#31034;&#65292;&#36825;&#20123;&#36755;&#20837;&#21487;&#33021;&#28085;&#30422;&#19981;&#21516;&#30340;&#31995;&#25968;&#12289;&#20960;&#20309;&#22270;&#24418;&#25110;&#26041;&#31243;&#12290;&#20316;&#20026;&#36890;&#21521;&#21487;&#27867;&#21270;PDE&#24314;&#27169;&#30340;&#19968;&#27493;&#65292;&#25105;&#20204;&#20026;PDEs&#35843;&#25972;&#20102;&#25513;&#30721;&#39044;&#35757;&#32451;&#12290;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#36328;&#36234;PDEs&#65292;&#25513;&#30721;&#33258;&#21160;&#32534;&#30721;&#22120;&#21487;&#20197;&#23398;&#20064;&#26377;&#29992;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#20197;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#29305;&#21035;&#26159;&#65292;&#25513;&#30721;&#39044;&#35757;&#32451;&#21487;&#20197;&#25913;&#21892;&#31070;&#32463;&#27714;&#35299;&#22120;&#23545;&#26410;&#35265;&#26041;&#31243;&#30340;&#31995;&#25968;&#22238;&#24402;&#21644;&#26102;&#38388;&#27493;&#39588;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25513;&#30721;&#39044;&#35757;&#32451;&#33021;&#25104;&#20026;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#22411;&#12289;&#26410;&#26631;&#35760;&#21644;&#24322;&#26500;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#35268;&#27169;&#21270;&#30340;&#28508;&#22312;&#29289;&#29702;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17728v1 Announce Type: new  Abstract: Neural solvers for partial differential equations (PDEs) have great potential, yet their practicality is currently limited by their generalizability. PDEs evolve over broad scales and exhibit diverse behaviors; predicting these phenomena will require learning representations across a wide variety of inputs, which may encompass different coefficients, geometries, or equations. As a step towards generalizable PDE modeling, we adapt masked pretraining for PDEs. Through self-supervised learning across PDEs, masked autoencoders can learn useful latent representations for downstream tasks. In particular, masked pretraining can improve coefficient regression and timestepping performance of neural solvers on unseen equations. We hope that masked pretraining can emerge as a unifying method across large, unlabeled, and heterogeneous datasets to learn latent physics at scale.
&lt;/p&gt;</description></item><item><title>Calib3D&#26159;&#19968;&#20010;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22810;&#20010;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#34429;&#28982;&#20934;&#30830;&#20294;&#19981;&#21487;&#38752;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#23433;&#20840;&#20851;&#38190;&#30340;&#32972;&#26223;&#19979;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17010</link><description>&lt;p&gt;
Calib3D&#65306;&#26657;&#20934;&#27169;&#22411;&#20559;&#22909;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;3D&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17010
&lt;/p&gt;
&lt;p&gt;
Calib3D&#26159;&#19968;&#20010;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22810;&#20010;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#34429;&#28982;&#20934;&#30830;&#20294;&#19981;&#21487;&#38752;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#23433;&#20840;&#20851;&#38190;&#30340;&#32972;&#26223;&#19979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#38656;&#35201;&#30340;&#19981;&#20165;&#20165;&#26159;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#36824;&#38656;&#35201;&#26469;&#33258;3D&#24863;&#30693;&#27169;&#22411;&#30340;&#33258;&#20449;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#25512;&#20986;&#20102;Calib3D&#65292;&#36825;&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#22522;&#20934;&#21644;&#23457;&#26597;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;28&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;10&#20010;&#19981;&#21516;&#30340;3D&#25968;&#25454;&#38598;&#19978;&#65292;&#25581;&#31034;&#20102;&#33021;&#22815;&#22788;&#29702;3D&#22330;&#26223;&#29702;&#35299;&#20013;&#30340;&#35823;&#24046;&#19981;&#30830;&#23450;&#24615;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#35265;&#22320;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#29616;&#26377;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24230;&#27700;&#24179;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745; -- &#36825;&#20010;&#20851;&#38190;&#30340;&#32570;&#38519;&#20005;&#37325;&#25439;&#23475;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#25935;&#24863;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#32593;&#32476;&#23481;&#37327;&#12289;LiDAR&#34920;&#31034;&#12289;&#20809;&#26629;&#20998;&#36776;&#29575;&#21644;3D&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#30452;&#25509;&#23558;&#36825;&#20123;&#26041;&#38754;&#19982;&#27169;&#22411;&#26657;&#20934;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17010v1 Announce Type: cross  Abstract: Safety-critical 3D scene understanding tasks necessitate not only accurate but also confident predictions from 3D perception models. This study introduces Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D scene understanding models from an uncertainty estimation viewpoint. We comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D datasets, uncovering insightful phenomena that cope with both the aleatoric and epistemic uncertainties in 3D scene understanding. We discover that despite achieving impressive levels of accuracy, existing models frequently fail to provide reliable uncertainty estimates -- a pitfall that critically undermines their applicability in safety-sensitive contexts. Through extensive analysis of key factors such as network capacity, LiDAR representations, rasterization resolutions, and 3D data augmentation techniques, we correlate these aspects directly with the model cal
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;</title><link>https://arxiv.org/abs/2403.16442</link><description>&lt;p&gt;
&#22914;&#26524;CLIP&#33021;&#35828;&#35805;: &#36890;&#36807;&#23427;&#20204;&#30340;&#39318;&#36873;&#27010;&#24565;&#25551;&#36848;&#29702;&#35299;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16442
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;Extract and Explore&#65288;EX2&#65289;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#20013;&#65292;&#37325;&#35201;&#30340;&#29305;&#24449;&#25551;&#36848;&#21253;&#25324;&#38750;&#35270;&#35273;&#23646;&#24615;&#65292;&#34394;&#20551;&#25551;&#36848;&#24433;&#21709;VLM&#34920;&#31034;&#65292;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24120;&#24120;&#20551;&#35774;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#30340;&#34920;&#31034;&#26159;&#22522;&#20110;&#24418;&#29366;&#31561;&#35270;&#35273;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;VLM&#22312;&#34920;&#31034;&#27010;&#24565;&#26102;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#23558;&#36825;&#20123;&#20449;&#24687;&#20316;&#20026;&#20248;&#20808;&#32771;&#34385;&#23545;&#35937;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;Extract and Explore&#65288;EX2&#65289;&#65292;&#29992;&#20110;&#21051;&#30011;VLM&#30340;&#37325;&#35201;&#25991;&#26412;&#29305;&#24449;&#12290;EX2&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#23558;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;VLM&#39318;&#36873;&#39033;&#23545;&#40784;&#65292;&#24182;&#29983;&#25104;&#21253;&#21547;VLM&#37325;&#35201;&#29305;&#24449;&#30340;&#25551;&#36848;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#36825;&#20123;&#25551;&#36848;&#20197;&#30830;&#23450;&#23545;VLM&#34920;&#31034;&#26377;&#36129;&#29486;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#25552;&#20379;&#20102;&#27809;&#26377;&#24110;&#21161;&#20449;&#24687;&#30340;&#34394;&#20551;&#25551;&#36848;&#65288;&#20363;&#22914;&#65292;&#21333;&#20987;&#25918;&#22823;&#27010;&#24565;&#30340;&#29031;&#29255;&#65289;&#65292;&#20294;&#22312;VLM&#34920;&#31034;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;&#25551;&#36848;&#20013;&#65292;VLM&#22312;&#34920;&#31034;&#35270;&#35273;&#27010;&#24565;&#26102;&#26174;&#33879;&#20381;&#36182;&#38750;&#35270;&#35273;&#23646;&#24615;&#65288;&#22914;&#26646;&#24687;&#22320;&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#19981;&#21516;&#30340;VLM&#20248;&#20808;&#32771;&#34385;&#19981;&#21516;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16442v1 Announce Type: new  Abstract: Recent works often assume that Vision-Language Model (VLM) representations are based on visual attributes like shape. However, it is unclear to what extent VLMs prioritize this information to represent concepts. We propose Extract and Explore (EX2), a novel approach to characterize important textual features for VLMs. EX2 uses reinforcement learning to align a large language model with VLM preferences and generates descriptions that incorporate the important features for the VLM. Then, we inspect the descriptions to identify the features that contribute to VLM representations. We find that spurious descriptions have a major role in VLM representations despite providing no helpful information, e.g., Click to enlarge photo of CONCEPT. More importantly, among informative descriptions, VLMs rely significantly on non-visual attributes like habitat to represent visual concepts. Also, our analysis reveals that different VLMs prioritize differen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12562</link><description>&lt;p&gt;
&#36890;&#36807;&#33719;&#21462;&#36171;&#26435;&#65306;&#25903;&#25345;&#23567;&#35268;&#27169;&#28145;&#24230;&#23398;&#20064;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Equity through Access: A Case for Small-scale Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12562
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;PePR&#20998;&#25968;&#65292;&#30740;&#31350;&#20154;&#21592;&#23637;&#31034;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#22312;&#21307;&#23398;&#22270;&#20687;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#24471;&#30410;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#21644;&#35745;&#31639;&#21147;&#30340;&#25552;&#21319;&#12290;&#36825;&#20123;&#22823;&#35268;&#27169;&#36164;&#28304;&#34987;&#29992;&#20110;&#35757;&#32451;&#26085;&#30410;&#24222;&#22823;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#20123;&#27169;&#22411;&#22312;&#35745;&#31639;&#12289;&#25968;&#25454;&#12289;&#33021;&#28304;&#21644;&#30899;&#25490;&#25918;&#26041;&#38754;&#28040;&#32791;&#24040;&#22823;&#12290;&#36825;&#20123;&#25104;&#26412;&#27491;&#22312;&#25104;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#26032;&#22411;&#20934;&#20837;&#38556;&#30861;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#37027;&#20123;&#22312;&#20840;&#29699;&#21335;&#26041;&#22320;&#21306;&#36164;&#28304;&#26377;&#38480;&#30340;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#35270;&#20102;&#29616;&#26377;&#35270;&#35273;&#20219;&#21153;&#30340;DL&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#20026;&#20102;&#32771;&#34385;DL&#27169;&#22411;&#30340;&#36164;&#28304;&#28040;&#32791;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#34913;&#37327;&#24615;&#33021;&#19982;&#36164;&#28304;&#21333;&#20803;&#30340;&#26032;&#25351;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;PePR&#20998;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;131&#31181;&#29420;&#29305;&#30340;DL&#26550;&#26500;&#65288;&#36328;&#24230;&#20174;1M&#21040;130M&#20010;&#21487;&#35757;&#32451;&#21442;&#25968;&#65289;&#21644;&#19977;&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#33719;&#21462;&#20102;&#26377;&#20851;&#24615;&#33021;&#21644;&#36164;&#28304;&#20043;&#38388;&#20851;&#31995;&#30340;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12562v1 Announce Type: cross  Abstract: The recent advances in deep learning (DL) have been accelerated by access to large-scale data and compute. These large-scale resources have been used to train progressively larger models which are resource intensive in terms of compute, data, energy, and carbon emissions. These costs are becoming a new type of entry barrier to researchers and practitioners with limited access to resources at such scale, particularly in the Global South. In this work, we take a comprehensive look at the landscape of existing DL models for vision tasks and demonstrate their usefulness in settings where resources are limited. To account for the resource consumption of DL models, we introduce a novel measure to estimate the performance per resource unit, which we call the PePR score. Using a diverse family of 131 unique DL architectures (spanning 1M to 130M trainable parameters) and three medical image datasets, we capture trends about the performance-reso
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;SDP&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11351</link><description>&lt;p&gt;
&#22522;&#20110;SDP&#30340;&#20108;&#20998;&#22270;&#32858;&#31867;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An SDP-based Branch-and-Cut Algorithm for Biclustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11351
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;SDP&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20108;&#20998;&#22270;&#32858;&#31867;&#65292;&#20063;&#31216;&#20026;&#20849;&#32858;&#31867;&#12289;&#22359;&#32858;&#31867;&#25110;&#21452;&#21521;&#32858;&#31867;&#65292;&#28041;&#21450;&#23558;&#25968;&#25454;&#30697;&#38453;&#30340;&#34892;&#21644;&#21015;&#21516;&#26102;&#32858;&#31867;&#25104;&#19981;&#21516;&#30340;&#32452;&#65292;&#20351;&#24471;&#21516;&#19968;&#32452;&#20869;&#30340;&#34892;&#21644;&#21015;&#26174;&#31034;&#20986;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#20316;&#20026;&#20108;&#20998;&#22270;&#32858;&#31867;&#30340;&#27169;&#22411;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;$k$-&#26368;&#23494;&#19981;&#30456;&#20132;&#21452;&#22242;&#38382;&#39064;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#21152;&#26435;&#23436;&#20840;&#20108;&#20998;&#22270;&#20013;&#35782;&#21035; $k$ &#20010;&#19981;&#30456;&#20132;&#30340;&#23436;&#20840;&#20108;&#37096;&#23376;&#22270;&#65288;&#31216;&#20026;&#21452;&#22242;&#65289;&#65292;&#20351;&#23427;&#20204;&#30340;&#23494;&#24230;&#20043;&#21644;&#26368;&#22823;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#20998;&#25903;&#23450;&#30028;&#31639;&#27861;&#12290;&#23545;&#20110;&#19978;&#30028;&#20363;&#31243;&#65292;&#25105;&#20204;&#32771;&#34385;&#21322;&#23450;&#35268;&#21010;&#25918;&#26494;&#24182;&#25552;&#20986;&#20102;&#29992;&#20110;&#21152;&#24378;&#30028;&#38480;&#30340;&#26377;&#25928;&#19981;&#31561;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#19968;&#38454;&#26041;&#27861;&#20197;&#20999;&#24179;&#38754;&#26041;&#24335;&#35299;&#20915;&#36825;&#20010;&#25918;&#26494;&#38382;&#39064;&#12290;&#23545;&#20110;&#19979;&#30028;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21033;&#29992;&#35299;&#20915;&#26041;&#26696;&#30340;&#26368;&#22823;&#26435;&#21305;&#37197;&#33293;&#20837;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11351v1 Announce Type: cross  Abstract: Biclustering, also called co-clustering, block clustering, or two-way clustering, involves the simultaneous clustering of both the rows and columns of a data matrix into distinct groups, such that the rows and columns within a group display similar patterns. As a model problem for biclustering, we consider the $k$-densest-disjoint biclique problem, whose goal is to identify $k$ disjoint complete bipartite subgraphs (called bicliques) of a given weighted complete bipartite graph such that the sum of their densities is maximized. To address this problem, we present a tailored branch-and-cut algorithm. For the upper bound routine, we consider a semidefinite programming relaxation and propose valid inequalities to strengthen the bound. We solve this relaxation in a cutting-plane fashion using a first-order method. For the lower bound, we design a maximum weight matching rounding procedure that exploits the solution of the relaxation solved
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10182</link><description>&lt;p&gt;
&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10182
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#31609;&#23398;(OR)&#20013;&#65292;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#39046;&#22495;&#30340;&#20986;&#33394;&#24615;&#33021;&#20351;&#20854;&#22312;OR&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;OOD&#25968;&#25454;&#26102;&#65292;NNs&#24448;&#24448;&#20250;&#20570;&#20986;&#33258;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20026;&#33258;&#20449;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#24403;&#36755;&#20986;&#24212;(&#19981;&#24212;)&#34987;&#20449;&#20219;&#26102;&#36827;&#34892;&#36890;&#20449;&#12290;&#22240;&#27492;&#65292;&#22312;OR&#39046;&#22495;&#20013;&#65292;NNs&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#22810;&#20010;&#29420;&#31435;NNs&#32452;&#25104;&#30340;&#28145;&#24230;&#38598;&#21512;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#36824;&#33021;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#30001;&#20110;&#36739;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22522;&#30784;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;NN&#38598;&#25104;&#65292;&#21363;sna
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10182v1 Announce Type: new  Abstract: In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the sna
&lt;/p&gt;</description></item><item><title>S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.07384</link><description>&lt;p&gt;
SmallToLarge (S2L): &#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#25552;&#20379;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07384
&lt;/p&gt;
&lt;p&gt;
S2L&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#24635;&#32467;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#65292;&#26469;&#25351;&#23548;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25968;&#25454;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30417;&#30563;&#24494;&#35843;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#24615;&#33021;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#36873;&#25321;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#38454;&#27573;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#22312;&#19987;&#19994;&#39046;&#22495;&#30340;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#20013;&#25913;&#21892;&#25968;&#25454;&#25928;&#29575;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#21407;&#22240;&#26159;&#24494;&#35843;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#19988;&#21487;&#20280;&#32553;&#30340;&#25968;&#25454;&#36873;&#25321;&#26041;&#27861;S2L&#65288;SmallToLarge&#65289;&#65292;&#23427;&#21033;&#29992;&#23567;&#27169;&#22411;&#30340;&#35757;&#32451;&#36712;&#36857;&#26469;&#25351;&#23548;&#26356;&#22823;&#27169;&#22411;&#30340;&#25968;&#25454;&#36873;&#25321;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;S2L&#26174;&#33879;&#25552;&#39640;&#20102;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#30340;SFT&#25968;&#25454;&#25928;&#29575;&#65292;&#23558;&#35757;&#32451;&#25968;&#25454;&#32553;&#20943;&#21040;&#21407;&#22987;MathInstruct&#25968;&#25454;&#38598;&#65288;Yue&#31561;&#20154;&#65292;2023&#65289;&#30340;&#20165;11%&#65292;&#20197;&#36798;&#21040;&#20840;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;6&#20010;&#39046;&#22495;&#20869;&#22806;&#35780;&#20272;&#25968;&#25454;&#38598;&#20013;&#24179;&#22343;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#36873;&#25321;&#31639;&#27861;4.7%&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20165;&#36873;&#25321;50K&#25968;&#25454;&#36827;&#34892;SFT&#65292;S2L&#23454;&#29616;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07384v1 Announce Type: cross  Abstract: Despite the effectiveness of data selection for large language models (LLMs) during pretraining and instruction fine-tuning phases, improving data efficiency in supervised fine-tuning (SFT) for specialized domains poses significant challenges due to the complexity of fine-tuning data. To bridge this gap, we introduce an effective and scalable data selection method for SFT, SmallToLarge (S2L), which leverages training trajectories from small models to guide the data selection for larger models. We demonstrate through extensive experiments that S2L significantly improves data efficiency in SFT for mathematical problem-solving, reducing the training data to just 11% of the original MathInstruct dataset (Yue et al., 2023) to match full dataset performance while outperforming state-of-the-art data selection algorithms by an average of 4.7% across 6 in- and out-domain evaluation datasets. Remarkably, selecting only 50K data for SFT, S2L achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#25935;&#24863;&#24230;&#26469;&#25233;&#21046;&#36951;&#24536;&#25968;&#25454;&#30340;&#36129;&#29486;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15109</link><description>&lt;p&gt;
&#25233;&#21046;&#26679;&#26412;&#36129;&#29486;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning by Suppressing Sample Contribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#25935;&#24863;&#24230;&#26469;&#25233;&#21046;&#36951;&#24536;&#25968;&#25454;&#30340;&#36129;&#29486;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#26159;&#25351;&#20174;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#28041;&#21450;&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#12290;&#26412;&#25991;&#20174;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#35265;&#25968;&#25454;&#23545;&#27169;&#22411;&#36129;&#29486;&#30340;&#22522;&#26412;&#21306;&#21035;&#20837;&#25163;&#65306;&#35757;&#32451;&#25968;&#25454;&#23545;&#26368;&#32456;&#27169;&#22411;&#26377;&#36129;&#29486;&#65292;&#32780;&#26410;&#35265;&#25968;&#25454;&#27809;&#26377;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21457;&#29616;&#36755;&#20837;&#25935;&#24863;&#24230;&#21487;&#20197;&#36817;&#20284;&#34913;&#37327;&#36129;&#29486;&#65292;&#24182;&#23454;&#38469;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#31216;&#20026;MU-Mis&#65288;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#25935;&#24863;&#24230;&#36827;&#34892;&#26426;&#22120;&#36951;&#24536;&#65289;&#65292;&#26469;&#25233;&#21046;&#36951;&#24536;&#25968;&#25454;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MU-Mis&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;MU&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MU-Mis&#19982;MU&#30340;&#24212;&#29992;&#26356;&#21152;&#23494;&#20999;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15109v1 Announce Type: new  Abstract: Machine Unlearning (MU) is to forget data from a well-trained model, which is practically important due to the "right to be forgotten". In this paper, we start from the fundamental distinction between training data and unseen data on their contribution to the model: the training data contributes to the final model while the unseen data does not. We theoretically discover that the input sensitivity can approximately measure the contribution and practically design an algorithm, called MU-Mis (machine unlearning via minimizing input sensitivity), to suppress the contribution of the forgetting data. Experimental results demonstrate that MU-Mis outperforms state-of-the-art MU methods significantly. Additionally, MU-Mis aligns more closely with the application of MU as it does not require the use of remaining data.
&lt;/p&gt;</description></item><item><title>DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;</title><link>https://arxiv.org/abs/2402.14123</link><description>&lt;p&gt;
DeiSAM&#65306;&#36890;&#36807;&#25351;&#31034;&#25552;&#31034;&#20998;&#21106;&#20219;&#20309;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
DeiSAM: Segment Anything with Deictic Prompting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14123
&lt;/p&gt;
&lt;p&gt;
DeiSAM&#25552;&#20986;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#65292;&#23454;&#29616;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#23545;&#35937;&#30340;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24378;&#22823;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#38646;-shot&#22270;&#20687;&#20998;&#21106;&#12290;&#20026;&#20102;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#35782;&#21035;&#20855;&#20307;&#23545;&#35937;&#65292;&#20154;&#31867;&#26412;&#33021;&#22320;&#20381;&#36182;&#20110;&#33258;&#28982;&#35821;&#35328;&#20013;&#30340;&#25351;&#31034;&#24615;&#25551;&#36848;&#65292;&#21363;&#26681;&#25454;&#19978;&#19979;&#25991;&#25351;&#31216;&#26576;&#29289;&#65292;&#27604;&#22914;&#8220;&#22312;&#26700;&#23376;&#19978;&#24182;&#22312;&#26479;&#23376;&#21518;&#38754;&#30340;&#29289;&#20307;&#8221;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30001;&#20110;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#32570;&#20047;&#25512;&#29702;&#33021;&#21147;&#65292;&#26080;&#27861;&#21487;&#38752;&#22320;&#35299;&#37322;&#36825;&#31181;&#25351;&#31034;&#24615;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeiSAM&#8212;&#8212;&#23558;&#22823;&#22411;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#19982;&#21487;&#21306;&#20998;&#36923;&#36753;&#25512;&#29702;&#22120;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25351;&#31034;&#25552;&#31034;&#24615;&#20998;&#21106;&#12290;&#32473;&#23450;&#22797;&#26434;&#30340;&#25991;&#26412;&#20998;&#21106;&#25551;&#36848;&#65292;DeiSAM&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19968;&#38454;&#36923;&#36753;&#35268;&#21017;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#22330;&#26223;&#22270;&#36827;&#34892;&#21487;&#21306;&#20998;&#30340;&#21069;&#21521;&#25512;&#29702;&#12290;&#38543;&#21518;&#65292;DeiSAM&#36890;&#36807;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14123v1 Announce Type: cross  Abstract: Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as "The object that is on the desk and behind the cup.". However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM -- a combination of large pre-trained neural networks with differentiable logic reasoners -- for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by match
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07754</link><description>&lt;p&gt;
&#24605;&#24819;&#20256;&#25773;&#65306;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07754
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#25512;&#29702;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#25955;&#20256;&#25773;&#25512;&#29702;&#27493;&#39588;&#65292;&#25552;&#20379;&#20102;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#21644;&#25512;&#29702;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#20013;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#30456;&#23545;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#20855;&#26377;&#35768;&#22810;&#28508;&#22312;&#20248;&#21183;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#38598;&#25104;&#30340;&#26041;&#27861;&#65292;CoT&#26159;&#19968;&#31181;&#22312;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#20013;&#25913;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;&#25104;&#29087;&#25216;&#26415;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24605;&#32500;&#25193;&#25955;&#65288;DoT&#65289;&#27169;&#22411;&#65292;&#20801;&#35768;&#25512;&#29702;&#27493;&#39588;&#36890;&#36807;&#25193;&#25955;&#36807;&#31243;&#22312;&#26102;&#38388;&#19978;&#20256;&#25773;&#12290;&#19982;&#20256;&#32479;&#30340;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#36880;&#20010;token&#20174;&#24038;&#21040;&#21491;&#20570;&#20986;&#20915;&#31574;&#30340;&#26041;&#24335;&#30456;&#27604;&#65292;DoT&#22312;&#35745;&#31639;&#21644;&#25512;&#29702;&#24615;&#33021;&#20043;&#38388;&#20855;&#26377;&#26356;&#22823;&#30340;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DoT&#22312;&#22810;&#20301;&#25968;&#20056;&#27861;&#21644;&#23567;&#23398;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;DoT&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#33258;&#25105;&#32416;&#27491;&#33021;&#21147;&#65292;&#24182;&#20174;&#29616;&#26377;&#30340;&#22686;&#24378;&#25512;&#29702;&#25216;&#26415;&#65288;&#22914;&#33258;&#19968;&#33268;&#35299;&#30721;&#65289;&#20013;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#21457;&#23637;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have gained attention in text processing, offering many potential advantages over traditional autoregressive models. This work explores the integration of diffusion models and Chain-of-Thought (CoT), a well-established technique to improve the reasoning ability in autoregressive language models. We propose Diffusion-of-Thought (DoT), allowing reasoning steps to diffuse over time through the diffusion process. In contrast to traditional autoregressive language models that make decisions in a left-to-right, token-by-token manner, DoT offers more flexibility in the trade-off between computation and reasoning performance. Our experimental results demonstrate the effectiveness of DoT in multi-digit multiplication and grade school math problems. Additionally, DoT showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning capab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;K&#31354;&#38388;&#36827;&#34892;&#22270;&#20687;&#36864;&#21270;&#21644;&#24674;&#22797;&#30340;&#20919;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22122;&#38899;&#65292;&#33021;&#22815;&#20026;&#21152;&#36895;MRI&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;</title><link>https://arxiv.org/abs/2311.10162</link><description>&lt;p&gt;
K&#31354;&#38388;&#20919;&#25193;&#25955;&#65306;&#23398;&#20064;&#22312;&#27809;&#26377;&#22122;&#38899;&#30340;&#24773;&#20917;&#19979;&#37325;&#24314;&#21152;&#36895;MRI
&lt;/p&gt;
&lt;p&gt;
K-space Cold Diffusion: Learning to Reconstruct Accelerated MRI without Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10162
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;K&#31354;&#38388;&#36827;&#34892;&#22270;&#20687;&#36864;&#21270;&#21644;&#24674;&#22797;&#30340;&#20919;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22122;&#38899;&#65292;&#33021;&#22815;&#20026;&#21152;&#36895;MRI&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MRI&#37325;&#24314;&#27169;&#22411;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#20462;&#34917;&#12289;&#36229;&#20998;&#36776;&#29575;&#12289;&#22270;&#20687;&#32534;&#36753;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#20919;&#25193;&#25955;&#36827;&#19968;&#27493;&#25299;&#23485;&#20102;&#33539;&#22260;&#65292;&#24182;&#32771;&#34385;&#20102;&#22260;&#32469;&#20219;&#24847;&#22270;&#20687;&#21464;&#25442;&#26500;&#24314;&#30340;&#27169;&#22411;&#65292;&#20363;&#22914;&#27169;&#31946;&#12289;&#19979;&#37319;&#26679;&#31561;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;K&#31354;&#38388;&#20013;&#25191;&#34892;&#22270;&#20687;&#36864;&#21270;&#21644;&#24674;&#22797;&#30340;K&#31354;&#38388;&#20919;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#39640;&#26031;&#22122;&#22768;&#12290;&#25105;&#20204;&#19982;&#22810;&#20010;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;MRI&#37325;&#24314;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#22312;&#19968;&#20010;&#30693;&#21517;&#30340;&#22823;&#22411;&#24320;&#28304;MRI&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26032;&#39062;&#30340;&#36864;&#21270;&#26041;&#24335;&#21487;&#20197;&#20026;&#21152;&#36895;MRI&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10162v2 Announce Type: replace-cross  Abstract: Deep learning-based MRI reconstruction models have achieved superior performance these days. Most recently, diffusion models have shown remarkable performance in image generation, in-painting, super-resolution, image editing and more. As a generalized diffusion model, cold diffusion further broadens the scope and considers models built around arbitrary image transformations such as blurring, down-sampling, etc. In this paper, we propose a k-space cold diffusion model that performs image degradation and restoration in k-space without the need for Gaussian noise. We provide comparisons with multiple deep learning-based MRI reconstruction models and perform tests on a well-known large open-source MRI dataset. Our results show that this novel way of performing degradation can generate high-quality reconstruction images for accelerated MRI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2401.01951</link><description>&lt;p&gt;
&#20351;&#29992;&#21367;&#31215;&#33021;&#21542;&#20165;&#29983;&#25104;&#36924;&#30495;&#30340;&#25163;&#37096;&#22270;&#20687;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can We Generate Realistic Hands Only Using Convolution?. (arXiv:2401.01951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#20855;&#26377;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#30340;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#21487;&#20197;&#32531;&#35299;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#20960;&#20309;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;GAN&#21644;VAE&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#36798;&#21313;&#24180;&#20043;&#20037;&#65292;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#19968;&#30452;&#26080;&#27861;&#37325;&#29616;&#22797;&#26434;&#30340;&#20960;&#20309;&#29305;&#24449;&#65292;&#20363;&#22914;&#20154;&#25163;&#21644;&#25163;&#25351;&#20013;&#25152;&#23384;&#22312;&#30340;&#29305;&#24449;&#65292;&#36825;&#19968;&#38382;&#39064;&#22312;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#23384;&#22312;&#12290;&#34429;&#28982;&#36890;&#36807;&#22686;&#21152;&#27169;&#22411;&#22823;&#23567;&#21644;&#22810;&#26679;&#21270;&#35757;&#32451;&#25968;&#25454;&#38598;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#36825;&#20010;&#38382;&#39064;&#22312;&#21508;&#31181;&#27169;&#22411;&#20013;&#20173;&#28982;&#26222;&#36941;&#23384;&#22312;&#65292;&#20174;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#21040;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#65292;&#36825;&#25351;&#21521;&#20102;&#24213;&#23618;&#32467;&#26500;&#30340;&#26681;&#26412;&#32570;&#38519;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#21367;&#31215;&#23618;&#25552;&#20379;&#19968;&#20010;&#21333;&#19968;&#36755;&#20837;&#36890;&#36947;&#65292;&#20854;&#20013;&#21253;&#21547;&#30456;&#23545;$n$&#32500;&#31515;&#21345;&#23572;&#22352;&#26631;&#31995;&#65292;&#26469;&#23637;&#31034;&#22914;&#20309;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#26497;&#22823;&#22320;&#25913;&#21892;&#20102;GAN&#21644;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#29983;&#25104;&#30340;&#25163;&#37096;&#21644;&#38754;&#37096;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying training datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convolution layers geometric capabilities through providing them with a single input channel incorporating the relative $n$-dimensional Cartesian coordinate system. We show that this drastically improves quality of hand and face images generated by GANs and Variational AutoEncoders (VAE).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Polish&#31354;&#38388;&#20013;&#30340;&#29109;&#27491;&#21017;&#21270;Markov&#20915;&#31574;&#36807;&#31243;&#19978;&#30340;Fisher-Rao&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#26799;&#24230;&#35780;&#20272;&#26041;&#38754;&#30340;&#31283;&#23450;&#24615;&#65292;&#20026;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;</title><link>http://arxiv.org/abs/2310.02951</link><description>&lt;p&gt;
Fisher-Rao&#26799;&#24230;&#27969;&#22312;Polish&#31354;&#38388;&#20013;&#23545;&#29109;&#27491;&#21017;&#21270;Markov&#20915;&#31574;&#36807;&#31243;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Fisher-Rao gradient flow for entropy-regularised Markov decision processes in Polish spaces. (arXiv:2310.02951v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02951
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;Polish&#31354;&#38388;&#20013;&#30340;&#29109;&#27491;&#21017;&#21270;Markov&#20915;&#31574;&#36807;&#31243;&#19978;&#30340;Fisher-Rao&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#25351;&#25968;&#25910;&#25947;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#26799;&#24230;&#35780;&#20272;&#26041;&#38754;&#30340;&#31283;&#23450;&#24615;&#65292;&#20026;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Polish&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#26080;&#38480;&#26102;&#22495;&#30340;&#29109;&#27491;&#21017;&#21270;&#30340;Markov&#20915;&#31574;&#36807;&#31243;&#30340;Fisher-Rao&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#12290;&#36825;&#20010;&#27969;&#26159;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#38388;&#31867;&#27604;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#33391;&#23450;&#20041;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#25351;&#25968;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#22312;&#26799;&#24230;&#35780;&#20272;&#26041;&#38754;&#30340;&#31283;&#23450;&#24615;&#65292;&#20026;&#23545;&#25968;&#32447;&#24615;&#31574;&#30053;&#21442;&#25968;&#21270;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#27969;&#30340;&#24615;&#33021;&#25552;&#20379;&#20102;&#27934;&#35265;&#12290;&#20026;&#20102;&#20811;&#26381;&#30446;&#26631;&#20989;&#25968;&#38750;&#20984;&#24615;&#21644;&#29109;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#19981;&#36830;&#32493;&#24615;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#21033;&#29992;&#24615;&#33021;&#24046;&#21035;&#24341;&#29702;&#21644;&#26799;&#24230;&#19982;&#38236;&#20687;&#19979;&#38477;&#27969;&#20043;&#38388;&#30340;&#23545;&#20598;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the global convergence of a Fisher-Rao policy gradient flow for infinite-horizon entropy-regularised Markov decision processes with Polish state and action space. The flow is a continuous-time analogue of a policy mirror descent method. We establish the global well-posedness of the gradient flow and demonstrate its exponential convergence to the optimal policy. Moreover, we prove the flow is stable with respect to gradient evaluation, offering insights into the performance of a natural policy gradient flow with log-linear policy parameterisation. To overcome challenges stemming from the lack of the convexity of the objective function and the discontinuity arising from the entropy regulariser, we leverage the performance difference lemma and the duality relationship between the gradient and mirror descent flows.
&lt;/p&gt;</description></item><item><title>RLLTE&#26159;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#39044;&#35745;&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#24182;&#21050;&#28608;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;</title><link>http://arxiv.org/abs/2309.16382</link><description>&lt;p&gt;
RLLTE&#65306;&#24378;&#21270;&#23398;&#20064;&#30340;&#38271;&#26399;&#28436;&#36827;&#39033;&#30446;
&lt;/p&gt;
&lt;p&gt;
RLLTE: Long-Term Evolution Project of Reinforcement Learning. (arXiv:2309.16382v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16382
&lt;/p&gt;
&lt;p&gt;
RLLTE&#26159;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29983;&#24577;&#31995;&#32479;&#65292;&#39044;&#35745;&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#24182;&#21050;&#28608;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;RLLTE&#65306;&#19968;&#31181;&#38271;&#26399;&#28436;&#36827;&#12289;&#26497;&#24230;&#27169;&#22359;&#21270;&#21644;&#24320;&#28304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30740;&#31350;&#19982;&#24212;&#29992;&#26694;&#26550;&#12290;&#38500;&#20102;&#25552;&#20379;&#19968;&#27969;&#30340;&#31639;&#27861;&#23454;&#29616;&#20043;&#22806;&#65292;RLLTE&#36824;&#20316;&#20026;&#19968;&#20010;&#31639;&#27861;&#24320;&#21457;&#24037;&#20855;&#21253;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;RLLTE&#23436;&#20840;&#35299;&#32806;&#20102;RL&#31639;&#27861;&#19982;&#24320;&#21457;&#31639;&#27861;&#30340;&#23454;&#36341;&#35282;&#24230;&#65292;&#25552;&#20379;&#20102;&#22823;&#37327;&#32452;&#20214;&#26469;&#21152;&#36895;&#31639;&#27861;&#30340;&#21457;&#23637;&#21644;&#28436;&#36827;&#12290;&#29305;&#21035;&#22320;&#65292;RLLTE&#26159;&#31532;&#19968;&#20010;&#26500;&#24314;&#20102;&#23436;&#25972;&#20016;&#23500;&#30340;&#29983;&#24577;&#31995;&#32479;&#30340;RL&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#22411;&#35757;&#32451;&#12289;&#35780;&#20272;&#12289;&#37096;&#32626;&#12289;&#22522;&#20934;&#27979;&#35797;&#20013;&#24515;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22686;&#24378;&#30340;&#21103;&#39550;&#39542;&#12290;&#39044;&#26399;RLLTE&#23558;&#20026;RL&#24037;&#31243;&#23454;&#36341;&#35774;&#23450;&#26631;&#20934;&#65292;&#24182;&#23545;&#20135;&#19994;&#21644;&#23398;&#26415;&#30028;&#20855;&#26377;&#39640;&#24230;&#21050;&#28608;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present RLLTE: a long-term evolution, extremely modular, and open-source framework for reinforcement learning (RL) research and application. Beyond delivering top-notch algorithm implementations, RLLTE also serves as a toolkit for developing algorithms. More specifically, RLLTE decouples the RL algorithms completely from the exploitation-exploration perspective, providing a large number of components to accelerate algorithm development and evolution. In particular, RLLTE is the first RL framework to build a complete and luxuriant ecosystem, which includes model training, evaluation, deployment, benchmark hub, and large language model (LLM)-empowered copilot. RLLTE is expected to set standards for RL engineering practice and be highly stimulative for industry and academia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2308.15513</link><description>&lt;p&gt;
&#35843;&#25972;&#22256;&#24785;&#24230;&#24182;&#35745;&#31639;&#22522;&#20110;&#37319;&#26679;&#30340;t-SNE&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Tuning the perplexity for and computing sampling-based t-SNE embeddings. (arXiv:2308.15513v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#37319;&#26679;&#30340;&#26041;&#27861;&#25913;&#36827;&#20102;&#22823;&#25968;&#25454;&#38598;&#19979;t-SNE&#23884;&#20837;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#20998;&#26512;&#24120;&#29992;&#30340;&#31649;&#36947;&#21033;&#29992;&#20108;&#32500;&#21487;&#35270;&#21270;&#65292;&#20363;&#22914;&#36890;&#36807;t&#20998;&#24067;&#37051;&#36817;&#38543;&#26426;&#23884;&#20837;&#65288;t-SNE&#65289;&#12290;&#20294;&#22312;&#22788;&#29702;&#22823;&#25968;&#25454;&#38598;&#26102;&#65292;&#24212;&#29992;&#36825;&#20123;&#21487;&#35270;&#21270;&#25216;&#26415;&#20250;&#29983;&#25104;&#27425;&#20248;&#30340;&#23884;&#20837;&#65292;&#22240;&#20026;&#36229;&#21442;&#25968;&#19981;&#36866;&#29992;&#20110;&#22823;&#25968;&#25454;&#12290;&#23558;&#36825;&#20123;&#21442;&#25968;&#22686;&#21152;&#36890;&#24120;&#19981;&#36215;&#20316;&#29992;&#65292;&#22240;&#20026;&#35745;&#31639;&#23545;&#20110;&#23454;&#38469;&#24037;&#20316;&#27969;&#31243;&#26469;&#35828;&#22826;&#26114;&#36149;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#37319;&#26679;&#30340;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24517;&#39035;&#35880;&#24910;&#36873;&#25321;&#36229;&#21442;&#25968;&#65292;&#21462;&#20915;&#20110;&#37319;&#26679;&#29575;&#21644;&#39044;&#26399;&#30340;&#26368;&#32456;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#22914;&#20309;&#21152;&#36895;&#35745;&#31639;&#24182;&#25552;&#39640;&#23884;&#20837;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Widely used pipelines for the analysis of high-dimensional data utilize two-dimensional visualizations. These are created, e.g., via t-distributed stochastic neighbor embedding (t-SNE). When it comes to large data sets, applying these visualization techniques creates suboptimal embeddings, as the hyperparameters are not suitable for large data. Cranking up these parameters usually does not work as the computations become too expensive for practical workflows. In this paper, we argue that a sampling-based embedding approach can circumvent these problems. We show that hyperparameters must be chosen carefully, depending on the sampling rate and the intended final embedding. Further, we show how this approach speeds up the computation and increases the quality of the embeddings.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.10968</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#38480;&#25968;&#25454;&#30340;MRI&#22330;&#36716;&#31227;&#37325;&#24314;&#65306;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;
&lt;/p&gt;
&lt;p&gt;
MRI Field-transfer Reconstruction with Limited Data: Regularization by Neural Style Transfer. (arXiv:2308.10968v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10968
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#36827;&#34892;&#27491;&#35268;&#21270;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#26465;&#20214;&#19979;&#20174;&#20302;&#36136;&#37327;&#22270;&#20687;&#37325;&#24314;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#22312;&#20020;&#24202;MRI&#25195;&#25551;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;MRI&#37325;&#24314;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#25253;&#21578;&#30340;&#26041;&#27861;&#37117;&#38656;&#35201;&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#38477;&#22122;&#65288;RED&#65289;&#27491;&#35268;&#21270;&#26159;&#19968;&#31181;&#23558;&#38477;&#22122;&#22120;&#20316;&#20026;&#22270;&#20687;&#37325;&#24314;&#20808;&#39564;&#30340;&#36890;&#29992;&#27969;&#31243;&#12290;RED&#30340;&#28508;&#21147;&#24050;&#32463;&#22312;&#22810;&#20010;&#19982;&#22270;&#20687;&#30456;&#20851;&#30340;&#20219;&#21153;&#65288;&#22914;&#38477;&#22122;&#12289;&#21435;&#27169;&#31946;&#21644;&#36229;&#20998;&#36776;&#29575;&#65289;&#20013;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31070;&#32463;&#39118;&#26684;&#36716;&#25442;&#65288;RNST&#65289;&#26041;&#27861;&#36827;&#34892;&#27491;&#35268;&#21270;&#30340;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#21033;&#29992;&#31070;&#32463;&#36716;&#31227;&#21644;&#38477;&#22122;&#24341;&#25806;&#30340;&#20808;&#39564;&#20449;&#24687;&#12290;&#36825;&#20351;&#24471;RNST&#33021;&#22815;&#20174;&#26377;&#22122;&#22768;&#30340;&#20302;&#36136;&#37327;&#22270;&#20687;&#20013;&#37325;&#24314;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#22270;&#20687;&#39118;&#26684;&#21644;&#26377;&#38480;&#25968;&#25454;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;1.5T&#21644;3T&#30340;&#20020;&#24202;MRI&#25195;&#25551;&#39564;&#35777;&#20102;RNST&#65292;&#24182;&#19988;&#26174;&#31034;RNST&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#31361;&#26174;&#20102;RNST&#26694;&#26550;&#22312;MRI&#37325;&#24314;&#21644;&#26377;&#38480;&#25968;&#25454;&#37325;&#24314;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have demonstrated success in MRI reconstruction using deep learning-based models. However, most reported approaches require training on a task-specific, large-scale dataset. Regularization by denoising (RED) is a general pipeline which embeds a denoiser as a prior for image reconstruction. The potential of RED has been demonstrated for multiple image-related tasks such as denoising, deblurring and super-resolution. In this work, we propose a regularization by neural style transfer (RNST) method to further leverage the priors from the neural transfer and denoising engine. This enables RNST to reconstruct a high-quality image from a noisy low-quality image with different image styles and limited data. We validate RNST with clinical MRI scans from 1.5T and 3T and show that RNST can significantly boost image quality. Our results highlight the capability of the RNST framework for MRI reconstruction and the potential for reconstruction tasks with limited data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#27604;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.19770</link><description>&lt;p&gt;
&#8220;&#36136;&#37327;&#36827;/&#36136;&#37327;&#20986;&#65306;&#35780;&#20272;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#30340;&#25968;&#25454;&#36136;&#37327;&#8221;
&lt;/p&gt;
&lt;p&gt;
Quality In / Quality Out: Assessing Data quality in an Anomaly Detection Benchmark. (arXiv:2305.19770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#27979;&#35797;&#26102;&#65292;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#24433;&#21709;&#24456;&#22823;&#65292;&#27604;&#29305;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26356;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#25110;&#33258;&#21160;&#39550;&#39542;&#32593;&#32476;&#34987;&#26399;&#26395;&#25104;&#20026;&#26410;&#26469;&#20114;&#32852;&#32593;&#20013;&#26497;&#23500;&#25361;&#25112;&#21644;&#38656;&#27714;&#30340;&#26032;&#22411;&#24212;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22788;&#29702;&#22797;&#26434;&#24615;&#30340;&#20851;&#38190;&#22312;&#20110;&#36890;&#36807;&#26368;&#23569;&#30340;&#20154;&#24037;&#24178;&#39044;&#25191;&#34892;&#32593;&#32476;&#20248;&#21270;&#21644;&#25925;&#38556;&#24674;&#22797;&#30340;&#20219;&#21153;&#12290;&#20026;&#27492;&#65292;&#31038;&#21306;&#20381;&#36182;&#20110;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#25216;&#26415;&#30340;&#24320;&#21457;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#22909;&#22351;&#21462;&#20915;&#20110;&#23427;&#25152;&#25311;&#21512;&#30340;&#25968;&#25454;&#12290;&#20026;&#30740;&#31350;&#30446;&#30340;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65288;&#23545;&#30740;&#31350;&#32467;&#26524;&#21644;&#26041;&#21521;&#26377;&#37325;&#35201;&#24433;&#21709;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65289;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#40664;&#35748;&#20855;&#26377;&#33391;&#22909;&#36136;&#37327;&#30340;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#23545;&#21516;&#19968;&#22522;&#20934;&#25968;&#25454;&#38598;&#65288;UGR'16&#65292;&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#20110;&#27969;&#37327;&#30340;&#23454;&#26102;&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#30456;&#23545;&#36739;&#23567;&#30340;&#20462;&#25913;&#65292;&#27604;&#25152;&#32771;&#34385;&#30340;&#20855;&#20307;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#26356;&#26174;&#33879;&#22320;&#24433;&#21709;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30740;&#31350;&#36825;&#20123;&#24046;&#24322;&#26681;&#26412;&#21407;&#22240;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous or self-driving networks are expected to provide a solution to the myriad of extremely demanding new applications in the Future Internet. The key to handle complexity is to perform tasks like network optimization and failure recovery with minimal human supervision. For this purpose, the community relies on the development of new Machine Learning (ML) models and techniques. However, ML can only be as good as the data it is fitted with. Datasets provided to the community as benchmarks for research purposes, which have a relevant impact in research findings and directions, are often assumed to be of good quality by default. In this paper, we show that relatively minor modifications on the same benchmark dataset (UGR'16, a flow-based real-traffic dataset for anomaly detection) cause significantly more impact on model performance than the specific ML technique considered. To understand this finding, we contribute a methodology to investigate the root causes for those differences,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#38160;&#24230;&#24418;&#24335;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; WSAM&#65292;&#29992;&#20110;&#25913;&#36827; Sharpness-Aware Minimization (SAM) &#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#25110;&#31454;&#20105;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.15817</link><description>&lt;p&gt;
&#38160;&#24230;&#24863;&#30693;&#26368;&#23567;&#21270;&#65306;&#23558;&#38160;&#24230;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#30340;&#21152;&#26435;&#24418;&#24335;
&lt;/p&gt;
&lt;p&gt;
Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term. (arXiv:2305.15817v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#26435;&#38160;&#24230;&#24418;&#24335;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861; WSAM&#65292;&#29992;&#20110;&#25913;&#36827; Sharpness-Aware Minimization (SAM) &#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#25110;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#26368;&#23567;&#20540;&#30340;&#24179;&#22374;&#24230;&#23494;&#20999;&#30456;&#20851;&#65292;&#23548;&#33268;&#21457;&#23637;&#20102;Sharpness-Aware Minimization (SAM)&#26469;&#23547;&#25214;&#26356;&#24179;&#22374;&#30340;&#26368;&#23567;&#20540;&#21644;&#26356;&#22909;&#30340;&#27867;&#21270;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102; SAM &#30340;&#25439;&#22833;&#20989;&#25968;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026; WSAM&#65292;&#36890;&#36807;&#23558;&#38160;&#24230;&#20316;&#20026;&#27491;&#21017;&#21270;&#39033;&#36827;&#34892;&#25913;&#36827;&#12290;&#25105;&#20204;&#36890;&#36807;PAC&#21644;Bayes-PAC&#25216;&#26415;&#30340;&#32467;&#21512;&#35777;&#26126;&#20102;&#23427;&#30340;&#27867;&#21270;&#36793;&#30028;&#65292;&#24182;&#22312;&#21508;&#31181;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#23427;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982; vanilla optimizer&#12289;SAM &#21450;&#20854;&#21464;&#20307;&#30456;&#27604;&#65292;WSAM &#21462;&#24471;&#20102;&#25913;&#21892;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#25110;&#32773;&#33267;&#23569;&#26159;&#38750;&#24120;&#26377;&#31454;&#20105;&#21147;&#30340;&#12290;&#20195;&#30721;&#21487;&#20174; https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) generalization is known to be closely related to the flatness of minima, leading to the development of Sharpness-Aware Minimization (SAM) for seeking flatter minima and better generalization. In this paper, we revisit the loss of SAM and propose a more general method, called WSAM, by incorporating sharpness as a regularization term. We prove its generalization bound through the combination of PAC and Bayes-PAC techniques, and evaluate its performance on various public datasets. The results demonstrate that WSAM achieves improved generalization, or is at least highly competitive, compared to the vanilla optimizer, SAM and its variants. The code is available at https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.15560</link><description>&lt;p&gt;
&#22522;&#20110; Foundation Model APIs &#30340;&#24046;&#20998;&#38544;&#31169;&#21512;&#25104;&#25968;&#25454;&#65306;&#22270;&#29255;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Synthetic Data via Foundation Model APIs 1: Images. (arXiv:2305.15560v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;API&#30340;&#26041;&#27861;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#65292;&#21487;&#20197;&#26356;&#36731;&#26494;&#22320;&#37096;&#32626;&#12290;&#20351;&#29992;Private Evolution&#65288;PE&#65289;&#26694;&#26550;&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#65292;&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#65292;&#24182;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#39537;&#21160;&#30340;&#19990;&#30028;&#20013;&#65292;&#29983;&#25104;&#23494;&#20999;&#31867;&#20284;&#20110;&#21407;&#22987;&#31169;&#26377;&#25968;&#25454;&#30340;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#65292;&#21487;&#20943;&#36731;&#38544;&#31169;&#38382;&#39064;&#12290;&#19982;&#24403;&#21069;&#20026;&#27492;&#20219;&#21153;&#35757;&#32451;&#23450;&#21046;&#27169;&#22411;&#30340;&#20570;&#27861;&#30456;&#21453;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;API&#29983;&#25104;DP&#21512;&#25104;&#25968;&#25454;&#65288;DPSDA&#65289;&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#22522;&#30784;&#27169;&#22411;&#35270;&#20026;&#40657;&#30418;&#24182;&#21482;&#21033;&#29992;&#20854;&#25512;&#29702;API&#12290;&#36825;&#20123;&#22522;&#20110;API&#30340;&#12289;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#26356;&#23481;&#26131;&#37096;&#32626;&#65292;&#22914;&#26368;&#36817; API &#24212;&#29992;&#31243;&#24207;&#30340;&#28608;&#22686;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#36825;&#20123;&#26041;&#27861;&#36824;&#21487;&#20197;&#21033;&#29992;&#21487;&#36890;&#36807;&#20854;&#25512;&#29702;API&#35775;&#38382;&#20854;&#26435;&#37325;&#26410;&#21457;&#24067;&#30340;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#27169;&#22411;&#35775;&#38382;&#26356;&#21152;&#20005;&#26684;&#65292;&#36824;&#38656;&#20445;&#25252;API&#25552;&#20379;&#21830;&#30340;&#38544;&#31169;&#65292;&#36825;&#23558;&#24102;&#26469;&#26356;&#22823;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026; Private Evolution&#65288;PE&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20351;&#29992;&#22522;&#30784;&#27169;&#22411;API&#29983;&#25104;DP&#21512;&#25104;&#22270;&#20687;&#26041;&#38754;&#30340;&#21021;&#22987;&#23454;&#29616;&#12290;PE&#32467;&#21512;&#20102;&#24046;&#20998;&#38544;&#31169;&#12289;&#36827;&#21270;&#31639;&#27861;&#21644;&#20803;&#23398;&#20064;&#30340;&#25216;&#26415;&#65292;&#26377;&#25928;&#22320;&#29983;&#25104;&#26082;&#20026;DP&#21448;&#19982;&#21407;&#22987;&#22270;&#20687;&#22806;&#35266;&#30456;&#20284;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;&#25105;&#20204;&#36824;&#22312;&#27969;&#34892;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#22914;CIFAR-10&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#24182;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25928;&#29992;&#21644;&#38544;&#31169;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;DP&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating differentially private (DP) synthetic data that closely resembles the original private data without leaking sensitive user information is a scalable way to mitigate privacy concerns in the current data-driven world. In contrast to current practices that train customized models for this task, we aim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundation models as blackboxes and only utilize their inference APIs. Such API-based, training-free approaches are easier to deploy as exemplified by the recent surge in the number of API-based apps. These approaches can also leverage the power of large foundation models which are accessible via their inference APIs while the model weights are unreleased. However, this comes with greater challenges due to strictly more restrictive model access and the additional need to protect privacy from the API provider.  In this paper, we present a new framework called Private Evolution (PE) to solve this problem and show its ini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.13646</link><description>&lt;p&gt;
&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#29992;&#20110;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Data-driven Piecewise Affine Decision Rules for Stochastic Programming with Covariate Information. (arXiv:2304.13646v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#25968;&#20540;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#24102;&#21327;&#21464;&#20449;&#24687;&#30340;&#38543;&#26426;&#35268;&#21010;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#20837;&#38750;&#20984;&#20998;&#27573;&#20223;&#23556;&#20915;&#31574;&#35268;&#21017;(PADR)&#30340;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;(ERM)&#26041;&#27861;&#65292;&#26088;&#22312;&#23398;&#20064;&#29305;&#24449;&#19982;&#26368;&#20248;&#20915;&#31574;&#20043;&#38388;&#30340;&#30452;&#25509;&#26144;&#23556;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#20110;PADR&#30340;ERM&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#65292;&#21487;&#29992;&#20110;&#26080;&#32422;&#26463;&#38382;&#39064;&#65292;&#20197;&#21450;&#32422;&#26463;&#38382;&#39064;&#30340;&#28176;&#36817;&#19968;&#33268;&#24615;&#32467;&#26524;&#12290;&#20026;&#20102;&#35299;&#20915;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;ERM&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22686;&#24378;&#30340;&#38543;&#26426;&#20027;&#23548;&#19979;&#38477;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#27839;&#65288;&#22797;&#21512;&#24378;&#65289;&#26041;&#21521;&#31283;&#23450;&#24615;&#30340;&#28176;&#36817;&#25910;&#25947;&#20197;&#21450;&#22797;&#26434;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PADR-based ERM&#26041;&#27861;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#38750;&#20984;&#22411;SP&#38382;&#39064;&#65292;&#24182;&#20855;&#26377;&#29702;&#35770;&#19968;&#33268;&#24615;&#20445;&#35777;&#21644;&#35745;&#31639;&#21487;&#22788;&#29702;&#24615;&#12290;&#25968;&#20540;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#35774;&#32622;&#19979;&#65292;PADR-based ERM&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Focusing on stochastic programming (SP) with covariate information, this paper proposes an empirical risk minimization (ERM) method embedded within a nonconvex piecewise affine decision rule (PADR), which aims to learn the direct mapping from features to optimal decisions. We establish the nonasymptotic consistency result of our PADR-based ERM model for unconstrained problems and asymptotic consistency result for constrained ones. To solve the nonconvex and nondifferentiable ERM problem, we develop an enhanced stochastic majorization-minimization algorithm and establish the asymptotic convergence to (composite strong) directional stationarity along with complexity analysis. We show that the proposed PADR-based ERM method applies to a broad class of nonconvex SP problems with theoretical consistency guarantees and computational tractability. Our numerical study demonstrates the superior performance of PADR-based ERM methods compared to state-of-the-art approaches under various settings,
&lt;/p&gt;</description></item></channel></rss>