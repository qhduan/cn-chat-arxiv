<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#26679;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#23646;&#24615;&#22270;&#20013;&#22788;&#29702;&#33410;&#28857;&#12289;&#36793;&#21644;&#36335;&#24452;&#20551;&#35774;&#65292;&#36890;&#36807;&#25552;&#20986;&#36335;&#24452;&#20551;&#35774;&#24863;&#30693;&#37319;&#26679;&#22120; PHASE &#20197;&#21450; PHASEopt&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#25277;&#26679;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20551;&#35774;&#26816;&#39564;&#19978;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.13286</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#26679;&#30340;&#22823;&#23646;&#24615;&#22270;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Sampling-based Framework for Hypothesis Testing on Large Attributed Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#26679;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#22823;&#23646;&#24615;&#22270;&#20013;&#22788;&#29702;&#33410;&#28857;&#12289;&#36793;&#21644;&#36335;&#24452;&#20551;&#35774;&#65292;&#36890;&#36807;&#25552;&#20986;&#36335;&#24452;&#20551;&#35774;&#24863;&#30693;&#37319;&#26679;&#22120; PHASE &#20197;&#21450; PHASEopt&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#19988;&#39640;&#25928;&#30340;&#25277;&#26679;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20551;&#35774;&#26816;&#39564;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20551;&#35774;&#26816;&#39564;&#26159;&#19968;&#31181;&#29992;&#20110;&#20174;&#26679;&#26412;&#25968;&#25454;&#20013;&#24471;&#20986;&#20851;&#20110;&#24635;&#20307;&#30340;&#32467;&#35770;&#30340;&#32479;&#35745;&#26041;&#27861;&#65292;&#36890;&#24120;&#29992;&#34920;&#26684;&#34920;&#31034;&#12290;&#38543;&#30528;&#29616;&#23454;&#24212;&#29992;&#20013;&#22270;&#34920;&#31034;&#30340;&#26222;&#21450;&#65292;&#22270;&#20013;&#30340;&#20551;&#35774;&#26816;&#39564;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#23646;&#24615;&#22270;&#20013;&#30340;&#33410;&#28857;&#12289;&#36793;&#21644;&#36335;&#24452;&#20551;&#35774;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#25277;&#26679;&#30340;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#21487;&#20197;&#23481;&#32435;&#29616;&#26377;&#30340;&#20551;&#35774;&#19981;&#21487;&#30693;&#30340;&#22270;&#25277;&#26679;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#20934;&#30830;&#21644;&#39640;&#25928;&#30340;&#25277;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36335;&#24452;&#20551;&#35774;&#24863;&#30693;&#37319;&#26679;&#22120; PHASE&#65292;&#23427;&#26159;&#19968;&#31181;&#32771;&#34385;&#20551;&#35774;&#20013;&#25351;&#23450;&#36335;&#24452;&#30340; m-&#32500;&#38543;&#26426;&#28216;&#36208;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20248;&#21270;&#20102;&#20854;&#26102;&#38388;&#25928;&#29575;&#24182;&#25552;&#20986;&#20102; PHASEopt&#12290;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#21033;&#29992;&#24120;&#35265;&#30340;&#22270;&#25277;&#26679;&#26041;&#27861;&#36827;&#34892;&#20551;&#35774;&#26816;&#39564;&#65292;&#24182;&#19988;&#22312;&#20934;&#30830;&#24615;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#20551;&#35774;&#24863;&#30693;&#25277;&#26679;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13286v1 Announce Type: cross  Abstract: Hypothesis testing is a statistical method used to draw conclusions about populations from sample data, typically represented in tables. With the prevalence of graph representations in real-life applications, hypothesis testing in graphs is gaining importance. In this work, we formalize node, edge, and path hypotheses in attributed graphs. We develop a sampling-based hypothesis testing framework, which can accommodate existing hypothesis-agnostic graph sampling methods. To achieve accurate and efficient sampling, we then propose a Path-Hypothesis-Aware SamplEr, PHASE, an m- dimensional random walk that accounts for the paths specified in a hypothesis. We further optimize its time efficiency and propose PHASEopt. Experiments on real datasets demonstrate the ability of our framework to leverage common graph sampling methods for hypothesis testing, and the superiority of hypothesis-aware sampling in terms of accuracy and time efficiency.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;</title><link>https://arxiv.org/abs/2403.02774</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#23610;&#24230;&#21644;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#22330;&#38477;&#23610;&#24230;&#19982;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02774
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(ESM)&#27169;&#25311;&#23545;&#20110;&#35780;&#20272;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#23545;&#29983;&#24577;&#21644;&#31038;&#20250;&#32463;&#27982;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;ESM&#27169;&#25311;&#30340;&#38477;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#27599;&#20010;ESM&#37117;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#27668;&#20505;&#39044;&#27979;&#25928;&#26524;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#19968;&#33268;&#24615;&#27169;&#22411;(CM)&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#39640;&#25928;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;ESM&#27169;&#25311;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#26041;&#27861;&#20197;&#21482;&#21463;&#35266;&#27979;&#21442;&#32771;&#25968;&#25454;&#38480;&#21046;&#30340;&#20998;&#36776;&#29575;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CM&#22312;&#32500;&#25345;&#39640;&#21487;&#25511;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02774v1 Announce Type: cross  Abstract: Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#21644;&#28388;&#27874;&#22120;&#21407;&#23376;&#30340;&#27010;&#24565;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#22823;&#22411;&#21367;&#31215;&#27169;&#22411;&#26102;&#20165;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26469;&#25552;&#21462;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00269</link><description>&lt;p&gt;
&#22823;&#22411;&#21367;&#31215;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Tuning of Large Convolutional Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00269
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#21644;&#28388;&#27874;&#22120;&#21407;&#23376;&#30340;&#27010;&#24565;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#24494;&#35843;&#22823;&#22411;&#21367;&#31215;&#27169;&#22411;&#26102;&#20165;&#35843;&#25972;&#23569;&#37327;&#21442;&#25968;&#26469;&#25552;&#21462;&#20219;&#21153;&#29305;&#23450;&#34920;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#25152;&#38656;&#30340;&#39640;&#35745;&#31639;&#21644;&#21442;&#25968;&#22797;&#26434;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#20165;&#26356;&#26032;&#19979;&#28216;&#20219;&#21153;&#30340;&#37096;&#20998;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20316;&#36890;&#24120;&#24573;&#35270;&#20102;&#21367;&#31215;&#26680;&#30340;&#29420;&#29305;&#23646;&#24615;&#65292;&#32780;&#21367;&#31215;&#26680;&#20173;&#28982;&#26159;&#35768;&#22810;&#22823;&#22411;&#27169;&#22411;&#30340;&#22522;&#26412;&#20803;&#32032;&#65292;&#27604;&#22914;Stable Diffusion&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#22312;&#27599;&#20010;&#32593;&#32476;&#23618;&#20869;&#20998;&#35299;&#21367;&#31215;&#26680;&#21040;&#19968;&#23567;&#32452;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#20803;&#32032;&#65292;&#21363;&#28388;&#27874;&#22120;&#21407;&#23376;&#65292;&#24341;&#20837;&#20102;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#20165;&#35843;&#25972;&#28388;&#27874;&#22120;&#21407;&#23376;&#65288;&#36890;&#24120;&#20026;&#20960;&#30334;&#20010;&#21442;&#25968;&#65289;&#23545;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#25552;&#21462;&#20219;&#21153;&#29305;&#23450;&#30340;&#34920;&#31034;&#12290;&#20026;&#20102;&#28508;&#22312;&#22320;&#25193;&#23637;&#35843;&#25972;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36882;&#24402;&#22320;&#23558;&#27599;&#20010;&#31579;&#36873;&#21407;&#23376;&#20998;&#35299;&#21040;&#21478;&#19968;&#32452;&#31579;&#36873;&#21407;&#23376;&#26469;&#29983;&#25104;&#19968;&#20010;&#36807;&#23436;&#22791;&#30340;&#28388;&#27874;&#22120;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00269v1 Announce Type: cross  Abstract: To address the high computational and parameter complexity associated with fine-tuning large pre-trained models, researchers have developed parameter-efficient methods, where only partial parameters are updated for downstream tasks. However, these works often overlook the distinct properties of convolutional kernels, which still remain essential elements in many large models, such as Stable Diffusion. In this study, we first introduce filter subspace by decomposing convolutional kernels within each network layer over a small set of filter subspace elements, referred to as filter atoms. We then fine-tune these models to extract task-specific representation by only adapting the filter atoms, a few hundred parameters typically. To potentially expand the parameter space for tuning, we further show a simple approach to generate an overcomplete filter subspace by recursively decomposing each filter atom over another set of filter atoms. The 
&lt;/p&gt;</description></item><item><title>&#26435;&#37325;&#21442;&#25968;&#22312;&#35774;&#22791;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#21151;&#32791;&#24433;&#21709;&#26377;&#25152;&#19981;&#21516;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#26435;&#37325;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#26377;&#38024;&#23545;&#24615;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#33021;&#28304;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;47%&#32780;&#32500;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2402.13076</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#26435;&#37325;&#37117;&#26159;&#24179;&#31561;&#30340;: &#22312;&#35774;&#22791;&#19978;&#22686;&#24378;&#33021;&#25928;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Not All Weights Are Created Equal: Enhancing Energy Efficiency in On-Device Streaming Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13076
&lt;/p&gt;
&lt;p&gt;
&#26435;&#37325;&#21442;&#25968;&#22312;&#35774;&#22791;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#21151;&#32791;&#24433;&#21709;&#26377;&#25152;&#19981;&#21516;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#22522;&#20110;&#26435;&#37325;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#26377;&#38024;&#23545;&#24615;&#21387;&#32553;&#26041;&#27861;&#65292;&#23558;&#33021;&#28304;&#20351;&#29992;&#20943;&#23569;&#39640;&#36798;47%&#32780;&#32500;&#25345;&#27169;&#22411;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#28040;&#32791;&#22312;&#35774;&#22791;&#19978;&#30340;&#27969;&#24335;&#35821;&#38899;&#35782;&#21035;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#22240;&#20026;&#23427;&#30452;&#25509;&#24433;&#21709;&#29992;&#25143;&#20307;&#39564;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#20013;&#30340;&#26435;&#37325;&#21442;&#25968;&#22914;&#20309;&#24433;&#21709;&#36825;&#20123;&#27169;&#22411;&#30340;&#24635;&#20307;&#21151;&#32791;&#12290;&#25105;&#20204;&#21457;&#29616;&#26435;&#37325;&#21442;&#25968;&#23545;&#21151;&#32791;&#30340;&#24433;&#21709;&#22240;&#22810;&#31181;&#22240;&#32032;&#32780;&#24322;&#65292;&#21463;&#21040;&#35843;&#29992;&#39057;&#29575;&#21450;&#20854;&#22312;&#20869;&#23384;&#20013;&#30340;&#20301;&#32622;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#20973;&#20511;&#36825;&#19968;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#26088;&#22312;&#20248;&#21270;&#35774;&#22791;&#19978;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#35774;&#35745;&#25351;&#21335;&#12290;&#36825;&#20123;&#25351;&#21335;&#20391;&#37325;&#20110;&#22312;&#23613;&#37327;&#19981;&#26174;&#33879;&#24433;&#21709;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#21151;&#32791;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#26435;&#37325;&#21442;&#25968;&#21464;&#21270;&#25935;&#24863;&#24615;&#30340;&#26377;&#38024;&#23545;&#24615;&#21387;&#32553;&#65292;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;47%&#30340;&#33021;&#28304;&#20351;&#29992;&#20943;&#23569;&#65292;&#21516;&#26102;&#20445;&#25345;&#31867;&#20284;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#24182;&#25913;&#21892;&#23454;&#26102;&#27969;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13076v1 Announce Type: cross  Abstract: Power consumption plays an important role in on-device streaming speech recognition, as it has a direct impact on the user experience. This study delves into how weight parameters in speech recognition models influence the overall power consumption of these models. We discovered that the impact of weight parameters on power consumption varies, influenced by factors including how often they are invoked and their placement in memory. Armed with this insight, we developed design guidelines aimed at optimizing on-device speech recognition models. These guidelines focus on minimizing power use without substantially affecting accuracy. Our method, which employs targeted compression based on the varying sensitivities of weight parameters, demonstrates superior performance compared to state-of-the-art compression methods. It achieves a reduction in energy usage of up to 47% while maintaining similar model accuracy and improving the real-time f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.05864</link><description>&lt;p&gt;
Permute-and-Flip&#65306;&#19968;&#31181;&#20855;&#26377;&#26368;&#20339;&#40065;&#26834;&#24615;&#21644;&#21487;&#21152;&#27700;&#21360;&#30340;LLMs&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Permute-and-Flip: An optimally robust and watermarkable decoder for LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05864
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#65292;&#20854;&#20855;&#26377;&#26368;&#20339;&#30340;&#40065;&#26834;&#24615;&#21644;&#36136;&#37327;-&#40065;&#26834;&#24615;&#30340; tradeoff&#65292;&#19988;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#12290;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#30340;&#27700;&#21360;&#26041;&#26696;&#65292;&#33021;&#22815;&#20445;&#25345;&#26679;&#26412;&#30340;&#20998;&#24067;&#19981;&#21464;&#65292;&#24182;&#23454;&#29616;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;PF&#35299;&#30721;&#22120;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65292;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Permute-and-Flip&#65288;PF&#65289;&#35299;&#30721;&#22120;&#30340;&#26032;&#35299;&#30721;&#26041;&#27861;&#12290;&#23427;&#20855;&#26377;&#19982;&#26631;&#20934;&#37319;&#26679;&#35299;&#30721;&#22120;&#30456;&#20284;&#30340;&#40065;&#26834;&#24615;&#29305;&#24615;&#65292;&#20294;&#22312;&#36136;&#37327;&#21644;&#40065;&#26834;&#24615;&#30340; tradeoff &#19978;&#35777;&#26126;&#27604;&#37319;&#26679;&#26041;&#27861;&#26356;&#22909;&#65292;&#19988;&#27704;&#36828;&#19981;&#20250;&#24046;&#20110;&#20219;&#20309;&#20854;&#20182;&#35299;&#30721;&#22120;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#31867;&#20284;&#20110;Aaronson&#30340;Gumbel&#27700;&#21360;&#30340;&#21152;&#23494;&#27700;&#21360;&#26041;&#26696;&#65292;&#20294;&#26159;&#38024;&#23545;PF&#35299;&#30721;&#22120;&#32780;&#33258;&#28982;&#37327;&#36523;&#23450;&#21046;&#12290;&#35813;&#27700;&#21360;&#26041;&#26696;&#19981;&#25913;&#21464;&#26679;&#26412;&#30340;&#20998;&#24067;&#65292;&#21516;&#26102;&#20801;&#35768;&#20219;&#24847;&#20302;&#30340;&#20551;&#38451;&#24615;&#29575;&#21644;&#39640;&#30340;&#21484;&#22238;&#29575;&#65292;&#21482;&#35201;&#29983;&#25104;&#30340;&#25991;&#26412;&#20855;&#26377;&#39640;&#29109;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;PF&#35299;&#30721;&#22120;&#65288;&#21450;&#20854;&#24102;&#26377;&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#22312;&#22256;&#24785;&#24230;&#26041;&#38754;&#26126;&#26174;&#20248;&#20110;&#26420;&#32032;&#37319;&#26679;&#65288;&#21450;&#20854;&#24102;&#26377;Gumbel&#27700;&#21360;&#30340;&#23545;&#24212;&#29289;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#21516;&#30340;&#40065;&#26834;&#24615;&#65288;&#21644;&#21487;&#26816;&#27979;&#24615;&#65289;&#65292;&#22240;&#27492;&#20026;LLM&#35299;&#30721;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26032;&#26041;&#27861;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/XuandongZhao/pf-decoding&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a new decoding method called Permute-and-Flip (PF) decoder. It enjoys robustness properties similar to the standard sampling decoder, but is provably up to 2x better in its quality-robustness tradeoff than sampling and never worse than any other decoder. We also design a cryptographic watermarking scheme analogous to Aaronson's Gumbel watermark, but naturally tailored for PF decoder. The watermarking scheme does not change the distribution to sample, while allowing arbitrarily low false positive rate and high recall whenever the generated text has high entropy. Our experiments show that the PF decoder (and its watermarked counterpart) significantly outperform(s) naive sampling (and it's Gumbel watermarked counterpart) in terms of perplexity, while retaining the same robustness (and detectability), hence making it a promising new approach for LLM decoding. The code is available at https://github.com/XuandongZhao/pf-decoding
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;</title><link>https://arxiv.org/abs/2402.01881</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Agent for Hyper-Parameter Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01881
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;AgentHPO&#25216;&#26415;&#36890;&#36807;&#33258;&#21160;&#21270;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#22823;&#22823;&#20943;&#23569;&#20102;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#12289;&#22823;&#37327;&#23454;&#39564;&#20197;&#21450;&#39640;&#35745;&#31639;&#21644;&#20154;&#21147;&#36164;&#28304;&#12290;&#23613;&#31649;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35797;&#39564;&#25928;&#29575;&#12289;&#35774;&#32622;&#22797;&#26434;&#24615;&#21644;&#20114;&#25805;&#20316;&#24615;&#26041;&#38754;&#20173;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#33258;&#21160;&#21270;&#19981;&#21516;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#65292;&#31216;&#20026;AgentHPO&#65288;LLM Agent-based Hyperparameter Optimization&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AgentHPO&#33258;&#20027;&#22788;&#29702;&#20219;&#21153;&#20449;&#24687;&#65292;&#26681;&#25454;&#21382;&#21490;&#35797;&#39564;&#23545;&#29305;&#23450;&#36229;&#21442;&#25968;&#65288;HPs&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36827;&#34892;&#36845;&#20195;&#20248;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;AutoML&#26041;&#27861;&#30456;&#27604;&#65292;&#36825;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#20248;&#21270;&#36807;&#31243;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#25152;&#38656;&#30340;&#35797;&#39564;&#27425;&#25968;&#65292;&#31616;&#21270;&#20102;&#35774;&#32622;&#36807;&#31243;&#65292;&#24182;&#25552;&#21319;&#20102;&#35299;&#37322;&#24615;&#21644;&#29992;&#25143;&#20449;&#20219;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization is critical in modern machine learning, requiring expert knowledge, numerous trials, and high computational and human resources. Despite the advancements in Automated Machine Learning (AutoML), challenges in terms of trial efficiency, setup complexity, and interoperability still persist. To address these issues, we introduce a novel paradigm leveraging Large Language Models (LLMs) to automate hyperparameter optimization across diverse machine learning tasks, which is named AgentHPO (short for LLM Agent-based Hyperparameter Optimization). Specifically, AgentHPO processes the task information autonomously, conducts experiments with specific hyperparameters (HPs), and iteratively optimizes them based on historical trials. This human-like optimization process largely reduces the number of required trials, simplifies the setup process, and enhances interpretability and user trust, compared to traditional AutoML methods. Extensive empirical experiments conducted o
&lt;/p&gt;</description></item><item><title>SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.09622</link><description>&lt;p&gt;
SMOOTHIE: &#36719;&#20214;&#20998;&#26512;&#30340;&#36229;&#21442;&#25968;&#20248;&#21270;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE: A Theory of Hyper-parameter Optimization for Software Analytics. (arXiv:2401.09622v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09622
&lt;/p&gt;
&lt;p&gt;
SMOOTHIE&#26159;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#26032;&#22411;&#26041;&#27861;&#65292;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#24212;&#29992;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#35843;&#25972;&#23398;&#20064;&#22120;&#25511;&#21046;&#21442;&#25968;&#30340;&#40657;&#39764;&#27861;&#12290;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#65292;&#32463;&#24120;&#21457;&#29616;&#35843;&#20248;&#21487;&#20197;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#36229;&#21442;&#25968;&#20248;&#21270;&#22312;&#36719;&#20214;&#20998;&#26512;&#20013;&#36890;&#24120;&#34987;&#24456;&#23569;&#25110;&#24456;&#24046;&#22320;&#24212;&#29992;&#65292;&#21487;&#33021;&#26159;&#22240;&#20026;&#25506;&#32034;&#25152;&#26377;&#21442;&#25968;&#36873;&#39033;&#30340;CPU&#25104;&#26412;&#22826;&#39640;&#12290;&#25105;&#20204;&#20551;&#35774;&#24403;&#25439;&#22833;&#20989;&#25968;&#30340;&#8220;&#20809;&#28369;&#24230;&#8221;&#26356;&#22909;&#26102;&#65292;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#26356;&#24378;&#12290;&#36825;&#20010;&#29702;&#35770;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#21487;&#20197;&#24456;&#24555;&#27979;&#35797;&#19981;&#21516;&#36229;&#21442;&#25968;&#36873;&#25321;&#23545;&#8220;&#20809;&#28369;&#24230;&#8221;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#22120;&#65292;&#22312;&#19968;&#20010;epoch&#20043;&#21518;&#23601;&#21487;&#20197;&#36827;&#34892;&#27979;&#35797;&#65289;&#12290;&#20026;&#20102;&#27979;&#35797;&#36825;&#20010;&#29702;&#35770;&#65292;&#26412;&#25991;&#23454;&#29616;&#21644;&#27979;&#35797;&#20102;SMOOTHIE&#65292;&#19968;&#31181;&#36890;&#36807;&#32771;&#34385;&#8220;&#20809;&#28369;&#24230;&#8221;&#26469;&#24341;&#23548;&#20248;&#21270;&#30340;&#26032;&#22411;&#36229;&#21442;&#25968;&#20248;&#21270;&#22120;&#12290;&#26412;&#25991;&#30340;&#23454;&#39564;&#23558;SMOOTHIE&#24212;&#29992;&#20110;&#22810;&#20010;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21253;&#25324;&#65288;a&#65289;GitHub&#38382;&#39064;&#23551;&#21629;&#39044;&#27979;&#65307;&#65288;b&#65289;&#38745;&#24577;&#20195;&#30721;&#35686;&#21578;&#20013;&#38169;&#35823;&#35686;&#25253;&#30340;&#26816;&#27979;&#65307;&#65288;c&#65289;&#32570;&#38519;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter optimization is the black art of tuning a learner's control parameters. In software analytics, a repeated result is that such tuning can result in dramatic performance improvements. Despite this, hyper-parameter optimization is often applied rarely or poorly in software analytics--perhaps due to the CPU cost of exploring all those parameter options can be prohibitive.  We theorize that learners generalize better when the loss landscape is ``smooth''. This theory is useful since the influence on ``smoothness'' of different hyper-parameter choices can be tested very quickly (e.g. for a deep learner, after just one epoch).  To test this theory, this paper implements and tests SMOOTHIE, a novel hyper-parameter optimizer that guides its optimizations via considerations of ``smothness''. The experiments of this paper test SMOOTHIE on numerous SE tasks including (a) GitHub issue lifetime prediction; (b) detecting false alarms in static code warnings; (c) defect prediction, and
&lt;/p&gt;</description></item><item><title>&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2310.20360</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#25968;&#23398;&#20171;&#32461;&#65306;&#26041;&#27861;&#12289;&#23454;&#29616;&#21644;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory. (arXiv:2310.20360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.20360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#25552;&#20379;&#20102;&#23545;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#25968;&#23398;&#20171;&#32461;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#29702;&#35770;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#12290;&#24076;&#26395;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#20070;&#26088;&#22312;&#20171;&#32461;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20027;&#39064;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65288;&#22914;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12289;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#12289;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#21644;&#24102;&#26377;&#25209;&#24402;&#19968;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65289;&#20197;&#21450;&#19981;&#21516;&#30340;&#20248;&#21270;&#31639;&#27861;&#65288;&#22914;&#22522;&#26412;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#27861;&#12289;&#21152;&#36895;&#26041;&#27861;&#21644;&#33258;&#36866;&#24212;&#26041;&#27861;&#65289;&#12290;&#25105;&#20204;&#36824;&#28085;&#30422;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#20960;&#20010;&#29702;&#35770;&#26041;&#38754;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#36924;&#36817;&#33021;&#21147;&#65288;&#21253;&#25324;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#31215;&#20998;&#65289;&#12289;&#20248;&#21270;&#29702;&#35770;&#65288;&#21253;&#25324;Kurdyka-Lojasiewicz&#19981;&#31561;&#24335;&#65289;&#21644;&#27867;&#21270;&#35823;&#24046;&#12290;&#22312;&#26412;&#20070;&#30340;&#26368;&#21518;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#19968;&#20123;&#29992;&#20110;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#28145;&#24230;&#23398;&#20064;&#36924;&#36817;&#26041;&#27861;&#65292;&#21253;&#25324;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#21644;&#28145;&#24230;Galerkin&#26041;&#27861;&#12290;&#24076;&#26395;&#26412;&#20070;&#33021;&#23545;&#23398;&#29983;&#21644;&#31185;&#23398;&#23478;&#20204;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#37327;&#21270;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#65292;&#23545;&#20110;&#23494;&#24230;&#32858;&#31867;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.12806</link><description>&lt;p&gt;
DCSI -- &#22522;&#20110;&#20998;&#31163;&#21644;&#36830;&#36890;&#24615;&#30340;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
DCSI -- An improved measure of cluster separability based on separation and connectedness. (arXiv:2310.12806v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#37327;&#21270;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#65292;&#23545;&#20110;&#23494;&#24230;&#32858;&#31867;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#23450;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#30340;&#31867;&#21035;&#26631;&#31614;&#26159;&#21542;&#23545;&#24212;&#20110;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#23545;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#38598;&#35780;&#20272;&#32858;&#31867;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20010;&#29305;&#24615;&#21487;&#20197;&#36890;&#36807;&#21487;&#20998;&#31163;&#24615;&#24230;&#37327;&#26469;&#37327;&#21270;&#12290;&#29616;&#26377;&#25991;&#29486;&#30340;&#32508;&#36848;&#26174;&#31034;&#65292;&#26082;&#26377;&#30340;&#22522;&#20110;&#20998;&#31867;&#30340;&#22797;&#26434;&#24615;&#24230;&#37327;&#26041;&#27861;&#21644;&#32858;&#31867;&#26377;&#25928;&#24615;&#25351;&#26631; (CVIs) &#37117;&#27809;&#26377;&#20805;&#20998;&#34701;&#20837;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#30340;&#26680;&#24515;&#29305;&#24449;&#65306;&#31867;&#38388;&#20998;&#31163;&#21644;&#31867;&#20869;&#36830;&#36890;&#24615;&#12290;&#19968;&#31181;&#26032;&#24320;&#21457;&#30340;&#24230;&#37327;&#26041;&#27861; (&#23494;&#24230;&#32858;&#31867;&#21487;&#20998;&#31163;&#24615;&#25351;&#25968;, DCSI) &#26088;&#22312;&#37327;&#21270;&#36825;&#20004;&#20010;&#29305;&#24449;&#65292;&#24182;&#19988;&#20063;&#21487;&#29992;&#20316; CVI&#12290;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;DCSI &#19982;&#36890;&#36807;&#35843;&#25972;&#20848;&#24503;&#25351;&#25968; (ARI) &#27979;&#37327;&#30340;DBSCAN&#30340;&#24615;&#33021;&#20043;&#38388;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#65292;&#20294;&#22312;&#23545;&#22810;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#23494;&#24230;&#32858;&#31867;&#19981;&#36866;&#24403;&#30340;&#37325;&#21472;&#31867;&#21035;&#26102;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#23545;&#32463;&#24120;&#20351;&#29992;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#35814;&#32454;&#35780;&#20272;&#26174;&#31034;&#65292;DCSI &#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#23494;&#24230;&#32858;&#31867;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whether class labels in a given data set correspond to meaningful clusters is crucial for the evaluation of clustering algorithms using real-world data sets. This property can be quantified by separability measures. A review of the existing literature shows that neither classification-based complexity measures nor cluster validity indices (CVIs) adequately incorporate the central aspects of separability for density-based clustering: between-class separation and within-class connectedness. A newly developed measure (density cluster separability index, DCSI) aims to quantify these two characteristics and can also be used as a CVI. Extensive experiments on synthetic data indicate that DCSI correlates strongly with the performance of DBSCAN measured via the adjusted rand index (ARI) but lacks robustness when it comes to multi-class data sets with overlapping classes that are ill-suited for density-based hard clustering. Detailed evaluation on frequently used real-world data sets shows that
&lt;/p&gt;</description></item><item><title>LGL-BCI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;EEG&#25968;&#25454;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#25429;&#25417;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;LGL-BCI&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.08051</link><description>&lt;p&gt;
LGL-BCI&#65306;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#36816;&#21160;&#24819;&#35937;&#30340;&#33041;&#26426;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
LGL-BCI: A Lightweight Geometric Learning Framework for Motor Imagery-Based Brain-Computer Interfaces. (arXiv:2310.08051v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08051
&lt;/p&gt;
&lt;p&gt;
LGL-BCI&#26159;&#19968;&#31181;&#36731;&#37327;&#32423;&#20960;&#20309;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22788;&#29702;EEG&#25968;&#25454;&#22312;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#25429;&#25417;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#24182;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#20197;&#25552;&#39640;&#25512;&#26029;&#36895;&#24230;&#12290;&#23454;&#39564;&#35777;&#26126;LGL-BCI&#30456;&#27604;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#26159;&#19968;&#31181;&#20351;&#29992;&#33041;&#20449;&#21495;&#19982;&#22806;&#37096;&#35774;&#22791;&#36827;&#34892;&#20132;&#20114;&#30340;&#24320;&#21019;&#24615;&#25216;&#26415;&#12290;&#23613;&#31649;&#26377;&#25152;&#36827;&#23637;&#65292;&#22522;&#20110;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#36816;&#21160;&#24819;&#35937;&#20219;&#21153;&#38754;&#20020;&#25361;&#25112;&#65292;&#22914;&#24133;&#24230;&#21644;&#30456;&#20301;&#21464;&#24322;&#65292;&#20197;&#21450;&#22797;&#26434;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#38656;&#35201;&#26356;&#23567;&#30340;&#27169;&#22411;&#22823;&#23567;&#21644;&#26356;&#24555;&#30340;&#25512;&#26029;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;LGL-BCI&#26694;&#26550;&#65292;&#37319;&#29992;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#22788;&#29702;&#38750;&#27431;&#20960;&#37324;&#24503;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;EEG&#65292;&#29305;&#21035;&#26159;&#23545;&#31216;&#27491;&#23450;&#65288;SPD&#65289;&#27969;&#24418;&#31354;&#38388;&#12290;LGL-BCI&#25552;&#20379;&#20102;&#31283;&#20581;&#30340;EEG&#25968;&#25454;&#34920;&#31034;&#65292;&#24182;&#25429;&#25417;&#20102;&#31354;&#38388;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#29305;&#24449;&#20998;&#35299;&#31639;&#27861;&#36827;&#34892;EEG&#36890;&#36947;&#36873;&#25321;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#20943;&#23569;SPD&#30697;&#38453;&#30340;&#32500;&#24230;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#25512;&#26029;&#36895;&#24230;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#19982;&#24403;&#21069;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#65292;LGL-BCI&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#31361;&#20986;&#20102;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#22312;&#36816;&#21160;&#24819;&#35937;-&#33041;&#26426;&#25509;&#21475;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Brain-Computer Interfaces (BCIs) are a groundbreaking technology for interacting with external devices using brain signals. Despite advancements, electroencephalogram (EEG)-based Motor Imagery (MI) tasks face challenges like amplitude and phase variability, and complex spatial correlations, with a need for smaller model size and faster inference. This study introduces the LGL-BCI framework, employing a Geometric Deep Learning Framework for EEG processing in non-Euclidean metric spaces, particularly the Symmetric Positive Definite (SPD) Manifold space. LGL-BCI offers robust EEG data representation and captures spatial correlations. We propose an EEG channel selection solution via a feature decomposition algorithm to reduce SPD matrix dimensionality, with a lossless transformation boosting inference speed. Extensive experiments show LGL-BCI's superior accuracy and efficiency compared to current solutions, highlighting geometric deep learning's potential in MI-BCI applications. The effici
&lt;/p&gt;</description></item><item><title>AdaRec&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36317;&#31163;&#30340;&#34920;&#31034;&#25439;&#22833;&#26469;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.03984</link><description>&lt;p&gt;
AdaRec&#65306;&#29992;&#20110;&#22686;&#24378;&#29992;&#25143;&#38271;&#26399;&#21442;&#19982;&#24230;&#30340;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
AdaRec: Adaptive Sequential Recommendation for Reinforcing Long-term User Engagement. (arXiv:2310.03984v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03984
&lt;/p&gt;
&lt;p&gt;
AdaRec&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36317;&#31163;&#30340;&#34920;&#31034;&#25439;&#22833;&#26469;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#20013;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39034;&#24207;&#25512;&#33616;&#20219;&#21153;&#20013;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#26469;&#20248;&#21270;&#29992;&#25143;&#30340;&#38271;&#26399;&#21442;&#19982;&#24230;&#12290;&#22823;&#35268;&#27169;&#22312;&#32447;&#25512;&#33616;&#31995;&#32479;&#38754;&#20020;&#30340;&#19968;&#20010;&#25361;&#25112;&#26159;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#65288;&#22914;&#20114;&#21160;&#39057;&#29575;&#21644;&#20445;&#30041;&#20542;&#21521;&#65289;&#30340;&#19981;&#26029;&#22797;&#26434;&#21464;&#21270;&#12290;&#24403;&#23558;&#38382;&#39064;&#24314;&#27169;&#20026;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#26102;&#65292;&#25512;&#33616;&#31995;&#32479;&#30340;&#21160;&#24577;&#21644;&#22870;&#21169;&#20989;&#25968;&#20250;&#19981;&#26029;&#21463;&#21040;&#36825;&#20123;&#21464;&#21270;&#30340;&#24433;&#21709;&#12290;&#29616;&#26377;&#30340;&#25512;&#33616;&#31995;&#32479;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20250;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#24182;&#38590;&#20197;&#36866;&#24212;&#36825;&#31181;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#39034;&#24207;&#25512;&#33616;&#65288;AdaRec&#65289;&#65292;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;AdaRec&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#34920;&#31034;&#25439;&#22833;&#65292;&#20174;&#29992;&#25143;&#30340;&#20114;&#21160;&#36712;&#36857;&#20013;&#25552;&#21462;&#28508;&#22312;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#21453;&#26144;&#20102;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#19982;&#24403;&#21069;&#29992;&#25143;&#34892;&#20026;&#27169;&#24335;&#30340;&#21305;&#37197;&#31243;&#24230;&#65292;&#24182;&#24110;&#21161;&#31574;&#30053;&#35782;&#21035;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#32454;&#24494;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Growing attention has been paid to Reinforcement Learning (RL) algorithms when optimizing long-term user engagement in sequential recommendation tasks. One challenge in large-scale online recommendation systems is the constant and complicated changes in users' behavior patterns, such as interaction rates and retention tendencies. When formulated as a Markov Decision Process (MDP), the dynamics and reward functions of the recommendation system are continuously affected by these changes. Existing RL algorithms for recommendation systems will suffer from distribution shift and struggle to adapt in such an MDP. In this paper, we introduce a novel paradigm called Adaptive Sequential Recommendation (AdaRec) to address this issue. AdaRec proposes a new distance-based representation loss to extract latent information from users' interaction trajectories. Such information reflects how RL policy fits to current user behavior patterns, and helps the policy to identify subtle changes in the recomm
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2309.08454</link><description>&lt;p&gt;
&#28151;&#21512;&#32534;&#30721;&#22120;&#25903;&#25345;&#36830;&#32493;&#35821;&#38899;&#20998;&#31163;&#29992;&#20110;&#20250;&#35758;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Mixture Encoder Supporting Continuous Speech Separation for Meeting Recognition. (arXiv:2309.08454v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28151;&#21512;&#32534;&#30721;&#22120;&#26041;&#27861;&#20174;&#20004;&#20010;&#35828;&#35805;&#20154;&#24773;&#20917;&#25193;&#23637;&#21040;&#20102;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#30340;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#38656;&#35201;&#22788;&#29702;&#37325;&#21472;&#30340;&#35821;&#38899;&#12290;&#19968;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#39318;&#20808;&#23558;&#35821;&#38899;&#20998;&#31163;&#25104;&#26080;&#37325;&#21472;&#30340;&#27969;&#65292;&#28982;&#21518;&#23545;&#29983;&#25104;&#30340;&#20449;&#21495;&#36827;&#34892;ASR&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#22312;ASR&#27169;&#22411;&#20013;&#21253;&#21547;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#26041;&#27861;&#12290;&#35813;&#28151;&#21512;&#32534;&#30721;&#22120;&#21033;&#29992;&#21407;&#22987;&#37325;&#21472;&#30340;&#35821;&#38899;&#26469;&#20943;&#36731;&#35821;&#38899;&#20998;&#31163;&#24341;&#20837;&#30340;&#20266;&#24433;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#20165;&#38024;&#23545;&#20004;&#20010;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#33258;&#28982;&#30340;&#20250;&#35758;&#29615;&#22659;&#65292;&#21253;&#25324;&#20219;&#24847;&#25968;&#37327;&#30340;&#35828;&#35805;&#20154;&#21644;&#21160;&#24577;&#37325;&#21472;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#38899;&#20998;&#31163;&#22120;&#65288;&#21253;&#25324;&#24378;&#22823;&#30340;TF-GridNet&#27169;&#22411;&#65289;&#35780;&#20272;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;LibriCSS&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#20984;&#26174;&#20102;&#28151;&#21512;&#32534;&#30721;&#22120;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#36824;&#23637;&#31034;&#20102;TF-GridNet&#30340;&#24378;&#22823;&#20998;&#31163;&#33021;&#21147;&#65292;&#22823;&#22823;&#32553;&#23567;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many real-life applications of automatic speech recognition (ASR) require processing of overlapped speech. A commonmethod involves first separating the speech into overlap-free streams and then performing ASR on the resulting signals. Recently, the inclusion of a mixture encoder in the ASR model has been proposed. This mixture encoder leverages the original overlapped speech to mitigate the effect of artifacts introduced by the speech separation. Previously, however, the method only addressed two-speaker scenarios. In this work, we extend this approach to more natural meeting contexts featuring an arbitrary number of speakers and dynamic overlaps. We evaluate the performance using different speech separators, including the powerful TF-GridNet model. Our experiments show state-of-the-art performance on the LibriCSS dataset and highlight the advantages of the mixture encoder. Furthermore, they demonstrate the strong separation of TF-GridNet which largely closes the gap between previous m
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#36890;&#36807;&#27604;&#36739;qGAN&#21644;QCBM&#31561;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08448</link><description>&lt;p&gt;
&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#29616;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;qGAN&#65289;&#21644;QCBM
&lt;/p&gt;
&lt;p&gt;
Implementing Quantum Generative Adversarial Network (qGAN) and QCBM in Finance. (arXiv:2308.08448v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#30740;&#31350;&#26041;&#21521;&#65292;&#36890;&#36807;&#27604;&#36739;qGAN&#21644;QCBM&#31561;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#26159;&#19968;&#20010;&#36328;&#23398;&#31185;&#30340;&#39046;&#22495;&#65292;&#30001;&#20004;&#20010;&#26368;&#20855;&#21019;&#26032;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#32452;&#25104;&#65306;&#37327;&#23376;&#35745;&#31639;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#65292;ML&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#34987;&#35748;&#20026;&#26159;&#23558;&#21463;&#21040;&#37327;&#23376;&#35745;&#31639;&#26426;&#20852;&#36215;&#24433;&#21709;&#30340;&#31532;&#19968;&#20010;&#39046;&#22495;&#12290;&#36825;&#39033;&#24037;&#20316;&#35752;&#35770;&#20102;&#22312;&#37329;&#34701;&#20013;&#24212;&#29992;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#30340;&#19968;&#20123;&#26032;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#24050;&#22312;&#37329;&#34701;&#30028;&#24341;&#36215;&#20851;&#27880;&#30340;QML&#27169;&#22411;&#65292;&#20197;&#21450;&#20351;&#29992;&#27169;&#25311;&#29615;&#22659;&#20013;&#30340;&#30495;&#23454;&#37329;&#34701;&#25968;&#25454;&#38598;&#23545;qGAN&#65288;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65289;&#21644;QCBM&#65288;&#37327;&#23376;&#30005;&#36335;Born&#26426;&#65289;&#31561;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;&#23545;&#20110;qGAN&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#37492;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#37327;&#23376;&#30005;&#36335;&#65292;&#24182;&#23637;&#31034;&#20102;&#26410;&#26469;&#22312;&#37329;&#34701;&#39046;&#22495;&#20013;&#36890;&#36807;QML&#23454;&#29616;&#37327;&#23376;&#20248;&#21183;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning (QML) is a cross-disciplinary subject made up of two of the most exciting research areas: quantum computing and classical machine learning (ML), with ML and artificial intelligence (AI) being projected as the first fields that will be impacted by the rise of quantum machines. Quantum computers are being used today in drug discovery, material &amp; molecular modelling and finance. In this work, we discuss some upcoming active new research areas in application of quantum machine learning (QML) in finance. We discuss certain QML models that has become areas of active interest in the financial world for various applications. We use real world financial dataset and compare models such as qGAN (quantum generative adversarial networks) and QCBM (quantum circuit Born machine) among others, using simulated environments. For the qGAN, we define quantum circuits for discriminators and generators and show promises of future quantum advantage via QML in finance.
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2307.13658</link><description>&lt;p&gt;
&#20851;&#20110;AI&#38382;&#36131;&#25919;&#31574;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards an AI Accountability Policy. (arXiv:2307.13658v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.13658
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#30340;&#22238;&#24212;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#30333;&#30382;&#20070;&#26159;&#23545;&#32654;&#22269;&#22269;&#23478;&#30005;&#20449;&#21644;&#20449;&#24687;&#31649;&#29702;&#23616;&#30340;&#8220;AI&#38382;&#36131;&#25919;&#31574;&#35780;&#35770;&#35831;&#27714;&#8221;&#20316;&#20986;&#30340;&#22238;&#24212;&#12290;&#22312;&#22238;&#31572;&#30456;&#20851;&#38382;&#39064;&#30340;&#20851;&#38190;&#21477;&#23376;&#26411;&#23614;&#65292;&#25552;&#20379;&#20102;&#35201;&#27714;&#35780;&#35770;&#30340;&#38382;&#39064;&#32534;&#21495;&#30340;&#19978;&#26631;&#12290;&#35813;&#30333;&#30382;&#20070;&#25552;&#20986;&#20102;&#19968;&#32452;&#30456;&#20114;&#20851;&#32852;&#30340;AI&#38382;&#36131;&#25919;&#31574;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
This white paper is a response to the "AI Accountability Policy Request for Comments" by the National Telecommunications and Information Administration of the United States. The question numbers for which comments were requested are provided in superscripts at the end of key sentences answering the respective questions. The white paper offers a set of interconnected recommendations for an AI accountability policy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#35299;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#36890;&#36807;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#32553;&#23567;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#36890;&#36807;&#20020;&#24202;&#24212;&#29992;&#30740;&#31350;&#21644;&#24314;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#33021;&#25552;&#20379;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#36824;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#22270;&#35299;&#22411;&#35299;&#37322;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#26356;&#21463;&#25512;&#23815;&#12290;</title><link>http://arxiv.org/abs/2302.01241</link><description>&lt;p&gt;
&#22270;&#35299;&#21270;&#65306;&#21033;&#29992;&#22270;&#35299;&#22411;AI&#35299;&#37322;&#23545;&#20551;&#35774;&#24615;&#28436;&#32462;&#25512;&#29702;&#30340;&#29702;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Diagrammatization: Rationalizing with diagrammatic AI explanations for abductive-deductive reasoning on hypotheses. (arXiv:2302.01241v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#35299;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#25903;&#25345;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#36890;&#36807;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#32553;&#23567;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#36890;&#36807;&#20020;&#24202;&#24212;&#29992;&#30740;&#31350;&#21644;&#24314;&#27169;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#33021;&#25552;&#20379;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#36824;&#20855;&#26377;&#36739;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#19988;&#22270;&#35299;&#22411;&#35299;&#37322;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#26356;&#21463;&#25512;&#23815;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21487;&#35270;&#21270;&#24037;&#20855;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#29992;&#25143;&#36827;&#19968;&#27493;&#25512;&#29702;&#26469;&#35299;&#37322;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;XAI&#24212;&#35813;&#25903;&#25345;&#22270;&#35299;&#22411;&#21644;&#20551;&#35774;&#24615;&#25512;&#29702;&#65292;&#20197;&#20415;AI&#33021;&#22815;&#36827;&#34892;&#20551;&#35774;&#29983;&#25104;&#21644;&#35780;&#20272;&#65292;&#20174;&#32780;&#20943;&#23569;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22270;&#35299;&#21270;&#26041;&#27861;&#65292;&#20197;i)&#36827;&#34892;Peircean&#25512;&#23548;-&#28436;&#32462;&#25512;&#29702;&#65292;ii)&#36981;&#24490;&#39046;&#22495;&#24815;&#20363;&#65292;&#21644;iii)&#29992;&#22270;&#31034;&#25110;&#35821;&#35328;&#36827;&#34892;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;&#20020;&#24202;&#24212;&#29992;&#39046;&#22495;&#23454;&#29616;&#20102;DiagramNet&#65292;&#20197;&#39044;&#27979;&#24515;&#33039;&#21548;&#35786;&#20013;&#30340;&#24515;&#33039;&#35786;&#26029;&#65292;&#24182;&#29992;&#22522;&#20110;&#24418;&#29366;&#30340;&#26434;&#38899;&#22270;&#35299;&#36827;&#34892;&#35299;&#37322;&#12290;&#22312;&#24314;&#27169;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;DiagramNet&#19981;&#20165;&#25552;&#20379;&#20102;&#24544;&#23454;&#30340;&#26434;&#38899;&#24418;&#29366;&#35299;&#37322;&#65292;&#32780;&#19988;&#27604;&#22522;&#32447;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21307;&#23398;&#29983;&#30340;&#23450;&#24615;&#29992;&#25143;&#30740;&#31350;&#23637;&#31034;&#20102;&#22270;&#35299;&#22411;&#35299;&#37322;&#30340;&#21487;&#29702;&#35299;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#24182;&#34920;&#26126;&#22312;&#20020;&#24202;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#35299;&#24335;&#35299;&#37322;&#27604;&#20854;&#20182;&#26041;&#24335;&#26356;&#21463;&#25512;&#23815;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many visualizations have been developed for explainable AI (XAI), but they often require further reasoning by users to interpret. We argue that XAI should support diagrammatic and abductive reasoning for the AI to perform hypothesis generation and evaluation to reduce the interpretability gap. We propose Diagrammatization to i) perform Peircean abductive-deductive reasoning, ii) follow domain conventions, and iii) explain with diagrams visually or verbally. We implemented DiagramNet for a clinical application to predict cardiac diagnoses from heart auscultation, and explain with shape-based murmur diagrams. In modeling studies, we found that DiagramNet not only provides faithful murmur shape explanations, but also has better prediction performance than baseline models. We further demonstrate the interpretability and trustworthiness of diagrammatic explanations in a qualitative user study with medical students, showing that clinically-relevant, diagrammatic explanations are preferred ov
&lt;/p&gt;</description></item></channel></rss>