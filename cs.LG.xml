<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#28508;&#22312;&#34920;&#31034;&#30340;&#20108;&#20998;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29983;&#24577;&#32593;&#32476;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;HSIC&#24809;&#32602;&#39033;&#65292;&#30830;&#20445;&#20102;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#19982;&#36830;&#32493;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.02011</link><description>&lt;p&gt;
&#20844;&#24179;&#28508;&#22312;&#34920;&#31034;&#30340;&#20108;&#20998;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65292;&#20197;&#35299;&#20915;&#29983;&#24577;&#32593;&#32476;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Bipartite Graph Variational Auto-Encoder with Fair Latent Representation to Account for Sampling Bias in Ecological Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20844;&#24179;&#28508;&#22312;&#34920;&#31034;&#30340;&#20108;&#20998;&#22270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29983;&#24577;&#32593;&#32476;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#24341;&#20837;&#39069;&#22806;&#30340;HSIC&#24809;&#32602;&#39033;&#65292;&#30830;&#20445;&#20102;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#19982;&#36830;&#32493;&#21464;&#37327;&#30340;&#29420;&#31435;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#20351;&#29992;&#22270;&#23884;&#20837;&#26469;&#34920;&#31034;&#20108;&#20998;&#32593;&#32476;&#65292;&#20197;&#35299;&#20915;&#30740;&#31350;&#29983;&#24577;&#32593;&#32476;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#27604;&#22914;&#36830;&#25509;&#26893;&#29289;&#21644;&#20256;&#31881;&#32773;&#31561;&#32593;&#32476;&#65292;&#38656;&#32771;&#34385;&#35768;&#22810;&#21327;&#21464;&#37327;&#65292;&#23588;&#20854;&#35201;&#25511;&#21046;&#25277;&#26679;&#20559;&#24046;&#12290;&#25105;&#20204;&#23558;&#21464;&#20998;&#22270;&#33258;&#21160;&#32534;&#30721;&#22120;&#26041;&#27861;&#35843;&#25972;&#20026;&#20108;&#20998;&#24773;&#20917;&#65292;&#20174;&#32780;&#33021;&#22815;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#29983;&#25104;&#23884;&#20837;&#65292;&#20854;&#20013;&#20004;&#32452;&#33410;&#28857;&#30340;&#20301;&#32622;&#22522;&#20110;&#23427;&#20204;&#30340;&#36830;&#25509;&#27010;&#29575;&#12290;&#25105;&#20204;&#23558;&#22312;&#31038;&#20250;&#23398;&#20013;&#24120;&#32771;&#34385;&#30340;&#20844;&#24179;&#24615;&#26694;&#26550;&#36716;&#21270;&#20026;&#29983;&#24577;&#23398;&#20013;&#30340;&#25277;&#26679;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#28155;&#21152;Hilbert-Schmidt&#29420;&#31435;&#20934;&#21017;&#65288;HSIC&#65289;&#20316;&#20026;&#39069;&#22806;&#24809;&#32602;&#39033;&#65292;&#25105;&#20204;&#30830;&#20445;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#19982;&#36830;&#32493;&#21464;&#37327;&#65288;&#19982;&#25277;&#26679;&#36807;&#31243;&#30456;&#20851;&#65289;&#26080;&#20851;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#25913;&#21464;&#25105;&#20204;&#23545;&#29983;&#24577;&#32593;&#32476;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02011v1 Announce Type: cross  Abstract: We propose a method to represent bipartite networks using graph embeddings tailored to tackle the challenges of studying ecological networks, such as the ones linking plants and pollinators, where many covariates need to be accounted for, in particular to control for sampling bias. We adapt the variational graph auto-encoder approach to the bipartite case, which enables us to generate embeddings in a latent space where the two sets of nodes are positioned based on their probability of connection. We translate the fairness framework commonly considered in sociology in order to address sampling bias in ecology. By incorporating the Hilbert-Schmidt independence criterion (HSIC) as an additional penalty term in the loss we optimize, we ensure that the structure of the latent space is independent of continuous variables, which are related to the sampling process. Finally, we show how our approach can change our understanding of ecological n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2402.03167</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;: &#26080;&#29615;&#31639;&#27861;&#26356;&#26032;&#21644;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
Decentralized Bilevel Optimization over Graphs: Loopless Algorithmic Update and Transient Iteration Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#20248;&#21270;&#31639;&#27861;&#65288;D-SOBA&#65289;&#65292;&#39318;&#27425;&#38416;&#26126;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#22312;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#26041;&#38754;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21452;&#32423;&#20248;&#21270;&#65288;SBO&#65289;&#22312;&#22788;&#29702;&#23884;&#22871;&#32467;&#26500;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#20351;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;SBO&#65292;&#21435;&#20013;&#24515;&#21270;&#26041;&#27861;&#20316;&#20026;&#26377;&#25928;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#20854;&#20013;&#33410;&#28857;&#19982;&#30452;&#25509;&#30456;&#37051;&#33410;&#28857;&#36827;&#34892;&#36890;&#20449;&#65292;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#21644;&#22686;&#24378;&#31639;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#31639;&#27861;&#38754;&#20020;&#25361;&#25112;&#65292;&#21253;&#25324;&#26114;&#36149;&#30340;&#20869;&#37096;&#24490;&#29615;&#26356;&#26032;&#21644;&#23545;&#32593;&#32476;&#25299;&#25169;&#12289;&#25968;&#25454;&#24322;&#26500;&#24615;&#21644;&#23884;&#22871;&#21452;&#32423;&#31639;&#27861;&#32467;&#26500;&#30340;&#24433;&#21709;&#19981;&#26126;&#30830;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21333;&#24490;&#29615;&#30340;&#21435;&#20013;&#24515;&#21270;SBO&#65288;D-SOBA&#65289;&#31639;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#30636;&#24577;&#36845;&#20195;&#22797;&#26434;&#24615;&#65292;&#39318;&#27425;&#28548;&#28165;&#20102;&#32593;&#32476;&#25299;&#25169;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#21435;&#20013;&#24515;&#21270;&#21452;&#32423;&#31639;&#27861;&#30340;&#20849;&#21516;&#24433;&#21709;&#12290;D-SOBA&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#28176;&#36817;&#36895;&#29575;&#12289;&#28176;&#36817;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#21644;&#30636;&#24577;&#26799;&#24230;/&#28023;&#26862;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stochastic bilevel optimization (SBO) is becoming increasingly essential in machine learning due to its versatility in handling nested structures. To address large-scale SBO, decentralized approaches have emerged as effective paradigms in which nodes communicate with immediate neighbors without a central server, thereby improving communication efficiency and enhancing algorithmic robustness. However, current decentralized SBO algorithms face challenges, including expensive inner-loop updates and unclear understanding of the influence of network topology, data heterogeneity, and the nested bilevel algorithmic structures. In this paper, we introduce a single-loop decentralized SBO (D-SOBA) algorithm and establish its transient iteration complexity, which, for the first time, clarifies the joint influence of network topology and data heterogeneity on decentralized bilevel algorithms. D-SOBA achieves the state-of-the-art asymptotic rate, asymptotic gradient/Hessian complexity, and transien
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SaFaRI&#30340;&#22522;&#20110;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#12290;&#22312;&#21508;&#31181;&#22122;&#22768;&#36870;&#38382;&#39064;&#19978;&#65292;SaFaRI&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;FFHQ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17629</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#22270;&#20687;&#24674;&#22797;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Spatial-and-Frequency-aware Restoration method for Images based on Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SaFaRI&#30340;&#22522;&#20110;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#20687;&#24674;&#22797;&#12290;&#22312;&#21508;&#31181;&#22122;&#22768;&#36870;&#38382;&#39064;&#19978;&#65292;SaFaRI&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;FFHQ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26368;&#36817;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#22270;&#20687;&#24674;&#22797;&#65288;IR&#65289;&#26694;&#26550;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#32467;&#26524;&#24182;&#19988;&#19982;&#29616;&#26377;&#26041;&#27861;&#20860;&#23481;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;IR&#20013;&#22122;&#22768;&#36870;&#38382;&#39064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20165;&#32771;&#34385;&#20687;&#32032;&#32423;&#30340;&#25968;&#25454;&#20445;&#30495;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SaFaRI&#30340;&#22522;&#20110;&#31354;&#38388;&#21644;&#39057;&#29575;&#24863;&#30693;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24102;&#26377;&#39640;&#26031;&#22122;&#22768;&#30340;IR&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#40723;&#21169;&#22270;&#20687;&#22312;&#31354;&#38388;&#21644;&#39057;&#29575;&#22495;&#20013;&#20445;&#25345;&#25968;&#25454;&#20445;&#30495;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21508;&#31181;&#22122;&#22768;&#36870;&#38382;&#39064;&#19978;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#20462;&#22797;&#12289;&#38477;&#22122;&#21644;&#36229;&#20998;&#36776;&#29575;&#12290;&#25105;&#20204;&#30340;&#32454;&#33268;&#35780;&#20272;&#34920;&#26126;&#65292;SaFaRI&#22312;ImageNet&#25968;&#25454;&#38598;&#21644;FFHQ&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20197;LPIPS&#21644;FID&#25351;&#26631;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#38646;&#26679;&#26412;IR&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have recently emerged as a promising framework for Image Restoration (IR), owing to their ability to produce high-quality reconstructions and their compatibility with established methods. Existing methods for solving noisy inverse problems in IR, considers the pixel-wise data-fidelity. In this paper, we propose SaFaRI, a spatial-and-frequency-aware diffusion model for IR with Gaussian noise. Our model encourages images to preserve data-fidelity in both the spatial and frequency domains, resulting in enhanced reconstruction quality. We comprehensively evaluate the performance of our model on a variety of noisy inverse problems, including inpainting, denoising, and super-resolution. Our thorough evaluation demonstrates that SaFaRI achieves state-of-the-art performance on both the ImageNet datasets and FFHQ datasets, outperforming existing zero-shot IR methods in terms of LPIPS and FID metrics.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17435</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#21462;&#20195;&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#23454;&#39564;&#23460;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models Replace Economic Choice Prediction Labs?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17435
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#21462;&#20195;&#32463;&#27982;&#23454;&#39564;&#23460;&#36827;&#34892;&#36873;&#25321;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#30456;&#20851;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#27982;&#36873;&#25321;&#39044;&#27979;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#37325;&#35201;&#20219;&#21153;&#65292;&#24448;&#24448;&#21463;&#38480;&#20110;&#33719;&#21462;&#20154;&#31867;&#36873;&#25321;&#25968;&#25454;&#30340;&#22256;&#38590;&#12290;&#23454;&#39564;&#32463;&#27982;&#23398;&#30740;&#31350;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#19987;&#27880;&#20110;&#31616;&#21333;&#30340;&#36873;&#25321;&#29615;&#22659;&#12290;&#26368;&#36817;&#65292;&#20154;&#24037;&#26234;&#33021;&#30028;&#20197;&#20004;&#31181;&#26041;&#24335;&#20026;&#35813;&#21162;&#21147;&#20570;&#20986;&#20102;&#36129;&#29486;&#65306;&#32771;&#34385;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#21487;&#20197;&#20195;&#26367;&#20154;&#31867;&#22312;&#19978;&#36848;&#31616;&#21333;&#36873;&#25321;&#39044;&#27979;&#29615;&#22659;&#20013;&#65292;&#20197;&#21450;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#35270;&#35282;&#30740;&#31350;&#26356;&#22797;&#26434;&#20294;&#20173;&#20005;&#26684;&#30340;&#23454;&#39564;&#32463;&#27982;&#23398;&#29615;&#22659;&#65292;&#21253;&#25324;&#19981;&#23436;&#20840;&#20449;&#24687;&#12289;&#37325;&#22797;&#21338;&#24328;&#21644;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#30340;&#35828;&#26381;&#28216;&#25103;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#28789;&#24863;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#33021;&#22815;&#23436;&#20840;&#27169;&#25311;&#32463;&#27982;&#29615;&#22659;&#65292;&#24182;&#29983;&#25104;&#29992;&#20110;&#39640;&#25928;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#26367;&#20195;&#22797;&#26434;&#30340;&#32463;&#27982;&#23454;&#39564;&#23460;&#30740;&#31350;&#65311;&#25105;&#20204;&#22312;&#36825;&#20010;&#20027;&#39064;&#19978;&#24320;&#21019;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#21487;&#34892;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#34920;&#26126;&#20165;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Economic choice prediction is an essential challenging task, often constrained by the difficulties in acquiring human choice data. Indeed, experimental economics studies had focused mostly on simple choice settings. The AI community has recently contributed to that effort in two ways: considering whether LLMs can substitute for humans in the above-mentioned simple choice prediction settings, and the study through ML lens of more elaborated but still rigorous experimental economics settings, employing incomplete information, repetitive play, and natural language communication, notably language-based persuasion games. This leaves us with a major inspiration: can LLMs be used to fully simulate the economic environment and generate data for efficient human choice prediction, substituting for the elaborated economic lab studies? We pioneer the study of this subject, demonstrating its feasibility. In particular, we show that a model trained solely on LLM-generated data can effectively predic
&lt;/p&gt;</description></item><item><title>LSAP&#36890;&#36807;&#23545;&#28508;&#31354;&#38388;&#23454;&#29616;&#23545;&#40784;&#35299;&#20915;&#20102;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#20013;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21069;&#25552;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.12746</link><description>&lt;p&gt;
LSAP: &#37325;&#26032;&#24605;&#32771;GAN&#28508;&#31354;&#38388;&#20013;&#21453;&#28436;&#30340;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;
&lt;/p&gt;
&lt;p&gt;
LSAP: Rethinking Inversion Fidelity, Perception and Editability in GAN Latent Space. (arXiv:2209.12746v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12746
&lt;/p&gt;
&lt;p&gt;
LSAP&#36890;&#36807;&#23545;&#28508;&#31354;&#38388;&#23454;&#29616;&#23545;&#40784;&#35299;&#20915;&#20102;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#20013;&#20445;&#30495;&#24230;&#12289;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#38382;&#39064;&#65292;&#20351;&#24471;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21069;&#25552;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#21453;&#28436;&#20027;&#35201;&#20998;&#20026;&#20004;&#20010;&#27493;&#39588;&#12290;&#31532;&#19968;&#27493;&#26159;&#22270;&#20687;&#23884;&#20837;&#65292;&#22312;&#36825;&#20010;&#27493;&#39588;&#20013;&#65292;&#32534;&#30721;&#22120;&#25110;&#32773;&#20248;&#21270;&#36807;&#31243;&#23884;&#20837;&#22270;&#20687;&#20197;&#33719;&#21462;&#30456;&#24212;&#30340;&#28508;&#22312;&#30721;&#12290;&#20043;&#21518;&#65292;&#31532;&#20108;&#27493;&#26088;&#22312;&#25913;&#21892;&#21453;&#28436;&#21644;&#32534;&#36753;&#32467;&#26524;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32467;&#26524;&#32454;&#21270;&#12290;&#23613;&#31649;&#31532;&#20108;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#20445;&#30495;&#24230;&#65292;&#20294;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#20960;&#20046;&#27809;&#26377;&#25913;&#21464;&#65292;&#28145;&#24230;&#20381;&#36182;&#20110;&#22312;&#31532;&#19968;&#27493;&#20013;&#33719;&#24471;&#30340;&#21453;&#21521;&#28508;&#22312;&#30721;&#12290;&#22240;&#27492;&#65292;&#37325;&#35201;&#30340;&#38382;&#39064;&#26159;&#22312;&#20445;&#30041;&#37325;&#24314;&#20445;&#30495;&#24230;&#30340;&#21516;&#26102;&#33719;&#24471;&#20855;&#26377;&#26356;&#22909;&#24863;&#30693;&#21644;&#21487;&#32534;&#36753;&#24615;&#30340;&#28508;&#22312;&#30721;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#36825;&#20004;&#20010;&#29305;&#24449;&#19982;&#21453;&#21521;&#30721;&#19982;&#21512;&#25104;&#20998;&#24067;&#30340;&#23545;&#40784;&#65288;&#25110;&#19981;&#23545;&#40784;&#65289;&#31243;&#24230;&#26377;&#20851;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#31354;&#38388;&#23545;&#40784;&#21453;&#28436;&#33539;&#20363;&#65288;LSAP&#65289;&#65292;&#20854;&#20013;&#21253;&#25324;&#35780;&#20272;&#25351;&#26631;&#21644;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26631;&#20934;&#21270;&#39118;&#26684;&#31354;&#38388;&#65288;$\mathcal{S^N}$&#65289;&#21644;&#26631;&#20934;&#21270;&#20869;&#23481;&#31354;&#38388;&#65288;$\mathcal{C^N}$&#65289;&#65292;&#20998;&#21035;&#22312;&#39118;&#26684;&#21644;&#20869;&#23481;&#19978;&#23545;&#40784;&#27491;&#21521;&#21644;&#36127;&#21521;&#28508;&#22312;&#30721;&#21644;&#21512;&#25104;&#20998;&#24067;&#12290; LSAP&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#22270;&#20687;&#32534;&#36753;&#12289;&#22270;&#20687;&#36716;&#25442;&#21644;&#22270;&#20687;&#21512;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;LSAP&#20855;&#26377;&#27604;&#20197;&#21069;&#26041;&#27861;&#26356;&#22909;&#30340;&#29305;&#24615;&#65292;&#22914;&#25913;&#36827;&#30340;&#21487;&#32534;&#36753;&#24615;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#26356;&#23569;&#30340;&#27169;&#24335;&#23849;&#22604;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the methods evolve, inversion is mainly divided into two steps. The first step is Image Embedding, in which an encoder or optimization process embeds images to get the corresponding latent codes. Afterward, the second step aims to refine the inversion and editing results, which we named Result Refinement. Although the second step significantly improves fidelity, perception and editability are almost unchanged, deeply dependent on inverse latent codes attained in the first step. Therefore, a crucial problem is gaining the latent codes with better perception and editability while retaining the reconstruction fidelity. In this work, we first point out that these two characteristics are related to the degree of alignment (or disalignment) of the inverse codes with the synthetic distribution. Then, we propose Latent Space Alignment Inversion Paradigm (LSAP), which consists of evaluation metric and solution for this problem. Specifically, we introduce Normalized Style Space ($\mathcal{S^N
&lt;/p&gt;</description></item></channel></rss>