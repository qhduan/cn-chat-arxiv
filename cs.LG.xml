<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02058</link><description>&lt;p&gt;
&#20855;&#26377;&#24555;&#36895;prop&#30340;&#21487;&#25512;&#24191;&#12289;&#24555;&#36895;&#21644;&#20934;&#30830;&#30340;DeepQSPR Part 1: &#26694;&#26550;&#21644;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Generalizable, Fast, and Accurate DeepQSPR with fastprop Part 1: Framework and Benchmarks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02058
&lt;/p&gt;
&lt;p&gt;
fastprop&#26159;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#26102;&#38388;&#20869;&#65292;&#22312;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#24182;&#36229;&#36234;&#20102;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#32467;&#26500;-&#24615;&#36136;&#20851;&#31995;&#30740;&#31350;&#26088;&#22312;&#23450;&#20041;&#20998;&#23376;&#32467;&#26500;&#19982;&#20219;&#24847;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#21382;&#21490;&#19978;&#65292;&#36825;&#26159;&#36890;&#36807;&#24320;&#21457;&#25551;&#36848;&#31526;&#26469;&#23454;&#29616;&#30340;&#65292;&#36825;&#38656;&#35201;&#26174;&#33879;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#19988;&#38590;&#20197;&#27867;&#21270;&#12290;&#22240;&#27492;&#65292;&#35813;&#39046;&#22495;&#24050;&#32463;&#28436;&#21464;&#20026;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#65292;&#24182;&#36716;&#20026;&#20351;&#29992;&#39640;&#24230;&#21487;&#25512;&#24191;&#30340;&#23398;&#20064;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;fastprop&#65292;&#19968;&#31181;DeepQSPR&#26694;&#26550;&#65292;&#20351;&#29992;&#19968;&#32452;&#26126;&#26234;&#30340;&#20998;&#23376;&#32423;&#25551;&#36848;&#31526;&#65292;&#22312;&#26497;&#22823;&#32553;&#30701;&#30340;&#26102;&#38388;&#20869;&#28385;&#36275;&#24182;&#36229;&#36234;&#20102;&#22810;&#26679;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#34920;&#31034;&#30340;&#24615;&#33021;&#12290;fastprop&#21487;&#20197;&#22312;github&#19978;&#20813;&#36153;&#33719;&#21462;&#65292;&#32593;&#22336;&#20026;github.com/JacksonBurns/fastprop&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02058v1 Announce Type: new  Abstract: Quantitative Structure Property Relationship studies aim to define a mapping between molecular structure and arbitrary quantities of interest. This was historically accomplished via the development of descriptors which requires significant domain expertise and struggles to generalize. Thus the field has morphed into Molecular Property Prediction and been given over to learned representations which are highly generalizable. The paper introduces fastprop, a DeepQSPR framework which uses a cogent set of molecular level descriptors to meet and exceed the performance of learned representations on diverse datasets in dramatically less time. fastprop is freely available on github at github.com/JacksonBurns/fastprop.
&lt;/p&gt;</description></item><item><title>FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.12026</link><description>&lt;p&gt;
FlexCap&#65306;&#22312;&#22270;&#20687;&#20013;&#29983;&#25104;&#20016;&#23500;&#12289;&#26412;&#22320;&#21270;&#21644;&#28789;&#27963;&#30340;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
FlexCap: Generating Rich, Localized, and Flexible Captions in Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12026
&lt;/p&gt;
&lt;p&gt;
FlexCap&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22270;&#20687;&#20013;&#20855;&#26377;&#19981;&#21516;&#38271;&#24230;&#30340;&#21306;&#22495;&#25551;&#36848;&#65292;&#22312;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#21644;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;$\textit{&#28789;&#27963;&#23383;&#24149;}$&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#65292;&#33021;&#22815;&#29983;&#25104;&#38271;&#24230;&#19981;&#21516;&#30340;&#29305;&#23450;&#21306;&#22495;&#25551;&#36848;&#12290;&#35813;&#27169;&#22411;FlexCap&#32463;&#36807;&#35757;&#32451;&#65292;&#21487;&#20026;&#36755;&#20837;&#30340;&#36793;&#30028;&#26694;&#29983;&#25104;&#38271;&#24230;&#26465;&#20214;&#30340;&#23383;&#24149;&#65292;&#20174;&#32780;&#21487;&#20197;&#25511;&#21046;&#20854;&#36755;&#20986;&#30340;&#20449;&#24687;&#23494;&#24230;&#65292;&#25551;&#36848;&#33539;&#22260;&#20174;&#31616;&#27905;&#30340;&#23545;&#35937;&#26631;&#31614;&#21040;&#35814;&#32454;&#30340;&#23383;&#24149;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#24102;&#23383;&#24149;&#30340;&#22270;&#20687;&#24320;&#22987;&#21019;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#22270;&#20687;&#21306;&#22495;&#25551;&#36848;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#36825;&#31181;&#28789;&#27963;&#30340;&#23383;&#24149;&#21151;&#33021;&#26377;&#20960;&#20010;&#23453;&#36149;&#30340;&#24212;&#29992;&#12290;&#39318;&#20808;&#65292;FlexCap&#22312;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#23494;&#38598;&#23383;&#24149;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#20854;&#27425;&#65292;&#21487;&#20197;&#36890;&#36807;&#37319;&#29992;FlexCap&#29983;&#25104;&#26412;&#22320;&#21270;&#25551;&#36848;&#20316;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#26469;&#26500;&#24314;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#31995;&#32479;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31995;&#32479;&#22312;&#35768;&#22810;VQ&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12026v1 Announce Type: cross  Abstract: We introduce a versatile $\textit{flexible-captioning}$ vision-language model (VLM) capable of generating region-specific descriptions of varying lengths. The model, FlexCap, is trained to produce length-conditioned captions for input bounding boxes, and this allows control over the information density of its output, with descriptions ranging from concise object labels to detailed captions. To achieve this we create large-scale training datasets of image region descriptions of varying length, starting from captioned images. This flexible-captioning capability has several valuable applications.   First, FlexCap demonstrates superior performance in dense captioning tasks on the Visual Genome dataset. Second, a visual question answering (VQA) system can be built by employing FlexCap to generate localized descriptions as inputs to a large language model. The resulting system achieves state-of-the-art zero-shot performance on a number of VQ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#27010;&#29575;&#35770;&#20026;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#22522;&#30784;&#65292;&#21487;&#20197;&#25193;&#23637;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#21040;&#26356;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.09580</link><description>&lt;p&gt;
&#31639;&#27861;&#21477;&#27861;&#22240;&#26524;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Algorithmic syntactic causal identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09580
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26367;&#25442;&#20256;&#32479;&#27010;&#29575;&#35770;&#20026;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#22522;&#30784;&#65292;&#21487;&#20197;&#25193;&#23637;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#21040;&#26356;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#20013;&#36827;&#34892;&#22240;&#26524;&#35782;&#21035;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#19968;&#39033;&#37325;&#35201;&#24037;&#20855;&#65292;&#20801;&#35768;&#20174;&#29702;&#35770;&#19978;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#30340;&#35266;&#27979;&#20998;&#24067;&#25512;&#23548;&#24178;&#39044;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22240;&#26524;&#35782;&#21035;&#24418;&#24335;&#65292;&#22914;&#20351;&#29992;d&#20998;&#31163;&#21644;do-&#28436;&#31639;&#30340;&#25216;&#26415;&#37117;&#26159;&#22312;CBN&#19978;&#21033;&#29992;&#32463;&#20856;&#27010;&#29575;&#35770;&#30340;&#25968;&#23398;&#35821;&#35328;&#34920;&#36798;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22240;&#26524;&#35774;&#32622;&#20013;&#65292;&#27010;&#29575;&#35770;&#21644;&#22240;&#27492;&#30446;&#21069;&#30340;&#22240;&#26524;&#35782;&#21035;&#25216;&#26415;&#19981;&#36866;&#29992;&#65292;&#22914;&#20851;&#31995;&#25968;&#25454;&#24211;&#12289;&#25968;&#25454;&#27969;&#31243;&#24207;&#65288;&#20363;&#22914;&#30828;&#20214;&#25551;&#36848;&#35821;&#35328;&#65289;&#12289;&#20998;&#24067;&#24335;&#31995;&#32479;&#21644;&#22823;&#22810;&#25968;&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#23545;&#31216;&#21333;&#35843;&#33539;&#30068;&#30340;&#26367;&#20195;&#20844;&#29702;&#22522;&#30784;&#26469;&#28040;&#38500;&#36825;&#31181;&#38480;&#21046;&#12290;&#22312;&#36825;&#31181;&#26367;&#20195;&#20844;&#29702;&#21270;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#33719;&#24471;&#19968;&#20010;&#26126;&#30830;&#19988;&#28165;&#26224;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09580v1 Announce Type: new  Abstract: Causal identification in causal Bayes nets (CBNs) is an important tool in causal inference allowing the derivation of interventional distributions from observational distributions where this is possible in principle. However, most existing formulations of causal identification using techniques such as d-separation and do-calculus are expressed within the mathematical language of classical probability theory on CBNs. However, there are many causal settings where probability theory and hence current causal identification techniques are inapplicable such as relational databases, dataflow programs such as hardware description languages, distributed systems and most modern machine learning algorithms. We show that this restriction can be lifted by replacing the use of classical probability theory with the alternative axiomatic foundation of symmetric monoidal categories. In this alternative axiomatization, we show how an unambiguous and clean
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;</title><link>https://arxiv.org/abs/2403.05652</link><description>&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#26631;&#39064;
&lt;/p&gt;
&lt;p&gt;
What is different between these datasets?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05652
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#32763;&#35793;&#36807;&#30340;&#35770;&#25991;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05652v1 Announce Type: cross  Abstract: The performance of machine learning models heavily depends on the quality of input data, yet real-world applications often encounter various data-related challenges. One such challenge could arise when curating training data or deploying the model in the real world - two comparable datasets in the same domain may have different distributions. While numerous techniques exist for detecting distribution shifts, the literature lacks comprehensive approaches for explaining dataset differences in a human-understandable manner. To address this gap, we propose a suite of interpretable methods (toolbox) for comparing two datasets. We demonstrate the versatility of our approach across diverse data modalities, including tabular data, language, images, and signals in both low and high-dimensional settings. Our methods not only outperform comparable and related approaches in terms of explanation quality and correctness, but also provide actionable,
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2402.14184</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#35821;&#35328;&#27169;&#22411;&#22810;&#26679;&#24615;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Diversity-Aware Ensembling of Language Models Based on Topological Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14184
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#65292;&#25552;&#39640;&#20102;&#38598;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26159;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#22312;&#19982;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30456;&#20851;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#24320;&#28304;&#20013;&#23384;&#22312;&#22810;&#20010;&#22823;&#22411;&#27169;&#22411;&#65292;&#38598;&#25104;&#26377;&#21161;&#20110;&#25552;&#21319;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#23545;&#38598;&#25104;&#20013;&#27599;&#20010;&#27169;&#22411;&#30340;&#39044;&#27979;&#36827;&#34892;&#31616;&#21333;&#24179;&#22343;&#65292;&#23545;&#27599;&#20010;&#27169;&#22411;&#36171;&#20104;&#30456;&#21516;&#26435;&#37325;&#65292;&#24573;&#30053;&#20102;&#27169;&#22411;&#36136;&#37327;&#21644;&#19968;&#33268;&#24615;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19981;&#20165;&#21333;&#20010;&#27169;&#22411;&#34920;&#29616;&#30693;&#35782;&#65292;&#36824;&#20351;&#29992;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#20272;&#35745;NLP&#27169;&#22411;&#38598;&#25104;&#30340;&#26435;&#37325;&#12290;&#36890;&#36807;&#37319;&#29992;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#65288;TDA&#65289;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#25105;&#20204;&#30340;&#38598;&#25104;&#12290;&#25991;&#26412;&#20998;&#31867;&#20934;&#30830;&#24615;&#21644;&#30456;&#20851;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#36136;&#37327;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14184v1 Announce Type: cross  Abstract: Ensembles are important tools for improving the performance of machine learning models. In cases related to natural language processing, ensembles boost the performance of a method due to multiple large models available in open source. However, existing approaches mostly rely on simple averaging of predictions by ensembles with equal weights for each model, ignoring differences in the quality and conformity of models. We propose to estimate weights for ensembles of NLP models using not only knowledge of their individual performance but also their similarity to each other. By adopting distance measures based on Topological Data Analysis (TDA), we improve our ensemble. The quality improves for both text classification accuracy and relevant uncertainty estimation.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;</title><link>https://arxiv.org/abs/2402.09615</link><description>&lt;p&gt;
API Pack&#65306;&#19968;&#20010;&#29992;&#20110;API&#35843;&#29992;&#29983;&#25104;&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
API Pack: A Massive Multilingual Dataset for API Call Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09615
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;API Pack&#30340;&#22823;&#35268;&#27169;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#26041;&#38754;&#30340;&#39640;&#20934;&#30830;&#29575;&#65292;&#24182;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;API Pack&#65292;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#25351;&#20196;-API&#35843;&#29992;&#23545;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;API&#35843;&#29992;&#29983;&#25104;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;API Pack&#22312;&#25552;&#21319;&#27169;&#22411;&#22312;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#25928;&#26524;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;&#20854;&#22312;&#19968;&#33324;&#32534;&#30721;&#26041;&#38754;&#30340;&#25972;&#20307;&#29087;&#32451;&#31243;&#24230;&#12290;&#20165;&#22312;20,000&#20010;Python&#23454;&#20363;&#19978;&#23545;CodeLlama-13B&#36827;&#34892;&#24494;&#35843;&#65292;&#20854;&#29983;&#25104;&#26410;&#35265;&#36807;&#30340;API&#35843;&#29992;&#30340;&#20934;&#30830;&#29575;&#27604;GPT-3.5&#21644;GPT-4&#20998;&#21035;&#39640;&#20986;10%&#21644;5%&#12290;&#25193;&#23637;&#21040;100k&#20010;&#20363;&#23376;&#21487;&#20197;&#25552;&#39640;&#23545;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;API&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#23454;&#29616;&#20102;&#36328;&#35821;&#35328;&#30340;API&#35843;&#29992;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22823;&#37327;&#35821;&#35328;&#29305;&#23450;&#30340;&#25968;&#25454;&#12290;&#25968;&#25454;&#38598;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#27169;&#22411;&#21644;&#25972;&#20307;&#20195;&#30721;&#24211;&#21487;&#22312;https://github.com/anonymous_url&#19978;&#20844;&#24320;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09615v1 Announce Type: cross  Abstract: We introduce API Pack, a multilingual dataset featuring over one million instruction-API call pairs aimed at advancing large language models' API call generation capabilities. Through experiments, we demonstrate API Pack's efficacy in enhancing models for this specialized task while maintaining their overall proficiency at general coding. Fine-tuning CodeLlama-13B on just 20,000 Python instances yields over 10% and 5% higher accuracy than GPT-3.5 and GPT-4 respectively in generating unseen API calls. Scaling to 100k examples improves generalization to new APIs not seen during training. In addition, cross-lingual API call generation is achieved without needing extensive data per language. The dataset, fine-tuned models, and overall code base are publicly available at https://github.com/anonymous_url.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#26222;&#36866;&#24615;&#23450;&#29702;&#26469;&#20811;&#26381;&#20197;&#21069;&#24037;&#20316;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24357;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.06578</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#26222;&#36866;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Universality of Coupling-based Normalizing Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06578
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#26222;&#36866;&#24615;&#23450;&#29702;&#26469;&#20811;&#26381;&#20197;&#21069;&#24037;&#20316;&#30340;&#38480;&#21046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#24357;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22522;&#20110;&#32806;&#21512;&#30340;&#26631;&#20934;&#21270;&#27969;&#65288;&#22914;RealNVP&#65289;&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#23613;&#31649;&#32806;&#21512;&#27969;&#22312;&#31185;&#23398;&#24212;&#29992;&#20013;&#24456;&#26222;&#36941;&#65292;&#20294;&#30001;&#20110;&#20854;&#21463;&#38480;&#30340;&#26550;&#26500;&#65292;&#23545;&#20110;&#32806;&#21512;&#27969;&#30340;&#20840;&#38754;&#29702;&#35299;&#20173;&#28982;&#22256;&#38590;&#12290;&#29616;&#26377;&#30340;&#23450;&#29702;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#20351;&#29992;&#20219;&#24847;&#30149;&#24577;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36825;&#20123;&#32467;&#26500;&#26412;&#36136;&#19978;&#23548;&#33268;&#20307;&#31215;&#20445;&#25345;&#27969;&#65292;&#36825;&#26159;&#19968;&#20010;&#38480;&#21046;&#34920;&#36798;&#33021;&#21147;&#30340;&#22522;&#26412;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#32806;&#21512;&#26631;&#20934;&#21270;&#27969;&#26222;&#36866;&#24615;&#23450;&#29702;&#65292;&#20811;&#26381;&#20102;&#20197;&#21069;&#24037;&#20316;&#30340;&#20960;&#20010;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25903;&#25345;&#32806;&#21512;&#26550;&#26500;&#20855;&#26377;&#34920;&#36798;&#33021;&#21147;&#30340;&#26222;&#36941;&#32463;&#39564;&#65292;&#24182;&#20026;&#36873;&#25321;&#32806;&#21512;&#20989;&#25968;&#30340;&#34920;&#36798;&#33021;&#21147;&#25552;&#20379;&#20102;&#32454;&#33268;&#20837;&#24494;&#30340;&#35266;&#28857;&#65292;&#22635;&#34917;&#20102;&#23454;&#35777;&#32467;&#26524;&#21644;&#29702;&#35770;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel theoretical framework for understanding the expressive power of coupling-based normalizing flows such as RealNVP. Despite their prevalence in scientific applications, a comprehensive understanding of coupling flows remains elusive due to their restricted architectures. Existing theorems fall short as they require the use of arbitrarily ill-conditioned neural networks, limiting practical applicability. Additionally, we demonstrate that these constructions inherently lead to volume-preserving flows, a property which we show to be a fundamental constraint for expressivity. We propose a new distributional universality theorem for coupling-based normalizing flows, which overcomes several limitations of prior work. Our results support the general wisdom that the coupling architecture is expressive and provide a nuanced view for choosing the expressivity of coupling functions, bridging a gap between empirical results and theoretical understanding.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27010;&#36848;&#12290;&#23427;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#36825;&#20123;&#31995;&#32479;&#30340;&#36890;&#29992;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2402.01968</link><description>&lt;p&gt;
&#23545;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;agent&#31995;&#32479;&#30340;&#35843;&#26597;&#65306;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Context-Aware Multi-Agent Systems: Techniques, Challenges and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01968
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#25216;&#26415;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#32508;&#21512;&#30340;&#27010;&#36848;&#12290;&#23427;&#20171;&#32461;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;&#20195;&#29702;&#31995;&#32479;&#30340;&#29305;&#24615;&#65292;&#20197;&#21450;&#38598;&#25104;&#36825;&#20123;&#31995;&#32479;&#30340;&#36890;&#29992;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26032;&#20852;&#20027;&#39064;&#30340;&#20852;&#36215;&#65292;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#20852;&#36259;&#27491;&#22312;&#22686;&#21152;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26174;&#33879;&#25104;&#23601;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#33258;&#20027;&#20195;&#29702;&#20013;&#36798;&#21040;&#20154;&#31867;&#26234;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#20351;&#36825;&#20123;&#20195;&#29702;&#33021;&#22815;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#12289;&#25512;&#29702;&#21644;&#23548;&#33322;&#19981;&#30830;&#23450;&#24615;&#12290;&#24403;&#22788;&#29702;&#21160;&#24577;&#24773;&#20917;&#26102;&#65292;&#19978;&#19979;&#25991;&#24847;&#35782;&#25104;&#20026;&#24378;&#21270;&#22810;agent&#31995;&#32479;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810;agent&#31995;&#32479;&#65292;&#20294;&#32570;&#20047;&#20840;&#38754;&#27010;&#36848;&#22914;&#20309;&#23558;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#19982;&#22810;agent&#31995;&#32479;&#38598;&#25104;&#30340;&#32508;&#21512;&#35843;&#26597;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#26368;&#20808;&#36827;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#22810;agent&#31995;&#32479;&#30340;&#20840;&#38754;&#27010;&#36848;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#20419;&#36827;&#36825;&#20123;&#31995;&#32479;&#20043;&#38388;&#38598;&#25104;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#31995;&#32479;&#21644;&#22810; agent &#31995;&#32479;&#30340;&#29305;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#36807;&#31243;&#26469;&#24314;&#27169;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#22810;agent&#31995;&#32479;&#30340;&#38598;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research interest in autonomous agents is on the rise as an emerging topic. The notable achievements of Large Language Models (LLMs) have demonstrated the considerable potential to attain human-like intelligence in autonomous agents. However, the challenge lies in enabling these agents to learn, reason, and navigate uncertainties in dynamic environments. Context awareness emerges as a pivotal element in fortifying multi-agent systems when dealing with dynamic situations. Despite existing research focusing on both context-aware systems and multi-agent systems, there is a lack of comprehensive surveys outlining techniques for integrating context-aware systems with multi-agent systems. To address this gap, this survey provides a comprehensive overview of state-of-the-art context-aware multi-agent systems. First, we outline the properties of both context-aware systems and multi-agent systems that facilitate integration between these systems. Subsequently, we propose a general process for c
&lt;/p&gt;</description></item><item><title>SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01855</link><description>&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#31471;&#21040;&#31471;&#31070;&#32463;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01855
&lt;/p&gt;
&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#36890;&#24120;&#36890;&#36807;&#26368;&#20248;&#25554;&#20540;(Optimal Interpolation&#65292;OI)&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27169;&#22411;&#25110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25968;&#25454;&#21516;&#21270;&#25216;&#26415;&#26469;&#22788;&#29702;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;(Spatio-temporal Partial Differential Equations&#65292;SPDE)&#21644;&#39640;&#26031;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;(Gaussian Markov Random Fields&#65292;GMRF)&#20043;&#38388;&#30340;&#32852;&#31995;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#65292;&#29992;&#20110;&#22788;&#29702;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#22823;&#25968;&#25454;&#38598;&#21644;&#29289;&#29702;&#35825;&#23548;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#26368;&#26032;&#36827;&#23637;&#20063;&#20351;&#24471;&#21487;&#20197;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#23884;&#20837;&#25968;&#25454;&#21516;&#21270;&#21464;&#20998;&#26694;&#26550;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#12290;&#37325;&#24314;&#20219;&#21153;&#34987;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#22312;&#21464;&#20998;&#20869;&#37096;&#25104;&#26412;&#20013;&#30340;&#20808;&#39564;&#23398;&#20064;&#38382;&#39064;&#21644;&#21518;&#32773;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#23567;&#21270;&#65306;&#20808;&#39564;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#37117;&#34987;&#34920;&#31034;&#20026;&#20855;&#26377;&#33258;&#21160;&#24494;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#20123;&#30495;&#23454;&#20540;&#21644;&#37325;&#24314;&#20540;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spatio-temporal interpolation of large geophysical datasets has historically been adressed by Optimal Interpolation (OI) and more sophisticated model-based or data-driven DA techniques. In the last ten years, the link established between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) opened a new way of handling both large datasets and physically-induced covariance matrix in Optimal Interpolation. Recent advances in the deep learning community also enables to adress this problem as neural architecture embedding data assimilation variational framework. The reconstruction task is seen as a joint learning problem of the prior involved in the variational inner cost and the gradient-based minimization of the latter: both prior models and solvers are stated as neural networks with automatic differentiation which can be trained by minimizing a loss function, typically stated as the mean squared error between some ground truth and the reconstructi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#25104;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#20915;&#31574;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#21518;&#39564;&#25277;&#26679;&#25110;BayesUCB&#36827;&#34892;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.10699</link><description>&lt;p&gt;
&#25104;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#20915;&#31574;&#65306;&#19968;&#31181;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Cost-Efficient Online Decision Making: A Combinatorial Multi-Armed Bandit Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10699
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#25104;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#20915;&#31574;&#26694;&#26550;&#65292;&#24182;&#21033;&#29992;&#21518;&#39564;&#25277;&#26679;&#25110;BayesUCB&#36827;&#34892;&#25506;&#32034;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#23454;&#38469;&#38382;&#39064;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20915;&#31574;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#20915;&#31574;&#26159;&#22522;&#20110;&#23545;&#20256;&#20837;&#25968;&#25454;&#28857;&#36827;&#34892;&#19968;&#31995;&#21015;&#27979;&#35797;&#26469;&#36827;&#34892;&#30340;&#12290;&#28982;&#32780;&#65292;&#25191;&#34892;&#25152;&#26377;&#27979;&#35797;&#21487;&#33021;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#24182;&#38750;&#24635;&#26159;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#30340;&#26032;&#24418;&#24335;&#65292;&#24182;&#32771;&#34385;&#20102;&#25191;&#34892;&#27979;&#35797;&#30340;&#65288;&#21487;&#33021;&#26159;&#38543;&#26426;&#30340;&#65289;&#25104;&#26412;&#12290;&#22522;&#20110;&#36825;&#31181;&#24418;&#24335;&#21270;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#25104;&#26412;&#26377;&#25928;&#30340;&#22312;&#32447;&#20915;&#31574;&#26694;&#26550;&#65292;&#21487;&#20197;&#21033;&#29992;&#21518;&#39564;&#25277;&#26679;&#25110;BayesUCB&#36827;&#34892;&#25506;&#32034;&#12290;&#25105;&#20204;&#23545;&#29992;&#20110;&#25104;&#26412;&#26377;&#25928;&#22312;&#32447;&#20915;&#31574;&#30340;Thompson&#25277;&#26679;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#25552;&#20379;&#20102;&#21508;&#31181;&#23454;&#39564;&#32467;&#26524;&#65292;&#35777;&#26126;&#25105;&#20204;&#30340;&#26694;&#26550;&#36866;&#29992;&#20110;&#23454;&#38469;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online decision making plays a crucial role in numerous real-world applications. In many scenarios, the decision is made based on performing a sequence of tests on the incoming data points. However, performing all tests can be expensive and is not always possible. In this paper, we provide a novel formulation of the online decision making problem based on combinatorial multi-armed bandits and take the (possibly stochastic) cost of performing tests into account. Based on this formulation, we provide a new framework for cost-efficient online decision making which can utilize posterior sampling or BayesUCB for exploration. We provide a theoretical analysis of Thompson Sampling for cost-efficient online decision making, and present various experimental results that demonstrate the applicability of our framework to real-world problems.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/1909.03820</link><description>&lt;p&gt;
&#29992;&#35745;&#25968;&#31526;&#21495;&#30340;&#19968;&#38454;&#36923;&#36753;&#23450;&#20041;&#30340;&#27010;&#24565;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Concepts Definable in First-Order Logic with Counting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1909.03820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Grohe&#21644;Tur\'an&#24341;&#20837;&#30340;&#36923;&#36753;&#26694;&#26550;&#19979;&#30340;&#20851;&#31995;&#32972;&#26223;&#32467;&#26500;&#19978;&#30340;&#24067;&#23572;&#20998;&#31867;&#38382;&#39064;&#12290;&#20247;&#25152;&#21608;&#30693;(Grohe&#21644;Ritzert, LICS 2017)&#65292;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19978;&#30340;&#19968;&#38454;&#36923;&#36753;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#23398;&#20064;&#65292;&#20854;&#20013;&#32467;&#26500;&#30340;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#26159;&#20197;&#32467;&#26500;&#30340;&#22823;&#23567;&#20026;&#21333;&#20301;&#26469;&#34913;&#37327;&#30340;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#30001;Kuske&#21644;Schweikardt(LICS 2017)&#24341;&#20837;&#30340;&#24102;&#35745;&#25968;&#30340;&#19968;&#38454;&#36923;&#36753;FOCN&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#24191;&#27867;&#25512;&#24191;&#21508;&#31181;&#35745;&#25968;&#36923;&#36753;&#30340;&#34920;&#29616;&#36923;&#36753;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#31867;&#19978;&#23450;&#20041;&#30340;FOCN&#20013;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#19968;&#33268;&#22320;&#23398;&#20064;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#23398;&#20064;&#26694;&#26550;&#25193;&#23637;&#20197;&#21253;&#21547;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#20540;&#26041;&#38754;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26080;&#35270;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:1909.03820v2 Announce Type: replace-cross  Abstract: We study Boolean classification problems over relational background structures in the logical framework introduced by Grohe and Tur\'an (TOCS 2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in first-order logic over structures of polylogarithmic degree can be learned in sublinear time, where the degree of the structure and the running time are measured in terms of the size of the structure. We generalise the results to the first-order logic with counting FOCN, which was introduced by Kuske and Schweikardt (LICS 2017) as an expressive logic generalising various other counting logics. Specifically, we prove that classifiers definable in FOCN over classes of structures of polylogarithmic degree can be consistently learned in sublinear time. This can be seen as a first step towards extending the learning framework to include numerical aspects of machine learning. We extend the result to agnostic probabl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.15800</link><description>&lt;p&gt;
&#20351;&#29992;SHAP&#21644;LIME&#36827;&#34892;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#29305;&#24449;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Provably Stable Feature Rankings with SHAP and LIME. (arXiv:2401.15800v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26222;&#36941;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20998;&#36755;&#20837;&#21464;&#37327;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;SHAP&#21644;LIME&#65292;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#32780;&#20855;&#26377;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#20511;&#37492;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#27491;&#30830;&#25490;&#21517;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;RankSHAP&#20445;&#35777;$K$&#20010;&#26368;&#39640;Shapley&#20540;&#20855;&#26377;&#36229;&#36807;$1-\alpha$&#30340;&#27491;&#30830;&#25490;&#24207;&#27010;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#20026;LIME&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#30830;&#20445;&#20197;&#27491;&#30830;&#39034;&#24207;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attributions are ubiquitous tools for understanding the predictions of machine learning models. However, popular methods for scoring input variables such as SHAP and LIME suffer from high instability due to random sampling. Leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley values have the proper ordering with probability exceeding $1-\alpha$. Empirical results demonstrate its validity and impressive computational efficiency. We also build on previous work to yield similar results for LIME, ensuring the most important features are selected in the right order.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20809;&#35889;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20013;&#30340;&#22270;&#23646;&#24615;&#21644;&#32467;&#26500;&#21464;&#21270;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20445;&#25345;&#20302;&#39057;&#29305;&#24449;&#20540;&#19981;&#21464;&#21487;&#20197;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#21452;&#26865;&#38236;&#65288;DP&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28789;&#27963;&#22320;&#20445;&#30041;&#20851;&#38190;&#30340;&#22270;&#23646;&#24615;&#21516;&#26102;&#22686;&#21152;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09953</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#26865;&#38236;: &#20809;&#35889;&#35270;&#35282;&#19979;&#30340;&#22270;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#22270;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Through the Dual-Prism: A Spectral Perspective on Graph Data Augmentation for Graph Classification. (arXiv:2401.09953v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09953
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20809;&#35889;&#35270;&#35282;&#65292;&#30740;&#31350;&#20102;&#22270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#20013;&#30340;&#22270;&#23646;&#24615;&#21644;&#32467;&#26500;&#21464;&#21270;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20445;&#25345;&#20302;&#39057;&#29305;&#24449;&#20540;&#19981;&#21464;&#21487;&#20197;&#20445;&#30041;&#20851;&#38190;&#23646;&#24615;&#65292;&#25552;&#20986;&#20102;&#21452;&#26865;&#38236;&#65288;DP&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28789;&#27963;&#22320;&#20445;&#30041;&#20851;&#38190;&#30340;&#22270;&#23646;&#24615;&#21516;&#26102;&#22686;&#21152;&#22270;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#22788;&#29702;&#22270;&#25968;&#25454;&#30340;&#39318;&#36873;&#24037;&#20855;&#65292;&#20854;&#36890;&#36807;&#22270;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#25552;&#39640;&#21152;&#24378;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#23613;&#31649;&#22686;&#24378;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#20294;&#22270;&#23646;&#24615;&#25197;&#26354;&#21644;&#21463;&#38480;&#32467;&#26500;&#21464;&#21270;&#31561;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#26159;&#21542;&#21487;&#33021;&#24320;&#21457;&#26356;&#21152;&#20445;&#30041;&#23646;&#24615;&#24182;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#22686;&#24378;&#26041;&#27861;&#65311;&#36890;&#36807;&#20809;&#35889;&#38236;&#22836;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22270;&#23646;&#24615;&#12289;&#23427;&#20204;&#30340;&#22686;&#24378;&#21644;&#23427;&#20204;&#30340;&#20809;&#35889;&#34892;&#20026;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#20445;&#25345;&#20302;&#39057;&#29305;&#24449;&#20540;&#19981;&#21464;&#21487;&#20197;&#20445;&#25345;&#29983;&#25104;&#30340;&#22686;&#24378;&#22270;&#30340;&#20851;&#38190;&#23646;&#24615;&#12290;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#21551;&#21457;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#26865;&#38236;&#65288;DP&#65289;&#22686;&#24378;&#26041;&#27861;&#65292;&#21253;&#25324;DP-Noise&#21644;DP-Mask&#65292;&#23427;&#20204;&#28789;&#27963;&#22320;&#20445;&#30041;&#20102;&#20851;&#38190;&#30340;&#22270;&#23646;&#24615;&#24182;&#20016;&#23500;&#20102;&#22686;&#24378;&#22270;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#19968;&#31181;&#26032;&#30340;&#12289;&#26377;&#21069;&#26223;&#30340;&#30452;&#25509;&#26041;&#27861;&#25552;&#20379;&#20102;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have become the preferred tool to process graph data, with their efficacy being boosted through graph data augmentation techniques. Despite the evolution of augmentation methods, issues like graph property distortions and restricted structural changes persist. This leads to the question: Is it possible to develop more property-conserving and structure-sensitive augmentation methods? Through a spectral lens, we investigate the interplay between graph properties, their augmentation, and their spectral behavior, and found that keeping the low-frequency eigenvalues unchanged can preserve the critical properties at a large scale when generating augmented graphs. These observations inform our introduction of the Dual-Prism (DP) augmentation method, comprising DP-Noise and DP-Mask, which adeptly retains essential graph properties while diversifying augmented graphs. Extensive experiments validate the efficiency of our approach, providing a new and promising direct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04482</link><description>&lt;p&gt;
&#22312;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#25345;&#32493;&#23398;&#20064;&#26032;&#35789;
&lt;/p&gt;
&lt;p&gt;
Continuously Learning New Words in Automatic Speech Recognition. (arXiv:2401.04482v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04482
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#20013;&#35782;&#21035;&#26032;&#35789;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#35762;&#24231;&#24405;&#38899;&#36827;&#34892;&#25512;&#29702;&#21644;&#25910;&#38598;&#21253;&#21547;&#26032;&#35789;&#30340;&#35805;&#35821;&#65292;&#28982;&#21518;&#22312;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#25552;&#39640;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#31995;&#32479;&#20173;&#28982;&#36828;&#26410;&#23436;&#32654;&#12290;&#20856;&#22411;&#30340;&#38169;&#35823;&#21253;&#25324;&#32553;&#20889;&#35789;&#12289;&#21629;&#21517;&#23454;&#20307;&#21644;&#39046;&#22495;&#29305;&#23450;&#30340;&#19987;&#29992;&#35789;&#65292;&#36825;&#20123;&#35789;&#20960;&#20046;&#27809;&#26377;&#25110;&#27809;&#26377;&#25968;&#25454;&#21487;&#29992;&#26469;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#35782;&#21035;&#36825;&#20123;&#35789;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#25105;&#30417;&#30563;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#24102;&#26377;&#23545;&#24212;&#24187;&#28783;&#29255;&#30340;&#35762;&#24231;&#24405;&#38899;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#35760;&#24518;&#22686;&#24378;&#22411;ASR&#27169;&#22411;&#26469;&#23558;&#27169;&#22411;&#20559;&#21521;&#20110;&#20174;&#24187;&#28783;&#29255;&#20013;&#35299;&#30721;&#26032;&#35789;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#35762;&#24231;&#36827;&#34892;&#25512;&#29702;&#65292;&#23558;&#21253;&#21547;&#26816;&#27979;&#21040;&#30340;&#26032;&#35789;&#30340;&#35805;&#35821;&#25910;&#38598;&#21040;&#33258;&#36866;&#24212;&#25968;&#25454;&#38598;&#20013;&#12290;&#25509;&#30528;&#65292;&#23545;&#36825;&#20010;&#38598;&#21512;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#65292;&#36890;&#36807;&#35843;&#25972;&#28155;&#21152;&#21040;&#27169;&#22411;&#30340;&#27599;&#20010;&#26435;&#37325;&#30697;&#38453;&#30340;&#20302;&#31209;&#30697;&#38453;&#26435;&#37325;&#12290;&#25972;&#20010;&#36807;&#31243;&#23545;&#22810;&#20010;&#35762;&#24231;&#36827;&#34892;&#36845;&#20195;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#22312;&#26032;&#35789;&#20986;&#29616;&#39057;&#29575;&#36739;&#39640;&#26102;&#33719;&#24471;&#20102;&#24615;&#33021;&#30340;&#25552;&#21319;&#65288;&#36229;&#36807;80%&#30340;&#21484;&#22238;&#29575;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.16877</link><description>&lt;p&gt;
&#20351;&#29992;&#35757;&#32451;&#26631;&#31614;&#36827;&#34892;&#22635;&#20805;&#21644;&#36890;&#36807;&#26631;&#31614;&#22635;&#20805;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Imputation using training labels and classification via label imputation. (arXiv:2311.16877v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.16877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22312;&#22635;&#20805;&#32570;&#22833;&#25968;&#25454;&#26102;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22635;&#20805;&#25928;&#26524;&#65292;&#24182;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#65292;&#19988;&#22312;&#23454;&#39564;&#35777;&#26126;&#20855;&#26377;&#26377;&#24076;&#26395;&#30340;&#20934;&#30830;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#32570;&#22833;&#25968;&#25454;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#22635;&#20805;&#26041;&#27861;&#26469;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#37117;&#26377;&#26631;&#31614;&#65292;&#20294;&#24120;&#35265;&#30340;&#22635;&#20805;&#26041;&#27861;&#36890;&#24120;&#21482;&#20381;&#36182;&#20110;&#36755;&#20837;&#32780;&#24573;&#30053;&#26631;&#31614;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38416;&#36848;&#20102;&#23558;&#26631;&#31614;&#22534;&#21472;&#21040;&#36755;&#20837;&#20013;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#36755;&#20837;&#30340;&#22635;&#20805;&#25928;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#23558;&#39044;&#27979;&#30340;&#27979;&#35797;&#26631;&#31614;&#21021;&#22987;&#21270;&#20026;&#32570;&#22833;&#20540;&#65292;&#24182;&#23558;&#26631;&#31614;&#19982;&#36755;&#20837;&#22534;&#21472;&#22312;&#19968;&#36215;&#36827;&#34892;&#22635;&#20805;&#12290;&#36825;&#26679;&#21487;&#20197;&#21516;&#26102;&#22635;&#20805;&#26631;&#31614;&#21644;&#36755;&#20837;&#12290;&#32780;&#19988;&#65292;&#35813;&#25216;&#26415;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#32570;&#22833;&#26631;&#31614;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#26080;&#38656;&#20219;&#20309;&#20808;&#21069;&#30340;&#22635;&#20805;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#36830;&#32493;&#22411;&#12289;&#20998;&#31867;&#22411;&#25110;&#28151;&#21512;&#22411;&#25968;&#25454;&#12290;&#23454;&#39564;&#35777;&#26126;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Missing data is a common problem in practical settings. Various imputation methods have been developed to deal with missing data. However, even though the label is usually available in the training data, the common practice of imputation usually only relies on the input and ignores the label. In this work, we illustrate how stacking the label into the input can significantly improve the imputation of the input. In addition, we propose a classification strategy that initializes the predicted test label with missing values and stacks the label with the input for imputation. This allows imputing the label and the input at the same time. Also, the technique is capable of handling data training with missing labels without any prior imputation and is applicable to continuous, categorical, or mixed-type data. Experiments show promising results in terms of accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#36830;&#32493;Galerkin&#26041;&#27861;&#20013;&#21152;&#20837;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#35745;&#31639;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2310.18897</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#22686;&#24378;&#20302;&#38454;&#19981;&#36830;&#32493;Galerkin&#26041;&#27861;&#22312;&#21487;&#21387;Navier-Stokes&#26041;&#31243;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Enhancing Low-Order Discontinuous Galerkin Methods with Neural Ordinary Differential Equations for Compressible Navier--Stokes Equations. (arXiv:2310.18897v2 [physics.flu-dyn] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19981;&#36830;&#32493;Galerkin&#26041;&#27861;&#20013;&#21152;&#20837;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#25928;&#26524;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#25311;&#30340;&#20934;&#30830;&#24615;&#21644;&#21152;&#36895;&#35745;&#31639;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#38271;&#65292;&#27169;&#25311;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#21644;&#20934;&#30830;&#12290;&#28982;&#32780;&#65292;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#36890;&#24120;&#20250;&#36816;&#34892;&#19968;&#20010;&#20302;&#20445;&#30495;&#24230;&#27169;&#22411;&#24182;&#37319;&#29992;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#65292;&#20294;&#36873;&#25321;&#36866;&#24403;&#30340;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#24182;&#23545;&#20854;&#36827;&#34892;&#35843;&#33410;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#22312;&#19981;&#36830;&#32493;Galerkin&#65288;DG&#65289;&#31354;&#38388;&#31163;&#25955;&#21270;&#30340;&#32972;&#26223;&#19979;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#27169;&#25311;&#20013;&#24341;&#20837;&#31070;&#32463;&#24120;&#24494;&#20998;&#31639;&#23376;&#26469;&#23398;&#20064;&#23376;&#32593;&#26684;&#23610;&#24230;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36830;&#32493;&#32423;&#21035;&#19978;&#23398;&#20064;&#20302;&#38454;DG&#27714;&#35299;&#22120;&#20013;&#32570;&#22833;&#30340;&#23610;&#24230;&#65292;&#20174;&#32780;&#25552;&#39640;&#20302;&#38454;DG&#36817;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20197;&#19968;&#23450;&#31243;&#24230;&#30340;&#31934;&#24230;&#21152;&#36895;&#28388;&#27874;&#39640;&#38454;DG&#27169;&#25311;&#12290;&#25105;&#20204;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing computing power over the years has enabled simulations to become more complex and accurate. While immensely valuable for scientific discovery and problem-solving, however, high-fidelity simulations come with significant computational demands. As a result, it is common to run a low-fidelity model with a subgrid-scale model to reduce the computational cost, but selecting the appropriate subgrid-scale models and tuning them are challenging. We propose a novel method for learning the subgrid-scale model effects when simulating partial differential equations augmented by neural ordinary differential operators in the context of discontinuous Galerkin (DG) spatial discretization. Our approach learns the missing scales of the low-order DG solver at a continuous level and hence improves the accuracy of the low-order DG approximations as well as accelerates the filtered high-order DG simulations with a certain degree of precision. We demonstrate the performance of our approach throug
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38170;&#23450;&#31354;&#38388;&#20248;&#21270;&#20256;&#36755;&#65288;ASOT&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20998;&#24067;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#38170;&#28857;&#31354;&#38388;&#65292;&#23398;&#20064;&#20854;&#28508;&#22312;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#22810;&#20010;&#20256;&#36755;&#38382;&#39064;&#30340;&#25209;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.16123</link><description>&lt;p&gt;
&#38170;&#23450;&#31354;&#38388;&#20248;&#21270;&#20256;&#36755;&#65306;&#21152;&#36895;&#22810;&#20010;&#20256;&#36755;&#38382;&#39064;&#30340;&#25209;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Anchor Space Optimal Transport: Accelerating Batch Processing of Multiple OT Problems. (arXiv:2310.16123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16123
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38170;&#23450;&#31354;&#38388;&#20248;&#21270;&#20256;&#36755;&#65288;ASOT&#65289;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#20998;&#24067;&#26144;&#23556;&#21040;&#20849;&#20139;&#30340;&#38170;&#28857;&#31354;&#38388;&#65292;&#23398;&#20064;&#20854;&#28508;&#22312;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#22810;&#20010;&#20256;&#36755;&#38382;&#39064;&#30340;&#25209;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#29702;&#35770;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#27604;&#36739;&#23450;&#20041;&#22312;&#25351;&#23450;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20026;&#31435;&#26041;&#32423;&#12290;&#23613;&#31649;Sinkhorn&#31639;&#27861;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;OT&#35299;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#22810;&#20010;OT&#38382;&#39064;&#30340;&#35299;&#20173;&#28982;&#32791;&#26102;&#21644;&#21344;&#29992;&#20869;&#23384;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;OT&#35745;&#31639;&#21152;&#36895;&#30340;&#35768;&#22810;&#24037;&#20316;&#36890;&#24120;&#22522;&#20110;&#21333;&#20010;OT&#38382;&#39064;&#30340;&#21069;&#25552;&#65292;&#24573;&#35270;&#20102;&#19968;&#20010;&#23567;&#25209;&#37327;&#20013;&#20998;&#24067;&#30340;&#28508;&#22312;&#20849;&#21516;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32763;&#35793;&#30340;OT&#38382;&#39064;&#65292;&#31216;&#20026;&#38170;&#23450;&#31354;&#38388;&#20248;&#21270;&#20256;&#36755;&#65288;ASOT&#65289;&#38382;&#39064;&#65292;&#19987;&#38376;&#29992;&#20110;&#25209;&#22788;&#29702;&#22810;&#20010;OT&#38382;&#39064;&#30340;&#35299;&#12290;&#23545;&#20110;&#25552;&#20986;&#30340;ASOT&#38382;&#39064;&#65292;&#20998;&#24067;&#23558;&#34987;&#26144;&#23556;&#21040;&#19968;&#20010;&#20849;&#20139;&#30340;&#38170;&#28857;&#31354;&#38388;&#65292;&#35813;&#31354;&#38388;&#23398;&#20064;&#28508;&#22312;&#30340;&#20849;&#21516;&#29305;&#24449;&#65292;&#20174;&#32780;&#24110;&#21161;&#21152;&#36895;OT&#25209;&#22788;&#29702;&#12290;&#22522;&#20110;&#25152;&#25552;&#20986;&#30340;ASOT&#65292;Wasserstei
&lt;/p&gt;
&lt;p&gt;
The optimal transport (OT) theory provides an effective way to compare probability distributions on a defined metric space, but it suffers from cubic computational complexity. Although the Sinkhorn's algorithm greatly reduces the computational complexity of OT solutions, the solutions of multiple OT problems are still time-consuming and memory-comsuming in practice. However, many works on the computational acceleration of OT are usually based on the premise of a single OT problem, ignoring the potential common characteristics of the distributions in a mini-batch. Therefore, we propose a translated OT problem designated as the anchor space optimal transport (ASOT) problem, which is specially designed for batch processing of multiple OT problem solutions. For the proposed ASOT problem, the distributions will be mapped into a shared anchor point space, which learns the potential common characteristics and thus help accelerate OT batch processing. Based on the proposed ASOT, the Wasserstei
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#22312;&#20302;&#32500;&#25968;&#25454;&#27969;&#24418;&#19978;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;&#65292;&#36890;&#36807;&#20272;&#35745;&#23494;&#24230;&#21644;&#27979;&#37327;&#19982;&#27969;&#24418;&#30340;&#36317;&#31163;&#26469;&#21028;&#26029;&#22806;&#22495;&#25968;&#25454;&#65292;&#26377;&#25928;&#25552;&#39640;&#22806;&#22495;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13792</link><description>&lt;p&gt;
&#21033;&#29992;&#25968;&#25454;&#27969;&#24418;&#19978;&#30340;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution detection using normalizing flows on the data manifold. (arXiv:2308.13792v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13792
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#27491;&#21017;&#21270;&#27969;&#22312;&#20302;&#32500;&#25968;&#25454;&#27969;&#24418;&#19978;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;&#65292;&#36890;&#36807;&#20272;&#35745;&#23494;&#24230;&#21644;&#27979;&#37327;&#19982;&#27969;&#24418;&#30340;&#36317;&#31163;&#26469;&#21028;&#26029;&#22806;&#22495;&#25968;&#25454;&#65292;&#26377;&#25928;&#25552;&#39640;&#22806;&#22495;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#22495;&#26816;&#27979;&#30340;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#20272;&#35745;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#65292;&#20026;&#22806;&#22495;&#25968;&#25454;&#20998;&#37197;&#36739;&#20302;&#30340;&#21487;&#33021;&#24615;&#20540;&#12290;&#27491;&#21017;&#21270;&#27969;&#26159;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#20445;&#25345;&#32500;&#24230;&#30340;&#21487;&#36870;&#21464;&#25442;&#25552;&#20379;&#21487;&#35745;&#31639;&#30340;&#23494;&#24230;&#20272;&#35745;&#12290;&#20256;&#32479;&#30340;&#27491;&#21017;&#21270;&#27969;&#22312;&#22806;&#22495;&#26816;&#27979;&#20013;&#23481;&#26131;&#22833;&#36133;&#65292;&#22240;&#20026;&#22522;&#20110;&#21487;&#33021;&#24615;&#30340;&#27169;&#22411;&#38754;&#20020;&#30528;&#32500;&#24230;&#35781;&#21650;&#30340;&#38382;&#39064;&#12290;&#26681;&#25454;&#27969;&#24418;&#20551;&#35774;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#36890;&#24120;&#20301;&#20110;&#20302;&#32500;&#27969;&#24418;&#19978;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#20351;&#29992;&#27491;&#21017;&#21270;&#27969;&#36827;&#34892;&#22806;&#22495;&#26816;&#27979;&#26102;&#30340;&#27969;&#24418;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20302;&#32500;&#27969;&#24418;&#19978;&#20272;&#35745;&#23494;&#24230;&#65292;&#24182;&#32467;&#21512;&#27979;&#37327;&#19982;&#27969;&#24418;&#30340;&#36317;&#31163;&#20316;&#20026;&#22806;&#22495;&#26816;&#27979;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#21333;&#29420;&#20351;&#29992;&#23427;&#20204;&#23545;&#20110;&#36825;&#20010;&#20219;&#21153;&#26159;&#19981;&#36275;&#22815;&#30340;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#27969;&#24418;&#23398;&#20064;&#23545;&#22806;&#22495;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common approach for out-of-distribution detection involves estimating an underlying data distribution, which assigns a lower likelihood value to out-of-distribution data. Normalizing flows are likelihood-based generative models providing a tractable density estimation via dimension-preserving invertible transformations. Conventional normalizing flows are prone to fail in out-of-distribution detection, because of the well-known curse of dimensionality problem of the likelihood-based models. According to the manifold hypothesis, real-world data often lie on a low-dimensional manifold. This study investigates the effect of manifold learning using normalizing flows on out-of-distribution detection. We proceed by estimating the density on a low-dimensional manifold, coupled with measuring the distance from the manifold, as criteria for out-of-distribution detection. However, individually, each of them is insufficient for this task. The extensive experimental results show that manifold lea
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32534;&#35793;&#26102;&#38388;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.08167</link><description>&lt;p&gt;
&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#30340;&#26799;&#24230;
&lt;/p&gt;
&lt;p&gt;
Computing the gradients with respect to all parameters of a quantum neural network using a single circuit. (arXiv:2307.08167v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08167
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#30005;&#36335;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#25152;&#26377;&#21442;&#25968;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32534;&#35793;&#26102;&#38388;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#21442;&#25968;&#24179;&#31227;&#35268;&#21017;&#35745;&#31639;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#26102;&#65292;&#38656;&#35201;&#23545;&#32593;&#32476;&#30340;&#21333;&#20010;&#21487;&#35843;&#21442;&#25968;&#35745;&#31639;&#20004;&#27425;&#20195;&#20215;&#20989;&#25968;&#12290;&#24403;&#21442;&#25968;&#24635;&#25968;&#36739;&#39640;&#26102;&#65292;&#38656;&#35201;&#35843;&#25972;&#21644;&#36816;&#34892;&#22810;&#27425;&#29992;&#20110;&#35745;&#31639;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20351;&#29992;&#19968;&#20010;&#30005;&#36335;&#35745;&#31639;&#25152;&#26377;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#23427;&#20855;&#26377;&#36739;&#20302;&#30340;&#30005;&#36335;&#28145;&#24230;&#21644;&#36739;&#23569;&#30340;&#32463;&#20856;&#23492;&#23384;&#22120;&#12290;&#25105;&#20204;&#36824;&#22312;&#30495;&#23454;&#37327;&#23376;&#30828;&#20214;&#21644;&#27169;&#25311;&#22120;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#30005;&#36335;&#32534;&#35793;&#26102;&#38388;&#26126;&#26174;&#32553;&#30701;&#30340;&#20248;&#21183;&#65292;&#20174;&#32780;&#21152;&#36895;&#20102;&#24635;&#20307;&#36816;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
When computing the gradients of a quantum neural network using the parameter-shift rule, the cost function needs to be calculated twice for the gradient with respect to a single adjustable parameter of the network. When the total number of parameters is high, the quantum circuit for the computation has to be adjusted and run for many times. Here we propose an approach to compute all the gradients using a single circuit only, with a much reduced circuit depth and less classical registers. We also demonstrate experimentally, on both real quantum hardware and simulator, that our approach has the advantages that the circuit takes a significantly shorter time to compile than the conventional approach, resulting in a speedup on the total runtime.
&lt;/p&gt;</description></item></channel></rss>