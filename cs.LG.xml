<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17852</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#25968;&#25454;&#36716;&#21270;&#20026;&#19982;&#20559;&#35265;&#27491;&#20132;&#30340;&#26041;&#24335;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Fairness through Transforming Data Orthogonal to Bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17852
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#65292;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#20010;&#39046;&#22495;&#30340;&#22797;&#26434;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#21487;&#33021;&#34920;&#29616;&#20986;&#26377;&#20559;&#35265;&#30340;&#20915;&#31574;&#65292;&#23548;&#33268;&#19981;&#21516;&#32676;&#20307;&#20043;&#38388;&#30340;&#24453;&#36935;&#19981;&#24179;&#31561;&#12290;&#23613;&#31649;&#20844;&#24179;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#24050;&#32463;&#24456;&#24191;&#27867;&#65292;&#20294;&#22810;&#20803;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#23545;&#20915;&#31574;&#32467;&#26524;&#30340;&#24494;&#22937;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#31639;&#27861;&#65292;&#21363;&#27491;&#20132;&#20110;&#20559;&#35265;&#65288;OB&#65289;&#65292;&#26088;&#22312;&#28040;&#38500;&#36830;&#32493;&#25935;&#24863;&#21464;&#37327;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#20419;&#36827;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#20013;&#32852;&#21512;&#27491;&#24577;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#35777;&#26126;&#20102;&#36890;&#36807;&#30830;&#20445;&#25968;&#25454;&#19982;&#25935;&#24863;&#21464;&#37327;&#19981;&#30456;&#20851;&#21363;&#21487;&#23454;&#29616;&#21453;&#20107;&#23454;&#20844;&#24179;&#24615;&#12290;OB&#31639;&#27861;&#19982;&#27169;&#22411;&#26080;&#20851;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17852v1 Announce Type: new  Abstract: Machine learning models have shown exceptional prowess in solving complex issues across various domains. Nonetheless, these models can sometimes exhibit biased decision-making, leading to disparities in treatment across different groups. Despite the extensive research on fairness, the nuanced effects of multivariate and continuous sensitive variables on decision-making outcomes remain insufficiently studied. We introduce a novel data pre-processing algorithm, Orthogonal to Bias (OB), designed to remove the influence of a group of continuous sensitive variables, thereby facilitating counterfactual fairness in machine learning applications. Our approach is grounded in the assumption of a jointly normal distribution within a structural causal model (SCM), proving that counterfactual fairness can be achieved by ensuring the data is uncorrelated with sensitive variables. The OB algorithm is model-agnostic, catering to a wide array of machine 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14488</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#22240;&#26524;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23433;&#20840;&#31283;&#20581;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action Selection in Robot Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#29289;&#20307;&#25805;&#20316;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#20851;&#38190;&#25512;&#25163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25361;&#25112;&#22312;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#24517;&#39035;&#23545;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35753;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#23545;&#20505;&#36873;&#21160;&#20316;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#19968;&#20010;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#21018;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#19982;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23450;&#20041;&#20102;&#26426;&#22120;&#20154;&#20915;&#31574;&#36807;&#31243;&#30340;&#22240;&#26524;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#12290;&#36890;&#36807;&#22522;&#20110;&#20223;&#30495;&#30340;&#33945;&#29305;&#21345;&#27931;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#33021;&#22815;&#65306;(1) &#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#31215;&#26408;&#22612;&#30340;&#31283;&#23450;&#24615;&#65288;&#39044;&#27979;&#20934;&#30830;&#29575;&#65306;88.6%&#65289;&#65307;&#21644;&#65292;(2) &#20026;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#36873;&#25321;&#19968;&#20010;&#36817;&#20284;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#65292;&#20379;&#25972;&#21512;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#65292;&#23454;&#29616;94.2%&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14488v1 Announce Type: cross  Abstract: Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task succe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11624</link><description>&lt;p&gt;
&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Channel Multiplex Graph Neural Networks for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#25429;&#25417;&#21453;&#26144;&#20010;&#20154;&#20559;&#22909;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23646;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#25512;&#33616;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#19978;&#30340;&#28857;&#20987;&#12289;&#26631;&#35760;&#25910;&#34255;&#21644;&#36141;&#20080;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#65306;(1) &#19981;&#36275;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#22810;&#36890;&#36335;&#20851;&#31995;&#24418;&#25104;&#30340;&#21508;&#31181;&#34892;&#20026;&#27169;&#24335;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;(2) &#24573;&#30053;&#20102;&#34892;&#20026;&#27169;&#24335;&#20013;&#19981;&#21516;&#20851;&#31995;&#23545;&#25512;&#33616;&#31995;&#32479;&#22330;&#26223;&#20013;&#30446;&#26631;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
&lt;/p&gt;</description></item><item><title>FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.08059</link><description>&lt;p&gt;
FluoroSAM: &#29992;&#20110;X&#20809;&#22270;&#20687;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08059
&lt;/p&gt;
&lt;p&gt;
FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;X&#20809;&#22270;&#20687;&#20998;&#21106;&#23558;&#21152;&#36895;&#35786;&#26029;&#21644;&#20171;&#20837;&#31934;&#20934;&#21307;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#39046;&#22495;&#65292;&#35201;&#25299;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#21017;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#37325;&#26032;&#35757;&#32451;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289; - &#35757;&#32451;&#22312;&#22823;&#37327;&#39640;&#24230;&#21464;&#21270;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22240;&#27492;&#20351;&#24471;&#24191;&#27867;&#36866;&#29992;&#24615;&#25104;&#20026;&#21487;&#33021; - &#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;FMs&#32858;&#28966;&#20110;&#23545;&#35937;&#34987;&#26126;&#26174;&#21487;&#35265;&#36793;&#30028;&#28165;&#26224;&#23450;&#20041;&#30340;&#22330;&#26223;&#21644;&#27169;&#24335;&#65292;&#22914;&#20869;&#31397;&#38236;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;X&#20809;&#25104;&#20687;&#36890;&#24120;&#27809;&#26377;&#25552;&#20379;&#36825;&#31181;&#28165;&#26224;&#30340;&#36793;&#30028;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;&#22312;X&#20809;&#22270;&#20687;&#24418;&#25104;&#26399;&#38388;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#23616;&#38480;&#65292;&#25552;&#20986;&#22270;&#21367;&#31215;&#28145;&#24230;&#20869;&#26680;&#26426;&#65288;graph convolutional deep kernel machine&#65289;&#26469;&#30740;&#31350;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06525</link><description>&lt;p&gt;
&#28789;&#27963;&#30340;&#26080;&#38480;&#23485;&#22270;&#21367;&#31215;&#32593;&#32476;&#21450;&#34920;&#31034;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;
&lt;/p&gt;
&lt;p&gt;
Flexible infinite-width graph convolutional networks and the importance of representation learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#22312;&#29702;&#35770;&#19978;&#30340;&#23616;&#38480;&#65292;&#25552;&#20986;&#22270;&#21367;&#31215;&#28145;&#24230;&#20869;&#26680;&#26426;&#65288;graph convolutional deep kernel machine&#65289;&#26469;&#30740;&#31350;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#24120;&#35265;&#29702;&#35770;&#26041;&#27861;&#26159;&#36827;&#34892;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#65292;&#27492;&#26102;&#36755;&#20986;&#25104;&#20026;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20998;&#24067;&#12290;&#36825;&#34987;&#31216;&#20026;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#12290;&#28982;&#32780;&#65292;NNGP&#20869;&#26680;&#26159;&#22266;&#23450;&#30340;&#65292;&#21482;&#33021;&#36890;&#36807;&#23569;&#37327;&#36229;&#21442;&#25968;&#36827;&#34892;&#35843;&#33410;&#65292;&#28040;&#38500;&#20102;&#20219;&#20309;&#34920;&#31034;&#23398;&#20064;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#19982;&#26377;&#38480;&#23485;&#24230;&#30340;&#31070;&#32463;&#32593;&#32476;&#24418;&#25104;&#23545;&#27604;&#65292;&#21518;&#32773;&#36890;&#24120;&#34987;&#35748;&#20026;&#33021;&#22815;&#34920;&#29616;&#33391;&#22909;&#65292;&#27491;&#26159;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#31616;&#21270;&#31070;&#32463;&#32593;&#32476;&#20197;&#20351;&#20854;&#22312;&#29702;&#35770;&#19978;&#21487;&#22788;&#29702;&#30340;&#21516;&#26102;&#65292;NNGP&#21487;&#33021;&#20250;&#28040;&#38500;&#20351;&#20854;&#24037;&#20316;&#33391;&#22909;&#30340;&#22240;&#32032;&#65288;&#34920;&#31034;&#23398;&#20064;&#65289;&#12290;&#36825;&#28608;&#21457;&#20102;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#22270;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#31034;&#23398;&#20064;&#26159;&#21542;&#24517;&#35201;&#30340;&#29702;&#35299;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#31934;&#30830;&#30340;&#24037;&#20855;&#26469;&#23436;&#25104;&#36825;&#20010;&#20219;&#21153;&#65292;&#21363;&#22270;&#21367;&#31215;&#28145;&#24230;&#20869;&#26680;&#26426;&#65288;graph convolutional deep kernel machine&#65289;&#12290;&#36825;&#19982;NNGP&#38750;&#24120;&#30456;&#20284;&#65292;&#22240;&#20026;&#23427;&#26159;&#26080;&#38480;&#23485;&#24230;&#38480;&#21046;&#24182;&#20351;&#29992;&#20869;&#26680;&#65292;&#20294;&#23427;&#24102;&#26377;&#19968;&#20010;&#8220;&#26059;&#38062;&#8221;&#26469;&#25511;&#21046;&#34920;&#31034;&#23398;&#20064;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
A common theoretical approach to understanding neural networks is to take an infinite-width limit, at which point the outputs become Gaussian process (GP) distributed. This is known as a neural network Gaussian process (NNGP). However, the NNGP kernel is fixed, and tunable only through a small number of hyperparameters, eliminating any possibility of representation learning. This contrasts with finite-width NNs, which are often believed to perform well precisely because they are able to learn representations. Thus in simplifying NNs to make them theoretically tractable, NNGPs may eliminate precisely what makes them work well (representation learning). This motivated us to understand whether representation learning is necessary in a range of graph classification tasks. We develop a precise tool for this task, the graph convolutional deep kernel machine. This is very similar to an NNGP, in that it is an infinite width limit and uses kernels, but comes with a `knob' to control the amount 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.01259</link><description>&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#23616;&#37096;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22522;&#30784;&#23398;&#20064;&#36890;&#36807;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#20351;&#29992;&#20154;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#29420;&#31435;&#20110;&#20854;&#20182;&#27010;&#24565;&#30340;&#32473;&#23450;&#27010;&#24565;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#28872;&#26263;&#31034;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#22312;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#20013;&#19981;&#33021;&#25104;&#31435;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#27010;&#24565;&#26082;&#22312;&#31354;&#38388;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#23436;&#20840;&#30001;&#22266;&#23450;&#23376;&#38598;&#30340;&#29305;&#24449;&#23450;&#20041;&#65289;&#21448;&#22312;&#35821;&#20041;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#20165;&#19982;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#23376;&#38598;&#30340;&#27010;&#24565;&#30456;&#20851;&#32852;&#65289;&#23450;&#20301;&#26102;&#65292;CBMs&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#23616;&#37096;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27010;&#24565;&#20043;&#22806;&#30340;&#29305;&#24449;&#21464;&#21270;&#23545;&#27010;&#24565;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;SA-Solver&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;SOTA FID&#20998;&#25968;&#12290;</title><link>http://arxiv.org/abs/2309.05019</link><description>&lt;p&gt;
SA-Solver&#65306;&#29992;&#20110;&#24555;&#36895;&#37319;&#26679;&#25193;&#25955;&#27169;&#22411;&#30340;&#38543;&#26426;&#20122;&#24403;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
SA-Solver: Stochastic Adams Solver for Fast Sampling of Diffusion Models. (arXiv:2309.05019v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.05019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;SA-Solver&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#30456;&#36739;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;SOTA FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#30001;&#20110;&#20174;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#20013;&#36827;&#34892;&#37319;&#26679;&#30456;&#24403;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#32791;&#26102;&#30340;&#24037;&#20316;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#35768;&#22810;&#22522;&#20110;&#25913;&#36827;&#30340;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#30340;&#24555;&#36895;&#37319;&#26679;&#26041;&#27861;&#12290;&#36825;&#20123;&#25216;&#26415;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#27861;&#37117;&#32771;&#34385;&#35299;&#25193;&#25955;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38543;&#26426;&#37319;&#26679;&#21487;&#20197;&#22312;&#29983;&#25104;&#22810;&#26679;&#21270;&#21644;&#39640;&#36136;&#37327;&#25968;&#25454;&#26041;&#38754;&#25552;&#20379;&#39069;&#22806;&#30340;&#20248;&#21183;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#26041;&#38754;&#36827;&#34892;&#20102;&#23545;&#38543;&#26426;&#37319;&#26679;&#30340;&#32508;&#21512;&#20998;&#26512;&#65306;&#26041;&#24046;&#25511;&#21046;&#30340;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#21644;&#32447;&#24615;&#22810;&#27493;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SA-Solver&#65292;&#23427;&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#39640;&#25928;&#38543;&#26426;&#20122;&#24403;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#25193;&#25955;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;SA-Solver&#23454;&#29616;&#20102;&#65306;1&#65289;&#22312;&#23569;&#27493;&#37319;&#26679;&#20013;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#26041;&#27861;&#30456;&#27604;&#65292;&#26377;&#25913;&#36827;&#25110;&#21487;&#27604;&#24615;&#33021;&#65307;2&#65289;SOTA FID&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probabilistic Models (DPMs) have achieved considerable success in generation tasks. As sampling from DPMs is equivalent to solving diffusion SDE or ODE which is time-consuming, numerous fast sampling methods built upon improved differential equation solvers are proposed. The majority of such techniques consider solving the diffusion ODE due to its superior efficiency. However, stochastic sampling could offer additional advantages in generating diverse and high-quality data. In this work, we engage in a comprehensive analysis of stochastic sampling from two aspects: variance-controlled diffusion SDE and linear multi-step SDE solver. Based on our analysis, we propose SA-Solver, which is an improved efficient stochastic Adams method for solving diffusion SDE to generate data with high quality. Our experiments show that SA-Solver achieves: 1) improved or comparable performance compared with the existing state-of-the-art sampling methods for few-step sampling; 2) SOTA FID scores o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#31639;&#27861;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.03334</link><description>&lt;p&gt;
&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variational quantum regression algorithm with encoded data structure. (arXiv:2307.03334v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#32534;&#30721;&#25968;&#25454;&#32467;&#26500;&#30340;&#21464;&#20998;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#22312;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#20855;&#26377;&#27169;&#22411;&#35299;&#37322;&#24615;&#65292;&#24182;&#33021;&#26377;&#25928;&#22320;&#22788;&#29702;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#31639;&#27861;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#22312;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;(VQAs)&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#23454;&#38469;&#38382;&#39064;&#65292;&#22914;&#32452;&#21512;&#20248;&#21270;&#12289;&#37327;&#23376;&#21270;&#23398;&#27169;&#25311;&#12289;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#22122;&#22768;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#37327;&#23376;&#38169;&#35823;&#32416;&#27491;&#12290;&#23545;&#20110;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#23578;&#26410;&#24320;&#21457;&#20986;&#23558;&#27169;&#22411;&#35299;&#37322;&#24615;&#20869;&#23884;&#21040;&#31639;&#27861;&#20013;&#30340;&#21464;&#20998;&#31639;&#27861;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#37327;&#23376;&#22238;&#24402;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#20102;&#21464;&#20998;&#21442;&#25968;&#19982;&#23398;&#20064;&#22238;&#24402;&#31995;&#25968;&#20043;&#38388;&#30340;&#30452;&#25509;&#20851;&#31995;&#65292;&#21516;&#26102;&#37319;&#29992;&#20102;&#23558;&#25968;&#25454;&#30452;&#25509;&#32534;&#30721;&#20026;&#21453;&#26144;&#32463;&#20856;&#25968;&#25454;&#34920;&#32467;&#26500;&#30340;&#37327;&#23376;&#24133;&#24230;&#30340;&#30005;&#36335;&#12290;&#35813;&#31639;&#27861;&#29305;&#21035;&#36866;&#29992;&#20110;&#20114;&#36830;&#24230;&#36739;&#39640;&#30340;&#37327;&#23376;&#27604;&#29305;&#12290;&#36890;&#36807;&#21387;&#32553;&#32534;&#30721;&#21644;&#25968;&#23383;-&#27169;&#25311;&#38376;&#25805;&#20316;&#65292;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#22312;&#25968;&#25454;&#36755;&#20837;&#37327;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#23545;&#25968;&#32423;&#26356;&#26377;&#20248;&#21183;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22122;&#22768;&#20013;&#23610;&#24230;&#37327;&#23376;&#35745;&#31639;&#26426;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational quantum algorithms (VQAs) prevail to solve practical problems such as combinatorial optimization, quantum chemistry simulation, quantum machine learning, and quantum error correction on noisy quantum computers. For variational quantum machine learning, a variational algorithm with model interpretability built into the algorithm is yet to be exploited. In this paper, we construct a quantum regression algorithm and identify the direct relation of variational parameters to learned regression coefficients, while employing a circuit that directly encodes the data in quantum amplitudes reflecting the structure of the classical data table. The algorithm is particularly suitable for well-connected qubits. With compressed encoding and digital-analog gate operation, the run time complexity is logarithmically more advantageous than that for digital 2-local gate native hardware with the number of data entries encoded, a decent improvement in noisy intermediate-scale quantum computers a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#32479;&#19968;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;RVFL&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38544;&#34255;&#23618;&#30456;&#23545;&#20110;&#36755;&#20837;&#32500;&#24230;&#26159;&#25351;&#25968;&#32423;&#23485;&#24230;&#30340;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;&#20102;$L_\infty$&#36924;&#36817;&#35823;&#24046;&#21644;&#39640;&#26031;&#20869;&#37096;&#26435;&#37325;&#26465;&#20214;&#19979;&#30340;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#30340;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#19979;&#30028;&#12290;</title><link>http://arxiv.org/abs/2306.17501</link><description>&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#32479;&#19968;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Efficient uniform approximation using Random Vector Functional Link networks. (arXiv:2306.17501v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;&#32593;&#32476;&#36827;&#34892;&#39640;&#25928;&#32479;&#19968;&#36924;&#36817;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;RVFL&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#38544;&#34255;&#23618;&#30456;&#23545;&#20110;&#36755;&#20837;&#32500;&#24230;&#26159;&#25351;&#25968;&#32423;&#23485;&#24230;&#30340;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#35777;&#26126;&#20102;$L_\infty$&#36924;&#36817;&#35823;&#24046;&#21644;&#39640;&#26031;&#20869;&#37096;&#26435;&#37325;&#26465;&#20214;&#19979;&#30340;&#32467;&#26524;&#65292;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#30340;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#21521;&#37327;&#21151;&#33021;&#36830;&#25509;(RVFL)&#32593;&#32476;&#26159;&#19968;&#20010;&#20855;&#26377;&#38543;&#26426;&#20869;&#37096;&#26435;&#37325;&#21644;&#20559;&#32622;&#30340;&#20108;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#36825;&#31181;&#26550;&#26500;&#21482;&#38656;&#35201;&#23398;&#20064;&#22806;&#37096;&#26435;&#37325;&#65292;&#23398;&#20064;&#36807;&#31243;&#21487;&#20197;&#31616;&#21270;&#20026;&#32447;&#24615;&#20248;&#21270;&#20219;&#21153;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#22256;&#25200;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20855;&#26377;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;RVFL&#32593;&#32476;&#21487;&#20197;&#36924;&#36817;&#21033;&#26222;&#24076;&#33576;&#36830;&#32493;&#20989;&#25968;&#65292;&#21069;&#25552;&#26159;&#20854;&#38544;&#34255;&#23618;&#30456;&#23545;&#20110;&#36755;&#20837;&#32500;&#24230;&#26159;&#25351;&#25968;&#32423;&#23485;&#24230;&#30340;&#12290;&#23613;&#31649;&#20043;&#21069;&#24050;&#32463;&#35777;&#26126;&#20102;&#20197;$L_2$&#26041;&#24335;&#21487;&#20197;&#23454;&#29616;&#36825;&#26679;&#30340;&#36924;&#36817;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;$L_\infty$&#36924;&#36817;&#35823;&#24046;&#21644;&#39640;&#26031;&#20869;&#37096;&#26435;&#37325;&#24773;&#20917;&#19979;&#30340;&#21487;&#34892;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#38750;&#28176;&#36827;&#24615;&#30340;&#38544;&#34255;&#23618;&#33410;&#28857;&#25968;&#37327;&#30340;&#19979;&#30028;&#65292;&#21462;&#20915;&#20110;&#30446;&#26631;&#20989;&#25968;&#30340;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#12289;&#26399;&#26395;&#30340;&#20934;&#30830;&#24230;&#21644;&#36755;&#20837;&#32500;&#24230;&#31561;&#22240;&#32032;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#26041;&#27861;&#26681;&#26893;&#20110;&#27010;&#29575;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Random Vector Functional Link (RVFL) network is a depth-2 neural network with random inner weights and biases. As only the outer weights of such architectures need to be learned, the learning process boils down to a linear optimization task, allowing one to sidestep the pitfalls of nonconvex optimization problems. In this paper, we prove that an RVFL with ReLU activation functions can approximate Lipschitz continuous functions provided its hidden layer is exponentially wide in the input dimension. Although it has been established before that such approximation can be achieved in $L_2$ sense, we prove it for $L_\infty$ approximation error and Gaussian inner weights. To the best of our knowledge, our result is the first of this kind. We give a nonasymptotic lower bound for the number of hidden layer nodes, depending on, among other things, the Lipschitz constant of the target function, the desired accuracy, and the input dimension. Our method of proof is rooted in probability theory an
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2211.06665</link><description>&lt;p&gt;
&#20851;&#20110;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#30340;&#32508;&#36848;&#65306;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
A Survey on Explainable Reinforcement Learning: Concepts, Algorithms, Challenges. (arXiv:2211.06665v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.06665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#35843;&#26597;&#20102;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#35299;&#37322;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#24565;&#12289;&#31639;&#27861;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#26234;&#33021;&#20195;&#29702;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#20197;&#23454;&#29616;&#38271;&#26399;&#30446;&#26631;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#22797;&#20852;&#25512;&#21160;&#19979;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#22797;&#26434;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#65292;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#24178;&#32467;&#26500;&#34987;&#26222;&#36941;&#35270;&#20026;&#40657;&#30418;&#23376;&#65292;&#38459;&#30861;&#20102;&#20174;&#19994;&#32773;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#20449;&#20219;&#21644;&#20351;&#29992;&#35757;&#32451;&#20195;&#29702;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#37327;&#30340;&#25991;&#29486;&#33268;&#21147;&#20110;&#25581;&#31034;&#26234;&#33021;&#20195;&#29702;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#36890;&#36807;&#26500;&#24314;&#20869;&#22312;&#21487;&#35299;&#37322;&#24615;&#25110;&#20107;&#21518;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#21487;&#35299;&#37322;&#24615;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#65292;&#23558;&#20808;&#21069;&#30340;&#24037;&#20316;&#26126;&#30830;&#22320;&#20998;&#20026;&#27169;&#22411;&#35299;&#37322;&#12289;&#22870;&#21169;&#35299;&#37322;&#12289;&#29366;&#24577;&#35299;&#37322;&#21644;&#20219;&#21153;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) is a popular machine learning paradigm where intelligent agents interact with the environment to fulfill a long-term goal. Driven by the resurgence of deep learning, Deep RL (DRL) has witnessed great success over a wide spectrum of complex control tasks. Despite the encouraging results achieved, the deep neural network-based backbone is widely deemed as a black box that impedes practitioners to trust and employ trained agents in realistic scenarios where high security and reliability are essential. To alleviate this issue, a large volume of literature devoted to shedding light on the inner workings of the intelligent agents has been proposed, by constructing intrinsic interpretability or post-hoc explainability. In this survey, we provide a comprehensive review of existing works on eXplainable RL (XRL) and introduce a new taxonomy where prior works are clearly categorized into model-explaining, reward-explaining, state-explaining, and task-explaining methods
&lt;/p&gt;</description></item></channel></rss>