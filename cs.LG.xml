<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;</title><link>https://arxiv.org/abs/2110.08902</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Variance Reduction Based Experience Replay for Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08902
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26102;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#26679;&#26412;&#20013;&#30340;&#20449;&#24687;&#20197;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#26159;&#24456;&#26377;&#24517;&#35201;&#30340;&#12290;&#20256;&#32479;&#30340;&#32463;&#39564;&#22238;&#25918;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#26159;&#23558;&#25152;&#26377;&#35266;&#27979;&#37117;&#35270;&#20026;&#30456;&#21516;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24046;&#20943;&#23569;&#32463;&#39564;&#22238;&#25918;&#65288;VRER&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#23545;&#30456;&#20851;&#26679;&#26412;&#30340;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;VRER&#20316;&#20026;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#20013;&#65292;&#26500;&#24314;&#20102;&#25105;&#20204;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;Policy Optimization with VRER (PG-VRER)&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#20013;&#23545;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#32771;&#34385;&#26679;&#26412;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08902v3 Announce Type: replace-cross  Abstract: For reinforcement learning on complex stochastic systems, it is desirable to effectively leverage the information from historical samples collected in previous iterations to accelerate policy optimization. Classical experience replay, while effective, treats all observations uniformly, neglecting their relative importance. To address this limitation, we introduce a novel Variance Reduction Experience Replay (VRER) framework, enabling the selective reuse of relevant samples to improve policy gradient estimation. VRER, as an adaptable method that can seamlessly integrate with different policy optimization algorithms, forms the foundation of our sample-efficient off-policy algorithm known as Policy Optimization with VRER (PG-VRER). Furthermore, the lack of a rigorous theoretical understanding of the experience replay method in the literature motivates us to introduce a novel theoretical framework that accounts for sample dependenc
&lt;/p&gt;</description></item><item><title>AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.11408</link><description>&lt;p&gt;
AlignAtt&#65306;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#20316;&#20026;&#21516;&#26102;&#35821;&#38899;&#32763;&#35793;&#30340;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
AlignAtt: Using Attention-based Audio-Translation Alignments as a Guide for Simultaneous Speech Translation. (arXiv:2305.11408v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11408
&lt;/p&gt;
&lt;p&gt;
AlignAtt&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;SimulST&#31574;&#30053;&#65292;&#20351;&#29992;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#38899;&#39057;&#32763;&#35793;&#23545;&#40784;&#26469;&#25351;&#23548;&#27169;&#22411;&#65292;&#22312;BLEU&#21644;&#24310;&#36831;&#26041;&#38754;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27880;&#24847;&#21147;&#26159;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#24120;&#29992;&#30340;&#26550;&#26500;&#30340;&#26680;&#24515;&#26426;&#21046;&#65292;&#24182;&#24050;&#20174;&#35768;&#22810;&#35282;&#24230;&#36827;&#34892;&#20998;&#26512;&#65292;&#21253;&#25324;&#20854;&#22312;&#26426;&#22120;&#32763;&#35793;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#36825;&#20123;&#30740;&#31350;&#20013;&#65292;&#27880;&#24847;&#21147;&#22312;&#36755;&#20837;&#25991;&#26412;&#34987;&#26367;&#25442;&#20026;&#38899;&#39057;&#29255;&#27573;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#26159;&#33719;&#21462;&#26377;&#20851;&#21333;&#35789;&#23545;&#40784;&#30340;&#26377;&#29992;&#20449;&#24687;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#20363;&#22914;&#35821;&#38899;&#32763;&#35793;&#65288;ST&#65289;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AlignAtt&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21516;&#26102;ST&#65288;SimulST&#65289;&#31574;&#30053;&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#20449;&#24687;&#26469;&#29983;&#25104;&#28304;-&#30446;&#26631;&#23545;&#40784;&#65292;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#25351;&#23548;&#27169;&#22411;&#12290;&#36890;&#36807;&#23545;MuST-C v1.0&#30340;8&#31181;&#35821;&#35328;&#23545;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32447;&#19979;&#35757;&#32451;&#30340;&#27169;&#22411;&#19978;&#24212;&#29992;&#20808;&#21069;&#30340;&#26368;&#26032;SimulST&#31574;&#30053;&#65292;AlignAtt&#22312;BLEU&#26041;&#38754;&#33719;&#24471;&#20102;2&#20010;&#20998;&#25968;&#30340;&#25552;&#39640;&#65292;&#24182;&#19988;8&#31181;&#35821;&#35328;&#30340;&#24310;&#36831;&#32553;&#20943;&#22312;0.5&#31186;&#21040;0.8&#31186;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention is the core mechanism of today's most used architectures for natural language processing and has been analyzed from many perspectives, including its effectiveness for machine translation-related tasks. Among these studies, attention resulted to be a useful source of information to get insights about word alignment also when the input text is substituted with audio segments, as in the case of the speech translation (ST) task. In this paper, we propose AlignAtt, a novel policy for simultaneous ST (SimulST) that exploits the attention information to generate source-target alignments that guide the model during inference. Through experiments on the 8 language pairs of MuST-C v1.0, we show that AlignAtt outperforms previous state-of-the-art SimulST policies applied to offline-trained models with gains in terms of BLEU of 2 points and latency reductions ranging from 0.5s to 0.8s across the 8 languages.
&lt;/p&gt;</description></item></channel></rss>