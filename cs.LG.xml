<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#24341;&#20837;&#20061;&#31181;&#38598;&#25104;&#23398;&#20064;&#22120;&#24182;&#21033;&#29992;&#26032;&#39062;&#29305;&#24449;&#24037;&#31243;&#31574;&#30053;&#65292;&#32467;&#21512;&#22810;&#31181;&#20998;&#20301;&#25968;&#22238;&#24402;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#31354;&#38388;&#25554;&#20540;&#20013;&#38598;&#25104;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;</title><link>https://arxiv.org/abs/2403.10567</link><description>&lt;p&gt;
&#38598;&#25104;&#23398;&#20064;&#20013;&#30340;&#21355;&#26143;&#38477;&#27700;&#31354;&#38388;&#25554;&#20540;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Uncertainty estimation in spatial interpolation of satellite precipitation with ensemble learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10567
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20061;&#31181;&#38598;&#25104;&#23398;&#20064;&#22120;&#24182;&#21033;&#29992;&#26032;&#39062;&#29305;&#24449;&#24037;&#31243;&#31574;&#30053;&#65292;&#32467;&#21512;&#22810;&#31181;&#20998;&#20301;&#25968;&#22238;&#24402;&#31639;&#27861;&#65292;&#22635;&#34917;&#20102;&#31354;&#38388;&#25554;&#20540;&#20013;&#38598;&#25104;&#23398;&#20064;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#30333;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10567v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#27010;&#29575;&#20998;&#24067;&#24418;&#24335;&#30340;&#39044;&#27979;&#23545;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#20998;&#20301;&#25968;&#22238;&#24402;&#22312;&#31354;&#38388;&#25554;&#20540;&#35774;&#32622;&#20013;&#33021;&#22815;&#21512;&#24182;&#36965;&#24863;&#21644;&#38632;&#37327;&#25968;&#25454;&#65292;&#23454;&#29616;&#27492;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#24773;&#22659;&#19979;&#65292;&#20998;&#20301;&#25968;&#22238;&#24402;&#31639;&#27861;&#30340;&#38598;&#25104;&#23398;&#20064;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#20061;&#31181;&#22522;&#20110;&#20998;&#20301;&#25968;&#30340;&#38598;&#25104;&#23398;&#20064;&#22120;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22823;&#22411;&#38477;&#27700;&#25968;&#25454;&#38598;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#24037;&#31243;&#31574;&#30053;&#65292;&#23558;&#39044;&#27979;&#22240;&#23376;&#20943;&#23569;&#20026;&#30456;&#20851;&#20301;&#32622;&#30340;&#21152;&#26435;&#36317;&#31163;&#21355;&#26143;&#38477;&#27700;&#65292;&#32467;&#21512;&#20301;&#32622;&#39640;&#31243;&#12290;&#25105;&#20204;&#30340;&#38598;&#25104;&#23398;&#20064;&#22120;&#21253;&#25324;&#20845;&#31181;&#22534;&#21472;&#26041;&#27861;&#21644;&#19977;&#31181;&#31616;&#21333;&#26041;&#27861;&#65288;&#22343;&#20540;&#12289;&#20013;&#20301;&#25968;&#12289;&#26368;&#20339;&#32452;&#21512;&#22120;&#65289;&#65292;&#32467;&#21512;&#20102;&#20845;&#31181;&#20010;&#20307;&#31639;&#27861;&#65306;&#20998;&#20301;&#25968;&#22238;&#24402;(QR)&#12289;&#20998;&#20301;&#25968;&#22238;&#24402;&#26862;&#26519;(QRF)&#12289;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;(GRF)&#12289;&#26799;&#24230;&#25552;&#21319;&#26426;(GBM)&#12289;&#36731;&#37327;&#32423;&#26799;&#24230;&#25552;&#21319;&#26426;(LightGBM)&#21644;&#20998;&#20301;&#25968;&#22238;&#24402;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10567v1 Announce Type: new  Abstract: Predictions in the form of probability distributions are crucial for decision-making. Quantile regression enables this within spatial interpolation settings for merging remote sensing and gauge precipitation data. However, ensemble learning of quantile regression algorithms remains unexplored in this context. Here, we address this gap by introducing nine quantile-based ensemble learners and applying them to large precipitation datasets. We employed a novel feature engineering strategy, reducing predictors to distance-weighted satellite precipitation at relevant locations, combined with location elevation. Our ensemble learners include six stacking and three simple methods (mean, median, best combiner), combining six individual algorithms: quantile regression (QR), quantile regression forests (QRF), generalized random forests (GRF), gradient boosting machines (GBM), light gradient boosting machines (LightGBM), and quantile regression neur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38142;&#36335;&#39044;&#27979;&#20013;&#25913;&#36827;GNN&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NodeDup&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#22797;&#21046;&#20302;&#24230;&#33410;&#28857;&#24182;&#21019;&#24314;&#38142;&#25509;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.09711</link><description>&lt;p&gt;
&#33410;&#28857;&#22797;&#21046;&#25913;&#21892;&#20919;&#21551;&#21160;&#38142;&#36335;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Node Duplication Improves Cold-start Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09711
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38142;&#36335;&#39044;&#27979;&#20013;&#25913;&#36827;GNN&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NodeDup&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#36890;&#36807;&#22797;&#21046;&#20302;&#24230;&#33410;&#28857;&#24182;&#21019;&#24314;&#38142;&#25509;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#31361;&#20986;&#65292;&#24182;&#22312;&#38142;&#36335;&#39044;&#27979;&#65288;LP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23613;&#31649;&#25972;&#20307;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;GNN&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#30340;&#34920;&#29616;&#21364;&#36739;&#24046;&#12290;&#22312;&#25512;&#33616;&#31995;&#32479;&#31561;LP&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#25913;&#21892;&#20302;&#24230;&#33410;&#28857;&#30340;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#31561;&#21516;&#20110;&#35299;&#20915;&#20919;&#21551;&#21160;&#38382;&#39064;&#65292;&#25552;&#39640;&#29992;&#25143;&#22312;&#23569;&#25968;&#35266;&#23519;&#30340;&#30456;&#20114;&#20316;&#29992;&#20013;&#30340;&#20307;&#39564;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#25913;&#36827;GNN&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#30340;LP&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#39640;&#24230;&#33410;&#28857;&#19978;&#30340;&#24615;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#38750;&#24120;&#26377;&#25928;&#30340;&#22686;&#24378;&#25216;&#26415;&#65292;&#31216;&#20026;NodeDup&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NodeDup&#22312;&#26631;&#20934;&#30340;&#30417;&#30563;LP&#35757;&#32451;&#26041;&#26696;&#20013;&#65292;&#22312;&#20302;&#24230;&#33410;&#28857;&#19978;&#22797;&#21046;&#33410;&#28857;&#24182;&#22312;&#33410;&#28857;&#21644;&#20854;&#21103;&#26412;&#20043;&#38388;&#21019;&#24314;&#38142;&#25509;&#12290;&#36890;&#36807;&#21033;&#29992;&#8220;&#22810;&#35270;&#22270;&#8221;&#35270;&#35282;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LP&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09711v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) are prominent in graph machine learning and have shown state-of-the-art performance in Link Prediction (LP) tasks. Nonetheless, recent studies show that GNNs struggle to produce good results on low-degree nodes despite their overall strong performance. In practical applications of LP, like recommendation systems, improving performance on low-degree nodes is critical, as it amounts to tackling the cold-start problem of improving the experiences of users with few observed interactions. In this paper, we investigate improving GNNs' LP performance on low-degree nodes while preserving their performance on high-degree nodes and propose a simple yet surprisingly effective augmentation technique called NodeDup. Specifically, NodeDup duplicates low-degree nodes and creates links between nodes and their own duplicates before following the standard supervised LP training scheme. By leveraging a ''multi-view'' perspectiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#39640;&#32500;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#65292;&#21033;&#29992;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#25913;&#36827;&#36951;&#25022;&#12290;&#36890;&#36807;&#24320;&#21457;&#22312;&#32447;&#30828;&#38408;&#20540;&#31639;&#27861;&#21644;&#21407;&#22987;-&#23545;&#20598;&#26694;&#26550;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#32500;&#24230;&#30340;&#23545;&#25968;&#25913;&#36827;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;</title><link>http://arxiv.org/abs/2311.01327</link><description>&lt;p&gt;
&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#39640;&#32500;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Linear Bandits with Knapsacks. (arXiv:2311.01327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#39640;&#32500;&#32447;&#24615;&#36172;&#33218;&#38382;&#39064;&#65292;&#21033;&#29992;&#31232;&#30095;&#32467;&#26500;&#23454;&#29616;&#25913;&#36827;&#36951;&#25022;&#12290;&#36890;&#36807;&#24320;&#21457;&#22312;&#32447;&#30828;&#38408;&#20540;&#31639;&#27861;&#21644;&#21407;&#22987;-&#23545;&#20598;&#26694;&#26550;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#29305;&#24449;&#32500;&#24230;&#30340;&#23545;&#25968;&#25913;&#36827;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#29305;&#24449;&#32500;&#24230;&#36739;&#22823;&#30340;&#39640;&#32500;&#35774;&#32622;&#19979;&#30340;&#20855;&#26377;&#32972;&#21253;&#32422;&#26463;&#30340;&#19978;&#19979;&#25991;&#36172;&#33218;&#38382;&#39064;&#12290;&#27599;&#20010;&#25163;&#33218;&#25289;&#21160;&#30340;&#22870;&#21169;&#31561;&#20110;&#31232;&#30095;&#39640;&#32500;&#26435;&#37325;&#21521;&#37327;&#19982;&#24403;&#21069;&#21040;&#36798;&#30340;&#29305;&#24449;&#30340;&#20056;&#31215;&#65292;&#21152;&#19978;&#39069;&#22806;&#30340;&#38543;&#26426;&#22122;&#22768;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#22914;&#20309;&#21033;&#29992;&#36825;&#31181;&#31232;&#30095;&#32467;&#26500;&#26469;&#23454;&#29616;CBwK&#38382;&#39064;&#30340;&#25913;&#36827;&#36951;&#25022;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#30340;&#30828;&#38408;&#20540;&#31639;&#27861;&#30340;&#21464;&#20307;&#65292;&#20197;&#22312;&#32447;&#26041;&#24335;&#36827;&#34892;&#31232;&#30095;&#20272;&#35745;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25105;&#20204;&#30340;&#22312;&#32447;&#20272;&#35745;&#22120;&#19982;&#21407;&#22987;-&#23545;&#20598;&#26694;&#26550;&#32467;&#21512;&#36215;&#26469;&#65292;&#22312;&#27599;&#20010;&#32972;&#21253;&#32422;&#26463;&#19978;&#20998;&#37197;&#19968;&#20010;&#23545;&#20598;&#21464;&#37327;&#65292;&#24182;&#21033;&#29992;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#26469;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#65292;&#20174;&#32780;&#25511;&#21046;&#32972;&#21253;&#23481;&#37327;&#30340;&#28040;&#32791;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#38598;&#25104;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#23454;&#29616;&#23545;&#29305;&#24449;&#32500;&#24230;&#30340;&#23545;&#25968;&#25913;&#36827;&#30340;&#27425;&#32447;&#24615;&#36951;&#25022;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#39033;&#24335;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the contextual bandits with knapsack (CBwK) problem under the high-dimensional setting where the dimension of the feature is large. The reward of pulling each arm equals the multiplication of a sparse high-dimensional weight vector and the feature of the current arrival, with additional random noise. In this paper, we investigate how to exploit this sparsity structure to achieve improved regret for the CBwK problem. To this end, we first develop an online variant of the hard thresholding algorithm that performs the sparse estimation in an online manner. We further combine our online estimator with a primal-dual framework, where we assign a dual variable to each knapsack constraint and utilize an online learning algorithm to update the dual variable, thereby controlling the consumption of the knapsack capacity. We show that this integrated approach allows us to achieve a sublinear regret that depends logarithmically on the feature dimension, thus improving the polynomial depend
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;EqDrive&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;EqMotion&#31561;&#21464;&#31890;&#23376;&#21644;&#20154;&#31867;&#39044;&#27979;&#27169;&#22411;&#20197;&#21450;&#22810;&#27169;&#24335;&#39044;&#27979;&#26426;&#21046;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#27169;&#22411;&#23481;&#37327;&#36739;&#20302;&#12289;&#21442;&#25968;&#26356;&#23569;&#12289;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17540</link><description>&lt;p&gt;
EqDrive: &#33258;&#21160;&#39550;&#39542;&#30340;&#39640;&#25928;&#31561;&#21464;&#36816;&#21160;&#39044;&#27979;&#19982;&#22810;&#27169;&#24335;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
EqDrive: Efficient Equivariant Motion Forecasting with Multi-Modality for Autonomous Driving. (arXiv:2310.17540v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#23637;&#20102;EqDrive&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;EqMotion&#31561;&#21464;&#31890;&#23376;&#21644;&#20154;&#31867;&#39044;&#27979;&#27169;&#22411;&#20197;&#21450;&#22810;&#27169;&#24335;&#39044;&#27979;&#26426;&#21046;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#22312;&#27169;&#22411;&#23481;&#37327;&#36739;&#20302;&#12289;&#21442;&#25968;&#26356;&#23569;&#12289;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#30340;&#24773;&#20917;&#19979;&#65292;&#21462;&#24471;&#20102;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#39044;&#27979;&#36710;&#36742;&#36816;&#21160;&#38656;&#35201;&#23545;&#36710;&#36742;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#65292;&#24182;&#20445;&#25345;&#22312;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#21464;&#25442;&#19979;&#30340;&#36816;&#21160;&#31561;&#21464;&#24615;&#12290;&#20256;&#32479;&#27169;&#22411;&#24448;&#24448;&#32570;&#20047;&#22788;&#29702;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20013;&#22797;&#26434;&#21160;&#21147;&#23398;&#21644;&#22330;&#26223;&#20013;&#21508;&#20027;&#20307;&#20043;&#38388;&#20132;&#20114;&#20851;&#31995;&#25152;&#38656;&#30340;&#22797;&#26434;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#27169;&#22411;&#23481;&#37327;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#39044;&#27979;&#35823;&#24046;&#21644;&#36739;&#20302;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;EqMotion&#65292;&#19968;&#20010;&#39046;&#20808;&#30340;&#31561;&#21464;&#31890;&#23376;&#21644;&#20154;&#31867;&#39044;&#27979;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36824;&#32771;&#34385;&#21040;&#19981;&#21464;&#30340;&#20027;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#29992;&#20110;&#22810;&#20195;&#29702;&#36710;&#36742;&#36816;&#21160;&#39044;&#27979;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#22810;&#27169;&#24335;&#39044;&#27979;&#26426;&#21046;&#20197;&#27010;&#29575;&#21270;&#26041;&#24335;&#32771;&#34385;&#22810;&#20010;&#21487;&#33021;&#30340;&#26410;&#26469;&#36335;&#24452;&#12290;&#36890;&#36807;&#21033;&#29992;EqMotion&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#21442;&#25968;&#26356;&#23569;&#65288;120&#19975;&#65289;&#21644;&#35757;&#32451;&#26102;&#38388;&#26174;&#33879;&#32553;&#30701;&#65288;&#23569;&#20110;..&#65289;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19994;&#30028;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Forecasting vehicular motions in autonomous driving requires a deep understanding of agent interactions and the preservation of motion equivariance under Euclidean geometric transformations. Traditional models often lack the sophistication needed to handle the intricate dynamics inherent to autonomous vehicles and the interaction relationships among agents in the scene. As a result, these models have a lower model capacity, which then leads to higher prediction errors and lower training efficiency. In our research, we employ EqMotion, a leading equivariant particle, and human prediction model that also accounts for invariant agent interactions, for the task of multi-agent vehicle motion forecasting. In addition, we use a multi-modal prediction mechanism to account for multiple possible future paths in a probabilistic manner. By leveraging EqMotion, our model achieves state-of-the-art (SOTA) performance with fewer parameters (1.2 million) and a significantly reduced training time (less 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#34917;&#25937;&#26694;&#26550;&#65292;RecAD&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#25512;&#33616;&#20197;&#26368;&#23567;&#25104;&#26412;&#20462;&#22797;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#22914;&#20309;&#20462;&#22797;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.16896</link><description>&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Recourse for Anomaly Detection in Multivariate Time Series. (arXiv:2309.16896v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#34917;&#25937;&#26694;&#26550;&#65292;RecAD&#65292;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#25512;&#33616;&#20197;&#26368;&#23567;&#25104;&#26412;&#20462;&#22797;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24110;&#21161;&#39046;&#22495;&#19987;&#23478;&#29702;&#35299;&#22914;&#20309;&#20462;&#22797;&#24322;&#24120;&#34892;&#20026;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#65292;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#26816;&#27979;&#24050;&#32463;&#21463;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#36890;&#24120;&#34920;&#31034;&#20020;&#30028;&#20107;&#20214;&#65292;&#20363;&#22914;&#31995;&#32479;&#25925;&#38556;&#25110;&#22806;&#37096;&#25915;&#20987;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#26377;&#25928;&#20043;&#22806;&#65292;&#25512;&#33616;&#24322;&#24120;&#32531;&#35299;&#34892;&#21160;&#22312;&#23454;&#36341;&#20013;&#20063;&#24456;&#37325;&#35201;&#20294;&#30740;&#31350;&#19981;&#36275;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#31639;&#27861;&#34917;&#25937;&#26041;&#27861;&#65292;&#21363;&#25512;&#33616;&#20197;&#26368;&#23567;&#25104;&#26412;&#20462;&#22797;&#24322;&#24120;&#26102;&#38388;&#24207;&#21015;&#65292;&#20197;&#20415;&#39046;&#22495;&#19987;&#23478;&#21487;&#20197;&#29702;&#35299;&#22914;&#20309;&#20462;&#22797;&#24322;&#24120;&#34892;&#20026;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#34917;&#25937;&#26694;&#26550;&#65292;&#31216;&#20026;RecAD&#65292;&#21487;&#20197;&#25512;&#33616;&#32763;&#36716;&#24322;&#24120;&#26102;&#38388;&#27493;&#39588;&#30340;&#34917;&#25937;&#34892;&#21160;&#12290;&#23545;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection in multivariate time series has received extensive study due to the wide spectrum of applications. An anomaly in multivariate time series usually indicates a critical event, such as a system fault or an external attack. Therefore, besides being effective in anomaly detection, recommending anomaly mitigation actions is also important in practice yet under-investigated. In this work, we focus on algorithmic recourse in time series anomaly detection, which is to recommend fixing actions on abnormal time series with a minimum cost so that domain experts can understand how to fix the abnormal behavior. To this end, we propose an algorithmic recourse framework, called RecAD, which can recommend recourse actions to flip the abnormal time steps. Experiments on two synthetic and one real-world datasets show the effectiveness of our framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.02562</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32852;&#21512;&#34920;&#31034;&#36827;&#34892;&#39135;&#29289;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Food Classification using Joint Representation of Visual and Textual Data. (arXiv:2308.02562v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32852;&#21512;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#36890;&#36807;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#23454;&#29616;&#22270;&#20687;&#20998;&#31867;&#65292;&#20351;&#29992;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#23454;&#29616;&#25991;&#26412;&#20998;&#31867;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#20934;&#30830;&#29575;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#27604;&#36739;&#20998;&#26512;&#36824;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#29289;&#20998;&#31867;&#26159;&#20581;&#24247;&#20445;&#20581;&#20013;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#31867;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;&#20102;&#20462;&#25913;&#29256;&#30340;EfficientNet&#21644;Mish&#28608;&#27963;&#20989;&#25968;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#65292;&#21516;&#26102;&#20351;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;BERT&#30340;&#32593;&#32476;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#22823;&#22411;&#24320;&#28304;&#25968;&#25454;&#38598;UPMC Food-101&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#19978;&#30340;&#20934;&#30830;&#29575;&#20998;&#21035;&#27604;&#31532;&#20108;&#26368;&#22909;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;11.57%&#21644;6.34%&#12290;&#25105;&#20204;&#36824;&#27604;&#36739;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#25991;&#26412;&#20998;&#31867;&#30340;&#20934;&#30830;&#29575;&#12289;&#31934;&#30830;&#29575;&#21644;&#21484;&#22238;&#29575;&#12290;&#36890;&#36807;&#23545;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Food classification is an important task in health care. In this work, we propose a multimodal classification framework that uses the modified version of EfficientNet with the Mish activation function for image classification, and the traditional BERT transformer-based network is used for text classification. The proposed network and the other state-of-the-art methods are evaluated on a large open-source dataset, UPMC Food-101. The experimental results show that the proposed network outperforms the other methods, a significant difference of 11.57% and 6.34% in accuracy is observed for image and text classification, respectively, when compared with the second-best performing method. We also compared the performance in terms of accuracy, precision, and recall for text classification using both machine learning and deep learning-based models. The comparative analysis from the prediction results of both images and text demonstrated the efficiency and robustness of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#26469;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.15933</link><description>&lt;p&gt;
&#36890;&#36807;&#39564;&#35777;&#21644;&#32416;&#27491;&#25552;&#31034;&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
You Can Generate It Again: Data-to-text Generation with Verification and Correction Prompting. (arXiv:2306.15933v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27493;&#39588;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#30340;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#26041;&#27861;&#65292;&#36890;&#36807;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#26469;&#25913;&#21892;&#36755;&#20986;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#26377;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#36755;&#20837;&#29983;&#25104;&#25991;&#26412;&#25551;&#36848;&#65288;&#31216;&#20026;&#25968;&#25454;&#29983;&#25104;&#25991;&#26412;&#65289;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#21253;&#25324;&#29983;&#25104;&#12289;&#39564;&#35777;&#21644;&#32416;&#27491;&#38454;&#27573;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#65292;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#19968;&#27425;&#24615;&#29983;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;VCP&#65288;&#39564;&#35777;&#21644;&#32416;&#27491;&#25552;&#31034;&#65289;&#65292;&#20174;&#27169;&#22411;&#29983;&#25104;&#21021;&#22987;&#36755;&#20986;&#24320;&#22987;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32487;&#32493;&#39564;&#35777;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#27491;&#30830;&#24615;&#12290;&#39564;&#35777;&#27493;&#39588;&#30340;&#35266;&#23519;&#32467;&#26524;&#34987;&#36716;&#21270;&#20026;&#19987;&#38376;&#30340;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#65292;&#35813;&#25552;&#31034;&#25351;&#31034;&#27169;&#22411;&#22312;&#37325;&#26032;&#29983;&#25104;&#36755;&#20986;&#26102;&#32771;&#34385;&#24050;&#35782;&#21035;&#30340;&#38169;&#35823;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#30340;&#32416;&#27491;&#33021;&#21147;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22521;&#35757;&#36807;&#31243;&#12290;&#35813;&#36807;&#31243;&#20351;&#27169;&#22411;&#33021;&#22815;&#34701;&#20837;&#38169;&#35823;&#25351;&#31034;&#25552;&#31034;&#30340;&#21453;&#39304;&#65292;&#20174;&#32780;&#25913;&#21892;&#36755;&#20986;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite significant advancements in existing models, generating text descriptions from structured data input, known as data-to-text generation, remains a challenging task. In this paper, we propose a novel approach that goes beyond traditional one-shot generation methods by introducing a multi-step process consisting of generation, verification, and correction stages. Our approach, VCP(Verification and Correction Prompting), begins with the model generating an initial output. We then proceed to verify the correctness of different aspects of the generated text. The observations from the verification step are converted into a specialized error-indication prompt, which instructs the model to regenerate the output while considering the identified errors. To enhance the model's correction ability, we have developed a carefully designed training procedure. This procedure enables the model to incorporate feedback from the error-indication prompt, resulting in improved output generation. Throu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#32806;&#21512;&#27969;&#21644;&#33258;&#22238;&#24402;&#27969;&#30340;&#19981;&#21516;&#26550;&#26500;&#21644;&#22810;&#26679;&#30446;&#26631;&#20998;&#24067;&#65292;&#21033;&#29992;&#21508;&#31181;&#27979;&#35797;&#32479;&#35745;&#37327;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#27491;&#35268;&#21270;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#23454;&#35777;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2302.12024</link><description>&lt;p&gt;
&#27604;&#36739;&#32806;&#21512;&#27969;&#21644;&#33258;&#22238;&#24402;&#27969;&#30340;&#40065;&#26834;&#32479;&#35745;&#26816;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Comparative Study of Coupling and Autoregressive Flows through Robust Statistical Tests. (arXiv:2302.12024v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12024
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#27604;&#36739;&#32806;&#21512;&#27969;&#21644;&#33258;&#22238;&#24402;&#27969;&#30340;&#19981;&#21516;&#26550;&#26500;&#21644;&#22810;&#26679;&#30446;&#26631;&#20998;&#24067;&#65292;&#21033;&#29992;&#21508;&#31181;&#27979;&#35797;&#32479;&#35745;&#37327;&#36827;&#34892;&#24615;&#33021;&#27604;&#36739;&#65292;&#20026;&#27491;&#35268;&#21270;&#27969;&#30340;&#29983;&#25104;&#27169;&#22411;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#30740;&#31350;&#21644;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#35268;&#21270;&#27969;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#20165;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#22797;&#26434;&#30446;&#26631;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#19988;&#36824;&#36890;&#36807;&#26500;&#36896;&#25552;&#20379;&#23494;&#24230;&#20272;&#35745;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#23545;&#32806;&#21512;&#27969;&#21644;&#33258;&#22238;&#24402;&#27969;&#36827;&#34892;&#28145;&#20837;&#27604;&#36739;&#30340;&#30740;&#31350;&#65292;&#21253;&#25324;&#20223;&#23556;&#21644;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#31867;&#22411;&#30340;&#22235;&#31181;&#19981;&#21516;&#26550;&#26500;&#65306;&#23454;&#20540;&#38750;&#20307;&#31215;&#20445;&#25345;&#65288;RealNVP&#65289;&#12289;&#25513;&#34109;&#33258;&#22238;&#24402;&#27969;&#65288;MAF&#65289;&#12289;&#32806;&#21512;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#65288;C-RQS&#65289;&#21644;&#33258;&#22238;&#24402;&#26377;&#29702;&#20108;&#27425;&#26679;&#26465;&#65288;A-RQS&#65289;&#12290;&#25105;&#20204;&#20851;&#27880;&#19968;&#32452;&#20174;4&#32500;&#21040;400&#32500;&#36882;&#22686;&#30340;&#22810;&#27169;&#24577;&#30446;&#26631;&#20998;&#24067;&#12290;&#36890;&#36807;&#20351;&#29992;&#19981;&#21516;&#30340;&#20004;&#26679;&#26412;&#27979;&#35797;&#30340;&#27979;&#35797;&#32479;&#35745;&#37327;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#24050;&#30693;&#36317;&#31163;&#24230;&#37327;&#30340;&#27979;&#35797;&#32479;&#35745;&#37327;&#65306;&#20999;&#29255;Wasserstein&#36317;&#31163;&#12289;&#32500;&#24230;&#24179;&#22343;&#19968;&#32500;Kolmogorov-Smirnov&#26816;&#39564;&#21644;&#30456;&#20851;&#30697;&#38453;&#20043;&#24046;&#30340;Frobenius&#33539;&#25968;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#21253;&#25324;&#20102;&#20197;&#19979;&#20272;&#35745;&#65306;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows have emerged as a powerful brand of generative models, as they not only allow for efficient sampling of complicated target distributions, but also deliver density estimation by construction. We propose here an in-depth comparison of coupling and autoregressive flows, both of the affine and rational quadratic spline type, considering four different architectures: Real-valued Non-Volume Preserving (RealNVP), Masked Autoregressive Flow (MAF), Coupling Rational Quadratic Spline (C-RQS), and Autoregressive Rational Quadratic Spline (A-RQS). We focus on a set of multimodal target distributions of increasing dimensionality ranging from 4 to 400. The performances are compared by means of different test-statistics for two-sample tests, built from known distance measures: the sliced Wasserstein distance, the dimension-averaged one-dimensional Kolmogorov-Smirnov test, and the Frobenius norm of the difference between correlation matrices. Furthermore, we include estimations of th
&lt;/p&gt;</description></item><item><title>AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.</title><link>http://arxiv.org/abs/2205.12787</link><description>&lt;p&gt;
&#20844;&#27491;&#28216;&#25103;&#65306;&#23545;&#24378;&#21270;&#23398;&#20064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.12787
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning algorithms excel in various board games but face challenges with impartial games. The researchers present a concrete example of the game nim, and show that AlphaZero-style algorithms have difficulty learning these impartial games on larger board sizes. The difference between impartial games and partisan games can be explained by the vulnerability to adversarial attacks and perturbations.
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#20284;AlphaZero&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#26827;&#30424;&#28216;&#25103;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20844;&#27491;&#28216;&#25103;&#20013;&#21364;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#20123;&#28216;&#25103;&#20013;&#29609;&#23478;&#20849;&#20139;&#26827;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#28216;&#25103;&#20363;&#23376;&#65292;&#21363;&#23567;&#23401;&#20204;&#29609;&#30340;&#23612;&#22982;&#28216;&#25103;&#65292;&#20197;&#21450;&#20854;&#20182;&#19968;&#20123;&#20844;&#27491;&#28216;&#25103;&#65292;&#36825;&#20123;&#28216;&#25103;&#20284;&#20046;&#25104;&#20026;AlphaZero&#21644;&#31867;&#20284;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#32458;&#33050;&#30707;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#19982;&#26368;&#36817;&#30340;&#30740;&#31350;&#19968;&#33268;&#65292;&#34920;&#26126;AlphaZero-style&#31639;&#27861;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25915;&#20987;&#21644;&#25932;&#23545;&#25200;&#21160;&#30340;&#24433;&#21709;&#65292;&#26174;&#31034;&#20102;&#22312;&#25152;&#26377;&#21512;&#27861;&#29366;&#24577;&#19979;&#23398;&#20064;&#25484;&#25569;&#36825;&#20123;&#28216;&#25103;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#21457;&#29616;&#23612;&#22982;&#28216;&#25103;&#22312;&#23567;&#22411;&#26827;&#30424;&#19978;&#21487;&#20197;&#23398;&#20064;&#65292;&#20294;&#24403;&#26827;&#30424;&#23610;&#23544;&#22686;&#22823;&#26102;&#65292;AlphaZero-style&#31639;&#27861;&#30340;&#23398;&#20064;&#36895;&#24230;&#26174;&#33879;&#20943;&#24930;&#12290;&#30452;&#35266;&#19978;&#65292;&#23612;&#22982;&#31561;&#20844;&#27491;&#28216;&#25103;&#19982;&#35937;&#26827;&#21644;&#22260;&#26827;&#31561;&#20826;&#27966;&#28216;&#25103;&#20043;&#38388;&#30340;&#21306;&#21035;&#22312;&#20110;&#65292;&#22914;&#26524;&#31995;&#32479;&#20013;&#28155;&#21152;&#20102;&#24494;&#23567;&#30340;&#22122;&#38899;&#65288;&#20363;&#22914;&#65292;&#26827;&#30424;&#30340;&#19968;&#23567;&#37096;&#20998;&#34987;&#35206;&#30422;&#65289;&#65292;&#23545;&#20110;&#20844;&#27491;&#28216;&#25103;&#26469;&#35828;&#65292;&#36825;&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
AlphaZero-style reinforcement learning (RL) algorithms excel in various board games but face challenges with impartial games, where players share pieces. We present a concrete example of a game - namely the children's game of nim - and other impartial games that seem to be a stumbling block for AlphaZero-style and similar reinforcement learning algorithms.  Our findings are consistent with recent studies showing that AlphaZero-style algorithms are vulnerable to adversarial attacks and adversarial perturbations, showing the difficulty of learning to master the games in all legal states.  We show that nim can be learned on small boards, but AlphaZero-style algorithms learning dramatically slows down when the board size increases. Intuitively, the difference between impartial games like nim and partisan games like Chess and Go can be explained by the fact that if a tiny amount of noise is added to the system (e.g. if a small part of the board is covered), for impartial games, it is typica
&lt;/p&gt;</description></item></channel></rss>