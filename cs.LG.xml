<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#22522;&#20110;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20248;&#21270;&#33258;&#20027;&#21345;&#36710;&#30340;&#25112;&#26415;&#20915;&#31574;&#65292;&#23558;&#39640;&#32423;&#20915;&#31574;&#19982;&#20302;&#32423;&#25511;&#21046;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#25216;&#24039;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06524</link><description>&lt;p&gt;
&#22522;&#20110;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#22870;&#21169;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#33258;&#20027;&#21345;&#36710;&#25112;&#26415;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Tactical Decision Making for Autonomous Trucks by Deep Reinforcement Learning with Total Cost of Operation Based Reward
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06524
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20248;&#21270;&#33258;&#20027;&#21345;&#36710;&#30340;&#25112;&#26415;&#20915;&#31574;&#65292;&#23558;&#39640;&#32423;&#20915;&#31574;&#19982;&#20302;&#32423;&#25511;&#21046;&#20998;&#31163;&#65292;&#24182;&#37319;&#29992;&#19981;&#21516;&#25216;&#24039;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#33258;&#20027;&#21345;&#36710;&#30340;&#25112;&#26415;&#20915;&#31574;&#21046;&#23450;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29305;&#21035;&#26159;&#38024;&#23545;&#39640;&#36895;&#20844;&#36335;&#22330;&#26223;&#20013;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#21644;&#21464;&#36947;&#21160;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#21644;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#30340;&#20302;&#32423;&#25511;&#21046;&#22120;&#20043;&#38388;&#20998;&#31163;&#39640;&#32423;&#20915;&#31574;&#36807;&#31243;&#21644;&#20302;&#32423;&#25511;&#21046;&#21160;&#20316;&#26159;&#26377;&#30410;&#30340;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#36807;&#22312;&#21345;&#36710;&#30340;&#20840;&#38754;&#36816;&#33829;&#25104;&#26412;&#65288;TCOP&#65289;&#20026;&#22522;&#30784;&#30340;&#22810;&#30446;&#26631;&#22870;&#21169;&#20989;&#25968;&#20248;&#21270;&#24615;&#33021;&#30340;&#19981;&#21516;&#26041;&#27861;&#65307;&#36890;&#36807;&#20026;&#22870;&#21169;&#20998;&#37327;&#28155;&#21152;&#26435;&#37325;&#65292;&#36890;&#36807;&#23545;&#22870;&#21169;&#20998;&#37327;&#36827;&#34892;&#24402;&#19968;&#21270;&#65292;&#20197;&#21450;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06524v1 Announce Type: cross  Abstract: We develop a deep reinforcement learning framework for tactical decision making in an autonomous truck, specifically for Adaptive Cruise Control (ACC) and lane change maneuvers in a highway scenario. Our results demonstrate that it is beneficial to separate high-level decision-making processes and low-level control actions between the reinforcement learning agent and the low-level controllers based on physical models. In the following, we study optimizing the performance with a realistic and multi-objective reward function based on Total Cost of Operation (TCOP) of the truck using different approaches; by adding weights to reward components, by normalizing the reward components and by using curriculum learning techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25506;&#32034;&#20102;&#20174;&#32447;&#24615;&#25506;&#27979;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#30340;&#39034;&#24207;&#24494;&#35843;&#29616;&#35937;&#21450;&#20854;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;</title><link>https://arxiv.org/abs/2402.18905</link><description>&lt;p&gt;
&#35770;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#30340;&#25910;&#25947;&#24615;&#65306;&#24212;&#32447;&#24615;&#25506;&#27979;&#36824;&#26159;&#23436;&#20840;&#24494;&#35843;&#65311;
&lt;/p&gt;
&lt;p&gt;
On the Convergence of Differentially-Private Fine-tuning: To Linearly Probe or to Fully Fine-tune?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24046;&#20998;&#38544;&#31169;&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#25506;&#32034;&#20102;&#20174;&#32447;&#24615;&#25506;&#27979;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#30340;&#39034;&#24207;&#24494;&#35843;&#29616;&#35937;&#21450;&#20854;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#24046;&#20998;&#38544;&#31169;&#24494;&#35843;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#21644;&#38544;&#31169;&#39044;&#31639;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#26426;&#22120;&#23398;&#20064;&#27969;&#27700;&#32447;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#30340;&#36807;&#31243;&#65306;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#38750;&#31169;&#26377;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#20351;&#29992;DP&#20248;&#21270;&#25216;&#26415;&#22312;&#31169;&#26377;&#25968;&#25454;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;DP&#35774;&#32622;&#20013;&#65292;&#24050;&#32463;&#35266;&#23519;&#21040;&#23436;&#20840;&#24494;&#35843;&#26377;&#26102;&#20505;&#24182;&#19981;&#24635;&#26159;&#20135;&#29983;&#26368;&#20339;&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#65292;&#21363;&#20351;&#23545;&#20110;&#20998;&#24067;&#20869;&#25968;&#25454;&#20063;&#26159;&#22914;&#27492;&#12290;&#26412;&#25991;&#65288;1&#65289;&#20998;&#26512;&#20102;DP&#32447;&#24615;&#25506;&#27979;&#65288;LP&#65289;&#21644;&#23436;&#20840;&#24494;&#35843;&#65288;FT&#65289;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#20197;&#21450;&#65288;2&#65289;&#25506;&#32034;&#20102;&#39034;&#24207;&#24494;&#35843;&#30340;&#29616;&#35937;&#65292;&#20174;&#32447;&#24615;&#25506;&#27979;&#24320;&#22987;&#65292;&#36807;&#28193;&#21040;&#23436;&#20840;&#24494;&#35843;&#65288;LP-FT&#65289;&#65292;&#20197;&#21450;&#23427;&#23545;&#27979;&#35797;&#25439;&#22833;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#26377;&#20851;DP&#24494;&#35843;&#22312;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25910;&#25947;&#24615;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#30830;&#23450;&#38544;&#31169;&#39044;&#31639;&#22312;&#32447;&#24615;&#25506;&#27979;&#21644;&#23436;&#20840;&#24494;&#35843;&#20043;&#38388;&#20998;&#37197;&#30340;&#25928;&#29992;&#26354;&#32447;&#12290;&#29702;&#35770;&#32467;&#26524;&#24471;&#21040;&#20102;&#23545;&#21508;&#31181;&#22522;&#20934;&#21644;&#27169;&#22411;&#30340;&#32463;&#39564;&#35780;&#20272;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18905v1 Announce Type: cross  Abstract: Differentially private (DP) machine learning pipelines typically involve a two-phase process: non-private pre-training on a public dataset, followed by fine-tuning on private data using DP optimization techniques. In the DP setting, it has been observed that full fine-tuning may not always yield the best test accuracy, even for in-distribution data. This paper (1) analyzes the training dynamics of DP linear probing (LP) and full fine-tuning (FT), and (2) explores the phenomenon of sequential fine-tuning, starting with linear probing and transitioning to full fine-tuning (LP-FT), and its impact on test loss. We provide theoretical insights into the convergence of DP fine-tuning within an overparameterized neural network and establish a utility curve that determines the allocation of privacy budget between linear probing and full fine-tuning. The theoretical results are supported by empirical evaluations on various benchmarks and models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;</title><link>http://arxiv.org/abs/2307.11127</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#23454;&#29616;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#19979;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Synthetic Control Methods by Density Matching under Implicit Endogeneitiy. (arXiv:2307.11127v1 [econ.EM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11127
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65292;&#36890;&#36807;&#23494;&#24230;&#21305;&#37197;&#26469;&#35299;&#20915;&#29616;&#26377;SCMs&#20013;&#30340;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#19982;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#36827;&#34892;&#21152;&#26435;&#24179;&#22343;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#23545;&#29031;&#26041;&#27861;&#65288;SCMs&#65289;&#24050;&#25104;&#20026;&#27604;&#36739;&#26696;&#20363;&#30740;&#31350;&#20013;&#22240;&#26524;&#25512;&#26029;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;SCMs&#30340;&#22522;&#26412;&#24605;&#24819;&#26159;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#35266;&#27979;&#32467;&#26524;&#30340;&#21152;&#26435;&#21644;&#26469;&#20272;&#35745;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#21453;&#20107;&#23454;&#32467;&#26524;&#12290;&#21512;&#25104;&#23545;&#29031;&#65288;SC&#65289;&#30340;&#20934;&#30830;&#24615;&#23545;&#20110;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;SC&#26435;&#37325;&#30340;&#20272;&#35745;&#25104;&#20026;&#20102;&#30740;&#31350;&#30340;&#28966;&#28857;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#25351;&#20986;&#29616;&#26377;&#30340;SCMs&#23384;&#22312;&#19968;&#20010;&#38544;&#24335;&#20869;&#29983;&#24615;&#38382;&#39064;&#65292;&#21363;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#19982;&#21453;&#20107;&#23454;&#32467;&#26524;&#27169;&#22411;&#20013;&#30340;&#35823;&#24046;&#39033;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#38382;&#39064;&#20250;&#23545;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#22120;&#20135;&#29983;&#20559;&#24046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#21305;&#37197;&#30340;&#26032;&#22411;SCM&#65292;&#20551;&#35774;&#32463;&#36807;&#22788;&#29702;&#21333;&#20803;&#30340;&#32467;&#26524;&#23494;&#24230;&#21487;&#20197;&#29992;&#26410;&#22788;&#29702;&#21333;&#20803;&#30340;&#23494;&#24230;&#30340;&#21152;&#26435;&#24179;&#22343;&#26469;&#36817;&#20284;&#65288;&#21363;&#28151;&#21512;&#27169;&#22411;&#65289;&#12290;&#22522;&#20110;&#36825;&#19968;&#20551;&#35774;&#65292;&#25105;&#20204;&#36890;&#36807;&#21305;&#37197;&#26469;&#20272;&#35745;SC&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic control methods (SCMs) have become a crucial tool for causal inference in comparative case studies. The fundamental idea of SCMs is to estimate counterfactual outcomes for a treated unit by using a weighted sum of observed outcomes from untreated units. The accuracy of the synthetic control (SC) is critical for estimating the causal effect, and hence, the estimation of SC weights has been the focus of much research. In this paper, we first point out that existing SCMs suffer from an implicit endogeneity problem, which is the correlation between the outcomes of untreated units and the error term in the model of a counterfactual outcome. We show that this problem yields a bias in the causal effect estimator. We then propose a novel SCM based on density matching, assuming that the density of outcomes of the treated unit can be approximated by a weighted average of the densities of untreated units (i.e., a mixture model). Based on this assumption, we estimate SC weights by matchi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;DOGD&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#24403;&#24310;&#36831;&#19981;&#25913;&#21464;&#26799;&#24230;&#21040;&#36798;&#39034;&#24207;&#26102;&#65292;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#12290;</title><link>http://arxiv.org/abs/2305.12131</link><description>&lt;p&gt;
&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Non-stationary Online Convex Optimization with Arbitrary Delays. (arXiv:2305.12131v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20219;&#24847;&#26102;&#24310;&#30340;&#38750;&#31283;&#24577;&#22312;&#32447;&#20984;&#20248;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;DOGD&#65292;&#24182;&#35777;&#26126;&#23427;&#33021;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#21516;&#26102;&#24403;&#24310;&#36831;&#19981;&#25913;&#21464;&#26799;&#24230;&#21040;&#36798;&#39034;&#24207;&#26102;&#65292;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20197;&#26799;&#24230;&#25110;&#20854;&#20182;&#20989;&#25968;&#20449;&#24687;&#21487;&#20197;&#20219;&#24847;&#24310;&#36831;&#20026;&#29305;&#28857;&#30340;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#19982;&#20043;&#21069;&#30740;&#31350;&#31283;&#24577;&#29615;&#22659;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#38750;&#31283;&#24577;&#29615;&#22659;&#19979;&#30340;&#24310;&#36831;OCO&#65292;&#24182;&#26088;&#22312;&#26368;&#23567;&#21270;&#19982;&#20219;&#20309;&#27604;&#36739;&#22120;&#24207;&#21015;&#30456;&#20851;&#30340;&#21160;&#24577;&#36951;&#25022;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21363;DOGD&#65292;&#35813;&#31639;&#27861;&#26681;&#25454;&#20854;&#21040;&#36798;&#39034;&#24207;&#20026;&#27599;&#20010;&#24310;&#36831;&#26799;&#24230;&#25191;&#34892;&#28176;&#21464;&#19979;&#38477;&#27493;&#39588;&#12290;&#23613;&#31649;&#23427;&#24456;&#31616;&#21333;&#65292;&#20294;&#25105;&#20204;&#30340;&#26032;&#22411;&#20998;&#26512;&#34920;&#26126;&#65292;DOGD&#21487;&#20197;&#22312;&#26368;&#22351;&#24773;&#20917;&#19979;&#33719;&#24471;$O(\sqrt{dT}(P_T+1))$&#30340;&#21160;&#24577;&#36951;&#25022;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#26368;&#22823;&#24310;&#36831;&#65292;$T$&#26159;&#26102;&#38388;&#36328;&#24230;&#65292;$P_T$&#26159;&#27604;&#36739;&#22120;&#30340;&#36335;&#24452;&#38271;&#24230;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#22312;&#24310;&#36831;&#19981;&#25913;&#21464;&#28176;&#21464;&#30340;&#21040;&#36798;&#39034;&#24207;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23558;&#21160;&#24577;&#36951;&#25022;&#20943;&#23569;&#21040;$O(\sqrt{S}(1+P_T))$&#65292;&#20854;&#20013;$S$&#26159;&#24310;&#36831;&#20043;&#21644;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;DOGD&#25193;&#23637;&#20026;&#26356;&#36890;&#29992;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#23427;&#23454;&#29616;&#20102;&#19982;DOGD&#30456;&#21516;&#30340;&#36951;&#25022;&#30028;&#12290;&#24191;&#27867;&#30340;&#27169;&#25311;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online convex optimization (OCO) with arbitrary delays, in which gradients or other information of functions could be arbitrarily delayed, has received increasing attention recently. Different from previous studies that focus on stationary environments, this paper investigates the delayed OCO in non-stationary environments, and aims to minimize the dynamic regret with respect to any sequence of comparators. To this end, we first propose a simple algorithm, namely DOGD, which performs a gradient descent step for each delayed gradient according to their arrival order. Despite its simplicity, our novel analysis shows that DOGD can attain an $O(\sqrt{dT}(P_T+1)$ dynamic regret bound in the worst case, where $d$ is the maximum delay, $T$ is the time horizon, and $P_T$ is the path length of comparators. More importantly, in case delays do not change the arrival order of gradients, it can automatically reduce the dynamic regret to $O(\sqrt{S}(1+P_T))$, where $S$ is the sum of delays. Furtherm
&lt;/p&gt;</description></item></channel></rss>