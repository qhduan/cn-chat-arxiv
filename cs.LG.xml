<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>EmSHAP&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;GRU&#26469;&#28040;&#38500;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;Shapley&#20540;&#36129;&#29486;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.01078</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;&#20934;&#30830;Shapley&#20540;&#20272;&#35745;&#29992;&#20110;&#21487;&#35299;&#37322;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Energy Model-based Accurate Shapley Value Estimation for Interpretable Deep Learning Predictive Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01078
&lt;/p&gt;
&lt;p&gt;
EmSHAP&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;GRU&#26469;&#28040;&#38500;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#21487;&#20197;&#26377;&#25928;&#36817;&#20284;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;Shapley&#20540;&#36129;&#29486;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26377;&#21033;&#24037;&#20855;&#65292;Shapley&#20540;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35745;&#31639;&#36127;&#36733;&#38543;&#30528;&#36755;&#20837;&#29305;&#24449;&#30340;&#22686;&#21152;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#20272;&#35745;Shapley&#20540;&#26159;&#19968;&#39033;&#22256;&#38590;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21152;&#36895;Shapley&#20540;&#20272;&#35745;&#26041;&#27861;&#24517;&#39035;&#22312;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EmSHAP&#65288;&#22522;&#20110;&#33021;&#37327;&#27169;&#22411;&#30340;Shapley&#20540;&#20272;&#35745;&#65289;&#65292;&#23427;&#21487;&#20197;&#26377;&#25928;&#22320;&#36817;&#20284;&#39044;&#26399;Shapley&#36129;&#29486;&#20989;&#25968;/&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20219;&#24847;&#29305;&#24449;&#23376;&#38598;&#19979;&#32473;&#20986;&#20854;&#20313;&#29305;&#24449;&#30340;&#24773;&#20917;&#12290;&#20026;&#20102;&#30830;&#23450;&#33021;&#37327;&#27169;&#22411;&#20013;&#30340;&#25552;&#35758;&#26465;&#20214;&#20998;&#24067;&#65292;&#24341;&#20837;&#20102;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#29305;&#24449;&#26144;&#23556;&#21040;&#38544;&#34255;&#31354;&#38388;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#36755;&#20837;&#29305;&#24449;&#39034;&#24207;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#21160;&#24577;&#25513;&#34109;&#26041;&#26696;.
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01078v1 Announce Type: new  Abstract: As a favorable tool for explainable artificial intelligence (XAI), Shapley value has been widely used to interpret deep learning based predictive models. However, accurate and efficient estimation of Shapley value is a difficult task since the computation load grows exponentially with the increase of input features. Most existing accelerated Shapley value estimation methods have to compromise on estimation accuracy with efficiency. In this article, we present EmSHAP(Energy model-based Shapley value estimation), which can effectively approximate the expectation of Shapley contribution function/deep learning model under arbitrary subset of features given the rest. In order to determine the proposal conditional distribution in the energy model, a gated recurrent unit(GRU) is introduced by mapping the input features onto a hidden space, so that the impact of input feature orderings can be eliminated. In addition, a dynamic masking scheme is 
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.05066</link><description>&lt;p&gt;
&#22797;&#20301;&#21644;&#25552;&#28860;&#65306;&#20811;&#26381;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reset &amp; Distill: A Recipe for Overcoming Negative Transfer in Continual Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05066
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#32622;&#20195;&#29702;&#30340;&#32593;&#32476;&#21644;&#25552;&#28860;&#30693;&#35782;&#65292;&#26377;&#25928;&#20811;&#26381;&#20102;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35748;&#20026;&#21457;&#23637;&#26377;&#25928;&#30340;&#25345;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#31639;&#27861;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#26159;&#24403;&#38656;&#35201;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#20250;&#21457;&#29983;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#23454;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#31181;&#38382;&#39064;&#22312;CRL&#20013;&#32463;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#26080;&#27861;&#36890;&#36807;&#26368;&#36817;&#19968;&#20123;&#26088;&#22312;&#20943;&#36731;RL&#20195;&#29702;&#30340;&#21487;&#22609;&#24615;&#25439;&#22833;&#30340;&#24037;&#20316;&#26469;&#26377;&#25928;&#35299;&#20915;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Reset &amp; Distill&#65288;R&amp;D&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20811;&#26381;CRL&#20013;&#36127;&#36801;&#31227;&#38382;&#39064;&#12290;R&amp;D&#32467;&#21512;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#21363;&#37325;&#32622;&#20195;&#29702;&#30340;&#22312;&#32447;&#28436;&#21592;&#21644;&#35780;&#35770;&#32593;&#32476;&#20197;&#23398;&#20064;&#26032;&#20219;&#21153;&#65292;&#20197;&#21450;&#31163;&#32447;&#23398;&#20064;&#27493;&#39588;&#65292;&#29992;&#20110;&#25552;&#28860;&#22312;&#32447;&#28436;&#21592;&#21644;&#20197;&#21069;&#19987;&#23478;&#21160;&#20316;&#27010;&#29575;&#30340;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;Meta-World&#20219;&#21153;&#30340;&#38271;&#24207;&#21015;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;&#26368;&#36817;&#30340;&#22522;&#32447;&#65292;&#21462;&#24471;&#20102;&#26174;&#30528;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05066v1 Announce Type: cross  Abstract: We argue that one of the main obstacles for developing effective Continual Reinforcement Learning (CRL) algorithms is the negative transfer issue occurring when the new task to learn arrives. Through comprehensive experimental validation, we demonstrate that such issue frequently exists in CRL and cannot be effectively addressed by several recent work on mitigating plasticity loss of RL agents. To that end, we develop Reset &amp; Distill (R&amp;D), a simple yet highly effective method, to overcome the negative transfer problem in CRL. R&amp;D combines a strategy of resetting the agent's online actor and critic networks to learn a new task and an offline learning step for distilling the knowledge from the online actor and previous expert's action probabilities. We carried out extensive experiments on long sequence of Meta-World tasks and show that our method consistently outperforms recent baselines, achieving significantly higher success rates acr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#27010;&#29575;&#31354;&#38388;&#19978;&#30340;Riemannian SGD&#21644;SVRG&#27969;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#20016;&#23500;Wasserstein&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.13530</link><description>&lt;p&gt;
&#22312;Wasserstein&#27010;&#29575;&#31354;&#38388;&#19978;&#29702;&#35299;Riemannian SGD&#21644;SVRG&#27969;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding the Riemannian SGD and SVRG Flows on Wasserstein Probabilistic Space. (arXiv:2401.13530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;Wasserstein&#27010;&#29575;&#31354;&#38388;&#19978;&#30340;Riemannian SGD&#21644;SVRG&#27969;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#26469;&#20016;&#23500;Wasserstein&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#20110;Riemannian&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#30740;&#31350;&#20026;&#20248;&#21270;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#35265;&#35299;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#27010;&#29575;&#27979;&#24230;&#24230;&#37327;&#31354;&#38388;&#20316;&#20026;&#27969;&#24418;&#65292;&#37197;&#22791;&#31532;&#20108;&#38454;Wasserstein&#36317;&#31163;&#65292;&#23588;&#20854;&#24341;&#20154;&#20851;&#27880;&#65292;&#22240;&#20026;&#22312;&#20854;&#19978;&#30340;&#20248;&#21270;&#21487;&#20197;&#19982;&#23454;&#38469;&#30340;&#37319;&#26679;&#36807;&#31243;&#30456;&#20851;&#32852;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;Wasserstein&#31354;&#38388;&#19978;&#30340;&#26368;&#20248;&#21270;&#26041;&#27861;&#26159;Riemannian&#26799;&#24230;&#27969;&#65288;&#21363;&#65292;&#22312;&#26368;&#23567;&#21270;KL&#25955;&#24230;&#26102;&#30340;Langevin&#21160;&#21147;&#23398;&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#23558;&#26799;&#24230;&#27969;&#24310;&#23637;&#21040;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#27969;&#21644;&#38543;&#26426;&#26041;&#24046;&#20943;&#23569;&#26799;&#24230;&#65288;SVRG&#65289;&#27969;&#65292;&#20016;&#23500;Wasserstein&#31354;&#38388;&#20013;&#30340;&#36830;&#32493;&#20248;&#21270;&#26041;&#27861;&#12290;Euclidean&#31354;&#38388;&#19978;&#30340;&#36825;&#20004;&#31181;&#27969;&#26159;&#26631;&#20934;&#30340;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#65292;&#32780;&#23427;&#20204;&#22312;Riemannian&#31354;&#38388;&#20013;&#30340;&#23545;&#24212;&#26041;&#27861;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#36890;&#36807;&#21033;&#29992;Wasserstein&#31354;&#38388;&#20013;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#26469;&#36817;&#20284;&#31163;&#25955;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, optimization on the Riemannian manifold has provided new insights to the optimization community. In this regard, the manifold taken as the probability measure metric space equipped with the second-order Wasserstein distance is of particular interest, since optimization on it can be linked to practical sampling processes. In general, the oracle (continuous) optimization method on Wasserstein space is Riemannian gradient flow (i.e., Langevin dynamics when minimizing KL divergence). In this paper, we aim to enrich the continuous optimization methods in the Wasserstein space by extending the gradient flow into the stochastic gradient descent (SGD) flow and stochastic variance reduction gradient (SVRG) flow. The two flows on Euclidean space are standard stochastic optimization methods, while their Riemannian counterparts are not explored yet. By leveraging the structures in Wasserstein space, we construct a stochastic differential equation (SDE) to approximate the discrete dynamic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2308.15734</link><description>&lt;p&gt;
&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Efficient and Explainable Graph Neural Architecture Search via Monte-Carlo Tree Search. (arXiv:2308.15734v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15734
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21517;&#20026;ExGNAS&#12290;&#23427;&#21253;&#25324;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#33021;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#22312;&#21508;&#20010;&#39046;&#22495;&#36827;&#34892;&#25968;&#25454;&#31185;&#23398;&#20219;&#21153;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23613;&#31649;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#20013;&#20351;&#29992;GNNs&#65292;&#20294;&#23545;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26469;&#35828;&#65292;&#22312;&#19981;&#21516;&#30340;&#22270;&#20013;&#35774;&#35745;/&#36873;&#25321;&#26368;&#20339;GNN&#26550;&#26500;&#26159;&#19968;&#39033;&#36153;&#21147;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#33410;&#30465;&#20154;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#65292;&#24050;&#32463;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#65288;Graph NAS&#65289;&#26469;&#25628;&#32034;&#32467;&#21512;&#29616;&#26377;&#32452;&#20214;&#30340;&#27425;&#20248;GNN&#26550;&#26500;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#29616;&#26377;&#30340;Graph NAS&#26041;&#27861;&#33021;&#22815;&#21516;&#26102;&#28385;&#36275;&#21487;&#35299;&#37322;&#24615;&#12289;&#39640;&#25928;&#24615;&#21644;&#36866;&#24212;&#22810;&#26679;&#21270;&#22270;&#24418;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;Graph NAS&#26041;&#27861;&#65292;&#31216;&#20026;ExGNAS&#65292;&#23427;&#21253;&#25324;&#65288;i&#65289;&#19968;&#20010;&#21487;&#20197;&#36866;&#24212;&#21508;&#31181;&#22270;&#24418;&#30340;&#31616;&#21333;&#25628;&#32034;&#31354;&#38388;&#21644;&#65288;ii&#65289;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#30340;&#25628;&#32034;&#31639;&#27861;&#12290;&#25628;&#32034;&#31354;&#38388;&#20165;&#21253;&#21547;&#21487;&#20197;&#22788;&#29702;&#21516;&#36136;&#21644;&#24322;&#36136;&#22270;&#30340;&#22522;&#26412;&#20989;&#25968;&#12290;&#25628;&#32034;&#31639;&#27861;&#36890;&#36807;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#39640;&#25928;&#22320;&#25628;&#32034;&#26368;&#20339;GNN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are powerful tools for performing data science tasks in various domains. Although we use GNNs in wide application scenarios, it is a laborious task for researchers and practitioners to design/select optimal GNN rchitectures in diverse graphs. To save human efforts and computational costs, graph neural architecture search (Graph NAS) has been used to search for a sub-optimal GNN architecture that combines existing components. However, there are no existing Graph NAS methods that satisfy explainability, efficiency, and adaptability to various graphs. Therefore, we propose an efficient and explainable Graph NAS method, called ExGNAS, which consists of (i) a simple search space that can adapt to various graphs and (ii) a search algorithm that makes the decision process explainable. The search space includes only fundamental functions that can handle homophilic and heterophilic graphs. The search algorithm efficiently searches for the best GNN architecture via M
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#26377;&#25928;&#23398;&#20064;&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#30528;&#31283;&#23450;&#32500;&#25968;&#22686;&#22823;&#32780;&#23398;&#20064;&#25152;&#26377;&#29366;&#24577;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.13409</link><description>&lt;p&gt;
&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#26377;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Quantum States Prepared With Few Non-Clifford Gates. (arXiv:2305.13409v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13409
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#26377;&#25928;&#23398;&#20064;&#20960;&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#30340;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#20010;&#38543;&#30528;&#31283;&#23450;&#32500;&#25968;&#22686;&#22823;&#32780;&#23398;&#20064;&#25152;&#26377;&#29366;&#24577;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#23398;&#20064;&#36890;&#36807;&#20811;&#21033;&#31119;&#24503;&#38376;&#21644;$O(\log(n))$&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23545;&#20110;&#26368;&#22810;&#20351;&#29992;$t$&#20010;&#38750;&#20811;&#21033;&#31119;&#24503;&#38376;&#21046;&#22791;&#30340;$n$&#37327;&#23376;&#27604;&#29305;&#29366;&#24577;$|\psi\rangle$&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#29992;$\mathsf{poly}(n,2^t,1/\epsilon)$&#26102;&#38388;&#21644;$|\psi\rangle$&#30340;&#22797;&#21046;&#26469;&#23398;&#20064;$|\psi\rangle$&#65292;&#20351;&#20854;&#36319;&#30495;&#23454;&#29366;&#24577;&#30340;&#36317;&#31163;&#19981;&#36229;&#36807;$\epsilon$&#12290;&#35813;&#32467;&#26524;&#26159;&#19968;&#20010;&#31283;&#23450;&#32500;&#25968;&#36739;&#22823;&#30340;&#29366;&#24577;&#23398;&#20064;&#31639;&#27861;&#30340;&#29305;&#20363;&#65292;&#24403;&#19968;&#20010;&#37327;&#23376;&#29366;&#24577;&#30340;&#31283;&#23450;&#23376;&#32500;&#25968;&#20026;$k$&#65292;&#34920;&#31034;&#23427;&#34987;&#19968;&#20010;&#30001;$2^k$&#20010;Pauli&#31639;&#23376;&#30340;Abel&#32676;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give an algorithm that efficiently learns a quantum state prepared by Clifford gates and $O(\log(n))$ non-Clifford gates. Specifically, for an $n$-qubit state $\lvert \psi \rangle$ prepared with at most $t$ non-Clifford gates, we show that $\mathsf{poly}(n,2^t,1/\epsilon)$ time and copies of $\lvert \psi \rangle$ suffice to learn $\lvert \psi \rangle$ to trace distance at most $\epsilon$. This result follows as a special case of an algorithm for learning states with large stabilizer dimension, where a quantum state has stabilizer dimension $k$ if it is stabilized by an abelian group of $2^k$ Pauli operators. We also develop an efficient property testing algorithm for stabilizer dimension, which may be of independent interest.
&lt;/p&gt;</description></item></channel></rss>