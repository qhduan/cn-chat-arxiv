<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(mDCGAN)&#65292;&#38024;&#23545;&#39640;&#36136;&#37327;&#33402;&#26415;&#21697;&#29983;&#25104;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#26222;&#36941;&#35757;&#32451;&#38382;&#39064;&#65292;&#26377;&#25928;&#25506;&#32034;&#25277;&#35937;&#32472;&#30011;&#20013;&#30340;&#39068;&#33394;&#21644;&#31508;&#35302;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.18397</link><description>&lt;p&gt;
&#20351;&#29992;&#25913;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#22312;&#25277;&#35937;&#33402;&#26415;&#20013;&#36827;&#34892;&#39068;&#33394;&#21644;&#31508;&#35302;&#27169;&#24335;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Colour and Brush Stroke Pattern Recognition in Abstract Art using Modified Deep Convolutional Generative Adversarial Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18397
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#25913;&#36827;&#30340;&#28145;&#24230;&#21367;&#31215;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(mDCGAN)&#65292;&#38024;&#23545;&#39640;&#36136;&#37327;&#33402;&#26415;&#21697;&#29983;&#25104;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#35299;&#20915;&#20102;&#26222;&#36941;&#35757;&#32451;&#38382;&#39064;&#65292;&#26377;&#25928;&#25506;&#32034;&#25277;&#35937;&#32472;&#30011;&#20013;&#30340;&#39068;&#33394;&#21644;&#31508;&#35302;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#33402;&#26415;&#26159;&#19968;&#31181;&#24191;&#21463;&#27426;&#36814;&#12289;&#34987;&#24191;&#27867;&#35752;&#35770;&#30340;&#33402;&#26415;&#24418;&#24335;&#65292;&#36890;&#24120;&#33021;&#22815;&#25551;&#32472;&#20986;&#33402;&#26415;&#23478;&#30340;&#24773;&#24863;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#23581;&#35797;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#36793;&#32536;&#26816;&#27979;&#12289;&#31508;&#35302;&#21644;&#24773;&#24863;&#35782;&#21035;&#31639;&#27861;&#26469;&#30740;&#31350;&#25277;&#35937;&#33402;&#26415;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#31070;&#32463;&#32593;&#32476;(GAN)&#23545;&#24191;&#27867;&#20998;&#24067;&#30340;&#25277;&#35937;&#32472;&#30011;&#36827;&#34892;&#30740;&#31350;&#12290; GAN&#20855;&#26377;&#23398;&#20064;&#21644;&#20877;&#29616;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#33021;&#22815;&#26377;&#25928;&#22320;&#25506;&#32034;&#21644;&#30740;&#31350;&#29983;&#25104;&#30340;&#22270;&#20687;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#20811;&#26381;&#24120;&#35265;&#35757;&#32451;&#38382;&#39064;&#30340;&#39640;&#25928;GAN&#26550;&#26500;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#39640;&#36136;&#37327;&#33402;&#26415;&#21697;&#29983;&#25104;&#30340;&#25913;&#36827;DCGAN(mDCGAN)&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#23545;&#25152;&#20570;&#20462;&#25913;&#30340;&#28145;&#20837;&#25506;&#35752;&#65292;&#28145;&#20837;&#30740;&#31350;DCGAN&#30340;&#22797;&#26434;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18397v1 Announce Type: cross  Abstract: Abstract Art is an immensely popular, discussed form of art that often has the ability to depict the emotions of an artist. Many researchers have made attempts to study abstract art in the form of edge detection, brush stroke and emotion recognition algorithms using machine and deep learning. This papers describes the study of a wide distribution of abstract paintings using Generative Adversarial Neural Networks(GAN). GANs have the ability to learn and reproduce a distribution enabling researchers and scientists to effectively explore and study the generated image space. However, the challenge lies in developing an efficient GAN architecture that overcomes common training pitfalls. This paper addresses this challenge by introducing a modified-DCGAN (mDCGAN) specifically designed for high-quality artwork generation. The approach involves a thorough exploration of the modifications made, delving into the intricate workings of DCGANs, opt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#25935;&#24863;&#24230;&#26469;&#25233;&#21046;&#36951;&#24536;&#25968;&#25454;&#30340;&#36129;&#29486;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15109</link><description>&lt;p&gt;
&#25233;&#21046;&#26679;&#26412;&#36129;&#29486;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning by Suppressing Sample Contribution
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#36951;&#24536;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#25935;&#24863;&#24230;&#26469;&#25233;&#21046;&#36951;&#24536;&#25968;&#25454;&#30340;&#36129;&#29486;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#65288;MU&#65289;&#26159;&#25351;&#20174;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#21024;&#38500;&#25968;&#25454;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#28041;&#21450;&#8220;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#8221;&#12290;&#26412;&#25991;&#20174;&#35757;&#32451;&#25968;&#25454;&#21644;&#26410;&#35265;&#25968;&#25454;&#23545;&#27169;&#22411;&#36129;&#29486;&#30340;&#22522;&#26412;&#21306;&#21035;&#20837;&#25163;&#65306;&#35757;&#32451;&#25968;&#25454;&#23545;&#26368;&#32456;&#27169;&#22411;&#26377;&#36129;&#29486;&#65292;&#32780;&#26410;&#35265;&#25968;&#25454;&#27809;&#26377;&#12290;&#25105;&#20204;&#29702;&#35770;&#19978;&#21457;&#29616;&#36755;&#20837;&#25935;&#24863;&#24230;&#21487;&#20197;&#36817;&#20284;&#34913;&#37327;&#36129;&#29486;&#65292;&#24182;&#23454;&#38469;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#31216;&#20026;MU-Mis&#65288;&#36890;&#36807;&#26368;&#23567;&#21270;&#36755;&#20837;&#25935;&#24863;&#24230;&#36827;&#34892;&#26426;&#22120;&#36951;&#24536;&#65289;&#65292;&#26469;&#25233;&#21046;&#36951;&#24536;&#25968;&#25454;&#30340;&#36129;&#29486;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MU-Mis&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;MU&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;MU-Mis&#19982;MU&#30340;&#24212;&#29992;&#26356;&#21152;&#23494;&#20999;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20351;&#29992;&#21097;&#20313;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15109v1 Announce Type: new  Abstract: Machine Unlearning (MU) is to forget data from a well-trained model, which is practically important due to the "right to be forgotten". In this paper, we start from the fundamental distinction between training data and unseen data on their contribution to the model: the training data contributes to the final model while the unseen data does not. We theoretically discover that the input sensitivity can approximately measure the contribution and practically design an algorithm, called MU-Mis (machine unlearning via minimizing input sensitivity), to suppress the contribution of the forgetting data. Experimental results demonstrate that MU-Mis outperforms state-of-the-art MU methods significantly. Additionally, MU-Mis aligns more closely with the application of MU as it does not require the use of remaining data.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#26465;&#24418;&#30721;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#20934;&#25345;&#20037;&#21516;&#35843;&#19982;&#22270;&#20687;&#25345;&#20037;&#21516;&#35843;&#32467;&#21512;&#65292;&#21487;&#20197;&#37327;&#21270;&#20219;&#24847;&#32500;&#24230;&#20004;&#20010;&#28857;&#38598;&#20043;&#38388;&#30340;&#20960;&#20309;-&#25299;&#25169;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#24341;&#20837;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;&#26469;&#37327;&#21270;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.15058</link><description>&lt;p&gt;
&#28151;&#21512;&#26465;&#24418;&#30721;&#65306;&#37327;&#21270;&#28857;&#20113;&#20043;&#38388;&#30340;&#20960;&#20309;-&#25299;&#25169;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mixup Barcodes: Quantifying Geometric-Topological Interactions between Point Clouds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#28151;&#21512;&#26465;&#24418;&#30721;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#26631;&#20934;&#25345;&#20037;&#21516;&#35843;&#19982;&#22270;&#20687;&#25345;&#20037;&#21516;&#35843;&#32467;&#21512;&#65292;&#21487;&#20197;&#37327;&#21270;&#20219;&#24847;&#32500;&#24230;&#20004;&#20010;&#28857;&#38598;&#20043;&#38388;&#30340;&#20960;&#20309;-&#25299;&#25169;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#24341;&#20837;&#31616;&#21333;&#30340;&#32479;&#35745;&#37327;&#26469;&#37327;&#21270;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23558;&#26631;&#20934;&#25345;&#20037;&#21516;&#35843;&#19982;&#22270;&#20687;&#25345;&#20037;&#21516;&#35843;&#30456;&#32467;&#21512;&#65292;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#24449;&#24418;&#29366;&#21644;&#23427;&#20204;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#65306;&#65288;1&#65289;&#28151;&#21512;&#26465;&#24418;&#30721;&#65292;&#25429;&#25417;&#20219;&#24847;&#32500;&#24230;&#20004;&#20010;&#28857;&#38598;&#20043;&#38388;&#30340;&#20960;&#20309;-&#25299;&#25169;&#30456;&#20114;&#20316;&#29992;&#65288;&#28151;&#21512;&#65289;&#65307;&#65288;2&#65289;&#31616;&#21333;&#30340;&#24635;&#28151;&#21512;&#21644;&#24635;&#30334;&#20998;&#27604;&#28151;&#21512;&#32479;&#35745;&#37327;&#65292;&#20316;&#20026;&#19968;&#20010;&#21333;&#19968;&#25968;&#23383;&#26469;&#37327;&#21270;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65307;&#65288;3&#65289;&#19968;&#20010;&#29992;&#20110;&#25805;&#20316;&#19978;&#36848;&#24037;&#20855;&#30340;&#36719;&#20214;&#24037;&#20855;&#12290;&#20316;&#20026;&#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#23558;&#35813;&#24037;&#20855;&#24212;&#29992;&#21040;&#19968;&#20010;&#28304;&#33258;&#26426;&#22120;&#23398;&#20064;&#30340;&#38382;&#39064;&#19978;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#31867;&#21035;&#23884;&#20837;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25299;&#25169;&#28151;&#21512;&#26159;&#19968;&#31181;&#29992;&#20110;&#34920;&#24449;&#20302;&#32500;&#21644;&#39640;&#32500;&#25968;&#25454;&#20132;&#20114;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#19982;&#25345;&#20037;&#21516;&#35843;&#30340;&#20856;&#22411;&#29992;&#27861;&#30456;&#27604;&#65292;&#36825;&#20010;&#26032;&#24037;&#20855;&#23545;&#20110;&#25299;&#25169;&#29305;&#24449;&#30340;&#20960;&#20309;&#20301;&#32622;&#26356;&#20026;&#25935;&#24863;&#65292;&#36825;&#36890;&#24120;&#26159;&#21487;&#21462;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15058v1 Announce Type: cross  Abstract: We combine standard persistent homology with image persistent homology to define a novel way of characterizing shapes and interactions between them. In particular, we introduce: (1) a mixup barcode, which captures geometric-topological interactions (mixup) between two point sets in arbitrary dimension; (2) simple summary statistics, total mixup and total percentage mixup, which quantify the complexity of the interactions as a single number; (3) a software tool for playing with the above.   As a proof of concept, we apply this tool to a problem arising from machine learning. In particular, we study the disentanglement in embeddings of different classes. The results suggest that topological mixup is a useful method for characterizing interactions for low and high-dimensional data. Compared to the typical usage of persistent homology, the new tool is sensitive to the geometric locations of the topological features, which is often desirabl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.10793</link><description>&lt;p&gt;
&#25513;&#30721;&#27880;&#24847;&#21147;&#26159;&#22270;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Masked Attention is All You Need for Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10793
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#31616;&#21333;&#26367;&#20195;&#26041;&#27861;&#65292;&#31216;&#20026;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#65292;&#20854;&#21033;&#29992;&#27880;&#24847;&#21147;&#30697;&#38453;&#26469;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#24182;&#32988;&#36807;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#21644;&#28040;&#24687;&#20256;&#36882;&#31639;&#27861;&#30340;&#21464;&#31181;&#20027;&#35201;&#29992;&#20110;&#22312;&#22270;&#19978;&#23398;&#20064;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#23427;&#20204;&#30340;&#28789;&#27963;&#24615;&#12289;&#36895;&#24230;&#21644;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#24378;&#22823;&#32780;&#36890;&#29992;&#30340;GNNs&#38656;&#35201;&#22823;&#37327;&#30340;&#30740;&#31350;&#24037;&#20316;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#31934;&#24515;&#36873;&#25321;&#30340;&#25163;&#24037;&#21046;&#20316;&#30340;&#28040;&#24687;&#20256;&#36882;&#25805;&#20316;&#31526;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22270;&#19978;&#23398;&#20064;&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#23427;&#23436;&#20840;&#20381;&#36182;&#20110;&#27880;&#24847;&#21147;&#12290;&#22270;&#34987;&#34920;&#31034;&#20026;&#33410;&#28857;&#25110;&#36793;&#38598;&#65292;&#24182;&#36890;&#36807;&#25513;&#30721;&#27880;&#24847;&#26435;&#37325;&#30697;&#38453;&#26469;&#24378;&#21046;&#23427;&#20204;&#30340;&#36830;&#25509;&#65292;&#26377;&#25928;&#22320;&#20026;&#27599;&#20010;&#22270;&#21019;&#24314;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#23613;&#31649;&#20854;&#31616;&#21333;&#24615;&#65292;&#29992;&#20110;&#22270;&#30340;&#25513;&#30721;&#27880;&#24847;&#21147;&#65288;MAG&#65289;&#22312;&#38271;&#36317;&#31163;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;55&#22810;&#20010;&#33410;&#28857;&#21644;&#22270;&#32423;&#20219;&#21153;&#19978;&#20248;&#20110;&#24378;&#28040;&#24687;&#20256;&#36882;&#22522;&#32447;&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10793v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) and variations of the message passing algorithm are the predominant means for learning on graphs, largely due to their flexibility, speed, and satisfactory performance. The design of powerful and general purpose GNNs, however, requires significant research efforts and often relies on handcrafted, carefully-chosen message passing operators. Motivated by this, we propose a remarkably simple alternative for learning on graphs that relies exclusively on attention. Graphs are represented as node or edge sets and their connectivity is enforced by masking the attention weight matrix, effectively creating custom attention patterns for each graph. Despite its simplicity, masked attention for graphs (MAG) has state-of-the-art performance on long-range tasks and outperforms strong message passing baselines and much more involved attention-based methods on over 55 node and graph-level tasks. We also show significantly 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08918</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#30417;&#30563;&#22312;&#22270;&#19978;&#23398;&#20064;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21152;&#36895;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Graph Inference Acceleration by Learning MLPs on Graphs without Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08918
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#36890;&#36807;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#25552;&#39640;&#20102;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#32463;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#23545;&#28040;&#24687;&#20256;&#36882;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#65292;&#27604;&#22914;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20174;GNNs&#20013;&#25552;&#21462;&#30693;&#35782;&#21040;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLPs&#65289;&#26469;&#21152;&#36895;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#30340;&#26377;&#30417;&#30563;&#33976;&#39311;&#38480;&#21046;&#20102;&#23545;&#26410;&#35265;&#33410;&#28857;&#30340;&#27867;&#21270;&#65292;&#32780;&#22312;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#20013;&#36825;&#31181;&#24773;&#20917;&#24456;&#24120;&#35265;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550;SimMLP&#65292;&#29992;&#20110;&#22312;&#22270;&#19978;&#26080;&#30417;&#30563;&#23398;&#20064;MLPs&#65292;&#20197;&#22686;&#24378;&#27867;&#21270;&#33021;&#21147;&#12290;SimMLP&#21033;&#29992;&#33258;&#30417;&#30563;&#23545;&#40784;GNNs&#21644;MLPs&#20043;&#38388;&#30340;&#33410;&#28857;&#29305;&#24449;&#21644;&#22270;&#32467;&#26500;&#20043;&#38388;&#30340;&#31934;&#32454;&#21644;&#27867;&#21270;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#20943;&#36731;&#24179;&#20961;&#35299;&#30340;&#39118;&#38505;&#12290;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08918v1 Announce Type: cross Abstract: Graph Neural Networks (GNNs) have demonstrated effectiveness in various graph learning tasks, yet their reliance on message-passing constraints their deployment in latency-sensitive applications such as financial fraud detection. Recent works have explored distilling knowledge from GNNs to Multi-Layer Perceptrons (MLPs) to accelerate inference. However, this task-specific supervised distillation limits generalization to unseen nodes, which are prevalent in latency-sensitive applications. To this end, we present \textbf{\textsc{SimMLP}}, a \textbf{\textsc{Sim}}ple yet effective framework for learning \textbf{\textsc{MLP}}s on graphs without supervision, to enhance generalization. \textsc{SimMLP} employs self-supervised alignment between GNNs and MLPs to capture the fine-grained and generalizable correlation between node features and graph structures, and proposes two strategies to alleviate the risk of trivial solutions. Theoretically, w
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Voronoi&#20505;&#36873;&#28857;&#36793;&#30028;&#21487;&#20197;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26377;&#25928;&#22320;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#22810;&#36215;&#22987;&#36830;&#32493;&#25628;&#32034;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;</title><link>https://arxiv.org/abs/2402.04922</link><description>&lt;p&gt;
Voronoi Candidates&#29992;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Voronoi Candidates for Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04922
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Voronoi&#20505;&#36873;&#28857;&#36793;&#30028;&#21487;&#20197;&#22312;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#26377;&#25928;&#22320;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#22810;&#36215;&#22987;&#36830;&#32493;&#25628;&#32034;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#20026;&#39640;&#25928;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#25552;&#20379;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#37319;&#38598;&#20934;&#21017;&#38656;&#35201;&#36827;&#34892;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20869;&#37096;&#20248;&#21270;&#65292;&#36825;&#21487;&#33021;&#24341;&#36215;&#24456;&#22823;&#30340;&#24320;&#38144;&#12290;&#35768;&#22810;&#23454;&#38469;&#30340;BO&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#19981;&#37319;&#29992;&#23545;&#37319;&#38598;&#20989;&#25968;&#36827;&#34892;&#24418;&#24335;&#21270;&#36830;&#32493;&#20248;&#21270;&#65292;&#32780;&#26159;&#22312;&#26377;&#38480;&#30340;&#31354;&#38388;&#22635;&#20805;&#20505;&#36873;&#38598;&#19978;&#36827;&#34892;&#31163;&#25955;&#25628;&#32034;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#35758;&#20351;&#29992;&#20505;&#36873;&#28857;&#65292;&#20854;&#20301;&#20110;&#24403;&#21069;&#35774;&#35745;&#28857;&#30340;Voronoi&#38262;&#23884;&#36793;&#30028;&#19978;&#65292;&#22240;&#27492;&#23427;&#20204;&#19982;&#20004;&#20010;&#25110;&#22810;&#20010;&#35774;&#35745;&#28857;&#31561;&#36317;&#31163;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36890;&#36807;&#30452;&#25509;&#37319;&#26679;Voronoi&#36793;&#30028;&#32780;&#19981;&#26126;&#30830;&#29983;&#25104;&#38262;&#23884;&#30340;&#31574;&#30053;&#65292;&#20174;&#32780;&#36866;&#24212;&#39640;&#32500;&#24230;&#20013;&#30340;&#22823;&#35774;&#35745;&#12290;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#36807;&#31243;&#21644;&#26399;&#26395;&#25913;&#36827;&#26469;&#23545;&#19968;&#32452;&#27979;&#35797;&#38382;&#39064;&#36827;&#34892;&#20248;&#21270;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#22810;&#36215;&#22987;&#36830;&#32493;&#25628;&#32034;&#30340;&#25191;&#34892;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) offers an elegant approach for efficiently optimizing black-box functions. However, acquisition criteria demand their own challenging inner-optimization, which can induce significant overhead. Many practical BO methods, particularly in high dimension, eschew a formal, continuous optimization of the acquisition function and instead search discretely over a finite set of space-filling candidates. Here, we propose to use candidates which lie on the boundary of the Voronoi tessellation of the current design points, so they are equidistant to two or more of them. We discuss strategies for efficient implementation by directly sampling the Voronoi boundary without explicitly generating the tessellation, thus accommodating large designs in high dimension. On a battery of test problems optimized via Gaussian processes with expected improvement, our proposed approach significantly improves the execution time of a multi-start continuous search without a loss in accuracy
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2312.01201</link><description>&lt;p&gt;
PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
PAC Privacy Preserving Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01201
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;&#37319;&#26679;&#36807;&#31243;&#20013;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#24182;&#21457;&#23637;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#65292;&#22312;&#20445;&#25252;&#24615;&#33021;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#27491;&#22312;&#24341;&#36215;&#30740;&#31350;&#20154;&#21592;&#30340;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#20005;&#26684;&#30340;&#24046;&#20998;&#38544;&#31169;&#65292;&#26377;&#21487;&#33021;&#29983;&#25104;&#26082;&#20855;&#26377;&#39640;&#38544;&#31169;&#24615;&#21448;&#20855;&#26377;&#33391;&#22909;&#35270;&#35273;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#25361;&#25112;&#22312;&#20110;&#30830;&#20445;&#22312;&#31169;&#26377;&#21270;&#29305;&#23450;&#25968;&#25454;&#23646;&#24615;&#26102;&#30340;&#24378;&#22823;&#20445;&#25252;&#65292;&#24403;&#21069;&#27169;&#22411;&#22312;&#36825;&#20123;&#26041;&#38754;&#32463;&#24120;&#23384;&#22312;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;PAC&#38544;&#31169;&#20445;&#25252;&#25193;&#25955;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#25193;&#25955;&#21407;&#29702;&#24182;&#30830;&#20445;&#8220;&#21487;&#33021;&#22823;&#33268;&#27491;&#30830;&#65288;PAC&#65289;&#8221;&#38544;&#31169;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31169;&#26377;&#20998;&#31867;&#22120;&#25351;&#23548;&#38598;&#25104;&#21040;Langevin&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#12290;&#27492;&#22806;&#65292;&#35748;&#35782;&#21040;&#22312;&#34913;&#37327;&#27169;&#22411;&#38544;&#31169;&#24615;&#26041;&#38754;&#23384;&#22312;&#24046;&#36317;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#26469;&#34913;&#37327;&#38544;&#31169;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#36890;&#36807;&#36825;&#20010;&#26032;&#24230;&#37327;&#26631;&#20934;&#35780;&#20272;&#65292;&#24182;&#36890;&#36807;&#39640;&#26031;&#30697;&#38453;&#35745;&#31639;&#25903;&#25345;PAC&#30028;&#38480;&#65292;&#34920;&#29616;&#20986;&#26356;&#20248;&#24322;&#30340;&#38544;&#31169;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01201v2 Announce Type: replace-cross  Abstract: Data privacy protection is garnering increased attention among researchers. Diffusion models (DMs), particularly with strict differential privacy, can potentially produce images with both high privacy and visual quality. However, challenges arise such as in ensuring robust protection in privatizing specific data attributes, areas where current models often fall short. To address these challenges, we introduce the PAC Privacy Preserving Diffusion Model, a model leverages diffusion principles and ensure Probably Approximately Correct (PAC) privacy. We enhance privacy protection by integrating a private classifier guidance into the Langevin Sampling Process. Additionally, recognizing the gap in measuring the privacy of models, we have developed a novel metric to gauge privacy levels. Our model, assessed with this new metric and supported by Gaussian matrix computations for the PAC bound, has shown superior performance in privacy p
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2310.12000</link><description>&lt;p&gt;
Vecchia-Laplace&#36817;&#20284;&#27861;&#22312;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;&#36845;&#20195;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models. (arXiv:2310.12000v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12000
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#20171;&#32461;&#20102;&#29992;&#20110;&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#27169;&#22411;&#20013;&#30340;Vecchia-Laplace&#36817;&#20284;&#27861;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#30340;Cholesky&#20998;&#35299;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#27169;&#22411;&#26159;&#28789;&#27963;&#30340;&#27010;&#29575;&#38750;&#21442;&#25968;&#20989;&#25968;&#27169;&#22411;&#12290;Vecchia&#36817;&#20284;&#26159;&#29992;&#20110;&#20811;&#26381;&#22823;&#25968;&#25454;&#35745;&#31639;&#29942;&#39048;&#30340;&#20934;&#30830;&#36817;&#20284;&#26041;&#27861;&#65292;Laplace&#36817;&#20284;&#26159;&#19968;&#31181;&#24555;&#36895;&#26041;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#38750;&#39640;&#26031;&#20284;&#28982;&#20989;&#25968;&#30340;&#36793;&#32536;&#20284;&#28982;&#21644;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#65292;&#24182;&#20855;&#26377;&#28176;&#36817;&#25910;&#25947;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#30452;&#25509;&#27714;&#35299;&#26041;&#27861;&#65288;&#22914;Cholesky&#20998;&#35299;&#65289;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;Vecchia-Laplace&#36817;&#20284;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#22686;&#38271;&#36229;&#32447;&#24615;&#22320;&#38543;&#26679;&#26412;&#22823;&#23567;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#19982;Vecchia-Laplace&#36817;&#20284;&#35745;&#31639;&#30456;&#20851;&#30340;&#36816;&#31639;&#22312;&#36890;&#24120;&#24773;&#20917;&#19979;&#26159;&#26368;&#20934;&#30830;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#20250;&#21464;&#24471;&#38750;&#24120;&#32531;&#24930;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;Vecchia-Laplace&#36817;&#20284;&#25512;&#26029;&#30340;&#36845;&#20195;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#22522;&#20110;Cholesky&#30340;&#35745;&#31639;&#65292;&#21487;&#20197;&#22823;&#22823;&#21152;&#24555;&#35745;&#31639;&#36895;&#24230;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present several iterative methods for inference with Vecchia-Laplace approximations which make computations considerably faster compared to Cholesky-based calculations. We analyze our propo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#26500;&#24314;&#19968;&#20010;&#25554;&#20540;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#26080;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.00327</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#35760;&#24518;&#21270;&#65306;&#36229;&#36234;&#26368;&#22351;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Memorization with neural nets: going beyond the worst case. (arXiv:2310.00327v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#25554;&#20540;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#22312;&#32473;&#23450;&#30340;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#26500;&#24314;&#19968;&#20010;&#25554;&#20540;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#32467;&#26524;&#19982;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#26080;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#33021;&#22815;&#36731;&#26494;&#22320;&#25554;&#20540;&#20854;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#35768;&#22810;&#30740;&#31350;&#37117;&#26088;&#22312;&#37327;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#35760;&#24518;&#33021;&#21147;&#65306;&#21363;&#22312;&#20219;&#24847;&#25918;&#32622;&#36825;&#20123;&#28857;&#24182;&#20219;&#24847;&#20998;&#37197;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;&#26550;&#26500;&#33021;&#22815;&#25554;&#20540;&#30340;&#26368;&#22823;&#28857;&#25968;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#23454;&#38469;&#25968;&#25454;&#65292;&#20154;&#20204;&#30452;&#35273;&#22320;&#26399;&#26395;&#23384;&#22312;&#19968;&#31181;&#33391;&#24615;&#32467;&#26500;&#65292;&#20351;&#24471;&#25554;&#20540;&#22312;&#27604;&#35760;&#24518;&#33021;&#21147;&#24314;&#35758;&#30340;&#36739;&#23567;&#32593;&#32476;&#23610;&#23544;&#19978;&#24050;&#32463;&#21457;&#29983;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#29992;&#23454;&#20363;&#29305;&#23450;&#30340;&#35266;&#28857;&#26469;&#30740;&#31350;&#25554;&#20540;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#38543;&#26426;&#31639;&#27861;&#65292;&#23427;&#21487;&#20197;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#20869;&#32473;&#23450;&#19968;&#20010;&#22266;&#23450;&#30340;&#26377;&#38480;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#31867;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#24456;&#39640;&#30340;&#27010;&#29575;&#26500;&#24314;&#20986;&#19968;&#20010;&#25554;&#20540;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#12290;&#25152;&#38656;&#30340;&#21442;&#25968;&#25968;&#37327;&#19982;&#36825;&#20004;&#20010;&#31867;&#30340;&#20960;&#20309;&#29305;&#24615;&#21450;&#20854;&#30456;&#20114;&#25490;&#21015;&#26377;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#35757;&#32451;&#25968;&#25454;&#35268;&#27169;&#26080;&#20851;&#30340;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In practice, deep neural networks are often able to easily interpolate their training data. To understand this phenomenon, many works have aimed to quantify the memorization capacity of a neural network architecture: the largest number of points such that the architecture can interpolate any placement of these points with any assignment of labels. For real-world data, however, one intuitively expects the presence of a benign structure so that interpolation already occurs at a smaller network size than suggested by memorization capacity. In this paper, we investigate interpolation by adopting an instance-specific viewpoint. We introduce a simple randomized algorithm that, given a fixed finite dataset with two classes, with high probability constructs an interpolating three-layer neural network in polynomial time. The required number of parameters is linked to geometric properties of the two classes and their mutual arrangement. As a result, we obtain guarantees that are independent of t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNNs&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#36890;&#36807;&#38544;&#34255;&#29305;&#24449;&#26144;&#23556;&#23884;&#20837;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#37327;&#23376;&#20020;&#30028;&#24615;&#29983;&#25104;&#36866;&#21512;&#30340;&#22522;&#20989;&#25968;&#38598;&#65292;&#27744;&#21270;&#23618;&#36873;&#25321;&#33021;&#22815;&#24418;&#25104;&#39640;&#24615;&#33021;&#20915;&#31574;&#36793;&#30028;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20381;&#36182;&#20110;&#23884;&#20837;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2308.16664</link><description>&lt;p&gt;
&#25105;&#20204;&#21487;&#20197;&#20174;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#21040;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What can we learn from quantum convolutional neural networks?. (arXiv:2308.16664v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16664
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNNs&#65289;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#36890;&#36807;&#38544;&#34255;&#29305;&#24449;&#26144;&#23556;&#23884;&#20837;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#65292;&#24182;&#19988;&#21033;&#29992;&#37327;&#23376;&#20020;&#30028;&#24615;&#29983;&#25104;&#36866;&#21512;&#30340;&#22522;&#20989;&#25968;&#38598;&#65292;&#27744;&#21270;&#23618;&#36873;&#25321;&#33021;&#22815;&#24418;&#25104;&#39640;&#24615;&#33021;&#20915;&#31574;&#36793;&#30028;&#30340;&#22522;&#20989;&#25968;&#65292;&#32780;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#20381;&#36182;&#20110;&#23884;&#20837;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#37327;&#23376;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;QCNNs&#65289;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#20986;&#20197;&#19979;&#32467;&#35770;&#65306;1&#65289;&#36890;&#36807;&#38544;&#34255;&#29305;&#24449;&#26144;&#23556;&#65292;&#24037;&#20316;&#20110;&#37327;&#23376;&#25968;&#25454;&#21487;&#20197;&#34987;&#35270;&#20026;&#23884;&#20837;&#29289;&#29702;&#31995;&#32479;&#21442;&#25968;&#65307;2&#65289;&#23545;&#20110;&#37327;&#23376;&#30456;&#20301;&#35782;&#21035;&#65292;&#20854;&#39640;&#24615;&#33021;&#21487;&#20197;&#24402;&#22240;&#20110;&#22312;&#22522;&#24577;&#23884;&#20837;&#26399;&#38388;&#29983;&#25104;&#38750;&#24120;&#36866;&#21512;&#30340;&#22522;&#20989;&#25968;&#38598;&#65292;&#20854;&#20013;&#33258;&#26059;&#27169;&#22411;&#30340;&#37327;&#23376;&#20020;&#30028;&#24615;&#23548;&#33268;&#20855;&#26377;&#24555;&#36895;&#21464;&#21270;&#29305;&#24449;&#30340;&#22522;&#20989;&#25968;&#65307;3&#65289;QCNN&#30340;&#27744;&#21270;&#23618;&#36127;&#36131;&#36873;&#25321;&#37027;&#20123;&#33021;&#22815;&#26377;&#21161;&#20110;&#24418;&#25104;&#39640;&#24615;&#33021;&#20915;&#31574;&#36793;&#30028;&#30340;&#22522;&#20989;&#25968;&#65292;&#23398;&#20064;&#36807;&#31243;&#23545;&#24212;&#20110;&#36866;&#24212;&#24615;&#27979;&#37327;&#65292;&#20351;&#24471;&#23569;&#37327;&#37327;&#23376;&#27604;&#29305;&#31639;&#31526;&#26144;&#23556;&#21040;&#25972;&#20010;&#23492;&#23384;&#22120;&#21487;&#35266;&#27979;&#37327;&#65307;4&#65289;QCNN&#27169;&#22411;&#30340;&#27867;&#21270;&#24378;&#28872;&#20381;&#36182;&#20110;&#23884;&#20837;&#31867;&#22411;&#65292;&#22522;&#20110;&#20613;&#37324;&#21494;&#22522;&#30340;&#26059;&#36716;&#29305;&#24449;&#26144;&#23556;&#38656;&#35201;&#20180;&#32454;&#30340;&#29305;&#24449;&#24037;&#31243;&#65307;5&#65289;&#22522;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#27979;&#37327;&#27425;&#25968;&#30340;&#35835;&#20986;&#30340;QCNN&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#20542;&#21521;&#20110;&#22320;&#38754;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
We can learn from analyzing quantum convolutional neural networks (QCNNs) that: 1) working with quantum data can be perceived as embedding physical system parameters through a hidden feature map; 2) their high performance for quantum phase recognition can be attributed to generation of a very suitable basis set during the ground state embedding, where quantum criticality of spin models leads to basis functions with rapidly changing features; 3) pooling layers of QCNNs are responsible for picking those basis functions that can contribute to forming a high-performing decision boundary, and the learning process corresponds to adapting the measurement such that few-qubit operators are mapped to full-register observables; 4) generalization of QCNN models strongly depends on the embedding type, and that rotation-based feature maps with the Fourier basis require careful feature engineering; 5) accuracy and generalization of QCNNs with readout based on a limited number of shots favor the groun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#30340;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#20102;FedRDN&#26041;&#27861;&#65292;&#22312;&#36755;&#20837;&#23618;&#32423;&#19978;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#65292;&#23558;&#25972;&#20010;&#32852;&#37030;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#27880;&#20837;&#21040;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20013;&#65292;&#20197;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09363</link><description>&lt;p&gt;
&#19968;&#31181;&#31616;&#21333;&#30340;&#38754;&#21521;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#32852;&#37030;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple Data Augmentation for Feature Distribution Skewed Federated Learning. (arXiv:2306.09363v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#30340;&#32852;&#37030;&#23398;&#20064;&#25552;&#20986;&#20102;FedRDN&#26041;&#27861;&#65292;&#22312;&#36755;&#20837;&#23618;&#32423;&#19978;&#23454;&#29616;&#20102;&#25968;&#25454;&#22686;&#24378;&#65292;&#23558;&#25972;&#20010;&#32852;&#37030;&#25968;&#25454;&#38598;&#30340;&#32479;&#35745;&#20449;&#24687;&#27880;&#20837;&#21040;&#26412;&#22320;&#23458;&#25143;&#31471;&#25968;&#25454;&#20013;&#65292;&#20197;&#32531;&#35299;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#21327;&#20316;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#24322;&#26500;&#24615;&#65288;&#21363;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#65289;&#65292;&#23427;&#30340;&#24615;&#33021;&#24517;&#28982;&#21463;&#21040;&#24433;&#21709;&#12290;&#26412;&#25991;&#38024;&#23545;&#29305;&#24449;&#20998;&#24067;&#20559;&#26012;&#30340;FL&#22330;&#26223;&#23637;&#24320;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#30001;&#26412;&#22320;&#25968;&#25454;&#38598;&#20043;&#38388;&#28508;&#22312;&#20998;&#24067;&#19981;&#21516;&#23548;&#33268;&#30340;&#29305;&#24449;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) facilitates collaborative learning among multiple clients in a distributed manner, while ensuring privacy protection. However, its performance is inevitably degraded as suffering data heterogeneity, i.e., non-IID data. In this paper, we focus on the feature distribution skewed FL scenario, which is widespread in real-world applications. The main challenge lies in the feature shift caused by the different underlying distributions of local datasets. While the previous attempts achieved progress, few studies pay attention to the data itself, the root of this issue. Therefore, the primary goal of this paper is to develop a general data augmentation technique at the input level, to mitigate the feature shift. To achieve this goal, we propose FedRDN, a simple yet remarkably effective data augmentation method for feature distribution skewed FL, which randomly injects the statistics of the dataset from the entire federation into the client's data. By this, our method ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#24046;&#24322;&#27969;&#27169;&#22411;(SD flow)&#65292;&#23427;&#21487;&#20197;&#26368;&#20248;&#22320;&#20943;&#23569;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;Schr&#8203;&#8203;&#246;dinger&#26725;&#38382;&#39064;&#12290;&#19982;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#27809;&#26377;&#23545;&#20808;&#39564;&#20998;&#24067;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#65292;&#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.12906</link><description>&lt;p&gt;
&#35780;&#20998;&#24046;&#20540;&#27969;&#27169;&#22411;&#29992;&#20110;&#38544;&#24335;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
The Score-Difference Flow for Implicit Generative Modeling. (arXiv:2304.12906v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20998;&#24046;&#24322;&#27969;&#27169;&#22411;(SD flow)&#65292;&#23427;&#21487;&#20197;&#26368;&#20248;&#22320;&#20943;&#23569;&#20004;&#20010;&#20998;&#24067;&#20043;&#38388;&#30340;&#25955;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;Schr&#8203;&#8203;&#246;dinger&#26725;&#38382;&#39064;&#12290;&#19982;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#27809;&#26377;&#23545;&#20808;&#39564;&#20998;&#24067;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#65292;&#22312;&#19968;&#20123;&#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#29983;&#25104;&#24314;&#27169;(IGM)&#26088;&#22312;&#29983;&#25104;&#31526;&#21512;&#30446;&#26631;&#25968;&#25454;&#20998;&#24067;&#29305;&#24449;&#30340;&#21512;&#25104;&#25968;&#25454;&#26679;&#26412;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;(&#20363;&#22914;&#35780;&#20998;&#21305;&#37197;&#32593;&#32476;&#12289;&#25193;&#25955;&#27169;&#22411;)&#20174;&#36890;&#36807;&#29615;&#22659;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#25200;&#21160;&#25110;&#27969;&#23558;&#21512;&#25104;&#28304;&#25968;&#25454;&#25512;&#21521;&#30446;&#26631;&#20998;&#24067;&#30340;&#35282;&#24230;&#35299;&#20915;&#20102;IGM&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20219;&#24847;&#30446;&#26631;&#21644;&#28304;&#20998;&#24067;&#20043;&#38388;&#30340;&#35780;&#20998;&#24046;&#24322;(SD)&#20316;&#20026;&#27969;&#65292;&#23427;&#21487;&#20197;&#26368;&#20248;&#22320;&#20943;&#23569;&#23427;&#20204;&#20043;&#38388;&#30340;Kullback-Leibler&#25955;&#24230;&#65292;&#21516;&#26102;&#35299;&#20915;Schr&#8203;&#8203;&#246;dinger&#26725;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;SD&#27969;&#24212;&#29992;&#20110;&#26041;&#20415;&#30340;&#20195;&#29702;&#20998;&#24067;&#65292;&#24403;&#19988;&#20165;&#24403;&#21407;&#22987;&#20998;&#24067;&#23545;&#40784;&#26102;&#65292;&#23427;&#20204;&#26159;&#23545;&#40784;&#30340;&#12290;&#25105;&#20204;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#23637;&#31034;&#20102;&#36825;&#31181;&#20844;&#24335;&#19982;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#30340;&#24418;&#24335;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#19982;&#25193;&#25955;&#27169;&#22411;&#19981;&#21516;&#65292;SD&#27969;&#27809;&#26377;&#23545;&#20808;&#39564;&#20998;&#24067;&#26045;&#21152;&#20219;&#20309;&#38480;&#21046;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#22312;&#26080;&#38480;&#36776;&#21035;&#22120;&#33021;&#21147;&#30340;&#26497;&#38480;&#19979;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#35757;&#32451;&#21253;&#21547;SD&#27969;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SD&#27969;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit generative modeling (IGM) aims to produce samples of synthetic data matching the characteristics of a target data distribution. Recent work (e.g. score-matching networks, diffusion models) has approached the IGM problem from the perspective of pushing synthetic source data toward the target distribution via dynamical perturbations or flows in the ambient space. We introduce the score difference (SD) between arbitrary target and source distributions as a flow that optimally reduces the Kullback-Leibler divergence between them while also solving the Schr\"odinger bridge problem. We apply the SD flow to convenient proxy distributions, which are aligned if and only if the original distributions are aligned. We demonstrate the formal equivalence of this formulation to denoising diffusion models under certain conditions. However, unlike diffusion models, SD flow places no restrictions on the prior distribution. We also show that the training of generative adversarial networks includ
&lt;/p&gt;</description></item></channel></rss>