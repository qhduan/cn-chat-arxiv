<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#21644;&#36817;&#20284;&#26368;&#20248;&#30340;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#37327;&#21270;&#36807;&#31243;&#65292;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#19978;&#20855;&#26377;&#25913;&#36827;&#65292;&#21487;&#25193;&#23637;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2402.03158</link><description>&lt;p&gt;
&#26368;&#20248;&#21644;&#36817;&#20284;&#26368;&#20248;&#30340;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Optimal and Near-Optimal Adaptive Vector Quantization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#26368;&#20248;&#21644;&#36817;&#20284;&#26368;&#20248;&#30340;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;&#31639;&#27861;&#65292;&#33021;&#22815;&#20248;&#21270;&#37327;&#21270;&#36807;&#31243;&#65292;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#19978;&#20855;&#26377;&#25913;&#36827;&#65292;&#21487;&#25193;&#23637;&#24212;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#26159;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#29992;&#20363;&#20013;&#30340;&#22522;&#26412;&#20248;&#21270;&#65292;&#21253;&#25324;&#21387;&#32553;&#26799;&#24230;&#12289;&#27169;&#22411;&#26435;&#37325;&#21644;&#28608;&#27963;&#20197;&#21450;&#25968;&#25454;&#38598;&#12290;&#26368;&#20934;&#30830;&#30340;&#37327;&#21270;&#24418;&#24335;&#26159;&#8220;&#33258;&#36866;&#24212;&#8221;&#65292;&#20854;&#20013;&#36890;&#36807;&#26368;&#23567;&#21270;&#30456;&#23545;&#20110;&#32473;&#23450;&#36755;&#20837;&#30340;&#35823;&#24046;&#26469;&#20248;&#21270;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#26368;&#22351;&#24773;&#20917;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#32780;&#65292;&#26368;&#20248;&#30340;&#33258;&#36866;&#24212;&#37327;&#21270;&#26041;&#27861;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#33258;&#36866;&#24212;&#21521;&#37327;&#37327;&#21270;&#65288;AVQ&#65289;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#22312;&#28176;&#36817;&#25913;&#36827;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#19979;&#25214;&#21040;&#26368;&#20248;&#35299;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;&#65292;&#20197;&#22788;&#29702;&#22823;&#36755;&#20837;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#33021;&#20250;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#26356;&#24191;&#27867;&#22320;&#20351;&#29992;AVQ&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.   We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#26576;&#20123;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.14057</link><description>&lt;p&gt;
&#22522;&#20110;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#30340;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Weighted least-squares approximation with determinantal point processes and generalized volume sampling. (arXiv:2312.14057v2 [math.NA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14057
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#24191;&#20041;&#20307;&#31215;&#21462;&#26679;&#36827;&#34892;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#36924;&#36817;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#24615;&#20197;&#21450;&#22312;&#26576;&#20123;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;&#20013;&#30340;&#36924;&#36817;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#32473;&#23450;&#30340;m&#32500;&#31354;&#38388;V_m&#20013;&#30340;&#20803;&#32032;&#65292;&#20511;&#21161;&#20110;&#19968;&#20123;&#29305;&#24449;&#26144;&#23556;&#966;&#65292;&#36890;&#36807;&#23545;&#38543;&#26426;&#28857;x_1&#65292;...&#65292;x_n&#22788;&#30340;&#20989;&#25968;&#36827;&#34892;&#35780;&#20272;&#65292;&#26469;&#36924;&#36817;&#20989;&#25968;&#20174;L^2&#21040;&#20989;&#25968;&#12290;&#22312;&#22238;&#39038;&#19968;&#20123;&#20851;&#20110;&#20351;&#29992;&#29420;&#31435;&#21516;&#20998;&#24067;&#28857;&#30340;&#26368;&#20248;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#30340;&#32467;&#26524;&#20043;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20351;&#29992;&#25237;&#24433;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#65288;DPP&#65289;&#25110;&#20307;&#31215;&#21462;&#26679;&#30340;&#21152;&#26435;&#26368;&#23567;&#20108;&#20056;&#12290;&#36825;&#20123;&#20998;&#24067;&#22312;&#36873;&#23450;&#30340;&#29305;&#24449;&#966;(x_i)&#20013;&#24341;&#20837;&#20102;&#28857;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#65292;&#20197;&#20419;&#36827;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;&#24191;&#20041;&#29256;&#26412;&#30340;&#20307;&#31215;&#26631;&#20934;&#21270;&#21462;&#26679;&#65292;&#20351;&#29992;&#26679;&#26412;&#25968;n = O(mlog(m))&#24471;&#21040;&#20102;&#26399;&#26395;&#19978;&#30340;&#20934;&#26368;&#20248;&#32467;&#26524;&#65292;&#36825;&#24847;&#21619;&#30528;&#26399;&#26395;&#30340;L^2&#35823;&#24046;&#21463;&#21040;&#19968;&#20010;&#24120;&#25968;&#20056;&#20197;&#22312;L^2&#20013;&#30340;&#26368;&#20339;&#36924;&#36817;&#35823;&#24046;&#30340;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#36827;&#19968;&#27493;&#20551;&#35774;&#20989;&#25968;&#22312;&#26576;&#20010;&#23884;&#20837;&#22312;L^2&#20013;&#30340;&#35268;&#33539;&#21521;&#37327;&#31354;&#38388;H&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#36924;&#36817;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of approximating a function from $L^2$ by an element of a given $m$-dimensional space $V_m$, associated with some feature map $\varphi$, using evaluations of the function at random points $x_1,\dots,x_n$. After recalling some results on optimal weighted least-squares using independent and identically distributed points, we consider weighted least-squares using projection determinantal point processes (DPP) or volume sampling. These distributions introduce dependence between the points that promotes diversity in the selected features $\varphi(x_i)$. We first provide a generalized version of volume-rescaled sampling yielding quasi-optimality results in expectation with a number of samples $n = O(m\log(m))$, that means that the expected $L^2$ error is bounded by a constant times the best approximation error in $L^2$. Also, further assuming that the function is in some normed vector space $H$ continuously embedded in $L^2$, we further prove that the approximation is
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#32479;&#19968;&#20998;&#26512;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;IPM GAN&#20013;&#29983;&#25104;&#22120;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2306.01654</link><description>&lt;p&gt;
GANs&#35299;&#20915;&#20998;&#25968;&#20105;&#35758;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
GANs Settle Scores!. (arXiv:2306.01654v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01654
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#32479;&#19968;&#20998;&#26512;&#29983;&#25104;&#22120;&#30340;&#20248;&#21270;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270;&#21644;IPM GAN&#20013;&#29983;&#25104;&#22120;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#30001;&#19968;&#20010;&#29983;&#25104;&#22120;&#21644;&#19968;&#20010;&#21028;&#21035;&#22120;&#32452;&#25104;&#65292;&#29983;&#25104;&#22120;&#34987;&#35757;&#32451;&#20197;&#23398;&#20064;&#26399;&#26395;&#25968;&#25454;&#30340;&#22522;&#30784;&#20998;&#24067;&#65292;&#32780;&#21028;&#21035;&#22120;&#21017;&#34987;&#35757;&#32451;&#20197;&#21306;&#20998;&#30495;&#23454;&#26679;&#26412;&#21644;&#29983;&#25104;&#22120;&#36755;&#20986;&#30340;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21464;&#20998;&#26041;&#27861;&#26469;&#20998;&#26512;&#29983;&#25104;&#22120;&#20248;&#21270;&#12290;&#22312;f-&#25955;&#24230;&#26368;&#23567;&#21270; GAN &#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#26368;&#20248;&#29983;&#25104;&#22120;&#26159;&#36890;&#36807;&#23558;&#20854;&#36755;&#20986;&#20998;&#24067;&#30340;&#24471;&#20998;&#19982;&#25968;&#25454;&#20998;&#24067;&#30340;&#24471;&#20998;&#36827;&#34892;&#21305;&#37197;&#24471;&#21040;&#30340;&#12290;&#22312;IPM GAN&#20013;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20010;&#26368;&#20248;&#29983;&#25104;&#22120;&#21305;&#37197;&#24471;&#20998;&#22411;&#20989;&#25968;&#65292;&#21253;&#25324;&#19982;&#25152;&#36873;IPM&#32422;&#26463;&#31354;&#38388;&#30456;&#20851;&#30340;&#26680;&#27969;&#22330;&#12290;&#27492;&#22806;&#65292;IPM-GAN&#20248;&#21270;&#21487;&#20197;&#30475;&#20316;&#26159;&#24179;&#28369;&#20998;&#25968;&#21305;&#37197;&#20013;&#30340;&#19968;&#31181;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#29983;&#25104;&#22120;&#20998;&#24067;&#30340;&#24471;&#20998;&#19982;&#22312;&#26680;&#20989;&#25968;&#19978;&#36827;&#34892;&#21367;&#31215;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) comprise a generator, trained to learn the underlying distribution of the desired data, and a discriminator, trained to distinguish real samples from those output by the generator. A majority of GAN literature focuses on understanding the optimality of the discriminator through integral probability metric (IPM) or divergence based analysis. In this paper, we propose a unified approach to analyzing the generator optimization through variational approach. In $f$-divergence-minimizing GANs, we show that the optimal generator is the one that matches the score of its output distribution with that of the data distribution, while in IPM GANs, we show that this optimal generator matches score-like functions, involving the flow-field of the kernel associated with a chosen IPM constraint space. Further, the IPM-GAN optimization can be seen as one of smoothed score-matching, where the scores of the data and the generator distributions are convolved with the 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.01430</link><description>&lt;p&gt;
&#20998;&#31163;&#30340;&#20851;&#27880;&#21147;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#20998;&#31163;&#27133;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots. (arXiv:2304.01430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#22522;&#30784;&#30495;&#20540;&#25110;&#30417;&#30563;&#12290;&#23427;&#30001;&#22522;&#20110;&#27133;&#20851;&#27880;&#30340;&#23545;&#25239;&#26465;&#20214;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#32452;&#25104;&#65292;&#20462;&#25913;&#20026;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#35299;&#30721;&#20809;&#27969;&#65292;&#32780;&#19981;&#26159;&#23581;&#35797;&#37325;&#26500;&#22270;&#20687;&#26412;&#36523;&#12290;&#22312;&#32467;&#26524;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#20013;&#65292;&#19968;&#31181;&#27169;&#24335;&#65288;&#27969;&#65289;&#23558;&#39304;&#36865;&#32473;&#32534;&#30721;&#22120;&#20197;&#20135;&#29983;&#21333;&#29420;&#30340;&#28508;&#22312;&#20195;&#30721;&#65288;&#27133;&#65289;&#65292;&#32780;&#21478;&#19968;&#31181;&#27169;&#24335;&#65288;&#22270;&#20687;&#65289;&#23558;&#20915;&#23450;&#35299;&#30721;&#22120;&#20174;&#27133;&#29983;&#25104;&#31532;&#19968;&#20010;&#27169;&#24335;&#65288;&#27969;&#65289;&#12290;&#30001;&#20110;&#24815;&#24120;&#30340;&#33258;&#32534;&#30721;&#22522;&#20110;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#24182;&#19981;&#33021;&#38450;&#27490;&#25972;&#20010;&#27969;&#34987;&#32534;&#30721;&#21040;&#19968;&#20010;&#27133;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#25439;&#22833;&#20462;&#25913;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20998;&#31163;&#30340;&#23545;&#25239;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to segment the visual field into independently moving regions, trained with no ground truth or supervision. It consists of an adversarial conditional encoder-decoder architecture based on Slot Attention, modified to use the image as context to decode optical flow without attempting to reconstruct the image itself. In the resulting multi-modal representation, one modality (flow) feeds the encoder to produce separate latent codes (slots), whereas the other modality (image) conditions the decoder to generate the first (flow) from the slots. This design frees the representation from having to encode complex nuisance variability in the image due to, for instance, illumination and reflectance properties of the scene. Since customary autoencoding based on minimizing the reconstruction error does not preclude the entire flow from being encoded into a single slot, we modify the loss to an adversarial criterion based on Contextual Information Separation. The resulting min-m
&lt;/p&gt;</description></item></channel></rss>