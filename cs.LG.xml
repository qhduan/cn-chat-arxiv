<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;TESSERACT&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#20013;&#30340;&#23454;&#39564;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#20844;&#24179;&#23454;&#39564;&#35774;&#35745;&#32422;&#26463;&#21644;&#26032;&#25351;&#26631;AUT&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01359</link><description>&lt;p&gt;
TESSERACT: &#28040;&#38500;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#20013;&#30340;&#23454;&#39564;&#20559;&#24046;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#26041;&#27861;&#65288;&#25193;&#23637;&#29256;&#65289;
&lt;/p&gt;
&lt;p&gt;
TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time (Extended Version)
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01359
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;TESSERACT&#26041;&#27861;&#65292;&#28040;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#20998;&#31867;&#20013;&#30340;&#23454;&#39564;&#20559;&#24046;&#65292;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20559;&#24046;&#38382;&#39064;&#12290;&#36890;&#36807;&#20844;&#24179;&#23454;&#39564;&#35774;&#35745;&#32422;&#26463;&#21644;&#26032;&#25351;&#26631;AUT&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#21644;&#31283;&#23450;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#26816;&#27979;&#24694;&#24847;&#36719;&#20214;&#26041;&#38754;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#23613;&#31649;&#35768;&#22810;&#30740;&#31350;&#25253;&#21578;&#30340;F1&#20998;&#25968;&#39640;&#36798;0.99&#65292;&#20294;&#38382;&#39064;&#20173;&#26410;&#23436;&#20840;&#35299;&#20915;&#12290;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#22120;&#24120;&#24120;&#22312;&#25805;&#20316;&#31995;&#32479;&#21644;&#25915;&#20987;&#26041;&#27861;&#19981;&#26029;&#28436;&#21270;&#26102;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#36825;&#20250;&#23548;&#33268;&#20043;&#21069;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#23545;&#20110;&#26032;&#36755;&#20837;&#30340;&#20934;&#30830;&#20915;&#31574;&#21464;&#24471;&#19981;&#36275;&#22815;&#12290;&#26412;&#25991;&#35748;&#20026;&#24120;&#35265;&#30340;&#30740;&#31350;&#32467;&#26524;&#30001;&#20110;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#20004;&#31181;&#26222;&#36941;&#30340;&#23454;&#39564;&#20559;&#24046;&#32780;&#34987;&#22840;&#22823;&#65306;&#31354;&#38388;&#20559;&#24046;&#26159;&#30001;&#25968;&#25454;&#20998;&#24067;&#19981;&#20195;&#34920;&#30495;&#23454;&#37096;&#32626;&#30340;&#24773;&#20917;&#24341;&#36215;&#30340;&#65307;&#26102;&#38388;&#20559;&#24046;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#19981;&#27491;&#30830;&#26102;&#38388;&#20998;&#21106;&#24341;&#36215;&#30340;&#65292;&#23548;&#33268;&#20102;&#19981;&#29616;&#23454;&#30340;&#37197;&#32622;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20559;&#24046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#32452;&#20844;&#24179;&#23454;&#39564;&#35774;&#35745;&#30340;&#32422;&#26463;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#35780;&#20272;&#20998;&#31867;&#22120;&#31283;&#23450;&#24615;&#30340;&#26032;&#25351;&#26631;AUT&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35843;&#25972;&#35757;&#32451;&#25968;&#25454;&#20197;&#25552;&#39640;&#20998;&#31867;&#22120;&#40065;&#26834;&#24615;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) plays a pivotal role in detecting malicious software. Despite the high F1-scores reported in numerous studies reaching upwards of 0.99, the issue is not completely solved. Malware detectors often experience performance decay due to constantly evolving operating systems and attack methods, which can render previously learned knowledge insufficient for accurate decision-making on new inputs. This paper argues that commonly reported results are inflated due to two pervasive sources of experimental bias in the detection task: spatial bias caused by data distributions that are not representative of a real-world deployment; and temporal bias caused by incorrect time splits of data, leading to unrealistic configurations. To address these biases, we introduce a set of constraints for fair experiment design, and propose a new metric, AUT, for classifier robustness in real-world settings. We additionally propose an algorithm designed to tune training data to enhance classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#26469;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;</title><link>https://arxiv.org/abs/2403.16986</link><description>&lt;p&gt;
&#38754;&#21521;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#30340;&#21160;&#24577;&#30456;&#23545;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Dynamic Relative Representations for Goal-Oriented Semantic Communications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#26469;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#24182;&#23454;&#29616;&#20102;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26410;&#26469;&#30340;6G&#26080;&#32447;&#32593;&#32476;&#20013;&#65292;&#36890;&#20449;&#30340;&#35821;&#20041;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#23558;&#21457;&#25381;&#22522;&#30784;&#20316;&#29992;&#65292;&#23558;&#21547;&#20041;&#21644;&#30456;&#20851;&#24615;&#32435;&#20837;&#20256;&#36755;&#12290;&#28982;&#32780;&#65292;&#24403;&#35774;&#22791;&#20351;&#29992;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#36923;&#36753;&#25110;&#20869;&#37096;&#34920;&#31034;&#26102;&#65292;&#20250;&#20986;&#29616;&#38556;&#30861;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#21305;&#37197;&#65292;&#21487;&#33021;&#21361;&#21450;&#29702;&#35299;&#12290;&#22312;&#28508;&#22312;&#31354;&#38388;&#36890;&#20449;&#20013;&#65292;&#36825;&#19968;&#25361;&#25112;&#34920;&#29616;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25968;&#25454;&#36827;&#34892;&#32534;&#30721;&#26102;&#39640;&#32500;&#34920;&#31034;&#19981;&#21305;&#37197;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38754;&#21521;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#26694;&#26550;&#65292;&#21033;&#29992;&#30456;&#23545;&#34920;&#31034;&#26469;&#36890;&#36807;&#28508;&#22312;&#31354;&#38388;&#23545;&#40784;&#32531;&#35299;&#35821;&#20041;&#19981;&#21305;&#37197;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20248;&#21270;&#31574;&#30053;&#65292;&#20197;&#35843;&#25972;&#30456;&#23545;&#34920;&#31034;&#12289;&#36890;&#20449;&#21442;&#25968;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#23454;&#29616;&#33021;&#25928;&#39640;&#12289;&#24310;&#36831;&#20302;&#30340;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#12290;&#25968;&#20540;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#32531;&#35299;&#20013;&#36215;&#20316;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16986v1 Announce Type: cross  Abstract: In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions. However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding. In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data. This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment. We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications. Numerical results demonstrate our methodology's effectiveness in mitigating
&lt;/p&gt;</description></item><item><title>&#36951;&#20256;&#35268;&#21010;&#26041;&#27861;&#25552;&#20986;&#20102;&#29992;&#20110;&#21487;&#35299;&#37322;&#27969;&#24418;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#24403;&#21069;&#27969;&#24418;&#23398;&#20064;&#20013;&#21151;&#33021;&#26144;&#23556;&#19981;&#26126;&#30830;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14139</link><description>&lt;p&gt;
&#29992;&#20110;&#21487;&#35299;&#37322;&#27969;&#24418;&#23398;&#20064;&#30340;&#36951;&#20256;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Genetic Programming for Explainable Manifold Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14139
&lt;/p&gt;
&lt;p&gt;
&#36951;&#20256;&#35268;&#21010;&#26041;&#27861;&#25552;&#20986;&#20102;&#29992;&#20110;&#21487;&#35299;&#37322;&#27969;&#24418;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#24110;&#21161;&#35299;&#20915;&#24403;&#21069;&#27969;&#24418;&#23398;&#20064;&#20013;&#21151;&#33021;&#26144;&#23556;&#19981;&#26126;&#30830;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#36890;&#36807;&#25581;&#31034;&#39640;&#32500;&#25968;&#25454;&#20013;&#30340;&#20302;&#32500;&#23884;&#20837;&#65292;&#20174;&#32780;&#23558;&#25968;&#25454;&#36716;&#25442;&#20026;&#26356;&#20302;&#32500;&#30340;&#34920;&#31034;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#20998;&#26512;&#30340;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#30340;&#19968;&#20010;&#26174;&#33879;&#25361;&#25112;&#26159;&#23427;&#20204;&#32570;&#20047;&#26126;&#30830;&#30340;&#21151;&#33021;&#26144;&#23556;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#36951;&#20256;&#35268;&#21010;&#20197;&#20854;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;&#21151;&#33021;&#26641;&#30340;&#27169;&#22411;&#32780;&#38395;&#21517;&#65292;&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21033;&#29992;&#22810;&#30446;&#26631;&#36951;&#20256;&#35268;&#21010;&#26469;&#24179;&#34913;&#27969;&#24418;&#36136;&#37327;&#19982;&#23884;&#20837;&#32500;&#24230;&#65292;&#20135;&#29983;&#20102;&#19968;&#31995;&#21015;&#23884;&#20837;&#22823;&#23567;&#19979;&#30340;&#21151;&#33021;&#26144;&#23556;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26144;&#23556;&#26641;&#32463;&#24120;&#21464;&#24471;&#22797;&#26434;&#65292;&#38459;&#30861;&#20102;&#35299;&#37322;&#24615;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#21487;&#35299;&#37322;&#27969;&#24418;&#23398;&#20064;&#30340;&#36951;&#20256;&#35268;&#21010;&#65288;GP-EMaL&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14139v1 Announce Type: cross  Abstract: Manifold learning techniques play a pivotal role in machine learning by revealing lower-dimensional embeddings within high-dimensional data, thus enhancing both the efficiency and interpretability of data analysis by transforming the data into a lower-dimensional representation. However, a notable challenge with current manifold learning methods is their lack of explicit functional mappings, crucial for explainability in many real-world applications. Genetic programming, known for its interpretable functional tree-based models, has emerged as a promising approach to address this challenge. Previous research leveraged multi-objective GP to balance manifold quality against embedding dimensionality, producing functional mappings across a range of embedding sizes. Yet, these mapping trees often became complex, hindering explainability. In response, in this paper, we introduce Genetic Programming for Explainable Manifold Learning (GP-EMaL),
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#20844;&#24320;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#24182;&#20197;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.12072</link><description>&lt;p&gt;
Floralens&#65306;&#19968;&#31181;&#29992;&#20110;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Floralens: a Deep Learning Model for the Portuguese Native Flora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12072
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#20110;&#20174;&#20844;&#24320;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#25968;&#25454;&#38598;&#20197;&#21450;&#21033;&#29992;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25512;&#23548;&#27169;&#22411;&#30340;&#31616;&#21270;&#26041;&#27861;&#65292;&#24182;&#20197;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#35768;&#22810;&#20844;&#27665;&#31185;&#23398;&#24179;&#21488;&#20013;&#23545;&#29983;&#29289;&#29289;&#31181;&#36827;&#34892;&#22522;&#20110;&#22270;&#20687;&#30340;&#35782;&#21035;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36275;&#22815;&#22823;&#23567;&#21644;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#32593;&#32476;&#20197;&#21450;&#32593;&#32476;&#26550;&#26500;&#30340;&#36873;&#25321;&#26412;&#36523;&#20173;&#28982;&#24456;&#23569;&#26377;&#25991;&#29486;&#35760;&#24405;&#65292;&#22240;&#27492;&#19981;&#23481;&#26131;&#34987;&#22797;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#20844;&#24320;&#21487;&#29992;&#30340;&#30740;&#31350;&#32423;&#25968;&#25454;&#38598;&#26500;&#24314;&#29983;&#29289;&#20998;&#31867;&#32676;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#38598;&#20351;&#29992;&#35895;&#27468;&#30340;AutoML Vision&#20113;&#26381;&#21153;&#25552;&#20379;&#30340;&#29616;&#25104;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#25512;&#23548;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#26159;&#33889;&#33796;&#29273;&#26412;&#22320;&#26893;&#29289;&#65292;&#22522;&#20110;&#30001;&#33889;&#33796;&#29273;&#26893;&#29289;&#23398;&#20250;&#25552;&#20379;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#28155;&#21152;&#26469;&#33258;iNaturalist&#12289;Pl@ntNet&#21644;Observation.org&#30340;&#37319;&#38598;&#25968;&#25454;&#36827;&#34892;&#25193;&#23637;&#12290;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#35880;&#24910;&#22320;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12072v1 Announce Type: cross  Abstract: Machine-learning techniques, namely deep convolutional neural networks, are pivotal for image-based identification of biological species in many Citizen Science platforms. However, the construction of critically sized and sampled datasets to train the networks and the choice of the network architectures itself remains little documented and, therefore, does not lend itself to be easily replicated. In this paper, we develop a streamlined methodology for building datasets for biological taxa from publicly available research-grade datasets and for deriving models from these datasets using off-the-shelf deep convolutional neural networks such as those provided by Google's AutoML Vision cloud service. Our case study is the Portuguese native flora, anchored in a high-quality dataset, provided by the Sociedade Portuguesa de Bot\^anica, scaled up by adding sampled data from iNaturalist, Pl@ntNet, and Observation.org. We find that with a careful
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#28304;&#20195;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#65292;&#32780;&#20998;&#35789;&#22120;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#26159;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#34920;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.10646</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#28304;&#20195;&#30721;&#34920;&#31034;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#29992;&#20110;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#28304;&#20195;&#30721;&#34920;&#31034;&#26041;&#27861;&#65292;&#21457;&#29616;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#65292;&#32780;&#20998;&#35789;&#22120;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#26159;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10646v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#29992;&#20110;&#32593;&#32476;&#23433;&#20840;&#30456;&#20851;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28304;&#20195;&#30721;&#30340;&#34920;&#31034;&#26159;&#35813;&#25216;&#26415;&#30340;&#20851;&#38190;&#37096;&#20998;&#65292;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#23398;&#20064;&#28304;&#20195;&#30721;&#29305;&#24449;&#30340;&#26041;&#24335;&#12290;&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#36825;&#20123;&#25216;&#26415;&#34987;&#24320;&#21457;&#65292;&#20102;&#35299;&#35813;&#39046;&#22495;&#30340;&#24403;&#21069;&#29366;&#24577;&#23545;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#24050;&#26377;&#20869;&#23481;&#21644;&#23578;&#26410;&#23384;&#22312;&#30340;&#20869;&#23481;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#26412;&#25991;&#23545;&#36825;&#20123;&#29616;&#26377;&#30340;&#22522;&#20110;ML&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19981;&#21516;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#21644;&#32534;&#31243;&#35821;&#35328;&#25152;&#20351;&#29992;&#30340;&#34920;&#31034;&#31867;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#34920;&#31034;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#31867;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#22270;&#30340;&#34920;&#31034;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#34920;&#31034;&#31867;&#21035;&#65292;&#32780;&#20998;&#35789;&#22120;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;ASTs&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#26368;&#21463;&#27426;&#36814;&#30340;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10646v1 Announce Type: new  Abstract: Machine learning techniques for cybersecurity-related software engineering tasks are becoming increasingly popular. The representation of source code is a key portion of the technique that can impact the way the model is able to learn the features of the source code. With an increasing number of these techniques being developed, it is valuable to see the current state of the field to better understand what exists and what's not there yet. This paper presents a study of these existing ML-based approaches and demonstrates what type of representations were used for different cybersecurity tasks and programming languages. Additionally, we study what types of models are used with different representations. We have found that graph-based representations are the most popular category of representation, and Tokenizers and Abstract Syntax Trees (ASTs) are the two most popular representations overall. We also found that the most popular cybersecur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#22312;&#20851;&#31995;&#26597;&#35810;&#20013;&#35843;&#29992;LLM&#30340;&#20998;&#26512;&#22411;&#24037;&#20316;&#36127;&#36733;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21457;&#29616;&#20851;&#31995;&#26597;&#35810;&#20026;&#21152;&#36895;LLM&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.05821</link><description>&lt;p&gt;
&#22312;&#20851;&#31995;&#22411;&#24037;&#20316;&#36127;&#36733;&#20013;&#20248;&#21270;LLM&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Optimizing LLM Queries in Relational Workloads
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20248;&#21270;&#22312;&#20851;&#31995;&#26597;&#35810;&#20013;&#35843;&#29992;LLM&#30340;&#20998;&#26512;&#22411;&#24037;&#20316;&#36127;&#36733;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#21457;&#29616;&#20851;&#31995;&#26597;&#35810;&#20026;&#21152;&#36895;LLM&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05821v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20998;&#26512;&#24615;&#25968;&#25454;&#24211;&#25552;&#20379;&#21830;&#65288;&#20363;&#22914;Redshift&#12289;Databricks&#12289;BigQuery&#65289;&#24050;&#36805;&#36895;&#22686;&#21152;&#23545;&#36890;&#36807;&#26412;&#26426;&#29992;&#25143;&#33258;&#23450;&#20041;&#20989;&#25968;&#65288;UDFs&#65289;&#35843;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25903;&#25345;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#22312;&#20998;&#26512;&#22411;&#24037;&#20316;&#36127;&#36733;&#20869;&#25191;&#34892;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#65292;&#20363;&#22914;&#20998;&#31867;&#12289;&#23454;&#20307;&#25552;&#21462;&#21644;&#32763;&#35793;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20248;&#21270;&#20851;&#31995;&#26597;&#35810;&#20013;&#35843;&#29992;LLM&#30340;&#20998;&#26512;&#24037;&#20316;&#36127;&#36733;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20851;&#31995;&#26597;&#35810;&#20026;&#21152;&#36895;LLM&#25512;&#29702;&#25552;&#20379;&#20102;&#26032;&#39062;&#30340;&#26426;&#20250;&#65292;&#21253;&#25324;&#37325;&#26032;&#25490;&#24207;&#34892;&#20197;&#26368;&#22823;&#21270;LLM&#25512;&#29702;&#24341;&#25806;&#20869;&#30340;&#38190;&#20540;&#65288;KV&#65289;&#32531;&#23384;&#37325;&#29992;&#65292;&#37325;&#26032;&#25490;&#24207;&#34892;&#20869;&#30340;&#21015;&#20197;&#36827;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05821v1 Announce Type: new  Abstract: Analytical database providers (e.g., Redshift, Databricks, BigQuery) have rapidly added support for invoking Large Language Models (LLMs) through native user-defined functions (UDFs) to help users perform natural language tasks, such as classification, entity extraction, and translation, inside analytical workloads. For instance, an analyst might want to extract customer sentiments on millions of product reviews. However, LLM inference is highly expensive in both computational and economic terms: for example, an NVIDIA L4 GPU running Llama2-7B can only process 6 KB of text per second. In this paper, we explore how to optimize LLM inference for analytical workloads that invoke LLMs within relational queries. We show that relational queries present novel opportunities for accelerating LLM inference, including reordering rows to maximize key-value (KV) cache reuse within the LLM inference engine, reordering columns within a row to further i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;</title><link>https://arxiv.org/abs/2402.14664</link><description>&lt;p&gt;
&#22823;&#21160;&#20316;&#31354;&#38388;&#30340;&#36125;&#21494;&#26031;&#31163;&#31574;&#30053;&#35780;&#20272;&#19982;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bayesian Off-Policy Evaluation and Learning for Large Action Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14664
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#25429;&#25417;&#21160;&#20316;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36866;&#29992;&#20110;&#31163;&#31574;&#30053;&#35780;&#20272;&#21644;&#23398;&#20064;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;sDM&#65292;&#24182;&#24341;&#20837;&#20102;&#33021;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#34920;&#29616;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#20248;&#21183;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#24378;&#22823;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20132;&#20114;&#24335;&#31995;&#32479;&#20013;&#65292;&#21160;&#20316;&#32463;&#24120;&#26159;&#30456;&#20851;&#30340;&#65292;&#36825;&#20026;&#22823;&#21160;&#20316;&#31354;&#38388;&#20013;&#26356;&#26377;&#25928;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#21644;&#23398;&#20064;&#65288;OPL&#65289;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#21644;&#20449;&#24687;&#20016;&#23500;&#30340;&#20808;&#39564;&#26469;&#25429;&#25417;&#36825;&#20123;&#30456;&#20851;&#24615;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;sDM&#65292;&#19968;&#20010;&#20026;OPE&#21644;OPL&#35774;&#35745;&#30340;&#36890;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#26082;&#26377;&#31639;&#27861;&#22522;&#30784;&#21448;&#26377;&#29702;&#35770;&#22522;&#30784;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;sDM&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#32780;&#19981;&#20250;&#24433;&#21709;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#21463;&#22312;&#32447;&#36125;&#21494;&#26031;&#36172;&#21338;&#26426;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35780;&#20272;&#31639;&#27861;&#22312;&#22810;&#20010;&#38382;&#39064;&#23454;&#20363;&#20013;&#24179;&#22343;&#24615;&#33021;&#30340;&#36125;&#21494;&#26031;&#25351;&#26631;&#65292;&#20559;&#31163;&#20256;&#32479;&#30340;&#26368;&#22351;&#24773;&#20917;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;sDM&#22312;OPE&#21644;OPL&#20013;&#30340;&#34920;&#29616;&#65292;&#20984;&#26174;&#20102;&#21033;&#29992;&#21160;&#20316;&#30456;&#20851;&#24615;&#30340;&#22909;&#22788;&#12290;&#23454;&#35777;&#35777;&#25454;&#23637;&#31034;&#20102;sDM&#30340;&#24378;&#22823;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14664v1 Announce Type: cross  Abstract: In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach designed for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#24341;&#23548;&#27169;&#22411;&#21305;&#37197;&#65288;IMM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#23436;&#25972;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21463;&#38480;&#27169;&#22411;&#23545;&#40784;&#65292;&#23558;&#21463;&#38480;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23436;&#25972;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.12513</link><description>&lt;p&gt;
&#24341;&#23548;&#27169;&#22411;&#21305;&#37197;&#65306;&#21463;&#38480;&#27169;&#22411;&#22914;&#20309;&#24110;&#21161;&#26356;&#22823;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Induced Model Matching: How Restricted Models Can Help Larger Ones
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12513
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#24341;&#23548;&#27169;&#22411;&#21305;&#37197;&#65288;IMM&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#23436;&#25972;&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#21463;&#38480;&#27169;&#22411;&#23545;&#40784;&#65292;&#23558;&#21463;&#38480;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23436;&#25972;&#27169;&#22411;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#35757;&#32451;&#26356;&#22823;&#12289;&#20855;&#26377;&#23436;&#25972;&#29305;&#24449;&#30340;&#27169;&#22411;&#26102;&#65292;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#21463;&#38480;&#29305;&#24449;&#30340;&#38750;&#24120;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;&#36825;&#20010;&#21463;&#38480;&#27169;&#22411;&#21487;&#20197;&#34987;&#35270;&#20026;&#8220;&#36741;&#21161;&#20449;&#24687;&#8221;&#65292;&#21487;&#20197;&#36890;&#36807;&#26469;&#33258;&#36741;&#21161;&#28304;&#25968;&#25454;&#38598;&#30340;&#35814;&#23613;&#25968;&#25454;&#25110;&#22312;&#30456;&#21516;&#25968;&#25454;&#38598;&#19978;&#36890;&#36807;&#26045;&#21152;&#38480;&#21046;&#26469;&#33719;&#24471;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#21463;&#38480;&#27169;&#22411;&#30340;&#30693;&#35782;&#20256;&#36882;&#32473;&#23436;&#25972;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#23436;&#25972;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21463;&#38480;&#24615;&#33021;&#19982;&#21463;&#38480;&#27169;&#22411;&#30340;&#24615;&#33021;&#23545;&#40784;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24341;&#23548;&#27169;&#22411;&#21305;&#37197;&#65288;IMM&#65289;&#65292;&#39318;&#20808;&#36890;&#36807;&#20197;&#36923;&#36753;&#22238;&#24402;&#20026;&#29609;&#20855;&#31034;&#20363;&#26469;&#35828;&#26126;&#20854;&#26222;&#36866;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#25506;&#35752;&#20102;IMM&#22312;&#35821;&#35328;&#24314;&#27169;&#20013;&#30340;&#24212;&#29992;&#65292;&#36825;&#20063;&#26159;&#26368;&#21021;&#30340;&#28789;&#24863;&#26469;&#28304;&#65292;IMM&#22312;&#36825;&#37324;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#22522;&#30784;&#65292;&#19982;&#22312;&#25216;&#26415;&#20013;&#38544;&#24335;&#20351;&#29992;&#21463;&#38480;&#27169;&#22411;&#30340;&#26041;&#27861;&#30456;&#23545;&#24212;&#65292;&#20363;&#22914;&#28155;&#21152;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12513v1 Announce Type: new  Abstract: We consider scenarios where a very accurate predictive model using restricted features is available at the time of training of a larger, full-featured, model. This restricted model may be thought of as "side-information", derived either from an auxiliary exhaustive dataset or on the same dataset, by forcing the restriction. How can the restricted model be useful to the full model? We propose an approach for transferring the knowledge of the restricted model to the full model, by aligning the full model's context-restricted performance with that of the restricted model's. We call this methodology Induced Model Matching (IMM) and first illustrate its general applicability by using logistic regression as a toy example. We then explore IMM's use in language modeling, the application that initially inspired it, and where it offers an explicit foundation in contrast to the implicit use of restricted models in techniques such as noising. We dem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuseMoE&#30340;&#19987;&#23478;&#28151;&#21512;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#38376;&#25511;&#20989;&#25968;&#23454;&#29616;&#28789;&#27963;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#65292;&#21516;&#26102;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2402.03226</link><description>&lt;p&gt;
FuseMoE&#65306;&#29992;&#20110;&#28789;&#27963;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#19987;&#23478;&#28151;&#21512;Transformer
&lt;/p&gt;
&lt;p&gt;
FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FuseMoE&#30340;&#19987;&#23478;&#28151;&#21512;Transformer&#26694;&#26550;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#38376;&#25511;&#20989;&#25968;&#23454;&#29616;&#28789;&#27963;&#34701;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#65292;&#21516;&#26102;&#25913;&#21892;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#22312;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20851;&#38190;&#39046;&#22495;&#36234;&#26469;&#36234;&#22810;&#22320;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#23427;&#20204;&#38754;&#20020;&#22788;&#29702;&#22810;&#31181;&#27169;&#24577;&#30340;&#21452;&#37325;&#25361;&#25112;&#65292;&#36825;&#20123;&#27169;&#24577;&#32463;&#24120;&#22240;&#32570;&#22833;&#20803;&#32032;&#32780;&#19981;&#23436;&#25972;&#65292;&#20197;&#21450;&#25910;&#38598;&#26679;&#26412;&#30340;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#21644;&#31232;&#30095;&#24615;&#12290;&#25104;&#21151;&#21033;&#29992;&#36825;&#31181;&#22797;&#26434;&#25968;&#25454;&#65292;&#21516;&#26102;&#20811;&#26381;&#39640;&#36136;&#37327;&#35757;&#32451;&#26679;&#26412;&#30340;&#31232;&#32570;&#24615;&#65292;&#26159;&#25552;&#39640;&#36825;&#20123;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20851;&#38190;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;``FuseMoE''&#65292;&#36825;&#26159;&#19968;&#20010;&#38598;&#25104;&#21019;&#26032;&#38376;&#25511;&#20989;&#25968;&#30340;&#19987;&#23478;&#28151;&#21512;&#26694;&#26550;&#12290;FuseMoE&#26088;&#22312;&#25972;&#21512;&#22810;&#31181;&#27169;&#24577;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#32570;&#22833;&#27169;&#24577;&#21644;&#19981;&#35268;&#21017;&#37319;&#26679;&#25968;&#25454;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#38750;&#24120;&#26377;&#25928;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#29420;&#29305;&#30340;&#38376;&#25511;&#20989;&#25968;&#26377;&#21161;&#20110;&#25552;&#39640;&#25910;&#25947;&#36895;&#24230;&#65292;&#22312;&#22810;&#20010;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;FuseMoE&#30340;&#23454;&#38469;&#23454;&#29992;&#24615;&#36890;&#36807;&#19968;&#31995;&#21015;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20020;&#24202;&#39118;&#38505;&#39044;&#27979;&#20219;&#21153;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in real world is validated by a challenging set of clinical risk prediction tasks.
&lt;/p&gt;</description></item><item><title>Pruner&#26159;&#19968;&#31181;&#39640;&#25928;&#36328;&#24179;&#21488;&#24352;&#37327;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38745;&#24577;&#20998;&#26512;&#22120;&#65288;PSA&#65289;&#21644;&#27169;&#24335;&#24863;&#30693;&#25104;&#26412;&#27169;&#22411;&#65288;PaCM&#65289;&#23454;&#29616;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#65292;&#24182;&#20351;&#29992;&#21160;&#37327;&#36716;&#31227;&#23398;&#20064;&#65288;MTL&#65289;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02361</link><description>&lt;p&gt;
Pruner:&#19968;&#31181;&#20855;&#26377;&#21452;&#37325;&#24863;&#30693;&#33021;&#21147;&#30340;&#39640;&#25928;&#36328;&#24179;&#21488;&#24352;&#37327;&#32534;&#35793;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pruner: An Efficient Cross-Platform Tensor Compiler with Dual Awareness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02361
&lt;/p&gt;
&lt;p&gt;
Pruner&#26159;&#19968;&#31181;&#39640;&#25928;&#36328;&#24179;&#21488;&#24352;&#37327;&#32534;&#35793;&#22120;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#38745;&#24577;&#20998;&#26512;&#22120;&#65288;PSA&#65289;&#21644;&#27169;&#24335;&#24863;&#30693;&#25104;&#26412;&#27169;&#22411;&#65288;PaCM&#65289;&#23454;&#29616;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#65292;&#24182;&#20351;&#29992;&#21160;&#37327;&#36716;&#31227;&#23398;&#20064;&#65288;MTL&#65289;&#31574;&#30053;&#23454;&#29616;&#20102;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#23398;&#20064;&#21152;&#36895;&#22120;&#65288;DLAs&#65289;&#19978;&#30340;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#23545;&#20110;&#26377;&#25928;&#30340;&#27169;&#22411;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22522;&#20110;&#25628;&#32034;&#30340;&#28145;&#24230;&#23398;&#20064;&#32534;&#35793;&#22120;&#65288;DLC&#65289;&#19982;&#25163;&#21160;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#20173;&#28982;&#38754;&#20020;&#30528;&#25628;&#32034;&#25928;&#29575;&#20302;&#21644;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#24046;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Pruner&#65292;&#36981;&#24490;&#30828;&#20214;/&#36719;&#20214;&#21327;&#21516;&#35774;&#35745;&#21407;&#21017;&#26469;&#20998;&#23618;&#25552;&#21319;&#24352;&#37327;&#31243;&#24207;&#20248;&#21270;&#12290;Pruner&#30001;&#20004;&#20010;&#20027;&#35201;&#32452;&#20214;&#32452;&#25104;&#65306;&#21442;&#25968;&#21270;&#38745;&#24577;&#20998;&#26512;&#22120;&#65288;PSA&#65289;&#21644;&#27169;&#24335;&#24863;&#30693;&#25104;&#26412;&#27169;&#22411;&#65288;PaCM&#65289;&#12290;&#21069;&#32773;&#20316;&#20026;&#19968;&#31181;&#30828;&#20214;&#24863;&#30693;&#21644;&#20844;&#24335;&#21270;&#30340;&#24615;&#33021;&#20998;&#26512;&#24037;&#20855;&#65292;&#24341;&#23548;&#25628;&#32034;&#31354;&#38388;&#30340;&#20462;&#21098;&#65292;&#32780;&#21518;&#32773;&#26681;&#25454;&#20851;&#38190;&#30340;&#25968;&#25454;&#27969;&#27169;&#24335;&#23454;&#29616;&#20102;&#23545;&#24352;&#37327;&#31243;&#24207;&#30340;&#24615;&#33021;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#35777;&#26377;&#25928;&#30340;&#36328;&#24179;&#21488;&#36866;&#24212;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#37327;&#36716;&#31227;&#23398;&#20064;&#65288;MTL&#65289;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tensor program optimization on Deep Learning Accelerators (DLAs) is critical for efficient model deployment. Although search-based Deep Learning Compilers (DLCs) have achieved significant performance gains compared to manual methods, they still suffer from the persistent challenges of low search efficiency and poor cross-platform adaptability. In this paper, we propose $\textbf{Pruner}$, following hardware/software co-design principles to hierarchically boost tensor program optimization. Pruner comprises two primary components: a Parameterized Static Analyzer ($\textbf{PSA}$) and a Pattern-aware Cost Model ($\textbf{PaCM}$). The former serves as a hardware-aware and formulaic performance analysis tool, guiding the pruning of the search space, while the latter enables the performance prediction of tensor programs according to the critical data-flow patterns. Furthermore, to ensure effective cross-platform adaptation, we design a Momentum Transfer Learning ($\textbf{MTL}$) strategy using
&lt;/p&gt;</description></item><item><title>CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;</title><link>https://arxiv.org/abs/2402.00786</link><description>&lt;p&gt;
CroissantLLM: &#19968;&#20010;&#30495;&#27491;&#30340;&#21452;&#35821;&#27861;&#35821;-&#33521;&#35821;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CroissantLLM: A Truly Bilingual French-English Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00786
&lt;/p&gt;
&lt;p&gt;
CroissantLLM&#26159;&#19968;&#20010;1.3B&#30340;&#21452;&#35821;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#39640;&#24615;&#33021;&#21644;&#24320;&#28304;&#12290;&#27169;&#22411;&#36824;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#22810;&#20010;&#26816;&#26597;&#28857;&#65292;&#20197;&#21450;&#19968;&#20010;&#27861;&#35821;&#22522;&#20934;&#27979;&#35797; FrenchBench&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;CroissantLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#22312;3T&#20010;&#33521;&#35821;&#21644;&#27861;&#35821;&#26631;&#35760;&#19978;&#39044;&#35757;&#32451;&#30340;13&#20159;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#30740;&#31350;&#21644;&#24037;&#19994;&#31038;&#21306;&#24102;&#26469;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#12289;&#23436;&#20840;&#24320;&#28304;&#30340;&#21452;&#35821;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#28040;&#36153;&#32423;&#26412;&#22320;&#30828;&#20214;&#19978;&#24555;&#36895;&#36816;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;1:1&#30340;&#33521;&#35821;-&#27861;&#35821;&#39044;&#35757;&#32451;&#25968;&#25454;&#27604;&#20363;&#12289;&#33258;&#23450;&#20041;&#30340;&#20998;&#35789;&#22120;&#21644;&#21452;&#35821;&#35843;&#20248;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#19968;&#31181;&#20869;&#22312;&#21452;&#35821;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#27861;&#35821;&#20998;&#21106;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#25163;&#24037;&#31574;&#21010;&#12289;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#28304;&#12290;&#20026;&#20102;&#35780;&#20272;&#22312;&#33521;&#35821;&#20197;&#22806;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797; FrenchBench&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#20998;&#31867;&#21644;&#29983;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#22312;&#27861;&#35821;&#35821;&#35328;&#20013;&#24615;&#33021;&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20445;&#25345;&#36879;&#26126;&#24230;&#24182;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#20195;&#30721;&#24211;&#21644;&#21508;&#31181;&#27169;&#22411;&#35268;&#27169;&#12289;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#20960;&#21313;&#20010;&#26816;&#26597;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2306.02786</link><description>&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#36335;&#24452;&#20960;&#20309;&#23548;&#33322;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;
&lt;/p&gt;
&lt;p&gt;
Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02786
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#23548;&#33322;&#21644;&#27604;&#36739;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#35299;&#37322;&#26159;&#35299;&#37322;&#65288;&#19981;&#36879;&#26126;&#30340;&#65289;&#39044;&#27979;&#27169;&#22411;&#20915;&#31574;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#20854;&#29983;&#25104;&#24448;&#24448;&#21463;&#21040;&#31639;&#27861;&#21644;&#29305;&#23450;&#39046;&#22495;&#32422;&#26463;&#30340;&#24433;&#21709;&#65292;&#22914;&#22522;&#20110;&#23494;&#24230;&#30340;&#21487;&#34892;&#24615;&#21644;&#23646;&#24615;&#30340;&#65288;&#19981;&#65289;&#21487;&#21464;&#24615;&#25110;&#21464;&#21270;&#30340;&#26041;&#21521;&#24615;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#20854;&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#38500;&#20102;&#23545;&#21453;&#20107;&#23454;&#23454;&#20363;&#26412;&#36523;&#30340;&#35201;&#27714;&#20043;&#22806;&#65292;&#24050;&#30693;&#31639;&#27861;&#21487;&#34892;&#24615;&#36335;&#24452;&#19982;&#20107;&#23454;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#21363;&#31639;&#27861;&#21487;&#35785;&#27714;&#65292;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#25216;&#26415;&#32771;&#34385;&#22240;&#32032;&#12290;&#23613;&#31649;&#36825;&#20004;&#20010;&#35201;&#27714;&#30830;&#20445;&#20102;&#26053;&#31243;&#30340;&#27493;&#39588;&#21644;&#30446;&#30340;&#22320;&#30340;&#21512;&#29702;&#24615;&#65292;&#20294;&#30446;&#21069;&#30340;&#25991;&#29486;&#24573;&#30053;&#20102;&#36825;&#31181;&#21453;&#20107;&#23454;&#36335;&#24452;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#37322;&#24615;&#22810;&#20803;&#23431;&#23449;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#21487;&#33021;&#30340;&#21453;&#20107;&#23454;&#26053;&#31243;&#65307;&#28982;&#21518;&#23637;&#31034;&#20102;&#22914;&#20309;&#23548;&#33322;&#12289;&#25512;&#29702;&#21644;&#27604;&#36739;&#36825;&#20123;&#36712;&#36857;&#30340;&#20960;&#20309;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to algorithmic and domain-specific constraints -- such as density-based feasibility and attribute (im)mutability or directionality of change -- that aim to maximise their real-life utility. In addition to desiderata with respect to the counterfactual instance itself, existence of a viable path connecting it with the factual data point, known as algorithmic recourse, has become an important technical consideration. While both of these requirements ensure that the steps of the journey as well as its destination are admissible, current literature neglects the multiplicity of such counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys; we then show how to navigate, reason about and compare the geometry of these trajectories -
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;&#24555;&#36895;&#31283;&#23450;&#30340;&#39063;&#31890;&#29699;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18450</link><description>&lt;p&gt;
GBG++&#65306;&#20998;&#31867;&#30340;&#24555;&#36895;&#21644;&#31283;&#23450;&#30340;&#39063;&#31890;&#29699;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
GBG++: A Fast and Stable Granular Ball Generation Method for Classification. (arXiv:2305.18450v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;&#24555;&#36895;&#31283;&#23450;&#30340;&#39063;&#31890;&#29699;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39063;&#31890;&#29699;&#35745;&#31639;&#20316;&#20026;&#19968;&#31181;&#39640;&#25928;&#12289;&#31283;&#20581;&#12289;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#24050;&#25104;&#20026;&#39063;&#31890;&#35745;&#31639;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#39063;&#31890;&#29699;&#35745;&#31639;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#39063;&#31890;&#29699;&#29983;&#25104;&#65288;GBG&#65289;&#21644;&#22522;&#20110;&#39063;&#31890;&#29699;&#65288;GB&#65289;&#30340;&#22810;&#31890;&#24230;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;GBG&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#25928;&#29575;&#38656;&#35201;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;k&#22343;&#20540;&#25110;k&#20998;&#21106;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;GB&#30340;&#20998;&#31867;&#22120;&#20165;&#21333;&#21521;&#32771;&#34385;GB&#30340;&#20960;&#20309;&#29305;&#24449;&#26500;&#24314;&#20998;&#31867;&#35268;&#21017;&#65292;&#32780;&#24573;&#35270;&#20102;GB&#30340;&#36136;&#37327;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#31283;&#23450;&#30340;&#22522;&#20110;&#20851;&#27880;&#26426;&#21046;&#30340;GBG&#65288;GBG++&#65289;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;GBG++&#26041;&#27861;&#20165;&#38656;&#35201;&#22312;&#20998;&#21106;&#27599;&#20010;GB&#26102;&#35745;&#31639;&#20174;&#25968;&#25454;&#39537;&#21160;&#30340;&#20013;&#24515;&#21040;&#26410;&#20998;&#21106;&#26679;&#26412;&#30340;&#36317;&#31163;&#65292;&#32780;&#19981;&#26159;&#38543;&#26426;&#36873;&#25321;&#20013;&#24515;&#24182;&#35745;&#31639;&#23427;&#21040;&#25152;&#26377;&#26679;&#26412;&#30340;&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#24322;&#24120;&#20540;&#26816;&#27979;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Granular ball computing (GBC), as an efficient, robust, and scalable learning method, has become a popular research topic of granular computing. GBC includes two stages: granular ball generation (GBG) and multi-granularity learning based on the granular ball (GB). However, the stability and efficiency of existing GBG methods need to be further improved due to their strong dependence on $k$-means or $k$-division. In addition, GB-based classifiers only unilaterally consider the GB's geometric characteristics to construct classification rules, but the GB's quality is ignored. Therefore, in this paper, based on the attention mechanism, a fast and stable GBG (GBG++) method is proposed first. Specifically, the proposed GBG++ method only needs to calculate the distances from the data-driven center to the undivided samples when splitting each GB, instead of randomly selecting the center and calculating the distances between it to all samples. Moreover, an outlier detection method is introduced
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SIMGA&#65292;&#23427;&#36890;&#36807;SimRank&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#33410;&#28857;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340;&#20256;&#25773;&#25928;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09958</link><description>&lt;p&gt;
SIMGA&#65306;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#19982;&#39640;&#25928;&#30340;&#20840;&#23616;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
SIMGA: A Simple and Effective Heterophilous Graph Neural Network with Efficient Global Aggregation. (arXiv:2305.09958v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#24322;&#36136;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;SIMGA&#65292;&#23427;&#36890;&#36807;SimRank&#20840;&#23616;&#32858;&#21512;&#26469;&#35299;&#20915;&#24322;&#36136;&#24615;&#33410;&#28857;&#32858;&#21512;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340;&#20256;&#25773;&#25928;&#29575;&#65292;&#21516;&#26102;&#20855;&#26377;&#33391;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#36935;&#21040;&#24322;&#36136;&#24615;&#26102;&#20250;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#65292;&#21363;&#22240;&#20026;&#23616;&#37096;&#21644;&#32479;&#19968;&#32858;&#21512;&#32780;&#23548;&#33268;&#30340;&#30456;&#37051;&#33410;&#28857;&#19981;&#30456;&#20284;&#12290;&#29616;&#26377;&#30340;&#24322;&#36136;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#35797;&#22270;&#25972;&#21512;&#20840;&#23616;&#32858;&#21512;&#30340;&#23581;&#35797;&#36890;&#24120;&#38656;&#35201;&#36845;&#20195;&#22320;&#32500;&#25252;&#21644;&#26356;&#26032;&#20840;&#22270;&#20449;&#24687;&#65292;&#23545;&#20110;&#19968;&#20010;&#20855;&#26377; $n$ &#20010;&#33410;&#28857;&#30340;&#22270;&#65292;&#36825;&#38656;&#35201; $\mathcal{O}(n^2)$ &#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#20174;&#32780;&#23548;&#33268;&#23545;&#22823;&#22411;&#22270;&#30340;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; SIMGA&#65292;&#19968;&#31181;&#23558; SimRank &#32467;&#26500;&#30456;&#20284;&#24230;&#27979;&#37327;&#20316;&#20026;&#20840;&#23616;&#32858;&#21512;&#30340; GNN &#32467;&#26500;&#12290; SIMGA &#30340;&#35774;&#35745;&#31616;&#21333;&#65292;&#19988;&#22312;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#26041;&#38754;&#37117;&#26377;&#30528;&#26377; promising &#30340;&#32467;&#26524;&#12290;SIMGA &#30340;&#31616;&#21333;&#24615;&#20351;&#20854;&#25104;&#20026;&#31532;&#19968;&#20010;&#21487;&#20197;&#23454;&#29616;&#25509;&#36817;&#20110;&#32447;&#24615;&#30340; $n$ &#20256;&#25773;&#25928;&#29575;&#30340;&#24322;&#36136;&#24615; GNN &#27169;&#22411;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#23427;&#30340;&#26377;&#25928;&#24615;&#65292;&#23558; SimRank &#35270;&#20026; GNN &#30340;&#19968;&#31181;&#26032;&#35299;&#37322;&#65292;&#24182;&#35777;&#26126;&#20102;&#27719;&#32858;&#33410;&#28857;&#34920;&#31034;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) realize great success in graph learning but suffer from performance loss when meeting heterophily, i.e. neighboring nodes are dissimilar, due to their local and uniform aggregation. Existing attempts in incoorporating global aggregation for heterophilous GNNs usually require iteratively maintaining and updating full-graph information, which entails $\mathcal{O}(n^2)$ computation efficiency for a graph with $n$ nodes, leading to weak scalability to large graphs. In this paper, we propose SIMGA, a GNN structure integrating SimRank structural similarity measurement as global aggregation. The design of SIMGA is simple, yet it leads to promising results in both efficiency and effectiveness. The simplicity of SIMGA makes it the first heterophilous GNN model that can achieve a propagation efficiency near-linear to $n$. We theoretically demonstrate its effectiveness by treating SimRank as a new interpretation of GNN and prove that the aggregated node representation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#34920;&#31034;&#24615;&#30456;&#20284;&#21644;&#21151;&#33021;&#30456;&#20284;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#20010;&#23478;&#26063;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24182;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#20854;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.06329</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30456;&#20284;&#24230;&#65306;&#21151;&#33021;&#21644;&#34920;&#31034;&#24615;&#27979;&#37327;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Similarity of Neural Network Models: A Survey of Functional and Representational Measures. (arXiv:2305.06329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24230;&#30340;&#20004;&#20010;&#35266;&#28857;&#65306;&#34920;&#31034;&#24615;&#30456;&#20284;&#21644;&#21151;&#33021;&#30456;&#20284;&#65292;&#25552;&#20379;&#20102;&#36825;&#20004;&#20010;&#23478;&#26063;&#30340;&#35814;&#32454;&#25551;&#36848;&#65292;&#24182;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#20854;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#23454;&#36341;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34913;&#37327;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#38750;&#24120;&#37325;&#35201;&#19988;&#22791;&#21463;&#30740;&#31350;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#20197;&#20102;&#35299;&#21644;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#30340;&#24046;&#24322;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#35266;&#28857;&#21487;&#20197;&#25551;&#36848;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20284;&#24615;&#65292;&#20294;&#26159;&#26412;&#25991;&#29305;&#21035;&#20851;&#27880;&#20004;&#20010;&#20114;&#34917;&#30340;&#35266;&#28857;&#65292;&#21363;(i) &#34920;&#31034;&#24615;&#30456;&#20284;&#65292;&#32771;&#34385;&#20013;&#38388;&#31070;&#32463;&#23618;&#30340;&#28608;&#27963;&#24046;&#24322;&#65292;&#21644;(ii) &#21151;&#33021;&#30456;&#20284;&#65292;&#32771;&#34385;&#27169;&#22411;&#36755;&#20986;&#30340;&#24046;&#24322;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#27010;&#36848;&#20102;&#36825;&#20004;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30456;&#20284;&#24615;&#27979;&#37327;&#30340;&#23478;&#26063;&#12290;&#38500;&#20102;&#25552;&#20379;&#29616;&#26377;&#27979;&#37327;&#30340;&#35814;&#32454;&#25551;&#36848;&#22806;&#65292;&#25105;&#20204;&#36824;&#24635;&#32467;&#21644;&#35752;&#35770;&#20102;&#36825;&#20123;&#27979;&#37327;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#65292;&#24182;&#25351;&#20986;&#20102;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#24314;&#35758;&#65292;&#21487;&#20197;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21033;&#29992;&#36825;&#20123;&#27979;&#37327;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#20026;&#25105;&#20204;&#30340;&#31038;&#21306;&#21442;&#19982;&#26356;&#22810;&#26377;&#29992;&#30340;&#24037;&#20316;&#22880;&#23450;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Measuring similarity of neural networks has become an issue of great importance and research interest to understand and utilize differences of neural networks. While there are several perspectives on how neural networks can be similar, we specifically focus on two complementing perspectives, i.e., (i) representational similarity, which considers how activations of intermediate neural layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In this survey, we provide a comprehensive overview of these two families of similarity measures for neural network models. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties and relationships of these measures, and point to open research problems. Further, we provide practical recommendations that can guide researchers as well as practitioners in applying the measures. We hope our work lays a foundation for our community to engage in more s
&lt;/p&gt;</description></item></channel></rss>