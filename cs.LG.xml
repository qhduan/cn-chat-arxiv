<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#21152;&#26435;&#22240;&#26524;&#35268;&#21017;&#26469;&#20272;&#35745;&#21644;&#21152;&#24378;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.06746</link><description>&lt;p&gt;
&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65306;&#36890;&#36807;&#21152;&#26435;&#22240;&#26524;&#35268;&#21017;&#22686;&#24378;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Causal Rule Learning: Enhancing the Understanding of Heterogeneous Treatment Effect via Weighted Causal Rules. (arXiv:2310.06746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06746
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#21152;&#26435;&#22240;&#26524;&#35268;&#21017;&#26469;&#20272;&#35745;&#21644;&#21152;&#24378;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24615;&#26159;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20272;&#35745;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#26102;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#21307;&#30103;&#24212;&#29992;&#26469;&#35828;&#65292;&#24120;&#24120;&#38656;&#35201;&#20570;&#20986;&#39640;&#39118;&#38505;&#20915;&#31574;&#12290;&#21463;&#21040;&#35299;&#37322;&#24615;&#30340;&#39044;&#27979;&#24615;&#12289;&#25551;&#36848;&#24615;&#12289;&#30456;&#20851;&#24615;&#26694;&#26550;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#25214;&#21040;&#25551;&#36848;&#28508;&#22312;&#23376;&#32676;&#30340;&#31934;&#32454;&#22240;&#26524;&#35268;&#21017;&#38598;&#26469;&#20272;&#35745;&#21644;&#22686;&#24378;&#25105;&#20204;&#23545;&#24322;&#36136;&#27835;&#30103;&#25928;&#24212;&#30340;&#29702;&#35299;&#12290;&#22240;&#26524;&#35268;&#21017;&#23398;&#20064;&#21253;&#25324;&#19977;&#20010;&#38454;&#27573;&#65306;&#35268;&#21017;&#21457;&#29616;&#12289;&#35268;&#21017;&#36873;&#25321;&#21644;&#35268;&#21017;&#20998;&#26512;&#12290;&#22312;&#35268;&#21017;&#21457;&#29616;&#38454;&#27573;&#65292;&#25105;&#20204;&#21033;&#29992;&#22240;&#26524;&#26862;&#26519;&#29983;&#25104;&#19968;&#32452;&#20855;&#26377;&#30456;&#24212;&#23376;&#32676;&#24179;&#22343;&#27835;&#30103;&#25928;&#24212;&#30340;&#22240;&#26524;&#35268;&#21017;&#27744;&#12290;&#36873;&#25321;&#38454;&#27573;&#20351;&#29992;D-&#23398;&#20064;&#26041;&#27861;&#20174;&#36825;&#20123;&#35268;&#21017;&#20013;&#36873;&#25321;&#23376;&#38598;&#65292;&#23558;&#20010;&#20307;&#27700;&#24179;&#30340;&#27835;&#30103;&#25928;&#24212;&#20316;&#20026;&#23376;&#32676;&#27700;&#24179;&#25928;&#24212;&#30340;&#32447;&#24615;&#32452;&#21512;&#36827;&#34892;&#35299;&#26500;&#12290;&#36825;&#26377;&#21161;&#20110;&#22238;&#31572;&#20043;&#21069;&#25991;&#29486;&#24573;&#35270;&#30340;&#38382;&#39064;&#65306;&#22914;&#26524;&#19968;&#20010;&#20010;&#20307;&#21516;&#26102;&#23646;&#20110;&#22810;&#20010;&#19981;&#21516;&#30340;&#27835;&#30103;&#23376;&#32676;&#65292;&#20250;&#24590;&#20040;&#26679;&#21602;&#65311;
&lt;/p&gt;
&lt;p&gt;
Interpretability is a key concern in estimating heterogeneous treatment effects using machine learning methods, especially for healthcare applications where high-stake decisions are often made. Inspired by the Predictive, Descriptive, Relevant framework of interpretability, we propose causal rule learning which finds a refined set of causal rules characterizing potential subgroups to estimate and enhance our understanding of heterogeneous treatment effects. Causal rule learning involves three phases: rule discovery, rule selection, and rule analysis. In the rule discovery phase, we utilize a causal forest to generate a pool of causal rules with corresponding subgroup average treatment effects. The selection phase then employs a D-learning method to select a subset of these rules to deconstruct individual-level treatment effects as a linear combination of the subgroup-level effects. This helps to answer an ignored question by previous literature: what if an individual simultaneously bel
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32479;&#35745;&#20272;&#35745;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;Wasserstein&#20998;&#24067;&#20559;&#31227;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#20998;&#24067;&#20559;&#31227;&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#32479;&#35745;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#35770;&#25991;&#21457;&#29616;&#20102;&#26368;&#20248;&#30340;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#21644;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#65292;&#24182;&#35777;&#26126;&#20102;&#26679;&#26412;&#22343;&#20540;&#21644;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01853</link><description>&lt;p&gt;
&#32479;&#35745;&#20272;&#35745;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;: Wasserstein&#25200;&#21160;&#19982;&#26497;&#23567;&#26497;&#22823;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01853
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32479;&#35745;&#20272;&#35745;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20027;&#35201;&#20851;&#27880;Wasserstein&#20998;&#24067;&#20559;&#31227;&#65292;&#25552;&#20986;&#20102;&#32852;&#21512;&#20998;&#24067;&#20559;&#31227;&#27010;&#24565;&#65292;&#24182;&#20998;&#26512;&#20102;&#20960;&#20010;&#32479;&#35745;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#35770;&#25991;&#21457;&#29616;&#20102;&#26368;&#20248;&#30340;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#21644;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#65292;&#24182;&#35777;&#26126;&#20102;&#26679;&#26412;&#22343;&#20540;&#21644;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#20559;&#31227;&#26159;&#29616;&#20195;&#32479;&#35745;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#23558;&#25968;&#25454;&#30340;&#29305;&#24615;&#20174;&#30495;&#23454;&#24773;&#20917;&#20013;&#31995;&#32479;&#22320;&#25913;&#21464;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;Wasserstein&#20998;&#24067;&#20559;&#31227;&#65292;&#20854;&#20013;&#27599;&#20010;&#25968;&#25454;&#28857;&#21487;&#33021;&#20250;&#21457;&#29983;&#36731;&#24494;&#25200;&#21160;&#65292;&#32780;&#19981;&#26159;Huber&#27745;&#26579;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#37096;&#20998;&#35266;&#27979;&#20540;&#26159;&#24322;&#24120;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#36229;&#20986;&#29420;&#31435;&#25200;&#21160;&#30340;&#20559;&#31227;&#65292;&#25506;&#32034;&#20102;&#32852;&#21512;&#20998;&#24067;&#20559;&#31227;&#65292;&#20854;&#20013;&#27599;&#20010;&#35266;&#27979;&#28857;&#30340;&#25200;&#21160;&#21487;&#20197;&#21327;&#35843;&#36827;&#34892;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#20960;&#20010;&#37325;&#35201;&#30340;&#32479;&#35745;&#38382;&#39064;&#65292;&#21253;&#25324;&#20301;&#32622;&#20272;&#35745;&#12289;&#32447;&#24615;&#22238;&#24402;&#21644;&#38750;&#21442;&#25968;&#23494;&#24230;&#20272;&#35745;&#12290;&#22312;&#22343;&#20540;&#20272;&#35745;&#21644;&#32447;&#24615;&#22238;&#24402;&#30340;&#39044;&#27979;&#35823;&#24046;&#26041;&#24046;&#19979;&#65292;&#25105;&#20204;&#25214;&#21040;&#20102;&#31934;&#30830;&#30340;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#12289;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#65292;&#24182;&#35777;&#26126;&#20102;&#26679;&#26412;&#22343;&#20540;&#21644;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#37327;&#20998;&#21035;&#26159;&#26368;&#20248;&#30340;&#12290;&#36825;&#36866;&#29992;&#20110;&#29420;&#31435;&#21644;&#32852;&#21512;&#20559;&#31227;&#65292;&#20294;&#26368;&#19981;&#21033;&#30340;&#25200;&#21160;&#21644;&#26497;&#23567;&#26497;&#22823;&#39118;&#38505;&#26159;&#19981;&#21516;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For
&lt;/p&gt;</description></item></channel></rss>