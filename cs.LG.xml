<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>INCPrompt&#37319;&#29992;&#33258;&#36866;&#24212;&#20851;&#38190;&#23398;&#20064;&#32773;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#32467;&#21512;&#36890;&#29992;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#26377;&#25928;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#34920;&#29616;&#20248;&#36234;&#65292;&#23545;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2401.11667</link><description>&lt;p&gt;
INCPrompt&#65306;&#38754;&#21521;&#20219;&#21153;&#30340;&#22686;&#37327;&#25552;&#31034;&#65292;&#26080;&#38656;&#37325;&#22797;&#32451;&#20064;&#30340;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
INCPrompt: Task-Aware incremental Prompting for Rehearsal-Free Class-incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11667
&lt;/p&gt;
&lt;p&gt;
INCPrompt&#37319;&#29992;&#33258;&#36866;&#24212;&#20851;&#38190;&#23398;&#20064;&#32773;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#65292;&#32467;&#21512;&#36890;&#29992;&#21644;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#65292;&#26377;&#25928;&#32531;&#35299;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#34920;&#29616;&#20248;&#36234;&#65292;&#23545;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;INCPrompt&#65292;&#19968;&#31181;&#21019;&#26032;&#30340;&#25345;&#32493;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290; INCPrompt&#30340;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#20854;&#20351;&#29992;&#33258;&#36866;&#24212;&#30340;&#20851;&#38190;&#23398;&#20064;&#32773;&#21644;&#38754;&#21521;&#20219;&#21153;&#30340;&#25552;&#31034;&#26469;&#25429;&#33719;&#19982;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290; &#36825;&#31181;&#29420;&#29305;&#32452;&#21512;&#23553;&#35013;&#20102;&#36328;&#20219;&#21153;&#30340;&#36890;&#29992;&#30693;&#35782;&#24182;&#32534;&#30721;&#20102;&#20219;&#21153;&#29305;&#23450;&#30693;&#35782;&#12290; &#25105;&#20204;&#22312;&#22810;&#20010;&#25345;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#36827;&#34892;&#30340;&#20840;&#38754;&#35780;&#20272;&#34920;&#26126;&#65292;INCPrompt&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#26174;&#31034;&#20986;&#20854;&#22312;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#20445;&#25345;&#39640;&#24615;&#33021;&#30340;&#26377;&#25928;&#24615;&#12290; &#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#38754;&#21521;&#20219;&#21153;&#30340;&#22686;&#37327;&#25552;&#31034;&#23545;&#25345;&#32493;&#23398;&#20064;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11667v2 Announce Type: replace  Abstract: This paper introduces INCPrompt, an innovative continual learning solution that effectively addresses catastrophic forgetting. INCPrompt's key innovation lies in its use of adaptive key-learner and task-aware prompts that capture task-relevant information. This unique combination encapsulates general knowledge across tasks and encodes task-specific knowledge. Our comprehensive evaluation across multiple continual learning benchmarks demonstrates INCPrompt's superiority over existing algorithms, showing its effectiveness in mitigating catastrophic forgetting while maintaining high performance. These results highlight the significant impact of task-aware incremental prompting on continual learning performance.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#25511;&#21046;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#29366;&#24577;&#34920;&#31034;&#20989;&#25968;&#21644;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2212.14511</link><description>&lt;p&gt;
&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#25511;&#21046;&#38382;&#39064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Direct Latent Model Learning Solve Linear Quadratic Gaussian Control?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.14511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#25511;&#21046;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#29366;&#24577;&#34920;&#31034;&#20989;&#25968;&#21644;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20174;&#28508;&#22312;&#39640;&#32500;&#35266;&#27979;&#20013;&#23398;&#20064;&#29366;&#24577;&#34920;&#31034;&#30340;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#25511;&#21046;&#26410;&#30693;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#31995;&#32479;&#12290;&#25105;&#20204;&#37319;&#29992;&#30452;&#25509;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#27979;&#19982;&#35268;&#21010;&#30452;&#25509;&#30456;&#20851;&#30340;&#25968;&#37327;&#65288;&#20363;&#22914;&#25104;&#26412;&#65289;&#26469;&#23398;&#20064;&#28508;&#22312;&#29366;&#24577;&#31354;&#38388;&#20013;&#30340;&#21160;&#24577;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#37325;&#24314;&#35266;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#30452;&#35266;&#30340;&#22522;&#20110;&#25104;&#26412;&#39537;&#21160;&#30340;&#29366;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#65288;LQG&#65289;&#25511;&#21046;&#38382;&#39064;&#65292;&#36825;&#26159;&#26368;&#22522;&#26412;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#25511;&#21046;&#38382;&#39064;&#20043;&#19968;&#12290;&#20316;&#20026;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#22312;&#26377;&#38480;&#26679;&#26412;&#19979;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#29366;&#24577;&#34920;&#31034;&#20989;&#25968;&#21644;&#20351;&#29992;&#30452;&#25509;&#23398;&#20064;&#30340;&#28508;&#22312;&#27169;&#22411;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#25511;&#21046;&#22120;&#30340;&#20445;&#35777;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23613;&#31649;&#20197;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#21462;&#24471;&#20102;&#21508;&#31181;&#32463;&#39564;&#25104;&#21151;&#65292;&#20294;&#22312;&#36825;&#39033;&#24037;&#20316;&#20043;&#21069;&#65292;&#23578;&#19981;&#28165;&#26970;&#36825;&#31181;&#22522;&#20110;&#25104;&#26412;&#39537;&#21160;&#30340;&#28508;&#22312;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#26159;&#21542;&#20855;&#26377;&#26377;&#38480;&#26679;&#26412;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.14511v2 Announce Type: replace  Abstract: We study the task of learning state representations from potentially high-dimensional observations, with the goal of controlling an unknown partially observable system. We pursue a direct latent model learning approach, where a dynamic model in some latent state space is learned by predicting quantities directly related to planning (e.g., costs) without reconstructing the observations. In particular, we focus on an intuitive cost-driven state representation learning method for solving Linear Quadratic Gaussian (LQG) control, one of the most fundamental partially observable control problems. As our main results, we establish finite-sample guarantees of finding a near-optimal state representation function and a near-optimal controller using the directly learned latent model. To the best of our knowledge, despite various empirical successes, prior to this work it was unclear if such a cost-driven latent model learner enjoys finite-sampl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#29992;&#20110;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#35299;&#37322;&#20026;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26435;&#34913;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2310.03311</link><description>&lt;p&gt;
&#28145;&#24230;&#21464;&#20998;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;--&#19968;&#31181;&#21464;&#20998;&#25439;&#22833;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses. (arXiv:2310.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#29992;&#20110;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#29616;&#26377;&#30340;&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#26032;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#35299;&#37322;&#20026;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#26435;&#34913;&#65292;&#35813;&#26694;&#26550;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#38477;&#32500;&#26041;&#27861;&#20197;&#20854;&#39640;&#31934;&#24230;&#12289;&#29983;&#25104;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#32780;&#38395;&#21517;&#12290;&#36825;&#20123;&#26041;&#27861;&#26377;&#24456;&#22810;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#32479;&#19968;&#21407;&#29702;&#65292;&#37325;&#26032;&#25512;&#23548;&#21644;&#25512;&#24191;&#20102;&#29616;&#26377;&#30340;&#21464;&#20998;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22522;&#20110;&#22810;&#21464;&#37327;&#20449;&#24687;&#29942;&#39048;&#30340;&#35299;&#37322;&#65292;&#20854;&#20013;&#20004;&#20010;&#36125;&#21494;&#26031;&#32593;&#32476;&#30456;&#20114;&#26435;&#34913;&#12290;&#25105;&#20204;&#23558;&#31532;&#19968;&#20010;&#32593;&#32476;&#35299;&#37322;&#20026;&#32534;&#30721;&#22120;&#22270;&#65292;&#23427;&#25351;&#23450;&#20102;&#22312;&#21387;&#32553;&#25968;&#25454;&#26102;&#35201;&#20445;&#30041;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#31532;&#20108;&#20010;&#32593;&#32476;&#35299;&#37322;&#20026;&#35299;&#30721;&#22120;&#22270;&#65292;&#23427;&#20026;&#25968;&#25454;&#25351;&#23450;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#37325;&#26032;&#25512;&#23548;&#20102;&#29616;&#26377;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#22914;&#28145;&#24230;&#21464;&#20998;&#20449;&#24687;&#29942;&#39048;(DVIB)&#12289;beta&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(beta-VAE)&#21644;&#28145;&#24230;&#21464;&#20998;&#35268;&#33539;&#30456;&#20851;&#20998;&#26512;(DVCCA)&#12290;&#35813;&#26694;&#26550;&#33258;&#28982;&#22320;&#24341;&#20837;&#20102;&#19968;&#20010;&#22312;&#21387;&#32553;&#25968;&#25454;&#21644;&#20445;&#30041;&#20449;&#24687;&#20043;&#38388;&#30340;&#26435;&#34913;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational dimensionality reduction methods are known for their high accuracy, generative abilities, and robustness. These methods have many theoretical justifications. Here we introduce a unifying principle rooted in information theory to rederive and generalize existing variational methods and design new ones. We base our framework on an interpretation of the multivariate information bottleneck, in which two Bayesian networks are traded off against one another. We interpret the first network as an encoder graph, which specifies what information to keep when compressing the data. We interpret the second network as a decoder graph, which specifies a generative model for the data. Using this framework, we rederive existing dimensionality reduction methods such as the deep variational information bottleneck (DVIB), beta variational auto-encoders (beta-VAE), and deep variational canonical correlation analysis (DVCCA). The framework naturally introduces a trade-off parameter between compr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36830;&#32493;&#32431;&#31574;&#30053;&#38598;&#21644;&#23545;&#25910;&#30410;&#20989;&#25968;&#30340;&#19968;&#38454;&#35775;&#38382;&#30340;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#28385;&#36275;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20174;&#20219;&#20309;&#21021;&#22987;&#21270;&#25351;&#25968;&#25910;&#25947;&#20110;&#20934;&#30830;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2211.01280</link><description>&lt;p&gt;
&#19968;&#31181;&#36830;&#32493;&#21338;&#24328;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#30340;&#25351;&#25968;&#25910;&#25947;&#31890;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Exponentially Converging Particle Method for the Mixed Nash Equilibrium of Continuous Games. (arXiv:2211.01280v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01280
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20855;&#26377;&#36830;&#32493;&#32431;&#31574;&#30053;&#38598;&#21644;&#23545;&#25910;&#30410;&#20989;&#25968;&#30340;&#19968;&#38454;&#35775;&#38382;&#30340;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#38382;&#39064;&#65292;&#24182;&#22312;&#28385;&#36275;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#20174;&#20219;&#20309;&#21021;&#22987;&#21270;&#25351;&#25968;&#25910;&#25947;&#20110;&#20934;&#30830;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#32431;&#31574;&#30053;&#38598;&#21644;&#23545;&#25910;&#30410;&#20989;&#25968;&#30340;&#19968;&#38454;&#35775;&#38382;&#30340;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#35745;&#31639;&#38382;&#39064;&#12290;&#35813;&#38382;&#39064;&#22312;&#20197;&#21338;&#24328;&#29702;&#35770;&#20026;&#28789;&#24863;&#30340;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#20986;&#29616;&#65292;&#22914;&#20998;&#24067;&#24335;&#31283;&#20581;&#23398;&#20064;&#12290;&#22312;&#36825;&#20123;&#24212;&#29992;&#20013;&#65292;&#31574;&#30053;&#38598;&#26159;&#39640;&#32500;&#30340;&#65292;&#22240;&#27492;&#22522;&#20110;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#19981;&#33021;&#36820;&#22238;&#39640;&#31934;&#24230;&#30340;&#35299;&#12290;&#26412;&#25991;&#24341;&#20837;&#24182;&#20998;&#26512;&#20102;&#19968;&#31181;&#22522;&#20110;&#31890;&#23376;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#38024;&#23545;&#27492;&#38382;&#39064;&#20855;&#26377;&#20445;&#35777;&#30340;&#23616;&#37096;&#25910;&#25947;&#24615;&#12290;&#35813;&#26041;&#27861;&#23558;&#28151;&#21512;&#31574;&#30053;&#21442;&#25968;&#21270;&#20026;&#21407;&#23376;&#27979;&#24230;&#65292;&#24182;&#23545;&#21407;&#23376;&#30340;&#26435;&#37325;&#21644;&#20301;&#32622;&#24212;&#29992;&#36817;&#31471;&#28857;&#26356;&#26032;&#12290;&#23427;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#8220;&#30456;&#20114;&#20316;&#29992;&#8221;Wasserstein-Fisher-Rao&#26799;&#24230;&#27969;&#30340;&#26102;&#38388;&#38544;&#24335;&#31163;&#25955;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#38750;&#36864;&#21270;&#30340;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20174;&#20219;&#20309;&#21021;&#22987;&#21270;&#20197;&#25351;&#25968;&#36895;&#24230;&#25910;&#25947;&#20110;&#20934;&#30830;&#30340;&#28151;&#21512;&#32435;&#20160;&#22343;&#34913;&#65292;&#24182;&#25552;&#20379;&#25968;&#20540;&#23454;&#39564;&#26469;&#35828;&#26126;&#35813;&#26041;&#27861;&#30340;&#23454;&#38469;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of computing mixed Nash equilibria of two-player zero-sum games with continuous sets of pure strategies and with first-order access to the payoff function. This problem arises for example in game-theory-inspired machine learning applications, such as distributionally-robust learning. In those applications, the strategy sets are high-dimensional and thus methods based on discretisation cannot tractably return high-accuracy solutions.  In this paper, we introduce and analyze a particle-based method that enjoys guaranteed local convergence for this problem. This method consists in parametrizing the mixed strategies as atomic measures and applying proximal point updates to both the atoms' weights and positions. It can be interpreted as a time-implicit discretization of the "interacting" Wasserstein-Fisher-Rao gradient flow.  We prove that, under non-degeneracy assumptions, this method converges at an exponential rate to the exact mixed Nash equilibrium from any init
&lt;/p&gt;</description></item></channel></rss>