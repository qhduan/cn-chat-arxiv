<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25193;&#25955;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#36755;&#20837;&#32423;&#21518;&#38376;&#26816;&#27979;&#65292;&#24357;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#24182;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#30340;&#30333;&#30418;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2404.01101</link><description>&lt;p&gt;
UFID: &#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#25193;&#25955;&#27169;&#22411;&#19978;&#30340;&#36755;&#20837;&#32423;&#21518;&#38376;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
UFID: A Unified Framework for Input-level Backdoor Detection on Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01101
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#29992;&#20110;&#36755;&#20837;&#32423;&#21518;&#38376;&#26816;&#27979;&#65292;&#24357;&#34917;&#20102;&#35813;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#24182;&#19981;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#30340;&#30333;&#30418;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;&#24694;&#24847;&#25915;&#20987;&#32773;&#22312;&#35757;&#32451;&#38454;&#27573;&#36890;&#36807;&#23545;&#37096;&#20998;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#27602;&#21270;&#26469;&#27880;&#20837;&#21518;&#38376;&#12290;&#20026;&#20102;&#20943;&#36731;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#23545;&#21518;&#38376;&#26816;&#27979;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#20154;&#20026;&#25193;&#25955;&#27169;&#22411;&#35774;&#35745;&#20102;&#19987;&#38376;&#30340;&#21518;&#38376;&#26816;&#27979;&#26041;&#27861;&#65292;&#20351;&#24471;&#36825;&#19968;&#39046;&#22495;&#36739;&#23569;&#34987;&#25506;&#32034;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#24456;&#38590;&#36731;&#26494;&#22320;&#23558;&#20854;&#36866;&#24212;&#29983;&#25104;&#20219;&#21153;&#19978;&#30340;&#21518;&#38376;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#35775;&#38382;&#27169;&#22411;&#26435;&#37325;&#21644;&#26550;&#26500;&#30340;&#30333;&#30418;&#35775;&#38382;&#65292;&#25110;&#27010;&#29575;logits&#20316;&#20026;&#39069;&#22806;&#20449;&#24687;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#20999;&#23454;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01101v1 Announce Type: cross  Abstract: Diffusion Models are vulnerable to backdoor attacks, where malicious attackers inject backdoors by poisoning some parts of the training samples during the training stage. This poses a serious threat to the downstream users, who query the diffusion models through the API or directly download them from the internet. To mitigate the threat of backdoor attacks, there have been a plethora of investigations on backdoor detections. However, none of them designed a specialized backdoor detection method for diffusion models, rendering the area much under-explored. Moreover, these prior methods mainly focus on the traditional neural networks in the classification task, which cannot be adapted to the backdoor detections on the generative task easily. Additionally, most of the prior methods require white-box access to model weights and architectures, or the probability logits as additional information, which are not always practical. In this paper
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;</title><link>https://arxiv.org/abs/2403.19871</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#24930;&#21464;&#21270;&#30340;&#24207;&#21015;&#23454;&#29616;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19871
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#36138;&#23146;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#32771;&#34385;&#36890;&#36807;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#28436;&#21464;&#26469;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36890;&#36807;&#19981;&#21516;&#30340;&#25968;&#25454;&#25209;&#27425;&#26356;&#26032;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20445;&#30041;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265; - &#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#23454;&#26045;&#31616;&#26131;&#24615;&#21644;&#19982;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201; - &#36890;&#36807;&#20351;&#29992;&#21487;&#20197;&#30452;&#25509;&#32435;&#20837;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#23450;&#20041;&#23450;&#20041;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#27604;&#36138;&#23146;&#35757;&#32451;&#27169;&#22411;&#26356;&#24378;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22522;&#20110;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#22312;&#32447;&#24615;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#29616;&#26377;&#32467;&#26524;&#30340;&#29109;&#27491;&#21017;&#21270;&#35823;&#24046;&#20272;&#35745;&#65292;&#24182;&#23545;&#25200;&#21160;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#21644;&#33258;&#28982;&#26799;&#24230;&#27969;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.19448</link><description>&lt;p&gt;
Fisher-Rao&#32447;&#24615;&#35268;&#21010;&#21644;&#29366;&#24577;-&#21160;&#20316;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#30340;&#26799;&#24230;&#27969;
&lt;/p&gt;
&lt;p&gt;
Fisher-Rao Gradient Flows of Linear Programs and State-Action Natural Policy Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19448
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22522;&#20110;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#22312;&#32447;&#24615;&#35268;&#21010;&#20013;&#30340;&#24212;&#29992;&#65292;&#23637;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#29616;&#26377;&#32467;&#26524;&#30340;&#29109;&#27491;&#21017;&#21270;&#35823;&#24046;&#20272;&#35745;&#65292;&#24182;&#23545;&#25200;&#21160;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#21644;&#33258;&#28982;&#26799;&#24230;&#27969;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kakade&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36817;&#24180;&#26469;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#34920;&#26126;&#22312;&#26377;&#25110;&#26080;&#27491;&#21017;&#21270;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#32447;&#24615;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21478;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;-&#21160;&#20316;&#20998;&#24067;&#30340;Fisher&#20449;&#24687;&#30697;&#38453;&#30340;&#33258;&#28982;&#26799;&#24230;&#26041;&#27861;&#65292;&#20294;&#22312;&#29702;&#35770;&#26041;&#38754;&#25509;&#21463;&#24230;&#36739;&#20302;&#12290;&#22312;&#36825;&#37324;&#65292;&#29366;&#24577;-&#21160;&#20316;&#20998;&#24067;&#22312;&#29366;&#24577;-&#21160;&#20316;&#22810;&#38754;&#20307;&#20869;&#36981;&#24490;Fisher-Rao&#26799;&#24230;&#27969;&#65292;&#30456;&#23545;&#20110;&#32447;&#24615;&#21183;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26356;&#20840;&#38754;&#22320;&#30740;&#31350;&#32447;&#24615;&#35268;&#21010;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#65292;&#24182;&#26174;&#31034;&#20102;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#20854;&#36895;&#29575;&#21462;&#20915;&#20110;&#32447;&#24615;&#35268;&#21010;&#30340;&#20960;&#20309;&#29305;&#24615;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#25552;&#20379;&#20102;&#32447;&#24615;&#35268;&#21010;&#30340;&#29109;&#27491;&#21017;&#21270;&#24341;&#36215;&#30340;&#35823;&#24046;&#20272;&#35745;&#65292;&#36825;&#25913;&#36827;&#20102;&#29616;&#26377;&#32467;&#26524;&#12290;&#25105;&#20204;&#25299;&#23637;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#23637;&#31034;&#20102;&#23545;&#25200;&#21160;&#30340;Fisher-Rao&#26799;&#24230;&#27969;&#21644;&#33258;&#28982;&#26799;&#24230;&#27969;&#30340;&#27425;&#32447;&#24615;&#25910;&#25947;&#24615;&#65292;&#30452;&#21040;&#36924;&#36817;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19448v1 Announce Type: cross  Abstract: Kakade's natural policy gradient method has been studied extensively in the last years showing linear convergence with and without regularization. We study another natural gradient method which is based on the Fisher information matrix of the state-action distributions and has received little attention from the theoretical side. Here, the state-action distributions follow the Fisher-Rao gradient flow inside the state-action polytope with respect to a linear potential. Therefore, we study Fisher-Rao gradient flows of linear programs more generally and show linear convergence with a rate that depends on the geometry of the linear program. Equivalently, this yields an estimate on the error induced by entropic regularization of the linear program which improves existing results. We extend these results and show sublinear convergence for perturbed Fisher-Rao gradient flows and natural gradient flows up to an approximation error. In particul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;Mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#25509;&#21463;&#38750;&#27491;&#24335;&#26597;&#35810;&#24182;&#25214;&#21040;&#30456;&#20851;&#23450;&#29702;&#65292;&#20026;&#35299;&#20915;&#22312;mathlib4&#20013;&#25628;&#32034;&#22256;&#38590;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13310</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;Mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;
&lt;/p&gt;
&lt;p&gt;
A Semantic Search Engine for Mathlib4
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;Mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#33021;&#22815;&#25509;&#21463;&#38750;&#27491;&#24335;&#26597;&#35810;&#24182;&#25214;&#21040;&#30456;&#20851;&#23450;&#29702;&#65292;&#20026;&#35299;&#20915;&#22312;mathlib4&#20013;&#25628;&#32034;&#22256;&#38590;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#20114;&#24335;&#23450;&#29702;&#35777;&#26126;&#22120;Lean&#20351;&#24471;&#21487;&#20197;&#39564;&#35777;&#27491;&#24335;&#25968;&#23398;&#35777;&#26126;&#65292;&#24182;&#19988;&#24471;&#21040;&#19968;&#20010;&#19981;&#26029;&#25193;&#22823;&#30340;&#31038;&#21306;&#30340;&#25903;&#25345;&#12290;&#35813;&#29983;&#24577;&#31995;&#32479;&#30340;&#26680;&#24515;&#26159;&#20854;&#25968;&#23398;&#24211;mathlib4&#65292;&#20026;&#25193;&#23637;&#33539;&#22260;&#30340;&#25968;&#23398;&#29702;&#35770;&#30340;&#24418;&#24335;&#21270;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#22312;mathlib4&#20013;&#25628;&#32034;&#23450;&#29702;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25104;&#21151;&#22312;mathlib4&#20013;&#25628;&#32034;&#65292;&#29992;&#25143;&#36890;&#24120;&#38656;&#35201;&#29087;&#24713;&#20854;&#21629;&#21517;&#32422;&#23450;&#25110;&#25991;&#26723;&#23383;&#31526;&#20018;&#12290;&#22240;&#27492;&#65292;&#21019;&#24314;&#19968;&#20010;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#34987;&#20855;&#26377;&#19981;&#21516;&#29087;&#24713;&#31243;&#24230;&#30340;mathlib4&#30340;&#20010;&#20154;&#20351;&#29992;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;mathlib4&#30340;&#35821;&#20041;&#25628;&#32034;&#24341;&#25806;&#65292;&#21487;&#20197;&#25509;&#21463;&#38750;&#27491;&#24335;&#26597;&#35810;&#24182;&#25214;&#21040;&#30456;&#20851;&#23450;&#29702;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;mathlib4&#25628;&#32034;&#24341;&#25806;&#24615;&#33021;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13310v1 Announce Type: cross  Abstract: The interactive theorem prover, Lean, enables the verification of formal mathematical proofs and is backed by an expanding community. Central to this ecosystem is its mathematical library, mathlib4, which lays the groundwork for the formalization of an expanding range of mathematical theories. However, searching for theorems in mathlib4 can be challenging. To successfully search in mathlib4, users often need to be familiar with its naming conventions or documentation strings. Therefore, creating a semantic search engine that can be used easily by individuals with varying familiarity with mathlib4 is very important. In this paper, we present a semantic search engine for mathlib4 that accepts informal queries and finds the relevant theorems. We also establish a benchmark for assessing the performance of various search engines for mathlib4.
&lt;/p&gt;</description></item><item><title>&#32479;&#35745;&#21147;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#31232;&#30095;&#26041;&#31243;&#21457;&#29616;&#31639;&#27861;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#32423;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#26469;&#24179;&#34913;&#25968;&#25454;&#25311;&#21512;&#21644;&#31616;&#27905;&#24615;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#26497;&#38480;&#20013;&#33021;&#22815;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01723</link><description>&lt;p&gt;
&#21160;&#21147;&#23398;&#31995;&#32479;&#35782;&#21035;&#30340;&#32479;&#35745;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Statistical Mechanics of Dynamical System Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01723
&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#21147;&#23398;&#25552;&#20379;&#20102;&#19968;&#31181;&#31232;&#30095;&#26041;&#31243;&#21457;&#29616;&#31639;&#27861;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#32423;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#26469;&#24179;&#34913;&#25968;&#25454;&#25311;&#21512;&#21644;&#31616;&#27905;&#24615;&#65292;&#29305;&#21035;&#22312;&#20302;&#25968;&#25454;&#26497;&#38480;&#20013;&#33021;&#22815;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#21040;&#30340;&#22122;&#22768;&#25968;&#25454;&#20013;&#24674;&#22797;&#21160;&#21147;&#23398;&#26041;&#31243;&#26159;&#31995;&#32479;&#35782;&#21035;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#19968;&#31181;&#32479;&#35745;&#21147;&#23398;&#26041;&#27861;&#26469;&#20998;&#26512;&#31232;&#30095;&#26041;&#31243;&#21457;&#29616;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#36229;&#21442;&#25968;&#30340;&#35797;&#38169;&#36873;&#25321;&#24179;&#34913;&#25968;&#25454;&#25311;&#21512;&#21644;&#31616;&#27905;&#24615;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#32479;&#35745;&#21147;&#23398;&#25552;&#20379;&#20102;&#20998;&#26512;&#22797;&#26434;&#24615;&#21644;&#36866;&#24212;&#24615;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#24037;&#20855;&#65292;&#31867;&#20284;&#20110;&#29109;&#19982;&#33021;&#37327;&#20043;&#38388;&#30340;&#20998;&#26512;&#12290;&#20026;&#20102;&#24314;&#31435;&#36825;&#31181;&#31867;&#27604;&#65292;&#25105;&#20204;&#23558;&#20248;&#21270;&#36807;&#31243;&#23450;&#20041;&#20026;&#19968;&#20010;&#23558;&#21464;&#37327;&#36873;&#25321;&#19982;&#31995;&#25968;&#20540;&#20998;&#24320;&#30340;&#20004;&#32423;&#36125;&#21494;&#26031;&#25512;&#26029;&#38382;&#39064;&#65292;&#24182;&#20351;&#24471;&#21518;&#39564;&#21442;&#25968;&#20998;&#24067;&#21487;&#20197;&#20197;&#38381;&#24335;&#24418;&#24335;&#35745;&#31639;&#12290;&#37319;&#29992;&#32479;&#35745;&#21147;&#23398;&#27010;&#24565;&#65288;&#22914;&#33258;&#30001;&#33021;&#21644;&#37197;&#20998;&#20989;&#25968;&#65289;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#22312;&#20110;&#22312;&#20302;&#25968;&#25454;&#26497;&#38480;&#20013;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#32463;&#24120;&#36935;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01723v1 Announce Type: cross  Abstract: Recovering dynamical equations from observed noisy data is the central challenge of system identification. We develop a statistical mechanical approach to analyze sparse equation discovery algorithms, which typically balance data fit and parsimony through a trial-and-error selection of hyperparameters. In this framework, statistical mechanics offers tools to analyze the interplay between complexity and fitness, in analogy to that done between entropy and energy. To establish this analogy, we define the optimization procedure as a two-level Bayesian inference problem that separates variable selection from coefficient values and enables the computation of the posterior parameter distribution in closed form. A key advantage of employing statistical mechanical concepts, such as free energy and the partition function, is in the quantification of uncertainty, especially in in the low-data limit; frequently encountered in real-world applicati
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#23637;&#31034;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#23558;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#26174;&#33879;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.18164</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#36890;&#29992;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#23458;&#25143;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-based General Purpose Representation Learning for Customer Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18164
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#23637;&#31034;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#65292;&#24182;&#23558;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#26174;&#33879;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#21033;&#29992;&#25968;&#25454;&#30340;&#39046;&#22495;&#29305;&#23450;&#22522;&#30784;&#32467;&#26500;&#21450;&#20854;&#29983;&#25104;&#22240;&#32032;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#65292;&#22312;&#21508;&#31181;&#29992;&#20363;&#26080;&#20851;&#24212;&#29992;&#20013;&#21462;&#24471;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#36890;&#36807;&#22810;&#32500;&#21521;&#37327;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#34920;&#31034;&#36825;&#20123;&#32467;&#26500;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#26694;&#26550;&#29992;&#20110;&#26500;&#24314;&#36890;&#29992;&#23884;&#20837;&#65292;&#35780;&#20272;&#20102;&#19981;&#21516;&#33258;&#21160;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#24615;&#33021;&#65292;&#24182;&#23637;&#31034;&#20102;&#31616;&#21333;&#27169;&#22411;&#22312;&#23884;&#20837;&#39640;&#24230;&#22797;&#26434;&#34920;&#26684;&#25968;&#25454;&#26102;&#20248;&#20110;&#22797;&#26434;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#24212;&#29992;&#20110;&#29983;&#25104;&#25554;&#25300;&#24335;&#12289;&#20016;&#23500;&#21644;&#21311;&#21517;&#21270;&#30340;&#34920;&#31034;AWS&#23458;&#25143;&#30340;&#23884;&#20837;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#27169;&#22411;&#65292;&#33410;&#30465;&#24320;&#21457;&#26102;&#38388;&#39640;&#36798;45&#65285;&#65292;&#24182;&#35266;&#23519;&#21040;&#19979;&#28216;&#27169;&#22411;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#20110;&#22810;&#23618;&#25910;&#32553;&#33258;&#21160;&#32534;&#30721;&#22120;&#37325;&#26500;&#25439;&#22833;&#35745;&#31639;&#30340;&#37325;&#35201;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18164v1 Announce Type: cross  Abstract: In recent years, exploiting the domain-specific underlying structure of data and its generative factors for representation learning has shown success in various use-case agnostic applications. However, the diversity and complexity of tabular data have made it challenging to represent these structures in a latent space through multi-dimensional vectors. We design an autoencoder-based framework for building general purpose embeddings, we assess the performance of different autoencoder architectures, and show simpler models outperform complex ones in embedding highly complex tabular data. We apply our framework to produce plug-and-play, rich, and anonymized embeddings representing AWS customers for usage in any model, saving up to 45% of development time, and observe significant improvements in downstream models. Moreover, we propose a significant improvement to the calculation of reconstruction loss for multi-layer contractive autoencode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#30334;&#19975;&#38271;&#24230;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#35821;&#35328;&#30340;&#25991;&#26412;&#30693;&#35782;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;AI&#36741;&#21161;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.08268</link><description>&lt;p&gt;
&#30334;&#19975;&#38271;&#24230;&#35270;&#39057;&#21644;&#35821;&#35328;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
World Model on Million-Length Video And Language With RingAttention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08268
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20351;&#29992;&#30334;&#19975;&#38271;&#24230;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#30340;&#29615;&#24418;&#27880;&#24847;&#21147;&#19990;&#30028;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#20449;&#24687;&#21644;&#35821;&#35328;&#30340;&#25991;&#26412;&#30693;&#35782;&#20197;&#21450;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;AI&#36741;&#21161;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#29702;&#35299;&#38590;&#20197;&#29992;&#25991;&#23383;&#25551;&#36848;&#30340;&#19990;&#30028;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#19988;&#22312;&#22788;&#29702;&#22797;&#26434;&#30340;&#38271;&#31687;&#20219;&#21153;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#35270;&#39057;&#24207;&#21015;&#25552;&#20379;&#20102;&#21482;&#26377;&#35821;&#35328;&#21644;&#38745;&#24577;&#22270;&#20687;&#25152;&#19981;&#20855;&#22791;&#30340;&#23453;&#36149;&#26102;&#38388;&#20449;&#24687;&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#19982;&#35821;&#35328;&#36827;&#34892;&#32852;&#21512;&#24314;&#27169;&#26102;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290;&#36825;&#31181;&#27169;&#22411;&#21487;&#20197;&#23545;&#20154;&#31867;&#30340;&#25991;&#26412;&#30693;&#35782;&#21644;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#29702;&#35299;&#65292;&#20026;&#36741;&#21161;&#20154;&#31867;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#20154;&#24037;&#26234;&#33021;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20174;&#30334;&#19975;&#20010;&#26631;&#35760;&#30340;&#35270;&#39057;&#21644;&#35821;&#35328;&#24207;&#21015;&#20013;&#23398;&#20064;&#38754;&#20020;&#30528;&#35760;&#24518;&#32422;&#26463;&#12289;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#26377;&#38480;&#24615;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#26679;&#21270;&#35270;&#39057;&#21644;&#20070;&#31821;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#29615;&#24418;&#27880;&#24847;&#21147;&#25216;&#26415;&#23545;&#38271;&#24207;&#21015;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#35757;&#32451;&#65292;&#36880;&#28176;&#22686;&#21152;&#19978;&#19979;&#25991;&#22823;&#23567;&#20174;4K&#21040;1M&#20010;&#26631;&#35760;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#22914;&#19979;&#65306;
&lt;/p&gt;
&lt;p&gt;
Current language models fall short in understanding aspects of the world not easily described in words, and struggle with complex, long-form tasks. Video sequences offer valuable temporal information absent in language and static images, making them attractive for joint modeling with language. Such models could develop a understanding of both human textual knowledge and the physical world, enabling broader AI capabilities for assisting humans. However, learning from millions of tokens of video and language sequences poses challenges due to memory constraints, computational complexity, and limited datasets. To address these challenges, we curate a large dataset of diverse videos and books, utilize the RingAttention technique to scalably train on long sequences, and gradually increase context size from 4K to 1M tokens. This paper makes the following contributions: (a) Largest context size neural network: We train one of the largest context size transformers on long video and language seq
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37319;&#26679;&#26041;&#26696;"mix-cd"&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20248;&#20808;&#22788;&#29702;&#23454;&#38469;&#38754;&#20020;&#36951;&#24536;&#30340;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31616;&#21333;&#12289;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#33021;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#26080;&#32541;&#36816;&#29992;&#65292;&#26377;&#25928;&#22320;&#20445;&#25345;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08096</link><description>&lt;p&gt;
&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#26102;&#37325;&#26032;&#32451;&#20064;&#21738;&#20123;&#39044;&#35757;&#32451;&#26679;&#26412;&#26356;&#22909;&#65311;
&lt;/p&gt;
&lt;p&gt;
Which Pretrain Samples to Rehearse when Finetuning Pretrained Models?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#37319;&#26679;&#26041;&#26696;"mix-cd"&#65292;&#36890;&#36807;&#35782;&#21035;&#21644;&#20248;&#20808;&#22788;&#29702;&#23454;&#38469;&#38754;&#20020;&#36951;&#24536;&#30340;&#26679;&#26412;&#65292;&#20197;&#32531;&#35299;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#31616;&#21333;&#12289;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#33021;&#22312;&#29616;&#26377;&#27169;&#22411;&#20013;&#26080;&#32541;&#36816;&#29992;&#65292;&#26377;&#25928;&#22320;&#20445;&#25345;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#24050;&#25104;&#20026;&#20107;&#23454;&#19978;&#30340;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#30340;&#19968;&#20010;&#24050;&#30693;&#38382;&#39064;&#26159;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#20250;&#36951;&#24536;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#20174;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#38543;&#26426;&#36873;&#25321;&#26679;&#26412;&#26469;&#36827;&#34892;&#37325;&#26032;&#32451;&#20064;&#26159;&#32531;&#35299;&#36951;&#24536;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#38543;&#26426;&#28151;&#21512;&#19981;&#32463;&#24847;&#22320;&#21253;&#25324;&#20102;&#27169;&#22411;&#23578;&#26410;&#36951;&#24536;&#25110;&#26080;&#27861;&#23398;&#20064;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37319;&#26679;&#26041;&#26696;"mix-cd"&#65292;&#29992;&#20110;&#35782;&#21035;&#21644;&#20248;&#20808;&#22788;&#29702;&#23454;&#38469;&#38754;&#20020;&#36951;&#24536;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;"collateral damage"&#12290;&#30001;&#20110;&#30452;&#25509;&#35782;&#21035;"collateral damage"&#26679;&#26412;&#35745;&#31639;&#25104;&#26412;&#39640;&#26114;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36319;&#36394;&#24494;&#35843;&#26679;&#26412;&#30340;&#32479;&#35745;&#20449;&#24687;&#26469;&#20272;&#35745;&#36825;&#31867;&#26679;&#26412;&#20998;&#24067;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31616;&#27905;&#36731;&#37327;&#65292;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#27169;&#22411;&#20013;&#65292;&#20855;&#26377;&#26377;&#25928;&#22320;&#20445;&#25345;&#39044;&#35757;&#32451;&#24615;&#33021;&#32780;&#26080;&#38656;&#39069;&#22806;&#35745;&#31639;&#24320;&#38144;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning pretrained foundational models on specific tasks is now the de facto approach for text and vision tasks. A known pitfall of this approach is the forgetting of pretraining knowledge that happens during finetuning. Rehearsing samples randomly from the pretrain dataset is a common approach to alleviate such forgetting. However, we find that random mixing unintentionally includes samples which are not (yet) forgotten or unlearnable by the model. We propose a novel sampling scheme, mix-cd, that identifies and prioritizes samples that actually face forgetting, which we call collateral damage. Since directly identifying collateral damage samples is computationally expensive, we propose a procedure to estimate the distribution of such samples by tracking the statistics of finetuned samples. Our approach is lightweight, easy to implement, and can be seamlessly integrated into existing models, offering an effective means to retain pretrain performance without additional computational
&lt;/p&gt;</description></item><item><title>IDENAS&#26159;&#19968;&#31181;&#38598;&#25104;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#37096;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17250</link><description>&lt;p&gt;
IDENAS: &#20869;&#37096;&#20381;&#36182;&#24615;&#25506;&#32034;&#29992;&#20110;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
IDENAS: Internal Dependency Exploration for Neural Architecture Search. (arXiv:2310.17250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17250
&lt;/p&gt;
&lt;p&gt;
IDENAS&#26159;&#19968;&#31181;&#38598;&#25104;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#21644;&#29305;&#24449;&#36873;&#25321;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25506;&#32034;&#20869;&#37096;&#20381;&#36182;&#24615;&#26469;&#25552;&#39640;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26159;&#20174;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#21644;&#36827;&#34892;&#21508;&#31181;&#39044;&#27979;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#20256;&#32479;&#31639;&#27861;&#20381;&#36182;&#20110;&#26126;&#30830;&#23450;&#20041;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#65292;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#20043;&#38388;&#30340;&#21306;&#21035;&#20197;&#21450;&#27169;&#22411;&#30340;&#24213;&#23618;&#20851;&#32852;&#65288;&#36755;&#20837;&#21644;&#36755;&#20986;&#65289;&#23618;&#26159;&#26410;&#30693;&#30340;&#12290;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#21644;&#29305;&#24449;&#36873;&#25321;&#24050;&#25104;&#20026;&#36825;&#20123;&#22330;&#26223;&#20013;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;IDENAS&#65292;&#19968;&#31181;&#22522;&#20110;&#20869;&#37096;&#20381;&#36182;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#23558;NAS&#19982;&#29305;&#24449;&#36873;&#25321;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#28041;&#21450;1D&#20256;&#24863;&#22120;&#21644;2D&#22270;&#20687;&#25968;&#25454;&#30340;&#20998;&#31867;&#38382;&#39064;&#20013;&#25506;&#32034;&#20102;&#23436;&#25972;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#20869;&#37096;&#20381;&#36182;&#24615;&#12290;IDENAS&#37319;&#29992;&#20102;&#20462;&#25913;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#21644;&#39034;&#24207;&#21069;&#21521;&#25628;&#32034;&#65288;SFS&#65289;&#31639;&#27861;&#65292;&#23558;&#36755;&#20837;-&#36755;&#20986;&#37197;&#32622;&#25628;&#32034;&#19982;&#23884;&#20837;&#24335;&#29305;&#24449;&#36873;&#25321;&#30456;&#32467;&#21512;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;IDENAS&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is a powerful tool for extracting valuable information and making various predictions from diverse datasets. Traditional algorithms rely on well-defined input and output variables however, there are scenarios where the distinction between the input and output variables and the underlying, associated (input and output) layers of the model, are unknown. Neural Architecture Search (NAS) and Feature Selection have emerged as promising solutions in such scenarios. This research proposes IDENAS, an Internal Dependency-based Exploration for Neural Architecture Search, integrating NAS with feature selection. The methodology explores internal dependencies in the complete parameter space for classification involving 1D sensor and 2D image data as well. IDENAS employs a modified encoder-decoder model and the Sequential Forward Search (SFS) algorithm, combining input-output configuration search with embedded feature selection. Experimental results demonstrate IDENASs superior perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20266;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#26368;&#23567;&#35201;&#27714;&#30340;&#20844;&#29702;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#33021;&#30830;&#20445;&#40657;&#30418;&#20248;&#21270;&#25910;&#25947;&#24615;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.09766</link><description>&lt;p&gt;
&#20266;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Pseudo-Bayesian Optimization. (arXiv:2310.09766v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20266;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#26368;&#23567;&#35201;&#27714;&#30340;&#20844;&#29702;&#26694;&#26550;&#65292;&#26500;&#24314;&#20102;&#33021;&#30830;&#20445;&#40657;&#30418;&#20248;&#21270;&#25910;&#25947;&#24615;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#26114;&#36149;&#40657;&#30418;&#20989;&#25968;&#30340;&#27969;&#34892;&#26041;&#27861;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#20351;&#29992;&#19968;&#20010;&#26367;&#20195;&#27169;&#22411;&#26469;&#36817;&#20284;&#30446;&#26631;&#65292;&#24182;&#19988;&#37325;&#35201;&#30340;&#26159;&#37327;&#21270;&#30456;&#20851;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#25506;&#32034;&#21644;&#24320;&#21457;&#20043;&#38388;&#30340;&#24179;&#34913;&#30340;&#39034;&#24207;&#25628;&#32034;&#12290;&#39640;&#26031;&#36807;&#31243;(GP)&#19968;&#30452;&#26159;&#26367;&#20195;&#27169;&#22411;&#30340;&#39318;&#36873;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#36125;&#21494;&#26031;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33021;&#21147;&#21644;&#24314;&#27169;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#25361;&#25112;&#20063;&#24341;&#21457;&#20102;&#19968;&#31995;&#21015;&#25910;&#25947;&#24615;&#26356;&#26174;&#24471;&#19981;&#26126;&#26174;&#30340;&#22791;&#36873;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#30740;&#31350;&#24341;&#20986;&#26368;&#23567;&#35201;&#27714;&#30340;&#20844;&#29702;&#26694;&#26550;&#26469;&#30830;&#20445;&#40657;&#30418;&#20248;&#21270;&#30340;&#25910;&#25947;&#24615;&#65292;&#20197;&#24212;&#29992;&#20110;&#38500;&#20102;GP&#30456;&#20851;&#26041;&#27861;&#20043;&#22806;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#20013;&#30340;&#35774;&#35745;&#33258;&#30001;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20266;&#36125;&#21494;&#26031;&#20248;&#21270;&#65292;&#26469;&#26500;&#24314;&#32463;&#39564;&#19978;&#26356;&#20248;&#30340;&#31639;&#27861;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#31616;&#21333;&#30340;&#23616;&#37096;&#22238;&#24402;&#21644;&#19968;&#20010;&#36866;&#24212;&#38382;&#39064;&#29305;&#24615;&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization is a popular approach for optimizing expensive black-box functions. Its key idea is to use a surrogate model to approximate the objective and, importantly, quantify the associated uncertainty that allows a sequential search of query points that balance exploitation-exploration. Gaussian process (GP) has been a primary candidate for the surrogate model, thanks to its Bayesian-principled uncertainty quantification power and modeling flexibility. However, its challenges have also spurred an array of alternatives whose convergence properties could be more opaque. Motivated by these, we study in this paper an axiomatic framework that elicits the minimal requirements to guarantee black-box optimization convergence that could apply beyond GP-related methods. Moreover, we leverage the design freedom in our framework, which we call Pseudo-Bayesian Optimization, to construct empirically superior algorithms. In particular, we show how using simple local regression, and a sui
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelfFed&#30340;&#33258;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#20998;&#25955;&#35757;&#32451;&#21644;&#22686;&#24378;&#24314;&#27169;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01514</link><description>&lt;p&gt;
SelfFed: &#33258;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#29992;&#20110;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SelfFed: Self-supervised Federated Learning for Data Heterogeneity and Label Scarcity in IoMT. (arXiv:2307.01514v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01514
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelfFed&#30340;&#33258;&#30417;&#30563;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;IoMT&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#21294;&#20047;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#38454;&#27573;&#65292;&#36890;&#36807;&#20998;&#25955;&#35757;&#32451;&#21644;&#22686;&#24378;&#24314;&#27169;&#26469;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#24335;&#22312;&#34892;&#19994;&#21644;&#30740;&#31350;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#24456;&#22823;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21327;&#20316;&#23398;&#20064;&#26410;&#26631;&#35760;&#20294;&#23396;&#31435;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#33258;&#30417;&#30563;&#30340;&#32852;&#37030;&#23398;&#20064;&#31574;&#30053;&#22312;&#26631;&#31614;&#31232;&#32570;&#21644;&#25968;&#25454;&#24322;&#36136;&#24615;&#65288;&#21363;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#65289;&#26041;&#38754;&#23384;&#22312;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#21307;&#30103;&#29289;&#32852;&#32593;&#65288;IoMT&#65289;&#30340;SelfFed&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;SelfFed&#26694;&#26550;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#31532;&#19968;&#20010;&#38454;&#27573;&#26159;&#39044;&#35757;&#32451;&#33539;&#24335;&#65292;&#20351;&#29992;&#22522;&#20110;Swin Transformer&#30340;&#32534;&#30721;&#22120;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#36827;&#34892;&#22686;&#24378;&#24314;&#27169;&#12290;SelfFed&#26694;&#26550;&#30340;&#31532;&#19968;&#20010;&#38454;&#27573;&#26377;&#21161;&#20110;&#20811;&#26381;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#31532;&#20108;&#20010;&#38454;&#27573;&#26159;&#24494;&#35843;&#33539;&#24335;&#65292;&#24341;&#20837;&#23545;&#27604;&#32593;&#32476;&#21644;&#19968;&#31181;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;&#26032;&#22411;&#32858;&#21512;&#31574;&#30053;&#65292;&#29992;&#20110;&#30446;&#26631;&#20219;&#21153;&#30340;&#20998;&#25955;&#35757;&#32451;&#12290;&#36825;&#20010;&#24494;&#35843;&#38454;&#27573;&#20811;&#26381;&#20102;&#26631;&#31614;&#31232;&#32570;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning in federated learning paradigm has been gaining a lot of interest both in industry and research due to the collaborative learning capability on unlabeled yet isolated data. However, self-supervised based federated learning strategies suffer from performance degradation due to label scarcity and diverse data distributions, i.e., data heterogeneity. In this paper, we propose the SelfFed framework for Internet of Medical Things (IoMT). Our proposed SelfFed framework works in two phases. The first phase is the pre-training paradigm that performs augmentive modeling using Swin Transformer based encoder in a decentralized manner. The first phase of SelfFed framework helps to overcome the data heterogeneity issue. The second phase is the fine-tuning paradigm that introduces contrastive network and a novel aggregation strategy that is trained on limited labeled data for a target task in a decentralized manner. This fine-tuning stage overcomes the label scarcity problem
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#24191;&#25773;&#27169;&#22411;&#65292;&#30830;&#23450;&#20102;&#19982;$p$&#21644;$k$&#26377;&#20851;&#30340;&#38408;&#20540;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2306.01727</link><description>&lt;p&gt;
&#22312;&#38543;&#26426;&#36882;&#24402;&#26377;&#21521;&#26080;&#29615;&#22270;&#20013;&#30340;&#24191;&#25773;
&lt;/p&gt;
&lt;p&gt;
Broadcasting in random recursive dags. (arXiv:2306.01727v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01727
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#24191;&#25773;&#27169;&#22411;&#65292;&#30830;&#23450;&#20102;&#19982;$p$&#21644;$k$&#26377;&#20851;&#30340;&#38408;&#20540;&#65292;&#24182;&#35752;&#35770;&#20102;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#22343;&#21248;&#30340;$k$-dag&#36890;&#36807;&#20174;&#29616;&#26377;&#33410;&#28857;&#20013;&#22343;&#21248;&#38543;&#26426;&#36873;&#25321;$k$&#20010;&#29238;&#33410;&#28857;&#26469;&#25512;&#24191;&#22343;&#21248;&#30340;&#38543;&#26426;&#36882;&#24402;&#26641;&#12290;&#23427;&#20197;$k$&#20010;&#8220;&#26681;&#8221;&#24320;&#22987;&#12290;&#27599;&#20010;$k$&#20010;&#26681;&#33410;&#28857;&#37117;&#34987;&#20998;&#37197;&#19968;&#20010;&#20301;&#12290;&#36825;&#20123;&#20301;&#36890;&#36807;&#19968;&#20010;&#22024;&#26434;&#30340;&#20449;&#36947;&#20256;&#25773;&#12290;&#27599;&#20010;&#29238;&#33410;&#28857;&#30340;&#20301;&#37117;&#20197;&#27010;&#29575;$p$&#21457;&#29983;&#21464;&#21270;&#65292;&#24182;&#36827;&#34892;&#22823;&#22810;&#25968;&#34920;&#20915;&#12290;&#24403;&#25152;&#26377;&#33410;&#28857;&#37117;&#25509;&#25910;&#21040;&#23427;&#20204;&#30340;&#20301;&#21518;&#65292;$k$-dag&#34987;&#26174;&#31034;&#65292;&#19981;&#35782;&#21035;&#26681;&#33410;&#28857;&#12290;&#30446;&#26631;&#26159;&#20272;&#35745;&#25152;&#26377;&#26681;&#33410;&#28857;&#20013;&#30340;&#22823;&#22810;&#25968;&#20301;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;$p$&#30340;&#38408;&#20540;&#65292;&#20316;&#20026;&#19968;&#20010;&#20851;&#20110;$k$&#30340;&#20989;&#25968;&#65292;&#20351;&#24471;&#25152;&#26377;&#33410;&#28857;&#30340;&#22823;&#22810;&#25968;&#35268;&#21017;&#20135;&#29983;&#38169;&#35823;$c+o(1)$&#30340;&#27010;&#29575;&#23567;&#20110;$1/2$&#12290;&#22312;&#38408;&#20540;&#20197;&#19978;&#65292;&#22823;&#22810;&#25968;&#35268;&#21017;&#30340;&#38169;&#35823;&#27010;&#29575;&#20026;$1/2+o(1)$&#12290;
&lt;/p&gt;
&lt;p&gt;
A uniform $k$-{\sc dag} generalizes the uniform random recursive tree by picking $k$ parents uniformly at random from the existing nodes. It starts with $k$ ''roots''. Each of the $k$ roots is assigned a bit. These bits are propagated by a noisy channel. The parents' bits are flipped with probability $p$, and a majority vote is taken. When all nodes have received their bits, the $k$-{\sc dag} is shown without identifying the roots. The goal is to estimate the majority bit among the roots. We identify the threshold for $p$ as a function of $k$ below which the majority rule among all nodes yields an error $c+o(1)$ with $c&lt;1/2$. Above the threshold the majority rule errs with probability $1/2+o(1)$.
&lt;/p&gt;</description></item></channel></rss>