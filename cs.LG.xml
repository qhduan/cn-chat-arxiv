<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19243</link><description>&lt;p&gt;
&#29992;&#27491;&#24358;&#28608;&#27963;&#30340;&#20302;&#31209;&#30697;&#38453;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Sine Activated Low-Rank Matrices for Parameter Efficient Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19243
&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#27491;&#24358;&#20989;&#25968;&#21040;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#20445;&#25345;&#21442;&#25968;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#20998;&#35299;&#24050;&#32463;&#25104;&#20026;&#22312;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#20013;&#22686;&#24378;&#21442;&#25968;&#25928;&#29575;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#36825;&#20123;&#25216;&#26415;&#26174;&#33879;&#38477;&#20302;&#20102;&#21442;&#25968;&#25968;&#37327;&#65292;&#21462;&#24471;&#20102;&#31616;&#27905;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#26159;&#22312;&#21442;&#25968;&#25928;&#29575;&#21644;&#27169;&#22411;&#20934;&#30830;&#24615;&#20043;&#38388;&#20570;&#20986;&#22949;&#21327;&#65292;&#21442;&#25968;&#20943;&#23569;&#24448;&#24448;&#23548;&#33268;&#20934;&#30830;&#24615;&#19981;&#21450;&#23436;&#25972;&#31209;&#23545;&#24212;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#22312;&#20302;&#31209;&#20998;&#35299;&#36807;&#31243;&#20013;&#25972;&#21512;&#20102;&#19968;&#20010;&#27491;&#24358;&#20989;&#25968;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#20445;&#30041;&#20102;&#20302;&#31209;&#26041;&#27861;&#30340;&#21442;&#25968;&#25928;&#29575;&#29305;&#24615;&#30340;&#22909;&#22788;&#65292;&#36824;&#22686;&#21152;&#20102;&#20998;&#35299;&#30340;&#31209;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#34987;&#35777;&#26126;&#26159;&#29616;&#26377;&#20302;&#31209;&#27169;&#22411;&#30340;&#19968;&#31181;&#36866;&#24212;&#24615;&#22686;&#24378;&#65292;&#27491;&#22914;&#20854;&#25104;&#21151;&#35777;&#23454;&#30340;&#37027;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19243v1 Announce Type: new  Abstract: Low-rank decomposition has emerged as a vital tool for enhancing parameter efficiency in neural network architectures, gaining traction across diverse applications in machine learning. These techniques significantly lower the number of parameters, striking a balance between compactness and performance. However, a common challenge has been the compromise between parameter efficiency and the accuracy of the model, where reduced parameters often lead to diminished accuracy compared to their full-rank counterparts. In this work, we propose a novel theoretical framework that integrates a sinusoidal function within the low-rank decomposition process. This approach not only preserves the benefits of the parameter efficiency characteristic of low-rank methods but also increases the decomposition's rank, thereby enhancing model accuracy. Our method proves to be an adaptable enhancement for existing low-rank models, as evidenced by its successful 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.18035</link><description>&lt;p&gt;
&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bidirectional Consistency Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18035
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#32479;&#19968;&#20102;&#29983;&#25104;&#21644;&#32534;&#36753;&#22270;&#20687;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DMs&#65289;&#36890;&#36807;&#36845;&#20195;&#21435;&#22122;&#19968;&#20010;&#38543;&#26426;&#21521;&#37327;&#33021;&#22815;&#29983;&#25104;&#38750;&#24120;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#36825;&#20010;&#36807;&#31243;&#23545;&#24212;&#20110;&#27839;&#30528;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;PF ODE&#65289;&#31227;&#21160;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;DMs&#36824;&#21487;&#20197;&#36890;&#36807;&#27839;&#30528;PF ODE&#21521;&#21518;&#31227;&#21160;&#23558;&#36755;&#20837;&#22270;&#20687;&#36716;&#25442;&#20026;&#22122;&#22768;&#65292;&#36825;&#26159;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#25554;&#20540;&#21644;&#22270;&#20687;&#32534;&#36753;&#65289;&#30340;&#20851;&#38190;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#30340;&#36845;&#20195;&#24615;&#36136;&#38480;&#21046;&#20102;&#20854;&#36895;&#24230;&#65292;&#38459;&#30861;&#20102;&#20854;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#26368;&#36817;&#65292;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CMs&#65289;&#24050;&#32463;&#20986;&#29616;&#65292;&#20197;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#36890;&#36807;&#36817;&#20284;PF ODE&#30340;&#31215;&#20998;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;&#38656;&#35201;&#36845;&#20195;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#26174;&#24335;ODE&#27714;&#35299;&#22120;&#20351;&#24471;&#21453;&#28436;&#36807;&#31243;&#22797;&#26434;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21452;&#21521;&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;BCM&#65289;&#65292;&#23398;&#20064;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#21516;&#26102;&#23454;&#29616;&#27839;&#30528;PF ODE&#30340;&#21069;&#21521;&#21644;&#21518;&#21521;&#36941;&#21382;&#65292;&#26377;&#25928;&#22320;&#32479;&#19968;&#29983;&#25104;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18035v1 Announce Type: new  Abstract: Diffusion models (DMs) are capable of generating remarkably high-quality samples by iteratively denoising a random vector, a process that corresponds to moving along the probability flow ordinary differential equation (PF ODE). Interestingly, DMs can also invert an input image to noise by moving backward along the PF ODE, a key operation for downstream tasks such as interpolation and image editing. However, the iterative nature of this process restricts its speed, hindering its broader application. Recently, Consistency Models (CMs) have emerged to address this challenge by approximating the integral of the PF ODE, thereby bypassing the need to iterate. Yet, the absence of an explicit ODE solver complicates the inversion process. To resolve this, we introduce the Bidirectional Consistency Model (BCM), which learns a single neural network that enables both forward and backward traversal along the PF ODE, efficiently unifying generation an
&lt;/p&gt;</description></item><item><title>&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#22823;&#37096;&#20998;&#23618;&#30340;&#31227;&#38500;&#32780;&#20445;&#25345;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;</title><link>https://arxiv.org/abs/2403.17887</link><description>&lt;p&gt;
&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#23618;&#21098;&#26525;&#30340;&#19981;&#21512;&#29702;&#26080;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Unreasonable Ineffectiveness of the Deeper Layers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17887
&lt;/p&gt;
&lt;p&gt;
&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#22823;&#37096;&#20998;&#23618;&#30340;&#31227;&#38500;&#32780;&#20445;&#25345;&#24615;&#33021;&#65292;&#21516;&#26102;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#21487;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#35745;&#31639;&#36164;&#28304;&#65292;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#31616;&#21333;&#30340;&#23618;&#21098;&#26525;&#31574;&#30053;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#31227;&#38500;&#22823;&#37096;&#20998;&#23618;&#65288;&#26368;&#39640;&#36798;&#19968;&#21322;&#65289;&#20043;&#21069;&#65292;&#19981;&#21516;&#38382;&#31572;&#22522;&#20934;&#27979;&#35797;&#30340;&#24615;&#33021;&#20960;&#20046;&#27809;&#26377;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#21098;&#26525;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#23618;&#38388;&#30340;&#30456;&#20284;&#24615;&#26469;&#30830;&#23450;&#26368;&#20339;&#30340;&#21098;&#26525;&#23618;&#22359;&#65307;&#28982;&#21518;&#65292;&#20026;&#20102;&#8220;&#20462;&#22797;&#8221;&#25439;&#23475;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#23569;&#37327;&#24494;&#35843;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#20351;&#29992;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#20855;&#20307;&#21253;&#25324;&#37327;&#21270;&#21644;&#20302;&#31209;&#36866;&#37197;&#22120;&#65288;QLoRA&#65289;&#65292;&#36825;&#26679;&#25105;&#20204;&#30340;&#27599;&#20010;&#23454;&#39564;&#37117;&#21487;&#20197;&#22312;&#21333;&#20010;A100 GPU&#19978;&#25191;&#34892;&#12290;&#20174;&#23454;&#38469;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#23618;&#21098;&#26525;&#26041;&#27861;&#21487;&#20197;&#34917;&#20805;&#20854;&#20182;PEFT&#31574;&#30053;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20943;&#23569;&#24494;&#35843;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21478;&#19968;&#26041;&#38754;&#21487;&#20197;&#25552;&#39640;&#25512;&#26029;&#30340;&#20869;&#23384;&#21644;&#24310;&#36831;&#12290;&#20174;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35813;&#30740;&#31350;&#34920;&#26126;&#28145;&#23618;&#31070;&#32463;&#32593;&#32476;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#24182;&#19988;&#23545;&#27169;&#22411;&#30340;&#21098;&#26525;&#27809;&#26377;&#22826;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17887v1 Announce Type: new  Abstract: We empirically study a simple layer-pruning strategy for popular families of open-weight pretrained LLMs, finding minimal degradation of performance on different question-answering benchmarks until after a large fraction (up to half) of the layers are removed. To prune these models, we identify the optimal block of layers to prune by considering similarity across layers; then, to "heal" the damage, we perform a small amount of finetuning. In particular, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single A100 GPU. From a practical perspective, these results suggest that layer pruning methods can complement other PEFT strategies to further reduce computational resources of finetuning on the one hand, and can improve the memory and latency of inference on the other hand. From a scientific perspective, the robustness of 
&lt;/p&gt;</description></item><item><title>Calib3D&#26159;&#19968;&#20010;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22810;&#20010;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#34429;&#28982;&#20934;&#30830;&#20294;&#19981;&#21487;&#38752;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#23433;&#20840;&#20851;&#38190;&#30340;&#32972;&#26223;&#19979;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.17010</link><description>&lt;p&gt;
Calib3D&#65306;&#26657;&#20934;&#27169;&#22411;&#20559;&#22909;&#20197;&#23454;&#29616;&#21487;&#38752;&#30340;3D&#22330;&#26223;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Calib3D: Calibrating Model Preferences for Reliable 3D Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17010
&lt;/p&gt;
&lt;p&gt;
Calib3D&#26159;&#19968;&#20010;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#22810;&#20010;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#29616;&#26377;&#27169;&#22411;&#34429;&#28982;&#20934;&#30830;&#20294;&#19981;&#21487;&#38752;&#65292;&#20174;&#32780;&#38416;&#26126;&#20102;&#23433;&#20840;&#20851;&#38190;&#30340;&#32972;&#26223;&#19979;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#30340;3D&#22330;&#26223;&#29702;&#35299;&#20219;&#21153;&#38656;&#35201;&#30340;&#19981;&#20165;&#20165;&#26159;&#20934;&#30830;&#30340;&#39044;&#27979;&#65292;&#36824;&#38656;&#35201;&#26469;&#33258;3D&#24863;&#30693;&#27169;&#22411;&#30340;&#33258;&#20449;&#39044;&#27979;&#12290;&#26412;&#30740;&#31350;&#25512;&#20986;&#20102;Calib3D&#65292;&#36825;&#26159;&#19968;&#39033;&#24320;&#21019;&#24615;&#30340;&#24037;&#20316;&#65292;&#26088;&#22312;&#20174;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#35282;&#24230;&#22522;&#20934;&#21644;&#23457;&#26597;3D&#22330;&#26223;&#29702;&#35299;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#20840;&#38754;&#35780;&#20272;&#20102;28&#20010;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22312;10&#20010;&#19981;&#21516;&#30340;3D&#25968;&#25454;&#38598;&#19978;&#65292;&#25581;&#31034;&#20102;&#33021;&#22815;&#22788;&#29702;3D&#22330;&#26223;&#29702;&#35299;&#20013;&#30340;&#35823;&#24046;&#19981;&#30830;&#23450;&#24615;&#21644;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#35265;&#22320;&#30340;&#29616;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#23613;&#31649;&#29616;&#26377;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20934;&#30830;&#24230;&#27700;&#24179;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25552;&#20379;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745; -- &#36825;&#20010;&#20851;&#38190;&#30340;&#32570;&#38519;&#20005;&#37325;&#25439;&#23475;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#25935;&#24863;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#23545;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#32593;&#32476;&#23481;&#37327;&#12289;LiDAR&#34920;&#31034;&#12289;&#20809;&#26629;&#20998;&#36776;&#29575;&#21644;3D&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#20998;&#26512;&#65292;&#25105;&#20204;&#30452;&#25509;&#23558;&#36825;&#20123;&#26041;&#38754;&#19982;&#27169;&#22411;&#26657;&#20934;&#30456;&#20851;&#32852;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17010v1 Announce Type: cross  Abstract: Safety-critical 3D scene understanding tasks necessitate not only accurate but also confident predictions from 3D perception models. This study introduces Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D scene understanding models from an uncertainty estimation viewpoint. We comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D datasets, uncovering insightful phenomena that cope with both the aleatoric and epistemic uncertainties in 3D scene understanding. We discover that despite achieving impressive levels of accuracy, existing models frequently fail to provide reliable uncertainty estimates -- a pitfall that critically undermines their applicability in safety-sensitive contexts. Through extensive analysis of key factors such as network capacity, LiDAR representations, rasterization resolutions, and 3D data augmentation techniques, we correlate these aspects directly with the model cal
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;</title><link>https://arxiv.org/abs/2403.16829</link><description>&lt;p&gt;
&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#32452;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#24674;&#22797;&#19968;&#20010;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#22870;&#21169;&#65292;&#37319;&#29992;&#38543;&#26426;&#36719;&#31574;&#30053;&#36845;&#20195;&#26356;&#26032;&#31574;&#30053;&#12290;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#20351;&#29992;$\mathcal{O}(1/\varepsilon^{2})$&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26679;&#26412;&#24674;&#22797;&#20986;&#19968;&#20010;&#20351;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;$\mathcal{O}(1/\varepsilon^{4})$&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#24674;&#22797;&#22870;&#21169;&#23545;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#19978;&#19982;&#19987;&#23478;&#31574;&#30053;$\varepsilon$-&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16829v1 Announce Type: cross  Abstract: Given a dataset of expert demonstrations, inverse reinforcement learning (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a stochastic gradient descent update for the reward and a stochastic soft policy iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\varepsilon$-optimal using $\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP). Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\varepsilon$-close to the expert policy in total variation distance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.08743</link><description>&lt;p&gt;
&#23558;LLMs&#24341;&#23548;&#21040;&#26080;&#20559;&#21709;&#24212;&#65306;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24456;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#21644;&#27495;&#35270;&#24615;&#30340;&#21709;&#24212;&#12290;&#30001;&#20110;LLMs&#28041;&#21450;&#21040;&#37325;&#35201;&#30340;&#20915;&#31574;&#21046;&#23450;&#65288;&#20363;&#22914;&#25307;&#32856;&#21644;&#21307;&#30103;&#20445;&#20581;&#65289;&#65292;&#24320;&#21457;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#31038;&#20250;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#19982;LLM&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;LLMs&#36755;&#20837;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;LLM&#25512;&#29702;&#30340;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#22240;&#26524;&#29702;&#35299;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#21435;&#20559;&#20542;LLM&#36755;&#20986;&#30340;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21435;&#20559;&#25351;&#31034;&#26041;&#27861;&#65292;&#22914;&#25233;&#21046;&#25351;&#20196;&#21644;&#19978;&#19979;&#25991;&#23545;&#27604;&#20363;&#23376;&#65292;&#24182;&#36890;&#36807;&#40723;&#21169;&#26080;&#20559;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21551;&#31034;&#20102;&#26032;&#30340;&#21435;&#20559;&#20542;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24378;&#22823;&#23454;&#35777;&#34920;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08743v1 Announce Type: cross  Abstract: Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework pr
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#21487;&#25512;&#24191;&#21644;&#21487;&#36716;&#31227;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#20559;&#35265;&#35748;&#30693;&#12290;</title><link>https://arxiv.org/abs/2403.08632</link><description>&lt;p&gt;
&#21313;&#24180;&#25968;&#25454;&#38598;&#20559;&#35265;&#20043;&#25112;&#65306;&#25105;&#20204;&#24050;&#32463;&#25104;&#21151;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
A Decade's Battle on Dataset Bias: Are We There Yet?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08632
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#22312;&#20998;&#31867;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#21487;&#25512;&#24191;&#21644;&#21487;&#36716;&#31227;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#25361;&#25112;&#20102;&#20256;&#32479;&#30340;&#25968;&#25454;&#38598;&#20559;&#35265;&#35748;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26032;&#26102;&#20195;&#37325;&#26032;&#23457;&#35270;Torralba&#21644;Efros&#21313;&#24180;&#21069;&#25552;&#20986;&#30340;&#8220;&#25968;&#25454;&#38598;&#20998;&#31867;&#8221;&#23454;&#39564;&#65292;&#22312;&#25317;&#26377;&#22823;&#35268;&#27169;&#12289;&#22810;&#26679;&#21270;&#21644;&#24076;&#26395;&#26356;&#23569;&#20559;&#35265;&#30340;&#25968;&#25454;&#38598;&#20197;&#21450;&#26356;&#24378;&#22823;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#26032;&#26102;&#20195;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#20998;&#31867;&#22270;&#20687;&#26469;&#33258;&#21738;&#20010;&#25968;&#25454;&#38598;&#26041;&#38754;&#21462;&#24471;&#20986;&#33394;&#30340;&#20934;&#30830;&#24615;&#65306;&#20363;&#22914;&#65292;&#23545;&#20110;&#21253;&#21547;YFCC&#12289;CC&#21644;DataComp&#25968;&#25454;&#38598;&#30340;&#19977;&#20998;&#31867;&#38382;&#39064;&#30340;&#39564;&#35777;&#25968;&#25454;&#65292;&#25105;&#20204;&#25253;&#21578;84.7%&#30340;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#20998;&#31867;&#22120;&#21487;&#20197;&#23398;&#20064;&#21040;&#21487;&#25512;&#24191;&#21644;&#21487;&#36716;&#31227;&#30340;&#35821;&#20041;&#29305;&#24449;&#65292;&#36825;&#19981;&#33021;&#31616;&#21333;&#22320;&#35299;&#37322;&#20026;&#35760;&#24518;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#28608;&#21169;&#31038;&#21306;&#37325;&#26032;&#24605;&#32771;&#28041;&#21450;&#25968;&#25454;&#38598;&#20559;&#35265;&#21644;&#27169;&#22411;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08632v1 Announce Type: cross  Abstract: We revisit the "dataset classification" experiment suggested by Torralba and Efros a decade ago, in the new era with large-scale, diverse, and hopefully less biased datasets as well as more capable neural network architectures. Surprisingly, we observe that modern neural networks can achieve excellent accuracy in classifying which dataset an image is from: e.g., we report 84.7% accuracy on held-out validation data for the three-way classification problem consisting of the YFCC, CC, and DataComp datasets. Our further experiments show that such a dataset classifier could learn semantic features that are generalizable and transferable, which cannot be simply explained by memorization. We hope our discovery will inspire the community to rethink the issue involving dataset bias and model capabilities.
&lt;/p&gt;</description></item><item><title>SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>https://arxiv.org/abs/2403.03636</link><description>&lt;p&gt;
SheetAgent&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
SheetAgent: A Generalist Agent for Spreadsheet Reasoning and Manipulation via Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03636
&lt;/p&gt;
&lt;p&gt;
SheetAgent&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30005;&#23376;&#34920;&#26684;&#25512;&#29702;&#21644;&#25805;&#20316;&#30340;&#36890;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#22788;&#29702;&#22797;&#26434;&#29616;&#23454;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#24191;&#27867;&#23384;&#22312;&#20110;&#22823;&#22810;&#25968;&#26085;&#24120;&#24037;&#20316;&#20013;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24037;&#20316;&#25928;&#29575;&#12290;&#26368;&#36817;&#23581;&#35797;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#36827;&#34892;&#33258;&#21160;&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#65292;&#20294;&#23578;&#26410;&#22312;&#23384;&#22312;&#25512;&#29702;&#25361;&#25112;&#30340;&#22797;&#26434;&#21644;&#29616;&#23454;&#20219;&#21153;&#20013;&#36827;&#34892;&#25506;&#31350;&#65288;&#20363;&#22914;&#65292;&#20855;&#26377;&#22810;&#27493;&#25512;&#29702;&#21644;&#27169;&#31946;&#35201;&#27714;&#30340;&#38271;&#35270;&#37326;&#25805;&#20316;&#65289;&#12290;&#20026;&#20102;&#24357;&#21512;&#19982;&#30495;&#23454;&#19990;&#30028;&#35201;&#27714;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;$\textbf{SheetRM}$&#65292;&#19968;&#20010;&#29305;&#28857;&#26159;&#38271;&#35270;&#37326;&#21644;&#22810;&#31867;&#20219;&#21153;&#30340;&#22522;&#20934;&#65292;&#20855;&#26377;&#25512;&#29702;&#30456;&#20851;&#25805;&#32437;&#65292;&#30001;&#30495;&#23454;&#25361;&#25112;&#24341;&#36215;&#12290;&#20026;&#20102;&#32531;&#35299;&#20197;&#19978;&#25361;&#25112;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;$\textbf{SheetAgent}$&#65292;&#19968;&#31181;&#21033;&#29992;LLMs&#33021;&#21147;&#30340;&#26032;&#22411;&#33258;&#20027;&#20195;&#29702;&#12290;SheetAgent&#30001;&#19977;&#20010;&#21327;&#20316;&#27169;&#22359;&#32452;&#25104;&#65306;$\textit{Planner}$&#12289;$\textit{Informer}$&#21644;$\textit{Retriever}$&#65292;&#23454;&#29616;&#20102;&#23545;&#30005;&#23376;&#34920;&#26684;&#30340;&#39640;&#32423;&#25512;&#29702;&#21644;&#20934;&#30830;&#25805;&#20316;&#65292;&#32780;&#19981;&#38656;&#20154;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03636v1 Announce Type: new  Abstract: Spreadsheet manipulation is widely existing in most daily works and significantly improves working efficiency. Large language model (LLM) has been recently attempted for automatic spreadsheet manipulation but has not yet been investigated in complicated and realistic tasks where reasoning challenges exist (e.g., long horizon manipulation with multi-step reasoning and ambiguous requirements). To bridge the gap with the real-world requirements, we introduce $\textbf{SheetRM}$, a benchmark featuring long-horizon and multi-category tasks with reasoning-dependent manipulation caused by real-life challenges. To mitigate the above challenges, we further propose $\textbf{SheetAgent}$, a novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of three collaborative modules: $\textit{Planner}$, $\textit{Informer}$, and $\textit{Retriever}$, achieving both advanced reasoning and accurate manipulation over spreadsheets without hu
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.02957</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#28176;&#36817;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#21435;&#22122;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#29305;&#23450;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65288;CME&#65289;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#30740;&#31350;&#30340;&#22522;&#20110;DPM&#30340;&#21435;&#22122;&#22120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;DPMs&#20849;&#20139;&#65292;&#20294;&#22312;&#35757;&#32451;&#21518;&#30340;&#36870;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20256;&#36882;&#26465;&#20214;&#22343;&#20540;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#36870;&#36807;&#31243;&#20013;&#20999;&#25442;&#37325;&#26032;&#37319;&#26679;&#30340;&#26041;&#24335;&#32487;&#25215;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22120;&#12290;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02957v1 Announce Type: new  Abstract: Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02107</link><description>&lt;p&gt;
&#36845;&#20195;$Q$-&#32593;&#32476;&#65306;&#36229;&#36234;&#21333;&#27493;&#36125;&#23572;&#26364;&#31639;&#23376;
&lt;/p&gt;
&lt;p&gt;
Iterated $Q$-Network: Beyond the One-Step Bellman Operator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02107
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#29702;&#35770;&#19978;&#21487;&#34892;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20854;&#22312;&#28216;&#25103;&#21644;&#25511;&#21046;&#29615;&#22659;&#20013;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20540;&#22522;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#20381;&#36182;&#20110;&#36125;&#23572;&#26364;&#31639;&#23376;&#30340;&#24212;&#29992;&#65292;&#35813;&#31639;&#23376;&#38656;&#35201;&#20174;&#26679;&#26412;&#20013;&#36827;&#34892;&#36817;&#20284;&#12290;&#22823;&#22810;&#25968;&#26041;&#27861;&#21253;&#25324;&#20132;&#26367;&#24212;&#29992;&#36125;&#23572;&#26364;&#31639;&#23376;&#21644;&#38543;&#21518;&#25237;&#24433;&#27493;&#39588;&#21040;&#32771;&#34385;&#30340;&#20989;&#25968;&#31354;&#38388;&#30340;&#36845;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20123;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#19968;&#27425;&#32771;&#34385;&#22810;&#27425;&#36845;&#20195;&#30340;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36845;&#20195;$Q$-&#32593;&#32476;&#65288;iQN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23427;&#23398;&#20064;&#19968;&#31995;&#21015;$Q$&#20989;&#25968;&#36924;&#36817;&#65292;&#20854;&#20013;&#27599;&#20010;$Q$&#20989;&#25968;&#37117;&#20316;&#20026;&#19979;&#19968;&#20010;&#20989;&#25968;&#38142;&#20013;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;iQN&#22312;&#29702;&#35770;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#21487;&#20197;&#26080;&#32541;&#22320;&#29992;&#20110;&#20540;&#22522;&#21644;&#28436;&#21592;-&#35780;&#35770;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;Atari$2600$&#28216;&#25103;&#21644;&#36830;&#32493;&#25511;&#21046;MuJoCo&#29615;&#22659;&#20013;&#22312;&#23454;&#39564;&#19978;&#23637;&#31034;&#20102;&#23427;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02107v1 Announce Type: cross  Abstract: Value-based Reinforcement Learning (RL) methods rely on the application of the Bellman operator, which needs to be approximated from samples. Most approaches consist of an iterative scheme alternating the application of the Bellman operator and a subsequent projection step onto a considered function space. However, we observe that these algorithms can be improved by considering multiple iterations of the Bellman operator at once. Thus, we introduce iterated $Q$-Networks (iQN), a novel approach that learns a sequence of $Q$-function approximations where each $Q$-function serves as the target for the next one in a chain of consecutive Bellman iterations. We demonstrate that iQN is theoretically sound and show how it can be seamlessly used in value-based and actor-critic methods. We empirically demonstrate its advantages on Atari $2600$ games and in continuous-control MuJoCo environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.18477</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#31614;&#21517;&#26680;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#29992;&#20110;&#38543;&#26426;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Signature Kernel Conditional Independence Tests in Causal Discovery for Stochastic Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18477
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#38543;&#26426;&#36807;&#31243;&#20013;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31614;&#21517;&#26680;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#27979;&#35797;&#65292;&#23454;&#29616;&#20102;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#25512;&#26029;&#65292;&#20197;&#21450;&#24320;&#21457;&#20102;&#32422;&#26463;&#26465;&#20214;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#25512;&#26029;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#32972;&#21518;&#30340;&#22240;&#26524;&#32467;&#26500;&#22312;&#31185;&#23398;&#12289;&#20581;&#24247;&#21644;&#37329;&#34701;&#31561;&#39046;&#22495;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26368;&#36817;&#31614;&#21517;&#26680;&#25216;&#26415;&#30340;&#36827;&#23637;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#8220;&#36335;&#24452;&#31354;&#38388;&#8221;&#19978;&#26465;&#20214;&#29420;&#31435;&#24615;&#65288;CI&#65289;&#27979;&#35797;&#65292;&#29992;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#35299;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#22312;&#36335;&#24452;&#31354;&#38388;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;CI&#27979;&#35797;&#34920;&#29616;&#20986;&#20005;&#26684;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20026;&#38750;&#24490;&#29615;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#24320;&#21457;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#26469;&#24674;&#22797;&#25972;&#20010;&#26377;&#21521;&#22270;&#12290;&#22312;&#20551;&#35774;&#24544;&#23454;&#24615;&#21644;CI&#39044;&#35328;&#26426;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#23436;&#22791;&#19988;&#27491;&#30830;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18477v1 Announce Type: cross  Abstract: Inferring the causal structure underlying stochastic dynamical systems from observational data holds great promise in domains ranging from science and health to finance. Such processes can often be accurately modeled via stochastic differential equations (SDEs), which naturally imply causal relationships via "which variables enter the differential of which other variables". In this paper, we develop a kernel-based test of conditional independence (CI) on "path-space" -- solutions to SDEs -- by leveraging recent advances in signature kernels. We demonstrate strictly superior performance of our proposed CI test compared to existing approaches on path-space. Then, we develop constraint-based causal discovery algorithms for acyclic stochastic dynamical systems (allowing for loops) that leverage temporal information to recover the entire directed graph. Assuming faithfulness and a CI oracle, our algorithm is sound and complete. We empirical
&lt;/p&gt;</description></item><item><title>HetTree&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#26641;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26500;&#24314;&#35821;&#20041;&#26641;&#25968;&#25454;&#32467;&#26500;&#25429;&#25417;&#20803;&#36335;&#24452;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#30340;&#24322;&#26500;&#22270;&#20013;&#30340;&#26641;&#24418;&#23618;&#27425;&#32467;&#26500;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.13496</link><description>&lt;p&gt;
HetTree: &#24322;&#26500;&#26641;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HetTree: Heterogeneous Tree Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13496
&lt;/p&gt;
&lt;p&gt;
HetTree&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#26641;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#26500;&#24314;&#35821;&#20041;&#26641;&#25968;&#25454;&#32467;&#26500;&#25429;&#25417;&#20803;&#36335;&#24452;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#30340;&#24322;&#26500;&#22270;&#20013;&#30340;&#26641;&#24418;&#23618;&#27425;&#32467;&#26500;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#36807;&#21435;&#30475;&#21040;&#20102;&#23545;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNNs&#65289;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#22240;&#20026;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#26159;&#24322;&#26500;&#30340;&#65292;&#20174;&#24341;&#29992;&#22270;&#21040;&#30005;&#23376;&#37038;&#20214;&#22270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#20803;&#36335;&#24452;&#20043;&#38388;&#30340;&#26641;&#24418;&#23618;&#27425;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#26159;&#30001;&#19981;&#21516;&#30340;&#33410;&#28857;&#31867;&#22411;&#21644;&#20851;&#31995;&#31867;&#22411;&#33258;&#28982;&#26500;&#25104;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HetTree&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#26500;&#26641;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#21487;&#25193;&#23637;&#19988;&#26377;&#25928;&#30340;&#26041;&#24335;&#24314;&#27169;&#22270;&#32467;&#26500;&#21644;&#24322;&#26500;&#26041;&#38754;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HetTree&#26500;&#24314;&#20102;&#19968;&#20010;&#35821;&#20041;&#26641;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25429;&#25417;&#20803;&#36335;&#24452;&#20043;&#38388;&#30340;&#23618;&#27425;&#20851;&#31995;&#12290;&#29616;&#26377;&#30340;&#26641;&#24418;&#32534;&#30721;&#25216;&#26415;&#36890;&#36807;&#26681;&#25454;&#23376;&#33410;&#28857;&#19982;&#29238;&#33410;&#28857;&#30340;&#30456;&#20284;&#24615;&#26469;&#21152;&#26435;&#23376;&#33410;&#28857;&#30340;&#36129;&#29486;&#26469;&#32858;&#21512;&#23376;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#26641;&#24418;&#32534;&#30721;&#26410;&#33021;&#25429;&#25417;&#25972;&#20010;&#29238;&#23376;&#23618;&#27425;&#32467;&#26500;&#65292;&#22240;&#20026;&#21482;&#32771;&#34385;&#20102;&#29238;&#33410;&#28857;&#12290;&#22240;&#27492;&#65292;HetTree&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23376;&#26641;&#27880;&#24847;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13496v1 Announce Type: new  Abstract: The recent past has seen an increasing interest in Heterogeneous Graph Neural Networks (HGNNs) since many real-world graphs are heterogeneous in nature, from citation graphs to email graphs. However, existing methods ignore a tree hierarchy among metapaths, which is naturally constituted by different node types and relation types. In this paper, we present HetTree, a novel heterogeneous tree graph neural network that models both the graph structure and heterogeneous aspects in a scalable and effective manner. Specifically, HetTree builds a semantic tree data structure to capture the hierarchy among metapaths. Existing tree encoding techniques aggregate children nodes by weighting the contribution of children nodes based on similarity to the parent node. However, we find that this tree encoding fails to capture the entire parent-children hierarchy by only considering the parent node. Hence, HetTree uses a novel subtree attention mechanism
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09906</link><description>&lt;p&gt;
&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Generative Representational Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;&#25105;&#20204;&#30340;GritLM 8x7B&#25104;&#20026;&#26368;&#20339;&#30340;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20043;&#19968;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;GRIT&#30340;&#32479;&#19968;&#20063;&#22823;&#22823;&#25552;&#39640;&#20102;RAG&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#22522;&#20110;&#25991;&#26412;&#30340;&#35821;&#35328;&#38382;&#39064;&#37117;&#21487;&#20197;&#24402;&#32467;&#20026;&#29983;&#25104;&#25110;&#23884;&#20837;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#21482;&#33021;&#22312;&#20854;&#20013;&#19968;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#29983;&#25104;&#34920;&#31034;&#25351;&#20196;&#35843;&#25972;&#65288;GRIT&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#20196;&#26469;&#21306;&#20998;&#29983;&#25104;&#21644;&#23884;&#20837;&#20219;&#21153;&#65292;&#20174;&#32780;&#35757;&#32451;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#22788;&#29702;&#36825;&#20004;&#31181;&#20219;&#21153;&#12290;&#19982;&#20854;&#20182;&#24320;&#25918;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;GritLM 7B&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#22522;&#20934;&#27979;&#35797;&#65288;MTEB&#65289;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#65292;&#24182;&#22312;&#22810;&#31181;&#29983;&#25104;&#20219;&#21153;&#20013;&#36229;&#36807;&#20102;&#21516;&#31561;&#35268;&#27169;&#30340;&#25152;&#26377;&#27169;&#22411;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25193;&#22823;&#35268;&#27169;&#65292;GritLM 8x7B&#22312;&#23581;&#35797;&#30340;&#25152;&#26377;&#24320;&#25918;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#20013;&#34920;&#29616;&#26368;&#20339;&#65292;&#21516;&#26102;&#20173;&#28982;&#26159;&#26368;&#22909;&#30340;&#23884;&#20837;&#27169;&#22411;&#20043;&#19968;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;GRIT&#21487;&#20197;&#19982;&#20165;&#22312;&#29983;&#25104;&#25110;&#23884;&#20837;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#22240;&#27492;&#25105;&#20204;&#21487;&#20197;&#22312;&#19981;&#25439;&#22833;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#32479;&#19968;&#20004;&#32773;&#12290;&#38500;&#27492;&#20043;&#22806;&#65292;&#36890;&#36807;GRIT&#30340;&#32479;&#19968;&#21487;&#20197;&#23558;RAG&#65288;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65289;&#22312;&#38271;&#25991;&#26723;&#19978;&#30340;&#36895;&#24230;&#25552;&#39640;60%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09906v1 Announce Type: cross  Abstract: All text-based language problems can be reduced to either generation or embedding. Current models only perform well at one or the other. We introduce generative representational instruction tuning (GRIT) whereby a large language model is trained to handle both generative and embedding tasks by distinguishing between them through instructions. Compared to other open models, our resulting GritLM 7B sets a new state of the art on the Massive Text Embedding Benchmark (MTEB) and outperforms all models up to its size on a range of generative tasks. By scaling up further, GritLM 8x7B outperforms all open generative language models that we tried while still being among the best embedding models. Notably, we find that GRIT matches training on only generative or embedding data, thus we can unify both at no performance loss. Among other benefits, the unification via GRIT speeds up Retrieval-Augmented Generation (RAG) by &gt; 60% for long documents, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20197;&#36830;&#32493;&#26494;&#24347;&#30340;&#36755;&#20837;&#25552;&#31034;&#26469;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#31163;&#25955;&#20248;&#21270;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#27585;&#28781;&#24615;&#25915;&#20987;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.09154</link><description>&lt;p&gt;
&#29992;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Attacking Large Language Models with Projected Gradient Descent
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#65292;&#20197;&#36830;&#32493;&#26494;&#24347;&#30340;&#36755;&#20837;&#25552;&#31034;&#26469;&#25915;&#20987;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#31163;&#25955;&#20248;&#21270;&#26356;&#24555;&#30340;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#27585;&#28781;&#24615;&#25915;&#20987;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#29305;&#23450;&#35774;&#35745;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#24456;&#23481;&#26131;&#34987;&#30772;&#35299;&#12290;&#34429;&#28982;&#20351;&#29992;&#31163;&#25955;&#20248;&#21270;&#21046;&#20316;&#23545;&#25239;&#24615;&#25552;&#31034;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#36825;&#31181;&#25915;&#20987;&#36890;&#24120;&#38656;&#35201;&#36229;&#36807;100,000&#27425;&#30340;&#35821;&#35328;&#27169;&#22411;&#35843;&#29992;&#12290;&#36825;&#31181;&#39640;&#35745;&#31639;&#25104;&#26412;&#20351;&#24471;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#23450;&#37327;&#20998;&#26512;&#21644;&#23545;&#25239;&#24615;&#35757;&#32451;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#23545;&#36830;&#32493;&#26494;&#24347;&#30340;&#36755;&#20837;&#25552;&#31034;&#20351;&#29992;&#25237;&#24433;&#26799;&#24230;&#19979;&#38477;&#65288;PGD&#65289;&#30340;&#26041;&#27861;&#12290;&#23613;&#31649;&#20808;&#21069;&#20351;&#29992;&#26222;&#36890;&#26799;&#24230;&#25915;&#20987;&#30340;&#23581;&#35797;&#22522;&#26412;&#22833;&#36133;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#20180;&#32454;&#25511;&#21046;&#36830;&#32493;&#26494;&#24347;&#24341;&#20837;&#30340;&#35823;&#24046;&#26497;&#22823;&#22320;&#25552;&#21319;&#20102;&#23427;&#20204;&#30340;&#25928;&#21147;&#12290;&#25105;&#20204;&#30340;LLMs&#30340;PGD&#36895;&#24230;&#27604;&#26368;&#20808;&#36827;&#30340;&#31163;&#25955;&#20248;&#21270;&#24555;&#19968;&#20010;&#25968;&#37327;&#32423;&#65292;&#20197;&#36798;&#21040;&#30456;&#21516;&#30340;&#27585;&#28781;&#24615;&#25915;&#20987;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09154v1 Announce Type: new Abstract: Current LLM alignment methods are readily broken through specifically crafted adversarial prompts. While crafting adversarial prompts using discrete optimization is highly effective, such attacks typically use more than 100,000 LLM calls. This high computational cost makes them unsuitable for, e.g., quantitative analyses and adversarial training. To remedy this, we revisit Projected Gradient Descent (PGD) on the continuously relaxed input prompt. Although previous attempts with ordinary gradient-based attacks largely failed, we show that carefully controlling the error introduced by the continuous relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one order of magnitude faster than state-of-the-art discrete optimization to achieve the same devastating attack results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.08491</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#30340;&#24067;&#23572;&#27169;&#22411;&#21560;&#24341;&#23376;&#26223;&#35266;&#20013;&#30340;&#25511;&#21046;&#36941;&#21382;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#29992;&#20110;&#32454;&#32990;&#37325;&#32534;&#31243;&#20013;&#30340;&#37325;&#32534;&#31243;&#31574;&#30053;&#35782;&#21035;&#12290;&#22312;&#25511;&#21046;&#38382;&#39064;&#20013;&#65292;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35782;&#21035;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#35813;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#32990;&#37325;&#32534;&#31243;&#21487;&#29992;&#20110;&#39044;&#38450;&#21644;&#27835;&#30103;&#19981;&#21516;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20256;&#32479;&#28287;&#23454;&#39564;&#21457;&#29616;&#37325;&#32534;&#31243;&#31574;&#30053;&#30340;&#25928;&#29575;&#21463;&#21040;&#26102;&#38388;&#21644;&#25104;&#26412;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#20197;&#20415;&#24110;&#21161;&#35782;&#21035;&#37325;&#32534;&#31243;&#31574;&#30053;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#22312;&#32454;&#32990;&#37325;&#32534;&#31243;&#26694;&#26550;&#30340;BNs&#21644;PBNs&#20197;&#21450;&#24322;&#27493;&#26356;&#26032;&#27169;&#24335;&#19979;&#21046;&#23450;&#20102;&#19968;&#20010;&#25511;&#21046;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20266;&#21560;&#24341;&#23376;&#30340;&#27010;&#24565;&#21644;&#35757;&#32451;&#36807;&#31243;&#20013;&#20266;&#21560;&#24341;&#23376;&#29366;&#24577;&#30340;&#35782;&#21035;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#35299;&#20915;&#25511;&#21046;&#38382;&#39064;&#30340;&#35745;&#31639;&#26694;&#26550;&#65292;&#24182;&#22312;&#22810;&#20010;&#19981;&#21516;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a~novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a~control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a~pseudo-attractor and a~procedure for identification of pseudo-attractor state during training. Finally, we devise a~computational framework for solving the control problem, which we test on a~number of different models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;CP-E2LSH&#21644;TT-E2LSH&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;LSH&#65292;&#22312;&#22788;&#29702;&#24352;&#37327;&#25968;&#25454;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#26102;&#33021;&#22815;&#25552;&#20379;&#26356;&#24555;&#21644;&#26356;&#31354;&#38388;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07189</link><description>&lt;p&gt;
&#36890;&#36807;&#24352;&#37327;&#21270;&#38543;&#26426;&#25237;&#24433;&#25913;&#36827;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;LSH
&lt;/p&gt;
&lt;p&gt;
Improving LSH via Tensorized Random Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;CP-E2LSH&#21644;TT-E2LSH&#20004;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#31639;&#27861;LSH&#65292;&#22312;&#22788;&#29702;&#24352;&#37327;&#25968;&#25454;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#26102;&#33021;&#22815;&#25552;&#20379;&#26356;&#24555;&#21644;&#26356;&#31354;&#38388;&#26377;&#25928;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;(LSH)&#26159;&#25968;&#25454;&#31185;&#23398;&#23478;&#29992;&#20110;&#36817;&#20284;&#26368;&#36817;&#37051;&#25628;&#32034;&#38382;&#39064;&#30340;&#22522;&#26412;&#31639;&#27861;&#24037;&#20855;&#65292;&#24050;&#22312;&#35768;&#22810;&#22823;&#35268;&#27169;&#25968;&#25454;&#22788;&#29702;&#24212;&#29992;&#20013;&#24191;&#27867;&#20351;&#29992;&#65292;&#22914;&#36817;&#20284;&#37325;&#22797;&#26816;&#27979;&#12289;&#26368;&#36817;&#37051;&#25628;&#32034;&#12289;&#32858;&#31867;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#26356;&#24555;&#21644;&#31354;&#38388;&#26356;&#26377;&#25928;&#30340;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#20989;&#25968;&#65292;&#29992;&#20110;&#24352;&#37327;&#25968;&#25454;&#30340;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#12290;&#36890;&#24120;&#65292;&#23545;&#20110;&#24352;&#37327;&#25968;&#25454;&#33719;&#24471;LSH&#30340;&#26420;&#32032;&#26041;&#27861;&#28041;&#21450;&#23558;&#24352;&#37327;&#37325;&#22609;&#20026;&#21521;&#37327;&#65292;&#28982;&#21518;&#24212;&#29992;&#29616;&#26377;&#30340;&#21521;&#37327;&#25968;&#25454;LSH&#26041;&#27861;(E2LSH&#21644;SRP)&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#39640;&#38454;&#24352;&#37327;&#65292;&#36825;&#31181;&#26041;&#27861;&#21464;&#24471;&#19981;&#20999;&#23454;&#38469;&#65292;&#22240;&#20026;&#37325;&#22609;&#21521;&#37327;&#30340;&#22823;&#23567;&#22312;&#24352;&#37327;&#30340;&#38454;&#25968;&#20013;&#21576;&#25351;&#25968;&#22686;&#38271;&#12290;&#22240;&#27492;&#65292;LSH&#21442;&#25968;&#30340;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#21152;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#21644;&#20313;&#24358;&#30456;&#20284;&#24230;&#30340;LSH&#26041;&#27861;&#65292;&#20998;&#21035;&#26159;CP-E2LSH&#21644;TT-E2LSH&#12290;
&lt;/p&gt;
&lt;p&gt;
Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by data scientists for approximate nearest neighbour search problems that have been used extensively in many large scale data processing applications such as near duplicate detection, nearest neighbour search, clustering, etc. In this work, we aim to propose faster and space efficient locality sensitive hash functions for Euclidean distance and cosine similarity for tensor data. Typically, the naive approach for obtaining LSH for tensor data involves first reshaping the tensor into vectors, followed by applying existing LSH methods for vector data $E2LSH$ and $SRP$. However, this approach becomes impractical for higher order tensors because the size of the reshaped vector becomes exponential in the order of the tensor. Consequently, the size of LSH parameters increases exponentially. To address this problem, we suggest two methods for LSH for Euclidean distance and cosine similarity, namely $CP-E2LSH$, $TT-E2LSH
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2402.06287</link><description>&lt;p&gt;
AI&#65292;&#19982;&#20154;&#30456;&#36935;&#65306;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#23398;&#20064;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06287
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#22825;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#22810;&#22320;&#20381;&#36182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#33258;&#21160;&#21270;&#21644;&#25903;&#25345;&#39640;&#39118;&#38505;&#20219;&#21153;&#21644;&#20915;&#31574;&#12290;&#36825;&#31181;&#26085;&#30410;&#22686;&#38271;&#30340;&#23384;&#22312;&#24847;&#21619;&#30528;&#20154;&#31867;&#29616;&#22312;&#19981;&#26029;&#19982;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31995;&#32479;&#36827;&#34892;&#20114;&#21160;&#65292;&#27599;&#22825;&#36827;&#34892;&#27169;&#22411;&#30340;&#22521;&#35757;&#21644;&#20351;&#29992;&#12290;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#20013;&#26377;&#20960;&#31181;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#32771;&#34385;&#20154;&#19982;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20132;&#20114;&#65292;&#20294;&#20854;&#20998;&#31867;&#31232;&#30095;&#19988;&#30446;&#26631;&#21508;&#24322;&#12290;&#26412;&#35843;&#26597;&#25552;&#20986;&#20102;&#28151;&#21512;&#20915;&#31574;&#31995;&#32479;&#30340;&#20998;&#31867;&#26041;&#27861;&#65292;&#20026;&#29702;&#35299;&#24403;&#21069;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#22914;&#20309;&#23545;&#20154;&#19982;&#26426;&#22120;&#20043;&#38388;&#30340;&#20132;&#20114;&#36827;&#34892;&#24314;&#27169;&#25552;&#20379;&#20102;&#27010;&#24565;&#24615;&#21644;&#25216;&#26415;&#24615;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Everyday we increasingly rely on machine learning models to automate and support high-stake tasks and decisions. This growing presence means that humans are now constantly interacting with machine learning-based systems, training and using models everyday. Several different techniques in computer science literature account for the human interaction with machine learning systems, but their classification is sparse and the goals varied. This survey proposes a taxonomy of Hybrid Decision Making Systems, providing both a conceptual and technical framework for understanding how current computer science literature models interaction between humans and machines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.05569</link><description>&lt;p&gt;
&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Node Classification With Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;(WCE-GNN)&#23454;&#29616;&#20102;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;WCE-GNN&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#21644;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#26159;&#29992;&#26469;&#27169;&#25311;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#20851;&#38190;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#25104;&#21151;&#25581;&#31034;&#20102;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#25104;&#23545;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#36825;&#28608;&#21457;&#20102;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#20855;&#26377;&#39640;&#38454;&#30456;&#20114;&#20316;&#29992;&#30340;&#25968;&#25454;&#30340;&#24819;&#27861;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HyperGNNs&#65289;&#30340;&#21457;&#23637;&#12290;GNNs&#21644;HyperGNNs&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#19981;&#21516;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#34987;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#19981;&#21516;&#20960;&#20309;&#25299;&#25169;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#33410;&#28857;&#20998;&#31867;&#30340;&#19978;&#19979;&#25991;&#20013;&#65292;&#22823;&#22810;&#25968;HyperGNNs&#21487;&#20197;&#20351;&#29992;&#24102;&#26377;&#36229;&#22270;&#30340;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#30340;GNN&#26469;&#36817;&#20284;&#12290;&#36825;&#23548;&#33268;&#20102;WCE-GNN&#65292;&#19968;&#31181;&#31616;&#21333;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#19968;&#20010;GNN&#21644;&#19968;&#20010;&#21152;&#26435;&#23376;&#22270;&#25193;&#23637;&#65288;WCE&#65289;&#65292;&#29992;&#20110;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#12290;&#23545;&#20110;&#20061;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;WCE-GNN&#19981;&#20165;&#20855;&#26377;&#20248;&#31168;&#30340;&#39044;&#27979;&#25928;&#26524;&#65292;&#32780;&#19988;&#20855;&#26377;&#36739;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs, with hyperedges connecting more than two nodes, are key for modelling higher-order interactions in real-world data. The success of graph neural networks (GNNs) reveals the capability of neural networks to process data with pairwise interactions. This inspires the usage of neural networks for data with higher-order interactions, thereby leading to the development of hypergraph neural networks (HyperGNNs). GNNs and HyperGNNs are typically considered distinct since they are designed for data on different geometric topologies. However, in this paper, we theoretically demonstrate that, in the context of node classification, most HyperGNNs can be approximated using a GNN with a weighted clique expansion of the hypergraph. This leads to WCE-GNN, a simple and efficient framework comprising a GNN and a weighted clique expansion (WCE), for hypergraph node classification. Experiments on nine real-world hypergraph node classification benchmarks showcase that WCE-GNN demonstrates not o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.02611</link><description>&lt;p&gt;
PuzzleBench&#65306;LLMs&#33021;&#21542;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
PuzzleBench: Can LLMs Solve Challenging First-Order Combinatorial Reasoning Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;PuzzleBench&#25968;&#25454;&#38598;&#25506;&#32034;&#20102;LLMs&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;Puzzle-LM&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#22320;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;LLMs&#36827;&#34892;&#25512;&#29702;&#20219;&#21153;&#65292;&#37325;&#28857;&#26159;&#30456;&#23545;&#31616;&#21333;&#30340;&#38382;&#39064;&#65292;&#22914;&#36923;&#36753;&#38382;&#31572;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24076;&#26395;&#35299;&#20915;&#26356;&#22797;&#26434;&#30340;&#38382;&#39064;&#65292;&#26174;&#33879;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#30340;&#21151;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25506;&#35752;LLMs&#26159;&#21542;&#33021;&#22815;&#35299;&#20915;&#22256;&#38590;&#30340;&#19968;&#38454;&#32452;&#21512;&#25512;&#29702;&#38382;&#39064;&#65292;&#19968;&#20010;&#20363;&#23376;&#26159;&#27969;&#34892;&#30340;&#25968;&#29420;&#35868;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#26377;&#19968;&#20010;&#30001;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22522;&#30784;&#19968;&#38454;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#20363;&#21270;&#20026;&#19981;&#21516;&#22823;&#23567;&#30340;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#38382;&#39064;&#22312;&#35745;&#31639;&#19978;&#26159;&#23494;&#38598;&#22411;&#30340;&#65292;&#38656;&#35201;&#22810;&#20010;&#25512;&#29702;&#27493;&#39588;&#25165;&#33021;&#36798;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;PuzzleBench&#65292;&#19968;&#20010;&#21253;&#21547;31&#20010;&#36825;&#26679;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35868;&#39064;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#21363;&#20351;&#22312;&#31526;&#21495;&#27714;&#35299;&#22120;&#30340;&#24110;&#21161;&#19979;&#65292;LLMs&#22312;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#24471;&#30456;&#24403;&#31967;&#31957;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;Puzzle-LM&#65292;&#23427;&#23558;LLMs&#19982;&#31526;&#21495;&#27714;&#35299;&#22120;&#21644;&#31243;&#24207;&#35299;&#37322;&#22120;&#30456;&#32467;&#21512;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#25512;&#29702;&#36825;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have explored the use of LLMs for reasoning tasks focussing on relatively simple problems, such as logical question answering. In our work, we wish to tackle more complicated problems, significantly expanding the capabilities of these models. Particularly, we explore whether LLMs can solve challenging first-order combinatorial reasoning problems, an example being the popular puzzle Sudoku. These problems have an underlying first-order structure described by a general description in natural language and can be instantiated to instances of varying sizes. Moreover these problems are computationally intensive requiring several reasoning steps to reach the solution. We present PuzzleBench a dataset of 31 such challenging puzzles. We observe that LLMs even when aided by symbolic solvers perform rather poorly on our benchmark. In response we propose a new approach, Puzzle-LM which combines LLMs with both symbolic solvers and program interpreters enabling them to reason about such
&lt;/p&gt;</description></item><item><title>TIGT&#26159;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#20449;&#24687;&#30340;&#26032;&#22411;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#21306;&#20998;&#22270;&#21516;&#26500;&#24615;&#30340;&#33021;&#21147;&#21644;&#25552;&#39640;&#22270;&#24418;&#21464;&#25442;&#22120;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#21516;&#26500;&#24615;&#30340;&#26816;&#27979;&#21644;&#25972;&#20307;&#24615;&#33021;&#30340;&#22686;&#24378;&#12290;</title><link>https://arxiv.org/abs/2402.02005</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Topology-Informed Graph Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02005
&lt;/p&gt;
&lt;p&gt;
TIGT&#26159;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#20449;&#24687;&#30340;&#26032;&#22411;&#22270;&#24418;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#22686;&#24378;&#21306;&#20998;&#22270;&#21516;&#26500;&#24615;&#30340;&#33021;&#21147;&#21644;&#25552;&#39640;&#22270;&#24418;&#21464;&#25442;&#22120;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#21516;&#26500;&#24615;&#30340;&#26816;&#27979;&#21644;&#25972;&#20307;&#24615;&#33021;&#30340;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#24418;&#22120;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35270;&#35273;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#65292;&#20026;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#38598;&#25104;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#22686;&#24378;&#22270;&#24418;&#21464;&#25442;&#22120;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#22686;&#24378;&#21306;&#20998;&#22270;&#30340;&#21516;&#26500;&#24615;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#36825;&#22312;&#25552;&#39640;&#23427;&#20204;&#30340;&#39044;&#27979;&#24615;&#33021;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21464;&#24418;&#22120;&#8212;&#8212;&#8220;&#22522;&#20110;&#25299;&#25169;&#20449;&#24687;&#30340;&#22270;&#24418;&#21464;&#25442;&#22120;&#65288;TIGT&#65289;&#8221;&#65292;&#23427;&#22686;&#24378;&#20102;&#26816;&#27979;&#22270;&#21516;&#26500;&#24615;&#30340;&#21306;&#20998;&#33021;&#21147;&#21644;&#22270;&#24418;&#21464;&#25442;&#22120;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;TIGT&#30001;&#22235;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#20010;&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#24490;&#29615;&#23376;&#22270;&#30340;&#38750;&#21516;&#26500;&#21367;&#19978;&#30340;&#25299;&#25169;&#20301;&#32622;&#23884;&#20837;&#23618;&#65292;&#20197;&#30830;&#20445;&#21807;&#19968;&#30340;&#22270;&#34920;&#31034;&#65307;&#19968;&#20010;&#21452;&#36335;&#24452;&#28040;&#24687;&#20256;&#36882;&#23618;&#65292;&#20197;&#26126;&#30830;&#22320;&#32534;&#30721;&#25299;&#25169;&#29305;&#24449;&#65307;&#19968;&#20010;&#20840;&#23616;&#27880;&#24847;&#26426;&#21046;&#65307;&#21644;&#19968;&#20010;&#22270;&#20449;&#24687;&#23618;&#65292;&#29992;&#20110;&#37325;&#26032;&#26657;&#20934;&#36890;&#36947;&#32423;&#30340;&#22270;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformers have revolutionized performance in Natural Language Processing and Vision, paving the way for their integration with Graph Neural Networks (GNNs). One key challenge in enhancing graph transformers is strengthening the discriminative power of distinguishing isomorphisms of graphs, which plays a crucial role in boosting their predictive performances. To address this challenge, we introduce 'Topology-Informed Graph Transformer (TIGT)', a novel transformer enhancing both discriminative power in detecting graph isomorphisms and the overall performance of Graph Transformers. TIGT consists of four components: A topological positional embedding layer using non-isomorphic universal covers based on cyclic subgraphs of graphs to ensure unique graph representation: A dual-path message-passing layer to explicitly encode topological characteristics throughout the encoder layers: A global attention mechanism: And a graph information layer to recalibrate channel-wise graph features for be
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.15487</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;
&lt;/p&gt;
&lt;p&gt;
Global $\mathcal{L}^2$ minimization at uniform exponential rate via geometrically adapted gradient descent in Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15487
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20960;&#20309;&#35843;&#25972;&#30340;&#26799;&#24230;&#19979;&#38477;&#65292;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20197;&#22343;&#21248;&#25351;&#25968;&#36895;&#29575;&#23454;&#29616;&#20840;&#23616;$\mathcal{L}^2$&#26368;&#23567;&#21270;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#28145;&#24230;&#23398;&#20064;&#32593;&#32476;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#26368;&#23567;&#21270;$\mathcal{L}^2$&#20195;&#20215;&#20989;&#25968;&#30340;&#26799;&#24230;&#19979;&#38477;&#27969;&#65292;&#24182;&#24341;&#20837;&#20004;&#20010;&#25913;&#36827;&#29256;&#26412;&#65307;&#19968;&#20010;&#36866;&#29992;&#20110;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#65292;&#21478;&#19968;&#20010;&#36866;&#29992;&#20110;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#12290;&#36825;&#20004;&#20010;&#29256;&#26412;&#37117;&#20855;&#26377;&#26126;&#30830;&#33258;&#28982;&#30340;&#19981;&#21464;&#20960;&#20309;&#21547;&#20041;&#65292;&#32771;&#34385;&#21040;&#22312;&#36807;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25289;&#22238;&#21521;&#37327;&#19995;&#32467;&#26500;&#21644;&#22312;&#27424;&#21442;&#25968;&#21270;&#35774;&#32622;&#20013;&#30340;&#25512;&#21069;&#21521;&#37327;&#19995;&#32467;&#26500;&#12290;&#22312;&#36807;&#21442;&#25968;&#21270;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#65292;&#21482;&#35201;&#28385;&#36275;&#31209;&#26465;&#20214;&#65292;&#25913;&#36827;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#25152;&#26377;&#36712;&#36947;&#23558;&#20197;&#22343;&#21248;&#25351;&#25968;&#25910;&#25947;&#36895;&#29575;&#23558;$\mathcal{L}^2$&#20195;&#20215;&#39537;&#21160;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#65307;&#22240;&#27492;&#65292;&#23545;&#20110;&#20219;&#20309;&#39044;&#20808;&#25351;&#23450;&#30340;&#25509;&#36817;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#36817;&#20284;&#65292;&#25105;&#20204;&#21487;&#20197;&#24471;&#21040;&#20808;&#39564;&#20572;&#27490;&#26102;&#38388;&#12290;&#25105;&#20204;&#25351;&#20986;&#21518;&#32773;&#19982;&#27425;Riemann&#20960;&#20309;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15487v3 Announce Type: replace-cross  Abstract: We consider the gradient descent flow widely used for the minimization of the $\mathcal{L}^2$ cost function in Deep Learning networks, and introduce two modified versions; one adapted for the overparametrized setting, and the other for the underparametrized setting. Both have a clear and natural invariant geometric meaning, taking into account the pullback vector bundle structure in the overparametrized, and the pushforward vector bundle structure in the underparametrized setting. In the overparametrized case, we prove that, provided that a rank condition holds, all orbits of the modified gradient descent drive the $\mathcal{L}^2$ cost to its global minimum at a uniform exponential convergence rate; one thereby obtains an a priori stopping time for any prescribed proximity to the global minimum. We point out relations of the latter to sub-Riemannian geometry.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LCPO&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2302.02182</link><description>&lt;p&gt;
&#22312;&#38750;&#38745;&#24577;&#19978;&#19979;&#25991;&#39537;&#21160;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Reinforcement Learning in Non-Stationary Context-Driven Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.02182
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LCPO&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#38750;&#38745;&#24577;&#29615;&#22659;&#20013;&#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20854;&#20013;&#19968;&#20010;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22806;&#29983;&#19978;&#19979;&#25991;&#36807;&#31243;&#24433;&#21709;&#30528;&#29615;&#22659;&#21160;&#24577;&#12290;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#36825;&#26679;&#30340;&#29615;&#22659;&#20013;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23384;&#22312;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#29616;&#35937;&#12290;&#38543;&#30528;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#26032;&#32463;&#39564;&#22686;&#21152;&#65292;&#20195;&#29702; tend to forget &#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#20219;&#21153;&#26631;&#31614;&#65288;&#36825;&#22312;&#23454;&#36341;&#20013;&#24448;&#24448;&#26159;&#19981;&#23384;&#22312;&#30340;&#65289;&#25110;&#32773;&#20351;&#29992;&#33073;&#26426;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#19981;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#24046;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Locally Constrained Policy Optimization (LCPO) &#30340;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#20248;&#21270;&#24403;&#21069;&#32463;&#39564;&#22238;&#25253;&#30340;&#21516;&#26102;&#23558;&#31574;&#30053;&#23545;&#26087;&#30340;&#32463;&#39564;&#36827;&#34892;&#38170;&#23450;&#26469;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#38170;&#23450;&#65292;LCPO&#20351;&#29992;&#26469;&#33258;&#24403;&#21069;&#19978;&#19979;&#25991;&#20998;&#24067;&#20043;&#22806;&#30340;&#32463;&#39564;&#26679;&#26412;&#26469;&#23616;&#37096;&#32422;&#26463;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#22312;Mujoco&#12289;&#32463;&#20856;&#25511;&#21046;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#29615;&#22659;&#20013;&#20351;&#29992;&#22810;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19978;&#19979;&#25991;&#36319;&#36394;&#65292;&#35780;&#20272;&#20102;LCPO&#30340;&#24615;&#33021;&#65292;&#24182;&#21457;&#29616;&#23427;&#33021;&#22815;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study online reinforcement learning (RL) in non-stationary environments, where a time-varying exogenous context process affects the environment dynamics. Online RL is challenging in such environments due to "catastrophic forgetting" (CF). The agent tends to forget prior knowledge as it trains on new experiences. Prior approaches to mitigate this issue assume task labels (which are often not available in practice) or use off-policy methods that suffer from instability and poor performance.   We present Locally Constrained Policy Optimization (LCPO), an online RL approach that combats CF by anchoring policy outputs on old experiences while optimizing the return on current experiences. To perform this anchoring, LCPO locally constrains policy optimization using samples from experiences that lie outside of the current context distribution. We evaluate LCPO in Mujoco, classic control and computer systems environments with a variety of synthetic and real context traces, and find that it o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;Yang-Baxter&#26041;&#31243;&#26469;&#32531;&#35299;&#21644;&#20462;&#27491;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#38169;&#35823;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25511;&#21046;&#22122;&#22768;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#32463;&#20856;&#35745;&#31639;&#36827;&#34892;&#38169;&#35823;&#32531;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#32416;&#27491;&#26102;&#38388;&#28436;&#21270;&#30340;&#37327;&#23376;&#24577;&#20013;&#30340;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2401.17116</link><description>&lt;p&gt;
&#36890;&#36807;Yang-Baxter&#26041;&#31243;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20171;&#23548;&#30340;&#37327;&#23376;&#38169;&#35823;&#32531;&#35299;&#21644;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Quantum error mitigation and correction mediated by Yang-Baxter equation and artificial neural network. (arXiv:2401.17116v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17116
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#31574;&#30053;&#65292;&#36890;&#36807;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21644;Yang-Baxter&#26041;&#31243;&#26469;&#32531;&#35299;&#21644;&#20462;&#27491;&#37327;&#23376;&#35745;&#31639;&#20013;&#30340;&#38169;&#35823;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#25511;&#21046;&#22122;&#22768;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#32463;&#20856;&#35745;&#31639;&#36827;&#34892;&#38169;&#35823;&#32531;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#32416;&#27491;&#26102;&#38388;&#28436;&#21270;&#30340;&#37327;&#23376;&#24577;&#20013;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#38169;&#35823;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#21644;Yang-Baxter&#26041;&#31243;&#65288;YBE&#65289;&#26469;&#32531;&#35299;&#37327;&#23376;&#38169;&#35823;&#30340;&#26032;&#31574;&#30053;&#12290;&#19982;&#20256;&#32479;&#30340;&#38169;&#35823;&#20462;&#27491;&#26041;&#27861;&#19981;&#21516;&#65292;&#36825;&#20123;&#26041;&#27861;&#35745;&#31639;&#37327;&#24456;&#22823;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20154;&#24037;&#38169;&#35823;&#32531;&#35299;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#37327;&#23376;&#38169;&#35823;&#26469;&#28304;&#30340;&#22522;&#30784;&#30693;&#35782;&#65292;&#24182;&#25506;&#35752;&#20102;&#20351;&#29992;&#32463;&#20856;&#35745;&#31639;&#26469;&#36827;&#34892;&#38169;&#35823;&#32531;&#35299;&#30340;&#28508;&#21147;&#12290;Yang-Baxter&#26041;&#31243;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#26102;&#38388;&#21160;&#21147;&#23398;&#27169;&#25311;&#21387;&#32553;&#21040;&#24658;&#23450;&#28145;&#24230;&#30340;&#30005;&#36335;&#20013;&#12290;&#36890;&#36807;&#24341;&#20837;&#36890;&#36807;YBE&#25511;&#21046;&#30340;&#22122;&#22768;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#29992;&#20110;&#38169;&#35823;&#32531;&#35299;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#37096;&#20998;&#37327;&#23376;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#20102;&#19968;&#20010;ANN&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#22312;&#32416;&#27491;&#26102;&#38388;&#28436;&#21270;&#30340;&#37327;&#23376;&#24577;&#20013;&#30340;&#38169;&#35823;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum computing shows great potential, but errors pose a significant challenge. This study explores new strategies for mitigating quantum errors using artificial neural networks (ANN) and the Yang-Baxter equation (YBE). Unlike traditional error correction methods, which are computationally intensive, we investigate artificial error mitigation. The manuscript introduces the basics of quantum error sources and explores the potential of using classical computation for error mitigation. The Yang-Baxter equation plays a crucial role, allowing us to compress time dynamics simulations into constant-depth circuits. By introducing controlled noise through the YBE, we enhance the dataset for error mitigation. We train an ANN model on partial data from quantum simulations, demonstrating its effectiveness in correcting errors in time-evolving quantum states.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#35777;&#26126;&#24403;&#30495;&#23454;&#21442;&#25968;&#20026;0&#26102;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#22312;&#35813;&#25200;&#21160;&#19979;&#30340;&#26497;&#38480;&#20998;&#24067;&#21487;&#33021;&#22312;0&#22788;&#26377;&#19968;&#20010;&#27491;&#27010;&#29575;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#31232;&#30095;&#24615;&#24674;&#22797;&#33021;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#36807;&#31243;&#8212;&#8212;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15262</link><description>&lt;p&gt;
&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Asymptotic Behavior of Adversarial Training Estimator under $\ell_\infty$-Perturbation. (arXiv:2401.15262v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#35777;&#26126;&#24403;&#30495;&#23454;&#21442;&#25968;&#20026;0&#26102;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#22312;&#35813;&#25200;&#21160;&#19979;&#30340;&#26497;&#38480;&#20998;&#24067;&#21487;&#33021;&#22312;0&#22788;&#26377;&#19968;&#20010;&#27491;&#27010;&#29575;&#36136;&#37327;&#65292;&#25552;&#20379;&#20102;&#31232;&#30095;&#24615;&#24674;&#22797;&#33021;&#21147;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#36807;&#31243;&#8212;&#8212;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#35757;&#32451;&#34987;&#25552;&#20986;&#26469;&#25269;&#24481;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#20013;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#36825;&#20010;&#38382;&#39064;&#26368;&#36817;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#30340;&#20851;&#27880;&#12290;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30740;&#31350;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#30340;&#28176;&#36817;&#34892;&#20026;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#30495;&#23454;&#21442;&#25968;&#20026;0&#26102;&#65292;&#23545;&#25239;&#24615;&#35757;&#32451;&#20272;&#35745;&#22120;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#26497;&#38480;&#20998;&#24067;&#21487;&#33021;&#22312;0&#22788;&#26377;&#19968;&#20010;&#27491;&#27010;&#29575;&#36136;&#37327;&#65292;&#20026;&#30456;&#20851;&#30340;&#31232;&#30095;&#24615;&#24674;&#22797;&#33021;&#21147;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#36807;&#31243;&#8212;&#8212;&#33258;&#36866;&#24212;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#22312;$\ell_\infty$-&#25200;&#21160;&#19979;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#36807;&#31243;&#21487;&#20197;&#23454;&#29616;&#28176;&#36817;&#26080;&#20559;&#24615;&#21644;&#21464;&#37327;&#36873;&#25321;&#19968;&#33268;&#24615;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#31232;&#30095;&#24615;&#24674;&#22797;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training has been proposed to hedge against adversarial attacks in machine learning and statistical models. This paper focuses on adversarial training under $\ell_\infty$-perturbation, which has recently attracted much research attention. The asymptotic behavior of the adversarial training estimator is investigated in the generalized linear model. The results imply that the limiting distribution of the adversarial training estimator under $\ell_\infty$-perturbation could put a positive probability mass at $0$ when the true parameter is $0$, providing a theoretical guarantee of the associated sparsity-recovery ability. Alternatively, a two-step procedure is proposed -adaptive adversarial training, which could further improve the performance of adversarial training under $\ell_\infty$-perturbation. Specifically, the proposed procedure could achieve asymptotic unbiasedness and variable-selection consistency. Numerical experiments are conducted to show the sparsity-recovery a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#19981;&#21464;&#35270;&#35273;&#25551;&#36848;&#31526;&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#12290;&#35813;&#26694;&#26550;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.08603</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#35299;&#32534;&#30721;&#22120;&#35774;&#35745;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning in a Decomposed Encoder Design for Bio-inspired Hebbian Learning. (arXiv:2401.08603v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08603
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#19981;&#21516;&#30340;&#19981;&#21464;&#35270;&#35273;&#25551;&#36848;&#31526;&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#12290;&#35813;&#26694;&#26550;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#36739;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#35774;&#35745;&#21033;&#29992;&#20102;&#23545;&#26550;&#26500;&#32467;&#26500;&#30340;&#24402;&#32435;&#20559;&#35265;&#12289;&#19981;&#21464;&#24615;&#21644;&#31561;&#21464;&#24615;&#35201;&#27714;&#12289;&#20219;&#21153;&#29305;&#23450;&#30340;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#35745;&#31639;&#20248;&#21270;&#24037;&#20855;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#32534;&#30721;&#22120;&#30340;&#26089;&#26399;&#23618;&#20013;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#20197;&#20154;&#20026;&#25351;&#23450;&#30340;&#20934;&#19981;&#21464;&#28388;&#27874;&#22120;&#30340;&#24418;&#24335;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24402;&#32435;&#20559;&#35265;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#40065;&#26834;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#26412;&#25991;&#22312;&#29983;&#29289;&#21551;&#21457;&#24335;Hebbian&#23398;&#20064;&#30340;&#34920;&#31034;&#23398;&#20064;&#19978;&#36827;&#19968;&#27493;&#25506;&#32034;&#20102;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#20351;&#29992;&#29983;&#29289;&#21551;&#21457;&#24335;&#30340;&#23545;&#27604;&#39044;&#27979;&#32534;&#30721;&#65288;Hinge CLAPP Loss&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#30001;&#22810;&#20010;&#24182;&#34892;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#27599;&#20010;&#32534;&#30721;&#22120;&#21033;&#29992;&#19981;&#21516;&#30340;&#19981;&#21464;&#35270;&#35273;&#25551;&#36848;&#31526;&#20316;&#20026;&#24402;&#32435;&#20559;&#35265;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#38590;&#24230;&#30340;&#22270;&#20687;&#25968;&#25454;&#19978;&#30340;&#20998;&#31867;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#31995;&#32479;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65288;GTSRB, STL10, CODEBR&#65289;
&lt;/p&gt;
&lt;p&gt;
Modern data-driven machine learning system designs exploit inductive biases on architectural structure, invariance and equivariance requirements, task specific loss functions, and computational optimization tools. Previous works have illustrated that inductive bias in the early layers of the encoder in the form of human specified quasi-invariant filters can serve as a powerful inductive bias to attain better robustness and transparency in learned classifiers. This paper explores this further in the context of representation learning with local plasticity rules i.e. bio-inspired Hebbian learning . We propose a modular framework trained with a bio-inspired variant of contrastive predictive coding (Hinge CLAPP Loss). Our framework is composed of parallel encoders each leveraging a different invariant visual descriptor as an inductive bias. We evaluate the representation learning capacity of our system in a classification scenario on image data of various difficulties (GTSRB, STL10, CODEBR
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.04364</link><description>&lt;p&gt;
SoK&#65306;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
SoK: Facial Deepfake Detectors. (arXiv:2401.04364v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#38754;&#37096;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#20840;&#38754;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#24433;&#21709;&#22240;&#32032;&#30340;&#28145;&#20837;&#35265;&#35299;&#65292;&#24182;&#22312;&#21508;&#31181;&#25915;&#20987;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#36805;&#36895;&#25104;&#20026;&#23545;&#31038;&#20250;&#26500;&#25104;&#28145;&#36828;&#21644;&#20005;&#37325;&#23041;&#32961;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20027;&#35201;&#30001;&#20110;&#20854;&#26131;&#20110;&#21046;&#20316;&#21644;&#20256;&#25773;&#12290;&#36825;&#31181;&#24773;&#20917;&#21152;&#36895;&#20102;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#22312;&#39564;&#35777;&#26102; heavily &#20381;&#36182;&#23454;&#39564;&#23460;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#36825;&#21487;&#33021;&#26080;&#27861;&#26377;&#25928;&#22320;&#35753;&#23427;&#20204;&#24212;&#23545;&#26032;&#39062;&#12289;&#26032;&#20852;&#21644;&#23454;&#38469;&#30340;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#12290;&#26412;&#25991;&#23545;&#26368;&#26032;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#22120;&#36827;&#34892;&#24191;&#27867;&#20840;&#38754;&#30340;&#22238;&#39038;&#21644;&#20998;&#26512;&#65292;&#26681;&#25454;&#20960;&#20010;&#20851;&#38190;&#26631;&#20934;&#23545;&#23427;&#20204;&#36827;&#34892;&#35780;&#20272;&#12290;&#36825;&#20123;&#26631;&#20934;&#23558;&#36825;&#20123;&#26816;&#27979;&#22120;&#20998;&#20026; 4 &#20010;&#39640;&#32423;&#32452;&#21035;&#21644; 13 &#20010;&#32454;&#31890;&#24230;&#23376;&#32452;&#21035;&#65292;&#37117;&#36981;&#24490;&#19968;&#20010;&#32479;&#19968;&#30340;&#26631;&#20934;&#27010;&#24565;&#26694;&#26550;&#12290;&#36825;&#31181;&#20998;&#31867;&#21644;&#26694;&#26550;&#25552;&#20379;&#20102;&#23545;&#24433;&#21709;&#26816;&#27979;&#22120;&#21151;&#25928;&#30340;&#22240;&#32032;&#30340;&#28145;&#20837;&#21644;&#23454;&#29992;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#23545; 16 &#20010;&#20027;&#35201;&#30340;&#26816;&#27979;&#22120;&#22312;&#21508;&#31181;&#26631;&#20934;&#30340;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26222;&#36866;&#24615;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#40657;&#30418;&#25915;&#20987;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes have rapidly emerged as a profound and serious threat to society, primarily due to their ease of creation and dissemination. This situation has triggered an accelerated development of deepfake detection technologies. However, many existing detectors rely heavily on lab-generated datasets for validation, which may not effectively prepare them for novel, emerging, and real-world deepfake techniques. In this paper, we conduct an extensive and comprehensive review and analysis of the latest state-of-the-art deepfake detectors, evaluating them against several critical criteria. These criteria facilitate the categorization of these detectors into 4 high-level groups and 13 fine-grained sub-groups, all aligned with a unified standard conceptual framework. This classification and framework offer deep and practical insights into the factors that affect detector efficacy. We assess the generalizability of 16 leading detectors across various standard attack scenarios, including black-bo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#29983;&#25104;&#26694;&#26550;&#65288;ScoreAG&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#25110;&#26032;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#20445;&#25345;&#22270;&#20687;&#30340;&#26680;&#24515;&#35821;&#20041;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.04285</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#22270;&#20687;&#29983;&#25104;&#35780;&#20272;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Assessing Robustness via Score-Based Adversarial Image Generation. (arXiv:2310.04285v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04285
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#29983;&#25104;&#26694;&#26550;&#65288;ScoreAG&#65289;&#65292;&#21487;&#20197;&#29983;&#25104;&#36229;&#36807;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#24182;&#36890;&#36807;&#22270;&#20687;&#36716;&#25442;&#25110;&#26032;&#22270;&#20687;&#21512;&#25104;&#30340;&#26041;&#27861;&#20445;&#25345;&#22270;&#20687;&#30340;&#26680;&#24515;&#35821;&#20041;&#65292;&#22823;&#22823;&#22686;&#24378;&#20102;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#37117;&#38598;&#20013;&#22312;&#23567;&#30340;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#20869;&#30340;&#25200;&#21160;&#19978;&#12290;&#28982;&#32780;&#65292;$\ell_p$&#23041;&#32961;&#27169;&#22411;&#26080;&#27861;&#25429;&#25417;&#21040;&#25152;&#26377;&#30456;&#20851;&#30340;&#20445;&#30041;&#35821;&#20041;&#30340;&#25200;&#21160;&#65292;&#22240;&#27492;&#65292;&#40065;&#26834;&#24615;&#35780;&#20272;&#30340;&#33539;&#22260;&#26159;&#26377;&#38480;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#20998;&#25968;&#30340;&#23545;&#25239;&#29983;&#25104;&#65288;ScoreAG&#65289;&#65292;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;&#20998;&#25968;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#23637;&#26469;&#29983;&#25104;&#36229;&#36807;$\ell_p$-&#33539;&#25968;&#32422;&#26463;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#26080;&#38480;&#21046;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20811;&#26381;&#20102;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;ScoreAG&#22312;&#29983;&#25104;&#36924;&#30495;&#30340;&#23545;&#25239;&#24615;&#31034;&#20363;&#26102;&#20445;&#25345;&#22270;&#20687;&#30340;&#26680;&#24515;&#35821;&#20041;&#65292;&#21487;&#20197;&#36890;&#36807;&#36716;&#25442;&#29616;&#26377;&#22270;&#20687;&#25110;&#23436;&#20840;&#20174;&#38646;&#24320;&#22987;&#21512;&#25104;&#26032;&#22270;&#20687;&#30340;&#26041;&#24335;&#23454;&#29616;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;ScoreAG&#30340;&#29983;&#25104;&#33021;&#21147;&#26469;&#20928;&#21270;&#22270;&#20687;&#65292;&#20174;&#32463;&#39564;&#19978;&#22686;&#24378;&#20998;&#31867;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#30340;&#22823;&#37327;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;ScoreAG&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#23545;&#25239;&#25915;&#20987;&#26041;&#27861;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most adversarial attacks and defenses focus on perturbations within small $\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all relevant semantic-preserving perturbations, and hence, the scope of robustness evaluations is limited. In this work, we introduce Score-Based Adversarial Generation (ScoreAG), a novel framework that leverages the advancements in score-based generative models to generate adversarial examples beyond $\ell_p$-norm constraints, so-called unrestricted adversarial examples, overcoming their limitations. Unlike traditional methods, ScoreAG maintains the core semantics of images while generating realistic adversarial examples, either by transforming existing images or synthesizing new ones entirely from scratch. We further exploit the generative capability of ScoreAG to purify images, empirically enhancing the robustness of classifiers. Our extensive empirical evaluation demonstrates that ScoreAG matches the performance of state-of-the-art atta
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#30340;&#22312;&#32447;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#24322;&#24120;&#20540;&#27745;&#26579;&#21644;&#37325;&#23614;&#22870;&#21169;&#30340;&#38382;&#39064;&#26041;&#38754;&#24341;&#20837;&#20102;&#40065;&#26834;&#32479;&#35745;&#23398;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22312;&#32447;&#30340;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#37327;&#30340;&#26497;&#38480;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2310.02581</link><description>&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#30340;&#22312;&#32447;&#20272;&#35745;&#21644;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Online Estimation and Inference for Robust Policy Evaluation in Reinforcement Learning. (arXiv:2310.02581v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02581
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#30340;&#22312;&#32447;&#20272;&#35745;&#21644;&#25512;&#26029;&#26041;&#27861;&#65292;&#22312;&#35299;&#20915;&#24322;&#24120;&#20540;&#27745;&#26579;&#21644;&#37325;&#23614;&#22870;&#21169;&#30340;&#38382;&#39064;&#26041;&#38754;&#24341;&#20837;&#20102;&#40065;&#26834;&#32479;&#35745;&#23398;&#30340;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#22312;&#32447;&#30340;&#32479;&#35745;&#25512;&#26029;&#36807;&#31243;&#65292;&#24182;&#24314;&#31435;&#20102;&#20272;&#35745;&#37327;&#30340;&#26497;&#38480;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#24378;&#21270;&#23398;&#20064;&#22312;&#29616;&#20195;&#32479;&#35745;&#23398;&#20013;&#22791;&#21463;&#20851;&#27880;&#65292;&#31574;&#30053;&#35780;&#20272;&#26159;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#19982;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#19978;&#23545;&#35813;&#20027;&#39064;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35745;&#31639;&#30340;&#21442;&#25968;&#20272;&#35745;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#20998;&#26512;&#20551;&#35774;&#38543;&#26426;&#22870;&#21169;&#36981;&#24490;&#26631;&#20934;&#20998;&#24067;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;&#32479;&#19968;&#26694;&#26550;&#20013;&#21516;&#26102;&#35299;&#20915;&#20102;&#24322;&#24120;&#20540;&#27745;&#26579;&#21644;&#37325;&#23614;&#22870;&#21169;&#30340;&#38382;&#39064;&#65292;&#20174;&#32780;&#25317;&#25265;&#20102;&#40065;&#26834;&#32479;&#35745;&#23398;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#40065;&#26834;&#31574;&#30053;&#35780;&#20272;&#36807;&#31243;&#65292;&#24182;&#26681;&#25454;&#20854;Bahadur&#34920;&#31034;&#24314;&#31435;&#20102;&#25105;&#20204;&#20272;&#35745;&#37327;&#30340;&#26497;&#38480;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#22312;&#32447;&#30340;&#36807;&#31243;&#65292;&#20197;&#39640;&#25928;&#22320;&#36827;&#34892;&#22522;&#20110;&#28176;&#36817;&#20998;&#24067;&#30340;&#32479;&#35745;&#25512;&#26029;&#12290;&#36825;&#31687;&#35770;&#25991;&#22635;&#34917;&#20102;&#24378;&#21270;&#23398;&#20064;&#20013;&#40065;&#26834;&#32479;&#35745;&#23398;&#21644;&#32479;&#35745;&#25512;&#26029;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, reinforcement learning has gained prominence in modern statistics, with policy evaluation being a key component. Unlike traditional machine learning literature on this topic, our work places emphasis on statistical inference for the parameter estimates computed using reinforcement learning algorithms. While most existing analyses assume random rewards to follow standard distributions, limiting their applicability, we embrace the concept of robust statistics in reinforcement learning by simultaneously addressing issues of outlier contamination and heavy-tailed rewards within a unified framework. In this paper, we develop an online robust policy evaluation procedure, and establish the limiting distribution of our estimator, based on its Bahadur representation. Furthermore, we develop a fully-online procedure to efficiently conduct statistical inference based on the asymptotic distribution. This paper bridges the gap between robust statistics and statistical inference in reinfor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#23494;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27969;&#31243;&#21644;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#22312;&#22810;&#20010;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.02373</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#23433;&#20840;&#26377;&#25928;&#25968;&#25454;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Secure and Effective Data Appraisal for Machine Learning. (arXiv:2310.02373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26426;&#23494;&#30340;&#25968;&#25454;&#36873;&#25321;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#21019;&#26032;&#30340;&#27969;&#31243;&#21644;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;&#25805;&#20316;&#26469;&#23454;&#29616;&#65292;&#20197;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#24182;&#22312;&#22810;&#20010;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#26080;&#25304;&#26080;&#26463;&#30340;&#25968;&#25454;&#24066;&#22330;&#38656;&#35201;&#22312;&#25968;&#25454;&#25152;&#26377;&#32773;&#21644;&#27169;&#22411;&#25152;&#26377;&#32773;&#26368;&#32456;&#20132;&#26131;&#21069;&#33021;&#22815;&#23545;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#31169;&#23494;&#36873;&#25321;&#21644;&#35780;&#20272;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#38544;&#31169;&#65292;&#36825;&#20010;&#36807;&#31243;&#28041;&#21450;&#20351;&#29992;&#22810;&#26041;&#35745;&#31639;(MPC)&#26469;&#23457;&#26597;&#30446;&#26631;&#27169;&#22411;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#35748;&#20026;&#22522;&#20110;MPC&#30340;Transformer&#27169;&#22411;&#35780;&#20272;&#36807;&#20110;&#32791;&#36153;&#36164;&#28304;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#25968;&#25454;&#36873;&#25321;&#25104;&#20026;&#21487;&#34892;&#30340;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#21253;&#25324;&#19977;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;(1)&#20351;&#29992;MPC&#36827;&#34892;&#26426;&#23494;&#25968;&#25454;&#36873;&#25321;&#30340;&#24320;&#21019;&#24615;&#27969;&#31243;&#65307;(2)&#36890;&#36807;&#22312;&#26377;&#38480;&#30340;&#30456;&#20851;&#25968;&#25454;&#23376;&#38598;&#19978;&#35757;&#32451;&#31616;&#21270;&#30340;&#20302;&#32500;&#24230;MLP&#26469;&#22797;&#21046;&#22797;&#26434;&#30340;&#39640;&#32500;&#24230;&#25805;&#20316;&#65307;(3)&#24182;&#21457;&#12289;&#22810;&#38454;&#27573;&#22320;&#23454;&#29616;MPC&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#31995;&#21015;Transformer&#27169;&#22411;&#21644;NLP/CV&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#19982;&#30452;&#25509;&#22522;&#20110;MPC&#30340;&#35780;&#20272;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
Essential for an unfettered data market is the ability to discreetly select and evaluate training data before finalizing a transaction between the data owner and model owner. To safeguard the privacy of both data and model, this process involves scrutinizing the target model through Multi-Party Computation (MPC). While prior research has posited that the MPC-based evaluation of Transformer models is excessively resource-intensive, this paper introduces an innovative approach that renders data selection practical. The contributions of this study encompass three pivotal elements: (1) a groundbreaking pipeline for confidential data selection using MPC, (2) replicating intricate high-dimensional operations with simplified low-dimensional MLPs trained on a limited subset of pertinent data, and (3) implementing MPC in a concurrent, multi-phase manner. The proposed method is assessed across an array of Transformer models and NLP/CV benchmarks. In comparison to the direct MPC-based evaluation 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01405</link><description>&lt;p&gt;
&#34920;&#31034;&#24037;&#31243;&#21270;&#65306;AI&#36879;&#26126;&#21270;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Representation Engineering: A Top-Down Approach to AI Transparency. (arXiv:2310.01405v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01405
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#30340;&#33258;&#19978;&#32780;&#19979;&#26041;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#26041;&#27861;&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#20026;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#35299;&#20915;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#24182;&#25551;&#36848;&#20102;&#34920;&#31034;&#24037;&#31243;&#21270;&#65288;RepE&#65289;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20511;&#37492;&#35748;&#30693;&#31070;&#32463;&#31185;&#23398;&#30340;&#35265;&#35299;&#26469;&#22686;&#24378;AI&#31995;&#32479;&#36879;&#26126;&#24615;&#30340;&#26041;&#27861;&#12290;RepE&#23558;&#38598;&#32676;&#32423;&#21035;&#30340;&#34920;&#31034;&#25918;&#22312;&#20998;&#26512;&#30340;&#26680;&#24515;&#65292;&#32780;&#19981;&#26159;&#31070;&#32463;&#20803;&#25110;&#30005;&#36335;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#30417;&#27979;&#21644;&#25805;&#32437;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#39640;&#32423;&#35748;&#30693;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;RepE&#25216;&#26415;&#30340;&#22522;&#20934;&#21644;&#21021;&#27493;&#20998;&#26512;&#65292;&#26174;&#31034;&#23427;&#20204;&#25552;&#20379;&#20102;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25913;&#21892;&#25105;&#20204;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#21644;&#25511;&#21046;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22914;&#20309;&#22312;&#21253;&#25324;&#35802;&#23454;&#24615;&#12289;&#26080;&#23475;&#24615;&#12289;&#36861;&#27714;&#26435;&#21147;&#31561;&#19968;&#31995;&#21015;&#19982;&#23433;&#20840;&#30456;&#20851;&#30340;&#38382;&#39064;&#19978;&#21457;&#25381;&#20316;&#29992;&#65292;&#23637;&#31034;&#20102;&#33258;&#19978;&#32780;&#19979;&#36879;&#26126;&#24615;&#30740;&#31350;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#39033;&#24037;&#20316;&#33021;&#22815;&#20419;&#36827;RepE&#30340;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#24182;&#25512;&#21160;AI&#31995;&#32479;&#30340;&#36879;&#26126;&#24615;&#21644;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we identify and characterize the emerging area of representation engineering (RepE), an approach to enhancing the transparency of AI systems that draws on insights from cognitive neuroscience. RepE places population-level representations, rather than neurons or circuits, at the center of analysis, equipping us with novel methods for monitoring and manipulating high-level cognitive phenomena in deep neural networks (DNNs). We provide baselines and an initial analysis of RepE techniques, showing that they offer simple yet effective solutions for improving our understanding and control of large language models. We showcase how these methods can provide traction on a wide range of safety-relevant problems, including honesty, harmlessness, power-seeking, and more, demonstrating the promise of top-down transparency research. We hope that this work catalyzes further exploration of RepE and fosters advancements in the transparency and safety of AI systems.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;&#23558;&#26435;&#37325;&#25353;&#36755;&#20837;&#36890;&#36947;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#65292;&#21487;&#20197;&#35299;&#20915;&#28608;&#27963;&#24322;&#24120;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#20351;&#24471;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15531</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models. (arXiv:2309.15531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;&#23558;&#26435;&#37325;&#25353;&#36755;&#20837;&#36890;&#36947;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#65292;&#21487;&#20197;&#35299;&#20915;&#28608;&#27963;&#24322;&#24120;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#20351;&#24471;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#36817;&#26399;&#23637;&#31034;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#25928;&#22320;&#20026;LLMs&#25552;&#20379;&#26381;&#21153;&#26041;&#38754;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#22823;&#20869;&#23384;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#25209;&#37327;&#25512;&#29702;&#35774;&#32622;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#20013;&#12290;&#20165;&#23545;&#26435;&#37325;&#36827;&#34892;&#37327;&#21270;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#23384;&#22312;&#22823;&#24133;&#24230;&#28608;&#27963;&#24322;&#24120;&#20540;&#65292;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21487;&#21462;&#30340;&#24322;&#24120;&#25928;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#65288;IC&#65289;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#30340;per-IC&#37327;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#27599;&#20010;&#36755;&#20986;&#36890;&#36947;&#65288;OC&#65289;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21160;&#26426;&#26159;&#35266;&#23519;&#21040;&#28608;&#27963;&#24322;&#24120;&#20540;&#24433;&#21709;&#26435;&#37325;&#30697;&#38453;&#30340;&#36755;&#20837;&#32500;&#24230;&#65292;&#22240;&#27492;&#22312;IC&#26041;&#21521;&#19978;&#23545;&#26435;&#37325;&#36827;&#34892;&#31867;&#20284;&#20998;&#32452;&#21487;&#20197;&#23558;&#24322;&#24120;&#20540;&#38548;&#31163;&#21040;&#19968;&#20010;&#20998;&#32452;&#20869;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#28608;&#27963;&#30340;&#24322;&#24120;&#20540;&#24182;&#19981;&#20915;&#23450;&#37327;&#21270;&#30340;&#38590;&#24230;&#65292;&#20854;&#22266;&#26377;&#30340;&#26435;&#37325;&#25935;&#24863;&#24615;&#20063;&#23384;&#22312;&#12290;&#36890;&#36807;per-IC&#37327;&#21270;&#20316;&#20026;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36866;&#24212;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#24182;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.16471</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#38544;&#24335;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#31574;&#30053;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy Adaptation Method for Implicit Multitask Reinforcement Learning Problems. (arXiv:2308.16471v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#30340;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#36866;&#24212;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#24182;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21160;&#24577;&#36816;&#21160;&#29983;&#25104;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#25509;&#35302;&#21644;&#30896;&#25758;&#65292;&#31574;&#30053;&#21442;&#25968;&#30340;&#23567;&#25913;&#21464;&#21487;&#33021;&#23548;&#33268;&#26497;&#20854;&#19981;&#21516;&#30340;&#22238;&#25253;&#12290;&#20363;&#22914;&#65292;&#22312;&#36275;&#29699;&#20013;&#65292;&#36890;&#36807;&#31245;&#24494;&#25913;&#21464;&#36386;&#29699;&#20301;&#32622;&#25110;&#26045;&#21152;&#29699;&#30340;&#21147;&#25110;&#32773;&#29699;&#30340;&#25705;&#25830;&#21147;&#21457;&#29983;&#21464;&#21270;&#65292;&#29699;&#21487;&#20197;&#20197;&#23436;&#20840;&#19981;&#21516;&#30340;&#26041;&#21521;&#39134;&#34892;&#12290;&#28982;&#32780;&#65292;&#24456;&#38590;&#24819;&#35937;&#22312;&#19981;&#21516;&#30340;&#26041;&#21521;&#19978;&#22836;&#29699;&#38656;&#35201;&#23436;&#20840;&#19981;&#21516;&#30340;&#25216;&#33021;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#36816;&#21160;&#31867;&#21035;&#20013;&#36866;&#24212;&#30446;&#26631;&#25110;&#29615;&#22659;&#30340;&#38544;&#24335;&#21464;&#21270;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#22870;&#21169;&#20989;&#25968;&#25110;&#29615;&#22659;&#30340;&#29289;&#29702;&#21442;&#25968;&#12290;&#25105;&#20204;&#21033;&#29992;&#21333;&#33050;&#26426;&#22120;&#20154;&#27169;&#22411;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#22312;&#22836;&#29699;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#36866;&#24212;&#25928;&#26524;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#36866;&#24212;&#30446;&#26631;&#20301;&#32622;&#30340;&#38544;&#24335;&#21464;&#21270;&#25110;&#29699;&#30340;&#24674;&#22797;&#31995;&#25968;&#30340;&#21464;&#21270;&#65292;&#32780;&#26631;&#20934;&#30340;&#39046;&#22495;&#38543;&#26426;&#21270;&#26041;&#27861;&#21017;&#19981;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In dynamic motion generation tasks, including contact and collisions, small changes in policy parameters can lead to extremely different returns. For example, in soccer, the ball can fly in completely different directions with a similar heading motion by slightly changing the hitting position or the force applied to the ball or when the friction of the ball varies. However, it is difficult to imagine that completely different skills are needed for heading a ball in different directions. In this study, we proposed a multitask reinforcement learning algorithm for adapting a policy to implicit changes in goals or environments in a single motion category with different reward functions or physical parameters of the environment. We evaluated the proposed method on the ball heading task using a monopod robot model. The results showed that the proposed method can adapt to implicit changes in the goal positions or the coefficients of restitution of the ball, whereas the standard domain randomi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#31639;&#27861;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.14872</link><description>&lt;p&gt;
&#32447;&#24615;&#36172;&#21338;&#26426;&#20013;&#24179;&#34913;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#20960;&#20309;&#24863;&#30693;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Geometry-Aware Approaches for Balancing Performance and Theoretical Guarantees in Linear Bandits. (arXiv:2306.14872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14872
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#34913;&#31639;&#27861;&#24615;&#33021;&#19982;&#29702;&#35770;&#20445;&#35777;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#32447;&#24615;&#36172;&#21338;&#26426;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#23454;&#35777;&#24615;&#33021;&#19982;&#24754;&#35266;&#29702;&#35770;&#21518;&#24724;&#30028;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#24615;&#21551;&#21457;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#65292;&#36319;&#36394;&#19981;&#30830;&#23450;&#24230;&#26925;&#29699;&#20307;&#30340;&#20960;&#20309;&#24418;&#29366;&#65292;&#20026;&#21253;&#25324;&#36138;&#24515;&#12289;OFUL&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#31639;&#27861;&#22312;&#20869;&#30340;&#24191;&#27867;&#31639;&#27861;&#31867;&#24314;&#31435;&#23454;&#20363;&#30456;&#20851;&#30340;&#39057;&#29575;&#21518;&#24724;&#30028;&#65292;&#22312;&#20445;&#30041;&#22522;&#26412;&#31639;&#27861;&#22823;&#37096;&#20998;&#20248;&#33391;&#29305;&#24615;&#30340;&#21516;&#26102;&#8220;&#26657;&#27491;&#8221;&#22522;&#26412;&#31639;&#27861;&#22312;&#26576;&#20123;&#23454;&#20363;&#20013;&#34920;&#29616;&#24046;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#28176;&#36817;&#26368;&#20248;&#21518;&#24724;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is motivated by recent developments in the linear bandit literature, which have revealed a discrepancy between the promising empirical performance of algorithms such as Thompson sampling and Greedy, when compared to their pessimistic theoretical regret bounds. The challenge arises from the fact that while these algorithms may perform poorly in certain problem instances, they generally excel in typical instances. To address this, we propose a new data-driven technique that tracks the geometry of the uncertainty ellipsoid, enabling us to establish an instance-dependent frequentist regret bound for a broad class of algorithms, including Greedy, OFUL, and Thompson sampling. This result empowers us to identify and ``course-correct" instances in which the base algorithms perform poorly. The course-corrected algorithms achieve the minimax optimal regret of order $\tilde{\mathcal{O}}(d\sqrt{T})$, while retaining most of the desirable properties of the base algorithms. We present sim
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-SSAD&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.12033</link><description>&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#31471;&#21040;&#31471;&#22686;&#24378;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
End-to-End Augmentation Hyperparameter Tuning for Self-Supervised Anomaly Detection. (arXiv:2306.12033v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12033
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-SSAD&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#31995;&#32479;&#22320;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#25552;&#39640;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#65292;&#23427;&#20026;&#29616;&#23454;&#38382;&#39064;&#25552;&#20379;&#33258;&#20135;&#29983;&#30340;&#30417;&#30563;&#20449;&#21495;&#65292;&#36991;&#20813;&#20102;&#32321;&#29712;&#30340;&#25163;&#21160;&#26631;&#27880;&#24037;&#20316;&#12290;SSL&#23545;&#20110;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#23588;&#20854;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#26631;&#35760;&#30340;&#24322;&#24120;&#36890;&#24120;&#19981;&#23384;&#22312;&#25110;&#38590;&#20197;&#33719;&#24471;&#12290;&#34429;&#28982;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;SSAD&#65289;&#36817;&#24180;&#26469;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#20294;&#25991;&#29486;&#21364;&#26410;&#23558;&#25968;&#25454;&#22686;&#24378;&#35270;&#20026;&#36229;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22686;&#24378;&#36873;&#25321;&#23545;&#26816;&#27979;&#24615;&#33021;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;ST-SSAD&#65288;&#33258;&#25105;&#35843;&#25972;&#33258;&#25105;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20851;&#20110;&#20005;&#26684;&#35843;&#25972;&#22686;&#24378;&#30340;SSAD&#30340;&#31532;&#19968;&#20010;&#31995;&#32479;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#36129;&#29486;&#12290;&#31532;&#19968;&#26159;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#39564;&#35777;&#25439;&#22833;&#20989;&#25968;&#65292;&#37327;&#21270;&#22686;&#24378;&#35757;&#32451;&#25968;&#25454;&#19982;&#65288;&#26080;&#26631;&#31614;&#65289;&#27979;&#35797;&#25968;&#25454;&#20043;&#38388;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;&#22312;&#21407;&#21017;&#19978;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26368;&#36817;&#39640;&#25928;&#30340;&#26377;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20511;&#37492;&#30340;&#26080;&#30417;&#30563;&#39564;&#35777;&#26041;&#26696;&#21644;&#22686;&#24378;&#25968;&#25454;&#25628;&#32034;&#31574;&#30053;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;SSAD&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#25628;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#24418;&#24335;&#65292;&#23558;&#36731;&#37327;&#32423;&#25968;&#25454;&#22686;&#24378;&#25628;&#32034;&#22120;&#30340;&#31616;&#21333;&#38598;&#25104;&#12290;&#22312;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22686;&#24378;&#35843;&#25972;&#26041;&#27861;&#30456;&#23545;&#20110;&#20197;&#21069;&#30340;&#26368;&#26032;&#32467;&#26524;&#21487;&#20197;&#33719;&#24471;&#19968;&#33268;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#26368;&#36817;&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has emerged as a promising paradigm that presents self-generated supervisory signals to real-world problems, bypassing the extensive manual labeling burden. SSL is especially attractive for unsupervised tasks such as anomaly detection, where labeled anomalies are often nonexistent and costly to obtain. While self-supervised anomaly detection (SSAD) has seen a recent surge of interest, the literature has failed to treat data augmentation as a hyperparameter. Meanwhile, recent works have reported that the choice of augmentation has significant impact on detection performance. In this paper, we introduce ST-SSAD (Self-Tuning Self-Supervised Anomaly Detection), the first systematic approach to SSAD in regards to rigorously tuning augmentation. To this end, our work presents two key contributions. The first is a new unsupervised validation loss that quantifies the alignment between the augmented training data and the (unlabeled) test data. In principle we adop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2306.11908</link><description>&lt;p&gt;
&#22522;&#20110;&#23450;&#28857;&#26641;&#30340;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Accelerating Generalized Random Forests with Fixed-Point Trees. (arXiv:2306.11908v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11908
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#20351;&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#22312;&#26080;&#26799;&#24230;&#20248;&#21270;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#33410;&#30465;&#20102;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#20041;&#38543;&#26426;&#26862;&#26519;&#24314;&#31435;&#22312;&#20256;&#32479;&#38543;&#26426;&#26862;&#26519;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#33258;&#36866;&#24212;&#26680;&#21152;&#26435;&#31639;&#27861;&#26469;&#26500;&#24314;&#20272;&#31639;&#22120;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#26641;&#29983;&#38271;&#36807;&#31243;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26641;&#29983;&#38271;&#35268;&#21017;&#65292;&#22522;&#20110;&#23450;&#28857;&#36845;&#20195;&#36817;&#20284;&#34920;&#31034;&#26799;&#24230;&#36817;&#20284;&#65292;&#23454;&#29616;&#20102;&#26080;&#26799;&#24230;&#20248;&#21270;&#65292;&#24182;&#20026;&#27492;&#24320;&#21457;&#20102;&#28176;&#36817;&#29702;&#35770;&#12290;&#36825;&#26377;&#25928;&#22320;&#33410;&#30465;&#20102;&#26102;&#38388;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#37327;&#30340;&#32500;&#24230;&#36866;&#20013;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalized random forests arXiv:1610.01271 build upon the well-established success of conventional forests (Breiman, 2001) to offer a flexible and powerful non-parametric method for estimating local solutions of heterogeneous estimating equations. Estimators are constructed by leveraging random forests as an adaptive kernel weighting algorithm and implemented through a gradient-based tree-growing procedure. By expressing this gradient-based approximation as being induced from a single Newton-Raphson root-finding iteration, and drawing upon the connection between estimating equations and fixed-point problems arXiv:2110.11074, we propose a new tree-growing rule for generalized random forests induced from a fixed-point iteration type of approximation, enabling gradient-free optimization, and yielding substantial time savings for tasks involving even modest dimensionality of the target quantity (e.g. multiple/multi-level treatment effects). We develop an asymptotic theory for estimators o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#65288;FSA&#65289;&#26426;&#21046;&#26469;&#25913;&#21892;&#25968;&#25454;&#20013;&#24515;&#30340;&#33021;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#26469;&#39044;&#27979;&#27599;&#20010;&#26381;&#21153;&#30340;&#26410;&#26469;&#36127;&#36733;&#24182;&#33258;&#21160;&#31283;&#23450;&#30456;&#24212;&#30340;&#30446;&#26631;CPU&#20351;&#29992;&#29575;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2305.00706</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#21457;&#23637;&#32511;&#33394;&#25968;&#25454;&#20013;&#24515;&#30340;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Full Scaling Automation for Sustainable Development of Green Data Centers. (arXiv:2305.00706v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00706
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#65288;FSA&#65289;&#26426;&#21046;&#26469;&#25913;&#21892;&#25968;&#25454;&#20013;&#24515;&#30340;&#33021;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#35813;&#26426;&#21046;&#21033;&#29992;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#26469;&#39044;&#27979;&#27599;&#20010;&#26381;&#21153;&#30340;&#26410;&#26469;&#36127;&#36733;&#24182;&#33258;&#21160;&#31283;&#23450;&#30456;&#24212;&#30340;&#30446;&#26631;CPU&#20351;&#29992;&#29575;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20113;&#35745;&#31639;&#30340;&#24555;&#36895;&#23835;&#36215;&#23548;&#33268;&#25968;&#25454;&#20013;&#24515;&#30899;&#25490;&#25918;&#37327;&#24778;&#20154;&#22320;&#22686;&#21152;&#65292;&#29616;&#22312;&#21344;&#20840;&#29699;&#28201;&#23460;&#27668;&#20307;&#25490;&#25918;&#30340;&gt;3&#65285;&#65292;&#24517;&#39035;&#31435;&#21363;&#37319;&#21462;&#25514;&#26045;&#24212;&#23545;&#23427;&#20204;&#23545;&#20840;&#29699;&#27668;&#20505;&#26085;&#30410;&#22686;&#38271;&#30340;&#36127;&#25285;&#12290;&#36825;&#19968;&#21162;&#21147;&#30340;&#37325;&#28857;&#26159;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#20197;&#33410;&#30465;&#30005;&#21147;&#28040;&#32791;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20840;&#38754;&#33258;&#21160;&#21270;&#25193;&#23637;&#65288;FSA&#65289;&#26426;&#21046;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#38598;&#32676;&#20013;&#21160;&#24577;&#22320;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#24037;&#20316;&#36127;&#36733;&#65292;&#20351;&#25968;&#25454;&#20013;&#24515;&#20013;&#30340;&#38598;&#32676;&#20445;&#25345;&#20854;&#25152;&#38656;&#30340;CPU&#21033;&#29992;&#29575;&#30446;&#26631;&#65292;&#20174;&#32780;&#25913;&#21892;&#33021;&#28304;&#25928;&#29575;&#12290;FSA&#21033;&#29992;&#28145;&#24230;&#34920;&#24449;&#23398;&#20064;&#30340;&#23041;&#21147;&#26469;&#20934;&#30830;&#39044;&#27979;&#27599;&#20010;&#26381;&#21153;&#30340;&#26410;&#26469;&#24037;&#20316;&#36127;&#36733;&#65292;&#24182;&#33258;&#21160;&#31283;&#23450;&#30456;&#24212;&#30340;&#30446;&#26631;CPU&#20351;&#29992;&#29575;&#27700;&#24179;&#65292;&#19981;&#20687;&#20043;&#21069;&#30340;&#33258;&#21160;&#25193;&#23637;&#26041;&#27861;&#65292;&#22914;Autopilot&#25110;FIRM&#65292;&#38656;&#35201;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#21644;&#19987;&#23478;&#30693;&#35782;&#26469;&#35843;&#25972;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid rise in cloud computing has resulted in an alarming increase in data centers' carbon emissions, which now accounts for &gt;3% of global greenhouse gas emissions, necessitating immediate steps to combat their mounting strain on the global climate. An important focus of this effort is to improve resource utilization in order to save electricity usage. Our proposed Full Scaling Automation (FSA) mechanism is an effective method of dynamically adapting resources to accommodate changing workloads in large-scale cloud computing clusters, enabling the clusters in data centers to maintain their desired CPU utilization target and thus improve energy efficiency. FSA harnesses the power of deep representation learning to accurately predict the future workload of each service and automatically stabilize the corresponding target CPU usage level, unlike the previous autoscaling methods, such as Autopilot or FIRM, that need to adjust computing resources with statistical models and expert knowle
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#28857;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#25551;&#36848;&#28857;&#20113;&#20869;&#30340;&#25968;&#25454;&#28857;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22270;&#27169;&#22411;&#26041;&#27861;&#26356;&#20855;&#26377;&#20581;&#22766;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2303.16716</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#30340;&#28857;&#20113;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topological Point Cloud Clustering. (arXiv:2303.16716v1 [math.AT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25299;&#25169;&#30340;&#28857;&#32858;&#31867;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#21033;&#29992;&#25299;&#25169;&#29305;&#24449;&#25551;&#36848;&#28857;&#20113;&#20869;&#30340;&#25968;&#25454;&#28857;&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#22270;&#27169;&#22411;&#26041;&#27861;&#26356;&#20855;&#26377;&#20581;&#22766;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#25299;&#25169;&#28857;&#20113;&#32858;&#31867;&#65288;TPCC&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#22522;&#20110;&#28857;&#20113;&#23545;&#20110;&#20840;&#23616;&#25299;&#25169;&#29305;&#24449;&#30340;&#36129;&#29486;&#26469;&#32858;&#31867;&#28857;&#12290;TPCC&#20174;&#35889;&#32858;&#31867;&#21644;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#32508;&#21512;&#20102;&#26377;&#21033;&#30340;&#29305;&#24449;&#65292;&#22522;&#20110;&#32771;&#34385;&#19982;&#25152;&#32771;&#34385;&#30340;&#28857;&#20113;&#30456;&#20851;&#32852;&#30340;&#19968;&#20010;&#21333;&#24418;&#22797;&#21512;&#20307;&#30340;&#35889;&#29305;&#24615;&#12290;&#30001;&#20110;&#23427;&#22522;&#20110;&#32771;&#34385;&#31232;&#30095;&#29305;&#24449;&#21521;&#37327;&#35745;&#31639;&#65292;TPCC&#21516;&#26679;&#23481;&#26131;&#35299;&#37322;&#21644;&#23454;&#29616;&#65292;&#23601;&#20687;&#35889;&#32858;&#31867;&#19968;&#26679;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#19981;&#20165;&#20851;&#27880;&#19982;&#20174;&#28857;&#20113;&#25968;&#25454;&#21019;&#24314;&#30340;&#22270;&#30456;&#20851;&#32852;&#30340;&#21333;&#20010;&#30697;&#38453;&#65292;&#32780;&#26159;&#20851;&#27880;&#19982;&#24688;&#24403;&#26500;&#36896;&#30340;&#21333;&#24418;&#22797;&#21512;&#20307;&#30456;&#20851;&#32852;&#30340;&#25972;&#20010;Hodge-Laplacian&#30340;&#19968;&#25972;&#22871;&#30697;&#38453;&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#26356;&#20016;&#23500;&#30340;&#25299;&#25169;&#29305;&#24449;&#26469;&#25551;&#36848;&#28857;&#20113;&#20869;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#21463;&#30410;&#20110;&#25299;&#25169;&#25216;&#26415;&#30456;&#23545;&#20110;&#22122;&#22768;&#30340;&#30456;&#23545;&#20581;&#22766;&#24615;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#19978;&#27979;&#35797;&#20102;TPCC&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Topological Point Cloud Clustering (TPCC), a new method to cluster points in an arbitrary point cloud based on their contribution to global topological features. TPCC synthesizes desirable features from spectral clustering and topological data analysis and is based on considering the spectral properties of a simplicial complex associated to the considered point cloud. As it is based on considering sparse eigenvector computations, TPCC is similarly easy to interpret and implement as spectral clustering. However, by focusing not just on a single matrix associated to a graph created from the point cloud data, but on a whole set of Hodge-Laplacians associated to an appropriately constructed simplicial complex, we can leverage a far richer set of topological features to characterize the data points within the point cloud and benefit from the relative robustness of topological techniques against noise. We test the performance of TPCC on both synthetic and real-world data and compa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#21644;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#21327;&#35843;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#39044;&#27979;&#21644;&#21327;&#35843;&#30340;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.13706</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#21644;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#21327;&#35843;&#23454;&#29616;&#31471;&#21040;&#31471;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
End-to-End Modeling Hierarchical Time Series Using Autoregressive Transformer and Conditional Normalizing Flow based Reconciliation. (arXiv:2212.13706v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.13706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#21644;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#21327;&#35843;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#21516;&#26102;&#39044;&#27979;&#21644;&#21327;&#35843;&#30340;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#32467;&#26500;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#19981;&#20165;&#38656;&#35201;&#39044;&#27979;&#23618;&#27425;&#32467;&#26500;&#30340;&#27599;&#20010;&#32423;&#21035;&#65292;&#32780;&#19988;&#36824;&#38656;&#35201;&#21327;&#35843;&#25152;&#26377;&#30340;&#39044;&#27979;&#32467;&#26524;&#20197;&#30830;&#20445;&#19968;&#33268;&#24615;&#65292;&#21363;&#39044;&#27979;&#32467;&#26524;&#24212;&#28385;&#36275;&#23618;&#27425;&#32858;&#21512;&#32422;&#26463;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#20998;&#23618;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#65292;&#22522;&#20110;&#26465;&#20214;&#27491;&#24577;&#21270;&#27969;&#30340;&#33258;&#22238;&#24402;&#21464;&#21387;&#22120;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#21516;&#26102;&#36827;&#34892;&#39044;&#27979;&#21644;&#21327;&#35843;&#65292;&#26080;&#38656;&#20219;&#20309;&#26174;&#24335;&#30340;&#21518;&#22788;&#29702;&#27493;&#39588;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#24230;&#27169;&#22411;&#30340;&#24378;&#22823;&#21151;&#33021;&#65292;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20559;&#24046;&#20272;&#35745;&#31561;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series forecasting with hierarchical structure is pervasive in real-world applications, demanding not only predicting each level of the hierarchy, but also reconciling all forecasts to ensure coherency, i.e., the forecasts should satisfy the hierarchical aggregation constraints. Moreover, the disparities of statistical characteristics between levels can be huge, worsened by non-Gaussian distributions and non-linear correlations. To this extent, we propose a novel end-to-end hierarchical time series forecasting model, based on conditioned normalizing flow-based autoregressive transformer reconciliation, to represent complex data distribution while simultaneously reconciling the forecasts to ensure coherency. Unlike other state-of-the-art methods, we achieve the forecasting and reconciliation simultaneously without requiring any explicit post-processing step. In addition, by harnessing the power of deep model, we do not rely on any assumption such as unbiased estimates 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ReX&#65292;&#19968;&#20010;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26080;&#20851;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;&#35299;&#37322;&#28155;&#21152;&#26102;&#38388;&#20449;&#24687;&#65292;&#20351;&#19968;&#20123;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#24212;&#29992;&#30340;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;</title><link>http://arxiv.org/abs/2209.03798</link><description>&lt;p&gt;
ReX&#65306;&#19968;&#20010;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26080;&#20851;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
ReX: A Framework for Incorporating Temporal Information in Model-Agnostic Local Explanation Techniques. (arXiv:2209.03798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.03798
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ReX&#65292;&#19968;&#20010;&#23558;&#26102;&#38388;&#20449;&#24687;&#34701;&#20837;&#27169;&#22411;&#26080;&#20851;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20026;&#35299;&#37322;&#28155;&#21152;&#26102;&#38388;&#20449;&#24687;&#65292;&#20351;&#19968;&#20123;&#29616;&#26377;&#27169;&#22411;&#26080;&#27861;&#24212;&#29992;&#30340;&#23616;&#37096;&#35299;&#37322;&#25216;&#26415;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#30340;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#20197;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#21151;&#33021;&#65292;&#20294;&#36890;&#24120;&#24456;&#38590;&#35299;&#37322;&#12290;&#32570;&#20047;&#36879;&#26126;&#24230;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#35299;&#37322;&#25216;&#26415;&#23545;&#20110;&#25552;&#39640;&#36879;&#26126;&#24230;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#26080;&#20851;&#36890;&#29992;&#35299;&#37322;&#25216;&#26415;&#27809;&#26377;&#32771;&#34385;&#36755;&#20837;&#25968;&#25454;&#28857;&#30340;&#21487;&#21464;&#38271;&#24230;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ReX&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#20026;&#22788;&#29702;&#21487;&#21464;&#38271;&#24230;&#36755;&#20837;&#30340;&#27169;&#22411;&#36866;&#24212;&#21508;&#31181;&#35299;&#37322;&#25216;&#26415;&#65292;&#25193;&#23637;&#35299;&#37322;&#35206;&#30422;&#21040;&#19981;&#21516;&#38271;&#24230;&#30340;&#25968;&#25454;&#28857;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#25913;&#21464;&#29616;&#26377;&#25216;&#26415;&#26680;&#24515;&#31639;&#27861;&#30340;&#24773;&#20917;&#19979;&#65292;&#20026;&#29616;&#26377;&#25216;&#26415;&#29983;&#25104;&#30340;&#35299;&#37322;&#28155;&#21152;&#26102;&#38388;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#27969;&#34892;&#30340;&#35299;&#37322;&#25216;&#26415;LIME&#21644;Anchors&#19978;&#23454;&#29616;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35780;&#20272;ReX&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#19977;&#20010;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#22320;&#25552;&#39640;&#20102;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network models that can handle inputs of variable lengths are powerful, but often hard to interpret. The lack of transparency hinders their adoption in many domains. Explanation techniques are essential for improving transparency. However, existing model-agnostic general explanation techniques do not consider the variable lengths of input data points, which limits their effectiveness. To address this limitation, we propose ReX, a general framework for adapting various explanation techniques to models that process variable-length inputs, expanding explanation coverage to data points of different lengths. Our approach adds temporal information to the explanations generated by existing techniques without altering their core algorithms. We instantiate our approach on two popular explanation techniques: LIME and Anchors. To evaluate the effectiveness of ReX, we apply our approach to three models in two different tasks. Our evaluation results demonstrate that our approach significantl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#22871;&#24320;&#25918;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#21327;&#35758;&#65292;&#26088;&#22312;&#35299;&#20915;&#25918;&#23556;&#32452;&#23398;&#22312;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;BraTS 2020&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#25552;&#21462;&#23545;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2207.14776</link><description>&lt;p&gt;
&#24320;&#25918;&#25918;&#23556;&#32452;&#23398;&#65306;&#19968;&#31995;&#21015;&#26631;&#20934;&#21270;&#25968;&#25454;&#38598;&#21644;&#21487;&#37325;&#22797;&#25918;&#23556;&#32452;&#23398;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#25216;&#26415;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Open-radiomics: A Collection of Standardized Datasets and a Technical Protocol for Reproducible Radiomics Machine Learning Pipelines. (arXiv:2207.14776v2 [q-bio.QM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.14776
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#22871;&#24320;&#25918;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#21644;&#25216;&#26415;&#21327;&#35758;&#65292;&#26088;&#22312;&#35299;&#20915;&#25918;&#23556;&#32452;&#23398;&#22312;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#26041;&#38754;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#22312;BraTS 2020&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#30740;&#31350;&#20102;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#25552;&#21462;&#23545;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#20316;&#20026;&#21307;&#23398;&#24433;&#20687;&#20013;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#19968;&#20010;&#37325;&#35201;&#20998;&#25903;&#65292;&#25918;&#23556;&#32452;&#23398;&#38754;&#20020;&#30528;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#21363;&#21487;&#37325;&#22797;&#24615;&#21644;&#21487;&#35775;&#38382;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24320;&#25918;&#25918;&#23556;&#32452;&#23398;&#65292;&#19968;&#22871;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#20197;&#21450;&#22522;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#21327;&#35758;&#30340;&#32508;&#21512;&#25918;&#23556;&#32452;&#23398;&#27969;&#31243;&#65292;&#20197;&#30740;&#31350;&#25918;&#23556;&#32452;&#23398;&#29305;&#24449;&#25552;&#21462;&#23545;&#32467;&#26524;&#21487;&#37325;&#22797;&#24615;&#30340;&#24433;&#21709;&#12290;&#26448;&#26009;&#21644;&#26041;&#27861;&#65306;&#23454;&#39564;&#20351;&#29992;BraTS 2020&#24320;&#28304;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;MRI&#65289;&#25968;&#25454;&#38598;&#36827;&#34892;&#65292;&#21253;&#25324;369&#21517;&#24739;&#26377;&#33041;&#32959;&#30244;&#30340;&#25104;&#24180;&#24739;&#32773;&#65288;76&#20363;&#20302;&#32423;&#21035;&#33014;&#36136;&#30244;&#65288;LGG&#65289;&#21644;293&#20363;&#39640;&#32423;&#21035;&#33014;&#36136;&#30244;&#65288;HGG&#65289;&#65289;&#12290;&#20351;&#29992;PyRadiomics&#24211;&#36827;&#34892;LGG&#19982;HGG&#20998;&#31867;&#65292;&#24418;&#25104;&#20102;288&#20010;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#65307;&#20854;&#20013;&#21253;&#25324;4&#20010;MRI&#24207;&#21015;&#12289;3&#20010;binWidths&#12289;6&#31181;&#22270;&#20687;&#24402;&#19968;&#21270;&#26041;&#27861;&#21644;4&#20010;&#32959;&#30244;&#27425;&#21306;&#22495;&#30340;&#32452;&#21512;&#12290;&#20351;&#29992;&#38543;&#26426;&#26862;&#26519;&#20998;&#31867;&#22120;&#65292;&#24182;&#20026;&#27599;&#20010;&#25918;&#23556;&#32452;&#23398;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;-&#39564;&#35777;-&#27979;&#35797;&#65288;60%/20%/20%&#65289;&#23454;&#39564;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#25968;&#25454;&#21010;&#20998;&#21644;m
&lt;/p&gt;
&lt;p&gt;
Purpose: As an important branch of machine learning pipelines in medical imaging, radiomics faces two major challenges namely reproducibility and accessibility. In this work, we introduce open-radiomics, a set of radiomics datasets along with a comprehensive radiomics pipeline based on our proposed technical protocol to investigate the effects of radiomics feature extraction on the reproducibility of the results.  Materials and Methods: Experiments are conducted on BraTS 2020 open-source Magnetic Resonance Imaging (MRI) dataset that includes 369 adult patients with brain tumors (76 low-grade glioma (LGG), and 293 high-grade glioma (HGG)). Using PyRadiomics library for LGG vs. HGG classification, 288 radiomics datasets are formed; the combinations of 4 MRI sequences, 3 binWidths, 6 image normalization methods, and 4 tumor subregions.  Random Forest classifiers were used, and for each radiomics dataset the training-validation-test (60%/20%/20%) experiment with different data splits and m
&lt;/p&gt;</description></item></channel></rss>