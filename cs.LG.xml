<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>FraGNNet&#26159;&#19968;&#31181;&#29992;&#20110;&#21270;&#21512;&#29289;&#21040;&#36136;&#35889;&#39044;&#27979;&#30340;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#35889;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;</title><link>https://arxiv.org/abs/2404.02360</link><description>&lt;p&gt;
FraGNNet&#65306;&#19968;&#31181;&#29992;&#20110;&#36136;&#35889;&#39044;&#27979;&#30340;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02360
&lt;/p&gt;
&lt;p&gt;
FraGNNet&#26159;&#19968;&#31181;&#29992;&#20110;&#21270;&#21512;&#29289;&#21040;&#36136;&#35889;&#39044;&#27979;&#30340;&#28145;&#24230;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#35889;&#65292;&#22312;&#24615;&#33021;&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#22797;&#26434;&#28151;&#21512;&#29289;&#20013;&#21270;&#21512;&#29289;&#30340;&#26041;&#27861;&#20013;&#65292;&#36136;&#35889;&#30340;&#35782;&#21035;&#26159;&#19968;&#20010;&#20851;&#38190;&#27493;&#39588;&#12290;&#20256;&#32479;&#30340;&#36136;&#35889;&#21040;&#21270;&#21512;&#29289;&#65288;MS2C&#65289;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#28041;&#21450;&#23558;&#26410;&#30693;&#30340;&#36136;&#35889;&#19982;&#24050;&#30693;&#30340;&#36136;&#35889;-&#20998;&#23376;&#24211;&#36827;&#34892;&#21305;&#37197;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#21463;&#38480;&#20110;&#24211;&#35206;&#30422;&#19981;&#23436;&#25972;&#12290;&#21270;&#21512;&#29289;&#21040;&#36136;&#35889;&#65288;C2MS&#65289;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23558;&#30495;&#23454;&#24211;&#19982;&#39044;&#27979;&#35889;&#36827;&#34892;&#22686;&#24378;&#26469;&#25552;&#39640;&#26816;&#32034;&#29575;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#35768;&#22810;&#29616;&#26377;&#30340;C2MS&#27169;&#22411;&#22312;&#39044;&#27979;&#20998;&#36776;&#29575;&#12289;&#21487;&#25193;&#23637;&#24615;&#25110;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;C2MS&#39044;&#27979;&#27010;&#29575;&#26041;&#27861;&#8212;&#8212;FraGNNet&#65292;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#39044;&#27979;&#39640;&#20998;&#36776;&#29575;&#35889;&#12290;FraGNNet&#20351;&#29992;&#32467;&#26500;&#21270;&#30340;&#28508;&#22312;&#31354;&#38388;&#26469;&#25581;&#31034;&#23450;&#20041;&#35889;&#30340;&#22522;&#26412;&#36807;&#31243;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#39044;&#27979;&#35823;&#24046;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;C2MS&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02360v1 Announce Type: new  Abstract: The process of identifying a compound from its mass spectrum is a critical step in the analysis of complex mixtures. Typical solutions for the mass spectrum to compound (MS2C) problem involve matching the unknown spectrum against a library of known spectrum-molecule pairs, an approach that is limited by incomplete library coverage. Compound to mass spectrum (C2MS) models can improve retrieval rates by augmenting real libraries with predicted spectra. Unfortunately, many existing C2MS models suffer from problems with prediction resolution, scalability, or interpretability. We develop a new probabilistic method for C2MS prediction, FraGNNet, that can efficiently and accurately predict high-resolution spectra. FraGNNet uses a structured latent space to provide insight into the underlying processes that define the spectrum. Our model achieves state-of-the-art performance in terms of prediction error, and surpasses existing C2MS models as a t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#26694;&#26550;: &#26530;&#36724;&#12289;&#26816;&#27979;&#25928;&#29575;&#21644;&#26368;&#20248;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#20197;&#26469;&#65292;&#23558;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#32479;&#35745;&#20449;&#21495;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20063;&#34987;&#31216;&#20026;&#27700;&#21360;&#65292;&#24050;&#34987;&#29992;&#20316;&#20174;&#20854;&#20154;&#31867;&#25776;&#20889;&#23545;&#24212;&#29289;&#19978;&#21487;&#35777;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#24182;&#35774;&#35745;&#24378;&#22823;&#30340;&#26816;&#27979;&#35268;&#21017;&#12290;&#21463;&#27700;&#21360;&#26816;&#27979;&#30340;&#20551;&#35774;&#26816;&#39564;&#20844;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36873;&#25321;&#25991;&#26412;&#30340;&#26530;&#36724;&#32479;&#35745;&#37327;&#21644;&#30001;LLM&#25552;&#20379;&#32473;&#39564;&#35777;&#22120;&#30340;&#31192;&#23494;&#23494;&#38053;&#65292;&#20197;&#23454;&#29616;&#25511;&#21046;&#35823;&#25253;&#29575;&#65288;&#23558;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#65289;&#12290; &#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#33719;&#21462;&#28176;&#36817;&#38169;&#35823;&#36127;&#29575;&#65288;&#23558;LLM&#29983;&#25104;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;&#20154;&#31867;&#25776;&#20889;&#30340;&#38169;&#35823;&#65289;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#26469;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#24341;&#20837;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;Wasserstein&#36317;&#31163;&#30340;&#26399;&#26395;&#65292;&#25512;&#23548;&#20102;&#20854;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#36895;&#24230;&#22330;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.18705</link><description>&lt;p&gt;
&#20855;&#26377;&#36125;&#21494;&#26031;OT&#27969;&#21305;&#37197;&#24212;&#29992;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;
&lt;/p&gt;
&lt;p&gt;
Conditional Wasserstein Distances with Applications in Bayesian OT Flow Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#24341;&#20837;&#30340;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;Wasserstein&#36317;&#31163;&#30340;&#26399;&#26395;&#65292;&#25512;&#23548;&#20102;&#20854;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#36817;&#20284;&#36895;&#24230;&#22330;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36870;&#38382;&#39064;&#20013;&#65292;&#35768;&#22810;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#36890;&#36807;&#26368;&#23567;&#21270;&#32852;&#21512;&#24230;&#37327;&#19982;&#20854;&#23398;&#20064;&#36924;&#36817;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#36817;&#20284;&#21518;&#39564;&#27979;&#24230;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#22312;Kullback--Leibler&#20998;&#27495;&#30340;&#24773;&#20917;&#19979;&#20063;&#25511;&#21046;&#21518;&#39564;&#27979;&#24230;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#20294;&#19968;&#33324;&#26469;&#35828;&#23545;&#20110;Wasserstein&#36317;&#31163;&#24182;&#19981;&#25104;&#31435;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#32452;&#21463;&#38480;&#32806;&#21512;&#24341;&#20837;&#20102;&#19968;&#31181;&#26465;&#20214;Wasserstein&#36317;&#31163;&#65292;&#23427;&#31561;&#20110;&#21518;&#39564;Wasserstein&#36317;&#31163;&#30340;&#26399;&#26395;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#26465;&#20214;Wasserstein-1&#27969;&#30340;&#23545;&#20598;&#24418;&#24335;&#20197;&#19968;&#31181;&#38750;&#24120;&#33258;&#28982;&#30340;&#26041;&#24335;&#31867;&#20284;&#20110;&#26465;&#20214;Wasserstein GAN&#25991;&#29486;&#20013;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#26465;&#20214;Wasserstein&#36317;&#31163;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#34920;&#24449;&#30456;&#24212;&#30340;&#27979;&#22320;&#32447;&#21644;&#36895;&#24230;&#22330;&#20197;&#21450;&#27969;ODE&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#25918;&#23485;&#26465;&#20214;Wasserstein&#36317;&#31163;&#26469;&#36817;&#20284;&#36895;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18705v1 Announce Type: new  Abstract: In inverse problems, many conditional generative models approximate the posterior measure by minimizing a distance between the joint measure and its learned approximation. While this approach also controls the distance between the posterior measures in the case of the Kullback--Leibler divergence, this is in general not hold true for the Wasserstein distance. In this paper, we introduce a conditional Wasserstein distance via a set of restricted couplings that equals the expected Wasserstein distance of the posteriors. Interestingly, the dual formulation of the conditional Wasserstein-1 flow resembles losses in the conditional Wasserstein GAN literature in a quite natural way. We derive theoretical properties of the conditional Wasserstein distance, characterize the corresponding geodesics and velocity fields as well as the flow ODEs. Subsequently, we propose to approximate the velocity fields by relaxing the conditional Wasserstein dista
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26469;&#26368;&#23567;&#21270;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#22312;&#32473;&#23450;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#30340;&#27010;&#24565;&#65292;&#24182;&#25214;&#21040;&#20102;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#12290;&#26412;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#20844;&#24179;&#24615;&#32422;&#26463;&#21644;&#24120;&#35265;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2402.02817</link><description>&lt;p&gt;
&#22522;&#20110;&#20808;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#30340;&#32447;&#24615;&#24046;&#24322;&#32422;&#26463;&#19979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Bayes-Optimal Fair Classification with Linear Disparity Constraints via Pre-, In-, and Post-processing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#20808;&#22788;&#29702;&#12289;&#20013;&#22788;&#29702;&#21644;&#21518;&#22788;&#29702;&#26469;&#26368;&#23567;&#21270;&#20998;&#31867;&#38169;&#35823;&#65292;&#24182;&#22312;&#32473;&#23450;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#32447;&#24615;&#21644;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#30340;&#27010;&#24565;&#65292;&#24182;&#25214;&#21040;&#20102;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#12290;&#26412;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22810;&#20010;&#20844;&#24179;&#24615;&#32422;&#26463;&#21644;&#24120;&#35265;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#20844;&#24179;&#30340;&#24433;&#21709;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20844;&#24179;&#20998;&#31867;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#32473;&#23450;&#32676;&#20307;&#20844;&#24179;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#19979;&#26368;&#23567;&#21270;&#20998;&#31867;&#38169;&#35823;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#30340;&#27010;&#24565;&#65292;&#23427;&#20204;&#26159;&#27010;&#29575;&#20998;&#31867;&#22120;&#30340;&#32447;&#24615;&#20989;&#25968;&#65307;&#20197;&#21450;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#65292;&#23427;&#20204;&#22312;&#32676;&#20307;&#22238;&#24402;&#20989;&#25968;&#26041;&#38754;&#20063;&#26159;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20960;&#31181;&#24120;&#35265;&#30340;&#24046;&#24322;&#24230;&#37327;&#65288;&#22914;&#20154;&#21475;&#24179;&#31561;&#12289;&#26426;&#20250;&#24179;&#31561;&#21644;&#39044;&#27979;&#24179;&#31561;&#65289;&#37117;&#26159;&#21452;&#32447;&#24615;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25581;&#31034;&#19982;Neyman-Pearson&#24341;&#29702;&#30340;&#36830;&#25509;&#65292;&#25214;&#21040;&#20102;&#22312;&#21333;&#19968;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#19979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#30340;&#24418;&#24335;&#12290;&#23545;&#20110;&#21452;&#32447;&#24615;&#24046;&#24322;&#24230;&#37327;&#65292;&#36125;&#21494;&#26031;&#26368;&#20248;&#20844;&#24179;&#20998;&#31867;&#22120;&#21464;&#25104;&#20102;&#32676;&#20307;&#38408;&#20540;&#35268;&#21017;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#21487;&#20197;&#22788;&#29702;&#22810;&#20010;&#20844;&#24179;&#24615;&#32422;&#26463;&#65288;&#22914;&#24179;&#31561;&#30340;&#20960;&#29575;&#65289;&#21644;&#21463;&#20445;&#25252;&#23646;&#24615;&#24120;&#35265;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms may have disparate impacts on protected groups. To address this, we develop methods for Bayes-optimal fair classification, aiming to minimize classification error subject to given group fairness constraints. We introduce the notion of \emph{linear disparity measures}, which are linear functions of a probabilistic classifier; and \emph{bilinear disparity measures}, which are also linear in the group-wise regression functions. We show that several popular disparity measures -- the deviations from demographic parity, equality of opportunity, and predictive equality -- are bilinear.   We find the form of Bayes-optimal fair classifiers under a single linear disparity measure, by uncovering a connection with the Neyman-Pearson lemma. For bilinear disparity measures, Bayes-optimal fair classifiers become group-wise thresholding rules. Our approach can also handle multiple fairness constraints (such as equalized odds), and the common scenario when the protected attr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20102;&#31616;&#21270;Gr\"obner&#22522;&#30340;&#22522;&#25968;&#21644;&#26368;&#22823;&#24635;&#24230;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#32479;&#35745;&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#29468;&#27979;&#25110;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.05364</link><description>&lt;p&gt;
&#39044;&#27979;&#31616;&#21270;Gr\"obner&#22522;&#25968;&#21644;&#26368;&#22823;&#24230;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Predicting the cardinality and maximum degree of a reduced Gr\"obner basis. (arXiv:2302.05364v2 [math.AC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05364
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20102;&#31616;&#21270;Gr\"obner&#22522;&#30340;&#22522;&#25968;&#21644;&#26368;&#22823;&#24635;&#24230;&#25968;&#65292;&#32467;&#26524;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#32479;&#35745;&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#29468;&#27979;&#25110;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26500;&#24314;&#20102;&#31070;&#32463;&#32593;&#32476;&#22238;&#24402;&#27169;&#22411;&#65292;&#29992;&#20197;&#39044;&#27979;&#20108;&#39033;&#29702;&#24819;&#30340;Gr\"obner&#22522;&#22797;&#26434;&#24230;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#36825;&#39033;&#24037;&#20316;&#35828;&#26126;&#20102;&#20026;&#20160;&#20040;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20174;Gr\"obner&#35745;&#31639;&#20013;&#36827;&#34892;&#39044;&#27979;&#24182;&#19981;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#36807;&#31243;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#27010;&#29575;&#27169;&#22411;&#26469;&#29983;&#25104;&#21644;&#25552;&#20379;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#25429;&#25417;&#21040;Gr\"obner&#22797;&#26434;&#24230;&#30340;&#36275;&#22815;&#21464;&#24322;&#24615;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#26469;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#24182;&#39044;&#27979;&#31616;&#21270;Gr\"obner&#22522;&#30340;&#22522;&#25968;&#21644;&#20854;&#20803;&#32032;&#30340;&#26368;&#22823;&#24635;&#24230;&#25968;&#12290;&#34429;&#28982;&#22522;&#25968;&#39044;&#27979;&#38382;&#39064;&#19981;&#21516;&#20110;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#20294;&#25105;&#20204;&#30340;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#32479;&#35745;&#65292;&#22914;$r^2 = 0.401$&#65292;&#30456;&#27604;&#20110;&#26420;&#32032;&#29468;&#27979;&#25110;&#22810;&#20803;&#22238;&#24402;&#27169;&#22411;&#30340;$r^2 = 0.180$&#12290;
&lt;/p&gt;
&lt;p&gt;
We construct neural network regression models to predict key metrics of complexity for Gr\"obner bases of binomial ideals. This work illustrates why predictions with neural networks from Gr\"obner computations are not a straightforward process. Using two probabilistic models for random binomial ideals, we generate and make available a large data set that is able to capture sufficient variability in Gr\"obner complexity. We use this data to train neural networks and predict the cardinality of a reduced Gr\"obner basis and the maximum total degree of its elements. While the cardinality prediction problem is unlike classical problems tackled by machine learning, our simulations show that neural networks, providing performance statistics such as $r^2 = 0.401$, outperform naive guess or multiple regression models with $r^2 = 0.180$.
&lt;/p&gt;</description></item></channel></rss>