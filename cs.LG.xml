<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2404.01334</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#22686;&#24378;NER&#25968;&#25454;&#38598;&#65306;&#36808;&#21521;&#33258;&#21160;&#21270;&#21644;&#31934;&#32454;&#21270;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
Augmenting NER Datasets with LLMs: Towards Automated and Refined Annotation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26088;&#22312;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#20197;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#34987;&#35748;&#20026;&#26159;&#19968;&#39033;&#20851;&#38190;&#25216;&#26415;&#65292;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#24212;&#29992;&#12290;&#20256;&#32479;&#30340;&#29992;&#20110;&#20026;NER&#27169;&#22411;&#26631;&#27880;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#38754;&#20020;&#30528;&#39640;&#25104;&#26412;&#21644;&#25968;&#25454;&#38598;&#36136;&#37327;&#21464;&#21270;&#30340;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512;&#26631;&#27880;&#26041;&#27861;&#65292;&#23558;&#20154;&#21147;&#24037;&#20316;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26088;&#22312;&#25913;&#21892;&#25163;&#21160;&#27880;&#37322;&#20013;&#22266;&#26377;&#30340;&#22122;&#38899;&#65292;&#22914;&#36951;&#28431;&#65292;&#20174;&#32780;&#25552;&#39640;NER&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#20197;&#19968;&#31181;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#37319;&#29992;&#26631;&#31614;&#28151;&#21512;&#31574;&#30053;&#65292;&#23427;&#35299;&#20915;&#20102;LLM-based&#27880;&#37322;&#20013;&#36935;&#21040;&#30340;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#36825;&#31181;&#26041;&#27861;&#19968;&#30452;&#34920;&#29616;&#20986;&#27604;&#20256;&#32479;&#27880;&#37322;&#26041;&#27861;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#65292;&#21363;&#20351;&#22312;co
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01334v1 Announce Type: new  Abstract: In the field of Natural Language Processing (NLP), Named Entity Recognition (NER) is recognized as a critical technology, employed across a wide array of applications. Traditional methodologies for annotating datasets for NER models are challenged by high costs and variations in dataset quality. This research introduces a novel hybrid annotation approach that synergizes human effort with the capabilities of Large Language Models (LLMs). This approach not only aims to ameliorate the noise inherent in manual annotations, such as omissions, thereby enhancing the performance of NER models, but also achieves this in a cost-effective manner. Additionally, by employing a label mixing strategy, it addresses the issue of class imbalance encountered in LLM-based annotations. Through an analysis across multiple datasets, this method has been consistently shown to provide superior performance compared to traditional annotation methods, even under co
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20280</link><description>&lt;p&gt;
&#24102;&#26377;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#31232;&#30095;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Sparse multimodal fusion with modal channel attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20280
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#37327;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#20316;&#20026;&#27169;&#24577;&#31232;&#30095;&#24230;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#20013;&#24341;&#20837;&#20102;&#27169;&#24577;&#19981;&#23436;&#20840;&#36890;&#36947;&#65292;&#31216;&#20026;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#12290;&#20351;&#29992;&#20102;&#21253;&#21547;4&#31181;&#27169;&#24577;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;CMU-MOSEI&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;TCGA&#29992;&#20110;&#22810;&#32452;&#23398;&#12290;&#27169;&#22411;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#26679;&#26412;&#20013;&#21482;&#29992;&#20102;&#22235;&#31181;&#27169;&#24577;&#20013;&#30340;&#20004;&#31181;&#23601;&#23398;&#20064;&#20986;&#32479;&#19968;&#19988;&#23545;&#40784;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#21457;&#29616;&#65292;&#21363;&#20351;&#27809;&#26377;&#27169;&#24577;&#31232;&#30095;&#65292;&#25152;&#25552;&#20986;&#30340;MCA&#26426;&#21046;&#20063;&#33021;&#25552;&#39640;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#65292;&#21484;&#22238;&#25351;&#26631;&#65292;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20280v1 Announce Type: cross  Abstract: The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#37329;&#34701;&#26426;&#22120;&#20154;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#37329;&#34701;&#26426;&#22120;&#20154;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#26816;&#27979;&#24182;&#21019;&#24314;&#20102;&#22320;&#38754;&#23454;&#20917;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.19530</link><description>&lt;p&gt;
&#22312;&#20197;&#22826;&#22346;&#21306;&#22359;&#38142;&#19978;&#26816;&#27979;&#37329;&#34701;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Detecting Financial Bots on the Ethereum Blockchain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#37329;&#34701;&#26426;&#22120;&#20154;&#22312;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#37329;&#34701;&#26426;&#22120;&#20154;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#26426;&#22120;&#20154;&#36827;&#34892;&#20102;&#26816;&#27979;&#24182;&#21019;&#24314;&#20102;&#22320;&#38754;&#23454;&#20917;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38598;&#25104;&#26426;&#22120;&#20154;&#22312;&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;&#65288;DLTs&#65289;&#20013;&#20419;&#36827;&#20102;&#25928;&#29575;&#21644;&#33258;&#21160;&#21270;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20351;&#29992;&#20063;&#19982;&#25504;&#22842;&#24615;&#20132;&#26131;&#21644;&#24066;&#22330;&#25805;&#32437;&#30456;&#20851;&#65292;&#24182;&#21487;&#33021;&#23545;&#31995;&#32479;&#23436;&#25972;&#24615;&#26500;&#25104;&#23041;&#32961;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;DLTs&#20013;&#26426;&#22120;&#20154;&#30340;&#37096;&#32626;&#31243;&#24230;&#33267;&#20851;&#37325;&#35201;&#65307;&#23613;&#31649;&#22914;&#27492;&#65292;&#30446;&#21069;&#30340;&#26816;&#27979;&#31995;&#32479;&#20027;&#35201;&#22522;&#20110;&#35268;&#21017;&#65292;&#24182;&#19988;&#32570;&#20047;&#28789;&#27963;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26816;&#27979;&#20197;&#22826;&#22346;&#24179;&#21488;&#19978;&#37329;&#34701;&#26426;&#22120;&#20154;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#31995;&#32479;&#21270;&#29616;&#26377;&#30340;&#31185;&#23398;&#25991;&#29486;&#24182;&#25910;&#38598;&#36726;&#20107;&#35777;&#25454;&#65292;&#20197;&#24314;&#31435;&#37329;&#34701;&#26426;&#22120;&#20154;&#30340;&#20998;&#31867;&#27861;&#65292;&#21253;&#25324;7&#20010;&#31867;&#21035;&#21644;24&#20010;&#23376;&#31867;&#21035;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#21019;&#24314;&#19968;&#20010;&#21253;&#21547;133&#20010;&#20154;&#31867;&#21644;137&#20010;&#26426;&#22120;&#20154;&#22320;&#22336;&#30340;&#22320;&#38754;&#23454;&#20917;&#25968;&#25454;&#38598;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#20351;&#29992;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#26469;&#26816;&#27979;&#37096;&#32626;&#22312;&#20197;&#22826;&#22346;&#19978;&#30340;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19530v1 Announce Type: cross  Abstract: The integration of bots in Distributed Ledger Technologies (DLTs) fosters efficiency and automation. However, their use is also associated with predatory trading and market manipulation, and can pose threats to system integrity. It is therefore essential to understand the extent of bot deployment in DLTs; despite this, current detection systems are predominantly rule-based and lack flexibility. In this study, we present a novel approach that utilizes machine learning for the detection of financial bots on the Ethereum platform. First, we systematize existing scientific literature and collect anecdotal evidence to establish a taxonomy for financial bots, comprising 7 categories and 24 subcategories. Next, we create a ground-truth dataset consisting of 133 human and 137 bot addresses. Third, we employ both unsupervised and supervised machine learning algorithms to detect bots deployed on Ethereum. The highest-performing clustering algori
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15474</link><description>&lt;p&gt;
EC-IoU: &#36890;&#36807;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#35843;&#25972;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#65288;EC-IoU&#65289;&#24230;&#37327;&#26469;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26426;&#21046;&#26469;&#20248;&#21270;&#24191;&#27867;&#20351;&#29992;&#30340;IoU&#24230;&#37327;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#33258;&#25105;&#20195;&#29702;&#20154;&#30340;&#35270;&#35282;&#35206;&#30422;&#26356;&#36817;&#30340;&#22320;&#38754;&#30495;&#23454;&#23545;&#35937;&#28857;&#30340;&#39044;&#27979;&#20998;&#37197;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;EC-IoU&#24230;&#37327;&#21487;&#20197;&#29992;&#20110;&#20856;&#22411;&#30340;&#35780;&#20272;&#36807;&#31243;&#65292;&#36873;&#25321;&#26377;&#26356;&#39640;&#23433;&#20840;&#24615;&#34920;&#29616;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#23427;&#36824;&#21487;&#20197;&#38598;&#25104;&#21040;&#24120;&#35265;&#25439;&#22833;&#20989;&#25968;&#20013;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#23613;&#31649;&#38754;&#21521;&#23433;&#20840;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;EC-IoU&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22343;&#20540;&#24179;&#22343;&#31934;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20248;&#20110;&#20351;&#29992;IoU&#35757;&#32451;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15474v1 Announce Type: cross  Abstract: This paper presents safety-oriented object detection via a novel Ego-Centric Intersection-over-Union (EC-IoU) measure, addressing practical concerns when applying state-of-the-art learning-based perception models in safety-critical domains such as autonomous driving. Concretely, we propose a weighting mechanism to refine the widely used IoU measure, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with higher safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20498;&#25968;&#31532;&#20108;&#23618;&#20316;&#20026;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#31867;&#23396;&#31435;&#26862;&#26519;&#65288;MCIF&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#27599;&#20010;&#31867;&#21035;&#30340;&#23396;&#31435;&#26862;&#26519;&#65292;&#20197;&#25512;&#23548;&#24322;&#24120;&#20540;</title><link>https://arxiv.org/abs/2403.14742</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#22810;&#31867;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#22825;&#25991;&#26242;&#21464;&#26143;
&lt;/p&gt;
&lt;p&gt;
A Classifier-Based Approach to Multi-Class Anomaly Detection for Astronomical Transients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14742
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20498;&#25968;&#31532;&#20108;&#23618;&#20316;&#20026;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#31867;&#23396;&#31435;&#26862;&#26519;&#65288;MCIF&#65289;&#30340;&#26032;&#26041;&#27861;&#26469;&#35757;&#32451;&#27599;&#20010;&#31867;&#21035;&#30340;&#23396;&#31435;&#26862;&#26519;&#65292;&#20197;&#25512;&#23548;&#24322;&#24120;&#20540;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#33258;&#21160;&#24322;&#24120;&#26816;&#27979;&#23545;&#20110;&#22312;&#22823;&#35268;&#27169;&#22825;&#25991;&#35843;&#26597;&#26102;&#20195;&#35782;&#21035;&#31232;&#26377;&#30340;&#30701;&#26242;&#29616;&#35937;&#33267;&#20851;&#37325;&#35201;&#12290;&#30446;&#21069;&#65292;&#22823;&#37096;&#20998;&#22825;&#25991;&#26242;&#21464;&#26816;&#27979;&#31639;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#20174;&#20809;&#21464;&#26354;&#32447;&#20013;&#25552;&#21462;&#30340;&#25163;&#24037;&#29305;&#24449;&#65292;&#35201;&#20040;&#20381;&#36182;&#20110;&#36890;&#36807;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#29983;&#25104;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#19982;&#26631;&#20934;&#26426;&#22120;&#23398;&#20064;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#32467;&#21512;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21478;&#31867;&#30340;&#26816;&#27979;&#24322;&#24120;&#30340;&#26041;&#27861;&#65306;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#30340;&#20498;&#25968;&#31532;&#20108;&#23618;&#20316;&#20026;&#24322;&#24120;&#26816;&#27979;&#30340;&#28508;&#22312;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#31867;&#23396;&#31435;&#26862;&#26519;&#65288;MCIF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#31867;&#21035;&#35757;&#32451;&#21333;&#29420;&#30340;&#23396;&#31435;&#26862;&#26519;&#26469;&#25512;&#23548;&#24322;&#24120;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14742v1 Announce Type: cross  Abstract: Automating real-time anomaly detection is essential for identifying rare transients in the era of large-scale astronomical surveys. Modern survey telescopes are generating tens of thousands of alerts per night, and future telescopes, such as the Vera C. Rubin Observatory, are projected to increase this number dramatically. Currently, most anomaly detection algorithms for astronomical transients rely either on hand-crafted features extracted from light curves or on features generated through unsupervised representation learning, which are then coupled with standard machine learning anomaly detection algorithms. In this work, we introduce an alternative approach to detecting anomalies: using the penultimate layer of a neural network classifier as the latent space for anomaly detection. We then propose a novel method, named Multi-Class Isolation Forests (MCIF), which trains separate isolation forests for each class to derive an anomaly sc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.08618</link><description>&lt;p&gt;
Verifix: &#21518;&#35757;&#32451;&#26657;&#27491;&#20197;&#25913;&#21892;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#26679;&#26412;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#38169;&#35823;&#65292;&#21363;&#35757;&#32451;&#26679;&#26412;&#20855;&#26377;&#19981;&#27491;&#30830;&#30340;&#26631;&#31614;&#65292;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#38169;&#35823;&#24448;&#24448;&#26469;&#33258;&#38750;&#19987;&#23478;&#26631;&#27880;&#25110;&#25932;&#23545;&#25915;&#20987;&#12290;&#33719;&#21462;&#22823;&#22411;&#12289;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#65292;&#24403;&#26377;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#23601;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Verifix&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#19968;&#20010;&#23567;&#30340;&#12289;&#32463;&#36807;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21333;&#20010;&#26356;&#26032;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#12290;Verifix&#20351;&#29992;SVD&#20272;&#35745;&#24178;&#20928;&#28608;&#27963;&#31354;&#38388;&#65292;&#28982;&#21518;&#23558;&#27169;&#22411;&#30340;&#26435;&#37325;&#25237;&#24433;&#21040;&#36825;&#20010;&#31354;&#38388;&#19978;&#65292;&#20197;&#25233;&#21046;&#23545;&#24212;&#20110;&#25439;&#22351;&#25968;&#25454;&#30340;&#28608;&#27963;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Verifix&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08618v1 Announce Type: cross  Abstract: Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;</title><link>https://arxiv.org/abs/2403.02774</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#23610;&#24230;&#21644;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#22330;&#38477;&#23610;&#24230;&#19982;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02774
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(ESM)&#27169;&#25311;&#23545;&#20110;&#35780;&#20272;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#23545;&#29983;&#24577;&#21644;&#31038;&#20250;&#32463;&#27982;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;ESM&#27169;&#25311;&#30340;&#38477;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#27599;&#20010;ESM&#37117;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#27668;&#20505;&#39044;&#27979;&#25928;&#26524;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#19968;&#33268;&#24615;&#27169;&#22411;(CM)&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#39640;&#25928;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;ESM&#27169;&#25311;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#26041;&#27861;&#20197;&#21482;&#21463;&#35266;&#27979;&#21442;&#32771;&#25968;&#25454;&#38480;&#21046;&#30340;&#20998;&#36776;&#29575;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CM&#22312;&#32500;&#25345;&#39640;&#21487;&#25511;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02774v1 Announce Type: cross  Abstract: Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30830;&#23450;&#20102;&#22270;&#20013;&#33410;&#28857;&#30340;&#28789;&#27963;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#31639;&#27861;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#24182;&#20174;&#20056;&#23458;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#29992;&#20110;&#21021;&#22987;&#21270;GNNs&#30340;&#36793;&#26435;&#37325;&#12290;</title><link>https://arxiv.org/abs/2403.00276</link><description>&lt;p&gt;
&#20855;&#26377;&#28789;&#27963;&#33410;&#28857;&#30340;&#22270;&#26500;&#24314;&#29992;&#20110;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Graph Construction with Flexible Nodes for Traffic Demand Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00276
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#30340;&#22270;&#26500;&#24314;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30830;&#23450;&#20102;&#22270;&#20013;&#33410;&#28857;&#30340;&#28789;&#27963;&#23450;&#20301;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#31639;&#27861;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#24182;&#20174;&#20056;&#23458;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#20449;&#24687;&#29992;&#20110;&#21021;&#22987;&#21270;GNNs&#30340;&#36793;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#20013;&#65292;&#20132;&#36890;&#27169;&#24335;&#21487;&#20998;&#20026;&#22522;&#20110;&#31449;&#28857;&#30340;&#27169;&#24335;&#21644;&#33258;&#30001;&#28418;&#28014;&#20132;&#36890;&#27169;&#24335;&#12290;&#29616;&#26377;&#30340;&#20132;&#36890;&#22270;&#26500;&#24314;&#30740;&#31350;&#20027;&#35201;&#20381;&#36182;&#20110;&#22320;&#22270;&#21305;&#37197;&#65292;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#26500;&#24314;&#22270;&#12290;&#28982;&#32780;&#65292;&#22312;&#33258;&#30001;&#28418;&#28014;&#20132;&#36890;&#38656;&#27714;&#39044;&#27979;&#20013;&#65292;&#25968;&#25454;&#20998;&#24067;&#30340;&#22797;&#26434;&#24615;&#21644;&#19981;&#22343;&#21248;&#24615;&#20351;&#24471;&#36947;&#36335;&#32593;&#32476;&#21305;&#37197;&#19981;&#22815;&#28789;&#27963;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33258;&#30001;&#28418;&#28014;&#20132;&#36890;&#27169;&#24335;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#22270;&#26500;&#24314;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65288;HDPC-L&#65289;&#26469;&#30830;&#23450;&#22270;&#20013;&#33410;&#28857;&#30340;&#28789;&#27963;&#23450;&#20301;&#65292;&#20811;&#26381;&#20256;&#32479;&#32858;&#31867;&#31639;&#27861;&#30340;&#35745;&#31639;&#29942;&#39048;&#65292;&#23454;&#29616;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#26377;&#25928;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#20056;&#23458;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#20197;&#21021;&#22987;&#21270;GNNs&#30340;&#36793;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00276v1 Announce Type: new  Abstract: Graph neural networks (GNNs) have been widely applied in traffic demand prediction, and transportation modes can be divided into station-based mode and free-floating traffic mode. Existing research in traffic graph construction primarily relies on map matching to construct graphs based on the road network. However, the complexity and inhomogeneity of data distribution in free-floating traffic demand forecasting make road network matching inflexible. To tackle these challenges, this paper introduces a novel graph construction method tailored to free-floating traffic mode. We propose a novel density-based clustering algorithm (HDPC-L) to determine the flexible positioning of nodes in the graph, overcoming the computational bottlenecks of traditional clustering algorithms and enabling effective handling of large-scale datasets. Furthermore, we extract valuable information from ridership data to initialize the edge weights of GNNs. Comprehen
&lt;/p&gt;</description></item><item><title>TOTEM&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20196;&#29260;&#21270;&#26550;&#26500;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#31163;&#25955;&#30690;&#37327;&#21270;&#34920;&#31034;&#23884;&#20837;&#19981;&#21516;&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#33021;&#22815;&#23454;&#29616;&#36890;&#29992;&#12289;&#36328;&#39046;&#22495;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.16412</link><description>&lt;p&gt;
TOTEM&#65306;&#29992;&#20110;&#19968;&#33324;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#20196;&#29260;&#21270;&#26102;&#38388;&#24207;&#21015;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
TOTEM: TOkenized Time Series EMbeddings for General Time Series Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16412
&lt;/p&gt;
&lt;p&gt;
TOTEM&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20196;&#29260;&#21270;&#26550;&#26500;&#65292;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#31163;&#25955;&#30690;&#37327;&#21270;&#34920;&#31034;&#23884;&#20837;&#19981;&#21516;&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#33021;&#22815;&#23454;&#29616;&#36890;&#29992;&#12289;&#36328;&#39046;&#22495;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#33324;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#24320;&#22987;&#25506;&#32034;&#32479;&#19968;&#24314;&#27169;&#65292;&#20854;&#20013;&#19968;&#20010;&#36890;&#29992;&#30340;&#26550;&#26500;&#21487;&#20197;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#29305;&#23450;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#26412;&#25991;&#20174;&#19968;&#20010;&#20114;&#34917;&#30340;&#35282;&#24230;&#25509;&#36817;&#32479;&#19968;&#21270;&#65306;&#36328;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#32479;&#19968;&#21270;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#31163;&#25955;&#12289;&#23398;&#20064;&#21040;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#34920;&#31034;&#23545;&#21551;&#29992;&#36890;&#29992;&#12289;&#36328;&#39046;&#22495;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;TOTEM&#65292;&#21363;TOkenized Time Series EMbeddings&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26631;&#35760;&#22120;&#26550;&#26500;&#65292;&#36890;&#36807;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#23398;&#20064;&#30340;&#31163;&#25955;&#30690;&#37327;&#21270;&#34920;&#31034;&#23884;&#20837;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#12290;TOTEM&#21487;&#20197;&#36328;&#22810;&#20010;&#20219;&#21153;&#21644;&#39046;&#22495;&#24037;&#20316;&#65292;&#20960;&#20046;&#19981;&#38656;&#35201;&#35843;&#25972;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;3&#20010;&#20219;&#21153;&#19978;&#30340;17&#20010;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#26469;&#30740;&#31350;TOTEM&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19987;&#23478;&#65288;&#21363;&#22312;&#27599;&#20010;&#39046;&#22495;&#35757;&#32451;&#27169;&#22411;&#65289;&#21644;&#36890;&#29992;&#65288;&#21363;&#35757;&#32451;&#65289;&#30340;TOTEM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16412v1 Announce Type: new  Abstract: The field of general time series analysis has recently begun to explore unified modeling, where a common architectural backbone can be retrained on a specific task for a specific dataset. In this work, we approach unification from a complementary vantage point: unification across tasks and domains. To this end, we explore the impact of discrete, learnt, time series data representations that enable generalist, cross-domain training. Our method, TOTEM, or TOkenized Time Series EMbeddings, proposes a simple tokenizer architecture that embeds time series data from varying domains using a discrete vectorized representation learned in a self-supervised manner. TOTEM works across multiple tasks and domains with minimal to no tuning. We study the efficacy of TOTEM with an extensive evaluation on 17 real world time series datasets across 3 tasks. We evaluate both the specialist (i.e., training a model on each domain) and generalist (i.e., trainin
&lt;/p&gt;</description></item><item><title>&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#65292;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;</title><link>https://arxiv.org/abs/2402.13144</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Neural Network Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13144
&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#19982;&#35757;&#32451;&#32593;&#32476;&#30456;&#23218;&#32654;&#29978;&#33267;&#26356;&#22909;&#65292;&#19988;&#25104;&#26412;&#26497;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25193;&#25955;&#27169;&#22411;&#20063;&#21487;&#20197;\textit{&#29983;&#25104;&#34920;&#29616;&#20248;&#24322;&#30340;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;}&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65292;&#21033;&#29992;&#20102;&#33258;&#21160;&#32534;&#30721;&#22120;&#21644;&#26631;&#20934;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#33258;&#21160;&#32534;&#30721;&#22120;&#25552;&#21462;&#20102;&#37096;&#20998;&#21463;&#35757;&#32593;&#32476;&#21442;&#25968;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#28982;&#21518;&#35757;&#32451;&#20102;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#26469;&#20174;&#38543;&#26426;&#22122;&#22768;&#20013;&#21512;&#25104;&#36825;&#20123;&#28508;&#22312;&#21442;&#25968;&#34920;&#31034;&#12290;&#23427;&#29983;&#25104;&#20102;&#26032;&#30340;&#34920;&#31034;&#65292;&#32463;&#36807;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#35299;&#30721;&#22120;&#65292;&#36755;&#20986;&#20934;&#22791;&#29992;&#20316;&#26032;&#30340;&#32593;&#32476;&#21442;&#25968;&#23376;&#38598;&#12290;&#22312;&#21508;&#31181;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#25193;&#25955;&#36807;&#31243;&#22987;&#32456;&#29983;&#25104;&#24615;&#33021;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27169;&#22411;&#65292;&#38468;&#21152;&#25104;&#26412;&#26497;&#23567;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#22312;&#23454;&#35777;&#30740;&#31350;&#20013;&#21457;&#29616;&#65292;&#29983;&#25104;&#30340;&#27169;&#22411;&#19982;&#32463;&#36807;&#35757;&#32451;&#30340;&#32593;&#32476;&#34920;&#29616;&#20986;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13144v1 Announce Type: new  Abstract: Diffusion models have achieved remarkable success in image and video generation. In this work, we demonstrate that diffusion models can also \textit{generate high-performing neural network parameters}. Our approach is simple, utilizing an autoencoder and a standard latent diffusion model. The autoencoder extracts latent representations of a subset of the trained network parameters. A diffusion model is then trained to synthesize these latent parameter representations from random noise. It then generates new representations that are passed through the autoencoder's decoder, whose outputs are ready to use as new subsets of network parameters. Across various architectures and datasets, our diffusion process consistently generates models of comparable or improved performance over trained networks, with minimal additional cost. Notably, we empirically find that the generated models perform differently with the trained networks. Our results en
&lt;/p&gt;</description></item><item><title>Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.10941</link><description>&lt;p&gt;
Text2Data&#65306;&#20351;&#29992;&#25991;&#26412;&#25511;&#21046;&#30340;&#20302;&#36164;&#28304;&#25968;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Text2Data: Low-Resource Data Generation with Textual Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10941
&lt;/p&gt;
&lt;p&gt;
Text2Data&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#30340;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#20154;&#31867;&#19982;&#26426;&#22120;&#26080;&#32541;&#20132;&#20114;&#30340;&#19968;&#31181;&#24120;&#35265;&#30452;&#25509;&#25511;&#21046;&#20449;&#21495;&#12290;&#24847;&#35782;&#21040;&#36825;&#19968;&#25509;&#21475;&#30340;&#37325;&#35201;&#24615;&#65292;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#27491;&#22312;&#25237;&#20837;&#22823;&#37327;&#31934;&#21147;&#29983;&#25104;&#19982;&#25991;&#26412;&#25351;&#20196;&#22312;&#35821;&#20041;&#19978;&#19968;&#33268;&#30340;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#28085;&#30422;&#22270;&#20687;&#32534;&#36753;&#12289;&#38899;&#39057;&#21512;&#25104;&#12289;&#35270;&#39057;&#29983;&#25104;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20302;&#36164;&#28304;&#39046;&#22495;&#30001;&#20110;&#26114;&#36149;&#27880;&#37322;&#25110;&#22797;&#26434;&#25968;&#25454;&#32467;&#26500;&#65288;&#22914;&#20998;&#23376;&#12289;&#36816;&#21160;&#21160;&#24577;&#21644;&#26102;&#24207;&#65289;&#31561;&#29305;&#28857;&#65292;&#24448;&#24448;&#32570;&#20047;&#25991;&#26412;&#26631;&#31614;&#12290;&#36825;&#31181;&#19981;&#36275;&#38459;&#30861;&#20102;&#30417;&#30563;&#23398;&#20064;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23558;&#20808;&#36827;&#29983;&#25104;&#27169;&#22411;&#24212;&#29992;&#20110;&#25991;&#26412;&#21040;&#25968;&#25454;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#20302;&#36164;&#28304;&#22330;&#26223;&#20013;&#30340;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Text2Data&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36890;&#36807;&#26080;&#30417;&#30563;&#25193;&#25955;&#27169;&#22411;&#26469;&#29702;&#35299;&#22522;&#30784;&#25968;&#25454;&#20998;&#24067;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10941v1 Announce Type: cross  Abstract: Natural language serves as a common and straightforward control signal for humans to interact seamlessly with machines. Recognizing the importance of this interface, the machine learning community is investing considerable effort in generating data that is semantically coherent with textual instructions. While strides have been made in text-to-data generation spanning image editing, audio synthesis, video creation, and beyond, low-resource areas characterized by expensive annotations or complex data structures, such as molecules, motion dynamics, and time series, often lack textual labels. This deficiency impedes supervised learning, thereby constraining the application of advanced generative models for text-to-data tasks. In response to these challenges in the low-resource scenario, we propose Text2Data, a novel approach that utilizes unlabeled data to understand the underlying data distribution through an unsupervised diffusion model
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;Link-MoE&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25104;&#23545;&#20449;&#24687;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.08583</link><description>&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixture of Link Predictors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08583
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#30340;&#28151;&#21512;&#27169;&#22411;Link-MoE&#65292;&#36890;&#36807;&#36873;&#25321;&#21512;&#36866;&#30340;&#19987;&#23478;&#27169;&#22411;&#65292;&#21033;&#29992;&#19981;&#21516;&#31867;&#22411;&#30340;&#25104;&#23545;&#20449;&#24687;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#22270;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#65292;&#26088;&#22312;&#39044;&#27979;&#22270;&#20013;&#26410;&#35265;&#36830;&#25509;&#12290;&#21551;&#21457;&#24335;&#26041;&#27861;&#21033;&#29992;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#25104;&#23545;&#24230;&#37327;&#65292;&#22914;&#20849;&#21516;&#37051;&#23621;&#21644;&#26368;&#30701;&#36335;&#24452;&#65292;&#24120;&#24120;&#33021;&#22815;&#19982;&#32431;&#31929;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24615;&#33021;&#30456;&#23218;&#32654;&#12290;&#22240;&#27492;&#65292;&#36817;&#26399;GNNs&#22312;&#38142;&#25509;&#39044;&#27979;&#65288;GNN4LP&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20027;&#35201;&#38598;&#20013;&#22312;&#25972;&#21512;&#19968;&#31181;&#25110;&#23569;&#25968;&#20960;&#31181;&#25104;&#23545;&#20449;&#24687;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21516;&#19968;&#25968;&#25454;&#38598;&#20013;&#30340;&#19981;&#21516;&#33410;&#28857;&#23545;&#38656;&#35201;&#19981;&#21516;&#30340;&#25104;&#23545;&#20449;&#24687;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#65292;&#32780;&#21482;&#24212;&#29992;&#30456;&#21516;&#30340;&#25104;&#23545;&#20449;&#24687;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#27425;&#20248;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#19987;&#23478;&#27169;&#22411;Link-MoE&#29992;&#20110;&#38142;&#25509;&#39044;&#27979;&#12290;Link-MoE&#21033;&#29992;&#21508;&#31181;GNNs&#20316;&#20026;&#19987;&#23478;&#65292;&#24182;&#26681;&#25454;&#19981;&#21516;&#31867;&#22411;&#30340;&#25104;&#23545;&#20449;&#24687;&#20026;&#27599;&#20010;&#33410;&#28857;&#23545;&#36873;&#25321;&#21512;&#36866;&#30340;&#19987;&#23478;&#12290;&#22312;&#21508;&#31181;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;Link-MoE&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction, which aims to forecast unseen connections in graphs, is a fundamental task in graph machine learning. Heuristic methods, leveraging a range of different pairwise measures such as common neighbors and shortest paths, often rival the performance of vanilla Graph Neural Networks (GNNs). Therefore, recent advancements in GNNs for link prediction (GNN4LP) have primarily focused on integrating one or a few types of pairwise information. In this work, we reveal that different node pairs within the same dataset necessitate varied pairwise information for accurate prediction and models that only apply the same pairwise information uniformly could achieve suboptimal performance. As a result, we propose a simple mixture of experts model Link-MoE for link prediction. Link-MoE utilizes various GNNs as experts and strategically selects the appropriate expert for each node pair based on various types of pairwise information. Experimental results across diverse real-world datasets dem
&lt;/p&gt;</description></item><item><title>&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#65292;RAISocketAI&#25104;&#20026;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20987;&#36133;&#20102;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#65292;&#22312;&#26410;&#26469;&#30340;&#31454;&#36187;&#20013;&#21487;&#20197;&#20316;&#20026;&#22522;&#20934;&#21442;&#32771;&#65292;&#24182;&#20026;DRL&#30740;&#31350;&#25552;&#20379;&#36215;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.08112</link><description>&lt;p&gt;
&#19968;&#31181;&#22312;microRTS&#20013;&#33719;&#22870;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
A Competition Winning Deep Reinforcement Learning Agent in microRTS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08112
&lt;/p&gt;
&lt;p&gt;
&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#65292;RAISocketAI&#25104;&#20026;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65292;&#23427;&#36890;&#36807;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#20987;&#36133;&#20102;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#65292;&#22312;&#26410;&#26469;&#30340;&#31454;&#36187;&#20013;&#21487;&#20197;&#20316;&#20026;&#22522;&#20934;&#21442;&#32771;&#65292;&#24182;&#20026;DRL&#30740;&#31350;&#25552;&#20379;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;CIG&#21644;CoG&#20030;&#21150;&#30340;IEEE microRTS&#65288;$\mu$RTS&#65289;&#31454;&#36187;&#30340;&#20116;&#23626;&#20013;&#65292;&#33050;&#26412;&#20195;&#29702;&#20027;&#23548;&#20102;&#27604;&#36187;&#12290;&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31639;&#27861;&#22312;&#23454;&#26102;&#31574;&#30053;&#65288;RTS&#65289;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#22823;&#37327;&#30340;&#22521;&#35757;&#36164;&#28304;&#20197;&#21450;&#21019;&#24314;&#21644;&#35843;&#35797;&#27492;&#31867;&#20195;&#29702;&#25152;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#65292;&#23427;&#20204;&#22312;&#36825;&#20010;&#20027;&#35201;&#26159;&#23398;&#26415;&#31454;&#36187;&#20013;&#30340;&#37319;&#29992;&#20173;&#28982;&#26377;&#38480;&#12290;RAISocketAI&#26159;&#31532;&#19968;&#20010;&#22312;IEEE microRTS&#31454;&#36187;&#20013;&#33719;&#32988;&#30340;DRL&#20195;&#29702;&#12290;&#22312;&#19968;&#20010;&#27809;&#26377;&#24615;&#33021;&#38480;&#21046;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;RAISocketAI&#32463;&#24120;&#20987;&#36133;&#21069;&#20004;&#20301;&#31454;&#36187;&#33719;&#32988;&#32773;&#12290;&#36825;&#20010;&#31532;&#19968;&#20010;&#33719;&#32988;&#30340;DRL&#25552;&#20132;&#21487;&#20197;&#25104;&#20026;&#26410;&#26469;microRTS&#31454;&#36187;&#30340;&#22522;&#20934;&#65292;&#24182;&#25104;&#20026;&#26410;&#26469;DRL&#30740;&#31350;&#30340;&#36215;&#28857;&#12290;&#36880;&#27493;&#20248;&#21270;&#22522;&#26412;&#31574;&#30053;&#21644;&#23545;&#29305;&#23450;&#22320;&#22270;&#36827;&#34892;&#36801;&#31227;&#23398;&#20064;&#23545;RAISocketAI&#30340;&#33719;&#32988;&#34920;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#31574;&#30053;&#21487;&#20197;&#29992;&#20110;&#32463;&#27982;&#35757;&#32451;&#26410;&#26469;&#30340;DRL&#20195;&#29702;&#12290;&#22312;&#27169;&#20223;&#23398;&#20064;&#26041;&#38754;&#30340;&#36827;&#19968;&#27493;&#24037;&#20316;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;DRL&#20195;&#29702;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scripted agents have predominantly won the five previous iterations of the IEEE microRTS ($\mu$RTS) competitions hosted at CIG and CoG. Despite Deep Reinforcement Learning (DRL) algorithms making significant strides in real-time strategy (RTS) games, their adoption in this primarily academic competition has been limited due to the considerable training resources required and the complexity inherent in creating and debugging such agents. RAISocketAI is the first DRL agent to win the IEEE microRTS competition. In a benchmark without performance constraints, RAISocketAI regularly defeated the two prior competition winners. This first competition-winning DRL submission can be a benchmark for future microRTS competitions and a starting point for future DRL research. Iteratively fine-tuning the base policy and transfer learning to specific maps were critical to RAISocketAI's winning performance. These strategies can be used to economically train future DRL agents. Further work in Imitation L
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#26368;&#20248;&#24615;&#21407;&#29702;&#30740;&#31350;&#20102;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#25104;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20998;&#35299;&#23376;&#28216;&#25103;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#24320;&#20102;&#20915;&#31574;&#21464;&#37327;&#30340;&#32416;&#32544;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.02954</link><description>&lt;p&gt;
&#35299;&#20915;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65306;&#19968;&#31181;&#24191;&#20041;&#21338;&#24328;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Solving Hierarchical Information-Sharing Dec-POMDPs: An Extensive-Form Game Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24212;&#29992;&#26368;&#20248;&#24615;&#21407;&#29702;&#30740;&#31350;&#20102;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#25104;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#20998;&#35299;&#23376;&#28216;&#25103;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#35299;&#24320;&#20102;&#20915;&#31574;&#21464;&#37327;&#30340;&#32416;&#32544;&#65292;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#29702;&#35770;&#34920;&#26126;&#65292;&#22810;&#20154;&#20998;&#25955;&#30340;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21487;&#20197;&#36716;&#21270;&#20026;&#31561;&#25928;&#30340;&#21333;&#20154;&#28216;&#25103;&#65292;&#20351;&#24471;&#21487;&#20197;&#24212;&#29992;&#36125;&#23572;&#26364;&#30340;&#26368;&#20248;&#24615;&#21407;&#29702;&#36890;&#36807;&#23558;&#20854;&#20998;&#35299;&#20026;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#26469;&#35299;&#20915;&#21333;&#20154;&#28216;&#25103;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#27599;&#20010;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#20013;&#32416;&#32544;&#20102;&#25152;&#26377;&#29609;&#23478;&#30340;&#20915;&#31574;&#21464;&#37327;&#65292;&#23548;&#33268;&#25351;&#25968;&#22797;&#26434;&#24230;&#30340;&#22791;&#20221;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#20445;&#25345;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#21069;&#25552;&#19979;&#35299;&#24320;&#36825;&#20123;&#20915;&#31574;&#21464;&#37327;&#30340;&#32416;&#32544;&#65292;&#36825;&#26159;&#25105;&#20204;&#31038;&#20250;&#20013;&#19968;&#31181;&#31361;&#20986;&#30340;&#31649;&#29702;&#39118;&#26684;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24212;&#29992;&#26368;&#20248;&#24615;&#21407;&#29702;&#36890;&#36807;&#36827;&#19968;&#27493;&#23558;&#20219;&#20309;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#20998;&#35299;&#20026;&#26356;&#23567;&#30340;&#23376;&#28216;&#25103;&#26469;&#35299;&#20915;&#23427;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#36880;&#27425;&#36827;&#34892;&#21333;&#20154;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25581;&#31034;&#20102;&#23384;&#22312;&#20110;&#21333;&#38454;&#27573;&#23376;&#28216;&#25103;&#20013;&#30340;&#24191;&#20041;&#21338;&#24328;&#35299;&#20915;&#26041;&#26696;&#65292;&#26497;&#22823;&#22320;&#20943;&#23569;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35777;&#26126;&#23427;&#21487;&#20197;&#22312;&#35299;&#20915;&#20998;&#23618;&#20449;&#24687;&#20849;&#20139;&#30340;&#20998;&#24067;&#24335;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent theory shows that a multi-player decentralized partially observable Markov decision process can be transformed into an equivalent single-player game, enabling the application of \citeauthor{bellman}'s principle of optimality to solve the single-player game by breaking it down into single-stage subgames. However, this approach entangles the decision variables of all players at each single-stage subgame, resulting in backups with a double-exponential complexity. This paper demonstrates how to disentangle these decision variables while maintaining optimality under hierarchical information sharing, a prominent management style in our society. To achieve this, we apply the principle of optimality to solve any single-stage subgame by breaking it down further into smaller subgames, enabling us to make single-player decisions at a time. Our approach reveals that extensive-form games always exist with solutions to a single-stage subgame, significantly reducing time complexity. Our expe
&lt;/p&gt;</description></item><item><title>SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.01855</link><description>&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#31471;&#21040;&#31471;&#31070;&#32463;&#25968;&#25454;&#21516;&#21270;&#26041;&#26696;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SPDE priors for uncertainty quantification of end-to-end neural data assimilation schemes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01855
&lt;/p&gt;
&lt;p&gt;
SPDE&#20808;&#39564;&#22312;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#65292;&#20026;&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#22320;&#29699;&#29289;&#29702;&#25968;&#25454;&#38598;&#30340;&#26102;&#31354;&#25554;&#20540;&#36890;&#24120;&#36890;&#36807;&#26368;&#20248;&#25554;&#20540;(Optimal Interpolation&#65292;OI)&#21644;&#26356;&#22797;&#26434;&#30340;&#22522;&#20110;&#27169;&#22411;&#25110;&#25968;&#25454;&#39537;&#21160;&#30340;&#25968;&#25454;&#21516;&#21270;&#25216;&#26415;&#26469;&#22788;&#29702;&#12290;&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#38543;&#26426;&#20559;&#24494;&#20998;&#26041;&#31243;(Spatio-temporal Partial Differential Equations&#65292;SPDE)&#21644;&#39640;&#26031;&#39532;&#23572;&#31185;&#22827;&#38543;&#26426;&#22330;(Gaussian Markov Random Fields&#65292;GMRF)&#20043;&#38388;&#30340;&#32852;&#31995;&#24320;&#36767;&#20102;&#19968;&#26465;&#26032;&#30340;&#36884;&#24452;&#65292;&#29992;&#20110;&#22788;&#29702;&#26368;&#20248;&#25554;&#20540;&#20013;&#30340;&#22823;&#25968;&#25454;&#38598;&#21644;&#29289;&#29702;&#35825;&#23548;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#26368;&#26032;&#36827;&#23637;&#20063;&#20351;&#24471;&#21487;&#20197;&#23558;&#36825;&#20010;&#38382;&#39064;&#35270;&#20026;&#23884;&#20837;&#25968;&#25454;&#21516;&#21270;&#21464;&#20998;&#26694;&#26550;&#30340;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#32852;&#21512;&#23398;&#20064;&#38382;&#39064;&#12290;&#37325;&#24314;&#20219;&#21153;&#34987;&#35270;&#20026;&#19968;&#20010;&#21253;&#21547;&#22312;&#21464;&#20998;&#20869;&#37096;&#25104;&#26412;&#20013;&#30340;&#20808;&#39564;&#23398;&#20064;&#38382;&#39064;&#21644;&#21518;&#32773;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26368;&#23567;&#21270;&#65306;&#20808;&#39564;&#27169;&#22411;&#21644;&#27714;&#35299;&#22120;&#37117;&#34987;&#34920;&#31034;&#20026;&#20855;&#26377;&#33258;&#21160;&#24494;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#26368;&#23567;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#35757;&#32451;&#65292;&#35813;&#25439;&#22833;&#20989;&#25968;&#36890;&#24120;&#34987;&#34920;&#31034;&#20026;&#19968;&#20123;&#30495;&#23454;&#20540;&#21644;&#37325;&#24314;&#20540;&#20043;&#38388;&#30340;&#22343;&#26041;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The spatio-temporal interpolation of large geophysical datasets has historically been adressed by Optimal Interpolation (OI) and more sophisticated model-based or data-driven DA techniques. In the last ten years, the link established between Stochastic Partial Differential Equations (SPDE) and Gaussian Markov Random Fields (GMRF) opened a new way of handling both large datasets and physically-induced covariance matrix in Optimal Interpolation. Recent advances in the deep learning community also enables to adress this problem as neural architecture embedding data assimilation variational framework. The reconstruction task is seen as a joint learning problem of the prior involved in the variational inner cost and the gradient-based minimization of the latter: both prior models and solvers are stated as neural networks with automatic differentiation which can be trained by minimizing a loss function, typically stated as the mean squared error between some ground truth and the reconstructi
&lt;/p&gt;</description></item><item><title>CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00450</link><description>&lt;p&gt;
CPT: &#24212;&#29992;&#20110;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#30340;&#33021; &#21147;&#36882;&#36827;&#24335;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
CPT: Competence-progressive Training Strategy for Few-shot Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00450
&lt;/p&gt;
&lt;p&gt;
CPT&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#20256;&#32479;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#19978;&#30340;&#22256;&#38590;&#12290;&#23427;&#20351;&#29992;&#33021;&#21147;&#36882;&#36827;&#30340;&#35757;&#32451;&#31574;&#30053;&#26469;&#25552;&#39640;&#20803;&#23398;&#20064;&#22120;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#33410;&#28857;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#25104;&#21151;&#20173;&#28982;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#20013;&#27599;&#20010;&#31867;&#21035;&#26377;&#36275;&#22815;&#30340;&#26631;&#35760;&#33410;&#28857;&#12290;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#25968;&#25454;&#36890;&#24120;&#21576;&#29616;&#20986;&#38271;&#23614;&#20998;&#24067;&#65292;&#26631;&#31614;&#31232;&#30095;&#65292;&#24378;&#35843;&#20102;GNN&#22312;&#23569;&#26679;&#26412;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#37325;&#35201;&#24615;&#65292;&#21363;&#20351;&#29992;&#26377;&#38480;&#30340;&#25968;&#25454;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#12290;&#20256;&#32479;&#30340;&#24773;&#33410;&#20803;&#23398;&#20064;&#26041;&#27861;&#22312;&#36825;&#20010;&#39046;&#22495;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#22266;&#26377;&#30340;&#38480;&#21046;&#65306;&#38543;&#26426;&#21644;&#22343;&#21248;&#20219;&#21153;&#20998;&#37197;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#25910;&#25947;&#21040;&#27425;&#20248;&#35299;&#65292;&#24573;&#35270;&#20102;&#20219;&#21153;&#30340;&#38590;&#24230;&#27700;&#24179;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#20803;&#23398;&#20064;&#22120;&#36807;&#26089;&#22320;&#38754;&#20020;&#22797;&#26434;&#20219;&#21153;&#65292;&#38459;&#30861;&#20102;&#27491;&#24120;&#30340;&#23398;&#20064;&#12290;&#29702;&#24819;&#24773;&#20917;&#19979;&#65292;&#20803;&#23398;&#20064;&#22120;&#24212;&#35813;&#20174;&#31616;&#21333;&#27010;&#24565;&#24320;&#22987;&#65292;&#36880;&#28176;&#36827;&#20837;&#26356;&#22797;&#26434;&#30340;&#27010;&#24565;&#65292;&#23601;&#20687;&#20154;&#31867;&#23398;&#20064;&#19968;&#26679;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;CPT&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#35838;&#31243;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#38590;&#24230;&#19982;&#20803;&#23398;&#20064;&#22120;&#30340;&#36882;&#36827;&#33021;&#21147;&#30456;&#21305;&#37197;&#65292;&#22686;&#24378;&#20102;&#20803;&#23398;&#20064;&#30340;&#25928;&#26524;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have made significant advancements in node classification, but their success relies on sufficient labeled nodes per class in the training data. Real-world graph data often exhibits a long-tail distribution with sparse labels, emphasizing the importance of GNNs' ability in few-shot node classification, which entails categorizing nodes with limited data. Traditional episodic meta-learning approaches have shown promise in this domain, but they face an inherent limitation: it might lead the model to converge to suboptimal solutions because of random and uniform task assignment, ignoring task difficulty levels. This could lead the meta-learner to face complex tasks too soon, hindering proper learning. Ideally, the meta-learner should start with simple concepts and advance to more complex ones, like human learning. So, we introduce CPT, a novel two-stage curriculum learning method that aligns task difficulty with the meta-learner's progressive competence, enhanci
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18751</link><description>&lt;p&gt;
&#22312;Web&#19978;&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#22312;&#39034;&#24207;&#20219;&#21153;&#32452;&#21512;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702; (LMA) &#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#65292;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#24179;&#34913;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411; HTML-T5++&#65292;&#22312;&#29616;&#23454;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#36229;&#36234;&#20154;&#31867;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26032;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#26368;&#20339;&#38646;-shot&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;(LMA)&#20316;&#20026;&#19968;&#31181;&#22312;&#22810;&#27493;&#20915;&#31574;&#20219;&#21153;&#19978;&#30340;&#26377;&#24076;&#26395;&#30340;&#33539;&#20363;&#20986;&#29616;&#65292;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20154;&#31867;&#21644;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#24076;&#26395;&#65292;&#20294;&#23427;&#20204;&#22312;&#36890;&#24120;&#28041;&#21450;&#20219;&#21153;&#32452;&#21512;&#30340;&#29616;&#23454;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#21483;&#20570;CompWoB-&#21453;&#26144;&#26356;&#29616;&#23454;&#20551;&#35774;&#30340;50&#20010;&#32452;&#21512;&#24615;&#32593;&#31449;&#33258;&#21160;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#29616;&#26377;&#30340;&#25552;&#31034;&#22411;LMA&#65288;gpt-3.5-turbo&#25110;gpt-4&#65289;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;94.0&#65285;&#30340;&#24179;&#22343;&#25104;&#21151;&#29575;&#65292;&#20294;&#22312;&#32452;&#21512;&#20219;&#21153;&#19978;&#38477;&#33267;24.9&#65285;&#30340;&#25104;&#21151;&#29575;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21482;&#22312;&#22522;&#26412;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#36716;&#31227;&#24615;LMA&#34920;&#29616;&#20986;&#26356;&#23567;&#30340;&#27867;&#21270;&#24615;&#24046;&#36317;&#65292;&#20174;85.4&#65285;&#19979;&#38477;&#21040;54.8&#65285;&#12290;&#36890;&#36807;&#24179;&#34913;&#20219;&#21153;&#20043;&#38388;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#26032;&#27169;&#22411;HTML-T5++&#65292;&#22312;MiniWoB&#19978;&#36229;&#36807;&#20102;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#65288;95.2&#65285;&#65289;&#65292;&#24182;&#22312;CompWoB&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#30340;&#38646;-shot&#24615;&#33021;&#65288;61.5%&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language model agents (LMA) recently emerged as a promising paradigm on muti-step decision making tasks, often outperforming humans and other reinforcement learning agents. Despite the promise, their performance on real-world applications that often involve combinations of tasks is still underexplored. In this work, we introduce a new benchmark, called CompWoB -- 50 new compositional web automation tasks reflecting more realistic assumptions. We show that while existing prompted LMAs (gpt-3.5-turbo or gpt-4) achieve 94.0% average success rate on base tasks, their performance degrades to 24.9% success rate on compositional tasks. On the other hand, transferred LMAs (finetuned only on base tasks) show less generalization gap, dropping from 85.4% to 54.8%. By balancing data distribution across tasks, we train a new model, HTML-T5++, that surpasses human-level performance (95.2%) on MiniWoB, and achieves the best zero-shot performance on CompWoB (61.5%). While these highlight the promise o
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.14142</link><description>&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65306;&#32479;&#19968;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Energy-Based Concept Bottleneck Models: Unifying Prediction, Concept Intervention, and Conditional Interpretations. (arXiv:2401.14142v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14142
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#32479;&#19968;&#20102;&#39044;&#27979;&#12289;&#27010;&#24565;&#24178;&#39044;&#21644;&#26465;&#20214;&#35299;&#37322;&#30340;&#21151;&#33021;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#21644;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#19978;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26041;&#27861;&#65292;&#22914;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411; (CBM)&#65292;&#22312;&#20026;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#23427;&#20204;&#36890;&#24120;&#36890;&#36807;&#22312;&#32473;&#23450;&#36755;&#20837;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#27010;&#24565;&#65292;&#28982;&#21518;&#22312;&#32473;&#23450;&#39044;&#27979;&#30340;&#27010;&#24565;&#30340;&#24773;&#20917;&#19979;&#39044;&#27979;&#26368;&#32456;&#30340;&#31867;&#21035;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#26080;&#27861;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#39640;&#38454;&#38750;&#32447;&#24615;&#30456;&#20114;&#20316;&#29992;&#65292;&#20363;&#22914;&#32416;&#27491;&#19968;&#20010;&#39044;&#27979;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33016;&#37096;&#8221;&#65289;&#26080;&#27861;&#24110;&#21161;&#32416;&#27491;&#39640;&#24230;&#30456;&#20851;&#30340;&#27010;&#24565;&#65288;&#20363;&#22914;&#8220;&#40644;&#33394;&#33145;&#37096;&#8221;&#65289;&#65292;&#23548;&#33268;&#26368;&#32456;&#20934;&#30830;&#29575;&#19981;&#29702;&#24819;&#65307;&#23427;&#20204;&#26080;&#27861;&#33258;&#28982;&#22320;&#37327;&#21270;&#19981;&#21516;&#27010;&#24565;&#21644;&#31867;&#21035;&#26631;&#31614;&#20043;&#38388;&#30340;&#22797;&#26434;&#26465;&#20214;&#20381;&#36182;&#20851;&#31995;&#65288;&#20363;&#22914;&#23545;&#20110;&#19968;&#20010;&#24102;&#26377;&#31867;&#21035;&#26631;&#31614;&#8220;Kentucky Warbler&#8221;&#21644;&#27010;&#24565;&#8220;&#40657;&#33394;&#22068;&#24052;&#8221;&#30340;&#22270;&#20687;&#65292;&#27169;&#22411;&#33021;&#22815;&#27491;&#30830;&#39044;&#27979;&#21478;&#19968;&#20010;&#27010;&#24565;&#8220;&#40657;&#33394;&#20896;&#8221;&#30340;&#27010;&#29575;&#26159;&#22810;&#23569;&#65289;&#65292;&#22240;&#27492;&#26080;&#27861;&#25552;&#20379;&#20851;&#20110;&#40657;&#30418;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#26356;&#28145;&#23618;&#27425;&#30340;&#27934;&#23519;&#12290;&#38024;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#33021;&#37327;&#30340;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;Energy-based Concept Bottleneck Models&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing methods, such as concept bottleneck models (CBMs), have been successful in providing concept-based interpretations for black-box deep learning models. They typically work by predicting concepts given the input and then predicting the final class label given the predicted concepts. However, (1) they often fail to capture the high-order, nonlinear interaction between concepts, e.g., correcting a predicted concept (e.g., "yellow breast") does not help correct highly correlated concepts (e.g., "yellow belly"), leading to suboptimal final accuracy; (2) they cannot naturally quantify the complex conditional dependencies between different concepts and class labels (e.g., for an image with the class label "Kentucky Warbler" and a concept "black bill", what is the probability that the model correctly predicts another concept "black crown"), therefore failing to provide deeper insight into how a black-box model works. In response to these limitations, we propose Energy-based Concept Bot
&lt;/p&gt;</description></item><item><title>Des-q&#26159;&#19968;&#31181;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#12290;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#26641;&#37325;&#26032;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#26032;&#26679;&#26412;&#30340;&#21152;&#36733;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807; k &#20998;&#27573;&#32447;&#24615;&#26641;&#20998;&#35010;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2309.09976</link><description>&lt;p&gt;
Des-q: &#19968;&#31181;&#29992;&#20110;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#30340;&#26500;&#24314;&#21644;&#39640;&#25928;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#30340;&#37327;&#23376;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Des-q: a quantum algorithm to construct and efficiently retrain decision trees for regression and binary classification. (arXiv:2309.09976v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09976
&lt;/p&gt;
&lt;p&gt;
Des-q&#26159;&#19968;&#31181;&#37327;&#23376;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#12290;&#23427;&#26174;&#33879;&#20943;&#23569;&#20102;&#26641;&#37325;&#26032;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#26032;&#26679;&#26412;&#30340;&#21152;&#36733;&#26102;&#38388;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807; k &#20998;&#27573;&#32447;&#24615;&#26641;&#20998;&#35010;&#26469;&#26500;&#24314;&#20915;&#31574;&#26641;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#26641;&#30001;&#20110;&#20854;&#31616;&#21333;&#26500;&#36896;&#21644;&#21487;&#35299;&#37322;&#24615;&#32780;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#38271;&#65292;&#20256;&#32479;&#30340;&#20915;&#31574;&#26641;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#24930;&#65292;&#19982;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#21576;&#22810;&#39033;&#24335;&#35268;&#27169;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#31639;&#27861;Des-q&#65292;&#29992;&#20110;&#22312;&#22238;&#24402;&#21644;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#26500;&#24314;&#21644;&#37325;&#26032;&#35757;&#32451;&#20915;&#31574;&#26641;&#12290;&#20551;&#35774;&#25968;&#25454;&#27969;&#20135;&#29983;&#36739;&#23567;&#30340;&#26032;&#35757;&#32451;&#26679;&#26412;&#22686;&#37327;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;Des-q&#31639;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#26641;&#37325;&#26032;&#35757;&#32451;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#21363;&#20351;&#32771;&#34385;&#23558;&#26032;&#26679;&#26412;&#21152;&#36733;&#21040;&#37327;&#23376;&#21487;&#35775;&#38382;&#20869;&#23384;&#25152;&#38656;&#30340;&#26102;&#38388;&#65292;&#20854;&#26102;&#38388;&#22797;&#26434;&#24230;&#20063;&#36798;&#21040;&#20102;&#22810;&#23545;&#25968;&#32423;&#21035;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#19968;&#20010;&#20915;&#31574;&#26641;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#20869;&#37096;&#33410;&#28857;&#25191;&#34892;k&#20998;&#27573;&#32447;&#24615;&#26641;&#20998;&#35010;&#12290;&#36825;&#20123;&#20998;&#35010;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#36229;&#24179;&#38754;&#65292;&#23558;&#25968;&#25454;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#23376;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision trees are widely used in machine learning due to their simplicity in construction and interpretability. However, as data sizes grow, traditional methods for constructing and retraining decision trees become increasingly slow, scaling polynomially with the number of training examples. In this work, we introduce a novel quantum algorithm, named Des-q, for constructing and retraining decision trees in regression and binary classification tasks. Assuming the data stream produces small increments of new training examples, we demonstrate that our Des-q algorithm significantly reduces the time required for tree retraining, achieving a poly-logarithmic time complexity in the number of training examples, even accounting for the time needed to load the new examples into quantum-accessible memory. Our approach involves building a decision tree algorithm to perform k-piecewise linear tree splits at each internal node. These splits simultaneously generate multiple hyperplanes, dividing the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;</title><link>http://arxiv.org/abs/2309.04522</link><description>&lt;p&gt;
&#36830;&#25509;NTK&#21644;NNGP&#65306;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21160;&#21147;&#23398;&#22312;&#26680;&#21306;&#22495;&#30340;&#32479;&#19968;&#29702;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Connecting NTK and NNGP: A Unified Theoretical Framework for Neural Network Learning Dynamics in the Kernel Regime. (arXiv:2309.04522v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#65292;&#29992;&#20110;&#25551;&#36848;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20294;&#20854;&#23398;&#20064;&#36807;&#31243;&#32570;&#20047;&#19968;&#20010;&#23436;&#25972;&#30340;&#29702;&#35770;&#26694;&#26550;&#12290;&#23545;&#20110;&#26080;&#38480;&#23485;&#24230;&#32593;&#32476;&#65292;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#22312;&#36825;&#20010;&#33539;&#24335;&#20013;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#26694;&#26550;&#26469;&#25551;&#36848;&#32593;&#32476;&#30340;&#36755;&#20986;&#65306;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#20999;&#21521;&#26680;&#65288;NTK&#65289;&#30340;&#26694;&#26550;&#65292;&#20551;&#35774;&#20102;&#32447;&#24615;&#21270;&#30340;&#26799;&#24230;&#19979;&#38477;&#21160;&#21147;&#23398;&#65307;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#39640;&#26031;&#36807;&#31243;&#65288;NNGP&#65289;&#26680;&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#20004;&#31181;&#26694;&#26550;&#20043;&#38388;&#30340;&#20851;&#31995;&#19968;&#30452;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#36817;&#20284;&#23398;&#20064;&#27169;&#22411;&#65292;&#32479;&#19968;&#20102;&#36825;&#20004;&#31181;&#19981;&#21516;&#30340;&#29702;&#35770;&#65292;&#29992;&#20110;&#25551;&#36848;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#26080;&#38480;&#23485;&#24230;&#28145;&#23618;&#32593;&#32476;&#30340;&#23398;&#20064;&#21160;&#21147;&#23398;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#21644;&#23398;&#20064;&#21518;&#30340;&#32593;&#32476;&#36755;&#20837;-&#36755;&#20986;&#20989;&#25968;&#30340;&#31934;&#30830;&#20998;&#26512;&#34920;&#36798;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#31070;&#32463;&#21160;&#24577;&#26680;&#65288;NDK&#65289;&#65292;&#36825;&#20010;&#26680;&#21487;&#20197;&#21516;&#26102;&#20135;&#29983;NTK&#21644;NNGP&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial neural networks have revolutionized machine learning in recent years, but a complete theoretical framework for their learning process is still lacking. Substantial progress has been made for infinitely wide networks. In this regime, two disparate theoretical frameworks have been used, in which the network's output is described using kernels: one framework is based on the Neural Tangent Kernel (NTK) which assumes linearized gradient descent dynamics, while the Neural Network Gaussian Process (NNGP) kernel assumes a Bayesian framework. However, the relation between these two frameworks has remained elusive. This work unifies these two distinct theories using a Markov proximal learning model for learning dynamics in an ensemble of randomly initialized infinitely wide deep networks. We derive an exact analytical expression for the network input-output function during and after learning, and introduce a new time-dependent Neural Dynamical Kernel (NDK) from which both NTK and NNGP
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#37325;&#20849;&#32447;&#24615;&#38382;&#39064;&#65292;&#38024;&#23545;&#22235;&#24029;&#30465;&#30340;&#30899;&#25490;&#25918;&#24773;&#20917;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#30830;&#23450;&#20102;&#34892;&#19994;&#20998;&#32452;&#65292;&#35780;&#20272;&#20102;&#25490;&#25918;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#30340;&#20943;&#25490;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2309.01115</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22810;&#37325;&#20849;&#32447;&#24615;&#35299;&#20915;&#26041;&#26696;&#65306;&#22235;&#24029;&#30465;&#30899;&#25490;&#25918;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multicollinearity Resolution Based on Machine Learning: A Case Study of Carbon Emissions in Sichuan Province. (arXiv:2309.01115v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#37325;&#20849;&#32447;&#24615;&#38382;&#39064;&#65292;&#38024;&#23545;&#22235;&#24029;&#30465;&#30340;&#30899;&#25490;&#25918;&#24773;&#20917;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#30740;&#31350;&#32467;&#26524;&#30830;&#23450;&#20102;&#34892;&#19994;&#20998;&#32452;&#65292;&#35780;&#20272;&#20102;&#25490;&#25918;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#30340;&#20943;&#25490;&#31574;&#30053;&#65292;&#20197;&#25913;&#21892;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#30697;&#38453;&#24402;&#19968;&#21270;&#23545;&#22235;&#24029;&#30465;46&#20010;&#20851;&#38190;&#20135;&#19994;2000-2019&#24180;&#30340;&#33021;&#28304;&#28040;&#32791;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#12290;DBSCAN&#32858;&#31867;&#35782;&#21035;&#20102;16&#20010;&#29305;&#24449;&#31867;&#21035;&#20197;&#23458;&#35266;&#22320;&#20998;&#32452;&#34892;&#19994;&#12290;&#25509;&#19979;&#26469;&#65292;&#37319;&#29992;&#32602;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#36807;&#25311;&#21512;&#25511;&#21046;&#12289;&#39640;&#32500;&#25968;&#25454;&#22788;&#29702;&#21644;&#29305;&#24449;&#36873;&#25321;&#31561;&#22797;&#26434;&#33021;&#28304;&#25968;&#25454;&#22788;&#29702;&#30340;&#20248;&#21183;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#29028;&#28845;&#21608;&#22260;&#30340;&#31532;&#20108;&#20010;&#32858;&#31867;&#22240;&#29983;&#20135;&#38656;&#27714;&#32780;&#20135;&#29983;&#30340;&#25490;&#25918;&#37327;&#26368;&#39640;&#12290;&#20197;&#27773;&#27833;&#21644;&#28966;&#28845;&#20026;&#20013;&#24515;&#30340;&#32858;&#31867;&#30340;&#25490;&#25918;&#37327;&#20063;&#24456;&#26174;&#33879;&#12290;&#22522;&#20110;&#27492;&#65292;&#20943;&#25490;&#24314;&#35758;&#21253;&#25324;&#28165;&#27905;&#29028;&#25216;&#26415;&#12289;&#20132;&#36890;&#31649;&#29702;&#12289;&#38050;&#38081;&#20013;&#30340;&#29028;&#30005;&#26367;&#20195;&#21644;&#34892;&#19994;&#26631;&#20934;&#21270;&#12290;&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#23458;&#35266;&#36873;&#25321;&#22240;&#32032;&#65292;&#24182;&#26088;&#22312;&#25506;&#32034;&#26032;&#30340;&#20943;&#25490;&#36884;&#24452;&#12290;&#24635;&#32780;&#35328;&#20043;&#65292;&#26412;&#30740;&#31350;&#30830;&#23450;&#20102;&#34892;&#19994;&#20998;&#32452;&#65292;&#35780;&#20272;&#20102;&#25490;&#25918;&#39537;&#21160;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#31185;&#23398;&#30340;&#20943;&#25490;&#31574;&#30053;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study preprocessed 2000-2019 energy consumption data for 46 key Sichuan industries using matrix normalization. DBSCAN clustering identified 16 feature classes to objectively group industries. Penalized regression models were then applied for their advantages in overfitting control, high-dimensional data processing, and feature selection - well-suited for the complex energy data. Results showed the second cluster around coal had highest emissions due to production needs. Emissions from gasoline-focused and coke-focused clusters were also significant. Based on this, emission reduction suggestions included clean coal technologies, transportation management, coal-electricity replacement in steel, and industry standardization. The research introduced unsupervised learning to objectively select factors and aimed to explore new emission reduction avenues. In summary, the study identified industry groupings, assessed emissions drivers, and proposed scientific reduction strategies to bette
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2307.15398</link><description>&lt;p&gt;
&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Initial Screening Order Problem. (arXiv:2307.15398v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15398
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#22312;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#36215;&#21040;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#23545;&#21463;&#20445;&#25252;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20570;&#20986;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#36825;&#39033;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#65292;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20854;&#28508;&#22312;&#30340;&#33258;&#21160;&#21270;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#38382;&#39064;&#65292;&#36825;&#26159;&#20505;&#36873;&#20154;&#31579;&#36873;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23427;&#28041;&#21450;&#19968;&#20010;&#31867;&#20284;&#20154;&#31867;&#30340;&#31579;&#36873;&#32773;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#32473;&#23450;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#20505;&#36873;&#20154;&#27744;&#20013;&#25214;&#21040;&#21069;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#65292;&#32780;&#19981;&#26159;&#26368;&#22909;&#30340;k&#20010;&#36866;&#21512;&#30340;&#20505;&#36873;&#20154;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#34920;&#31034;&#31867;&#20154;&#31579;&#36873;&#32773;&#22312;&#31579;&#36873;&#20043;&#21069;&#22914;&#20309;&#23433;&#25490;&#20505;&#36873;&#20154;&#27744;&#12290;&#21021;&#22987;&#31579;&#36873;&#39034;&#24207;&#30340;&#36873;&#25321;&#23545;&#25152;&#36873;&#30340;k&#20010;&#20505;&#36873;&#20154;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#20505;&#36873;&#20154;&#27744;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65288;&#20363;&#22914;&#65292;&#30007;&#24615;&#20505;&#36873;&#20154;&#22810;&#20110;&#22899;&#24615;&#20505;&#36873;&#20154;&#65289;&#65292;&#31867;&#20154;&#31579;&#36873;&#32773;&#21487;&#33021;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#23545;&#21463;&#20445;&#25252;&#30340;&#12289;&#20195;&#34920;&#24615;&#19981;&#36275;&#30340;&#32676;&#20307;&#20135;&#29983;&#19981;&#24179;&#31561;&#30340;&#21162;&#21147;&#12290;&#20854;&#20182;&#20844;&#24179;&#24615;&#32467;&#26524;&#20063;&#22312;&#31867;&#20154;&#31579;&#36873;&#32773;&#19979;&#24471;&#21040;&#35777;&#26126;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#19982;&#19968;&#23478;&#22823;&#20844;&#21496;&#21512;&#20316;&#30340;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#20102;&#35299;&#20854;&#28508;&#22312;&#33258;&#21160;&#21270;&#30340;&#25307;&#32856;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present the initial screening order problem, a crucial step within candidate screening. It involves a human-like screener with an objective to find the first k suitable candidates rather than the best k suitable candidates in a candidate pool given an initial screening order. The initial screening order represents the way in which the human-like screener arranges the candidate pool prior to screening. The choice of initial screening order has considerable effects on the selected set of k candidates. We prove that under an unbalanced candidate pool (e.g., having more male than female candidates), the human-like screener can suffer from uneven efforts that hinder its decision-making over the protected, under-represented group relative to the non-protected, over-represented group. Other fairness results are proven under the human-like screener. This research is based on a collaboration with a large company to better understand its hiring process for potential automation. 
&lt;/p&gt;</description></item><item><title>&#23884;&#22871;&#28040;&#38500;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21019;&#26032;&#30340;&#28040;&#38500;&#20934;&#21017;&#21644;&#23884;&#22871;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#26368;&#23569;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#39640;&#32622;&#20449;&#27700;&#24179;&#35782;&#21035;&#20986;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#12290;</title><link>http://arxiv.org/abs/2307.09295</link><description>&lt;p&gt;
&#23884;&#22871;&#28040;&#38500;&#65306;&#19968;&#31181;&#20174;&#22522;&#20110;&#36873;&#25321;&#30340;&#21453;&#39304;&#20013;&#35782;&#21035;&#26368;&#20339;&#39033;&#30446;&#30340;&#31616;&#21333;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Nested Elimination: A Simple Algorithm for Best-Item Identification from Choice-Based Feedback. (arXiv:2307.09295v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09295
&lt;/p&gt;
&lt;p&gt;
&#23884;&#22871;&#28040;&#38500;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#23454;&#29616;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21019;&#26032;&#30340;&#28040;&#38500;&#20934;&#21017;&#21644;&#23884;&#22871;&#32467;&#26500;&#65292;&#33021;&#22815;&#20197;&#26368;&#23569;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#39640;&#32622;&#20449;&#27700;&#24179;&#35782;&#21035;&#20986;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#36873;&#25321;&#30340;&#21453;&#39304;&#20013;&#35782;&#21035;&#26368;&#20339;&#39033;&#30446;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#20844;&#21496;&#20381;&#27425;&#21521;&#19968;&#32676;&#39038;&#23458;&#23637;&#31034;&#26174;&#31034;&#38598;&#65292;&#24182;&#25910;&#38598;&#20182;&#20204;&#30340;&#36873;&#25321;&#12290;&#30446;&#26631;&#26159;&#20197;&#26368;&#23569;&#30340;&#26679;&#26412;&#25968;&#37327;&#21644;&#39640;&#32622;&#20449;&#27700;&#24179;&#35782;&#21035;&#20986;&#26368;&#21463;&#27426;&#36814;&#30340;&#39033;&#30446;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#38500;&#30340;&#31639;&#27861;&#65292;&#21363;&#23884;&#22871;&#28040;&#38500;(Nested Elimination&#65292;NE)&#65292;&#23427;&#21463;&#21040;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#25152;&#26263;&#31034;&#30340;&#23884;&#22871;&#32467;&#26500;&#30340;&#21551;&#21457;&#12290;NE&#30340;&#32467;&#26500;&#31616;&#21333;&#65292;&#26131;&#20110;&#23454;&#26045;&#65292;&#20855;&#26377;&#23545;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#24378;&#22823;&#29702;&#35770;&#20445;&#35777;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;NE&#21033;&#29992;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#28040;&#38500;&#20934;&#21017;&#65292;&#24182;&#36991;&#20813;&#20102;&#35299;&#20915;&#20219;&#20309;&#22797;&#26434;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NE&#30340;&#29305;&#23450;&#23454;&#20363;&#21644;&#38750;&#28176;&#36817;&#24615;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;NE&#23454;&#29616;&#20102;&#39640;&#38454;&#26368;&#22351;&#24773;&#20917;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#26368;&#21518;&#65292;&#26469;&#33258;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#30340;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of best-item identification from choice-based feedback. In this problem, a company sequentially and adaptively shows display sets to a population of customers and collects their choices. The objective is to identify the most preferred item with the least number of samples and at a high confidence level. We propose an elimination-based algorithm, namely Nested Elimination (NE), which is inspired by the nested structure implied by the information-theoretic lower bound. NE is simple in structure, easy to implement, and has a strong theoretical guarantee for sample complexity. Specifically, NE utilizes an innovative elimination criterion and circumvents the need to solve any complex combinatorial optimization problem. We provide an instance-specific and non-asymptotic bound on the expected sample complexity of NE. We also show NE achieves high-order worst-case asymptotic optimality. Finally, numerical experiments from both synthetic and real data corroborate our theore
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#26469;&#25913;&#36827;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#23545;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.04775</link><description>&lt;p&gt;
&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#25552;&#39640;&#30697;&#38453;&#34917;&#20840;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Exploiting Observation Bias to Improve Matrix Completion. (arXiv:2306.04775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#35266;&#27979;&#20559;&#24046;&#26469;&#25913;&#36827;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#19982;&#23545;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30340;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#21464;&#24418;&#30340;&#30697;&#38453;&#34917;&#20840;&#38382;&#39064;&#65292;&#20854;&#20013;&#36755;&#20837;&#25968;&#25454;&#20197;&#20559;&#24046;&#30340;&#26041;&#24335;&#21576;&#29616;&#65292;&#31867;&#20284;&#20110;Ma&#21644;Chen&#25152;&#24341;&#20837;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21033;&#29992;&#20559;&#24046;&#19982;&#24863;&#20852;&#36259;&#30340;&#32467;&#26524;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#26469;&#25913;&#36827;&#39044;&#27979;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#31639;&#27861;&#65306;&#65288;i&#65289;&#23558;&#35266;&#27979;&#27169;&#24335;&#35299;&#37322;&#20026;&#23436;&#20840;&#35266;&#27979;&#30340;&#22122;&#22768;&#30697;&#38453;&#65292;&#25105;&#20204;&#23545;&#35266;&#27979;&#27169;&#24335;&#24212;&#29992;&#20256;&#32479;&#30340;&#30697;&#38453;&#34917;&#20840;&#26041;&#27861;&#26469;&#20272;&#35745;&#28508;&#22312;&#22240;&#32032;&#20043;&#38388;&#30340;&#36317;&#31163;&#65307; (ii)&#25105;&#20204;&#23545;&#24674;&#22797;&#30340;&#29305;&#24449;&#24212;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#22635;&#34917;&#32570;&#22833;&#35266;&#23519;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#26377;&#38480;&#26679;&#26412;&#35823;&#24046;&#29575;&#65292;&#36825;&#20123;&#35823;&#24046;&#29575;&#19982;&#30456;&#24212;&#30340;&#30417;&#30563;&#23398;&#20064;&#21442;&#25968;&#29575;&#30456;&#31454;&#20105;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#23398;&#20064;&#24615;&#33021;&#19982;&#20351;&#29992;&#26410;&#35266;&#27979;&#21327;&#21464;&#37327;&#30456;&#24403;&#12290;&#23454;&#35777;&#35780;&#20272;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21453;&#26144;&#20102;&#31867;&#20284;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a variant of matrix completion where entries are revealed in a biased manner, adopting a model akin to that introduced by Ma and Chen. Instead of treating this observation bias as a disadvantage, as is typically the case, our goal is to exploit the shared information between the bias and the outcome of interest to improve predictions. Towards this, we propose a simple two-stage algorithm: (i) interpreting the observation pattern as a fully observed noisy matrix, we apply traditional matrix completion methods to the observation pattern to estimate the distances between the latent factors; (ii) we apply supervised learning on the recovered features to impute missing observations. We establish finite-sample error rates that are competitive with the corresponding supervised learning parametric rates, suggesting that our learning performance is comparable to having access to the unobserved covariates. Empirical evaluation using a real-world dataset reflects similar performance g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#29992;&#20110;&#31934;&#30830;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.18475</link><description>&lt;p&gt;
&#24207;&#21015;&#24314;&#27169;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340;&#36924;&#36817;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Approximation theory of transformer networks for sequence modeling. (arXiv:2305.18475v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#29992;&#20110;&#31934;&#30830;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#65292;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#65292;&#24182;&#35752;&#35770;&#20102;&#20854;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21387;&#22120;&#26159;&#24207;&#21015;&#24314;&#27169;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#26550;&#26500;&#65292;&#20294;&#20854;&#24037;&#20316;&#21407;&#29702;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21464;&#21387;&#22120;&#36924;&#36817;&#24207;&#21015;&#20851;&#31995;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#21464;&#21387;&#22120;&#20551;&#35774;&#31354;&#38388;&#30340;&#26222;&#36941;&#36924;&#36817;&#23450;&#29702;&#12290;&#36890;&#36807;&#25512;&#23548;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#26032;&#30340;&#35268;&#24459;&#27010;&#24565;&#65292;&#22312;&#27492;&#27010;&#24565;&#19979;&#65292;&#25105;&#20204;&#21487;&#20197;&#35777;&#26126;&#19968;&#20010;&#26126;&#30830;&#30340;&#36924;&#36817;&#36895;&#29575;&#20272;&#35745;&#12290;&#36825;&#20010;&#20272;&#35745;&#25581;&#31034;&#20102;&#21464;&#21387;&#22120;&#30340;&#20851;&#38190;&#32467;&#26500;&#29305;&#24615;&#65292;&#24182;&#26263;&#31034;&#20102;&#21464;&#21387;&#22120;&#36866;&#29992;&#20110;&#36924;&#36817;&#21738;&#20123;&#31867;&#22411;&#30340;&#24207;&#21015;&#20851;&#31995;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#20855;&#20307;&#22320;&#35752;&#35770;&#21464;&#21387;&#22120;&#19982;&#20256;&#32479;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65288;&#22914;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65289;&#20043;&#38388;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24471;&#21040;&#20102;&#25968;&#23383;&#23454;&#39564;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a widely applied architecture in sequence modeling applications, but the theoretical understanding of its working principles is limited. In this work, we investigate the ability of transformers to approximate sequential relationships. We first prove a universal approximation theorem for the transformer hypothesis space. From its derivation, we identify a novel notion of regularity under which we can prove an explicit approximation rate estimate. This estimate reveals key structural properties of the transformer and suggests the types of sequence relationships that the transformer is adapted to approximating. In particular, it allows us to concretely discuss the structural bias between the transformer and classical sequence modeling methods, such as recurrent neural networks. Our findings are supported by numerical experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.17330</link><description>&lt;p&gt;
MADiff&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;MADiff&#65292;&#35299;&#20915;&#20102;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#26159;&#31532;&#19968;&#20010;&#25193;&#25955;&#27169;&#22411;&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#26368;&#36817;&#22312;&#21253;&#25324;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#20869;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20854;&#20013;&#31574;&#30053;&#36890;&#36807;&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#20135;&#29983;&#36712;&#36857;&#26469;&#36827;&#34892;&#35268;&#21010;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21333;&#26234;&#33021;&#20307;&#23398;&#20064;&#26174;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#20294;&#20173;&#19981;&#28165;&#26970;DM&#22914;&#20309;&#22312;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#20013;&#25805;&#20316;&#65292;&#20854;&#20013;&#20195;&#29702;&#21830;&#24456;&#38590;&#22312;&#29420;&#31435;&#24314;&#27169;&#27599;&#20010;&#20195;&#29702;&#21830;&#36712;&#36857;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#22242;&#38431;&#21512;&#20316;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;MADiff&#65292;&#19968;&#31181;&#26032;&#30340;&#29983;&#25104;&#24335;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;MADiff&#26159;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#23545;&#22810;&#20010;&#25193;&#25955;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#22797;&#26434;&#21327;&#35843;&#24314;&#27169;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MADiff&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22810;&#26234;&#33021;&#20307;&#31163;&#32447;RL&#26694;&#26550;&#65292;&#23427;&#26082;&#21487;&#20197;&#34892;&#20026;&#20026;&#20998;&#25955;&#30340;&#25919;&#31574;&#65292;&#21448;&#21487;&#20197;&#20026;&#38598;&#20013;&#25511;&#21046;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#25163;&#24314;&#27169;&#65292;&#24182;&#21487;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#36712;&#36857;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion model (DM), as a powerful generative model, recently achieved huge success in various scenarios including offline reinforcement learning, where the policy learns to conduct planning by generating trajectory in the online evaluation. However, despite the effectiveness shown for single-agent learning, it remains unclear how DMs can operate in multi-agent problems, where agents can hardly complete teamwork without good coordination by independently modeling each agent's trajectories. In this paper, we propose MADiff, a novel generative multi-agent learning framework to tackle this problem. MADiff is realized with an attention-based diffusion model to model the complex coordination among behaviors of multiple diffusion agents. To the best of our knowledge, MADiff is the first diffusion-based multi-agent offline RL framework, which behaves as both a decentralized policy and a centralized controller, which includes opponent modeling and can be used for multi-agent trajectory predic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#25968;&#23383;&#29305;&#24449;&#32534;&#30721;&#20026;&#22522;&#20989;&#25968;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22240;&#23376;&#26426;&#20013;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22240;&#23376;&#26426;&#20013;&#65292;&#21487;&#20197;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14528</link><description>&lt;p&gt;
&#22522;&#20989;&#25968;&#32534;&#30721;&#25913;&#21892;&#22240;&#23376;&#26426;&#20013;&#25968;&#23383;&#29305;&#24449;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Basis Function Encoding of Numerical Features in Factorization Machines for Improved Accuracy. (arXiv:2305.14528v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#33021;&#22815;&#23558;&#25968;&#23383;&#29305;&#24449;&#32534;&#30721;&#20026;&#22522;&#20989;&#25968;&#21521;&#37327;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22240;&#23376;&#26426;&#20013;&#23558;&#35813;&#26041;&#27861;&#24212;&#29992;&#20110;&#22240;&#23376;&#26426;&#20013;&#65292;&#21487;&#20197;&#25913;&#21892;&#25512;&#33616;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#23376;&#26426;(FM)&#21464;&#20307;&#34987;&#24191;&#27867;&#29992;&#20110;&#22823;&#35268;&#27169;&#23454;&#26102;&#20869;&#23481;&#25512;&#33616;&#31995;&#32479;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#35757;&#32451;&#25512;&#29702;&#30340;&#20302;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#25552;&#20379;&#20102;&#20986;&#33394;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#31995;&#32479;&#12289;&#29702;&#35770;&#19978;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#20540;&#29305;&#24449;&#32534;&#30721;&#20026;&#25152;&#36873;&#20989;&#25968;&#38598;&#30340;&#20989;&#25968;&#20540;&#21521;&#37327;&#23558;&#25968;&#20540;&#29305;&#24449;&#32435;&#20837;FM&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
Factorization machine (FM) variants are widely used for large scale real-time content recommendation systems, since they offer an excellent balance between model accuracy and low computational costs for training and inference. These systems are trained on tabular data with both numerical and categorical columns. Incorporating numerical columns poses a challenge, and they are typically incorporated using a scalar transformation or binning, which can be either learned or chosen a-priori. In this work, we provide a systematic and theoretically-justified way to incorporate numerical features into FM variants by encoding them into a vector of function values for a set of functions of one's choice.  We view factorization machines as approximators of segmentized functions, namely, functions from a field's value to the real numbers, assuming the remaining fields are assigned some given constants, which we refer to as the segment. From this perspective, we show that our technique yields a model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20013;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#19981;&#38169;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.11586</link><description>&lt;p&gt;
&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#34701;&#20837;&#19981;&#30830;&#23450;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Bayesian approach to Gaussian process regression with uncertain inputs. (arXiv:2305.11586v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#25216;&#26415;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26041;&#27861;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#22238;&#24402;&#27169;&#22411;&#39044;&#27979;&#20013;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#20855;&#26377;&#26222;&#36866;&#24615;&#21644;&#19981;&#38169;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20165;&#20551;&#35774;&#27169;&#22411;&#35266;&#27979;&#25968;&#25454;&#30340;&#36755;&#20986;&#20855;&#26377;&#22122;&#22768;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#31185;&#23398;&#21644;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#30001;&#20110;&#24314;&#27169;&#20551;&#35774;&#12289;&#27979;&#37327;&#35823;&#24046;&#31561;&#22240;&#32032;&#65292;&#35266;&#27979;&#25968;&#25454;&#30340;&#36755;&#20837;&#20301;&#32622;&#21487;&#33021;&#20063;&#23384;&#22312;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#30340;&#21487;&#21464;&#24615;&#34701;&#20837;&#21040;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20013;&#12290;&#32771;&#34385;&#20004;&#31181;&#21487;&#35266;&#27979;&#37327;&#8212;&#8212;&#20855;&#26377;&#22266;&#23450;&#36755;&#20837;&#30340;&#22122;&#22768;&#27745;&#26579;&#36755;&#20986;&#21644;&#20855;&#26377;&#20808;&#39564;&#20998;&#24067;&#23450;&#20041;&#30340;&#19981;&#30830;&#23450;&#36755;&#20837;&#65292;&#36890;&#36807;&#36125;&#21494;&#26031;&#26694;&#26550;&#20272;&#35745;&#21518;&#39564;&#20998;&#24067;&#20197;&#25512;&#26029;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#20301;&#32622;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36793;&#38469;&#21270;&#26041;&#27861;&#23558;&#36825;&#20123;&#36755;&#20837;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#32435;&#20837;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#20013;&#12290;&#36890;&#36807;&#20960;&#20010;&#25968;&#20540;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#36825;&#31181;&#26032;&#22238;&#24402;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20854;&#20013;&#35266;&#23519;&#21040;&#19981;&#21516;&#27700;&#24179;&#36755;&#20837;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#26222;&#36866;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional Gaussian process regression exclusively assumes the existence of noise in the output data of model observations. In many scientific and engineering applications, however, the input locations of observational data may also be compromised with uncertainties owing to modeling assumptions, measurement errors, etc. In this work, we propose a Bayesian method that integrates the variability of input data into Gaussian process regression. Considering two types of observables -- noise-corrupted outputs with fixed inputs and those with prior-distribution-defined uncertain inputs, a posterior distribution is estimated via a Bayesian framework to infer the uncertain data locations. Thereafter, such quantified uncertainties of inputs are incorporated into Gaussian process predictions by means of marginalization. The effectiveness of this new regression technique is demonstrated through several numerical examples, in which a consistently good performance of generalization is observed, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#35299;&#20915;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25514;&#26045;&#19979;&#23454;&#29616;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#21033;&#28070;&#65292;&#24182;&#20026;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#12290;</title><link>http://arxiv.org/abs/2304.02022</link><description>&lt;p&gt;
&#22522;&#20110;MNL&#36873;&#25321;&#27169;&#22411;&#30340;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Online Joint Assortment-Inventory Optimization under MNL Choices. (arXiv:2304.02022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#35299;&#20915;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#24179;&#34913;&#25506;&#32034;&#19982;&#24320;&#21457;&#30340;&#25514;&#26045;&#19979;&#23454;&#29616;&#26368;&#22823;&#21270;&#39044;&#26399;&#24635;&#21033;&#28070;&#65292;&#24182;&#20026;&#35813;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#27599;&#20010;&#39038;&#23458;&#30340;&#36873;&#25321;&#34892;&#20026;&#37117;&#36981;&#24490;Multinomial Logit&#65288;MNL&#65289;&#36873;&#25321;&#27169;&#22411;&#65292;&#21560;&#24341;&#21147;&#21442;&#25968;&#26159;&#20808;&#39564;&#26410;&#30693;&#30340;&#12290;&#38646;&#21806;&#21830;&#36827;&#34892;&#21608;&#26399;&#24615;&#32452;&#21512;&#21644;&#24211;&#23384;&#20915;&#31574;&#65292;&#20197;&#21160;&#24577;&#22320;&#20174;&#23454;&#29616;&#30340;&#38656;&#27714;&#20013;&#23398;&#20064;&#21560;&#24341;&#21147;&#21442;&#25968;&#65292;&#21516;&#26102;&#22312;&#26102;&#38388;&#19978;&#26368;&#22823;&#21270;&#39044;&#26399;&#30340;&#24635;&#21033;&#28070;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24179;&#34913;&#32452;&#21512;&#21644;&#24211;&#23384;&#22312;&#32447;&#20915;&#31574;&#20013;&#30340;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24314;&#31435;&#22312;&#19968;&#20010;&#26032;&#30340;MNL&#21560;&#24341;&#21147;&#21442;&#25968;&#20272;&#35745;&#22120;&#65292;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#26576;&#20123;&#24050;&#30693;&#21644;&#26410;&#30693;&#21442;&#25968;&#26469;&#28608;&#21169;&#25506;&#32034;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#38745;&#24577;&#21333;&#21608;&#26399;&#32452;&#21512;&#24211;&#23384;&#35268;&#21010;&#38382;&#39064;&#30340;&#20248;&#21270;oracle&#22522;&#30784;&#20043;&#19978;&#12290;&#25105;&#20204;&#20026;&#25105;&#20204;&#30340;&#31639;&#27861;&#24314;&#31435;&#20102;&#36951;&#25022;&#19978;&#30028;&#65292;&#20197;&#21450;&#20851;&#20110;&#22312;&#32447;&#32852;&#21512;&#32452;&#21512;&#24211;&#23384;&#20248;&#21270;&#38382;&#39064;&#30340;&#19979;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study an online joint assortment-inventory optimization problem, in which we assume that the choice behavior of each customer follows the Multinomial Logit (MNL) choice model, and the attraction parameters are unknown a priori. The retailer makes periodic assortment and inventory decisions to dynamically learn from the realized demands about the attraction parameters while maximizing the expected total profit over time. In this paper, we propose a novel algorithm that can effectively balance the exploration and exploitation in the online decision-making of assortment and inventory. Our algorithm builds on a new estimator for the MNL attraction parameters, a novel approach to incentivize exploration by adaptively tuning certain known and unknown parameters, and an optimization oracle to static single-cycle assortment-inventory planning problems with given parameters. We establish a regret upper bound for our algorithm and a lower bound for the online joint assortment-inventory optimi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.16900</link><description>&lt;p&gt;
InceptionNeXt&#65306;&#24403;Inception&#36935;&#21040;ConvNeXt
&lt;/p&gt;
&lt;p&gt;
InceptionNeXt: When Inception Meets ConvNeXt. (arXiv:2303.16900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InceptionNeXt&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23558;&#22823;&#20869;&#26680;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#26469;&#25552;&#39640;&#27169;&#22411;&#25928;&#29575;&#65292;&#35299;&#20915;&#20102;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;ViTs&#38271;&#31243;&#24314;&#27169;&#33021;&#21147;&#30340;&#21551;&#21457;&#65292;&#36817;&#26399;&#24191;&#27867;&#30740;&#31350;&#21644;&#37319;&#29992;&#20102;&#22823;&#20869;&#26680;&#21367;&#31215;&#26469;&#25193;&#22823;&#24863;&#21463;&#37326;&#21644;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#20363;&#22914;ConvNeXt&#37319;&#29992;&#20102;7x7&#28145;&#24230;&#21367;&#31215;&#12290;&#34429;&#28982;&#36825;&#31181;&#28145;&#24230;&#25805;&#20316;&#20165;&#28040;&#32791;&#23569;&#37327;FLOPs&#65292;&#20294;&#30001;&#20110;&#39640;&#20869;&#23384;&#35775;&#38382;&#25104;&#26412;&#65292;&#36825;&#22312;&#21151;&#33021;&#24378;&#22823;&#30340;&#35745;&#31639;&#35774;&#22791;&#19978;&#22823;&#22823;&#25439;&#23475;&#20102;&#27169;&#22411;&#25928;&#29575;&#12290;&#23613;&#31649;&#32553;&#23567;ConvNeXt&#30340;&#20869;&#26680;&#22823;&#23567;&#33021;&#25552;&#39640;&#36895;&#24230;&#65292;&#20294;&#20250;&#23548;&#33268;&#24615;&#33021;&#26174;&#30528;&#19979;&#38477;&#12290;&#22914;&#20309;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#21152;&#24555;&#22522;&#20110;&#22823;&#20869;&#26680;&#30340;CNN&#27169;&#22411;&#20173;&#19981;&#28165;&#26970;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21463;Inceptions&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#22823;&#20869;&#26680;&#28145;&#24230;&#21367;&#31215;&#27839;&#36890;&#36947;&#32500;&#24230;&#20998;&#35299;&#20026;&#22235;&#20010;&#24179;&#34892;&#20998;&#25903;&#65292;&#21363;&#23567;&#26041;&#20869;&#26680;&#12289;&#20004;&#20010;&#27491;&#20132;&#24102;&#20869;&#26680;&#21644;&#19968;&#20010;&#20114;&#34917;&#20869;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the long-range modeling ability of ViTs, large-kernel convolutions are widely studied and adopted recently to enlarge the receptive field and improve model performance, like the remarkable work ConvNeXt which employs 7x7 depthwise convolution. Although such depthwise operator only consumes a few FLOPs, it largely harms the model efficiency on powerful computing devices due to the high memory access costs. For example, ConvNeXt-T has similar FLOPs with ResNet-50 but only achieves 60% throughputs when trained on A100 GPUs with full precision. Although reducing the kernel size of ConvNeXt can improve speed, it results in significant performance degradation. It is still unclear how to speed up large-kernel-based CNN models while preserving their performance. To tackle this issue, inspired by Inceptions, we propose to decompose large-kernel depthwise convolution into four parallel branches along channel dimension, i.e. small square kernel, two orthogonal band kernels, and an ide
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#30340;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#22312;&#27492;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;&#23545;&#25968;&#32423;&#21035;&#30340;&#36951;&#25022;&#12290;&#36825;&#26159;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#8220;&#36864;&#21270;&#8221;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#22312;&#20855;&#26377;&#36830;&#32493;&#20540;&#30340;NRM&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#32423;&#21035;&#36951;&#25022;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.07996</link><description>&lt;p&gt;
&#36864;&#21270;&#26159;&#21487;&#20197;&#25509;&#21463;&#30340;&#65306;&#24102;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#30340;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#20013;&#30340;&#23545;&#25968;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Degeneracy is OK: Logarithmic Regret for Network Revenue Management with Indiscrete Distributions. (arXiv:2210.07996v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07996
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#19981;&#36830;&#32493;&#20998;&#24067;&#30340;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#20854;&#22312;&#27492;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;&#23545;&#25968;&#32423;&#21035;&#30340;&#36951;&#25022;&#12290;&#36825;&#26159;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#8220;&#36864;&#21270;&#8221;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#39318;&#27425;&#22312;&#20855;&#26377;&#36830;&#32493;&#20540;&#30340;NRM&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#32423;&#21035;&#36951;&#25022;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#25509;&#21463;/&#25298;&#32477;&#20915;&#31574;&#21644;T&#27425;&#29420;&#31435;&#21516;&#20998;&#24067;&#21040;&#36798;&#30340;&#32463;&#20856;&#32593;&#32476;&#25910;&#20837;&#31649;&#29702;&#65288;NRM&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20998;&#24067;&#24418;&#24335;&#65292;&#27599;&#20010;&#21040;&#36798;&#24517;&#39035;&#23646;&#20110;&#26377;&#38480;&#25968;&#37327;&#30340;&#21487;&#33021;&#31867;&#21035;&#20043;&#19968;&#65292;&#27599;&#20010;&#31867;&#21035;&#20855;&#26377;&#30830;&#23450;&#30340;&#36164;&#28304;&#28040;&#32791;&#21521;&#37327;&#65292;&#20294;&#26159;&#19968;&#20010;&#22312;&#21306;&#38388;&#19978;&#36830;&#32493;&#20998;&#24067;&#30340;&#38543;&#26426;&#20540;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;&#32447;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#27492;&#27169;&#22411;&#19979;&#23454;&#29616;&#20102;O(log^2 T)&#30340;&#36951;&#25022;&#65292;&#21807;&#19968;&#65288;&#24517;&#35201;&#65289;&#30340;&#20551;&#35774;&#26159;&#27010;&#29575;&#23494;&#24230;&#36828;&#31163;0&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#31532;&#20108;&#20010;&#32467;&#26524;&#65292;&#22312;&#20108;&#38454;&#22686;&#38271;&#30340;&#39069;&#22806;&#20551;&#35774;&#19979;&#65292;&#23454;&#29616;&#20102;O(log T)&#30340;&#36951;&#25022;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#27809;&#26377;&#20219;&#20309;&#8220;&#36864;&#21270;&#8221;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#20855;&#26377;&#36830;&#32493;&#20540;&#30340;NRM&#27169;&#22411;&#20013;&#23454;&#29616;&#23545;&#25968;&#32423;&#21035;&#36951;&#25022;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#36890;&#36807;&#21253;&#25324;&#19968;&#31181;&#26032;&#30340;&#36793;&#30028;mypopic regret&#65292;&#31163;&#32447;&#20998;&#37197;&#30340;&#8220;&#21322;&#27969;&#20307;&#8221;&#25918;&#26494;&#20197;&#21450;&#25913;&#36827;&#36793;&#30028;&#30340;&#26032;&#25216;&#26415;&#23454;&#29616;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the classical Network Revenue Management (NRM) problem with accept/reject decisions and $T$ IID arrivals. We consider a distributional form where each arrival must fall under a finite number of possible categories, each with a deterministic resource consumption vector, but a random value distributed continuously over an interval. We develop an online algorithm that achieves $O(\log^2 T)$ regret under this model, with the only (necessary) assumption being that the probability densities are bounded away from 0. We derive a second result that achieves $O(\log T)$ regret under an additional assumption of second-order growth. To our knowledge, these are the first results achieving logarithmic-level regret in an NRM model with continuous values that do not require any kind of ``non-degeneracy'' assumptions. Our results are achieved via new techniques including a new method of bounding myopic regret, a ``semi-fluid'' relaxation of the offline allocation, and an improved bound on the 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#22270;&#27880;&#24847;&#24490;&#29615;&#32593;&#32476;&#30340;&#27963;&#21160;&#24863;&#30693;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#22270;&#21644;&#20351;&#29992;&#23618;&#27425;&#22270;&#27880;&#24847;&#27169;&#22359;&#26469;&#25429;&#25417;&#26102;&#38388;-&#27963;&#21160;-&#20301;&#32622;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21382;&#21490;&#22686;&#24378;&#32622;&#20449;&#26631;&#31614;&#65292;&#29992;&#20110;&#32858;&#28966;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#20307;&#32423;&#20559;&#22909;&#12290;</title><link>http://arxiv.org/abs/2210.07765</link><description>&lt;p&gt;
&#20855;&#26377;&#23618;&#27425;&#22270;&#27880;&#24847;&#21147;&#24490;&#29615;&#32593;&#32476;&#30340;&#27963;&#21160;&#24863;&#30693;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Activity-aware Human Mobility Prediction with Hierarchical Graph Attention Recurrent Network. (arXiv:2210.07765v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07765
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#22270;&#27880;&#24847;&#24490;&#29615;&#32593;&#32476;&#30340;&#27963;&#21160;&#24863;&#30693;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23618;&#27425;&#22270;&#21644;&#20351;&#29992;&#23618;&#27425;&#22270;&#27880;&#24847;&#27169;&#22359;&#26469;&#25429;&#25417;&#26102;&#38388;-&#27963;&#21160;-&#20301;&#32622;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#30340;&#20559;&#22909;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21382;&#21490;&#22686;&#24378;&#32622;&#20449;&#26631;&#31614;&#65292;&#29992;&#20110;&#32858;&#28966;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#20307;&#32423;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26159;&#19968;&#39033;&#22522;&#30784;&#20219;&#21153;&#65292;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#12289;&#22522;&#20110;&#20301;&#32622;&#30340;&#26381;&#21153;&#21644;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#31561;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23545;&#34892;&#20026;&#20449;&#24687;&#30340;&#32771;&#34385;&#65292;&#36825;&#26159;&#25512;&#29702;&#20154;&#31867;&#20559;&#22909;&#21644;&#20363;&#34892;&#27963;&#21160;&#30340;&#20851;&#38190;&#65292;&#25110;&#32773;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#26102;&#38388;&#12289;&#27963;&#21160;&#21644;&#20301;&#32622;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#34920;&#31034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23618;&#27425;&#22270;&#27880;&#24847;&#24490;&#29615;&#32593;&#32476;&#65288;HGARN&#65289;&#30340;&#20154;&#31867;&#31227;&#21160;&#39044;&#27979;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22522;&#20110;&#25152;&#26377;&#29992;&#25143;&#30340;&#21382;&#21490;&#31227;&#21160;&#35760;&#24405;&#26500;&#24314;&#20102;&#19968;&#20010;&#23618;&#27425;&#22270;&#65292;&#24182;&#20351;&#29992;&#23618;&#27425;&#22270;&#27880;&#24847;&#27169;&#22359;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#38388;-&#27963;&#21160;-&#20301;&#32622;&#20381;&#36182;&#20851;&#31995;&#12290;&#36825;&#26679;&#65292;HGARN&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#20016;&#23500;&#30340;&#20154;&#31867;&#20986;&#34892;&#35821;&#20041;&#30340;&#34920;&#31034;&#65292;&#20197;&#24314;&#27169;&#29992;&#25143;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#21382;&#21490;&#22686;&#24378;&#32622;&#20449;&#65288;MAHEC&#65289;&#26631;&#31614;&#65292;&#20197;&#20415;&#23558;&#25105;&#20204;&#30340;&#27169;&#22411;&#32858;&#28966;&#20110;&#27599;&#20010;&#29992;&#25143;&#30340;&#20010;&#20307;&#32423;&#20559;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26102;&#38388;&#27169;&#22359;...
&lt;/p&gt;
&lt;p&gt;
Human mobility prediction is a fundamental task essential for various applications, including urban planning, location-based services and intelligent transportation systems. Existing methods often ignore activity information crucial for reasoning human preferences and routines, or adopt a simplified representation of the dependencies between time, activities and locations. To address these issues, we present Hierarchical Graph Attention Recurrent Network (HGARN) for human mobility prediction. Specifically, we construct a hierarchical graph based on all users' history mobility records and employ a Hierarchical Graph Attention Module to capture complex time-activity-location dependencies. This way, HGARN can learn representations with rich human travel semantics to model user preferences at the global level. We also propose a model-agnostic history-enhanced confidence (MAHEC) label to focus our model on each user's individual-level preferences. Finally, we introduce a Temporal Module, wh
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2208.04957</link><description>&lt;p&gt;
&#24322;&#26500;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#36827;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Multi-agent Zero-Shot Coordination by Coevolution. (arXiv:2208.04957v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#30340;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#22810;&#26234;&#33021;&#20307;&#38646;&#26679;&#26412;&#21327;&#21516;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#23545;&#20110;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#29983;&#25104;&#33021;&#22815;&#19982;&#26410;&#30693;&#21512;&#20316;&#20249;&#20276;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#26234;&#33021;&#20307;&#26159;&#19968;&#20010;&#26032;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#22312;&#38646;&#26679;&#26412;&#21327;&#21516;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#26234;&#33021;&#20307;&#26292;&#38706;&#22810;&#26679;&#21270;&#30340;&#21512;&#20316;&#20249;&#20276;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;&#35757;&#32451;&#20249;&#20276;&#26102;&#28041;&#21450;&#33258;&#25105;&#23545;&#24328;&#65292;&#38544;&#24335;&#22320;&#20551;&#35774;&#20219;&#21153;&#26159;&#21516;&#36136;&#30340;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#30340;&#20219;&#21153;&#26159;&#24322;&#26500;&#30340;&#65292;&#22240;&#27492;&#20808;&#21069;&#30340;&#26041;&#27861;&#21487;&#33021;&#25928;&#29575;&#20302;&#19979;&#12290;&#26412;&#25991;&#39318;&#27425;&#30740;&#31350;&#20102;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21327;&#21516;&#36827;&#21270;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#19977;&#20010;&#23376;&#36807;&#31243;&#65306;&#37197;&#23545;&#12289;&#26356;&#26032;&#21644;&#36873;&#25321;&#65292;&#23545;&#20004;&#20010;&#26234;&#33021;&#20307;&#21644;&#21512;&#20316;&#20249;&#20276;&#36827;&#34892;&#21327;&#21516;&#36827;&#21270;&#12290;&#23545;&#19981;&#21516;&#24322;&#26500;&#20219;&#21153;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#20986;&#20102;&#32771;&#34385;&#24322;&#26500;&#24773;&#20917;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#35299;&#20915;&#24322;&#26500;&#38646;&#26679;&#26412;&#21327;&#21516;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating agents that can achieve zero-shot coordination (ZSC) with unseen partners is a new challenge in cooperative multi-agent reinforcement learning (MARL). Recently, some studies have made progress in ZSC by exposing the agents to diverse partners during the training process. They usually involve self-play when training the partners, implicitly assuming that the tasks are homogeneous. However, many real-world tasks are heterogeneous, and hence previous methods may be inefficient. In this paper, we study the heterogeneous ZSC problem for the first time and propose a general method based on coevolution, which coevolves two populations of agents and partners through three sub-processes: pairing, updating and selection. Experimental results on various heterogeneous tasks highlight the necessity of considering the heterogeneous setting and demonstrate that our proposed method is a promising solution for heterogeneous ZSC tasks.
&lt;/p&gt;</description></item></channel></rss>