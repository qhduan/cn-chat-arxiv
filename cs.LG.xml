<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#35821;&#20041;&#19968;&#33268;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#20016;&#23500;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#39640;&#20102;&#23398;&#21040;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.05966</link><description>&lt;p&gt;
&#33021;&#29983;&#25104;&#27169;&#22411;&#25913;&#36827;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Generative Models Improve Self-Supervised Representation Learning?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#35821;&#20041;&#19968;&#33268;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#20016;&#23500;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25552;&#39640;&#20102;&#23398;&#21040;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#24555;&#36895;&#21457;&#23637;&#31361;&#26174;&#20102;&#20854;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#23398;&#20064;&#24378;&#22823;&#35270;&#35273;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#21033;&#29992;&#21516;&#19968;&#22270;&#20687;&#30340;&#19981;&#21516;&#35270;&#22270;&#30340;&#26041;&#27861;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#38480;&#21046;&#20102;&#21464;&#25442;&#30340;&#22810;&#26679;&#24615;&#21644;&#36136;&#37327;&#65292;&#23548;&#33268;&#34920;&#31034;&#19981;&#22815;&#20248;&#21270;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#20135;&#29983;&#35821;&#20041;&#19968;&#33268;&#30340;&#22270;&#20687;&#22686;&#24378;&#65292;&#20016;&#23500;&#20102;&#33258;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#12290;&#36890;&#36807;&#30452;&#25509;&#22312;&#28304;&#22270;&#20687;&#34920;&#31034;&#19978;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#22686;&#24378;&#65292;&#21516;&#26102;&#20445;&#25345;&#28304;&#22270;&#20687;&#30340;&#35821;&#20041;&#65292;&#20026;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#20379;&#26356;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;&#23398;&#21040;&#30340;&#35270;&#35273;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05966v1 Announce Type: cross  Abstract: The rapid advancement in self-supervised learning (SSL) has highlighted its potential to leverage unlabeled data for learning powerful visual representations. However, existing SSL approaches, particularly those employing different views of the same image, often rely on a limited set of predefined data augmentations. This constrains the diversity and quality of transformations, which leads to sub-optimal representations. In this paper, we introduce a novel framework that enriches the SSL paradigm by utilizing generative models to produce semantically consistent image augmentations. By directly conditioning generative models on a source image representation, our method enables the generation of diverse augmentations while maintaining the semantics of the source image, thus offering a richer set of data for self-supervised learning. Our experimental results demonstrate that our framework significantly enhances the quality of learned visu
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#31354;&#38388;&#21152;&#26435;&#26041;&#26696;&#12289;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#23574;&#31471;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03328</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21487;&#35299;&#37322;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38598;&#25104;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
An Ensemble Framework for Explainable Geospatial Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03328
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#31354;&#38388;&#21152;&#26435;&#26041;&#26696;&#12289;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#23574;&#31471;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#25552;&#39640;&#22320;&#29702;&#31354;&#38388;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#31354;&#38388;&#21464;&#21270;&#25928;&#24212;&#22312;&#22320;&#29702;&#20998;&#26512;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22320;&#29702;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#38750;&#32447;&#24615;&#65292;&#20934;&#30830;&#25429;&#25417;&#21644;&#35299;&#37322;&#36825;&#31181;&#21464;&#24322;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#38598;&#25104;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#31354;&#38388;&#21152;&#26435;&#26041;&#26696;&#12289;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#21644;&#23574;&#31471;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20197;&#24357;&#21512;&#20256;&#32479;&#22320;&#29702;&#20998;&#26512;&#27169;&#22411;&#21644;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#35813;&#26694;&#26550;&#36890;&#36807;&#38416;&#26126;&#31354;&#38388;&#21464;&#24322;&#24615;&#65292;&#25552;&#39640;&#20102;&#22320;&#29702;&#22238;&#24402;&#21644;&#20998;&#31867;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#39044;&#27979;&#31934;&#24230;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#29702;&#35299;&#31354;&#38388;&#29616;&#35937;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03328v1 Announce Type: new  Abstract: Analyzing spatial varying effect is pivotal in geographic analysis. Yet, accurately capturing and interpreting this variability is challenging due to the complexity and non-linearity of geospatial data. Herein, we introduce an integrated framework that merges local spatial weighting scheme, Explainable Artificial Intelligence (XAI), and cutting-edge machine learning technologies to bridge the gap between traditional geographic analysis models and general machine learning approaches. Through tests on synthetic datasets, this framework is verified to enhance the interpretability and accuracy of predictions in both geographic regression and classification by elucidating spatial variability. It significantly boosts prediction precision, offering a novel approach to understanding spatial phenomena.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02780</link><description>&lt;p&gt;
&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data Collaboration Analysis Over Matrix Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#25913;&#36827;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#24535;&#30528;&#20248;&#36234;&#30340;&#36136;&#37327;&#65292;&#22686;&#24378;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#20197;&#20943;&#36731;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#21644;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24191;&#27867;&#30340;&#25968;&#25454;&#34701;&#21512;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#36947;&#24503;&#20851;&#20999;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#29992;&#25143;&#38544;&#31169;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#25259;&#38706;&#39118;&#38505;&#12290;&#24050;&#24314;&#31435;&#20102;&#21508;&#31181;&#20840;&#29699;&#31435;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#27861;&#35268;&#23545;&#20445;&#25252;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;ML&#25216;&#26415;&#30340;&#23454;&#38469;&#37096;&#32626;&#21464;&#24471;&#22797;&#26434;&#12290;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;(PPML)&#36890;&#36807;&#20445;&#25252;&#20174;&#20581;&#24247;&#35760;&#24405;&#21040;&#22320;&#29702;&#20301;&#32622;&#25968;&#25454;&#31561;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#23454;&#29616;&#23433;&#20840;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02780v1 Announce Type: new  Abstract: The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#36880;&#27493;&#20171;&#32461;&#23454;&#29616;&#31616;&#21333;&#33258;&#21160;&#24494;&#20998;&#31995;&#32479;&#65292;&#22635;&#34917;&#20102;&#25945;&#23398;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#31616;&#21270;&#20102;&#25968;&#23398;&#27010;&#24565;&#21644;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.16020</link><description>&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#23454;&#29616;&#30340;&#20998;&#27493;&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
A Step-by-step Introduction to the Implementation of Automatic Differentiation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16020
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#36880;&#27493;&#20171;&#32461;&#23454;&#29616;&#31616;&#21333;&#33258;&#21160;&#24494;&#20998;&#31995;&#32479;&#65292;&#22635;&#34917;&#20102;&#25945;&#23398;&#20013;&#30340;&#31354;&#30333;&#65292;&#24182;&#31616;&#21270;&#20102;&#25968;&#23398;&#27010;&#24565;&#21644;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#24494;&#20998;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#36825;&#20010;&#20027;&#39064;&#24471;&#21040;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#20248;&#31168;&#30340;&#35843;&#26597;&#65288;&#22914;Baydin&#31561;&#65292;2018&#24180;&#65289;&#24050;&#32463;&#21487;&#20197;&#28165;&#26224;&#22320;&#25551;&#36848;&#22522;&#26412;&#27010;&#24565;&#12290;&#27492;&#22806;&#65292;&#33258;&#21160;&#24494;&#20998;&#30340;&#22797;&#26434;&#23454;&#29616;&#29616;&#22312;&#26159;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#30452;&#25509;&#25945;&#25480;&#23398;&#29983;&#29616;&#26377;&#31995;&#32479;&#30340;&#23454;&#29616;&#26159;&#22256;&#38590;&#30340;&#65292;&#22914;&#26524;&#19981;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#22240;&#20026;&#23427;&#22826;&#22797;&#26434;&#20102;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#25945;&#23398;&#27490;&#27493;&#20110;&#22522;&#26412;&#27010;&#24565;&#65292;&#23398;&#29983;&#23558;&#26080;&#27861;&#24863;&#21463;&#21040;&#23454;&#29616;&#30340;&#23454;&#29616;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#22312;&#25945;&#23398;&#33258;&#21160;&#24494;&#20998;&#26102;&#32463;&#24120;&#25552;&#21450;&#35745;&#31639;&#22270;&#65292;&#20294;&#23398;&#29983;&#20204;&#24819;&#30693;&#36947;&#22914;&#20309;&#23454;&#29616;&#21644;&#20351;&#29992;&#23427;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36880;&#27493;&#20171;&#32461;&#23454;&#29616;&#31616;&#21333;&#33258;&#21160;&#24494;&#20998;&#31995;&#32479;&#26469;&#22635;&#34917;&#37096;&#20998;&#31354;&#30333;&#12290;&#25105;&#20204;&#31616;&#21270;&#20102;&#25968;&#23398;&#27010;&#24565;&#21644;&#23454;&#29616;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16020v1 Announce Type: new  Abstract: Automatic differentiation is a key component in deep learning. This topic is well studied and excellent surveys such as Baydin et al. (2018) have been available to clearly describe the basic concepts. Further, sophisticated implementations of automatic differentiation are now an important part of popular deep learning frameworks. However, it is difficult, if not impossible, to directly teach students the implementation of existing systems due to the complexity. On the other hand, if the teaching stops at the basic concept, students fail to sense the realization of an implementation. For example, we often mention the computational graph in teaching automatic differentiation, but students wonder how to implement and use it. In this document, we partially fill the gap by giving a step by step introduction of implementing a simple automatic differentiation system. We streamline the mathematical concepts and the implementation. Further, we gi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LlamaIT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.15061</link><description>&lt;p&gt;
&#38024;&#23545;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models for Domain-specific Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15061
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LlamaIT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#29992;&#20110;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39046;&#22495;&#29305;&#23450;&#26426;&#22120;&#32763;&#35793;&#20013;&#36935;&#21040;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#39046;&#22495;&#29305;&#23450;MT&#20013;&#30340;&#28508;&#21147;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#24403;&#21069;&#22522;&#20110;LLMs&#30340;MT&#31995;&#32479;&#20173;&#28982;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LlamaIT&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#39640;&#25928;&#22320;&#20026;&#39046;&#22495;&#29305;&#23450;MT&#20219;&#21153;&#24494;&#35843;&#36890;&#29992;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15061v1 Announce Type: new  Abstract: Large language models (LLMs) have made significant progress in machine translation (MT). However, their potential in domain-specific MT remains under-explored. Current LLM-based MT systems still face several challenges. First, for LLMs with in-context learning, their effectiveness is highly sensitive to input translation examples, and processing them can increase inference costs. They often require extra post-processing due to over-generation. Second, LLMs with fine-tuning on domain-specific data often require high training costs for domain adaptation, and may weaken the zero-shot MT capabilities of LLMs due to over-specialization. The aforementioned methods can struggle to translate rare words in domain transfer scenarios. To address these challenges, this paper proposes a prompt-oriented fine-tuning method, denoted as LlamaIT, to effectively and efficiently fine-tune a general-purpose LLM for domain-specific MT tasks. First, we constru
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;</title><link>https://arxiv.org/abs/2402.14948</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#36828;&#31243;&#30417;&#30563;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65306;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#21644;&#31616;&#21333;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Re-Examine Distantly Supervised NER: A New Benchmark and a Simple Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14948
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064; CuPUL &#26041;&#27861;&#65292;&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#22312;&#36828;&#31243;&#30417;&#30563;&#65288;DS-NER&#65289;&#26694;&#26550;&#19979;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#26631;&#31614;&#36136;&#37327;&#21463;&#21040;&#35823;&#24046;&#30340;&#24433;&#21709;&#65292;&#22914;&#20551;&#38451;&#24615;&#12289;&#20551;&#38452;&#24615;&#21644;&#27491;&#21521;&#31867;&#22411;&#38169;&#35823;&#12290;&#25105;&#20204;&#25209;&#21028;&#24615;&#22320;&#35780;&#20272;&#20102;&#24403;&#21069;DS-NER&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#21517;&#20026;QTL&#30340;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#23427;&#20204;&#30340;&#24615;&#33021;&#24448;&#24448;&#19981;&#31526;&#21512;&#39044;&#26399;&#12290;&#20026;&#20102;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#26222;&#36941;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#22522;&#20110;&#35838;&#31243;&#30340;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#65288;CuPUL&#65289;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#31574;&#30053;&#24615;&#22320;&#20174;&#8220;&#26131;&#8221;&#21644;&#26356;&#28165;&#27905;&#30340;&#26679;&#26412;&#24320;&#22987;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#22024;&#26434;&#26679;&#26412;&#30340;&#38887;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#31361;&#20986;&#20102;CuPUL&#20943;&#23569;&#22024;&#26434;&#26631;&#31614;&#24433;&#21709;&#24182;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14948v1 Announce Type: new  Abstract: This paper delves into Named Entity Recognition (NER) under the framework of Distant Supervision (DS-NER), where the main challenge lies in the compromised quality of labels due to inherent errors such as false positives, false negatives, and positive type errors. We critically assess the efficacy of current DS-NER methodologies using a real-world benchmark dataset named QTL, revealing that their performance often does not meet expectations. To tackle the prevalent issue of label noise, we introduce a simple yet effective approach, Curriculum-based Positive-Unlabeled Learning CuPUL, which strategically starts on "easy" and cleaner samples during the training process to enhance model resilience to noisy samples. Our empirical results highlight the capability of CuPUL to significantly reduce the impact of noisy labels and outperform existing methods.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.02196</link><description>&lt;p&gt;
&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#26679;&#26412;&#39640;&#25928;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient Clustering and Conquer Procedures for Parallel Large-Scale Ranking and Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02196
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#30340;&#32858;&#31867;&#21450;&#24449;&#26381;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#22312;&#22823;&#35268;&#27169;AI&#24212;&#29992;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;"&#32858;&#31867;&#21644;&#24449;&#26381;"&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#24182;&#34892;&#22823;&#35268;&#27169;&#25490;&#24207;&#21644;&#36873;&#25321;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#32858;&#31867;&#65292;&#20197;&#25171;&#30772;&#26679;&#26412;&#25928;&#29575;&#30340;&#29942;&#39048;&#12290;&#22312;&#24182;&#34892;&#35745;&#31639;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;&#30456;&#20851;&#24615;&#30340;&#32858;&#31867;&#21487;&#20197;&#23454;&#29616;O(p)&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20943;&#23569;&#36895;&#24230;&#65292;&#36825;&#26159;&#29702;&#35770;&#19978;&#21487;&#36798;&#21040;&#30340;&#26368;&#20339;&#20943;&#23569;&#36895;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26159;&#36890;&#29992;&#30340;&#65292;&#22312;&#22266;&#23450;&#39044;&#31639;&#21644;&#22266;&#23450;&#31934;&#24230;&#30340;&#33539;&#24335;&#19979;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21508;&#31181;&#24120;&#35265;&#30340;&#25490;&#24207;&#21644;&#36873;&#25321;&#26041;&#27861;&#12290;&#23427;&#21487;&#20197;&#22312;&#26080;&#38656;&#39640;&#31934;&#30830;&#24230;&#30456;&#20851;&#20272;&#35745;&#21644;&#31934;&#30830;&#32858;&#31867;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#25913;&#36827;&#12290;&#22312;&#22823;&#35268;&#27169;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#65292;&#22914;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#30340;&#26080;&#31579;&#36873;&#29256;&#26412;&#30340;&#26041;&#27861;&#24778;&#20154;&#22320;&#36229;&#36807;&#20102;&#23436;&#20840;&#39034;&#24207;&#21270;&#30340;&#22522;&#20934;&#65292;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#34920;&#26126;&#21033;&#29992;&#26377;&#20215;&#20540;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#22914;&#30456;&#20851;&#24615;&#65292;&#26159;&#32469;&#36807;&#20256;&#32479;&#26041;&#27861;&#30340;&#19968;&#26465;&#21487;&#34892;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose novel "clustering and conquer" procedures for the parallel large-scale ranking and selection (R&amp;S) problem, which leverage correlation information for clustering to break the bottleneck of sample efficiency. In parallel computing environments, correlation-based clustering can achieve an $\mathcal{O}(p)$ sample complexity reduction rate, which is the optimal reduction rate theoretically attainable. Our proposed framework is versatile, allowing for seamless integration of various prevalent R&amp;S methods under both fixed-budget and fixed-precision paradigms. It can achieve improvements without the necessity of highly accurate correlation estimation and precise clustering. In large-scale AI applications such as neural architecture search, a screening-free version of our procedure surprisingly surpasses fully-sequential benchmarks in terms of sample efficiency. This suggests that leveraging valuable structural information, such as correlation, is a viable path to bypassing the trad
&lt;/p&gt;</description></item><item><title>SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2401.11791</link><description>&lt;p&gt;
SemPLeS: &#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11791
&lt;/p&gt;
&lt;p&gt;
SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26088;&#22312;&#21033;&#29992;&#20165;&#20855;&#26377;&#22270;&#20687;&#32423;&#30417;&#30563;&#30340;&#22270;&#20687;&#25968;&#25454;&#26469;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#12290;&#30001;&#20110;&#26080;&#27861;&#33719;&#24471;&#31934;&#30830;&#30340;&#20687;&#32032;&#32423;&#26631;&#27880;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#36890;&#36807;&#20248;&#21270;CAM&#26679;&#24335;&#30340;&#28909;&#22270;&#26469;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#20266;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#28909;&#22270;&#21487;&#33021;&#20165;&#25429;&#33719;&#23545;&#35937;&#31867;&#21035;&#30340;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#22270;&#20687;&#21306;&#22495;&#25110;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;WSSS&#30340;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#65288;SemPLeS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#31034;CLIP&#28508;&#31354;&#38388;&#20197;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#21644;&#25552;&#31034;&#24341;&#23548;&#30340;&#35821;&#20041;&#32454;&#21270;&#65292;&#20197;&#23398;&#20064;&#36866;&#24403;&#25551;&#36848;&#21644;&#25233;&#21046;&#19982;&#27599;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11791v2 Announce Type: replace-cross  Abstract: Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each target object category. In thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#27969;&#34892;&#24230;&#25351;&#26631;&#25552;&#20986;&#36136;&#30097;&#65292;&#35748;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#32676;&#32452;&#20869;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23548;&#33268;&#30340;&#39044;&#27979;&#26631;&#31614;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2401.13391</link><description>&lt;p&gt;
&#36229;&#36234;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#65306;&#20572;&#27490;&#20165;&#26681;&#25454;&#32676;&#32452;&#38388;&#25351;&#26631;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Beyond Accuracy-Fairness: Stop evaluating bias mitigation methods solely on between-group metrics. (arXiv:2401.13391v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#27969;&#34892;&#24230;&#25351;&#26631;&#25552;&#20986;&#36136;&#30097;&#65292;&#35748;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#32676;&#32452;&#20869;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23548;&#33268;&#30340;&#39044;&#27979;&#26631;&#31614;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21508;&#20010;&#39046;&#22495;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#65292;&#24341;&#21457;&#20102;&#23545;&#20854;&#20844;&#24179;&#24615;&#30340;&#20851;&#27880;&#12290;&#34429;&#28982;&#20844;&#24179;&#24615;&#22312;&#20154;&#24037;&#26234;&#33021;&#20013;&#20173;&#28982;&#26159;&#19968;&#20010;&#26680;&#24515;&#20851;&#27880;&#28857;&#65292;&#20294;&#30446;&#21069;&#30340;&#35752;&#35770;&#24448;&#24448;&#24378;&#35843;&#22522;&#20110;&#32467;&#26524;&#30340;&#25351;&#26631;&#65292;&#32780;&#27809;&#26377;&#23545;&#23376;&#32676;&#20307;&#20013;&#30340;&#24046;&#24322;&#24433;&#21709;&#36827;&#34892;&#32454;&#33268;&#32771;&#34385;&#12290;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#19981;&#20165;&#24433;&#21709;&#25935;&#24863;&#32676;&#32452;&#20043;&#38388;&#30340;&#23454;&#20363;&#25490;&#24207;&#65292;&#32780;&#19988;&#36890;&#24120;&#36824;&#26174;&#33879;&#24433;&#21709;&#36825;&#20123;&#32676;&#32452;&#20869;&#23454;&#20363;&#30340;&#25490;&#24207;&#12290;&#36825;&#20123;&#21464;&#21270;&#38590;&#20197;&#35299;&#37322;&#65292;&#24182;&#24341;&#36215;&#20102;&#23545;&#24178;&#39044;&#30340;&#26377;&#25928;&#24615;&#30340;&#25285;&#24551;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#25928;&#24212;&#22312;&#36890;&#24120;&#24212;&#29992;&#30340;&#20934;&#30830;&#24615;-&#20844;&#24179;&#24615;&#35780;&#20272;&#26694;&#26550;&#20013;&#24448;&#24448;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#23545;&#35780;&#20272;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#30340;&#27969;&#34892;&#24230;&#25351;&#26631;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#35748;&#20026;&#23427;&#20204;&#27809;&#26377;&#32771;&#34385;&#21040;&#32676;&#32452;&#20869;&#30340;&#21464;&#21270;&#65292;&#24182;&#19988;&#23548;&#33268;&#30340;&#39044;&#27979;&#26631;&#31614;&#19981;&#33021;&#23436;&#20840;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) finds widespread applications across various domains, sparking concerns about fairness in its deployment. While fairness in AI remains a central concern, the prevailing discourse often emphasizes outcome-based metrics without a nuanced consideration of the differential impacts within subgroups. Bias mitigation techniques do not only affect the ranking of pairs of instances across sensitive groups, but often also significantly affect the ranking of instances within these groups. Such changes are hard to explain and raise concerns regarding the validity of the intervention. Unfortunately, these effects largely remain under the radar in the accuracy-fairness evaluation framework that is usually applied. This paper challenges the prevailing metrics for assessing bias mitigation techniques, arguing that they do not take into account the changes within-groups and that the resulting prediction labels fall short of reflecting real-world scenarios. We propose a para
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#24314;&#35758;&#25351;&#21335;&#12290;</title><link>http://arxiv.org/abs/2401.10304</link><description>&lt;p&gt;
&#35770;&#31185;&#23398;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20844;&#27491;&#21644;&#36879;&#26126;&#20351;&#29992;&#30340;&#20934;&#22791;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
On the Readiness of Scientific Data for a Fair and Transparent Use in Machine Learning. (arXiv:2401.10304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#22871;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#21644;&#21487;&#20449;&#24615;&#65292;&#26368;&#36817;&#30340;&#31435;&#27861;&#20030;&#25514;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#30456;&#20851;&#30740;&#31350;&#25351;&#20986;&#38656;&#35201;&#35760;&#24405;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#23454;&#29616;&#21487;&#37325;&#22797;&#24615;&#65292;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#20849;&#20139;&#23454;&#36341;&#36817;&#24180;&#26469;&#20063;&#26377;&#20102;&#21457;&#23637;&#12290;&#22312;&#36825;&#20010;&#24847;&#20041;&#19978;&#65292;&#23398;&#26415;&#26426;&#26500;&#37319;&#29992;&#20102;&#36825;&#20123;&#23454;&#36341;&#65292;&#40723;&#21169;&#30740;&#31350;&#20154;&#21592;&#23558;&#20182;&#20204;&#30340;&#25968;&#25454;&#21644;&#25216;&#26415;&#25991;&#20214;&#21457;&#24067;&#22312;&#21516;&#34892;&#35780;&#35758;&#30340;&#20986;&#29256;&#29289;&#19978;&#65292;&#22914;&#25968;&#25454;&#35770;&#25991;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#31185;&#23398;&#25968;&#25454;&#25991;&#26723;&#22914;&#20309;&#28385;&#36275;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#21644;&#30417;&#31649;&#26426;&#26500;&#23545;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#20013;&#20351;&#29992;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#23545;4041&#31687;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#35770;&#25991;&#26679;&#26412;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#35780;&#20272;&#20854;&#23436;&#25972;&#24615;&#21644;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#30740;&#31350;&#20102;&#36817;&#24180;&#26469;&#30340;&#36235;&#21183;&#65292;&#29305;&#21035;&#20851;&#27880;&#20102;&#26368;&#22810;&#21644;&#26368;&#23569;&#34987;&#35760;&#24405;&#30340;&#26041;&#38754;&#12290;&#20316;&#20026;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#22871;&#25968;&#25454;&#21019;&#24314;&#32773;&#30340;&#24314;&#35758;&#25351;&#21335;&#12290;
&lt;/p&gt;
&lt;p&gt;
To ensure the fairness and trustworthiness of machine learning (ML) systems, recent legislative initiatives and relevant research in the ML community have pointed out the need to document the data used to train ML models. Besides, data-sharing practices in many scientific domains have evolved in recent years for reproducibility purposes. In this sense, the adoption of these practices by academic institutions has encouraged researchers to publish their data and technical documentation in peer-reviewed publications such as data papers. In this study, we analyze how this scientific data documentation meets the needs of the ML community and regulatory bodies for its use in ML technologies. We examine a sample of 4041 data papers of different domains, assessing their completeness and coverage of the requested dimensions, and trends in recent years, putting special emphasis on the most and least documented dimensions. As a result, we propose a set of recommendation guidelines for data creato
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.09615</link><description>&lt;p&gt;
&#23398;&#20064;&#25463;&#24452;&#65306;&#20851;&#20110;&#35821;&#35328;&#27169;&#22411;&#20013;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35823;&#23548;&#24615;&#25215;&#35834;&#30340;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Learning Shortcuts: On the Misleading Promise of NLU in Language Models. (arXiv:2401.09615v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09615
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#29616;&#35937;&#65292;&#24378;&#35843;&#20102;&#36825;&#31181;&#29616;&#35937;&#23545;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#24433;&#21709;&#65292;&#24182;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#30740;&#31350;&#21147;&#24230;&#20197;&#25552;&#21319;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#22312;&#25191;&#34892;&#20219;&#21153;&#26102;&#24120;&#24120;&#37319;&#29992;&#25463;&#24452;&#65292;&#23548;&#33268;&#22312;&#20915;&#31574;&#35268;&#21017;&#19978;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24615;&#33021;&#19978;&#20135;&#29983;&#20102;&#19968;&#31181;&#38169;&#35273;&#12290;&#36825;&#19968;&#29616;&#35937;&#22312;&#20934;&#30830;&#35780;&#20272;LLMs&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#19978;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#30340;&#30456;&#20851;&#30740;&#31350;&#36827;&#34892;&#20102;&#31616;&#27905;&#30340;&#27010;&#36848;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#20351;&#29992;&#25463;&#24452;&#23398;&#20064;&#30340;&#24433;&#21709;&#30340;&#35266;&#28857;&#12290;&#26412;&#25991;&#21628;&#21505;&#21152;&#22823;&#23545;&#25463;&#24452;&#23398;&#20064;&#30340;&#28145;&#20837;&#29702;&#35299;&#30340;&#30740;&#31350;&#21147;&#24230;&#65292;&#20026;&#24320;&#21457;&#26356;&#24378;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#39640;&#30495;&#23454;&#22330;&#26223;&#19979;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#35780;&#20272;&#30340;&#26631;&#20934;&#20316;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
The advent of large language models (LLMs) has enabled significant performance gains in the field of natural language processing. However, recent studies have found that LLMs often resort to shortcuts when performing tasks, creating an illusion of enhanced performance while lacking generalizability in their decision rules. This phenomenon introduces challenges in accurately assessing natural language understanding in LLMs. Our paper provides a concise survey of relevant research in this area and puts forth a perspective on the implications of shortcut learning in the evaluation of language models, specifically for NLU tasks. This paper urges more research efforts to be put towards deepening our comprehension of shortcut learning, contributing to the development of more robust language models, and raising the standards of NLU evaluation in real-world scenarios.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04979</link><description>&lt;p&gt;
&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Invertible Solution of Neural Differential Equations for Analysis of Irregularly-Sampled Time Series. (arXiv:2401.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04979
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#36870;&#35299;&#20915;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#31070;&#32463;&#27969;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26082;&#20445;&#35777;&#20102;&#21487;&#36870;&#24615;&#21448;&#38477;&#20302;&#20102;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22788;&#29702;&#38750;&#35268;&#21017;&#21644;&#19981;&#23436;&#25972;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#65288;NDE&#65289;&#30340;&#21487;&#36870;&#35299;&#20915;&#26041;&#26696;&#12290;&#34429;&#28982;&#22522;&#20110;NDE&#30340;&#26041;&#27861;&#26159;&#20998;&#26512;&#38750;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#31181;&#24378;&#22823;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#19981;&#33021;&#20445;&#35777;&#22312;&#20854;&#26631;&#20934;&#24418;&#24335;&#19979;&#36827;&#34892;&#21487;&#36870;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24314;&#35758;&#20351;&#29992;&#20855;&#26377;&#31070;&#32463;&#27969;&#30340;&#31070;&#32463;&#25511;&#21046;&#24494;&#20998;&#26041;&#31243;&#65288;Neural CDEs&#65289;&#30340;&#21464;&#31181;&#65292;&#35813;&#26041;&#27861;&#22312;&#20445;&#25345;&#36739;&#20302;&#30340;&#35745;&#31639;&#36127;&#25285;&#30340;&#21516;&#26102;&#30830;&#20445;&#20102;&#21487;&#36870;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#35757;&#32451;&#21452;&#37325;&#28508;&#22312;&#31354;&#38388;&#65292;&#22686;&#24378;&#20102;&#23545;&#21160;&#24577;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#26694;&#26550;&#65292;&#22312;&#20998;&#31867;&#21644;&#25554;&#20540;&#20219;&#21153;&#20013;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#22686;&#24378;&#22411;&#21452;&#37325;&#28508;&#22312;&#29366;&#24577;&#26550;&#26500;&#65292;&#29992;&#20110;&#22312;&#21508;&#31181;&#26102;&#38388;&#24207;&#21015;&#20219;&#21153;&#20013;&#25552;&#39640;&#31934;&#24230;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
To handle the complexities of irregular and incomplete time series data, we propose an invertible solution of Neural Differential Equations (NDE)-based method. While NDE-based methods are a powerful method for analyzing irregularly-sampled time series, they typically do not guarantee reversible transformations in their standard form. Our method suggests the variation of Neural Controlled Differential Equations (Neural CDEs) with Neural Flow, which ensures invertibility while maintaining a lower computational burden. Additionally, it enables the training of a dual latent space, enhancing the modeling of dynamic temporal dynamics. Our research presents an advanced framework that excels in both classification and interpolation tasks. At the core of our approach is an enhanced dual latent states architecture, carefully designed for high precision across various time series tasks. Empirical analysis demonstrates that our method significantly outperforms existing models. This work significan
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#22235;&#31181;&#36755;&#20837;&#27169;&#24577;&#30340;&#22788;&#29702;&#21644;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#21644;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2311.00721</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#25110;&#29983;&#29702;&#20449;&#21495;&#19978;&#36827;&#34892;&#20849;&#24773;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Empathy Detection Using Machine Learning on Text, Audiovisual, Audio or Physiological Signals. (arXiv:2311.00721v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#22235;&#31181;&#36755;&#20837;&#27169;&#24577;&#30340;&#22788;&#29702;&#21644;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#21644;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#19968;&#20010;&#31038;&#20132;&#25216;&#33021;&#65292;&#34920;&#26126;&#19968;&#20010;&#20010;&#20307;&#29702;&#35299;&#20182;&#20154;&#30340;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20849;&#24773;&#24341;&#36215;&#20102;&#21253;&#25324;&#24773;&#24863;&#35745;&#31639;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#24515;&#29702;&#23398;&#22312;&#20869;&#30340;&#21508;&#20010;&#23398;&#31185;&#30340;&#20851;&#27880;&#12290;&#20849;&#24773;&#26159;&#19968;&#20010;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#26415;&#35821;&#65292;&#22240;&#27492;&#26816;&#27979;&#25110;&#35782;&#21035;&#20849;&#24773;&#22312;&#31038;&#20250;&#12289;&#21307;&#30103;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#28041;&#21450;&#33539;&#22260;&#24191;&#27867;&#19988;&#26377;&#37325;&#21472;&#65292;&#20294;&#20174;&#25972;&#20307;&#25991;&#29486;&#35282;&#24230;&#26469;&#30475;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20849;&#24773;&#26816;&#27979;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#25910;&#38598;&#21644;&#31579;&#36873;&#20102;&#26469;&#33258;10&#20010;&#30693;&#21517;&#25968;&#25454;&#24211;&#30340;801&#31687;&#35770;&#25991;&#65292;&#24182;&#20998;&#26512;&#20102;&#36873;&#23450;&#30340;54&#31687;&#35770;&#25991;&#12290;&#25105;&#20204;&#26681;&#25454;&#20849;&#24773;&#26816;&#27979;&#31995;&#32479;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#21363;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#65292;&#23545;&#35770;&#25991;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#20998;&#21035;&#30740;&#31350;&#20102;&#29305;&#23450;&#27169;&#24577;&#30340;&#39044;&#22788;&#29702;&#21644;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#21327;&#35758;&#12289;&#24120;&#35265;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#21644;&#21487;&#29992;&#24615;&#35814;&#24773;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is a social skill that indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science and Psychology. Empathy is a context-dependent term; thus, detecting or recognising empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection studies leveraging Machine Learning remains underexplored from a holistic literature perspective. To this end, we systematically collect and screen 801 papers from 10 well-known databases and analyse the selected 54 papers. We group the papers based on input modalities of empathy detection systems, i.e., text, audiovisual, audio and physiological signals. We examine modality-specific pre-processing and network architecture design protocols, popular dataset descriptions and availability details, and evaluation protocols. We fur
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;</title><link>http://arxiv.org/abs/2309.14780</link><description>&lt;p&gt;
&#36716;&#31227;&#27668;&#20505;&#21464;&#21270;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Transferring climate change knowledge. (arXiv:2309.14780v1 [physics.ao-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14780
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#30740;&#31350;&#34920;&#26126;&#26426;&#22120;&#23398;&#20064;&#65292;&#23588;&#20854;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#30340;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#27668;&#20505;&#39044;&#27979;&#23545;&#20110;&#27668;&#20505;&#36866;&#24212;&#21644;&#20943;&#32531;&#33267;&#20851;&#37325;&#35201;&#12290;&#29992;&#20110;&#39044;&#27979;&#27668;&#20505;&#21464;&#21270;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#22312;&#23545;&#23567;&#23610;&#24230;&#29289;&#29702;&#36807;&#31243;&#65288;&#20363;&#22914;&#20113;&#65289;&#30340;&#34920;&#31034;&#20013;&#26412;&#36136;&#19978;&#36827;&#34892;&#20102;&#36817;&#20284;&#65292;&#36825;&#26159;&#20840;&#29699;&#24179;&#22343;&#28201;&#24230;&#23545;&#22686;&#21152;&#30340;&#28201;&#23460;&#27668;&#20307;&#27987;&#24230;&#30340;&#21709;&#24212;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26681;&#28304;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#21382;&#21490;&#35266;&#27979;&#32422;&#26463;&#26410;&#26469;&#39044;&#27979;&#65292;&#24182;&#20943;&#23569;&#27668;&#20505;&#39044;&#27979;&#21644;&#27668;&#20505;&#21453;&#39304;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25429;&#25417;&#27668;&#20505;&#31995;&#32479;&#22266;&#26377;&#30340;&#38750;&#32447;&#24615;&#22797;&#26434;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#36716;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#29992;&#20110;&#26368;&#22823;&#31243;&#24230;&#22320;&#21033;&#29992;&#21644;&#25972;&#21512;&#20174;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#21644;&#21382;&#21490;&#35266;&#27979;&#20013;&#33719;&#24471;&#30340;&#30693;&#35782;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;21&#19990;&#32426;&#20840;&#29699;&#34920;&#38754;&#28201;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate climate projections are required for climate adaptation and mitigation. Earth system model simulations, used to project climate change, inherently make approximations in their representation of small-scale physical processes, such as clouds, that are at the root of the uncertainties in global mean temperature's response to increased greenhouse gas concentrations. Several approaches have been developed to use historical observations to constrain future projections and reduce uncertainties in climate projections and climate feedbacks. Yet those methods cannot capture the non-linear complexity inherent in the climate system. Using a Transfer Learning approach, we show that Machine Learning, in particular Deep Neural Networks, can be used to optimally leverage and merge the knowledge gained from Earth system model simulations and historical observations to more accurately project global surface temperature fields in the 21st century. For the Shared Socioeconomic Pathways (SSPs) 2-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#21644;&#32858;&#31867;&#31639;&#27861;&#30340;&#20248;&#21270;Mapper&#22270;&#35206;&#30422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#35206;&#30422;&#36873;&#25321;&#29983;&#25104;&#20102;&#20445;&#30041;&#25968;&#25454;&#38598;&#26412;&#36136;&#30340;Mapper&#22270;&#12290;</title><link>http://arxiv.org/abs/2309.06634</link><description>&lt;p&gt;
$G$-Mapper&#65306;&#23398;&#20064;Mapper&#26500;&#36896;&#20013;&#30340;&#35206;&#30422;
&lt;/p&gt;
&lt;p&gt;
$G$-Mapper: Learning a Cover in the Mapper Construction. (arXiv:2309.06634v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#32479;&#35745;&#26816;&#39564;&#21644;&#32858;&#31867;&#31639;&#27861;&#30340;&#20248;&#21270;Mapper&#22270;&#35206;&#30422;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#21106;&#35206;&#30422;&#36873;&#25321;&#29983;&#25104;&#20102;&#20445;&#30041;&#25968;&#25454;&#38598;&#26412;&#36136;&#30340;Mapper&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Mapper&#31639;&#27861;&#26159;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;(TDA)&#20013;&#19968;&#31181;&#21453;&#26144;&#32473;&#23450;&#25968;&#25454;&#38598;&#32467;&#26500;&#30340;&#21487;&#35270;&#21270;&#25216;&#26415;&#12290;Mapper&#31639;&#27861;&#38656;&#35201;&#35843;&#25972;&#22810;&#20010;&#21442;&#25968;&#20197;&#29983;&#25104;&#19968;&#20010;"&#22909;&#30475;&#30340;"Mapper&#22270;&#12290;&#35813;&#35770;&#25991;&#20851;&#27880;&#20110;&#36873;&#25321;&#35206;&#30422;&#21442;&#25968;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26681;&#25454;&#27491;&#24577;&#24615;&#30340;&#32479;&#35745;&#26816;&#39564;&#21453;&#22797;&#20998;&#21106;&#35206;&#30422;&#26469;&#20248;&#21270;Mapper&#22270;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#22522;&#20110;$G$-means&#32858;&#31867;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#36827;&#34892;Anderson-Darling&#26816;&#39564;&#26469;&#23547;&#25214;$k$-means&#20013;&#26368;&#20339;&#30340;&#31751;&#25968;&#12290;&#25105;&#20204;&#30340;&#20998;&#21106;&#36807;&#31243;&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#26681;&#25454;&#32473;&#23450;&#25968;&#25454;&#30340;&#20998;&#24067;&#31934;&#24515;&#36873;&#25321;&#35206;&#30422;&#12290;&#23545;&#20110;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#29983;&#25104;&#30340;&#35206;&#30422;&#20351;Mapper&#22270;&#20445;&#30041;&#20102;&#25968;&#25454;&#38598;&#30340;&#26412;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Mapper algorithm is a visualization technique in topological data analysis (TDA) that outputs a graph reflecting the structure of a given dataset. The Mapper algorithm requires tuning several parameters in order to generate a "nice" Mapper graph. The paper focuses on selecting the cover parameter. We present an algorithm that optimizes the cover of a Mapper graph by splitting a cover repeatedly according to a statistical test for normality. Our algorithm is based on $G$-means clustering which searches for the optimal number of clusters in $k$-means by conducting iteratively the Anderson-Darling test. Our splitting procedure employs a Gaussian mixture model in order to choose carefully the cover based on the distribution of a given data. Experiments for synthetic and real-world datasets demonstrate that our algorithm generates covers so that the Mapper graphs retain the essence of the datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RANL&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;Hessian&#21021;&#22987;&#21270;&#21644;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#21306;&#22495;&#20998;&#37197;&#65292;&#20811;&#26381;&#20102;&#29275;&#39039;&#27861;&#22312;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.10154</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#23398;&#20064;&#20013;&#30340;&#36164;&#28304;&#33258;&#36866;&#24212;&#29275;&#39039;&#27861;
&lt;/p&gt;
&lt;p&gt;
Resource-Adaptive Newton's Method for Distributed Learning. (arXiv:2308.10154v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10154
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RANL&#30340;&#26032;&#39062;&#21644;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#30340;Hessian&#21021;&#22987;&#21270;&#21644;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#21306;&#22495;&#20998;&#37197;&#65292;&#20811;&#26381;&#20102;&#29275;&#39039;&#27861;&#22312;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#24182;&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#26354;&#29575;&#20449;&#24687;&#25552;&#20379;&#20102;&#27604;&#19968;&#38454;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#35268;&#27169;&#21644;&#24322;&#26500;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;&#29275;&#39039;&#26041;&#27861;&#30340;&#23454;&#38469;&#36866;&#29992;&#24615;&#21463;&#21040;&#20102;&#35832;&#22810;&#25361;&#25112;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#19982;Hessian&#30697;&#38453;&#30456;&#20851;&#30340;&#39640;&#35745;&#31639;&#21644;&#36890;&#20449;&#25104;&#26412;&#12289;&#23376;&#27169;&#22411;&#22810;&#26679;&#24615;&#12289;&#35757;&#32451;&#30340;&#36807;&#26102;&#24615;&#21644;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;RANL&#30340;&#26032;&#39062;&#39640;&#25928;&#31639;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#31616;&#21333;&#30340;Hessian&#21021;&#22987;&#21270;&#21644;&#33258;&#36866;&#24212;&#30340;&#35757;&#32451;&#21306;&#22495;&#20998;&#37197;&#26469;&#20811;&#26381;&#29275;&#39039;&#26041;&#27861;&#30340;&#38480;&#21046;&#12290;&#35813;&#31639;&#27861;&#34920;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#22312;&#38543;&#26426;&#20248;&#21270;&#30340;&#26631;&#20934;&#20551;&#35774;&#19979;&#36827;&#34892;&#20102;&#20005;&#26684;&#20998;&#26512;&#12290;&#29702;&#35770;&#20998;&#26512;&#35777;&#26126;&#20102;RANL&#23454;&#29616;&#20102;&#32447;&#24615;&#25910;&#25947;&#29575;&#65292;&#21516;&#26102;&#26377;&#25928;&#22320;&#36866;&#24212;&#20102;&#21487;&#29992;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed stochastic optimization methods based on Newton's method offer significant advantages over first-order methods by leveraging curvature information for improved performance. However, the practical applicability of Newton's method is hindered in large-scale and heterogeneous learning environments due to challenges such as high computation and communication costs associated with the Hessian matrix, sub-model diversity, staleness in training, and data heterogeneity. To address these challenges, this paper introduces a novel and efficient algorithm called RANL, which overcomes the limitations of Newton's method by employing a simple Hessian initialization and adaptive assignments of training regions. The algorithm demonstrates impressive convergence properties, which are rigorously analyzed under standard assumptions in stochastic optimization. The theoretical analysis establishes that RANL achieves a linear convergence rate while effectively adapting to available resources and 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21360;&#35937;&#25968;&#25454;&#28304;&#25552;&#21319;&#25512;&#33616;&#36136;&#37327;&#65292;&#36890;&#36807;&#32508;&#36848;&#20998;&#31867;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.07857</link><description>&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Impression-Aware Recommender Systems. (arXiv:2308.07857v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07857
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#21033;&#29992;&#21360;&#35937;&#25968;&#25454;&#28304;&#25552;&#21319;&#25512;&#33616;&#36136;&#37327;&#65292;&#36890;&#36807;&#32508;&#36848;&#20998;&#31867;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#65292;&#25581;&#31034;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#22411;&#25968;&#25454;&#28304;&#20026;&#25913;&#36827;&#25512;&#33616;&#31995;&#32479;&#30340;&#36136;&#37327;&#24102;&#26469;&#20102;&#26032;&#30340;&#26426;&#36935;&#12290;&#21360;&#35937;&#26159;&#19968;&#31181;&#21253;&#21547;&#36807;&#21435;&#25512;&#33616;&#65288;&#23637;&#31034;&#30340;&#39033;&#30446;&#65289;&#21644;&#20256;&#32479;&#20114;&#21160;&#30340;&#26032;&#22411;&#25968;&#25454;&#28304;&#12290;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;&#21360;&#35937;&#26469;&#20248;&#21270;&#29992;&#25143;&#20559;&#22909;&#24182;&#20811;&#26381;&#24403;&#21069;&#25512;&#33616;&#31995;&#32479;&#30740;&#31350;&#20013;&#30340;&#38480;&#21046;&#12290;&#21360;&#35937;&#30340;&#30456;&#20851;&#24615;&#21644;&#20852;&#36259;&#24230;&#36880;&#24180;&#22686;&#21152;&#65292;&#22240;&#27492;&#38656;&#35201;&#23545;&#36825;&#31867;&#25512;&#33616;&#31995;&#32479;&#20013;&#30456;&#20851;&#24037;&#20316;&#36827;&#34892;&#32508;&#36848;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31687;&#20851;&#20110;&#20351;&#29992;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#20391;&#37325;&#20110;&#30740;&#31350;&#20013;&#30340;&#19977;&#20010;&#22522;&#26412;&#26041;&#38754;&#65306;&#25512;&#33616;&#31995;&#32479;&#12289;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#23545;&#20351;&#29992;&#21360;&#35937;&#30340;&#25512;&#33616;&#31995;&#32479;&#30340;&#35770;&#25991;&#36827;&#34892;&#20102;&#19977;&#20010;&#20998;&#31867;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#27599;&#31687;&#32508;&#36848;&#35770;&#25991;&#65292;&#25551;&#36848;&#20102;&#20855;&#26377;&#21360;&#35937;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20540;&#24471;&#20851;&#27880;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#24378;&#35843;&#20102;&#25991;&#29486;&#20013;&#32570;&#22833;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Novel data sources bring new opportunities to improve the quality of recommender systems. Impressions are a novel data source containing past recommendations (shown items) and traditional interactions. Researchers may use impressions to refine user preferences and overcome the current limitations in recommender systems research. The relevance and interest of impressions have increased over the years; hence, the need for a review of relevant work on this type of recommenders. We present a systematic literature review on recommender systems using impressions, focusing on three fundamental angles in research: recommenders, datasets, and evaluation methodologies. We provide three categorizations of papers describing recommenders using impressions, present each reviewed paper in detail, describe datasets with impressions, and analyze the existing evaluation methodologies. Lastly, we present open questions and future directions of interest, highlighting aspects missing in the literature that
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#35745;&#31639;&#23646;&#24615;&#23545;&#30340;&#21345;&#26041;&#32479;&#35745;&#37327;&#30340;&#21644;&#24471;&#21040;&#19968;&#20010;&#35299;&#26512;&#30340;p&#20540;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.07346</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;
&lt;/p&gt;
&lt;p&gt;
A testing-based approach to assess the clusterability of categorical data. (arXiv:2307.07346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07346
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#36890;&#36807;&#35745;&#31639;&#23646;&#24615;&#23545;&#30340;&#21345;&#26041;&#32479;&#35745;&#37327;&#30340;&#21644;&#24471;&#21040;&#19968;&#20010;&#35299;&#26512;&#30340;p&#20540;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;&#32467;&#26524;&#34920;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#24615;&#35780;&#20272;&#30340;&#30446;&#26631;&#26159;&#26816;&#26597;&#25968;&#25454;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;&#32858;&#31867;&#32467;&#26500;&#12290;&#20316;&#20026;&#32858;&#31867;&#20998;&#26512;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#24120;&#24120;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#24212;&#29992;&#20219;&#20309;&#32858;&#31867;&#31639;&#27861;&#20043;&#21069;&#37117;&#38656;&#35201;&#36827;&#34892;&#36825;&#26679;&#30340;&#26816;&#39564;&#12290;&#22914;&#26524;&#19968;&#20010;&#25968;&#25454;&#38598;&#19981;&#21487;&#32858;&#31867;&#65292;&#21017;&#20219;&#20309;&#21518;&#32493;&#30340;&#32858;&#31867;&#20998;&#26512;&#37117;&#19981;&#20250;&#20135;&#29983;&#26377;&#25928;&#32467;&#26524;&#12290;&#23613;&#31649;&#23427;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#29616;&#26377;&#30740;&#31350;&#20013;&#30340;&#22823;&#37096;&#20998;&#20851;&#27880;&#25968;&#20540;&#25968;&#25454;&#65292;&#23558;&#23545;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;&#35780;&#20272;&#38382;&#39064;&#30041;&#32473;&#25104;&#20026;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TestCat&#65292;&#19968;&#31181;&#22522;&#20110;&#27979;&#35797;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#20998;&#31867;&#25968;&#25454;&#30340;&#32858;&#31867;&#24615;&#65292;&#20351;&#29992;&#35299;&#26512;&#30340;p&#20540;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#12290;TestCat&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#21487;&#32858;&#31867;&#30340;&#20998;&#31867;&#25968;&#25454;&#20855;&#26377;&#35768;&#22810;&#24378;&#30456;&#20851;&#30340;&#23646;&#24615;&#23545;&#65292;&#22240;&#27492;&#23558;&#25152;&#26377;&#23646;&#24615;&#23545;&#30340;&#21345;&#26041;&#32479;&#35745;&#37327;&#30340;&#21644;&#20316;&#20026;p&#20540;&#35745;&#31639;&#30340;&#27979;&#35797;&#32479;&#35745;&#37327;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#32452;&#22522;&#20934;&#20998;&#31867;&#25968;&#25454;&#38598;&#65292;&#32467;&#26524;&#34920;&#26126;TestCat&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of clusterability evaluation is to check whether a clustering structure exists within the data set. As a crucial yet often-overlooked issue in cluster analysis, it is essential to conduct such a test before applying any clustering algorithm. If a data set is unclusterable, any subsequent clustering analysis would not yield valid results. Despite its importance, the majority of existing studies focus on numerical data, leaving the clusterability evaluation issue for categorical data as an open problem. Here we present TestCat, a testing-based approach to assess the clusterability of categorical data in terms of an analytical $p$-value. The key idea underlying TestCat is that clusterable categorical data possess many strongly correlated attribute pairs and hence the sum of chi-squared statistics of all attribute pairs is employed as the test statistic for $p$-value calculation. We apply our method to a set of benchmark categorical data sets, showing that TestCat outperforms
&lt;/p&gt;</description></item><item><title>OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2306.09301</link><description>&lt;p&gt;
OpenOOD v1.5&#65306;&#22686;&#24378;&#30340;OCC&#65288;Out-of-Distribution Detection&#65289;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection. (arXiv:2306.09301v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09301
&lt;/p&gt;
&lt;p&gt;
OpenOOD v1.5 &#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#23558;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
OCC&#26816;&#27979;&#23545;&#20110;&#24320;&#25918;&#19990;&#30028;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#38752;&#36816;&#34892;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#20986;&#29616;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;OCC&#26816;&#27979;&#26041;&#27861;&#65292;&#20294;&#35780;&#20272;&#19981;&#19968;&#33268;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#38590;&#20197;&#36319;&#36394;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OpenOOD v1.5&#65292;&#36825;&#26159;&#23545;&#21069;&#36523;&#30340;&#37325;&#22823;&#25913;&#36827;&#65292;&#30830;&#20445;OCC&#26816;&#27979;&#26041;&#27861;&#30340;&#20934;&#30830;&#12289;&#26631;&#20934;&#21270;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#35780;&#20272;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;OpenOOD v1.5&#23558;&#20854;&#35780;&#20272;&#33021;&#21147;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#22914;ImageNet&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#35843;&#26597;&#20102;&#20840;&#20809;&#35889;OCC&#26816;&#27979;&#65292;&#24341;&#20837;&#20102;&#22312;&#32447;&#25490;&#34892;&#27036;&#21644;&#26131;&#20110;&#20351;&#29992;&#30340;&#35780;&#20272;&#22120;&#31561;&#26032;&#21151;&#33021;&#12290;&#35813;&#24037;&#20316;&#36824;&#25552;&#20379;&#20102;&#28145;&#20837;&#30340;&#20998;&#26512;&#21644;&#32508;&#21512;&#23454;&#39564;&#32467;&#26524;&#30340;&#35265;&#35299;&#65292;&#20174;&#32780;&#20016;&#23500;&#20102;&#30693;&#35782;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution (OOD) detection is critical for the reliable operation of open-world intelligent systems. Despite the emergence of an increasing number of OOD detection methods, the evaluation inconsistencies present challenges for tracking the progress in this field. OpenOOD v1 initiated the unification of the OOD detection evaluation but faced limitations in scalability and usability. In response, this paper presents OpenOOD v1.5, a significant improvement from its predecessor that ensures accurate, standardized, and user-friendly evaluation of OOD detection methodologies. Notably, OpenOOD v1.5 extends its evaluation capabilities to large-scale datasets such as ImageNet, investigates full-spectrum OOD detection which is important yet underexplored, and introduces new features including an online leaderboard and an easy-to-use evaluator. This work also contributes in-depth analysis and insights derived from comprehensive experimental results, thereby enriching the knowledge pool o
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;PROVEXPLAINER&#26694;&#26550;&#65292;&#36890;&#36807;&#22797;&#21046;GNN-based security models&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21033;&#29992;&#20915;&#31574;&#26641;&#21644;&#22270;&#32467;&#26500;&#29305;&#24449;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;GNN&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#35810;&#38382;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00934</link><description>&lt;p&gt;
&#22522;&#20110;&#26435;&#23041;&#22270;&#32467;&#26500;&#29305;&#24449;&#23545;&#22522;&#20110;GNN&#30340;IDS&#26816;&#27979;&#36827;&#34892;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features. (arXiv:2306.00934v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00934
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;PROVEXPLAINER&#26694;&#26550;&#65292;&#36890;&#36807;&#22797;&#21046;GNN-based security models&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21033;&#29992;&#20915;&#31574;&#26641;&#21644;&#22270;&#32467;&#26500;&#29305;&#24449;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;GNN&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#35810;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#26412;&#36136;&#22952;&#30861;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#36923;&#36753;&#35299;&#37322;&#21644;&#21487;&#25191;&#34892;&#21518;&#32493;&#34892;&#21160;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#31995;&#32479;&#26469;&#28304;&#20998;&#26512;&#20013;&#20351;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROVEXPLAINER&#65292;&#19968;&#31181;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#29305;&#24449;&#31354;&#38388;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#31616;&#21333;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22914;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#65292;&#22797;&#21046;&#22522;&#20110;GNN&#30340;&#23433;&#20840;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#22270;&#35770;&#30340;&#22270;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23433;&#20840;&#39046;&#22495;&#30693;&#35782;&#30340;&#24191;&#27867;&#25968;&#25454;&#30740;&#31350;&#23545;&#20854;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#22270;&#32467;&#26500;&#29305;&#24449;&#19982;&#31995;&#32479;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#38382;&#39064;&#31354;&#38388;&#34892;&#21160;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#20351;&#26816;&#27979;&#32467;&#26524;&#21487;&#29992;&#20154;&#31867;&#35821;&#35328;&#25551;&#36848;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces.  We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human languag
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20840;&#23616;&#25928;&#24212;&#24191;&#20041;&#21487;&#21152;&#20998;&#35299;&#65288;GADGET&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#26368;&#23567;&#21270;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26412;&#22320;&#29305;&#24449;&#25928;&#24212;&#30340;&#20132;&#20114;&#24322;&#36136;&#24615;&#12290;&#21516;&#26102;&#36866;&#29992;&#20110;&#20559;&#20381;&#36182;&#12289;&#31215;&#32047;&#23616;&#37096;&#25928;&#24212;&#21644;Shapley&#21487;&#21152;&#35299;&#37322;&#65288;SHAP&#65289;&#20381;&#36182;&#30340;&#36793;&#38469;&#29305;&#24449;&#25928;&#24212;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#20132;&#20114;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#30528;&#30340;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.00541</link><description>&lt;p&gt;
&#22522;&#20110;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#36827;&#34892;&#20840;&#23616;&#29305;&#24449;&#25928;&#24212;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Decomposing Global Feature Effects Based on Feature Interactions. (arXiv:2306.00541v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00541
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20840;&#23616;&#25928;&#24212;&#24191;&#20041;&#21487;&#21152;&#20998;&#35299;&#65288;GADGET&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#26368;&#23567;&#21270;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26412;&#22320;&#29305;&#24449;&#25928;&#24212;&#30340;&#20132;&#20114;&#24322;&#36136;&#24615;&#12290;&#21516;&#26102;&#36866;&#29992;&#20110;&#20559;&#20381;&#36182;&#12289;&#31215;&#32047;&#23616;&#37096;&#25928;&#24212;&#21644;Shapley&#21487;&#21152;&#35299;&#37322;&#65288;SHAP&#65289;&#20381;&#36182;&#30340;&#36793;&#38469;&#29305;&#24449;&#25928;&#24212;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#20132;&#20114;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#30528;&#30340;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#23616;&#29305;&#24449;&#25928;&#24212;&#26041;&#27861;&#65292;&#22914;&#20559;&#20381;&#36182;&#22270;&#65292;&#25552;&#20379;&#20102;&#39044;&#26399;&#36793;&#38469;&#29305;&#24449;&#25928;&#24212;&#30340;&#21487;&#29702;&#35299;&#30340;&#21487;&#35270;&#21270;&#12290;&#20294;&#26159;&#65292;&#24403;&#23384;&#22312;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#26102;&#65292;&#36825;&#31181;&#20840;&#23616;&#29305;&#24449;&#25928;&#24212;&#26041;&#27861;&#21487;&#33021;&#20250;&#35823;&#23548;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#21333;&#20010;&#35266;&#27979;&#30340;&#23616;&#37096;&#29305;&#24449;&#25928;&#24212;&#12290;&#25105;&#20204;&#27491;&#24335;&#20171;&#32461;&#20102;&#22522;&#20110;&#36882;&#24402;&#20998;&#21306;&#30340;&#20840;&#23616;&#25928;&#24212;&#24191;&#20041;&#21487;&#21152;&#20998;&#35299;&#65288;GADGET&#65289;&#26694;&#26550;&#65292;&#20197;&#25214;&#21040;&#35299;&#37322;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#21306;&#22495;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#26412;&#22320;&#29305;&#24449;&#25928;&#24212;&#30340;&#20132;&#20114;&#24322;&#36136;&#24615;&#12290;&#25105;&#20204;&#20026;&#35813;&#26694;&#26550;&#25552;&#20379;&#20102;&#25968;&#23398;&#22522;&#30784;&#65292;&#24182;&#23637;&#31034;&#23427;&#36866;&#29992;&#20110;&#26368;&#27969;&#34892;&#30340;&#26041;&#27861;&#26469;&#21487;&#35270;&#21270;&#36793;&#38469;&#29305;&#24449;&#25928;&#24212;&#65292;&#21363;&#20559;&#20381;&#36182;&#65292;&#31215;&#32047;&#23616;&#37096;&#25928;&#24212;&#21644;Shapley&#21487;&#21152;&#35299;&#37322;&#65288;SHAP&#65289;&#20381;&#36182;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#32622;&#25442;&#30340;&#20132;&#20114;&#27979;&#35797;&#26469;&#26816;&#27979;&#26174;&#30528;&#30340;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Global feature effect methods, such as partial dependence plots, provide an intelligible visualization of the expected marginal feature effect. However, such global feature effect methods can be misleading, as they do not represent local feature effects of single observations well when feature interactions are present. We formally introduce generalized additive decomposition of global effects (GADGET), which is a new framework based on recursive partitioning to find interpretable regions in the feature space such that the interaction-related heterogeneity of local feature effects is minimized. We provide a mathematical foundation of the framework and show that it is applicable to the most popular methods to visualize marginal feature effects, namely partial dependence, accumulated local effects, and Shapley additive explanations (SHAP) dependence. Furthermore, we introduce a new permutation-based interaction test to detect significant feature interactions that is applicable to any feat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31867;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#26080;&#26465;&#20214;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#35813;&#26426;&#21046;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26080;&#38480;&#32500;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.12681</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#40065;&#26834;&#20248;&#21270;&#23454;&#29616;&#24046;&#20998;&#38544;&#31169;&#20445;&#25252;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy via Distributionally Robust Optimization. (arXiv:2304.12681v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31867;&#26426;&#21046;&#65292;&#20197;&#23454;&#29616;&#26080;&#26465;&#20214;&#26368;&#20248;&#24615;&#20445;&#35777;&#30340;&#24046;&#20998;&#38544;&#31169;&#12290;&#35813;&#26426;&#21046;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26080;&#38480;&#32500;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#25104;&#20026;&#20849;&#20139;&#25968;&#25454;&#38598;&#32479;&#35745;&#20449;&#24687;&#24182;&#38480;&#21046;&#28041;&#21450;&#20010;&#20154;&#30340;&#31169;&#20154;&#20449;&#24687;&#25259;&#38706;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#23558;&#35201;&#21457;&#24067;&#30340;&#32479;&#35745;&#25968;&#25454;&#36827;&#34892;&#38543;&#26426;&#25200;&#21160;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#36825;&#21453;&#36807;&#26469;&#23548;&#33268;&#20102;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65306;&#26356;&#22823;&#30340;&#25200;&#21160;&#25552;&#20379;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#65292;&#20294;&#32467;&#26524;&#26159;&#25552;&#20379;&#36739;&#20302;&#23454;&#29992;&#24230;&#30340;&#32479;&#35745;&#25968;&#25454;&#21644;&#26356;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#22240;&#27492;&#65292;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#26159;&#22312;&#39044;&#36873;&#38544;&#31169;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#26368;&#39640;&#20934;&#30830;&#24615;&#30340;&#26368;&#20339;&#26426;&#21046;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#38598;&#20013;&#22312;&#20107;&#20808;&#25351;&#23450;&#25200;&#21160;&#26063;&#24182;&#38543;&#21518;&#35777;&#26126;&#20854;&#28176;&#36817;&#21644;/&#25110;&#26368;&#20339;&#24615;&#19978;&#65292;&#26412;&#25991;&#21017;&#24320;&#21457;&#20102;&#19968;&#31867;&#26426;&#21046;&#65292;&#23427;&#20204;&#20855;&#26377;&#38750;&#28176;&#36817;&#21644;&#26080;&#26465;&#20214;&#30340;&#26368;&#20339;&#24615;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#21046;&#23450;&#20026;&#26080;&#38480;&#32500;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, differential privacy has emerged as the de facto standard for sharing statistics of datasets while limiting the disclosure of private information about the involved individuals. This is achieved by randomly perturbing the statistics to be published, which in turn leads to a privacy-accuracy trade-off: larger perturbations provide stronger privacy guarantees, but they result in less accurate statistics that offer lower utility to the recipients. Of particular interest are therefore optimal mechanisms that provide the highest accuracy for a pre-selected level of privacy. To date, work in this area has focused on specifying families of perturbations a priori and subsequently proving their asymptotic and/or best-in-class optimality. In this paper, we develop a class of mechanisms that enjoy non-asymptotic and unconditional optimality guarantees. To this end, we formulate the mechanism design problem as an infinite-dimensional distributionally robust optimization problem. W
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#24314;&#27169;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#21253;&#21547;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.00200</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#30340;&#31890;&#23376;&#31995;&#32479;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion map particle systems for generative modeling. (arXiv:2304.00200v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#21487;&#20197;&#29992;&#20110;&#39640;&#25928;&#29983;&#25104;&#24314;&#27169;&#65292;&#23454;&#39564;&#34920;&#26126;&#22312;&#21253;&#21547;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#25955;&#26144;&#23556;&#31890;&#23376;&#31995;&#32479;(DMPS)&#65292;&#29992;&#20110;&#29983;&#25104;&#24314;&#27169;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25193;&#25955;&#26144;&#23556;&#21644;Laplacian&#35843;&#25972;&#30340;Wasserstein&#26799;&#24230;&#19979;&#38477;&#65288;LAWGD&#65289;&#12290;&#25193;&#25955;&#26144;&#23556;&#34987;&#29992;&#26469;&#20174;&#26679;&#26412;&#20013;&#36817;&#20284;Langevin&#25193;&#25955;&#36807;&#31243;&#30340;&#29983;&#25104;&#22120;&#65292;&#20174;&#32780;&#23398;&#20064;&#28508;&#22312;&#30340;&#25968;&#25454;&#29983;&#25104;&#27969;&#24418;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;LAWGD&#33021;&#22815;&#22312;&#21512;&#36866;&#30340;&#26680;&#20989;&#25968;&#36873;&#25321;&#19979;&#39640;&#25928;&#22320;&#20174;&#30446;&#26631;&#20998;&#24067;&#20013;&#25277;&#26679;&#65292;&#25105;&#20204;&#22312;&#36825;&#37324;&#36890;&#36807;&#25193;&#25955;&#26144;&#23556;&#35745;&#31639;&#29983;&#25104;&#22120;&#30340;&#35889;&#36924;&#36817;&#26469;&#26500;&#36896;&#26680;&#20989;&#25968;&#12290;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21253;&#25324;&#20855;&#26377;&#27969;&#24418;&#32467;&#26500;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel diffusion map particle system (DMPS) for generative modeling, based on diffusion maps and Laplacian-adjusted Wasserstein gradient descent (LAWGD). Diffusion maps are used to approximate the generator of the Langevin diffusion process from samples, and hence to learn the underlying data-generating manifold. On the other hand, LAWGD enables efficient sampling from the target distribution given a suitable choice of kernel, which we construct here via a spectral approximation of the generator, computed with diffusion maps. Numerical experiments show that our method outperforms others on synthetic datasets, including examples with manifold structure.
&lt;/p&gt;</description></item></channel></rss>