<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;Follmer&#27969;&#26469;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#36716;&#21270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Wasserstein&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#35823;&#24046;&#20998;&#26512;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01460</link><description>&lt;p&gt;
&#28145;&#24230;&#26465;&#20214;&#29983;&#25104;&#23398;&#20064;&#65306;&#27169;&#22411;&#19982;&#35823;&#24046;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Deep Conditional Generative Learning: Model and Error Analysis
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26465;&#20214;Follmer&#27969;&#26469;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#36890;&#36807;&#31163;&#25955;&#21270;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#39640;&#25928;&#36716;&#21270;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;Wasserstein&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#35823;&#24046;&#20998;&#26512;&#65292;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#28145;&#24230;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#26465;&#20214;&#20998;&#24067;&#65292;&#31216;&#20026;&#26465;&#20214;Follmer&#27969;&#12290;&#20174;&#26631;&#20934;&#39640;&#26031;&#20998;&#24067;&#24320;&#22987;&#65292;&#25152;&#25552;&#20986;&#30340;&#27969;&#33021;&#22815;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#23558;&#20854;&#36716;&#21270;&#20026;&#30446;&#26631;&#26465;&#20214;&#20998;&#24067;&#65292;&#22312;&#26102;&#38388;1&#22788;&#36798;&#21040;&#31283;&#23450;&#12290;&#20026;&#20102;&#26377;&#25928;&#23454;&#29616;&#65292;&#25105;&#20204;&#20351;&#29992;&#27431;&#25289;&#26041;&#27861;&#23545;&#27969;&#36827;&#34892;&#31163;&#25955;&#21270;&#65292;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#38750;&#21442;&#25968;&#21270;&#20272;&#35745;&#36895;&#24230;&#22330;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#23398;&#20064;&#26679;&#26412;&#30340;&#20998;&#24067;&#19982;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#30340;Wasserstein&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#36895;&#29575;&#65292;&#22312;&#26465;&#20214;&#20998;&#24067;&#23398;&#20064;&#20013;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#20840;&#38754;&#30340;&#31471;&#21040;&#31471;&#35823;&#24046;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#23454;&#39564;&#23637;&#31034;&#20102;&#23427;&#22312;&#19968;&#31995;&#21015;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#65292;&#20174;&#26631;&#20934;&#30340;&#38750;&#21442;&#25968;&#21270;&#26465;&#20214;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#21040;&#28041;&#21450;&#22270;&#20687;&#25968;&#25454;&#30340;&#26356;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#35828;&#26126;&#23427;&#20248;&#20110;&#21508;&#31181;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce an Ordinary Differential Equation (ODE) based deep generative method for learning a conditional distribution, named the Conditional Follmer Flow. Starting from a standard Gaussian distribution, the proposed flow could efficiently transform it into the target conditional distribution at time 1. For effective implementation, we discretize the flow with Euler's method where we estimate the velocity field nonparametrically using a deep neural network. Furthermore, we derive a non-asymptotic convergence rate in the Wasserstein distance between the distribution of the learned samples and the target distribution, providing the first comprehensive end-to-end error analysis for conditional distribution learning via ODE flow. Our numerical experiments showcase its effectiveness across a range of scenarios, from standard nonparametric conditional density estimation problems to more intricate challenges involving image data, illustrating its superiority over various existing condition
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2402.03941</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Discovery of the Hidden World with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#65292;&#35813;&#21161;&#25163;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#30340;&#22240;&#26524;&#22240;&#23376;&#65292;&#24182;&#23558;&#20854;&#36716;&#21270;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20026;&#25506;&#32034;&#38544;&#34255;&#19990;&#30028;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#36215;&#28304;&#20110;&#20174;&#24050;&#30693;&#20107;&#23454;&#21644;&#35266;&#23519;&#20013;&#21457;&#29616;&#26032;&#30340;&#22240;&#26524;&#30693;&#35782;&#12290;&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#20110;&#39640;&#36136;&#37327;&#30340;&#27979;&#37327;&#21464;&#37327;&#65292;&#36890;&#24120;&#30001;&#20154;&#31867;&#19987;&#23478;&#25552;&#20379;&#65292;&#20197;&#25214;&#21040;&#22240;&#26524;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#22240;&#26524;&#21464;&#37327;&#36890;&#24120;&#26080;&#27861;&#33719;&#21462;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#23835;&#36215;&#20026;&#20174;&#21407;&#22987;&#35266;&#27979;&#25968;&#25454;&#20013;&#21457;&#29616;&#39640;&#32423;&#38544;&#34255;&#21464;&#37327;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;COAT&#65306;&#22240;&#26524;&#34920;&#31034;&#21161;&#25163;&#12290;COAT&#23558;LLMs&#20316;&#20026;&#22240;&#32032;&#25552;&#20379;&#22120;&#24341;&#20837;&#65292;&#25552;&#21462;&#20986;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#30340;&#28508;&#22312;&#22240;&#26524;&#22240;&#23376;&#12290;&#27492;&#22806;&#65292;LLMs&#36824;&#21487;&#20197;&#34987;&#25351;&#31034;&#25552;&#20379;&#29992;&#20110;&#25910;&#38598;&#25968;&#25454;&#20540;&#65288;&#20363;&#22914;&#27880;&#37322;&#26631;&#20934;&#65289;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#24182;&#23558;&#21407;&#22987;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#36827;&#19968;&#27493;&#35299;&#26512;&#20026;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#27880;&#37322;&#25968;&#25454;&#23558;&#34987;&#36755;&#20837;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Science originates with discovering new causal knowledge from a combination of known facts and observations. Traditional causal discovery approaches mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. However, the causal variables are usually unavailable in a wide range of real-world applications. The rise of large language models (LLMs) that are trained to learn rich knowledge from the massive observations of the world, provides a new opportunity to assist with discovering high-level hidden variables from the raw observational data. Therefore, we introduce COAT: Causal representatiOn AssistanT. COAT incorporates LLMs as a factor proposer that extracts the potential causal factors from unstructured data. Moreover, LLMs can also be instructed to provide additional information used to collect data values (e.g., annotation criteria) and to further parse the raw unstructured data into structured data. The annotated data will be fed to a
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.06344</link><description>&lt;p&gt;
&#36229;&#32423;-STTN&#65306;&#31038;&#20132;&#32676;&#20307;&#24863;&#30693;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#29992;&#20110;&#20154;&#20307;&#36712;&#36857;&#39044;&#27979;&#19982;&#36229;&#22270;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Hyper-STTN: Social Group-aware Spatial-Temporal Transformer Network for Human Trajectory Prediction with Hypergraph Reasoning. (arXiv:2401.06344v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Hyper-STTN&#65292;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#36890;&#36807;&#26500;&#24314;&#22810;&#23610;&#24230;&#36229;&#22270;&#26469;&#25429;&#25417;&#25317;&#25380;&#22330;&#26223;&#20013;&#30340;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#21033;&#29992;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#30340;&#25104;&#23545;&#28508;&#22312;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#65292;&#21253;&#25324;&#26381;&#21153;&#26426;&#22120;&#20154;&#21644;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65292;&#39044;&#27979;&#25317;&#25380;&#30340;&#24847;&#22270;&#21644;&#36712;&#36857;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#29702;&#35299;&#29615;&#22659;&#21160;&#24577;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#19981;&#20165;&#22240;&#20026;&#23545;&#24314;&#27169;&#25104;&#23545;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#24615;&#65292;&#36824;&#22240;&#20026;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#22810;&#26679;&#24615;&#12290;&#20026;&#20102;&#35299;&#30721;&#25317;&#25380;&#22330;&#26223;&#20013;&#20840;&#38754;&#30340;&#25104;&#23545;&#21644;&#32676;&#20307;&#38388;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hyper-STTN&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#30340;&#26102;&#31354;&#36716;&#25442;&#32593;&#32476;&#65292;&#29992;&#20110;&#20154;&#32676;&#36712;&#36857;&#39044;&#27979;&#12290;&#22312;Hyper-STTN&#20013;&#65292;&#36890;&#36807;&#19968;&#32452;&#22810;&#23610;&#24230;&#36229;&#22270;&#26500;&#24314;&#20102;&#25317;&#25380;&#30340;&#32676;&#20307;&#38388;&#30456;&#20851;&#24615;&#65292;&#36825;&#20123;&#36229;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#32676;&#20307;&#22823;&#23567;&#65292;&#36890;&#36807;&#22522;&#20110;&#38543;&#26426;&#28216;&#36208;&#27010;&#29575;&#30340;&#36229;&#22270;&#35889;&#21367;&#31215;&#36827;&#34892;&#25429;&#25417;&#12290;&#27492;&#22806;&#65292;&#36824;&#37319;&#29992;&#20102;&#31354;&#38388;-&#26102;&#38388;&#36716;&#25442;&#22120;&#26469;&#25429;&#25417;&#34892;&#20154;&#22312;&#31354;&#38388;-&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#23545;&#29031;&#30456;&#20114;&#20316;&#29992;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#24322;&#26500;&#30340;&#32676;&#20307;&#38388;&#21644;&#25104;&#23545;&#38388;&#30456;&#20114;&#20316;&#29992;&#36890;&#36807;&#19968;&#20010;&#22810;&#27169;&#24577;&#36716;&#25442;&#32593;&#32476;&#36827;&#34892;&#34701;&#21512;&#21644;&#23545;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting crowded intents and trajectories is crucial in varouls real-world applications, including service robots and autonomous vehicles. Understanding environmental dynamics is challenging, not only due to the complexities of modeling pair-wise spatial and temporal interactions but also the diverse influence of group-wise interactions. To decode the comprehensive pair-wise and group-wise interactions in crowded scenarios, we introduce Hyper-STTN, a Hypergraph-based Spatial-Temporal Transformer Network for crowd trajectory prediction. In Hyper-STTN, crowded group-wise correlations are constructed using a set of multi-scale hypergraphs with varying group sizes, captured through random-walk robability-based hypergraph spectral convolution. Additionally, a spatial-temporal transformer is adapted to capture pedestrians' pair-wise latent interactions in spatial-temporal dimensions. These heterogeneous group-wise and pair-wise are then fused and aligned though a multimodal transformer net
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#25353;&#29031;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#20449;&#24565;&#20998;&#24067;&#37319;&#26679;&#31062;&#20808;&#22270;&#65292;&#24182;&#24341;&#20837;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#19982;&#19987;&#23478;&#20114;&#21160;&#65292;&#20197;&#25552;&#20379;&#19987;&#23478;&#21487;&#39564;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36845;&#20195;&#25913;&#36827;&#22240;&#26524;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2309.12032</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#21516;&#19979;&#20351;&#29992;&#31062;&#20808;GFlowNets&#36827;&#34892;&#28508;&#22312;&#28151;&#28102;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop Causal Discovery under Latent Confounding using Ancestral GFlowNets. (arXiv:2309.12032v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#26426;&#21327;&#21516;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#25353;&#29031;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#30340;&#20449;&#24565;&#20998;&#24067;&#37319;&#26679;&#31062;&#20808;&#22270;&#65292;&#24182;&#24341;&#20837;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#19982;&#19987;&#23478;&#20114;&#21160;&#65292;&#20197;&#25552;&#20379;&#19987;&#23478;&#21487;&#39564;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36845;&#20195;&#25913;&#36827;&#22240;&#26524;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#23398;&#20064;&#26159;&#22240;&#26524;&#25512;&#26029;&#30340;&#20851;&#38190;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24403;&#25968;&#25454;&#31232;&#32570;&#26102;&#65292;&#22240;&#26524;&#21457;&#29616;&#65288;CD&#65289;&#31639;&#27861;&#24456;&#33030;&#24369;&#65292;&#21487;&#33021;&#25512;&#26029;&#20986;&#19982;&#19987;&#23478;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#19981;&#20934;&#30830;&#22240;&#26524;&#20851;&#31995;&#65292;&#23588;&#20854;&#26159;&#32771;&#34385;&#21040;&#28508;&#22312;&#28151;&#28102;&#22240;&#32032;&#26102;&#26356;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#21152;&#37325;&#36825;&#20010;&#38382;&#39064;&#65292;&#22823;&#22810;&#25968;CD&#26041;&#27861;&#24182;&#19981;&#25552;&#20379;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#36825;&#20351;&#24471;&#29992;&#25143;&#38590;&#20197;&#35299;&#37322;&#32467;&#26524;&#21644;&#25913;&#36827;&#25512;&#26029;&#36807;&#31243;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23613;&#31649;CD&#26159;&#19968;&#20010;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20107;&#21153;&#65292;&#20294;&#27809;&#26377;&#20219;&#20309;&#30740;&#31350;&#19987;&#27880;&#20110;&#26500;&#24314;&#26082;&#33021;&#36755;&#20986;&#19987;&#23478;&#21487;&#39564;&#35777;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21448;&#33021;&#19982;&#19987;&#23478;&#36827;&#34892;&#20132;&#20114;&#36845;&#20195;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#29983;&#25104;&#27969;&#32593;&#65292;&#26681;&#25454;&#22522;&#20110;&#35780;&#20998;&#20989;&#25968;&#65288;&#22914;&#36125;&#21494;&#26031;&#20449;&#24687;&#20934;&#21017;&#65289;&#30340;&#20449;&#24565;&#20998;&#24067;&#65292;&#25353;&#27604;&#20363;&#23545;&#65288;&#22240;&#26524;&#65289;&#31062;&#20808;&#22270;&#36827;&#34892;&#37319;&#26679;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20505;&#36873;&#22270;&#30340;&#22810;&#26679;&#24615;&#24182;&#24341;&#20837;&#26368;&#20339;&#23454;&#39564;&#35774;&#35745;&#65292;&#20197;&#36845;&#20195;&#24615;&#22320;&#25506;&#32034;&#23454;&#39564;&#26469;&#19982;&#19987;&#23478;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure learning is the crux of causal inference. Notably, causal discovery (CD) algorithms are brittle when data is scarce, possibly inferring imprecise causal relations that contradict expert knowledge -- especially when considering latent confounders. To aggravate the issue, most CD methods do not provide uncertainty estimates, making it hard for users to interpret results and improve the inference process. Surprisingly, while CD is a human-centered affair, no works have focused on building methods that both 1) output uncertainty estimates that can be verified by experts and 2) interact with those experts to iteratively refine CD. To solve these issues, we start by proposing to sample (causal) ancestral graphs proportionally to a belief distribution based on a score function, such as the Bayesian information criterion (BIC), using generative flow networks. Then, we leverage the diversity in candidate graphs and introduce an optimal experimental design to iteratively probe the expe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#25913;&#21892;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06701</link><description>&lt;p&gt;
&#20266;&#35013;&#22270;&#20687;&#21512;&#25104;&#26159;&#25552;&#39640;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Camouflaged Image Synthesis Is All You Need to Boost Camouflaged Detection. (arXiv:2308.06701v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06701
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#25913;&#21892;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#24182;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#30446;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34701;&#20837;&#33258;&#28982;&#22330;&#26223;&#30340;&#20266;&#35013;&#29289;&#20307;&#32473;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26816;&#27979;&#21644;&#21512;&#25104;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#65292;&#28982;&#32780;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#35813;&#30740;&#31350;&#35838;&#39064;&#19968;&#30452;&#21463;&#21040;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21512;&#25104;&#20266;&#35013;&#25968;&#25454;&#20197;&#22686;&#24378;&#23545;&#33258;&#28982;&#22330;&#26223;&#20013;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29983;&#25104;&#27169;&#22411;&#29983;&#25104;&#36924;&#30495;&#30340;&#20266;&#35013;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#29616;&#26377;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20266;&#35013;&#29615;&#22659;&#29983;&#25104;&#22120;&#65292;&#30001;&#20266;&#35013;&#20998;&#24067;&#20998;&#31867;&#22120;&#36827;&#34892;&#30417;&#30563;&#65292;&#21512;&#25104;&#20266;&#35013;&#22270;&#20687;&#65292;&#28982;&#21518;&#23558;&#20854;&#36755;&#20837;&#25105;&#20204;&#30340;&#29983;&#25104;&#22120;&#20197;&#25193;&#23637;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;COD10k&#12289;CAMO&#21644;CHAMELEON&#65289;&#19978;&#30340;&#25928;&#26524;&#36229;&#36807;&#20102;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25913;&#21892;&#20266;&#35013;&#29289;&#20307;&#26816;&#27979;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Camouflaged objects that blend into natural scenes pose significant challenges for deep-learning models to detect and synthesize. While camouflaged object detection is a crucial task in computer vision with diverse real-world applications, this research topic has been constrained by limited data availability. We propose a framework for synthesizing camouflage data to enhance the detection of camouflaged objects in natural scenes. Our approach employs a generative model to produce realistic camouflage images, which can be used to train existing object detection models. Specifically, we use a camouflage environment generator supervised by a camouflage distribution classifier to synthesize the camouflage images, which are then fed into our generator to expand the dataset. Our framework outperforms the current state-of-the-art method on three datasets (COD10k, CAMO, and CHAMELEON), demonstrating its effectiveness in improving camouflaged object detection. This approach can serve as a plug-
&lt;/p&gt;</description></item><item><title>&#20197;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#29616;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#23558;&#32508;&#21512;&#22122;&#22768;&#28155;&#21152;&#21040;&#25968;&#25454;&#20013;&#65292;&#20351;&#24471;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#27169;&#22411;&#36798;&#21040;&#29305;&#23450;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26680;&#26041;&#27861;&#20171;&#32461;&#20102;&#36830;&#32493;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#38544;&#31169;&#24863;&#30693;&#35299;&#24418;&#24335;&#65292;&#20197;&#21450;&#30740;&#31350;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21355;&#26143;&#36712;&#36857;&#36319;&#36394;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.16541</link><description>&lt;p&gt;
&#38754;&#21521;&#38544;&#31169;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Privacy-aware Gaussian Process Regression. (arXiv:2305.16541v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16541
&lt;/p&gt;
&lt;p&gt;
&#20197;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#20026;&#22522;&#30784;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23454;&#29616;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#27861;&#12290;&#26041;&#27861;&#23558;&#32508;&#21512;&#22122;&#22768;&#28155;&#21152;&#21040;&#25968;&#25454;&#20013;&#65292;&#20351;&#24471;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#27169;&#22411;&#36798;&#21040;&#29305;&#23450;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#26680;&#26041;&#27861;&#20171;&#32461;&#20102;&#36830;&#32493;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#38544;&#31169;&#24863;&#30693;&#35299;&#24418;&#24335;&#65292;&#20197;&#21450;&#30740;&#31350;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#21355;&#26143;&#36712;&#36857;&#36319;&#36394;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#38544;&#31169;&#32422;&#26463;&#26465;&#20214;&#19979;&#30340;&#39640;&#26031;&#36807;&#31243;&#22238;&#24402;&#30340;&#29702;&#35770;&#21644;&#26041;&#27861;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#25968;&#25454;&#25152;&#26377;&#32773;&#22240;&#38544;&#31169;&#25285;&#24551;&#32780;&#19981;&#24895;&#19982;&#20844;&#20247;&#20998;&#20139;&#20854;&#20174;&#20854;&#25968;&#25454;&#26500;&#24314;&#30340;&#39640;&#20445;&#30495;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#26102;&#20351;&#29992;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#36890;&#36807;&#28155;&#21152;&#32508;&#21512;&#22122;&#22768;&#26469;&#20351;&#39640;&#26031;&#36807;&#31243;&#39044;&#27979;&#27169;&#22411;&#30340;&#39044;&#27979;&#26041;&#24046;&#36798;&#21040;&#39044;&#20808;&#25351;&#23450;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;&#21512;&#25104;&#22122;&#22768;&#30340;&#26368;&#20248;&#21327;&#26041;&#24046;&#30697;&#38453;&#20197;&#21322;&#23450;&#32534;&#31243;&#30340;&#24418;&#24335;&#32473;&#20986;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#22312;&#36830;&#32493;&#32422;&#26463;&#38544;&#31169;&#26465;&#20214;&#19979;&#30340;&#38544;&#31169;&#24863;&#30693;&#35299;&#30340;&#24418;&#24335;&#65292;&#24182;&#30740;&#31350;&#20102;&#23427;&#20204;&#30340;&#29702;&#35770;&#23646;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#29992;&#36319;&#36394;&#21355;&#26143;&#36712;&#36857;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#35828;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the first theoretical and methodological framework for Gaussian process regression subject to privacy constraints. The proposed method can be used when a data owner is unwilling to share a high-fidelity supervised learning model built from their data with the public due to privacy concerns. The key idea of the proposed method is to add synthetic noise to the data until the predictive variance of the Gaussian process model reaches a prespecified privacy level. The optimal covariance matrix of the synthetic noise is formulated in terms of semi-definite programming. We also introduce the formulation of privacy-aware solutions under continuous privacy constraints using kernel-based approaches, and study their theoretical properties. The proposed method is illustrated by considering a model that tracks the trajectories of satellites.
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#33539;&#20363;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27599;&#27425;&#26356;&#26032;&#22522;&#26412;&#27169;&#22411;&#21442;&#25968;&#21518;&#33258;&#36866;&#24212;&#22320;&#22312;&#32447;&#26356;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#31283;&#23450;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2301.13247</link><description>&lt;p&gt;
&#22312;&#32447;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Loss Function Learning. (arXiv:2301.13247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13247
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#33539;&#20363;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27599;&#27425;&#26356;&#26032;&#22522;&#26412;&#27169;&#22411;&#21442;&#25968;&#21518;&#33258;&#36866;&#24212;&#22320;&#22312;&#32447;&#26356;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#31283;&#23450;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#20803;&#23398;&#20064;&#33539;&#20363;&#65292;&#26088;&#22312;&#33258;&#21160;&#21270;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35774;&#35745;&#25439;&#22833;&#20989;&#25968;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#32463;&#24120;&#25913;&#21892;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#32456;&#25512;&#29702;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#30340;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#26159;&#25439;&#22833;&#20989;&#25968;&#20197;&#32447;&#19979;&#26041;&#24335;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#20803;&#30446;&#26631;&#20165;&#32771;&#34385;&#35757;&#32451;&#30340;&#21069;&#20960;&#20010;&#27493;&#39588;&#65292;&#36825;&#19982;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20351;&#29992;&#30340;&#26102;&#38388;&#33539;&#22260;&#30456;&#27604;&#26174;&#33879;&#36739;&#30701;&#12290;&#36825;&#23548;&#33268;&#23545;&#20110;&#22312;&#35757;&#32451;&#24320;&#22987;&#26102;&#34920;&#29616;&#33391;&#22909;&#20294;&#22312;&#35757;&#32451;&#32467;&#26463;&#26102;&#34920;&#29616;&#19981;&#20339;&#30340;&#25439;&#22833;&#20989;&#25968;&#23384;&#22312;&#26126;&#26174;&#30340;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27599;&#27425;&#26356;&#26032;&#22522;&#26412;&#27169;&#22411;&#21442;&#25968;&#21518;&#33258;&#36866;&#24212;&#22320;&#22312;&#32447;&#26356;&#26032;&#25439;&#22833;&#20989;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#20219;&#21153;&#19978;&#31283;&#23450;&#22320;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Loss function learning is a new meta-learning paradigm that aims to automate the essential task of designing a loss function for a machine learning model. Existing techniques for loss function learning have shown promising results, often improving a model's training dynamics and final inference performance. However, a significant limitation of these techniques is that the loss functions are meta-learned in an offline fashion, where the meta-objective only considers the very first few steps of training, which is a significantly shorter time horizon than the one typically used for training deep neural networks. This causes significant bias towards loss functions that perform well at the very start of training but perform poorly at the end of training. To address this issue we propose a new loss function learning technique for adaptively updating the loss function online after each update to the base model parameters. The experimental results show that our proposed method consistently out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#35821;&#38899;&#21644;&#39640;&#26031;&#22122;&#22768;&#36827;&#34892;&#21453;&#21521;&#36807;&#31243;&#65292;&#20165;&#20351;&#29992;30&#27493;&#23601;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#35821;&#38899;&#20272;&#35745;&#12290;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#26041;&#27861;&#30340;&#31454;&#20105;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2208.05830</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#30340;&#35821;&#38899;&#22686;&#24378;&#21644;&#21435;&#28151;&#21709;
&lt;/p&gt;
&lt;p&gt;
Speech Enhancement and Dereverberation with Diffusion-based Generative Models. (arXiv:2208.05830v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.05830
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#28151;&#21512;&#22122;&#22768;&#35821;&#38899;&#21644;&#39640;&#26031;&#22122;&#22768;&#36827;&#34892;&#21453;&#21521;&#36807;&#31243;&#65292;&#20165;&#20351;&#29992;30&#27493;&#23601;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#35821;&#38899;&#20272;&#35745;&#12290;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#65292;&#36798;&#21040;&#20102;&#26368;&#26032;&#26041;&#27861;&#30340;&#31454;&#20105;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#25105;&#20204;&#20043;&#21069;&#30340;&#20986;&#29256;&#29289;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#35821;&#38899;&#22686;&#24378;&#12290;&#25105;&#20204;&#35814;&#32454;&#20171;&#32461;&#20102;&#22522;&#20110;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#24182;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29702;&#35770;&#25506;&#35752;&#20854;&#21547;&#20041;&#12290;&#19982;&#36890;&#24120;&#30340;&#26465;&#20214;&#29983;&#25104;&#20219;&#21153;&#30456;&#21453;&#65292;&#25105;&#20204;&#19981;&#26159;&#20174;&#32431;&#39640;&#26031;&#22122;&#22768;&#24320;&#22987;&#21453;&#21521;&#36807;&#31243;&#65292;&#32780;&#26159;&#20174;&#22122;&#22768;&#35821;&#38899;&#21644;&#39640;&#26031;&#22122;&#22768;&#30340;&#28151;&#21512;&#29289;&#24320;&#22987;&#12290;&#36825;&#19982;&#25105;&#20204;&#30340;&#27491;&#21521;&#36807;&#31243;&#30456;&#21305;&#37197;&#65292;&#35813;&#36807;&#31243;&#36890;&#36807;&#21253;&#25324;&#28418;&#31227;&#39033;&#23558;&#24178;&#20928;&#35821;&#38899;&#36716;&#21464;&#25104;&#22122;&#22768;&#35821;&#38899;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20010;&#36807;&#31243;&#20351;&#24471;&#20351;&#29992;&#20165;30&#20010;&#25193;&#25955;&#27493;&#39588;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#24178;&#20928;&#35821;&#38899;&#20272;&#35745;&#25104;&#20026;&#21487;&#33021;&#12290;&#36890;&#36807;&#35843;&#25972;&#32593;&#32476;&#26550;&#26500;&#65292;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35821;&#38899;&#22686;&#24378;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;&#32593;&#32476;&#32780;&#19981;&#26159;&#24418;&#24335;&#20027;&#20041;&#26159;&#25105;&#20204;&#21407;&#22987;&#26041;&#27861;&#30340;&#20027;&#35201;&#38480;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#36328;&#25968;&#25454;&#38598;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#36827;&#21518;&#30340;&#26041;&#27861;&#21487;&#20197;&#19982;&#26368;&#26032;&#30340;&#26041;&#27861;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we build upon our previous publication and use diffusion-based generative models for speech enhancement. We present a detailed overview of the diffusion process that is based on a stochastic differential equation and delve into an extensive theoretical examination of its implications. Opposed to usual conditional generation tasks, we do not start the reverse process from pure Gaussian noise but from a mixture of noisy speech and Gaussian noise. This matches our forward process which moves from clean speech to noisy speech by including a drift term. We show that this procedure enables using only 30 diffusion steps to generate high-quality clean speech estimates. By adapting the network architecture, we are able to significantly improve the speech enhancement performance, indicating that the network, rather than the formalism, was the main limitation of our original approach. In an extensive cross-dataset evaluation, we show that the improved method can compete with recent 
&lt;/p&gt;</description></item></channel></rss>