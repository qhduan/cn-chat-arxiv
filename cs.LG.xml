<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Unifews&#36890;&#36807;&#32479;&#19968;&#36880;&#26465;&#31232;&#30095;&#21270;&#30340;&#26041;&#24335;&#65292;&#32852;&#21512;&#36793;&#26435;&#37325;&#31232;&#30095;&#21270;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#26550;&#26500;&#35774;&#35745;&#24182;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#31232;&#30095;&#24230;&#30340;&#33258;&#36866;&#24212;&#21387;&#32553;&#12290;</title><link>https://arxiv.org/abs/2403.13268</link><description>&lt;p&gt;
Unifews&#65306;&#29992;&#20110;&#39640;&#25928;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32479;&#19968;&#36880;&#26465;&#31232;&#30095;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unifews: Unified Entry-Wise Sparsification for Efficient Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13268
&lt;/p&gt;
&lt;p&gt;
Unifews&#36890;&#36807;&#32479;&#19968;&#36880;&#26465;&#31232;&#30095;&#21270;&#30340;&#26041;&#24335;&#65292;&#32852;&#21512;&#36793;&#26435;&#37325;&#31232;&#30095;&#21270;&#20197;&#25552;&#39640;&#23398;&#20064;&#25928;&#29575;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#26550;&#26500;&#35774;&#35745;&#24182;&#20855;&#26377;&#36880;&#28176;&#22686;&#21152;&#31232;&#30095;&#24230;&#30340;&#33258;&#36866;&#24212;&#21387;&#32553;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#31181;&#22270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#65292;&#20294;&#20195;&#20215;&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35745;&#31639;&#12290;GNN&#26356;&#26032;&#30340;&#20027;&#35201;&#24320;&#38144;&#26469;&#33258;&#22270;&#20256;&#25773;&#21644;&#26435;&#37325;&#21464;&#25442;&#65292;&#20004;&#32773;&#37117;&#28041;&#21450;&#23545;&#22270;&#35268;&#27169;&#30697;&#38453;&#30340;&#25805;&#20316;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#21033;&#29992;&#22270;&#32423;&#21035;&#25110;&#32593;&#32476;&#32423;&#21035;&#30340;&#31232;&#30095;&#21270;&#25216;&#26415;&#26469;&#20943;&#23569;&#35745;&#31639;&#39044;&#31639;&#65292;&#20174;&#32780;&#20135;&#29983;&#32553;&#23567;&#30340;&#22270;&#25110;&#26435;&#37325;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Unifews&#65292;&#23427;&#20197;&#36880;&#20010;&#30697;&#38453;&#20803;&#32032;&#30340;&#26041;&#24335;&#32479;&#19968;&#20102;&#36825;&#20004;&#31181;&#25805;&#20316;&#65292;&#24182;&#36827;&#34892;&#32852;&#21512;&#36793;&#26435;&#37325;&#31232;&#30095;&#21270;&#20197;&#22686;&#24378;&#23398;&#20064;&#25928;&#29575;&#12290;Unifews&#30340;&#36880;&#26465;&#35774;&#35745;&#20351;&#20854;&#33021;&#22815;&#22312;GNN&#23618;&#20043;&#38388;&#36827;&#34892;&#33258;&#36866;&#24212;&#21387;&#32553;&#65292;&#31232;&#30095;&#24230;&#36880;&#28176;&#22686;&#21152;&#65292;&#24182;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#35774;&#35745;&#65292;&#20855;&#26377;&#21363;&#26102;&#25805;&#20316;&#31616;&#21270;&#12290;&#22312;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#34920;&#24449;&#31232;&#30095;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13268v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promising performance in various graph learning tasks, but at the cost of resource-intensive computations. The primary overhead of GNN update stems from graph propagation and weight transformation, both involving operations on graph-scale matrices. Previous studies attempt to reduce the computational budget by leveraging graph-level or network-level sparsification techniques, resulting in downsized graph or weights. In this work, we propose Unifews, which unifies the two operations in an entry-wise manner considering individual matrix elements, and conducts joint edge-weight sparsification to enhance learning efficiency. The entry-wise design of Unifews enables adaptive compression across GNN layers with progressively increased sparsity, and is applicable to a variety of architectural designs with on-the-fly operation simplification. Theoretically, we establish a novel framework to characterize spa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#34394;&#25311;&#24322;&#24120;&#20540;&#26469;&#25913;&#21892;&#26080;&#38656;&#22238;&#39038;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#31867;&#21035;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#25552;&#31034;&#26597;&#35810;&#21644;&#32452;&#21512;&#35745;&#31639;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2402.04129</link><description>&lt;p&gt;
OVOR&#65306;&#19968;&#31181;&#20351;&#29992;&#34394;&#25311;&#24322;&#24120;&#20540;&#27491;&#21017;&#21270;&#30340;OnePrompt&#26041;&#27861;&#65292;&#23454;&#29616;&#26080;&#38656;&#22238;&#39038;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
OVOR: OnePrompt with Virtual Outlier Regularization for Rehearsal-Free Class-Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#34394;&#25311;&#24322;&#24120;&#20540;&#26469;&#25913;&#21892;&#26080;&#38656;&#22238;&#39038;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#36807;&#31243;&#20013;&#19981;&#21516;&#20219;&#21153;&#38388;&#30340;&#31867;&#21035;&#28151;&#28102;&#38382;&#39064;&#65292;&#24182;&#19988;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;&#25552;&#31034;&#26597;&#35810;&#21644;&#32452;&#21512;&#35745;&#31639;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#22312;&#26080;&#38656;&#22238;&#39038;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#35774;&#32622;&#20013;&#21487;&#20197;&#23454;&#29616;&#27604;&#33879;&#21517;&#30340;&#22522;&#20110;&#22238;&#39038;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#26080;&#38656;&#22238;&#39038;&#30340;CIL&#26041;&#27861;&#22312;&#21306;&#20998;&#19981;&#21516;&#20219;&#21153;&#30340;&#31867;&#21035;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#24182;&#26410;&#19968;&#21516;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34394;&#25311;&#24322;&#24120;&#20540;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#32039;&#32553;&#20998;&#31867;&#22120;&#30340;&#20915;&#31574;&#36793;&#30028;&#65292;&#20943;&#36731;&#19981;&#21516;&#20219;&#21153;&#38388;&#31867;&#21035;&#30340;&#28151;&#28102;&#12290;&#26368;&#36817;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#19968;&#20010;&#23384;&#20648;&#21508;&#20219;&#21153;&#29305;&#23450;&#25552;&#31034;&#30340;&#38598;&#21512;&#65292;&#20197;&#38450;&#27490;&#26032;&#20219;&#21153;&#30340;&#30693;&#35782;&#35206;&#30422;&#20808;&#21069;&#20219;&#21153;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#23548;&#33268;&#39069;&#22806;&#30340;&#26597;&#35810;&#21644;&#32452;&#21512;&#36866;&#24403;&#25552;&#31034;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#22312;&#35770;&#25991;&#20013;&#25581;&#31034;&#65292;&#21487;&#20197;&#28040;&#38500;&#36825;&#31181;&#39069;&#22806;&#24320;&#38144;&#32780;&#19981;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#31616;&#21270;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#21487;&#20197;&#36798;&#21040;&#19982;&#20808;&#21069;&#26368;&#26032;&#29366;&#24577;-of-the-art&#26041;&#27861;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have shown that by using large pre-trained models along with learnable prompts, rehearsal-free methods for class-incremental learning (CIL) settings can achieve superior performance to prominent rehearsal-based ones. Rehearsal-free CIL methods struggle with distinguishing classes from different tasks, as those are not trained together. In this work we propose a regularization method based on virtual outliers to tighten decision boundaries of the classifier, such that confusion of classes among different tasks is mitigated. Recent prompt-based methods often require a pool of task-specific prompts, in order to prevent overwriting knowledge of previous tasks with that of the new task, leading to extra computation in querying and composing an appropriate prompt from the pool. This additional cost can be eliminated, without sacrificing accuracy, as we reveal in the paper. We illustrate that a simplified prompt-based method can achieve results comparable to previous state-of-the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2401.13796</link><description>&lt;p&gt;
&#19981;&#35201;&#25353;&#25353;&#38062;&#65281;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20026;&#22810;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#24037;&#20855;&#30340;&#26085;&#30410;&#21487;&#33719;&#24471;&#24615;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#32570;&#20047;&#28145;&#20837;&#30340;ML&#19987;&#19994;&#30693;&#35782;&#65292;&#37319;&#29992;&#20102;&#8220;&#25353;&#25353;&#38062;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#31639;&#27861;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;&#23427;&#24341;&#21457;&#20102;&#23545;&#32467;&#26524;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#24615;&#33021;&#35780;&#20272;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;ML&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#27844;&#38706;&#65292;&#20854;&#20013;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#30340;&#20048;&#35266;&#24615;&#33021;&#20272;&#35745;&#12290;&#35780;&#20272;&#24615;&#33021;&#19982;&#23454;&#38469;&#22312;&#26032;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#29305;&#21035;&#23558;ML&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#20998;&#20026;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35889;&#20272;&#35745;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27979;&#37327;&#36827;&#34892;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#34920;&#31034;&#65292;&#20998;&#26512;&#20102;&#35889;&#20272;&#35745;&#22120;&#22312;&#32467;&#26500;&#21270;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#39044;&#22788;&#29702;&#20197;&#26368;&#23567;&#21270;&#26679;&#26412;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2308.14507</link><description>&lt;p&gt;
&#36890;&#36807;&#36817;&#20284;&#20256;&#36882;&#28040;&#24687;&#23454;&#29616;&#32467;&#26500;&#21270;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#35889;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Spectral Estimators for Structured Generalized Linear Models via Approximate Message Passing. (arXiv:2308.14507v1 [math.ST])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#38024;&#23545;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#30340;&#21442;&#25968;&#20272;&#35745;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35889;&#20272;&#35745;&#22120;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#27979;&#37327;&#36827;&#34892;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#34920;&#31034;&#65292;&#20998;&#26512;&#20102;&#35889;&#20272;&#35745;&#22120;&#22312;&#32467;&#26500;&#21270;&#35774;&#35745;&#20013;&#30340;&#24615;&#33021;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#20248;&#39044;&#22788;&#29702;&#20197;&#26368;&#23567;&#21270;&#26679;&#26412;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20174;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#20013;&#30340;&#35266;&#27979;&#20013;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#30340;&#38382;&#39064;&#12290;&#35889;&#26041;&#27861;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20272;&#35745;&#26041;&#27861;&#65306;&#23427;&#36890;&#36807;&#23545;&#35266;&#27979;&#36827;&#34892;&#36866;&#24403;&#39044;&#22788;&#29702;&#24471;&#21040;&#30340;&#30697;&#38453;&#30340;&#20027;&#29305;&#24449;&#21521;&#37327;&#26469;&#20272;&#35745;&#21442;&#25968;&#12290;&#23613;&#31649;&#35889;&#20272;&#35745;&#22120;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#23545;&#20110;&#32467;&#26500;&#21270;&#65288;&#21363;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#39640;&#26031;&#21644;&#21704;&#23572;&#65289;&#35774;&#35745;&#65292;&#30446;&#21069;&#20165;&#26377;&#23545;&#35889;&#20272;&#35745;&#22120;&#30340;&#20005;&#26684;&#24615;&#33021;&#34920;&#24449;&#20197;&#21450;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#22788;&#29702;&#30340;&#22522;&#26412;&#26041;&#27861;&#21487;&#29992;&#12290;&#30456;&#21453;&#65292;&#23454;&#38469;&#30340;&#35774;&#35745;&#30697;&#38453;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#24182;&#19988;&#34920;&#29616;&#20986;&#38750;&#24179;&#20961;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#25429;&#25417;&#27979;&#37327;&#30340;&#38750;&#21508;&#21521;&#21516;&#24615;&#29305;&#24615;&#30340;&#30456;&#20851;&#39640;&#26031;&#35774;&#35745;&#65292;&#36890;&#36807;&#29305;&#24449;&#21327;&#26041;&#24046;&#30697;&#38453;&#931;&#36827;&#34892;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#32467;&#26524;&#26159;&#23545;&#20110;&#36825;&#31181;&#24773;&#20917;&#19979;&#35889;&#20272;&#35745;&#22120;&#24615;&#33021;&#30340;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#21487;&#20197;&#36890;&#36807;&#36825;&#19968;&#32467;&#26524;&#26469;&#30830;&#23450;&#26368;&#20248;&#39044;&#22788;&#29702;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#25152;&#38656;&#26679;&#26412;&#30340;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of parameter estimation from observations given by a generalized linear model. Spectral methods are a simple yet effective approach for estimation: they estimate the parameter via the principal eigenvector of a matrix obtained by suitably preprocessing the observations. Despite their wide use, a rigorous performance characterization of spectral estimators, as well as a principled way to preprocess the data, is available only for unstructured (i.e., i.i.d. Gaussian and Haar) designs. In contrast, real-world design matrices are highly structured and exhibit non-trivial correlations. To address this problem, we consider correlated Gaussian designs which capture the anisotropic nature of the measurements via a feature covariance matrix $\Sigma$. Our main result is a precise asymptotic characterization of the performance of spectral estimators in this setting. This then allows to identify the optimal preprocessing that minimizes the number of samples needed to meanin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#19978;&#30340;&#34920;&#29616;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.13651</link><description>&lt;p&gt;
&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#23545;&#25239;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Adversarial Defenses via Vector Quantization. (arXiv:2305.13651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13651
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#26032;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#25552;&#20379;&#29702;&#35770;&#20445;&#35777;&#21644;&#23454;&#39564;&#19978;&#30340;&#34920;&#29616;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38543;&#26426;&#31163;&#25955;&#21270;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#22312;&#39640;&#32500;&#31354;&#38388;&#20013;&#21033;&#29992;&#30690;&#37327;&#37327;&#21270;&#24320;&#21457;&#20102;&#20004;&#31181;&#26032;&#30340;&#23545;&#25239;&#24615;&#38450;&#24481;&#26041;&#27861;&#65292;&#20998;&#21035;&#31216;&#20026;pRD&#21644;swRD&#12290;&#36825;&#20123;&#26041;&#27861;&#19981;&#20165;&#22312;&#35777;&#26126;&#20934;&#30830;&#24230;&#26041;&#38754;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#32780;&#19988;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#19982;&#24403;&#21069;&#23545;&#25239;&#38450;&#24481;&#25216;&#26415;&#30456;&#24403;&#29978;&#33267;&#26356;&#20248;&#31168;&#12290;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#25193;&#23637;&#21040;&#19968;&#31181;&#29256;&#26412;&#65292;&#20801;&#35768;&#23545;&#30446;&#26631;&#20998;&#31867;&#22120;&#36827;&#34892;&#36827;&#19968;&#27493;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20986;&#36827;&#19968;&#27493;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building upon Randomized Discretization, we develop two novel adversarial defenses against white-box PGD attacks, utilizing vector quantization in higher dimensional spaces. These methods, termed pRD and swRD, not only offer a theoretical guarantee in terms of certified accuracy, they are also shown, via abundant experiments, to perform comparably or even superior to the current art of adversarial defenses. These methods can be extended to a version that allows further training of the target classifier and demonstrates further improved performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;ICDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#26679;&#26412;&#22686;&#24378;&#31574;&#30053;&#12289;&#26131;&#20110;&#35745;&#31639;&#30340;&#20195;&#29702;&#25439;&#22833;&#21644;&#20855;&#20307;&#26041;&#26696;&#65292;&#28040;&#38500;&#20102;&#34394;&#20551;&#20851;&#32852;&#24182;&#36827;&#34892;&#20102;&#31283;&#20581;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2304.13431</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Implicit Counterfactual Data Augmentation for Deep Neural Networks. (arXiv:2304.13431v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13431
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;ICDA&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#30340;&#26679;&#26412;&#22686;&#24378;&#31574;&#30053;&#12289;&#26131;&#20110;&#35745;&#31639;&#30340;&#20195;&#29702;&#25439;&#22833;&#21644;&#20855;&#20307;&#26041;&#26696;&#65292;&#28040;&#38500;&#20102;&#34394;&#20551;&#20851;&#32852;&#24182;&#36827;&#34892;&#20102;&#31283;&#20581;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26131;&#20110;&#25429;&#25417;&#38750;&#22240;&#26524;&#23646;&#24615;&#21644;&#31867;&#21035;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#26159;&#30772;&#38500;&#36825;&#20123;&#34394;&#20551;&#30340;&#32852;&#24819;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26126;&#30830;&#29983;&#25104;&#21453;&#20107;&#23454;&#25968;&#25454;&#24456;&#20855;&#25361;&#25112;&#24615;&#65292;&#35757;&#32451;&#25928;&#29575;&#20250;&#38477;&#20302;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38544;&#24335;&#21453;&#20107;&#23454;&#25968;&#25454;&#22686;&#24378;&#65288;Implicit Counterfactual Data Augmentation&#65292;ICDA&#65289;&#26041;&#27861;&#26469;&#28040;&#38500;&#34394;&#20551;&#20851;&#32852;&#24182;&#36827;&#34892;&#31283;&#20581;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26679;&#26412;&#22686;&#24378;&#31574;&#30053;&#65292;&#20026;&#27599;&#20010;&#26679;&#26412;&#29983;&#25104;&#22312;&#35821;&#20041;&#21644;&#21453;&#20107;&#23454;&#24847;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#28145;&#24230;&#29305;&#24449;&#65292;&#24182;&#20855;&#26377;&#19981;&#21516;&#30340;&#22686;&#24378;&#24378;&#24230;&#12290;&#20854;&#27425;&#65292;&#24403;&#22686;&#24191;&#26679;&#26412;&#25968;&#21464;&#20026;&#26080;&#31351;&#22823;&#26102;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23545;&#20110;&#22686;&#24191;&#29305;&#24449;&#38598;&#30340;&#26131;&#20110;&#35745;&#31639;&#30340;&#20195;&#29702;&#25439;&#22833;&#12290;&#31532;&#19977;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#20855;&#20307;&#30340;&#26041;&#26696;&#65292;&#21253;&#25324;&#30452;&#25509;&#37327;&#21270;&#21644;&#20803;&#23398;&#20064;&#65292;&#20197;&#30830;&#23450;&#40065;&#26834;&#24615;&#25439;&#22833;&#30340;&#20851;&#38190;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#36824;&#20174;&#23454;&#39564;&#30340;&#35282;&#24230;&#35299;&#37322;&#20102;ICDA&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine-learning models are prone to capturing the spurious correlations between non-causal attributes and classes, with counterfactual data augmentation being a promising direction for breaking these spurious associations. However, explicitly generating counterfactual data is challenging, with the training efficiency declining. Therefore, this study proposes an implicit counterfactual data augmentation (ICDA) method to remove spurious correlations and make stable predictions. Specifically, first, a novel sample-wise augmentation strategy is developed that generates semantically and counterfactually meaningful deep features with distinct augmentation strength for each sample. Second, we derive an easy-to-compute surrogate loss on the augmented feature set when the number of augmented samples becomes infinite. Third, two concrete schemes are proposed, including direct quantification and meta-learning, to derive the key parameters for the robust loss. In addition, ICDA is explained from 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23398;&#20064;&#26377;&#38480;&#33258;&#21160;&#26426;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#21644;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14111</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#20248;&#21270;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Anomaly Detection via Discrete Optimization. (arXiv:2303.14111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23398;&#20064;&#26377;&#38480;&#33258;&#21160;&#26426;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#21644;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20363;&#22914;&#32593;&#32476;&#23433;&#20840;&#12289;&#25191;&#27861;&#12289;&#21307;&#23398;&#21644;&#27450;&#35784;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#24448;&#24448;&#38590;&#20197;&#29702;&#35299;&#65292;&#36825;&#36890;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#32473;&#23450;&#30340;&#26410;&#26631;&#35760;&#24207;&#21015;&#22810;&#37325;&#38598;&#20013;&#23398;&#20064;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426; &#65288;DFA&#65289;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;&#35745;&#31639;&#38590;&#39064;&#65292;&#24182;&#22522;&#20110;&#32422;&#26463;&#20248;&#21270;&#24320;&#21457;&#20102;&#20004;&#20010;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20248;&#21270;&#38382;&#39064;&#24341;&#20837;&#20102;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#25105;&#20204;&#30340;DFA&#30340;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21407;&#22411;&#23454;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is essential in many application domains, such as cyber security, law enforcement, medicine, and fraud protection. However, the decision-making of current deep learning approaches is notoriously hard to understand, which often limits their practical applicability. To overcome this limitation, we propose a framework for learning inherently interpretable anomaly detectors from sequential data. More specifically, we consider the task of learning a deterministic finite automaton (DFA) from a given multi-set of unlabeled sequences. We show that this problem is computationally hard and develop two learning algorithms based on constraint optimization. Moreover, we introduce novel regularization schemes for our optimization problems that improve the overall interpretability of our DFAs. Using a prototype implementation, we demonstrate that our approach shows promising results in terms of accuracy and F1 score.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20845;&#20010;&#24230;&#37327;&#26469;&#35780;&#20272;&#22522;&#20110;&#26799;&#24230;&#12289;&#20256;&#25773;&#25110;&#24178;&#25200;&#30340;&#20107;&#21518;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#20449;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2203.07861</link><description>&lt;p&gt;
&#19981;&#35201;&#35823;&#20250;&#25105;&#65306;&#22914;&#20309;&#23558;&#28145;&#24230;&#35270;&#35273;&#35299;&#37322;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Don't Get Me Wrong: How to Apply Deep Visual Interpretations to Time Series. (arXiv:2203.07861v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.07861
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#20845;&#20010;&#24230;&#37327;&#26469;&#35780;&#20272;&#22522;&#20110;&#26799;&#24230;&#12289;&#20256;&#25773;&#25110;&#24178;&#25200;&#30340;&#20107;&#21518;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#37322;&#20855;&#26377;&#36739;&#39640;&#30340;&#21487;&#20449;&#24230;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#65292;&#27491;&#30830;&#35299;&#37322;&#21644;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#38024;&#23545;&#22270;&#20687;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#35299;&#37322;&#24615;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#20801;&#35768;&#39046;&#22495;&#19987;&#23478;&#39564;&#35777;&#21644;&#29702;&#35299;&#20960;&#20046;&#20219;&#20309;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#25512;&#24191;&#21040;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#26102;&#65292;&#23427;&#20204;&#22312;&#26412;&#36136;&#19978;&#26356;&#21152;&#22797;&#26434;&#21644;&#22810;&#26679;&#21270;&#12290;&#19968;&#20010;&#21487;&#35270;&#21270;&#35299;&#37322;&#26159;&#21542;&#35299;&#37322;&#20102;&#26377;&#25928;&#30340;&#25512;&#29702;&#25110;&#25429;&#25417;&#20102;&#23454;&#38469;&#29305;&#24449;&#26159;&#38590;&#20197;&#21028;&#26029;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#38656;&#35201;&#23458;&#35266;&#35780;&#20272;&#26469;&#33719;&#24471;&#21487;&#20449;&#30340;&#36136;&#37327;&#25351;&#26631;&#65292;&#32780;&#19981;&#26159;&#30450;&#30446;&#20449;&#20219;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21253;&#25324;&#20845;&#20010;&#27491;&#20132;&#24230;&#37327;&#65292;&#29992;&#20110;&#38024;&#23545;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#30340;&#22522;&#20110;&#26799;&#24230;&#12289;&#20256;&#25773;&#25110;&#24178;&#25200;&#30340;&#20107;&#21518;&#35270;&#35273;&#35299;&#37322;&#26041;&#27861;&#12290;&#23454;&#39564;&#30740;&#31350;&#21253;&#25324;&#20102;&#24120;&#35265;&#30340;&#26102;&#38388;&#24207;&#21015;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#21644;&#20061;&#31181;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;UCR r&#31561;&#22810;&#26679;&#30340;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#36825;&#20123;&#21487;&#35270;&#21270;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The correct interpretation and understanding of deep learning models are essential in many applications. Explanatory visual interpretation approaches for image, and natural language processing allow domain experts to validate and understand almost any deep learning model. However, they fall short when generalizing to arbitrary time series, which is inherently less intuitive and more diverse. Whether a visualization explains valid reasoning or captures the actual features is difficult to judge. Hence, instead of blind trust, we need an objective evaluation to obtain trustworthy quality metrics. We propose a framework of six orthogonal metrics for gradient-, propagation- or perturbation-based post-hoc visual interpretation methods for time series classification and segmentation tasks. An experimental study includes popular neural network architectures for time series and nine visual interpretation methods. We evaluate the visual interpretation methods with diverse datasets from the UCR r
&lt;/p&gt;</description></item></channel></rss>