<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02595</link><description>&lt;p&gt;
QFNN-FFD&#65306;&#29992;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#30340;&#37327;&#23376;&#32852;&#37030;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02595
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#65292;&#36825;&#26159;&#19968;&#20010;&#34701;&#21512;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21069;&#27839;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#26032;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;FL&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;QFNN-FFD&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#35782;&#21035;&#27450;&#35784;&#20132;&#26131;&#30340;&#26041;&#27861;&#12290;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#23454;&#26045;&#21452;&#38454;&#27573;&#35757;&#32451;&#27169;&#22411;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24615;&#33021;&#26041;&#27861;&#12290;QFNN-FFD&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#65292;&#26631;&#24535;&#30528;&#37329;&#34701;&#31185;&#25216;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#22823;&#36827;&#27493;&#65292;&#24182;&#20026;&#20197;&#38544;&#31169;&#20026;&#37325;&#28857;&#30340;&#27450;&#35784;&#26816;&#27979;&#24314;&#31435;&#20102;&#26032;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02595v1 Announce Type: cross  Abstract: This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection. Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15855</link><description>&lt;p&gt;
&#21021;&#22987;&#20540;&#21644;&#25299;&#25169;&#32467;&#26500;&#22312;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initialisation and Topology Effects in Decentralised Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15855
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23436;&#20840;&#20998;&#25955;&#24335;&#29305;&#24449;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22312;&#32593;&#32476;&#19978;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#23545;&#20010;&#20307;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#26412;&#22320;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#24615;&#65292;&#28040;&#38500;&#20102;&#21333;&#28857;&#25925;&#38556;&#21644;&#20013;&#22830;&#21327;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#19968;&#20010;&#31616;&#21270;&#30340;&#25968;&#20540;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#26089;&#26399;&#34892;&#20026;&#65292;&#20351;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20540;&#31574;&#30053;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#19979;&#30340;&#27604;&#20363;&#34892;&#20026;&#21644;&#29615;&#22659;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26356;&#22810;&#30740;&#31350;&#25171;&#24320;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15855v1 Announce Type: cross  Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for mor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20419;&#36827;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#65288;F3&#65289;&#26469;&#20013;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25935;&#24863;&#20559;&#35265;&#65292;&#36827;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.12474</link><description>&lt;p&gt;
FairSIN&#65306;&#36890;&#36807;&#25935;&#24863;&#20449;&#24687;&#20013;&#21644;&#23454;&#29616;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
FairSIN: Achieving Fairness in Graph Neural Networks through Sensitive Information Neutralization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20419;&#36827;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#65288;F3&#65289;&#26469;&#20013;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25935;&#24863;&#20559;&#35265;&#65292;&#36827;&#32780;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#36827;&#34892;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#20294;&#19982;&#20854;&#20182;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19968;&#26679;&#65292;GNNs&#20063;&#23481;&#26131;&#26681;&#25454;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#31181;&#26063;&#21644;&#24615;&#21035;&#65289;&#20570;&#20986;&#26377;&#20559;&#35265;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#20844;&#24179;&#32771;&#34385;&#65292;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#20986;&#20174;&#36755;&#20837;&#25110;&#34920;&#31034;&#20013;&#36807;&#28388;&#25481;&#25935;&#24863;&#20449;&#24687;&#65292;&#20363;&#22914;&#21024;&#38500;&#36793;&#25110;&#23631;&#34109;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#27492;&#31867;&#36807;&#28388;&#31574;&#30053;&#21487;&#33021;&#20063;&#20250;&#36807;&#28388;&#25481;&#19968;&#20123;&#38750;&#25935;&#24863;&#30340;&#29305;&#24449;&#20449;&#24687;&#65292;&#23548;&#33268;&#22312;&#39044;&#27979;&#24615;&#33021;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#20135;&#29983;&#27425;&#20248;&#30340;&#26435;&#34913;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21019;&#26032;&#30340;&#20013;&#21644;&#22522;&#30784;&#33539;&#24335;&#65292;&#21363;&#22312;&#20449;&#24687;&#20256;&#36882;&#20043;&#21069;&#23558;&#39069;&#22806;&#30340;&#20419;&#36827;&#20844;&#24179;&#24615;&#30340;&#29305;&#24449;&#65288;F3&#65289;&#32435;&#20837;&#33410;&#28857;&#29305;&#24449;&#25110;&#34920;&#31034;&#20013;&#12290;&#36825;&#20123;F3&#39044;&#26399;&#22312;&#32479;&#35745;&#19978;&#20013;&#21644;&#33410;&#28857;&#34920;&#31034;&#20013;&#30340;&#25935;&#24863;&#20559;&#35265;&#65292;&#24182;&#25552;&#20379;&#39069;&#22806;&#30340;&#38750;&#25935;&#24863;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12474v1 Announce Type: new  Abstract: Despite the remarkable success of graph neural networks (GNNs) in modeling graph-structured data, like other machine learning models, GNNs are also susceptible to making biased predictions based on sensitive attributes, such as race and gender. For fairness consideration, recent state-of-the-art (SOTA) methods propose to filter out sensitive information from inputs or representations, e.g., edge dropping or feature masking. However, we argue that such filtering-based strategies may also filter out some non-sensitive feature information, leading to a sub-optimal trade-off between predictive performance and fairness. To address this issue, we unveil an innovative neutralization-based paradigm, where additional Fairness-facilitating Features (F3) are incorporated into node features or representations before message passing. The F3 are expected to statistically neutralize the sensitive bias in node representations and provide additional nons
&lt;/p&gt;</description></item><item><title>SMOTE&#26159;&#19968;&#31181;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#29992;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;SMOTE&#30340;&#23494;&#24230;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#36793;&#30028;&#38468;&#36817;&#36880;&#28176;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;BorderLine SMOTE&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#19982;&#20854;&#20182;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;SMOTE&#12289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03819</link><description>&lt;p&gt;
SMOTE&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#30740;&#31350;&#65306;&#20851;&#20110;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#30340;&#38480;&#21046;&#21644;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Theoretical and experimental study of SMOTE: limitations and comparisons of rebalancing strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03819
&lt;/p&gt;
&lt;p&gt;
SMOTE&#26159;&#19968;&#31181;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#24120;&#29992;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#65292;&#23427;&#36890;&#36807;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#20102;SMOTE&#30340;&#23494;&#24230;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#36793;&#30028;&#38468;&#36817;&#36880;&#28176;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;BorderLine SMOTE&#31574;&#30053;&#30340;&#21512;&#29702;&#24615;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#19982;&#20854;&#20182;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#26368;&#32456;&#21457;&#29616;&#65292;&#22312;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#65292;SMOTE&#12289;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
SMOTE&#65288;Synthetic Minority Oversampling Technique&#65289;&#26159;&#22788;&#29702;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#24120;&#29992;&#30340;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#28176;&#36827;&#24773;&#20917;&#19979;&#65292;SMOTE&#65288;&#40664;&#35748;&#21442;&#25968;&#65289;&#36890;&#36807;&#31616;&#21333;&#22797;&#21046;&#21407;&#22987;&#23569;&#25968;&#26679;&#26412;&#26469;&#37325;&#26032;&#29983;&#25104;&#21407;&#22987;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22312;&#23569;&#25968;&#26679;&#26412;&#20998;&#24067;&#30340;&#25903;&#25345;&#36793;&#30028;&#38468;&#36817;&#65292;SMOTE&#30340;&#23494;&#24230;&#20250;&#20943;&#23567;&#65292;&#20174;&#32780;&#39564;&#35777;&#20102;&#24120;&#35265;&#30340;BorderLine SMOTE&#31574;&#30053;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;SMOTE&#30456;&#20851;&#31574;&#30053;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#30340;&#37325;&#26032;&#24179;&#34913;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21482;&#26377;&#24403;&#25968;&#25454;&#38598;&#26497;&#24230;&#19981;&#24179;&#34913;&#26102;&#25165;&#38656;&#35201;&#37325;&#26032;&#24179;&#34913;&#31574;&#30053;&#12290;&#23545;&#20110;&#36825;&#31181;&#25968;&#25454;&#38598;&#65292;SMOTE&#12289;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#25110;&#27424;&#37319;&#26679;&#31243;&#24207;&#26159;&#26368;&#20339;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic Minority Oversampling Technique (SMOTE) is a common rebalancing strategy for handling imbalanced data sets. Asymptotically, we prove that SMOTE (with default parameter) regenerates the original distribution by simply copying the original minority samples. We also prove that SMOTE density vanishes near the boundary of the support of the minority distribution, therefore justifying the common BorderLine SMOTE strategy. Then we introduce two new SMOTE-related strategies, and compare them with state-of-the-art rebalancing procedures. We show that rebalancing strategies are only required when the data set is highly imbalanced. For such data sets, SMOTE, our proposals, or undersampling procedures are the best strategies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26377;&#21521;&#22270;&#27169;&#22411;&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#21407;&#29702;&#21644;&#36830;&#25509;&#65292;&#20026;&#38750;&#19987;&#19994;&#32479;&#35745;&#29289;&#29702;&#39046;&#22495;&#30340;&#35835;&#32773;&#25552;&#20379;&#20102;&#26356;&#23481;&#26131;&#29702;&#35299;&#30340;&#20171;&#32461;&#12290;</title><link>http://arxiv.org/abs/2401.06281</link><description>&lt;p&gt;
&#25581;&#31192;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Demystifying Variational Diffusion Models. (arXiv:2401.06281v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06281
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20351;&#29992;&#26377;&#21521;&#22270;&#27169;&#22411;&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#21407;&#29702;&#65292;&#25581;&#31034;&#20102;&#21464;&#20998;&#25193;&#25955;&#27169;&#22411;&#30340;&#21407;&#29702;&#21644;&#36830;&#25509;&#65292;&#20026;&#38750;&#19987;&#19994;&#32479;&#35745;&#29289;&#29702;&#39046;&#22495;&#30340;&#35835;&#32773;&#25552;&#20379;&#20102;&#26356;&#23481;&#26131;&#29702;&#35299;&#30340;&#20171;&#32461;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25193;&#25955;&#27169;&#22411;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#20294;&#23545;&#20110;&#38750;&#24179;&#34913;&#32479;&#35745;&#29289;&#29702;&#39046;&#22495;&#30340;&#21021;&#23398;&#32773;&#26469;&#35828;&#65292;&#23545;&#35813;&#27169;&#22411;&#31867;&#30340;&#28145;&#20837;&#29702;&#35299;&#20173;&#28982;&#26377;&#20123;&#22256;&#38590;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#26377;&#21521;&#22270;&#27169;&#22411;&#21644;&#21464;&#20998;&#36125;&#21494;&#26031;&#21407;&#29702;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#25105;&#20204;&#35748;&#20026;&#26356;&#31616;&#21333;&#26131;&#25026;&#30340;&#25193;&#25955;&#27169;&#22411;&#20171;&#32461;&#65292;&#36825;&#23545;&#20110;&#19968;&#33324;&#35835;&#32773;&#26469;&#35828;&#38656;&#35201;&#30340;&#20808;&#20915;&#26465;&#20214;&#30456;&#23545;&#36739;&#23569;&#12290;&#25105;&#20204;&#30340;&#38416;&#36848;&#26500;&#25104;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25216;&#26415;&#32508;&#36848;&#65292;&#20174;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#31561;&#22522;&#26412;&#27010;&#24565;&#21040;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#31867;&#20043;&#38388;&#30340;&#29702;&#35770;&#32852;&#31995;&#12290;&#25105;&#20204;&#23613;&#21487;&#33021;&#22320;&#25552;&#20379;&#20102;&#22312;&#21021;&#22987;&#24037;&#20316;&#20013;&#34987;&#30465;&#30053;&#30340;&#39069;&#22806;&#25968;&#23398;&#27934;&#23519;&#65292;&#20197;&#24110;&#21161;&#29702;&#35299;&#65292;&#21516;&#26102;&#36991;&#20813;&#24341;&#20837;&#26032;&#30340;&#31526;&#21495;&#34920;&#31034;&#12290;&#25105;&#20204;&#24076;&#26395;&#36825;&#31687;&#25991;&#31456;&#23545;&#20110;&#35813;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#26469;&#35828;&#65292;&#33021;&#20316;&#20026;&#19968;&#20010;&#26377;&#29992;&#30340;&#25945;&#32946;&#34917;&#20805;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the growing popularity of diffusion models, gaining a deep understanding of the model class remains somewhat elusive for the uninitiated in non-equilibrium statistical physics. With that in mind, we present what we believe is a more straightforward introduction to diffusion models using directed graphical modelling and variational Bayesian principles, which imposes relatively fewer prerequisites on the average reader. Our exposition constitutes a comprehensive technical review spanning from foundational concepts like deep latent variable models to recent advances in continuous-time diffusion-based modelling, highlighting theoretical connections between model classes along the way. We provide additional mathematical insights that were omitted in the seminal works whenever possible to aid in understanding, while avoiding the introduction of new notation. We envision this article serving as a useful educational supplement for both researchers and practitioners in the area, and we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.02960</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#38388;&#29305;&#24449;&#21387;&#32553;&#21644;&#24046;&#21035;&#24615;&#23398;&#20064;&#29702;&#35299;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination. (arXiv:2311.02960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#29305;&#24449;&#30340;&#19968;&#31181;&#39640;&#25928;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#32593;&#32476;&#22914;&#20309;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#36827;&#34892;&#31561;&#32423;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#25581;&#31034;&#36825;&#20010;&#35868;&#22242;&#12290;&#21463;&#21040;&#25105;&#20204;&#23454;&#35777;&#21457;&#29616;&#30340;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#27169;&#20223;&#38750;&#32447;&#24615;&#32593;&#32476;&#20013;&#28145;&#23618;&#30340;&#35282;&#33394;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#22914;&#20309;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#21270;&#20026;&#36755;&#20986;&#65292;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#21518;&#30340;&#27599;&#20010;&#23618;&#30340;&#36755;&#20986;&#65288;&#21363;&#29305;&#24449;&#65289;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#34913;&#37327;&#20013;&#38388;&#29305;&#24449;&#30340;&#31867;&#20869;&#21387;&#32553;&#21644;&#31867;&#38388;&#24046;&#21035;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#20174;&#27973;&#23618;&#21040;&#28145;&#23618;&#30340;&#28436;&#21464;&#36981;&#24490;&#30528;&#19968;&#31181;&#31616;&#21333;&#32780;&#37327;&#21270;&#30340;&#27169;&#24335;&#65292;&#21069;&#25552;&#26159;&#36755;&#20837;&#25968;&#25454;&#26159;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;infoGAN&#20013;&#65292;&#36776;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#26679;&#26412;&#25968;&#37327;&#36235;&#21521;&#26080;&#31351;&#26102;&#65292;&#20004;&#20010;&#30446;&#26631;&#20989;&#25968;&#21464;&#24471;&#31561;&#20215;&#12290;</title><link>http://arxiv.org/abs/2310.00443</link><description>&lt;p&gt;
infoGAN&#30340;&#20004;&#23618;&#32593;&#32476;&#30340;&#30446;&#26631;&#20989;&#25968;&#31561;&#24335;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
The objective function equality property of infoGAN for two-layer network. (arXiv:2310.00443v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00443
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;infoGAN&#20013;&#65292;&#36776;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#30340;&#26679;&#26412;&#25968;&#37327;&#36235;&#21521;&#26080;&#31351;&#26102;&#65292;&#20004;&#20010;&#30446;&#26631;&#20989;&#25968;&#21464;&#24471;&#31561;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#24687;&#26368;&#22823;&#21270;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(infoGAN)&#21487;&#20197;&#29702;&#35299;&#20026;&#28041;&#21450;&#20004;&#20010;&#32593;&#32476;(&#36776;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;)&#30340;&#26497;&#23567;&#21270;&#26497;&#22823;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#20114;&#20449;&#24687;&#20989;&#25968;&#12290;infoGAN&#21253;&#25324;&#22810;&#31181;&#32452;&#20214;&#65292;&#21253;&#25324;&#28508;&#22312;&#21464;&#37327;&#12289;&#20114;&#20449;&#24687;&#21644;&#30446;&#26631;&#20989;&#25968;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;&#22312;&#36776;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#26679;&#26412;&#25968;&#37327;&#36235;&#21521;&#26080;&#31351;&#26102;&#65292;infoGAN&#20013;&#30340;&#20004;&#20010;&#30446;&#26631;&#20989;&#25968;&#21464;&#24471;&#31561;&#20215;&#12290;&#36825;&#31181;&#31561;&#20215;&#20851;&#31995;&#26159;&#36890;&#36807;&#32771;&#34385;&#30446;&#26631;&#20989;&#25968;&#30340;&#32463;&#39564;&#29256;&#26412;&#21644;&#24635;&#20307;&#29256;&#26412;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24314;&#31435;&#30340;&#12290;&#36825;&#20010;&#24046;&#24322;&#30340;&#30028;&#38480;&#30001;&#36776;&#21035;&#22120;&#21644;&#29983;&#25104;&#22120;&#20989;&#25968;&#31867;&#30340;Rademacher&#22797;&#26434;&#24230;&#20915;&#23450;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20855;&#26377;Lipschitz&#21644;&#38750;&#36882;&#20943;&#28608;&#27963;&#20989;&#25968;&#30340;&#20004;&#23618;&#32593;&#32476;&#26469;&#39564;&#35777;&#36825;&#20010;&#31561;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Information Maximizing Generative Adversarial Network (infoGAN) can be understood as a minimax problem involving two networks: discriminators and generators with mutual information functions. The infoGAN incorporates various components, including latent variables, mutual information, and objective function. This research demonstrates that the two objective functions in infoGAN become equivalent as the discriminator and generator sample size approaches infinity. This equivalence is established by considering the disparity between the empirical and population versions of the objective function. The bound on this difference is determined by the Rademacher complexity of the discriminator and generator function class. Furthermore, the utilization of a two-layer network for both the discriminator and generator, featuring Lipschitz and non-decreasing activation functions, validates this equality
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#24212;&#29992;&#20110;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#23545;Gross-Pitaevskii&#26041;&#31243;&#30340;&#27714;&#35299;&#65292;&#21487;&#20197;&#20197;&#25509;&#36817;1000&#20493;&#30340;&#36895;&#24230;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#29366;&#24577;&#30340;&#35299;&#65292;&#20026;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.15593</link><description>&lt;p&gt;
&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#65306;&#19968;&#31181;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exciton-Polariton Condensates: A Fourier Neural Operator Approach. (arXiv:2309.15593v1 [cond-mat.quant-gas])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23558;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#24212;&#29992;&#20110;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#23454;&#29616;&#23545;Gross-Pitaevskii&#26041;&#31243;&#30340;&#27714;&#35299;&#65292;&#21487;&#20197;&#20197;&#25509;&#36817;1000&#20493;&#30340;&#36895;&#24230;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#29366;&#24577;&#30340;&#35299;&#65292;&#20026;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#30340;&#22823;&#35268;&#27169;&#24212;&#29992;&#25552;&#20379;&#20102;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#21322;&#23548;&#20307;&#21046;&#36896;&#25216;&#26415;&#30340;&#36827;&#23637;&#20652;&#29983;&#20102;&#23545;&#30001;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#39537;&#21160;&#30340;&#20840;&#20809;&#23398;&#22120;&#20214;&#30340;&#24191;&#27867;&#30740;&#31350;&#12290;&#21253;&#25324;&#26230;&#20307;&#31649;&#22312;&#20869;&#30340;&#36825;&#31867;&#22120;&#20214;&#30340;&#21021;&#27493;&#39564;&#35777;&#24050;&#32463;&#22312;&#29615;&#22659;&#26465;&#20214;&#19979;&#21462;&#24471;&#20102;&#40723;&#33310;&#20154;&#24515;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#37325;&#35201;&#30340;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#39046;&#22495;&#65306;&#32570;&#20047;&#19968;&#20010;&#20581;&#22766;&#30340;&#27714;&#35299;&#22120;&#65292;&#21487;&#20197;&#29992;&#20110;&#27169;&#25311;&#38656;&#35201;&#36739;&#38271;&#26102;&#38388;&#36798;&#21040;&#31283;&#23450;&#30340;&#22797;&#26434;&#38750;&#32447;&#24615;&#31995;&#32479;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20613;&#37324;&#21494;&#31070;&#32463;&#31639;&#23376;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#19982;&#39069;&#22806;&#28608;&#23376;&#36895;&#29575;&#26041;&#31243;&#32806;&#21512;&#30340;Gross-Pitaevskii&#26041;&#31243;&#12290;&#36825;&#39033;&#24037;&#20316;&#26631;&#24535;&#30528;&#31070;&#32463;&#31639;&#23376;&#39318;&#27425;&#30452;&#25509;&#24212;&#29992;&#20110;&#28608;&#23376;&#26497;&#21270;&#23376;&#24211;&#20262;&#20957;&#32858;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20110;CUDA&#30340;GPU&#27714;&#35299;&#22120;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#20197;&#25509;&#36817;1000&#20493;&#30340;&#36895;&#24230;&#20934;&#30830;&#39044;&#27979;&#26368;&#32456;&#29366;&#24577;&#30340;&#35299;&#12290;&#27492;&#22806;&#65292;&#36825;&#20026;&#20170;&#21518;&#30340;&#28145;&#20837;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Advancements in semiconductor fabrication over the past decade have catalyzed extensive research into all-optical devices driven by exciton-polariton condensates. Preliminary validations of such devices, including transistors, have shown encouraging results even under ambient conditions. A significant challenge still remains for large scale application however: the lack of a robust solver that can be used to simulate complex nonlinear systems which require an extended period of time to stabilize. Addressing this need, we propose the application of a machine-learning-based Fourier Neural Operator approach to find the solution to the Gross-Pitaevskii equations coupled with extra exciton rate equations. This work marks the first direct application of Neural Operators to an exciton-polariton condensate system. Our findings show that the proposed method can predict final-state solutions to a high degree of accuracy almost 1000 times faster than CUDA-based GPU solvers. Moreover, this paves t
&lt;/p&gt;</description></item><item><title>SegMatch&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#20266;&#26631;&#31614;&#26469;&#20943;&#23569;&#26114;&#36149;&#30340;&#27880;&#37322;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#24369;&#22686;&#24378;&#21644;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#23454;&#29616;&#26080;&#30417;&#30563;&#25439;&#22833;&#30340;&#26045;&#21152;&#12290;</title><link>http://arxiv.org/abs/2308.05232</link><description>&lt;p&gt;
SegMatch: &#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SegMatch: A semi-supervised learning method for surgical instrument segmentation. (arXiv:2308.05232v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05232
&lt;/p&gt;
&lt;p&gt;
SegMatch&#26159;&#19968;&#31181;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#20266;&#26631;&#31614;&#26469;&#20943;&#23569;&#26114;&#36149;&#30340;&#27880;&#37322;&#38656;&#27714;&#65292;&#24182;&#36890;&#36807;&#24369;&#22686;&#24378;&#21644;&#29983;&#25104;&#20266;&#26631;&#31614;&#26469;&#23454;&#29616;&#26080;&#30417;&#30563;&#25439;&#22833;&#30340;&#26045;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#34987;&#35748;&#20026;&#26159;&#25552;&#20379;&#20808;&#36827;&#25163;&#26415;&#36741;&#21161;&#21644;&#25913;&#21892;&#35745;&#31639;&#26426;&#36741;&#21161;&#24178;&#39044;&#30340;&#20851;&#38190;&#25163;&#27573;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SegMatch&#65292;&#19968;&#31181;&#29992;&#20110;&#20943;&#23569;&#26114;&#36149;&#27880;&#37322;&#23545;&#33145;&#33108;&#38236;&#21644;&#26426;&#22120;&#20154;&#25163;&#26415;&#22270;&#20687;&#30340;&#38656;&#27714;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;SegMatch&#22522;&#20110;FixMatch&#65292;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#20266;&#26631;&#31614;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;&#27969;&#31243;&#65292;&#24182;&#23558;&#20854;&#35843;&#25972;&#20026;&#20998;&#21106;&#20219;&#21153;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;SegMatch&#20013;&#65292;&#26410;&#26631;&#35760;&#30340;&#22270;&#20687;&#36827;&#34892;&#24369;&#22686;&#24378;&#65292;&#24182;&#36890;&#36807;&#20998;&#21106;&#27169;&#22411;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#20197;&#23545;&#39640;&#32622;&#20449;&#24230;&#20687;&#32032;&#19978;&#30340;&#23545;&#25239;&#22686;&#24378;&#22270;&#20687;&#30340;&#27169;&#22411;&#36755;&#20986;&#26045;&#21152;&#26080;&#30417;&#30563;&#25439;&#22833;&#12290;&#25105;&#20204;&#38024;&#23545;&#20998;&#21106;&#20219;&#21153;&#30340;&#35843;&#25972;&#36824;&#21253;&#25324;&#20180;&#32454;&#32771;&#34385;&#25152;&#20381;&#36182;&#30340;&#22686;&#24378;&#20989;&#25968;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#23646;&#24615;&#65292;&#20026;&#22686;&#24378;&#30340;&#30456;&#20851;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;
Surgical instrument segmentation is recognised as a key enabler to provide advanced surgical assistance and improve computer assisted interventions. In this work, we propose SegMatch, a semi supervised learning method to reduce the need for expensive annotation for laparoscopic and robotic surgical images. SegMatch builds on FixMatch, a widespread semi supervised classification pipeline combining consistency regularization and pseudo labelling, and adapts it for the purpose of segmentation. In our proposed SegMatch, the unlabelled images are weakly augmented and fed into the segmentation model to generate a pseudo-label to enforce the unsupervised loss against the output of the model for the adversarial augmented image on the pixels with a high confidence score. Our adaptation for segmentation tasks includes carefully considering the equivariance and invariance properties of the augmentation functions we rely on. To increase the relevance of our augmentations, we depart from using only
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;, &#20854;&#23398;&#20064;&#26426;&#21046;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2306.01271</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#20250;&#21516;&#26102;&#20986;&#29616;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#65311;
&lt;/p&gt;
&lt;p&gt;
Why Clean Generalization and Robust Overfitting Both Happen in Adversarial Training. (arXiv:2306.01271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01271
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;, &#20854;&#23398;&#20064;&#26426;&#21046;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25239;&#20987;&#23545;&#25239;&#25200;&#21160;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#19982;&#22312;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#29615;&#22659;&#20013;&#20986;&#29616;&#24778;&#20154;&#30340;&#24178;&#20928;&#27867;&#21270;&#33021;&#21147;&#31867;&#20284;&#65292;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#20063;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#21040;&#26410;&#35265;&#36807;&#30340;&#24178;&#20928;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#19982;&#24178;&#20928;&#27867;&#21270;&#19981;&#21516;&#30340;&#26159;&#65292;&#23613;&#31649;&#23545;&#25239;&#35757;&#32451;&#33021;&#22815;&#23454;&#29616;&#20302;&#40065;&#26834;&#35757;&#32451;&#35823;&#24046;&#65292;&#20173;&#23384;&#22312;&#26174;&#33879;&#30340;&#40065;&#26834;&#27867;&#21270;&#36317;&#31163;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#25506;&#32034;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#30340;&#26426;&#21046;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23545;&#25239;&#35757;&#32451;&#20013;&#36825;&#31181;&#29616;&#35937;&#30340;&#29702;&#35770;&#29702;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#25239;&#35757;&#32451;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20998;&#26512;&#20102;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#65292;&#35299;&#37322;&#20102;&#23545;&#25239;&#35757;&#32451;&#22914;&#20309;&#23548;&#33268;&#32593;&#32476;&#23398;&#20064;&#32773;&#36827;&#20837;&#21040;&#24178;&#20928;&#27867;&#21270;&#21644;&#24378;&#20581;&#36807;&#25311;&#21512;&#29366;&#24577;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#36890;&#36807;&#36843;&#20351;&#23398;&#20064;&#22120;&#25104;&#20026;&#24378;&#39044;&#27979;&#32593;&#32476;&#65292;&#23545;&#25239;&#35757;&#32451;&#23558;&#23548;&#33268;&#24178;&#20928;&#27867;&#21270;&#21644;&#40065;&#26834;&#36807;&#25311;&#21512;&#29616;&#35937;&#21516;&#26102;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial training is a standard method to train deep neural networks to be robust to adversarial perturbation. Similar to surprising $\textit{clean generalization}$ ability in the standard deep learning setting, neural networks trained by adversarial training also generalize well for $\textit{unseen clean data}$. However, in constrast with clean generalization, while adversarial training method is able to achieve low $\textit{robust training error}$, there still exists a significant $\textit{robust generalization gap}$, which promotes us exploring what mechanism leads to both $\textit{clean generalization and robust overfitting (CGRO)}$ during learning process. In this paper, we provide a theoretical understanding of this CGRO phenomenon in adversarial training. First, we propose a theoretical framework of adversarial training, where we analyze $\textit{feature learning process}$ to explain how adversarial training leads network learner to CGRO regime. Specifically, we prove that, u
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; PNKA&#65292;&#19968;&#31181;&#21487;&#20197;&#37327;&#21270;&#21333;&#20010;&#36755;&#20837;&#22312;&#20004;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20840;&#23616;&#30456;&#20284;&#24615;&#24230;&#37327;&#19981;&#33021;&#23616;&#37096;&#35843;&#26597;&#34920;&#31034;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.19294</link><description>&lt;p&gt;
&#20010;&#21035;&#28857;&#34920;&#31034;&#30456;&#20284;&#24615;
&lt;/p&gt;
&lt;p&gt;
Pointwise Representational Similarity. (arXiv:2305.19294v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19294
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; PNKA&#65292;&#19968;&#31181;&#21487;&#20197;&#37327;&#21270;&#21333;&#20010;&#36755;&#20837;&#22312;&#20004;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#22635;&#34917;&#20102;&#20840;&#23616;&#30456;&#20284;&#24615;&#24230;&#37327;&#19981;&#33021;&#23616;&#37096;&#35843;&#26597;&#34920;&#31034;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20381;&#36182;&#24615;&#36234;&#26469;&#36234;&#22823;&#65292;&#21457;&#23637;&#26356;&#22909;&#22320;&#29702;&#35299;&#23427;&#20204;&#25152;&#23398;&#34920;&#31034;&#26041;&#24335;&#30340;&#26041;&#27861;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34920;&#24449;&#30456;&#20284;&#24615;&#24230;&#37327;&#24050;&#32463;&#25104;&#20026;&#20102;&#19968;&#31181;&#24120;&#29992;&#24037;&#20855;&#65292;&#29992;&#20110;&#26816;&#26597;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24230;&#37327;&#21482;&#33021;&#22312;&#20840;&#23616;&#23618;&#38754;&#19978;&#23545;&#30456;&#20284;&#24615;&#36827;&#34892;&#27719;&#24635;&#20272;&#35745;&#65292;&#21363;&#22312;N&#20010;&#36755;&#20837;&#31034;&#20363;&#30340;&#19968;&#32452;&#34920;&#31034;&#20013;&#36827;&#34892;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#24230;&#37327;&#19981;&#36866;&#21512;&#20110;&#22312;&#23616;&#37096;&#23618;&#38754;&#19978;&#35843;&#26597;&#34920;&#31034;&#65292;&#21363;&#21333;&#20010;&#36755;&#20837;&#31034;&#20363;&#30340;&#34920;&#31034;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#38656;&#35201;&#23616;&#37096;&#30456;&#20284;&#24615;&#24230;&#37327;&#65292;&#20197;&#20102;&#35299;&#21738;&#20123;&#21333;&#20010;&#36755;&#20837;&#34920;&#31034;&#21463;&#21040;&#20102;&#27169;&#22411;&#35757;&#32451;&#24178;&#39044;&#30340;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#26356;&#20844;&#24179;&#21644;&#26080;&#20559;&#65289;&#25110;&#26356;&#23481;&#26131;&#34987;&#38169;&#35823;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#24182;&#25552;&#20986;&#20102;PNKA&#65288;Pointwise Normalized Kernel Alignment&#65289;&#65292;&#23427;&#23545;&#27604;&#24230;&#37327;&#20102;&#22312;&#20004;&#20010;&#34920;&#31034;&#31354;&#38388;&#20013;&#19968;&#20010;&#20010;&#21035;&#36755;&#20837;&#30340;&#34920;&#31034;&#30456;&#20284;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing reliance on deep neural networks, it is important to develop ways to better understand their learned representations. Representation similarity measures have emerged as a popular tool for examining learned representations However, existing measures only provide aggregate estimates of similarity at a global level, i.e. over a set of representations for N input examples. As such, these measures are not well-suited for investigating representations at a local level, i.e. representations of a single input example. Local similarity measures are needed, for instance, to understand which individual input representations are affected by training interventions to models (e.g. to be more fair and unbiased) or are at greater risk of being misclassified. In this work, we fill in this gap and propose Pointwise Normalized Kernel Alignment (PNKA), a measure that quantifies how similarly an individual input is represented in two representation spaces. Intuitively, PNKA compares the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.06058</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#25968;&#32423;&#21035;&#30340;&#23569;&#37327;&#21464;&#20998;&#21442;&#25968;&#30340;&#24352;&#37327;&#32593;&#32476;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compressing neural network by tensor network with exponentially fewer variational parameters. (arXiv:2305.06058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25152;&#21253;&#21547;&#30340;&#24040;&#22823;&#21487;&#21464;&#30340;&#21442;&#25968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36825;&#20123;&#21442;&#25968; encoding &#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#26696;&#28436;&#31034;&#20102;&#20986;&#33394;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#27973;&#23618;&#24352;&#37327;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;VGG-16&#20013;&#30340;3&#20010;&#21367;&#31215;&#23618;&#30340;&#22823;&#32422;1000&#19975;&#21442;&#25968;&#34987;&#21387;&#32553;&#21040;&#20855;&#26377;&#20165;632&#20010;&#21442;&#25968;&#30340;TN&#20013;&#65292;&#32780;&#22312;CIFAR-10&#19978;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#20196;&#20154;&#24778;&#21916;&#22320;&#25552;&#39640;&#20102;81.14&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including over-fitting, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN by encoding them to multi-layer tensor networks (TN's) that contain exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, and VGG-16) and datasets (MNIST and CIFAR-10), surpassing the state-of-the-art method based on shallow tensor networks. For instance, about 10 million parameters in the three convolutional layers of VGG-16 are compressed in TN's with just $632$ parameters, while the testing accuracy on CIFAR-10 is surprisingly improved from $81.14
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20256;&#32479;&#30340;LS&#65292;&#20294;&#20063;&#21487;&#20197;&#24314;&#27169;&#23454;&#20363;&#29305;&#23450;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#27861;&#65288;LABO&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#26631;&#31614;&#27491;&#21017;&#21270;&#65292;&#24182;&#24471;&#21040;&#20102;&#21487;&#35299;&#37322;&#30340;&#26368;&#20248;&#26631;&#31614;&#24179;&#28369;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.04971</link><description>&lt;p&gt;
LABO: &#36890;&#36807;&#21452;&#23618;&#20248;&#21270;&#23454;&#29616;&#26368;&#20339;&#26631;&#31614;&#27491;&#21017;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LABO: Towards Learning Optimal Label Regularization via Bi-level Optimization. (arXiv:2305.04971v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#25324;&#20256;&#32479;&#30340;LS&#65292;&#20294;&#20063;&#21487;&#20197;&#24314;&#27169;&#23454;&#20363;&#29305;&#23450;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#20248;&#21270;&#30340;&#26041;&#27861;&#65288;LABO&#65289;&#65292;&#29992;&#20110;&#23398;&#20064;&#26631;&#31614;&#27491;&#21017;&#21270;&#65292;&#24182;&#24471;&#21040;&#20102;&#21487;&#35299;&#37322;&#30340;&#26368;&#20248;&#26631;&#31614;&#24179;&#28369;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27491;&#21017;&#21270;&#25216;&#26415;&#23545;&#20110;&#25913;&#21892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#35757;&#32451;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#12290;&#35768;&#22810;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#20381;&#36182;&#20110;&#26435;&#37325;&#34928;&#20943;&#12289;&#20002;&#24323;&#12289;&#25209;/&#23618;&#24402;&#19968;&#21270;&#31561;&#25216;&#26415;&#26469;&#26356;&#24555;&#22320;&#25910;&#25947;&#21644;&#27867;&#21270;&#12290;&#26631;&#31614;&#24179;&#28369;&#65288;LS&#65289;&#26159;&#21478;&#19968;&#31181;&#31616;&#21333;&#12289;&#36890;&#29992;&#19988;&#39640;&#25928;&#30340;&#27491;&#21017;&#21270;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;LS&#20551;&#35774;&#27599;&#20010;&#38750;&#30446;&#26631;&#31867;&#21035;&#20986;&#29616;&#30340;&#27010;&#29575;&#30456;&#31561;&#65292;&#19981;&#33021;&#26681;&#25454;&#23454;&#20363;&#23545;&#26631;&#31614;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21253;&#25324;&#20256;&#32479;&#30340;LS&#20294;&#20063;&#21487;&#20197;&#24314;&#27169;&#23454;&#20363;&#29305;&#23450;&#30340;&#21464;&#20307;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#35774;&#35745;&#21452;&#23618;&#20248;&#21270;&#65288;LABO&#65289;&#38382;&#39064;&#26469;&#23398;&#20064;&#26631;&#31614;&#27491;&#21017;&#21270;&#30340;&#39640;&#25928;&#26041;&#27861;&#12290;&#25105;&#20204;&#24471;&#20986;&#20102;&#20869;&#29615;&#33410;&#30340;&#30830;&#23450;&#24615;&#21644;&#21487;&#35299;&#37322;&#35299;&#65292;&#32780;&#26080;&#38656;&#23384;&#20648;&#32463;&#36807;&#35757;&#32451;&#27169;&#22411;&#30340;&#21442;&#25968;&#25110;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Regularization techniques are crucial to improving the generalization performance and training efficiency of deep neural networks. Many deep learning algorithms rely on weight decay, dropout, batch/layer normalization to converge faster and generalize. Label Smoothing (LS) is another simple, versatile and efficient regularization which can be applied to various supervised classification tasks. Conventional LS, however, regardless of the training instance assumes that each non-target class is equally likely. In this work, we present a general framework for training with label regularization, which includes conventional LS but can also model instance-specific variants. Based on this formulation, we propose an efficient way of learning LAbel regularization by devising a Bi-level Optimization (LABO) problem. We derive a deterministic and interpretable solution of the inner loop as the optimal label smoothing without the need to store the parameters or the output of a trained model. Finally
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#65292;&#24403;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#26102;&#65292;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#26041;&#27861;&#20248;&#20110;&#38598;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#21270;&#38388;&#38553;&#30340;&#28176;&#36827;&#20248;&#21183;&#30340;&#22343;&#20540;&#65292;&#25152;&#26377;&#20854;&#20182;&#26102;&#21051;&#21644;&#25972;&#20010;&#28176;&#36827;&#20998;&#24067;&#12290;</title><link>http://arxiv.org/abs/2304.06833</link><description>&lt;p&gt;
&#35780;&#20272;-&#20248;&#21270;&#26041;&#27861;&#19982;&#38598;&#25104;&#35780;&#20272;&#20248;&#21270;&#27861;&#65306;&#22522;&#20110;&#38543;&#26426;&#20248;&#21183;&#30340;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
Estimate-Then-Optimize Versus Integrated-Estimation-Optimization: A Stochastic Dominance Perspective. (arXiv:2304.06833v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#65292;&#24403;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#26102;&#65292;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#26041;&#27861;&#20248;&#20110;&#38598;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#20248;&#21270;&#38388;&#38553;&#30340;&#28176;&#36827;&#20248;&#21183;&#30340;&#22343;&#20540;&#65292;&#25152;&#26377;&#20854;&#20182;&#26102;&#21051;&#21644;&#25972;&#20010;&#28176;&#36827;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#38543;&#26426;&#20248;&#21270;&#20013;&#65292;&#38500;&#20102;&#38656;&#35201;&#20248;&#21270;&#20219;&#21153;&#65292;&#36824;&#38656;&#35201;&#20174;&#25968;&#25454;&#20013;&#20272;&#35745;&#28508;&#22312;&#20998;&#24067;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;&#26368;&#36817;&#30340;&#25991;&#29486;&#34920;&#26126;&#65292;&#36890;&#36807;&#36873;&#25321;&#23548;&#33268;&#26368;&#20339;&#32463;&#39564;&#30446;&#26631;&#24615;&#33021;&#30340;&#27169;&#22411;&#21442;&#25968;&#65292;&#21487;&#20197;&#38598;&#25104;&#20272;&#35745;&#21644;&#20248;&#21270;&#36807;&#31243;&#12290;&#24403;&#27169;&#22411;&#34987;&#38169;&#35823;&#22320;&#25351;&#23450;&#26102;&#65292;&#36825;&#31181;&#38598;&#25104;&#26041;&#27861;&#21487;&#20197;&#24456;&#23481;&#26131;&#22320;&#26174;&#31034;&#20986;&#20248;&#20110;&#31616;&#21333;&#30340;&#8220;&#20808;&#20272;&#35745;&#20877;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#22312;&#27169;&#22411;&#31867;&#36275;&#22815;&#20016;&#23500;&#20197;&#28085;&#30422;&#30495;&#23454;&#24773;&#20917;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#38382;&#39064;&#65292;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#24615;&#33021;&#25490;&#24207;&#22312;&#24378;&#28872;&#30340;&#24847;&#20041;&#19979;&#34987;&#39072;&#20498;&#12290;&#22312;&#21463;&#38480;&#26465;&#20214;&#21644;&#24403;&#19978;&#19979;&#25991;&#29305;&#24449;&#21487;&#29992;&#26102;&#65292;&#31867;&#20284;&#30340;&#32467;&#26524;&#20063;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
In data-driven stochastic optimization, model parameters of the underlying distribution need to be estimated from data in addition to the optimization task. Recent literature suggests the integration of the estimation and optimization processes, by selecting model parameters that lead to the best empirical objective performance. Such an integrated approach can be readily shown to outperform simple ``estimate then optimize" when the model is misspecified. In this paper, we argue that when the model class is rich enough to cover the ground truth, the performance ordering between the two approaches is reversed for nonlinear problems in a strong sense. Simple ``estimate then optimize" outperforms the integrated approach in terms of stochastic dominance of the asymptotic optimality gap, i,e, the mean, all other moments, and the entire asymptotic distribution of the optimality gap is always better. Analogous results also hold under constrained settings and when contextual features are availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20855;&#26377;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#22312;&#26631;&#20934;&#32852;&#21512;&#20219;&#21153;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;&#65292;&#20197;&#21450;&#22312;&#26377;&#36259;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#21487;&#20197;&#25552;&#20379;&#31867;&#20284;&#20110;&#32463;&#20856;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.10032</link><description>&lt;p&gt;
&#20855;&#26377;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Communication-Efficient Federated Learning With Data and Client Heterogeneity. (arXiv:2206.10032v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.10032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#20855;&#26377;&#25968;&#25454;&#21644;&#23458;&#25143;&#31471;&#24322;&#36136;&#24615;&#30340;&#36890;&#20449;&#39640;&#25928;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#20854;&#22312;&#26631;&#20934;&#32852;&#21512;&#20219;&#21153;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;&#65292;&#20197;&#21450;&#22312;&#26377;&#36259;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#21487;&#20197;&#25552;&#20379;&#31867;&#20284;&#20110;&#32463;&#20856;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20801;&#35768;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22823;&#35268;&#27169;&#20998;&#24067;&#24335;&#35757;&#32451;&#65292;&#21516;&#26102;&#20173;&#20801;&#35768;&#21508;&#20010;&#33410;&#28857;&#20445;&#25345;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36827;&#34892;&#22823;&#35268;&#27169;FL&#26102;&#23384;&#22312;&#22266;&#26377;&#30340;&#23454;&#29992;&#25361;&#25112;&#65306;1&#65289;&#23616;&#37096;&#33410;&#28857;&#25968;&#25454;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#65292;2&#65289;&#33410;&#28857;&#35745;&#31639;&#36895;&#24230;&#65288;&#24322;&#27493;&#24615;&#65289;&#30340;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;3&#65289;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#36890;&#20449;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#31181;&#32463;&#20856;&#32852;&#37030;&#24179;&#22343;&#31639;&#27861;&#65288;FedAvg&#65289;&#30340;&#21464;&#20307;&#65292;&#21516;&#26102;&#25903;&#25345;&#25968;&#25454;&#24322;&#36136;&#24615;&#12289;&#37096;&#20998;&#23458;&#25143;&#31471;&#24322;&#27493;&#24615;&#21644;&#36890;&#20449;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#25552;&#20379;&#20102;&#20005;&#23494;&#30340;&#20998;&#26512;&#65292;&#34920;&#26126;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#31995;&#32479;&#25918;&#23485;&#65292;&#23427;&#20173;&#28982;&#21487;&#20197;&#22312;&#26377;&#36259;&#30340;&#21442;&#25968;&#33539;&#22260;&#20869;&#25552;&#20379;&#31867;&#20284;&#20110;FedAvg&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#22810;&#36798;300&#20010;&#33410;&#28857;&#30340;&#20005;&#26684;LEAF&#22522;&#20934;&#27979;&#35797;&#26041;&#26696;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#30830;&#20445;&#20102;&#26631;&#20934;&#32852;&#21512;&#20219;&#21153;&#30340;&#24555;&#36895;&#25910;&#25947;&#65292;&#25552;&#39640;&#20102;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) enables large-scale distributed training of machine learning models, while still allowing individual nodes to maintain data locally.  However, executing FL at scale comes with inherent practical challenges:  1) heterogeneity of the local node data distributions,  2) heterogeneity of node computational speeds (asynchrony),  but also 3) constraints in the amount of communication between the clients and the server.  In this work, we present the first variant of the classic federated averaging (FedAvg) algorithm  which, at the same time, supports data heterogeneity, partial client asynchrony, and communication compression.  Our algorithm comes with a rigorous analysis showing that, in spite of these system relaxations,  it can provide similar convergence to FedAvg in interesting parameter regimes.  Experimental results in the rigorous LEAF benchmark on setups of up to $300$ nodes show that our algorithm ensures fast convergence for standard federated tasks, improvin
&lt;/p&gt;</description></item></channel></rss>