<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#39044;&#27979;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#30340;&#39057;&#29575;&#20869;&#23481;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#21644;&#23569;&#21463;&#35797;&#32773;&#24773;&#20917;&#19979;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.08592</link><description>&lt;p&gt;
&#29992;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#39044;&#35757;&#32451;&#23454;&#29616;&#39640;&#25928;&#30340;&#30561;&#30496;&#20998;&#26399;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Sleep Staging with Synthetic Time Series Pretraining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08592
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39044;&#27979;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#30340;&#39057;&#29575;&#20869;&#23481;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#38480;&#25968;&#25454;&#21644;&#23569;&#21463;&#35797;&#32773;&#24773;&#20917;&#19979;&#36229;&#36234;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#26102;&#38388;&#24207;&#21015;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#30001;&#20110;&#20154;&#31867;&#21463;&#35797;&#32773;&#20043;&#38388;&#30340;&#22823;&#37327;&#21464;&#24322;&#21644;&#36890;&#24120;&#35268;&#27169;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#31574;&#30053;&#65292;&#20363;&#22914;&#33258;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#24191;&#27867;&#30340;&#23454;&#35777;&#25968;&#25454;&#38598;&#12290;&#21463;&#35745;&#31639;&#26426;&#35270;&#35273;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#31216;&#20026;&#8220;&#39057;&#29575;&#39044;&#35757;&#32451;&#8221;&#65292;&#36890;&#36807;&#39044;&#27979;&#38543;&#26426;&#29983;&#25104;&#30340;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#30340;&#39057;&#29575;&#20869;&#23481;&#26469;&#20026;&#30561;&#30496;&#20998;&#26399;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#38480;&#25968;&#25454;&#21644;&#23569;&#21463;&#35797;&#32773;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#23436;&#20840;&#30417;&#30563;&#23398;&#20064;&#65292;&#24182;&#22312;&#35768;&#22810;&#21463;&#35797;&#32773;&#30340;&#24773;&#22659;&#20013;&#34920;&#29616;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#24378;&#35843;&#20102;&#39057;&#29575;&#20449;&#24687;&#23545;&#20110;&#30561;&#30496;&#20998;&#26399;&#35780;&#20998;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#34920;&#26126;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#20102;&#36229;&#20986;&#39057;&#29575;&#20449;&#24687;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08592v1 Announce Type: new  Abstract: Analyzing electroencephalographic (EEG) time series can be challenging, especially with deep neural networks, due to the large variability among human subjects and often small datasets. To address these challenges, various strategies, such as self-supervised learning, have been suggested, but they typically rely on extensive empirical datasets. Inspired by recent advances in computer vision, we propose a pretraining task termed "frequency pretraining" to pretrain a neural network for sleep staging by predicting the frequency content of randomly generated synthetic time series. Our experiments demonstrate that our method surpasses fully supervised learning in scenarios with limited data and few subjects, and matches its performance in regimes with many subjects. Furthermore, our results underline the relevance of frequency information for sleep stage scoring, while also demonstrating that deep neural networks utilize information beyond fr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21152;&#26435;&#28966;&#28857;&#21487;&#24494;MCC&#65292;&#29992;&#20110;&#25913;&#21892;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#39044;&#27979;&#21754;&#20083;&#21160;&#29289;&#34507;&#30333;&#36136;&#20013;&#30340;O-GlcNAcylation&#20301;&#28857;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;</title><link>https://arxiv.org/abs/2402.17131</link><description>&lt;p&gt;
&#20351;&#29992;Transformer&#21644;RNN&#22312;&#32463;&#36807;&#35757;&#32451;&#30340;&#26032;&#25439;&#22833;&#20989;&#25968;&#19979;&#39044;&#27979;&#21754;&#20083;&#21160;&#29289;&#34507;&#30333;&#36136;&#20013;&#30340;O-GlcNAcylation&#20301;&#28857;
&lt;/p&gt;
&lt;p&gt;
Predicting O-GlcNAcylation Sites in Mammalian Proteins with Transformers and RNNs Trained with a New Loss Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17131
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21152;&#26435;&#28966;&#28857;&#21487;&#24494;MCC&#65292;&#29992;&#20110;&#25913;&#21892;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#39044;&#27979;&#21754;&#20083;&#21160;&#29289;&#34507;&#30333;&#36136;&#20013;&#30340;O-GlcNAcylation&#20301;&#28857;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#22522;&#21270;&#26159;&#19968;&#31181;&#34507;&#30333;&#36136;&#20462;&#39280;&#65292;&#22312;&#21151;&#33021;&#21644;&#32467;&#26500;&#19978;&#36215;&#30528;&#22810;&#31181;&#37325;&#35201;&#20316;&#29992;&#12290;O-GlcNAcylation&#26159;&#31958;&#22522;&#21270;&#30340;&#19968;&#31181;&#20122;&#22411;&#65292;&#26377;&#28508;&#21147;&#25104;&#20026;&#27835;&#30103;&#30340;&#37325;&#35201;&#38774;&#28857;&#65292;&#20294;&#22312;2023&#24180;&#20043;&#21069;&#23578;&#26410;&#26377;&#21487;&#38752;&#39044;&#27979;O-GlcNAcylation&#20301;&#28857;&#30340;&#26041;&#27861;&#65307;2021&#24180;&#30340;&#19968;&#31687;&#35780;&#35770;&#27491;&#30830;&#25351;&#20986;&#24050;&#21457;&#34920;&#30340;&#27169;&#22411;&#19981;&#36275;&#65292;&#24182;&#19988;&#26410;&#33021;&#27867;&#21270;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27169;&#22411;&#24050;&#19981;&#20877;&#21487;&#29992;&#12290;2023&#24180;&#65292;&#19968;&#31687;&#20855;&#26377;F$_1$&#20998;&#25968;36.17%&#21644;MCC&#20998;&#25968;34.57%&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#30340;&#26174;&#30528;&#26356;&#22909;&#30340;RNN&#27169;&#22411;&#34987;&#21457;&#34920;&#12290;&#26412;&#25991;&#39318;&#27425;&#35797;&#22270;&#36890;&#36807;Transformer&#32534;&#30721;&#22120;&#25552;&#39640;&#36825;&#20123;&#25351;&#26631;&#12290;&#23613;&#31649;Transformer&#22312;&#35813;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#24615;&#33021;&#20173;&#19981;&#21450;&#20808;&#21069;&#21457;&#34920;&#30340;RNN&#12290;&#28982;&#21518;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#21152;&#26435;&#28966;&#28857;&#21487;&#24494;MCC&#65292;&#20197;&#25552;&#39640;&#20998;&#31867;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17131v1 Announce Type: new  Abstract: Glycosylation, a protein modification, has multiple essential functional and structural roles. O-GlcNAcylation, a subtype of glycosylation, has the potential to be an important target for therapeutics, but methods to reliably predict O-GlcNAcylation sites had not been available until 2023; a 2021 review correctly noted that published models were insufficient and failed to generalize. Moreover, many are no longer usable. In 2023, a considerably better RNN model with an F$_1$ score of 36.17% and an MCC of 34.57% on a large dataset was published. This article first sought to improve these metrics using transformer encoders. While transformers displayed high performance on this dataset, their performance was inferior to that of the previously published RNN. We then created a new loss function, which we call the weighted focal differentiable MCC, to improve the performance of classification models. RNN models trained with this new function di
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#29978;&#33267;&#21482;&#38656;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#35813;&#25915;&#20987;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2311.07550</link><description>&lt;p&gt;
Tabdoor&#65306;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#34920;&#26684;&#25968;&#25454;&#31070;&#32463;&#32593;&#32476;&#23384;&#22312;&#21518;&#38376;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Tabdoor: Backdoor Vulnerabilities in Transformer-based Neural Networks for Tabular Data. (arXiv:2311.07550v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.07550
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20840;&#38754;&#20998;&#26512;&#20102;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#29978;&#33267;&#21482;&#38656;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#35813;&#25915;&#20987;&#36824;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#26174;&#31034;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#19982;&#36825;&#20123;&#21457;&#23637;&#21516;&#26102;&#65292;&#19982;DNN&#35757;&#32451;&#30456;&#20851;&#30340;&#28431;&#27934;&#65292;&#22914;&#21518;&#38376;&#25915;&#20987;&#65292;&#26159;&#19968;&#20010;&#37325;&#22823;&#20851;&#20999;&#12290;&#36825;&#20123;&#25915;&#20987;&#28041;&#21450;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#24494;&#22937;&#22320;&#25554;&#20837;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#20801;&#35768;&#25805;&#32437;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#30001;&#20110;&#36716;&#25442;&#22120;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;DNNs&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23545;&#20351;&#29992;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#21518;&#38376;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#29305;&#21035;&#20851;&#27880;&#36716;&#25442;&#22120;&#12290;&#37492;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#23884;&#20837;&#21518;&#38376;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23545;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#31995;&#32479;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;DNNs&#23545;&#34920;&#26684;&#25968;&#25454;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#65292;&#21363;&#20351;&#21482;&#26377;&#26368;&#23567;&#30340;&#29305;&#24449;&#20540;&#20462;&#25913;&#12290;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#25915;&#20987;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#27169;&#22411;&#65292;&#22914;XGBoost&#21644;DeepFM&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20960;&#20046;&#34920;&#26126;&#21518;&#38376;&#25915;&#20987;&#21487;&#20197;&#23436;&#32654;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) have shown great promise in various domains. Alongside these developments, vulnerabilities associated with DNN training, such as backdoor attacks, are a significant concern. These attacks involve the subtle insertion of triggers during model training, allowing for manipulated predictions.More recently, DNNs for tabular data have gained increasing attention due to the rise of transformer models.  Our research presents a comprehensive analysis of backdoor attacks on tabular data using DNNs, particularly focusing on transformers. Given the inherent complexities of tabular data, we explore the challenges of embedding backdoors. Through systematic experimentation across benchmark datasets, we uncover that transformer-based DNNs for tabular data are highly susceptible to backdoor attacks, even with minimal feature value alterations. We also verify that our attack can be generalized to other models, like XGBoost and DeepFM. Our results indicate nearly perfect attac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#30340;&#39044;&#27979;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#24863;&#30693;&#21644;&#21160;&#20316;&#29983;&#25104;&#65292;&#20197;&#25903;&#25345;&#28921;&#39274;&#26426;&#22120;&#20154;&#22312;&#29038;&#40481;&#34507;&#36807;&#31243;&#20013;&#23545;&#40481;&#34507;&#29366;&#24577;&#30340;&#24863;&#30693;&#21644;&#25605;&#25292;&#21160;&#20316;&#30340;&#35843;&#25972;&#12290;</title><link>http://arxiv.org/abs/2309.14837</link><description>&lt;p&gt;
&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#23454;&#26102;&#21160;&#20316;&#29983;&#25104;&#21644;&#20027;&#21160;&#24863;&#30693;&#30340;&#28921;&#39274;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Realtime Motion Generation with Active Perception Using Attention Mechanism for Cooking Robot. (arXiv:2309.14837v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14837
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#27880;&#24847;&#26426;&#21046;&#30340;&#39044;&#27979;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#23454;&#29616;&#23454;&#26102;&#24863;&#30693;&#21644;&#21160;&#20316;&#29983;&#25104;&#65292;&#20197;&#25903;&#25345;&#28921;&#39274;&#26426;&#22120;&#20154;&#22312;&#29038;&#40481;&#34507;&#36807;&#31243;&#20013;&#23545;&#40481;&#34507;&#29366;&#24577;&#30340;&#24863;&#30693;&#21644;&#25605;&#25292;&#21160;&#20316;&#30340;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25903;&#25345;&#20154;&#31867;&#30340;&#26085;&#24120;&#29983;&#27963;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#33258;&#20027;&#23398;&#20064;&#65292;&#36866;&#24212;&#29289;&#20307;&#21644;&#29615;&#22659;&#65292;&#24182;&#25191;&#34892;&#36866;&#24403;&#30340;&#21160;&#20316;&#12290;&#25105;&#20204;&#23581;&#35797;&#20351;&#29992;&#30495;&#23454;&#30340;&#39135;&#26448;&#29038;&#28818;&#40481;&#34507;&#30340;&#20219;&#21153;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#38656;&#35201;&#23454;&#26102;&#24863;&#30693;&#40481;&#34507;&#30340;&#29366;&#24577;&#24182;&#35843;&#25972;&#25605;&#25292;&#21160;&#20316;&#65292;&#21516;&#26102;&#40481;&#34507;&#34987;&#21152;&#28909;&#19988;&#29366;&#24577;&#19981;&#26029;&#21464;&#21270;&#12290;&#22312;&#20197;&#21069;&#30340;&#30740;&#31350;&#20013;&#65292;&#22788;&#29702;&#21464;&#21270;&#30340;&#29289;&#20307;&#34987;&#21457;&#29616;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#24863;&#30693;&#20449;&#24687;&#21253;&#25324;&#21160;&#24577;&#30340;&#12289;&#37325;&#35201;&#25110;&#22024;&#26434;&#30340;&#20449;&#24687;&#65292;&#32780;&#19988;&#27599;&#27425;&#24212;&#35813;&#20851;&#27880;&#30340;&#27169;&#24577;&#19981;&#26029;&#21464;&#21270;&#65292;&#36825;&#20351;&#24471;&#23454;&#29616;&#23454;&#26102;&#24863;&#30693;&#21644;&#21160;&#20316;&#29983;&#25104;&#21464;&#24471;&#22256;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#27880;&#24847;&#26426;&#21046;&#30340;&#39044;&#27979;&#24615;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#26435;&#34913;&#20256;&#24863;&#22120;&#36755;&#20837;&#65292;&#21306;&#20998;&#27599;&#31181;&#27169;&#24577;&#30340;&#37325;&#35201;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#39640;&#25928;&#30340;&#24863;&#30693;&#21644;&#21160;&#20316;&#29983;&#25104;&#12290;&#27169;&#22411;&#36890;&#36807;&#31034;&#33539;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#20801;&#35768;&#19981;&#26029;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
To support humans in their daily lives, robots are required to autonomously learn, adapt to objects and environments, and perform the appropriate actions. We tackled on the task of cooking scrambled eggs using real ingredients, in which the robot needs to perceive the states of the egg and adjust stirring movement in real time, while the egg is heated and the state changes continuously. In previous works, handling changing objects was found to be challenging because sensory information includes dynamical, both important or noisy information, and the modality which should be focused on changes every time, making it difficult to realize both perception and motion generation in real time. We propose a predictive recurrent neural network with an attention mechanism that can weigh the sensor input, distinguishing how important and reliable each modality is, that realize quick and efficient perception and motion generation. The model is trained with learning from the demonstration, and allow
&lt;/p&gt;</description></item></channel></rss>