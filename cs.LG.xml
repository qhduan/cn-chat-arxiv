<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02595</link><description>&lt;p&gt;
QFNN-FFD&#65306;&#29992;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#30340;&#37327;&#23376;&#32852;&#37030;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02595
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#65292;&#36825;&#26159;&#19968;&#20010;&#34701;&#21512;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21069;&#27839;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#26032;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;FL&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;QFNN-FFD&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#35782;&#21035;&#27450;&#35784;&#20132;&#26131;&#30340;&#26041;&#27861;&#12290;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#23454;&#26045;&#21452;&#38454;&#27573;&#35757;&#32451;&#27169;&#22411;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24615;&#33021;&#26041;&#27861;&#12290;QFNN-FFD&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#65292;&#26631;&#24535;&#30528;&#37329;&#34701;&#31185;&#25216;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#22823;&#36827;&#27493;&#65292;&#24182;&#20026;&#20197;&#38544;&#31169;&#20026;&#37325;&#28857;&#30340;&#27450;&#35784;&#26816;&#27979;&#24314;&#31435;&#20102;&#26032;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02595v1 Announce Type: cross  Abstract: This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection. Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.
&lt;/p&gt;</description></item><item><title>COS-GNN&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#23545;&#22270;&#33410;&#28857;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#35299;&#20915;&#22312;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01897</link><description>&lt;p&gt;
&#36830;&#32493;&#33033;&#20914;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous Spiking Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01897
&lt;/p&gt;
&lt;p&gt;
COS-GNN&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#23545;&#22270;&#33410;&#28857;&#36827;&#34892;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#35299;&#20915;&#22312;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;CGNNs&#65289;&#22240;&#24341;&#20837;&#36830;&#32493;&#21160;&#21147;&#23398;&#32780;&#24341;&#36215;&#20102;&#26497;&#22823;&#20851;&#27880;&#65292;&#33021;&#22815;&#25512;&#24191;&#29616;&#26377;&#30340;&#31163;&#25955;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#23427;&#20204;&#36890;&#24120;&#21463;&#25193;&#25955;&#31867;&#26041;&#27861;&#21551;&#21457;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20256;&#25773;&#26041;&#26696;&#65292;&#24182;&#20351;&#29992;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#32780;&#65292;CGNNs&#30340;&#23454;&#29616;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#33021;&#21147;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#38590;&#20197;&#37096;&#32626;&#22312;&#30005;&#27744;&#20379;&#30005;&#35774;&#22791;&#19978;&#12290;&#21463;&#26368;&#36817;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#30340;&#21551;&#21457;&#65292;SNNs&#27169;&#25311;&#29983;&#29289;&#25512;&#29702;&#36807;&#31243;&#24182;&#25552;&#20379;&#19968;&#31181;&#33410;&#33021;&#30340;&#31070;&#32463;&#26550;&#26500;&#65292;&#25105;&#20204;&#23558;SNNs&#19982;CGNNs&#32467;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20013;&#65292;&#21629;&#21517;&#20026;&#36830;&#32493;&#33033;&#20914;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;COS-GNN&#65289;&#12290;&#25105;&#20204;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20351;&#29992;SNNs&#36827;&#34892;&#22270;&#33410;&#28857;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#36827;&#19968;&#27493;&#19982;&#26102;&#38388;&#19968;&#36215;&#38598;&#25104;&#21040;ODE&#36807;&#31243;&#20013;&#65292;&#20197;&#22686;&#24378;&#20449;&#24687;&#20445;&#23384;&#21644;&#32531;&#35299;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01897v1 Announce Type: cross  Abstract: Continuous graph neural networks (CGNNs) have garnered significant attention due to their ability to generalize existing discrete graph neural networks (GNNs) by introducing continuous dynamics. They typically draw inspiration from diffusion-based methods to introduce a novel propagation scheme, which is analyzed using ordinary differential equations (ODE). However, the implementation of CGNNs requires significant computational power, making them challenging to deploy on battery-powered devices. Inspired by recent spiking neural networks (SNNs), which emulate a biological inference process and provide an energy-efficient neural architecture, we incorporate the SNNs with CGNNs in a unified framework, named Continuous Spiking Graph Neural Networks (COS-GNN). We employ SNNs for graph node representation at each time step, which are further integrated into the ODE process along with time. To enhance information preservation and mitigate in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30446;&#30340;&#30340;VR&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;108&#21517;&#21442;&#19982;&#32773;&#22312;VR&#20013;&#23398;&#20064;&#32452;&#35013;&#20004;&#31181;&#19981;&#21516;&#20840;&#23610;&#23544;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.08969</link><description>&lt;p&gt;
&#20840;&#23610;&#23544;&#35013;&#37197;&#27169;&#25311;&#27979;&#35797;&#21488;&#65288;FAST&#65289;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
The Full-scale Assembly Simulation Testbed (FAST) Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08969
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#30446;&#30340;&#30340;VR&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;108&#21517;&#21442;&#19982;&#32773;&#22312;VR&#20013;&#23398;&#20064;&#32452;&#35013;&#20004;&#31181;&#19981;&#21516;&#20840;&#23610;&#23544;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#24320;&#22987;&#30740;&#31350;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#36319;&#36394;&#21644;&#20132;&#20114;&#25968;&#25454;&#22914;&#20309;&#29992;&#20110;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#30446;&#30340;&#65292;&#21253;&#25324;&#29992;&#25143;&#35782;&#21035;&#12289;&#39044;&#27979;&#32593;&#32476;&#26197;&#21160;&#30151;&#21644;&#20272;&#31639;&#23398;&#20064;&#22686;&#30410;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#20351;&#29992;&#25105;&#20204;&#30340;&#22522;&#20110;VR&#30340;&#20840;&#23610;&#23544;&#35013;&#37197;&#27169;&#25311;&#27979;&#35797;&#21488;&#65288;FAST&#65289;&#25429;&#33719;&#30340;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#21253;&#25324;&#20174;108&#21517;&#21442;&#19982;&#32773;&#65288;50&#21517;&#22899;&#24615;&#65292;56&#21517;&#30007;&#24615;&#65292;2&#21517;&#38750;&#20108;&#20803;&#24615;&#21035;&#65289;&#23398;&#20064;&#22914;&#20309;&#22312;VR&#20013;&#32452;&#35013;&#20004;&#31181;&#19981;&#21516;&#20840;&#23610;&#23544;&#32467;&#26500;&#26102;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#38500;&#20102;&#35299;&#37322;&#25968;&#25454;&#38598;&#26159;&#22914;&#20309;&#25910;&#38598;&#30340;&#24182;&#25551;&#36848;&#21253;&#21547;&#30340;&#25968;&#25454;&#22806;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#20351;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08969v1 Announce Type: cross  Abstract: In recent years, numerous researchers have begun investigating how virtual reality (VR) tracking and interaction data can be used for a variety of machine learning purposes, including user identification, predicting cybersickness, and estimating learning gains. One constraint for this research area is the dearth of open datasets. In this paper, we present a new open dataset captured with our VR-based Full-scale Assembly Simulation Testbed (FAST). This dataset consists of data collected from 108 participants (50 females, 56 males, 2 non-binary) learning how to assemble two distinct full-scale structures in VR. In addition to explaining how the dataset was collected and describing the data included, we discuss how the dataset may be used by future researchers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#32447;&#20048;&#35266;&#29275;&#39039;&#27969;&#24418;&#65288;OONM&#65289;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#22522;&#20110;&#20989;&#25968;&#24207;&#21015;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#38454;&#20449;&#24687;&#39044;&#27979;&#30340;&#22312;&#32447;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22312;&#32447;LQG&#32447;&#24615;&#32422;&#26463;&#25919;&#31574;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.08553</link><description>&lt;p&gt;
&#22312;&#32447;LQG&#32447;&#24615;&#32422;&#26463;&#25919;&#31574;&#20248;&#21270;&#30340;&#36951;&#25022;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Regret Analysis of Policy Optimization over Submanifolds for Linearly Constrained Online LQG
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08553
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#32447;&#20048;&#35266;&#29275;&#39039;&#27969;&#24418;&#65288;OONM&#65289;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#22522;&#20110;&#20989;&#25968;&#24207;&#21015;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#38454;&#20449;&#24687;&#39044;&#27979;&#30340;&#22312;&#32447;&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22312;&#32447;LQG&#32447;&#24615;&#32422;&#26463;&#25919;&#31574;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#20248;&#21270;&#21644;&#25511;&#21046;&#30340;&#26368;&#26032;&#36827;&#23637;&#20026;&#30740;&#31350;&#22312;&#32447;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#65288;LQR&#65289;&#38382;&#39064;&#25552;&#20379;&#20102;&#26032;&#24037;&#20855;&#65292;&#20854;&#20013;&#25104;&#26412;&#30697;&#38453;&#38543;&#26102;&#38388;&#21464;&#21270;&#23545;&#25239;&#24615;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20316;&#21697;&#30340;&#25511;&#21046;&#22120;&#21442;&#25968;&#21270;&#21487;&#33021;&#19981;&#28385;&#36275;&#23454;&#38469;&#26465;&#20214;&#65292;&#22914;&#30001;&#20110;&#29289;&#29702;&#36830;&#25509;&#32780;&#23548;&#33268;&#30340;&#31232;&#30095;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#32447;&#24615;&#20108;&#27425;&#39640;&#26031;&#38382;&#39064;&#65292;&#20854;&#20013;&#23545;&#25511;&#21046;&#22120;&#26045;&#21152;&#20102;&#32473;&#23450;&#30340;&#32447;&#24615;&#32422;&#26463;&#12290;&#21463;[1]&#26368;&#36817;&#25552;&#20986;&#30340;&#20851;&#20110;&#32447;&#24615;&#32422;&#26463;&#30340;&#32447;&#19979;LQR&#25919;&#31574;&#20248;&#21270;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#20108;&#38454;&#26041;&#27861;&#65292;&#37197;&#22791;&#20102;&#19968;&#31181;&#22312;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#33258;&#28982;&#20135;&#29983;&#30340;&#40654;&#26364;&#24230;&#37327;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#32447;&#20048;&#35266;&#29275;&#39039;&#27969;&#24418;&#65288;OONM&#65289;&#65292;&#25552;&#20379;&#22522;&#20110;&#20989;&#25968;&#24207;&#21015;&#30340;&#31532;&#19968;&#21644;&#31532;&#20108;&#38454;&#20449;&#24687;&#39044;&#27979;&#30340;&#22312;&#32447;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#37327;&#21270;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#36951;&#25022;&#30340;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08553v1 Announce Type: cross  Abstract: Recent advancement in online optimization and control has provided novel tools to study online linear quadratic regulator (LQR) problems, where cost matrices are varying adversarially over time. However, the controller parameterization of existing works may not satisfy practical conditions like sparsity due to physical connections. In this work, we study online linear quadratic Gaussian problems with a given linear constraint imposed on the controller. Inspired by the recent work of [1] which proposed, for a linearly constrained policy optimization of an offline LQR, a second order method equipped with a Riemannian metric that emerges naturally in the context of optimal control problems, we propose online optimistic Newton on manifold (OONM) which provides an online controller based on the prediction on the first and second order information of the function sequence. To quantify the proposed algorithm, we leverage the notion of regret 
&lt;/p&gt;</description></item><item><title>&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#65288;LTC&#65289;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#37319;&#29992;&#26684;&#28857;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#21387;&#32553;&#20013;&#25509;&#36817;&#36895;&#29575;&#22833;&#30495;&#26497;&#38480;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.07320</link><description>&lt;p&gt;
&#29992;&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#25509;&#36817;&#31070;&#32463;&#21387;&#32553;&#20013;&#30340;&#36895;&#29575;&#22833;&#30495;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Approaching Rate-Distortion Limits in Neural Compression with Lattice Transform Coding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07320
&lt;/p&gt;
&lt;p&gt;
&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#65288;LTC&#65289;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#37319;&#29992;&#26684;&#28857;&#37327;&#21270;&#65292;&#23454;&#29616;&#20102;&#31070;&#32463;&#21387;&#32553;&#20013;&#25509;&#36817;&#36895;&#29575;&#22833;&#30495;&#26497;&#38480;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21387;&#32553;&#22312;&#35774;&#35745;&#20855;&#26377;&#33391;&#22909;&#36895;&#29575;&#22833;&#30495;&#65288;RD&#65289;&#24615;&#33021;&#20294;&#22797;&#26434;&#24230;&#20302;&#30340;&#26377;&#25439;&#21387;&#32553;&#22120;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#31070;&#32463;&#21387;&#32553;&#35774;&#35745;&#28041;&#21450;&#23558;&#28304;&#36716;&#25442;&#20026;&#28508;&#21464;&#37327;&#65292;&#28982;&#21518;&#33293;&#20837;&#20026;&#25972;&#25968;&#24182;&#36827;&#34892;&#29109;&#32534;&#30721;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#24050;&#34987;&#35777;&#26126;&#22312;&#26576;&#20123;&#28304;&#19978;&#30340;&#19968;&#27425;&#24615;&#24773;&#20917;&#19979;&#26159;&#26368;&#20339;&#30340;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#22312;i.i.d.&#24207;&#21015;&#19978;&#23427;&#26159;&#39640;&#24230;&#27425;&#20248;&#30340;&#65292;&#20107;&#23454;&#19978;&#24635;&#26159;&#24674;&#22797;&#21407;&#22987;&#28304;&#24207;&#21015;&#30340;&#26631;&#37327;&#37327;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20122;&#20248;&#36234;&#24615;&#26159;&#30001;&#20110;&#28508;&#31354;&#38388;&#20013;&#37327;&#21270;&#26041;&#26696;&#30340;&#36873;&#25321;&#65292;&#32780;&#38750;&#21464;&#25442;&#35774;&#35745;&#25152;&#33268;&#12290;&#36890;&#36807;&#22312;&#28508;&#31354;&#38388;&#20013;&#37319;&#29992;&#26684;&#28857;&#37327;&#21270;&#32780;&#38750;&#26631;&#37327;&#37327;&#21270;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26684;&#28857;&#21464;&#25442;&#32534;&#30721;&#65288;Lattice Transform Coding&#65292;LTC&#65289;&#33021;&#22815;&#22312;&#21508;&#20010;&#32500;&#24230;&#19978;&#24674;&#22797;&#26368;&#20339;&#30690;&#37327;&#37327;&#21270;&#65292;&#24182;&#22312;&#21512;&#29702;&#30340;&#22797;&#26434;&#24230;&#19979;&#25509;&#36817;&#28176;&#36817;&#21487;&#23454;&#29616;&#30340;&#36895;&#29575;&#22833;&#30495;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07320v1 Announce Type: cross  Abstract: Neural compression has brought tremendous progress in designing lossy compressors with good rate-distortion (RD) performance at low complexity. Thus far, neural compression design involves transforming the source to a latent vector, which is then rounded to integers and entropy coded. While this approach has been shown to be optimal in a one-shot sense on certain sources, we show that it is highly sub-optimal on i.i.d. sequences, and in fact always recovers scalar quantization of the original source sequence. We demonstrate that the sub-optimality is due to the choice of quantization scheme in the latent space, and not the transform design. By employing lattice quantization instead of scalar quantization in the latent space, we demonstrate that Lattice Transform Coding (LTC) is able to recover optimal vector quantization at various dimensions and approach the asymptotically-achievable rate-distortion function at reasonable complexity. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.06759</link><description>&lt;p&gt;
&#24179;&#22343;&#26657;&#20934;&#35823;&#24046;&#65306;&#19968;&#31181;&#21487;&#24494;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#21487;&#38752;&#24615;
&lt;/p&gt;
&lt;p&gt;
Average Calibration Error: A Differentiable Loss for Improved Reliability in Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06759
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#29992;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#20687;&#32032;&#32423;&#26657;&#20934;&#65292;&#20943;&#23569;&#20102;&#26657;&#20934;&#35823;&#24046;&#24182;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#20197;&#25552;&#39640;&#26657;&#20934;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#20135;&#29983;&#19982;&#32463;&#39564;&#35266;&#23519;&#19981;&#19968;&#33268;&#30340;&#36807;&#20110;&#33258;&#20449;&#30340;&#32467;&#26524;&#65292;&#36825;&#31181;&#26657;&#20934;&#38169;&#35823;&#25361;&#25112;&#30528;&#23427;&#20204;&#30340;&#20020;&#24202;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24179;&#22343;L1&#26657;&#20934;&#35823;&#24046;&#65288;mL1-ACE&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#36741;&#21161;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#25913;&#21892;&#20687;&#32032;&#32423;&#26657;&#20934;&#32780;&#19981;&#20250;&#25439;&#23475;&#20998;&#21106;&#36136;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#23613;&#31649;&#20351;&#29992;&#30828;&#20998;&#31665;&#65292;&#36825;&#31181;&#25439;&#22833;&#26159;&#30452;&#25509;&#21487;&#24494;&#30340;&#65292;&#36991;&#20813;&#20102;&#38656;&#35201;&#36817;&#20284;&#20294;&#21487;&#24494;&#30340;&#26367;&#20195;&#25110;&#36719;&#20998;&#31665;&#26041;&#27861;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#24341;&#20837;&#20102;&#25968;&#25454;&#38598;&#21487;&#38752;&#24615;&#30452;&#26041;&#22270;&#30340;&#27010;&#24565;&#65292;&#36825;&#19968;&#27010;&#24565;&#25512;&#24191;&#20102;&#26631;&#20934;&#30340;&#21487;&#38752;&#24615;&#22270;&#65292;&#29992;&#20110;&#22312;&#25968;&#25454;&#38598;&#32423;&#21035;&#32858;&#21512;&#30340;&#35821;&#20041;&#20998;&#21106;&#20013;&#32454;&#21270;&#26657;&#20934;&#30340;&#35270;&#35273;&#35780;&#20272;&#12290;&#20351;&#29992;mL1-ACE&#65292;&#25105;&#20204;&#23558;&#24179;&#22343;&#21644;&#26368;&#22823;&#26657;&#20934;&#35823;&#24046;&#20998;&#21035;&#38477;&#20302;&#20102;45%&#21644;55%&#65292;&#21516;&#26102;&#22312;BraTS 2021&#25968;&#25454;&#38598;&#19978;&#20445;&#25345;&#20102;87%&#30340;Dice&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#20998;&#20139;&#25105;&#20204;&#30340;&#20195;&#30721;: https://github
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06759v1 Announce Type: cross  Abstract: Deep neural networks for medical image segmentation often produce overconfident results misaligned with empirical observations. Such miscalibration, challenges their clinical translation. We propose to use marginal L1 average calibration error (mL1-ACE) as a novel auxiliary loss function to improve pixel-wise calibration without compromising segmentation quality. We show that this loss, despite using hard binning, is directly differentiable, bypassing the need for approximate but differentiable surrogate or soft binning approaches. Our work also introduces the concept of dataset reliability histograms which generalises standard reliability diagrams for refined visual assessment of calibration in semantic segmentation aggregated at the dataset level. Using mL1-ACE, we reduce average and maximum calibration error by 45% and 55% respectively, maintaining a Dice score of 87% on the BraTS 2021 dataset. We share our code here: https://github
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.11462</link><description>&lt;p&gt;
&#29992;&#20110;&#26356;&#24555;&#30340;LLM&#25512;&#29702;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;
&lt;/p&gt;
&lt;p&gt;
Cascade Speculative Drafting for Even Faster LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11462
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;Cascade Speculative Drafting&#65288;CS Drafting&#65289;&#31639;&#27861;&#65292;&#36890;&#36807;&#22402;&#30452;&#32423;&#32852;&#28040;&#38500;&#31070;&#32463;&#27169;&#22411;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#36890;&#36807;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;LLM&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25512;&#29702;&#25928;&#29575;&#30340;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65292;&#36890;&#36807;&#36739;&#23567;&#30340;&#27169;&#22411;&#29983;&#25104;&#33609;&#31295;&#26469;&#36816;&#20316;&#12290;&#36739;&#22823;&#30340;&#30446;&#26631;&#27169;&#22411;&#28982;&#21518;&#26597;&#30475;&#36825;&#20010;&#33609;&#31295;&#20197;&#19982;&#20854;&#36755;&#20986;&#23545;&#40784;&#65292;&#30446;&#26631;&#27169;&#22411;&#30340;&#20219;&#20309;&#25509;&#21463;&#37117;&#23558;&#20943;&#23569;&#30446;&#26631;&#27169;&#22411;&#36816;&#34892;&#30340;&#25968;&#37327;&#65292;&#20174;&#32780;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#32423;&#32852;&#25512;&#27979;&#30340;&#33609;&#22270;&#36807;&#31243;&#20013;&#21253;&#25324;&#32531;&#24930;&#30340;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#24182;&#20026;&#29983;&#25104;&#30340;&#26631;&#35760;&#20998;&#37197;&#30456;&#21516;&#30340;&#26102;&#38388;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#20302;&#25928;&#24615;&#20849;&#21516;&#23548;&#33268;&#32423;&#32852;&#25512;&#27979;&#30340;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25913;&#21892;LLM&#25512;&#29702;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#32423;&#32852;&#25512;&#27979;&#33609;&#22270;&#65288;CS Drafting&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#25972;&#21512;&#20102;&#20004;&#31181;&#32423;&#32852;&#31867;&#22411;&#30340;&#25512;&#27979;&#25191;&#34892;&#31639;&#27861;&#12290;&#22402;&#30452;&#32423;&#32852;&#20174;&#31070;&#32463;&#27169;&#22411;&#20013;&#28040;&#38500;&#33258;&#22238;&#24402;&#29983;&#25104;&#65292;&#32780;&#27700;&#24179;&#32423;&#32852;&#20248;&#21270;&#20102;&#33609;&#31295;&#20013;&#30340;&#26102;&#38388;&#20998;&#37197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11462v3 Announce Type: replace-cross  Abstract: Introduced to enhance the efficiency of large language model (LLM) inference, speculative decoding operates by having a smaller model generate a draft. A larger target model then reviews this draft to align with its output, and any acceptance by the target model results in a reduction of the number of the target model runs, ultimately improving efficiency. However, the drafting process in speculative decoding includes slow autoregressive generation and allocates equal time to generating tokens, irrespective of their importance. These inefficiencies collectively contribute to the suboptimal performance of speculative decoding. To further improve LLM inference, we introduce Cascade Speculative Drafting (CS Drafting), a speculative execution algorithm that incorporates two types of cascades. The Vertical Cascade eliminates autoregressive generation from neural models, while the Horizontal Cascade optimizes time allocation in draft
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.05373</link><description>&lt;p&gt;
&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Dynamic Spiking Graph Neural Networks. (arXiv:2401.05373v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05373
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;DSGNN&#65289;&#30340;&#26694;&#26550;&#65292;&#23427;&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#35299;&#20915;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#24320;&#38144;&#38382;&#39064;&#12290;DSGNN&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#28176;&#28176;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#36825;&#26159;&#22240;&#20026;&#23427;&#22312;&#22788;&#29702;&#30001;&#22270;&#34920;&#31034;&#30340;&#38750;&#27431;&#20960;&#37324;&#24471;&#25968;&#25454;&#26102;&#20855;&#26377;&#20302;&#21151;&#32791;&#21644;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#19968;&#20010;&#24120;&#35265;&#30340;&#38382;&#39064;&#65292;&#21160;&#24577;&#22270;&#34920;&#31034;&#23398;&#20064;&#38754;&#20020;&#30528;&#39640;&#22797;&#26434;&#24615;&#21644;&#22823;&#20869;&#23384;&#24320;&#38144;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#30340;&#24037;&#20316;&#36890;&#24120;&#36890;&#36807;&#20351;&#29992;&#20108;&#36827;&#21046;&#29305;&#24449;&#32780;&#19981;&#26159;&#36830;&#32493;&#29305;&#24449;&#30340;SNNs&#26469;&#26367;&#20195;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#36827;&#34892;&#39640;&#25928;&#35757;&#32451;&#65292;&#36825;&#20250;&#24573;&#35270;&#22270;&#32467;&#26500;&#20449;&#24687;&#24182;&#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#23548;&#33268;&#32454;&#33410;&#30340;&#20002;&#22833;&#12290;&#27492;&#22806;&#65292;&#20248;&#21270;&#21160;&#24577;&#23574;&#23792;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#22312;&#26102;&#38388;&#27493;&#20043;&#38388;&#20256;&#25773;&#20449;&#24687;&#65292;&#36825;&#22686;&#21152;&#20102;&#20869;&#23384;&#38656;&#27714;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#21160;&#24577;&#23574;&#23792;&#22270;&#31070;&#32463;&#32593;&#32476;"&#65288;\method{}&#65289;&#30340;&#26694;&#26550;&#12290;&#20026;&#20102;&#20943;&#36731;&#20449;&#24687;&#20002;&#22833;&#38382;&#39064;&#65292;\method{} &#22312;&#20256;&#25773;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292;&#23427;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#20013;&#21160;&#24577;&#22320;&#35843;&#25972;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#29366;&#24577;&#21644;&#36830;&#25509;&#26435;&#37325;&#65292;&#20197;&#20445;&#25345;&#22270;&#32467;&#26500;&#20449;&#24687;&#30340;&#23436;&#25972;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of Spiking Neural Networks (SNNs) and Graph Neural Networks (GNNs) is gradually attracting attention due to the low power consumption and high efficiency in processing the non-Euclidean data represented by graphs. However, as a common problem, dynamic graph representation learning faces challenges such as high complexity and large memory overheads. Current work often uses SNNs instead of Recurrent Neural Networks (RNNs) by using binary features instead of continuous ones for efficient training, which would overlooks graph structure information and leads to the loss of details during propagation. Additionally, optimizing dynamic spiking models typically requires propagation of information across time steps, which increases memory requirements. To address these challenges, we present a framework named \underline{Dy}namic \underline{S}p\underline{i}king \underline{G}raph \underline{N}eural Networks (\method{}). To mitigate the information loss problem, \method{} propagates
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;DKFs&#65289;&#65292;&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#19968;&#31867;&#38750;&#39532;&#23572;&#21487;&#22827;&#21644;&#26465;&#20214;&#39640;&#26031;&#20449;&#21495;&#36807;&#31243;&#30340;&#26465;&#20214;&#20998;&#24067;&#24459;&#65292;&#20174;&#32780;&#20855;&#26377;&#22312;&#25968;&#23398;&#37329;&#34701;&#39046;&#22495;&#20013;&#20256;&#32479;&#27169;&#22411;&#22522;&#30784;&#19978;&#30340;&#28388;&#27874;&#38382;&#39064;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.19603</link><description>&lt;p&gt;
&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#21487;&#20197;&#36827;&#34892;&#28388;&#27874;
&lt;/p&gt;
&lt;p&gt;
Deep Kalman Filters Can Filter. (arXiv:2310.19603v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19603
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;&#30340;&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;DKFs&#65289;&#65292;&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#19968;&#31867;&#38750;&#39532;&#23572;&#21487;&#22827;&#21644;&#26465;&#20214;&#39640;&#26031;&#20449;&#21495;&#36807;&#31243;&#30340;&#26465;&#20214;&#20998;&#24067;&#24459;&#65292;&#20174;&#32780;&#20855;&#26377;&#22312;&#25968;&#23398;&#37329;&#34701;&#39046;&#22495;&#20013;&#20256;&#32479;&#27169;&#22411;&#22522;&#30784;&#19978;&#30340;&#28388;&#27874;&#38382;&#39064;&#30340;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#65288;DKFs&#65289;&#26159;&#19968;&#31867;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#29983;&#25104;&#39640;&#26031;&#27010;&#29575;&#27979;&#24230;&#12290;&#34429;&#28982;DKFs&#21463;&#21345;&#23572;&#26364;&#28388;&#27874;&#22120;&#30340;&#21551;&#21457;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#19982;&#38543;&#26426;&#28388;&#27874;&#38382;&#39064;&#30340;&#20855;&#20307;&#29702;&#35770;&#20851;&#32852;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20256;&#32479;&#27169;&#22411;&#22522;&#30784;&#19978;&#30340;&#28388;&#27874;&#38382;&#39064;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#25968;&#23398;&#37329;&#34701;&#20013;&#30340;&#20538;&#21048;&#21644;&#26399;&#26435;&#23450;&#20215;&#27169;&#22411;&#26657;&#20934;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#19968;&#31867;&#36830;&#32493;&#26102;&#38388;DKFs&#65292;&#21487;&#20197;&#36817;&#20284;&#23454;&#29616;&#19968;&#31867;&#38750;&#39532;&#23572;&#21487;&#22827;&#21644;&#26465;&#20214;&#39640;&#26031;&#20449;&#21495;&#36807;&#31243;&#30340;&#26465;&#20214;&#20998;&#24067;&#24459;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#28145;&#24230;&#23398;&#20064;&#25968;&#23398;&#22522;&#30784;&#20013;&#30340;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#36817;&#20284;&#32467;&#26524;&#22312;&#36335;&#24452;&#30340;&#36275;&#22815;&#35268;&#21017;&#30340;&#32039;&#33268;&#23376;&#38598;&#19978;&#19968;&#33268;&#25104;&#31435;&#65292;&#20854;&#20013;&#36817;&#20284;&#35823;&#24046;&#30001;&#22312;&#32473;&#23450;&#32039;&#33268;&#36335;&#24452;&#38598;&#19978;&#22343;&#19968;&#22320;&#35745;&#31639;&#30340;&#26368;&#22351;&#24773;&#20917;2-Wasserstein&#36317;&#31163;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Kalman filters (DKFs) are a class of neural network models that generate Gaussian probability measures from sequential data. Though DKFs are inspired by the Kalman filter, they lack concrete theoretical ties to the stochastic filtering problem, thus limiting their applicability to areas where traditional model-based filters have been used, e.g.\ model calibration for bond and option prices in mathematical finance. We address this issue in the mathematical foundations of deep learning by exhibiting a class of continuous-time DKFs which can approximately implement the conditional law of a broad class of non-Markovian and conditionally Gaussian signal processes given noisy continuous-times measurements. Our approximation results hold uniformly over sufficiently regular compact subsets of paths, where the approximation error is quantified by the worst-case 2-Wasserstein distance computed uniformly over the given compact set of paths.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#26368;&#24046;&#31867;&#21035;&#35757;&#32451;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#24182;&#38477;&#20302;&#20102;&#26368;&#24046;&#31867;&#21035;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.14890</link><description>&lt;p&gt;
Boosting&#29992;&#20110;&#30028;&#23450;&#26368;&#24046;&#20998;&#31867;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Boosting for Bounding the Worst-class Error. (arXiv:2310.14890v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14890
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Boosting&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#26368;&#24046;&#31867;&#21035;&#35757;&#32451;&#35823;&#24046;&#30340;&#19978;&#30028;&#65292;&#24182;&#38477;&#20302;&#20102;&#26368;&#24046;&#31867;&#21035;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#26368;&#24046;&#31867;&#21035;&#35823;&#24046;&#29575;&#30340;&#38382;&#39064;&#65292;&#32780;&#19981;&#26159;&#38024;&#23545;&#25152;&#26377;&#31867;&#21035;&#30340;&#26631;&#20934;&#35823;&#24046;&#29575;&#30340;&#24179;&#22343;&#12290;&#20363;&#22914;&#65292;&#19968;&#20010;&#19977;&#31867;&#21035;&#20998;&#31867;&#20219;&#21153;&#65292;&#20854;&#20013;&#21508;&#31867;&#21035;&#30340;&#35823;&#24046;&#29575;&#20998;&#21035;&#20026;10&#65285;&#65292;10&#65285;&#21644;40&#65285;&#65292;&#20854;&#26368;&#24046;&#31867;&#21035;&#35823;&#24046;&#29575;&#20026;40&#65285;&#65292;&#32780;&#22312;&#31867;&#21035;&#24179;&#34913;&#26465;&#20214;&#19979;&#30340;&#24179;&#22343;&#35823;&#24046;&#29575;&#20026;20&#65285;&#12290;&#26368;&#24046;&#31867;&#21035;&#38169;&#35823;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#24456;&#37325;&#35201;&#12290;&#20363;&#22914;&#65292;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23545;&#20110;&#24694;&#24615;&#32959;&#30244;&#31867;&#21035;&#20855;&#26377;40&#65285;&#30340;&#38169;&#35823;&#29575;&#32780;&#33391;&#24615;&#21644;&#20581;&#24247;&#31867;&#21035;&#20855;&#26377;10&#65285;&#30340;&#38169;&#35823;&#29575;&#26159;&#19981;&#33021;&#34987;&#25509;&#21463;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#35777;&#26368;&#24046;&#31867;&#21035;&#35757;&#32451;&#35823;&#24046;&#19978;&#30028;&#30340;&#25552;&#21319;&#31639;&#27861;&#65292;&#24182;&#25512;&#23548;&#20986;&#20854;&#27867;&#21270;&#30028;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#38477;&#20302;&#20102;&#26368;&#24046;&#31867;&#21035;&#30340;&#27979;&#35797;&#35823;&#24046;&#29575;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#23545;&#35757;&#32451;&#38598;&#30340;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper tackles the problem of the worst-class error rate, instead of the standard error rate averaged over all classes. For example, a three-class classification task with class-wise error rates of 10\%, 10\%, and 40\% has a worst-class error rate of 40\%, whereas the average is 20\% under the class-balanced condition. The worst-class error is important in many applications. For example, in a medical image classification task, it would not be acceptable for the malignant tumor class to have a 40\% error rate, while the benign and healthy classes have 10\% error rates.We propose a boosting algorithm that guarantees an upper bound of the worst-class training error and derive its generalization bound. Experimental results show that the algorithm lowers worst-class test error rates while avoiding overfitting to the training set.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#36870;&#29702;&#35770;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#33394;&#22686;&#24378;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#31616;&#21333;&#30697;&#38453;&#26041;&#31243;&#25551;&#36848;&#20840;&#33394;&#22686;&#24378;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#35299;&#30340;&#26465;&#20214;&#21644;&#20809;&#35889;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#12290;&#36890;&#36807;&#24341;&#20837;&#38477;&#37319;&#26679;&#22686;&#24378;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19982;&#20998;&#37327;&#26367;&#20195;&#21644;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#26041;&#27861;&#30456;&#23545;&#24212;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#20808;&#39564;&#26469;&#35299;&#20915;&#20840;&#33394;&#22686;&#24378;&#20013;&#30340;&#29702;&#35770;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.02718</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#36870;&#29702;&#35299;&#20840;&#33394;&#22686;&#24378;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Understanding Pan-Sharpening via Generalized Inverse. (arXiv:2310.02718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02718
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#24191;&#20041;&#36870;&#29702;&#35770;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#33394;&#22686;&#24378;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22522;&#20110;&#31616;&#21333;&#30697;&#38453;&#26041;&#31243;&#25551;&#36848;&#20840;&#33394;&#22686;&#24378;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#35299;&#30340;&#26465;&#20214;&#21644;&#20809;&#35889;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#12290;&#36890;&#36807;&#24341;&#20837;&#38477;&#37319;&#26679;&#22686;&#24378;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19982;&#20998;&#37327;&#26367;&#20195;&#21644;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#26041;&#27861;&#30456;&#23545;&#24212;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#20808;&#39564;&#26469;&#35299;&#20915;&#20840;&#33394;&#22686;&#24378;&#20013;&#30340;&#29702;&#35770;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#33394;&#22686;&#24378;&#31639;&#27861;&#21033;&#29992;&#20840;&#33394;&#22270;&#20687;&#21644;&#22810;&#20809;&#35889;&#22270;&#20687;&#33719;&#21462;&#20855;&#26377;&#39640;&#31354;&#38388;&#21644;&#39640;&#20809;&#35889;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31639;&#27861;&#30340;&#20248;&#21270;&#26159;&#26681;&#25454;&#19981;&#21516;&#30340;&#26631;&#20934;&#35774;&#35745;&#30340;&#12290;&#25105;&#20204;&#37319;&#29992;&#31616;&#21333;&#30340;&#30697;&#38453;&#26041;&#31243;&#26469;&#25551;&#36848;&#20840;&#33394;&#22686;&#24378;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#35299;&#30340;&#23384;&#22312;&#26465;&#20214;&#20197;&#21450;&#20809;&#35889;&#21644;&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#33719;&#21462;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38477;&#37319;&#26679;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#33719;&#21462;&#31354;&#38388;&#21644;&#20809;&#35889;&#38477;&#37319;&#26679;&#30697;&#38453;&#12290;&#36890;&#36807;&#24191;&#20041;&#36870;&#29702;&#35770;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#20004;&#31181;&#24418;&#24335;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#34920;&#36798;&#24335;&#65292;&#21487;&#20197;&#23545;&#24212;&#20110;&#20004;&#20010;&#20027;&#35201;&#30340;&#20840;&#33394;&#22686;&#24378;&#26041;&#27861;&#65306;&#20998;&#37327;&#26367;&#20195;&#21644;&#22810;&#20998;&#36776;&#29575;&#20998;&#26512;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Gram Schmidt&#33258;&#36866;&#24212;(GSA)&#26041;&#27861;&#36981;&#24490;&#20998;&#37327;&#26367;&#20195;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#20809;&#35889;&#20989;&#25968;&#30340;&#24191;&#20041;&#36870;&#30697;&#38453;&#20043;&#21069;&#30340;&#27169;&#22411;&#20808;&#39564;&#12290;&#25105;&#20204;&#23545;&#29702;&#35770;&#35823;&#24046;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pan-sharpening algorithm utilizes panchromatic image and multispectral image to obtain a high spatial and high spectral image. However, the optimizations of the algorithms are designed with different standards. We adopt the simple matrix equation to describe the Pan-sharpening problem. The solution existence condition and the acquirement of spectral and spatial resolution are discussed. A down-sampling enhancement method was introduced for better acquiring the spatial and spectral down-sample matrices. By the generalized inverse theory, we derived two forms of general inverse matrix formulations that can correspond to the two prominent classes of Pan-sharpening methods, that is, component substitution and multi-resolution analysis methods. Specifically, the Gram Schmidt Adaptive(GSA) was proved to follow the general inverse matrix formulation of component substitution. A model prior to the general inverse matrix of the spectral function was rendered. The theoretical errors are analyzed
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;Gen-neG&#65292;&#23427;&#21033;&#29992;&#39069;&#22806;&#30340;&#36741;&#21161;&#20449;&#24687;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#26397;&#30528;&#27491;&#25903;&#25345;&#21306;&#22495;&#29983;&#25104;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;&#20013;&#30340;&#36991;&#30896;&#24212;&#29992;&#21644;&#23433;&#20840;&#38450;&#25252;&#20154;&#20307;&#21160;&#20316;&#29983;&#25104;&#20013;&#23637;&#29616;&#20102;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.16463</link><description>&lt;p&gt;
&#19981;&#35201;&#37027;&#20040;&#28040;&#26497;&#65281;&#24102;&#26377;Oracle&#36741;&#21161;&#25351;&#23548;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Don't be so negative! Score-based Generative Modeling with Oracle-assisted Guidance. (arXiv:2307.16463v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16463
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;Gen-neG&#65292;&#23427;&#21033;&#29992;&#39069;&#22806;&#30340;&#36741;&#21161;&#20449;&#24687;&#26469;&#25351;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#26397;&#30528;&#27491;&#25903;&#25345;&#21306;&#22495;&#29983;&#25104;&#26679;&#26412;&#65292;&#35813;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;&#20013;&#30340;&#36991;&#30896;&#24212;&#29992;&#21644;&#23433;&#20840;&#38450;&#25252;&#20154;&#20307;&#21160;&#20316;&#29983;&#25104;&#20013;&#23637;&#29616;&#20102;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#20284;&#28982;&#21407;&#21017;&#25552;&#20513;&#36890;&#36807;&#20248;&#21270;&#25968;&#25454;&#20284;&#28982;&#20989;&#25968;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#12290;&#20197;&#36825;&#31181;&#26041;&#24335;&#20272;&#35745;&#30340;&#27169;&#22411;&#21487;&#20197;&#23637;&#29616;&#20986;&#21508;&#31181;&#30001;&#26550;&#26500;&#12289;&#21442;&#25968;&#21270;&#21644;&#20248;&#21270;&#20559;&#24046;&#31561;&#22240;&#32032;&#20915;&#23450;&#30340;&#27867;&#21270;&#29305;&#24615;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#23384;&#22312;&#39069;&#22806;&#36741;&#21161;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#30340;&#27169;&#22411;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#36741;&#21161;&#20449;&#24687;&#20197;Oracle&#30340;&#24418;&#24335;&#23384;&#22312;&#65292;&#21487;&#20197;&#26631;&#35760;&#26679;&#26412;&#26159;&#21542;&#22788;&#20110;&#30495;&#23454;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#30340;&#25903;&#25345;&#33539;&#22260;&#20043;&#22806;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#24314;&#27169;&#65288;DDPM&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;Gen-neG&#65292;&#23427;&#21033;&#29992;&#20102;&#36825;&#20010;&#39069;&#22806;&#30340;&#36741;&#21161;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37492;&#21035;&#22120;&#25351;&#23548;&#65292;&#20197;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#26397;&#30528;Oracle&#25152;&#25351;&#31034;&#30340;&#27491;&#25903;&#25345;&#21306;&#22495;&#29983;&#25104;&#26679;&#26412;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#21160;&#39550;&#39542;&#27169;&#25311;&#22120;&#20013;&#30340;&#36991;&#30896;&#24212;&#29992;&#21644;&#23433;&#20840;&#38450;&#25252;&#20154;&#20307;&#21160;&#20316;&#29983;&#25104;&#20013;&#30340;&#23454;&#35777;&#39564;&#35777;&#20102;Gen-neG&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The maximum likelihood principle advocates parameter estimation via optimization of the data likelihood function. Models estimated in this way can exhibit a variety of generalization characteristics dictated by, e.g. architecture, parameterization, and optimization bias. This work addresses model learning in a setting where there further exists side-information in the form of an oracle that can label samples as being outside the support of the true data generating distribution. Specifically we develop a new denoising diffusion probabilistic modeling (DDPM) methodology, Gen-neG, that leverages this additional side-information. Our approach builds on generative adversarial networks (GANs) and discriminator guidance in diffusion models to guide the generation process towards the positive support region indicated by the oracle. We empirically establish the utility of Gen-neG in applications including collision avoidance in self-driving simulators and safety-guarded human motion generation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#31867;&#21035;&#65292;&#36890;&#36807;&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#27169;&#22411;&#65288;MZNMGs&#65289;&#26469;&#27169;&#25311;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#23616;&#37096;&#20132;&#20114;&#32467;&#26500;&#12290;&#20316;&#32773;&#30830;&#23450;&#20102;MG&#21487;&#34987;&#34920;&#31034;&#20026;MZNMG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20854;Markov CCE&#38598;&#21512;&#19982;Markov NE&#38598;&#21512;&#30456;&#31561;&#65307;&#27492;&#22806;&#65292;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MZNMG&#20013;&#25214;&#21040;&#36817;&#20284;&#30340;Markov&#31283;&#23450;CCE&#26159;PPAD&#38590;&#39064;&#65292;&#38500;&#38750;&#32593;&#32476;&#20855;&#26377;&#8220;&#26143;&#29366;&#32467;&#26500;&#8221;&#12290;</title><link>http://arxiv.org/abs/2307.09470</link><description>&lt;p&gt;
&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;
&lt;/p&gt;
&lt;p&gt;
Multi-Player Zero-Sum Markov Games with Networked Separable Interactions. (arXiv:2307.09470v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09470
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#31867;&#21035;&#65292;&#36890;&#36807;&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#27169;&#22411;&#65288;MZNMGs&#65289;&#26469;&#27169;&#25311;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#23616;&#37096;&#20132;&#20114;&#32467;&#26500;&#12290;&#20316;&#32773;&#30830;&#23450;&#20102;MG&#21487;&#34987;&#34920;&#31034;&#20026;MZNMG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#20854;Markov CCE&#38598;&#21512;&#19982;Markov NE&#38598;&#21512;&#30456;&#31561;&#65307;&#27492;&#22806;&#65292;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MZNMG&#20013;&#25214;&#21040;&#36817;&#20284;&#30340;Markov&#31283;&#23450;CCE&#26159;PPAD&#38590;&#39064;&#65292;&#38500;&#38750;&#32593;&#32476;&#20855;&#26377;&#8220;&#26143;&#29366;&#32467;&#26500;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26032;&#30340;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#31867;&#21035;&#65292;&#21363;&#24102;&#26377;&#32593;&#32476;&#21487;&#20998;&#31163;&#20132;&#20114;&#30340;&#22810;&#20154;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#65288;MZNMGs&#65289;&#65292;&#20197;&#27169;&#25311;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#39034;&#24207;&#20915;&#31574;&#20013;&#30340;&#23616;&#37096;&#20132;&#20114;&#32467;&#26500;&#12290;&#25105;&#20204;&#23558;MZNMG&#23450;&#20041;&#20026;&#19968;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#19982;&#27599;&#20010;&#29366;&#24577;&#30456;&#20851;&#30340;&#36741;&#21161;&#28216;&#25103;&#30340;&#25910;&#30410;&#26159;&#38646;&#21644;&#30340;&#65292;&#24182;&#19988;&#22312;&#26576;&#20010;&#20132;&#20114;&#32593;&#32476;&#19978;&#30340;&#37051;&#23621;&#20043;&#38388;&#20855;&#26377;&#19968;&#20123;&#21487;&#20998;&#31163;&#65288;&#21363;&#32858;&#21512;&#30697;&#38453;&#65289;&#32467;&#26500;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#33021;&#22815;&#34987;&#34920;&#31034;&#20026;MZNMG&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#19988;&#35777;&#26126;&#22312;&#36825;&#20123;&#28216;&#25103;&#20013;&#65292;&#39532;&#23572;&#21487;&#22827;&#31895;&#31961;&#30456;&#20851;&#22343;&#34913;&#65288;CCE&#65289;&#30340;&#38598;&#21512;&#32553;&#20943;&#20026;&#39532;&#23572;&#21487;&#22827;&#32435;&#20160;&#22343;&#34913;&#65288;NE&#65289;&#30340;&#38598;&#21512;&#65292;&#21363;&#21069;&#32773;&#23545;&#25152;&#26377;&#29609;&#23478;&#30340;&#27599;&#20010;&#29366;&#24577;&#30340;&#36793;&#38469;&#21270;&#20056;&#31215;&#32467;&#26524;&#24471;&#21040;&#21518;&#32773;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#26080;&#38480;&#26102;&#38388;&#25240;&#25187;MZNMGs&#20013;&#25214;&#21040;&#36817;&#20284;&#39532;&#23572;&#21487;&#22827;\emph{&#31283;&#23450;}CCE&#26159;PPAD&#38590;&#39064;&#65292;&#38500;&#38750;&#24213;&#23618;&#32593;&#32476;&#20855;&#26377;``&#26143;&#29366;&#32467;&#26500;''&#12290;
&lt;/p&gt;
&lt;p&gt;
We study a new class of Markov games (MGs), \textit{Multi-player Zero-sum Markov Games} with {\it Networked separable interactions} (MZNMGs), to model the local interaction structure in non-cooperative multi-agent sequential decision-making. We define an MZNMG as a model where {the payoffs of the auxiliary games associated with each state are zero-sum and} have some separable (i.e., polymatrix) structure across the neighbors over some interaction network. We first identify the necessary and sufficient conditions under which an MG can be presented as an MZNMG, and show that the set of Markov coarse correlated equilibrium (CCE) collapses to the set of Markov Nash equilibrium (NE) in these games, in that the {product of} per-state marginalization of the former for all players yields the latter. Furthermore, we show that finding approximate Markov \emph{stationary} CCE in infinite-horizon discounted MZNMGs is \texttt{PPAD}-hard, unless the underlying network has a ``star topology''. Then, 
&lt;/p&gt;</description></item><item><title>CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2306.04979</link><description>&lt;p&gt;
CoCo: &#19968;&#31181;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#30340;&#32806;&#21512;&#23545;&#27604;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive Graph Classification. (arXiv:2306.04979v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04979
&lt;/p&gt;
&lt;p&gt;
CoCo&#26159;&#19968;&#31181;&#32806;&#21512;&#23545;&#27604;&#22270;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#19968;&#20010;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#65292;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#65292;&#29992;&#20110;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#22270;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20998;&#31867;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#20219;&#21153;&#30340;&#26631;&#31614;&#65292;&#36825;&#21487;&#33021;&#38656;&#35201;&#26497;&#22823;&#30340;&#20195;&#20215;&#26469;&#33719;&#24471;&#12290;&#19968;&#31181;&#21487;&#38752;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25506;&#32034;&#20854;&#20182;&#26631;&#27880;&#22270;&#20197;&#22686;&#24378;&#30446;&#26631;&#22495;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65292;&#20294;&#22914;&#20309;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#24212;&#29992;&#21040;&#39046;&#22495;&#36866;&#24212;&#20013;&#20173;&#26410;&#35299;&#20915;&#65292;&#22240;&#20026;&#23545;&#22270;&#25299;&#25169;&#30340;&#19981;&#20805;&#20998;&#25506;&#32034;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#39046;&#22495;&#20559;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CoCo&#65288;Coupled Contrastive Graph Representation Learning&#65289;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#20174;&#32806;&#21512;&#23398;&#20064;&#20998;&#25903;&#20013;&#25552;&#21462;&#25299;&#25169;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#32806;&#21512;&#23545;&#27604;&#23398;&#20064;&#20943;&#23569;&#39046;&#22495;&#24046;&#24322;&#12290;CoCo&#21253;&#21547;&#19968;&#20010;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#25903;&#21644;&#20998;&#23618;&#22270;&#20869;&#26680;&#32593;&#32476;&#20998;&#25903;&#65292;&#20998;&#21035;&#29992;&#38544;&#24335;&#21644;&#26174;&#24335;&#26041;&#24335;&#25506;&#32034;&#22270;&#25299;&#25169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#32806;&#21512;&#20998;&#25903;&#32467;&#21512;&#21040;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35270;&#35282;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
Although graph neural networks (GNNs) have achieved impressive achievements in graph classification, they often need abundant task-specific labels, which could be extensively costly to acquire. A credible solution is to explore additional labeled graphs to enhance unsupervised learning on the target domain. However, how to apply GNNs to domain adaptation remains unsolved owing to the insufficient exploration of graph topology and the significant domain discrepancy. In this paper, we propose \underline{Co}upled \underline{Co}ntrastive Graph Representation Learning (\method{}), which extracts the topological information from coupled learning branches and reduces the domain discrepancy with coupled contrastive learning. \method{} contains a graph convolutional network branch and a hierarchical graph kernel network branch, which explore graph topology in implicit and explicit manners. Besides, we incorporate coupled branches into a holistic multi-view contrastive learning framework, which 
&lt;/p&gt;</description></item></channel></rss>