<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20150</link><description>&lt;p&gt;
TFB&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20840;&#38754;&#19988;&#20844;&#24179;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20150
&lt;/p&gt;
&lt;p&gt;
TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20250;&#22312;&#32463;&#27982;&#12289;&#20132;&#36890;&#12289;&#20581;&#24247;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#39046;&#22495;&#20013;&#20135;&#29983;&#65292;&#23545;&#26410;&#26469;&#25968;&#20540;&#30340;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#35768;&#22810;&#39044;&#27979;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#20026;&#20102;&#30830;&#20445;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#33021;&#22815;&#20197;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#26041;&#24335;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TFB&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;TFB&#36890;&#36807;&#35299;&#20915;&#19982;&#25968;&#25454;&#38598;&#12289;&#27604;&#36739;&#26041;&#27861;&#21644;&#35780;&#20272;&#31649;&#36947;&#30456;&#20851;&#30340;&#32570;&#28857;&#65292;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#65306;1&#65289;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#65292;2&#65289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;3&#65289;&#19981;&#19968;&#33268;&#21644;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#39046;&#22495;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#26469;&#33258;10&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#12289;&#30005;&#21147;&#12289;&#33021;&#28304;&#12289;&#29615;&#22659;&#12289;&#33258;&#28982;&#12289;&#32463;&#27982;&#12289;&#32929;&#31080;&#24066;&#22330;&#12289;&#38134;&#34892;&#12289;&#20581;&#24247;&#21644;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
&lt;/p&gt;</description></item><item><title>PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.19103</link><description>&lt;p&gt;
&#29992;&#20110;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33258;&#21160;&#21270;&#40657;&#30418;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
Automated Black-box Prompt Engineering for Personalized Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19103
&lt;/p&gt;
&lt;p&gt;
PRISM&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#65292;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#24037;&#31243;&#23545;&#20110;&#25511;&#21046;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#30001;&#20110;&#38656;&#35201;&#25163;&#21160;&#21046;&#20316;&#25552;&#31034;&#32780;&#23548;&#33268;&#24037;&#20316;&#32321;&#37325;&#12290;&#36825;&#19968;&#25361;&#25112;&#20419;&#20351;&#20102;&#33258;&#21160;&#25552;&#31034;&#29983;&#25104;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#22312;T2I&#27169;&#22411;&#20043;&#38388;&#30340;&#21487;&#20256;&#36882;&#24615;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#65292;&#38656;&#35201;&#23545;&#22522;&#30784;&#27169;&#22411;&#36827;&#34892;&#30333;&#30418;&#35775;&#38382;&#65292;&#24182;&#20135;&#29983;&#38750;&#30452;&#35266;&#30340;&#25552;&#31034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PRISM&#65292;&#36825;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#40657;&#30418;&#35775;&#38382;T2I&#27169;&#22411;&#23601;&#33258;&#21160;&#35782;&#21035;&#20154;&#31867;&#21487;&#35299;&#37322;&#19988;&#26131;&#20256;&#36882;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#26377;&#25928;&#29983;&#25104;&#25152;&#38656;&#27010;&#24565;&#12290;&#21463;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#29425;&#30340;&#21551;&#21457;&#65292;PRISM&#21033;&#29992;LLM&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#26469;&#36845;&#20195;&#22320;&#25913;&#36827;&#32473;&#23450;&#21442;&#32771;&#22270;&#20687;&#30340;&#20505;&#36873;&#25552;&#31034;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;PRISM&#22312;&#20026;&#23545;&#35937;&#12289;&#26679;&#24335;&#31561;&#29983;&#25104;&#20934;&#30830;&#25552;&#31034;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19103v1 Announce Type: cross  Abstract: Prompt engineering is effective for controlling the output of text-to-image (T2I) generative models, but it is also laborious due to the need for manually crafted prompts. This challenge has spurred the development of algorithms for automated prompt generation. However, these methods often struggle with transferability across T2I models, require white-box access to the underlying model, and produce non-intuitive prompts. In this work, we introduce PRISM, an algorithm that automatically identifies human-interpretable and transferable prompts that can effectively generate desired concepts given only black-box access to T2I models. Inspired by large language model (LLM) jailbreaking, PRISM leverages the in-context learning ability of LLMs to iteratively refine the candidate prompts distribution for given reference images. Our experiments demonstrate the versatility and effectiveness of PRISM in generating accurate prompts for objects, sty
&lt;/p&gt;</description></item><item><title>&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;Transformer-based&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#35268;&#21010;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;LPT&#33021;&#22815;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2402.04647</link><description>&lt;p&gt;
&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65306;&#35268;&#21010;&#20316;&#20026;&#28508;&#22312;&#21464;&#37327;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Latent Plan Transformer: Planning as Latent Variable Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04647
&lt;/p&gt;
&lt;p&gt;
&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;Transformer-based&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#36830;&#25509;&#36215;&#26469;&#65292;&#24182;&#21033;&#29992;&#28508;&#22312;&#31354;&#38388;&#36827;&#34892;&#35268;&#21010;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#65292;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#21464;&#37327;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#12290;&#23454;&#39564;&#35777;&#26126;LPT&#33021;&#22815;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36861;&#27714;&#38271;&#26399;&#22238;&#25253;&#30340;&#20219;&#21153;&#20013;&#65292;&#35268;&#21010;&#21464;&#24471;&#24517;&#35201;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35268;&#21010;&#30340;&#29983;&#25104;&#24314;&#27169;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22312;&#32570;&#20047;&#36880;&#27493;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#30340;&#25216;&#26415;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28508;&#22312;&#35745;&#21010;&#21464;&#25442;&#22120;&#65288;LPT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#20102;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#26469;&#36830;&#25509;&#22522;&#20110;Transformer&#30340;&#36712;&#36857;&#29983;&#25104;&#22120;&#21644;&#26368;&#32456;&#22238;&#25253;&#12290;LPT&#21487;&#20197;&#36890;&#36807;&#36712;&#36857;-&#22238;&#25253;&#23545;&#30340;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23398;&#20064;&#12290;&#22312;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#23545;&#28508;&#22312;&#21464;&#37327;&#30340;&#21518;&#39564;&#37319;&#26679;&#65292;&#23613;&#31649;&#26377;&#38480;&#30340;&#19978;&#19979;&#25991;&#65292;&#33258;&#28982;&#22320;&#32858;&#38598;&#23376;&#36712;&#36857;&#20197;&#24418;&#25104;&#19968;&#33268;&#30340;&#25277;&#35937;&#12290;&#22312;&#27979;&#35797;&#26102;&#65292;&#36890;&#36807;&#39044;&#26399;&#22238;&#25253;&#23545;&#28508;&#22312;&#21464;&#37327;&#36827;&#34892;&#25512;&#26029;&#65292;&#23454;&#29616;&#20102;&#35268;&#21010;&#20316;&#20026;&#25512;&#26029;&#30340;&#24605;&#24819;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;&#25972;&#20010;&#22238;&#21512;&#20013;&#25351;&#23548;&#33258;&#22238;&#24402;&#31574;&#30053;&#65292;&#36215;&#21040;&#19968;&#20010;&#35745;&#21010;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LPT&#21487;&#20197;&#20174;&#27425;&#20248;&#35299;&#20013;&#21457;&#29616;&#25913;&#36827;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
In tasks aiming for long-term returns, planning becomes necessary. We study generative modeling for planning with datasets repurposed from offline reinforcement learning. Specifically, we identify temporal consistency in the absence of step-wise rewards as one key technical challenge. We introduce the Latent Plan Transformer (LPT), a novel model that leverages a latent space to connect a Transformer-based trajectory generator and the final return. LPT can be learned with maximum likelihood estimation on trajectory-return pairs. In learning, posterior sampling of the latent variable naturally gathers sub-trajectories to form a consistent abstraction despite the finite context. During test time, the latent variable is inferred from an expected return before policy execution, realizing the idea of planning as inference. It then guides the autoregressive policy throughout the episode, functioning as a plan. Our experiments demonstrate that LPT can discover improved decisions from suboptima
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2302.06670</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Explainable Anomaly Detection in Images and Videos: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21487;&#35270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#20197;&#21450;&#20026;&#20309;&#21487;&#20197;&#21306;&#20998;&#24322;&#24120;&#30340;&#21512;&#29702;&#35299;&#37322;&#21364;&#21313;&#20998;&#31232;&#32570;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#39033;&#38598;&#20013;&#20110;&#21487;&#35299;&#37322;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#35843;&#30740;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22270;&#20687;&#32423;&#21644;&#35270;&#39057;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#26412;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#20316;&#20026;&#26412;&#35843;&#30740;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20840;&#38754;&#21644;&#35814;&#23613;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#21482;&#33021;&#24212;&#29992;&#20110;&#19968;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06670v2 Announce Type: replace-cross  Abstract: Anomaly detection and localization of visual data, including images and videos, are of great significance in both machine learning academia and applied real-world scenarios. Despite the rapid development of visual anomaly detection techniques in recent years, the interpretations of these black-box models and reasonable explanations of why anomalies can be distinguished out are scarce. This paper provides the first survey concentrated on explainable visual anomaly detection methods. We first introduce the basic background of image-level and video-level anomaly detection. Then, as the main content of this survey, a comprehensive and exhaustive literature review of explainable anomaly detection methods for both images and videos is presented. Next, we analyze why some explainable anomaly detection methods can be applied to both images and videos and why others can be only applied to one modality. Additionally, we provide summaries
&lt;/p&gt;</description></item><item><title>TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2311.01301</link><description>&lt;p&gt;
TRIALSCOPE&#65306;&#19968;&#20010;&#32479;&#19968;&#30340;&#22240;&#26524;&#26694;&#26550;&#65292;&#29992;&#20110;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE A Unifying Causal Framework for Scaling Real-World Evidence Generation with Biomedical Language Models. (arXiv:2311.01301v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01301
&lt;/p&gt;
&lt;p&gt;
TRIALSCOPE&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#23558;&#20020;&#24202;&#25991;&#26412;&#36827;&#34892;&#32467;&#26500;&#21270;&#65292;&#37319;&#29992;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#28151;&#26434;&#22240;&#32032;&#65292;&#20197;&#20174;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#35777;&#35777;&#25454;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#30340;&#24555;&#36895;&#25968;&#23383;&#21270;&#20026;&#20248;&#21270;&#21307;&#30103;&#26381;&#21153;&#21644;&#21152;&#36895;&#29983;&#29289;&#21307;&#23398;&#21457;&#29616;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#25968;&#25454;&#24448;&#24448;&#20197;&#38750;&#32467;&#26500;&#21270;&#24418;&#24335;&#23384;&#22312;&#65292;&#22914;&#30005;&#23376;&#21307;&#30103;&#35760;&#24405;&#20013;&#30340;&#20020;&#24202;&#31508;&#35760;&#65292;&#24182;&#19988;&#36890;&#24120;&#21463;&#21040;&#28151;&#26434;&#22240;&#32032;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;TRIALSCOPE&#65292;&#19968;&#20010;&#29992;&#20110;&#20174;&#20154;&#32676;&#32423;&#35266;&#23519;&#25968;&#25454;&#20013;&#25552;&#21462;&#23454;&#38469;&#19990;&#30028;&#35777;&#25454;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;TRIALSCOPE&#21033;&#29992;&#29983;&#29289;&#21307;&#23398;&#35821;&#35328;&#27169;&#22411;&#26469;&#25193;&#23637;&#35268;&#27169;&#21270;&#30340;&#20020;&#24202;&#25991;&#26412;&#65292;&#37319;&#29992;&#20808;&#36827;&#30340;&#27010;&#29575;&#24314;&#27169;&#36827;&#34892;&#21435;&#22122;&#21644;&#25554;&#34917;&#65292;&#24182;&#32467;&#21512;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#25512;&#26029;&#25216;&#26415;&#26469;&#24212;&#23545;&#24120;&#35265;&#30340;&#28151;&#26434;&#22240;&#32032;&#12290;&#21033;&#29992;&#20020;&#24202;&#35797;&#39564;&#35268;&#33539;&#20316;&#20026;&#36890;&#29992;&#34920;&#31034;&#24418;&#24335;&#65292;TRIALSCOPE&#25552;&#20379;&#20102;&#19968;&#20010;&#19968;&#38190;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20351;&#29992;&#35266;&#23519;&#25968;&#25454;&#29983;&#25104;&#21644;&#25512;&#29702;&#20020;&#24202;&#20551;&#35774;&#12290;&#22312;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;&#19968;&#30334;&#19975;&#20010;&#30284;&#30151;&#24739;&#32773;&#30340;&#22823;&#35268;&#27169;&#23454;&#38469;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid digitization of real-world data offers an unprecedented opportunity for optimizing healthcare delivery and accelerating biomedical discovery. In practice, however, such data is most abundantly available in unstructured forms, such as clinical notes in electronic medical records (EMRs), and it is generally plagued by confounders. In this paper, we present TRIALSCOPE, a unifying framework for distilling real-world evidence from population-level observational data. TRIALSCOPE leverages biomedical language models to structure clinical text at scale, employs advanced probabilistic modeling for denoising and imputation, and incorporates state-of-the-art causal inference techniques to combat common confounders. Using clinical trial specification as generic representation, TRIALSCOPE provides a turn-key solution to generate and reason with clinical hypotheses using observational data. In extensive experiments and analyses on a large-scale real-world dataset with over one million canc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.17176</link><description>&lt;p&gt;
&#20174;&#20840;&#26223;X&#23556;&#32447;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Deep Learning Approach to Teeth Segmentation and Orientation from Panoramic X-rays. (arXiv:2310.17176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20462;&#25913;&#24050;&#26377;&#27169;&#22411;&#24182;&#24341;&#20837;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#21644;&#39640;&#24615;&#33021;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#21644;&#29273;&#40831;&#23450;&#20301;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#22312;&#29616;&#20195;&#21475;&#33108;&#20445;&#20581;&#20013;&#26159;&#22522;&#30784;&#65292;&#21487;&#23454;&#29616;&#31934;&#30830;&#35786;&#26029;&#12289;&#27835;&#30103;&#35745;&#21010;&#21644;&#29273;&#40831;&#31181;&#26893;&#35774;&#35745;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#20174;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#20013;&#36827;&#34892;&#29273;&#40831;&#20998;&#21106;&#21644;&#23450;&#20301;&#12290;&#25105;&#20204;&#26681;&#25454;FUSegNet&#26500;&#24314;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#21021;&#29992;&#20110;&#21019;&#38754;&#20998;&#21106;&#30340;&#27969;&#34892;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#23558;&#22522;&#20110;&#32593;&#26684;&#30340;&#27880;&#24847;&#21147;&#38376;&#24341;&#20837;&#36339;&#36291;&#36830;&#25509;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#24341;&#20837;&#23450;&#21521;&#36793;&#30028;&#26694;&#65288;OBB&#65289;&#29983;&#25104;&#65292;&#20197;&#23454;&#29616;&#31934;&#30830;&#30340;&#29273;&#40831;&#23450;&#20301;&#20272;&#35745;&#12290;&#22312;&#20844;&#24320;&#21487;&#33719;&#24471;&#30340;DNS&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;543&#20010;&#20840;&#26223;X&#23556;&#32447;&#22270;&#20687;&#65292;&#25105;&#20204;&#22312;&#29273;&#40831;&#23454;&#20363;&#20998;&#21106;&#20013;&#24471;&#21040;&#20102;&#26368;&#39640;&#30340;&#20132;&#24182;&#27604;&#65288;IoU&#65289;&#24471;&#20998;82.43%&#65292;Dice&#30456;&#20284;&#31995;&#25968;&#65288;DSC&#65289;&#24471;&#20998;90.37%&#65292;&#22312;OBB&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26059;&#36716;&#30340;&#20132;&#24182;&#27604;&#65288;RIoU&#65289;&#24471;&#20998;82.82%&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate teeth segmentation and orientation are fundamental in modern oral healthcare, enabling precise diagnosis, treatment planning, and dental implant design. In this study, we present a comprehensive approach to teeth segmentation and orientation from panoramic X-ray images, leveraging deep learning techniques. We build our model based on FUSegNet, a popular model originally developed for wound segmentation, and introduce modifications by incorporating grid-based attention gates into the skip connections. We introduce oriented bounding box (OBB) generation through principal component analysis (PCA) for precise tooth orientation estimation. Evaluating our approach on the publicly available DNS dataset, comprising 543 panoramic X-ray images, we achieve the highest Intersection-over-Union (IoU) score of 82.43% and Dice Similarity Coefficient (DSC) score of 90.37% among compared models in teeth instance segmentation. In OBB analysis, we obtain the Rotated IoU (RIoU) score of 82.82%. We
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#21333;&#25351;&#25968;&#27169;&#22411;&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#20449;&#24687;&#20934;&#21017;&#26469;&#30830;&#23450;&#25903;&#25345;&#30340;&#22238;&#24402;&#31995;&#25968;&#22823;&#23567;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#35843;&#20248;&#38656;&#27714;&#65292;&#24182;&#20855;&#26377;&#23376;&#38598;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#39640;&#27010;&#29575;&#19979;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.06230</link><description>&lt;p&gt;
&#21333;&#25351;&#25968;&#27169;&#22411;&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#25193;&#23637;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Consistent and Scalable Algorithm for Best Subset Selection in Single Index Models. (arXiv:2309.06230v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#32500;&#21333;&#25351;&#25968;&#27169;&#22411;&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#19968;&#33268;&#24615;&#21644;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#24191;&#20041;&#20449;&#24687;&#20934;&#21017;&#26469;&#30830;&#23450;&#25903;&#25345;&#30340;&#22238;&#24402;&#31995;&#25968;&#22823;&#23567;&#65292;&#28040;&#38500;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#35843;&#20248;&#38656;&#27714;&#65292;&#24182;&#20855;&#26377;&#23376;&#38598;&#36873;&#25321;&#19968;&#33268;&#24615;&#21644;&#39640;&#27010;&#29575;&#19979;&#30340;&#29702;&#24819;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#20998;&#26512;&#24341;&#21457;&#20102;&#23545;&#21333;&#25351;&#25968;&#27169;&#22411;&#65288;SIMs&#65289;&#21644;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#22686;&#21152;&#20852;&#36259;&#12290;SIMs&#20026;&#39640;&#32500;&#25968;&#25454;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#28789;&#27963;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#32780;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#26088;&#22312;&#20174;&#22823;&#37327;&#30340;&#39044;&#27979;&#22240;&#23376;&#20013;&#25214;&#21040;&#31232;&#30095;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#27169;&#22411;&#20013;&#30340;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#34987;&#35748;&#20026;&#26159;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20542;&#21521;&#20110;&#25918;&#23485;&#36873;&#25321;&#65292;&#20294;&#19981;&#33021;&#24471;&#21040;&#26368;&#20339;&#23376;&#38598;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#31532;&#19968;&#20010;&#32463;&#36807;&#35777;&#26126;&#30340;&#38024;&#23545;&#39640;&#32500;SIMs&#20013;&#26368;&#20339;&#23376;&#38598;&#36873;&#25321;&#30340;&#21487;&#25193;&#23637;&#31639;&#27861;&#65292;&#30452;&#25509;&#35299;&#20915;&#20102;&#35745;&#31639;&#38590;&#39064;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#35299;&#20855;&#26377;&#23376;&#38598;&#36873;&#25321;&#19968;&#33268;&#24615;&#65292;&#24182;&#19988;&#20960;&#20046;&#32943;&#23450;&#20855;&#26377;&#29992;&#20110;&#21442;&#25968;&#20272;&#35745;&#30340;&#34394;&#25311;&#23646;&#24615;&#12290;&#35813;&#31639;&#27861;&#21253;&#25324;&#19968;&#20010;&#24191;&#20041;&#20449;&#24687;&#20934;&#21017;&#26469;&#30830;&#23450;&#22238;&#24402;&#31995;&#25968;&#30340;&#25903;&#25345;&#22823;&#23567;&#65292;&#28040;&#38500;&#27169;&#22411;&#36873;&#25321;&#35843;&#25972;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20551;&#35774;&#35823;&#24046;&#20998;&#24067;&#25110;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of high-dimensional data has led to increased interest in both single index models (SIMs) and best subset selection. SIMs provide an interpretable and flexible modeling framework for high-dimensional data, while best subset selection aims to find a sparse model from a large set of predictors. However, best subset selection in high-dimensional models is known to be computationally intractable. Existing methods tend to relax the selection, but do not yield the best subset solution. In this paper, we directly tackle the intractability by proposing the first provably scalable algorithm for best subset selection in high-dimensional SIMs. Our algorithmic solution enjoys the subset selection consistency and has the oracle property with a high probability. The algorithm comprises a generalized information criterion to determine the support size of the regression coefficients, eliminating the model selection tuning. Moreover, our method does not assume an error distribution or a specif
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;&#22312;&#26080;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#26469;&#36798;&#21040;Nash&#22343;&#34913;&#12290;</title><link>http://arxiv.org/abs/2309.04272</link><description>&lt;p&gt;
&#23398;&#20064;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Learning Zero-Sum Linear Quadratic Games with Improved Sample Complexity. (arXiv:2309.04272v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04272
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#26679;&#26412;&#22797;&#26434;&#24615;&#30340;&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#21338;&#24328;&#65292;&#24182;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;&#22312;&#26080;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#65292;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#26469;&#36798;&#21040;Nash&#22343;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#21644;&#32447;&#24615;&#20108;&#27425;&#65288;LQ&#65289;&#21338;&#24328;&#22312;&#26368;&#20248;&#25511;&#21046;&#20013;&#26159;&#22522;&#30784;&#24615;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#65288;i&#65289;&#39118;&#38505;&#25935;&#24863;&#25110;&#40065;&#26834;&#25511;&#21046;&#30340;&#21160;&#24577;&#21338;&#24328;&#24418;&#24335;&#65292;&#25110;&#32773;&#65288;ii&#65289;&#20316;&#20026;&#36830;&#32493;&#29366;&#24577;-&#25511;&#21046;&#31354;&#38388;&#20013;&#20004;&#20010;&#31454;&#20105;&#26234;&#33021;&#20307;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#35774;&#32622;&#12290;&#19982;&#24191;&#27867;&#30740;&#31350;&#30340;&#21333;&#26234;&#33021;&#20307;&#32447;&#24615;&#20108;&#27425;&#35843;&#33410;&#22120;&#38382;&#39064;&#19981;&#21516;&#65292;&#38646;&#21644;LQ&#21338;&#24328;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#20855;&#26377;&#32570;&#20047;&#24378;&#21046;&#24615;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38750;&#20984;&#38750;&#20985;&#26368;&#23567;-&#26368;&#22823;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#24352;&#31561;&#20154;&#21457;&#29616;&#20102;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#65292;&#36825;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#30340;&#25511;&#21046;&#31995;&#32479;&#38750;&#24120;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#20445;&#25345;&#20102;&#25511;&#21046;&#22120;&#30340;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#22312;&#27809;&#26377;&#27169;&#22411;&#21442;&#25968;&#30693;&#35782;&#30340;&#27169;&#22411;&#26080;&#20851;&#35774;&#32622;&#20013;&#65292;&#24352;&#31561;&#20154;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24615;&#31639;&#27861;&#65292;&#20197;&#36798;&#21040;Nash&#22343;&#34913;&#30340;&#949;-&#37051;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#29702;&#24819;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero-sum Linear Quadratic (LQ) games are fundamental in optimal control and can be used (i) as a dynamic game formulation for risk-sensitive or robust control, or (ii) as a benchmark setting for multi-agent reinforcement learning with two competing agents in continuous state-control spaces. In contrast to the well-studied single-agent linear quadratic regulator problem, zero-sum LQ games entail solving a challenging nonconvex-nonconcave min-max problem with an objective function that lacks coercivity. Recently, Zhang et al. discovered an implicit regularization property of natural policy gradient methods which is crucial for safety-critical control systems since it preserves the robustness of the controller during learning. Moreover, in the model-free setting where the knowledge of model parameters is not available, Zhang et al. proposed the first polynomial sample complexity algorithm to reach an $\epsilon$-neighborhood of the Nash equilibrium while maintaining the desirable implicit 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01316</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#39550;&#39542;&#29615;&#22659;&#21644;&#22810;&#26679;&#21270;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23384;&#22312;&#32473;&#20915;&#31574;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#27169;&#25311;&#29615;&#22659;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24102;&#26377;&#31526;&#21495;&#36923;&#36753;&#30340;DRL(DRLSL)&#65292;&#23427;&#23558;DRL(&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;)&#21644;&#31526;&#21495;&#19968;&#38454;&#36923;&#36753;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#30340;&#23454;&#26102;&#20132;&#20114;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#31215;&#26497;&#19982;&#29289;&#29702;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#25919;&#31574;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#32500;&#24230;&#25968;&#25454;&#23454;&#29616;&#20102;&#33258;&#20027;&#39550;&#39542;&#30340;DRLSL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#19968;&#33268;&#25512;&#26029;&#21644;&#32622;&#20449;&#24102;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.06578</link><description>&lt;p&gt;
&#26680;&#23725;&#22238;&#24402;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Kernel Ridge Regression Inference. (arXiv:2302.06578v2 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06578
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#30340;&#19968;&#33268;&#25512;&#26029;&#21644;&#32622;&#20449;&#24102;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20272;&#35745;&#22120;&#25552;&#20379;&#20102;&#20934;&#30830;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#26680;&#23725;&#22238;&#24402;(KRR)&#30340;&#19968;&#33268;&#25512;&#26029;&#21644;&#32622;&#20449;&#24102;&#65292;&#36825;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#21253;&#25324;&#25490;&#21517;&#12289;&#22270;&#20687;&#21644;&#22270;&#34920;&#22312;&#20869;&#30340;&#19968;&#33324;&#25968;&#25454;&#31867;&#22411;&#30340;&#38750;&#21442;&#25968;&#22238;&#24402;&#20272;&#35745;&#22120;&#12290;&#23613;&#31649;&#36825;&#20123;&#25968;&#25454;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#22914;&#23398;&#26657;&#20998;&#37197;&#20013;&#30340;&#25490;&#24207;&#20248;&#20808;&#32423;&#21015;&#34920;&#65292;&#20294;KRR&#30340;&#25512;&#26029;&#29702;&#35770;&#23578;&#26410;&#23436;&#20840;&#30693;&#24713;&#65292;&#38480;&#21046;&#20102;&#23427;&#22312;&#32463;&#27982;&#23398;&#21644;&#20854;&#20182;&#31185;&#23398;&#39046;&#22495;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#38024;&#23545;&#19968;&#33324;&#22238;&#24402;&#22120;&#30340;&#23574;&#38160;&#12289;&#19968;&#33268;&#30340;&#32622;&#20449;&#21306;&#38388;&#12290;&#20026;&#20102;&#36827;&#34892;&#25512;&#26029;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#20030;&#31243;&#24207;&#65292;&#36890;&#36807;&#23545;&#31216;&#21270;&#26469;&#28040;&#38500;&#20559;&#24046;&#24182;&#38480;&#21046;&#35745;&#31639;&#24320;&#38144;&#12290;&#20026;&#20102;&#35777;&#26126;&#35813;&#31243;&#24207;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;(RKHS)&#20013;&#37096;&#20998;&#21644;&#30340;&#26377;&#38480;&#26679;&#26412;&#12289;&#22343;&#21248;&#39640;&#26031;&#21644;&#33258;&#20030;&#32806;&#21512;&#12290;&#36825;&#20123;&#25512;&#23548;&#26263;&#31034;&#20102;&#22522;&#20110;RKHS&#21333;&#20301;&#29699;&#30340;&#32463;&#39564;&#36807;&#31243;&#30340;&#24378;&#36924;&#36817;&#65292;&#23545;&#35206;&#30422;&#25968;&#20855;&#26377;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#12290;&#27169;&#25311;&#39564;&#35777;&#20102;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide uniform inference and confidence bands for kernel ridge regression (KRR), a widely-used non-parametric regression estimator for general data types including rankings, images, and graphs. Despite the prevalence of these data -e.g., ranked preference lists in school assignment -- the inferential theory of KRR is not fully known, limiting its role in economics and other scientific domains. We construct sharp, uniform confidence sets for KRR, which shrink at nearly the minimax rate, for general regressors. To conduct inference, we develop an efficient bootstrap procedure that uses symmetrization to cancel bias and limit computational overhead. To justify the procedure, we derive finite-sample, uniform Gaussian and bootstrap couplings for partial sums in a reproducing kernel Hilbert space (RKHS). These imply strong approximation for empirical processes indexed by the RKHS unit ball with logarithmic dependence on the covering number. Simulations verify coverage. We use our proce
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.09949</link><description>&lt;p&gt;
&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#22312;&#35821;&#38899;&#22788;&#29702;&#20013;&#36827;&#34892;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Compressing Transformer-based self-supervised models for speech processing. (arXiv:2211.09949v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23545;&#22522;&#20110;Transformer&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#25913;&#21892;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;Transformer&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#24182;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#20294;&#26159;&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;&#35745;&#31639;&#25104;&#26412;&#20173;&#28982;&#26159;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#21508;&#31181;&#35774;&#22791;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#30446;&#21069;&#24050;&#26377;&#19968;&#20123;&#23396;&#31435;&#30340;&#23581;&#35797;&#26469;&#21387;&#32553;Transformer&#65292;&#20294;&#30740;&#31350;&#20013;&#30340;&#35774;&#32622;&#21644;&#25351;&#26631;&#21508;&#19981;&#30456;&#21516;&#12290;&#27492;&#21069;&#30340;&#24037;&#20316;&#24456;&#23569;&#28041;&#21450;&#19981;&#21516;&#21387;&#32553;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#36825;&#20351;&#24471;&#27604;&#36739;&#21387;&#32553;&#25216;&#26415;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#36825;&#20123;&#23396;&#31435;&#32467;&#26524;&#25552;&#20379;&#32972;&#26223;&#65292;&#30740;&#31350;&#20960;&#31181;&#24120;&#29992;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#21253;&#25324;&#26435;&#37325;&#20462;&#21098;&#12289;&#22836;&#37096;&#20462;&#21098;&#12289;&#20302;&#31209;&#36924;&#36817;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#22312;&#19981;&#21516;&#21387;&#32553;&#29575;&#19979;&#30340;&#26435;&#34913;&#65292;&#21253;&#25324;&#22681;&#38047;&#26102;&#38388;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#20056;&#21152;&#25805;&#20316;&#25968;&#37327;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#36817;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#22522;&#26412;&#30340;&#21387;&#32553;&#25216;&#26415;&#26159;&#24378;&#22823;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#20960;&#31181;&#21387;&#32553;&#26041;&#27861;&#26469;&#25913;&#36827;&#27169;&#22411;&#30340;&#21387;&#32553;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Transformers in self- supervised learning with applications to various downstream tasks, the computational cost of training and inference remains a major challenge for applying these models to a wide spectrum of devices. Several isolated attempts have been made to compress Transformers, but the settings and metrics are different across studies. Trade-off at various compression rates are also largely missing in prior work, making it difficult to compare compression techniques. In this work, we aim to provide context for the isolated results, studying several commonly used compression techniques, including weight pruning, head pruning, low-rank approximation, and knowledge distillation. We report trade- off at various compression rate, including wall-clock time, the number of parameters, and the number of multiply-accumulate operations. Our results show that compared to recent approaches, basic compression techniques are strong baselines. We further present several
&lt;/p&gt;</description></item></channel></rss>