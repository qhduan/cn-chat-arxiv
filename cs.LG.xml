<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20301;&#39537;&#21160;&#30340;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#26694;&#26550;PhASER&#65292;&#36890;&#36807;&#30456;&#20301;&#22686;&#24378;&#12289;&#20998;&#31163;&#29305;&#24449;&#32534;&#30721;&#21644;&#29305;&#24449;&#24191;&#25773;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#25968;&#25454;&#30340;&#36890;&#29992;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05960</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20301;&#39537;&#21160;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#36890;&#29992;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Phase-driven Domain Generalizable Learning for Nonstationary Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05960
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30456;&#20301;&#39537;&#21160;&#30340;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#26694;&#26550;PhASER&#65292;&#36890;&#36807;&#30456;&#20301;&#22686;&#24378;&#12289;&#20998;&#31163;&#29305;&#24449;&#32534;&#30721;&#21644;&#29305;&#24449;&#24191;&#25773;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#38750;&#24179;&#31283;&#25968;&#25454;&#30340;&#36890;&#29992;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#21644;&#35782;&#21035;&#36830;&#32493;&#24863;&#30693;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#26159;&#38750;&#24179;&#31283;&#30340;&#65292;&#20854;&#32479;&#35745;&#21644;&#35889;&#29305;&#24615;&#38543;&#26102;&#38388;&#21464;&#21270;&#12290;&#36825;&#22312;&#24320;&#21457;&#33021;&#22815;&#26377;&#25928;&#27867;&#21270;&#19981;&#21516;&#20998;&#24067;&#30340;&#23398;&#20064;&#27169;&#22411;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#38750;&#24179;&#31283;&#32479;&#35745;&#19982;&#30456;&#20301;&#20449;&#24687;&#20869;&#22312;&#30456;&#20851;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#26694;&#26550;PhASER&#12290;&#23427;&#21253;&#25324;&#19977;&#20010;&#26032;&#39062;&#30340;&#20803;&#32032;&#65306;1&#65289;&#30456;&#20301;&#22686;&#24378;&#65292;&#20351;&#38750;&#24179;&#31283;&#24615;&#22810;&#26679;&#21270;&#21516;&#26102;&#20445;&#30041;&#26377;&#21306;&#21035;&#24615;&#30340;&#35821;&#20041;&#65307;2&#65289;&#23558;&#26102;&#21464;&#24133;&#24230;&#21644;&#30456;&#20301;&#35270;&#20026;&#29420;&#31435;&#27169;&#24577;&#36827;&#34892;&#21333;&#29420;&#29305;&#24449;&#32534;&#30721;&#65307;3&#65289;&#21033;&#29992;&#26032;&#39062;&#30340;&#27531;&#24046;&#36830;&#25509;&#23558;&#30456;&#20301;&#19982;&#29305;&#24449;&#32467;&#21512;&#65292;&#20197;&#24378;&#21270;&#20998;&#24067;&#19981;&#21464;&#24615;&#23398;&#20064;&#30340;&#22266;&#26377;&#27491;&#21017;&#21270;&#20316;&#29992;&#12290;&#36890;&#36807;&#22312;5&#20010;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;
&lt;/p&gt;
&lt;p&gt;
Monitoring and recognizing patterns in continuous sensing data is crucial for many practical applications. These real-world time-series data are often nonstationary, characterized by varying statistical and spectral properties over time. This poses a significant challenge in developing learning models that can effectively generalize across different distributions. In this work, based on our observation that nonstationary statistics are intrinsically linked to the phase information, we propose a time-series learning framework, PhASER. It consists of three novel elements: 1) phase augmentation that diversifies non-stationarity while preserving discriminatory semantics, 2) separate feature encoding by viewing time-varying magnitude and phase as independent modalities, and 3) feature broadcasting by incorporating phase with a novel residual connection for inherent regularization to enhance distribution invariant learning. Upon extensive evaluation on 5 datasets from human activity recognit
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#30340;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#20998;&#26512;&#25152;&#38656;&#30340;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2312.10107</link><description>&lt;p&gt;
&#36808;&#21521;&#38754;&#21521;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#65306;&#29702;&#35299;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#30340;&#22909;&#22788;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10107
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#19978;&#19979;&#25991;&#24863;&#30693;&#39046;&#22495;&#27867;&#21270;&#30340;&#26465;&#20214;&#65292;&#25552;&#20986;&#20102;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#20998;&#26512;&#25152;&#38656;&#30340;&#26631;&#20934;&#65292;&#24182;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#21487;&#20197;&#26816;&#27979;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20851;&#20110;&#36755;&#20837;$X$&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#22914;&#20309;&#25913;&#21892;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#39044;&#27979;&#30340;&#26465;&#20214;&#12290;&#22312;&#39046;&#22495;&#27867;&#21270;&#20013;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#19978;&#19979;&#25991;&#30340;&#27010;&#24565;&#24418;&#24335;&#21270;&#20026;&#19968;&#32452;&#25968;&#25454;&#28857;&#30340;&#25490;&#21015;&#19981;&#21464;&#34920;&#31034;&#65292;&#36825;&#20123;&#25968;&#25454;&#28857;&#26469;&#33258;&#20110;&#19982;&#36755;&#20837;&#26412;&#36523;&#30456;&#21516;&#30340;&#22495;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#26041;&#27861;&#22312;&#21407;&#21017;&#19978;&#21487;&#20197;&#20135;&#29983;&#22909;&#22788;&#30340;&#26465;&#20214;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#24182;&#21046;&#23450;&#20102;&#20004;&#20010;&#22312;&#23454;&#36341;&#20013;&#21487;&#20197;&#36731;&#26494;&#39564;&#35777;&#30340;&#24517;&#35201;&#26631;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#36793;&#32536;&#20256;&#36882;&#23398;&#20064;&#26041;&#27861;&#26377;&#26395;&#20855;&#26377;&#31283;&#20581;&#24615;&#30340;&#20998;&#24067;&#21464;&#21270;&#31867;&#22411;&#30340;&#35265;&#35299;&#12290;&#23454;&#35777;&#20998;&#26512;&#34920;&#26126;&#25105;&#20204;&#30340;&#26631;&#20934;&#26377;&#25928;&#22320;&#21306;&#20998;&#20102;&#26377;&#21033;&#21644;&#19981;&#21033;&#30340;&#22330;&#26223;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#21487;&#38752;&#22320;&#26816;&#27979;&#27169;&#22411;&#38754;&#20020;&#38750;&#24120;&#25968;&#22495;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10107v2 Announce Type: replace-cross  Abstract: In this work, we analyze the conditions under which information about the context of an input $X$ can improve the predictions of deep learning models in new domains. Following work in marginal transfer learning in Domain Generalization (DG), we formalize the notion of context as a permutation-invariant representation of a set of data points that originate from the same domain as the input itself. We offer a theoretical analysis of the conditions under which this approach can, in principle, yield benefits, and formulate two necessary criteria that can be easily verified in practice. Additionally, we contribute insights into the kind of distribution shifts for which the marginal transfer learning approach promises robustness. Empirical analysis shows that our criteria are effective in discerning both favorable and unfavorable scenarios. Finally, we demonstrate that we can reliably detect scenarios where a model is tasked with unw
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34507;&#30333;&#36136;&#20316;&#20026;3D&#32593;&#26684;&#30340;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#34920;&#38754;&#30340;&#21327;&#21516;&#26041;&#27861;&#65292;&#26082;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#21448;&#26377;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.16519</link><description>&lt;p&gt;
AtomSurf&#65306;&#34507;&#30333;&#36136;&#32467;&#26500;&#19978;&#30340;&#23398;&#20064;&#30340;&#34920;&#38754;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AtomSurf : Surface Representation for Learning on Protein Structures. (arXiv:2309.16519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#34507;&#30333;&#36136;&#20316;&#20026;3D&#32593;&#26684;&#30340;&#34920;&#38754;&#34920;&#31034;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22270;&#34920;&#38754;&#30340;&#21327;&#21516;&#26041;&#27861;&#65292;&#26082;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#21448;&#26377;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;Cryo-EM&#21644;&#34507;&#30333;&#36136;&#32467;&#26500;&#39044;&#27979;&#31639;&#27861;&#30340;&#36827;&#23637;&#20351;&#24471;&#22823;&#35268;&#27169;&#34507;&#30333;&#36136;&#32467;&#26500;&#21487;&#33719;&#24471;&#65292;&#20026;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#21151;&#33021;&#27880;&#37322;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;&#20960;&#20309;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20851;&#27880;&#21019;&#24314;&#36866;&#29992;&#20110;&#20960;&#20309;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20174;&#34507;&#30333;&#36136;&#32467;&#26500;&#20013;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#26159;&#23558;&#36825;&#20123;&#32467;&#26500;&#34920;&#31034;&#20026;&#20960;&#20309;&#23545;&#35937;&#65288;&#22914;&#32593;&#26684;&#12289;&#22270;&#25110;&#34920;&#38754;&#65289;&#24182;&#24212;&#29992;&#36866;&#21512;&#36825;&#31181;&#34920;&#31034;&#24418;&#24335;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#32473;&#23450;&#26041;&#27861;&#30340;&#24615;&#33021;&#23558;&#21462;&#20915;&#20110;&#34920;&#31034;&#21644;&#30456;&#24212;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#23558;&#34507;&#30333;&#36136;&#34920;&#31034;&#20026;$\textit{3D mesh surfaces}$&#24182;&#23558;&#20854;&#32435;&#20837;&#24050;&#24314;&#31435;&#30340;&#34920;&#31034;&#22522;&#20934;&#20013;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21457;&#29616;&#26159;&#65292;&#23613;&#31649;&#26377;&#30528;&#26377;&#24076;&#26395;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#20165;&#21333;&#29420;&#34920;&#38754;&#34920;&#31034;&#20284;&#20046;&#26080;&#27861;&#19982;3D&#32593;&#26684;&#31454;&#20105;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21327;&#21516;&#26041;&#27861;&#65292;&#23558;&#34920;&#38754;&#34920;&#31034;&#19982;&#22270;&#34920;&#38754;&#32467;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Cryo-EM and protein structure prediction algorithms have made large-scale protein structures accessible, paving the way for machine learning-based functional annotations.The field of geometric deep learning focuses on creating methods working on geometric data. An essential aspect of learning from protein structures is representing these structures as a geometric object (be it a grid, graph, or surface) and applying a learning method tailored to this representation. The performance of a given approach will then depend on both the representation and its corresponding learning method.  In this paper, we investigate representing proteins as $\textit{3D mesh surfaces}$ and incorporate them into an established representation benchmark. Our first finding is that despite promising preliminary results, the surface representation alone does not seem competitive with 3D grids. Building on this, we introduce a synergistic approach, combining surface representations with gra
&lt;/p&gt;</description></item><item><title>DR-VIDAL&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24178;&#39044;&#25514;&#26045;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#22788;&#29702;&#28151;&#28102;&#20559;&#24046;&#21644;&#27169;&#22411;&#19981;&#33391;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.04201</link><description>&lt;p&gt;
DR-VIDAL--&#21452;&#37325;&#31283;&#20581;&#21464;&#20998;&#20449;&#24687;&#35770;&#28145;&#24230;&#23545;&#25239;&#23398;&#20064;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21453;&#20107;&#23454;&#39044;&#27979;&#21644;&#27835;&#30103;&#25928;&#26524;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
DR-VIDAL -- Doubly Robust Variational Information-theoretic Deep Adversarial Learning for Counterfactual Prediction and Treatment Effect Estimation on Real World Data. (arXiv:2303.04201v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04201
&lt;/p&gt;
&lt;p&gt;
DR-VIDAL&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20013;&#30340;&#24178;&#39044;&#25514;&#26045;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#65292;&#24182;&#20855;&#26377;&#22788;&#29702;&#28151;&#28102;&#20559;&#24046;&#21644;&#27169;&#22411;&#19981;&#33391;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#30495;&#23454;&#19990;&#30028;&#30340;&#35266;&#23519;&#24615;&#65288;&#38750;&#38543;&#26426;&#21270;&#65289;&#25968;&#25454;&#20013;&#30830;&#23450;&#24178;&#39044;&#25514;&#26045;&#23545;&#32467;&#26524;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#20363;&#22914;&#20351;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#27835;&#30103;&#37325;&#29992;&#65292;&#30001;&#20110;&#28508;&#22312;&#20559;&#24046;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22240;&#26524;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#25913;&#36827;&#20102;&#20256;&#32479;&#25216;&#26415;&#65292;&#29992;&#20110;&#20272;&#35745;&#20010;&#24615;&#21270;&#27835;&#30103;&#25928;&#26524;&#65288;ITE&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21452;&#37325;&#31283;&#20581;&#21464;&#20998;&#20449;&#24687;&#35770;&#28145;&#24230;&#23545;&#25239;&#23398;&#20064;&#65288;DR-VIDAL&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#27835;&#30103;&#21644;&#32467;&#26524;&#20004;&#20010;&#32852;&#21512;&#27169;&#22411;&#30340;&#26032;&#22411;&#29983;&#25104;&#26694;&#26550;&#65292;&#30830;&#20445;&#26080;&#20559;&#30340;ITE&#20272;&#35745;&#65292;&#21363;&#20351;&#20854;&#20013;&#19968;&#20010;&#27169;&#22411;&#35774;&#23450;&#19981;&#27491;&#30830;&#12290;DR-VIDAL&#25972;&#21512;&#20102;&#65306; &#65288;i&#65289;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26681;&#25454;&#22240;&#26524;&#20551;&#35774;&#23558;&#28151;&#28102;&#21464;&#37327;&#20998;&#35299;&#20026;&#28508;&#22312;&#21464;&#37327;; &#65288;ii&#65289;&#22522;&#20110;&#20449;&#24687;&#35770;&#30340;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;Info-GAN&#65289;&#29992;&#20110;&#29983;&#25104;&#21453;&#20107;&#23454;&#24773;&#20917;; &#65288;iii&#65289;&#19968;&#20010;&#21452;&#37325;&#31283;&#20581;&#22359;&#65292;&#20854;&#20013;&#21253;&#25324;&#27835;&#30103;&#20542;&#21521;&#20110;&#39044;&#27979;&#32467;&#26524;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#65288;Infant Health&#21644;Development Program&#65292;Transforming Clinical Practice Initiative [TCPI]&#65289;&#20013;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;DR-VIDAL&#22312;&#20272;&#35745;ITE&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#20855;&#26377;&#22788;&#29702;&#28151;&#28102;&#20559;&#24046;&#21644;&#27169;&#22411;&#19981;&#27491;&#30830;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining causal effects of interventions onto outcomes from real-world, observational (non-randomized) data, e.g., treatment repurposing using electronic health records, is challenging due to underlying bias. Causal deep learning has improved over traditional techniques for estimating individualized treatment effects (ITE). We present the Doubly Robust Variational Information-theoretic Deep Adversarial Learning (DR-VIDAL), a novel generative framework that combines two joint models of treatment and outcome, ensuring an unbiased ITE estimation even when one of the two is misspecified. DR-VIDAL integrates: (i) a variational autoencoder (VAE) to factorize confounders into latent variables according to causal assumptions; (ii) an information-theoretic generative adversarial network (Info-GAN) to generate counterfactuals; (iii) a doubly robust block incorporating treatment propensities for outcome predictions. On synthetic and real-world datasets (Infant Health and Development Program, T
&lt;/p&gt;</description></item></channel></rss>