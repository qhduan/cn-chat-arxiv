<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2402.06512</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multimodal Clinical Trial Outcome Prediction with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFTED&#30340;&#22810;&#27169;&#24577;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#25968;&#25454;&#65292;&#24182;&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#26159;&#19968;&#20010;&#20851;&#38190;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#36890;&#24120;&#38656;&#35201;&#22810;&#24180;&#26102;&#38388;&#21644;&#22823;&#37327;&#36130;&#21147;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#26088;&#22312;&#25490;&#38500;&#21487;&#33021;&#22833;&#36133;&#30340;&#33647;&#29289;&#65292;&#24182;&#20855;&#26377;&#26174;&#33879;&#30340;&#25104;&#26412;&#33410;&#32422;&#28508;&#21147;&#12290;&#26368;&#36817;&#30340;&#25968;&#25454;&#39537;&#21160;&#23581;&#35797;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25972;&#21512;&#22810;&#27169;&#24577;&#25968;&#25454;&#26469;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#65292;&#36825;&#38480;&#21046;&#20102;&#36866;&#24212;&#26032;&#27169;&#24577;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#35782;&#21035;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30456;&#20284;&#20449;&#24687;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#65288;LIFTED&#65289;&#26041;&#27861;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LIFTED&#36890;&#36807;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26469;&#32479;&#19968;&#19981;&#21516;&#27169;&#24577;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;LIFTED&#26500;&#24314;&#32479;&#19968;&#30340;&#25239;&#22122;&#22768;&#32534;&#30721;&#22120;&#65292;&#20174;&#27169;&#24577;&#29305;&#23450;&#30340;&#35821;&#35328;&#25551;&#36848;&#20013;&#25552;&#21462;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
The clinical trial is a pivotal and costly process, often spanning multiple years and requiring substantial financial resources. Therefore, the development of clinical trial outcome prediction models aims to exclude drugs likely to fail and holds the potential for significant cost savings. Recent data-driven attempts leverage deep learning methods to integrate multimodal data for predicting clinical trial outcomes. However, these approaches rely on manually designed modal-specific encoders, which limits both the extensibility to adapt new modalities and the ability to discern similar information patterns across different modalities. To address these issues, we propose a multimodal mixture-of-experts (LIFTED) approach for clinical trial outcome prediction. Specifically, LIFTED unifies different modality data by transforming them into natural language descriptions. Then, LIFTED constructs unified noise-resilient encoders to extract information from modal-specific language descriptions. S
&lt;/p&gt;</description></item><item><title>&#22312;&#37327;&#21270;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;&#27169;&#25311;&#37327;&#21270;&#35823;&#24046;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30828;&#20214;&#23454;&#29616;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>https://arxiv.org/abs/2402.02593</link><description>&lt;p&gt;
&#22312;&#37327;&#21270;&#22122;&#22768;&#29615;&#22659;&#20013;&#21033;&#29992;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Leveraging Continuously Differentiable Activation Functions for Learning in Quantized Noisy Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02593
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37327;&#21270;&#22122;&#22768;&#29615;&#22659;&#20013;&#65292;&#21033;&#29992;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#36827;&#34892;&#23398;&#20064;&#21487;&#20197;&#20943;&#36731;&#27169;&#25311;&#37327;&#21270;&#35823;&#24046;&#65292;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30828;&#20214;&#23454;&#29616;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#27169;&#25311;&#31995;&#32479;&#22266;&#26377;&#22320;&#21463;&#21040;&#22122;&#22768;&#30340;&#24433;&#21709;&#65292;&#36825;&#21487;&#33021;&#20250;&#38459;&#30861;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20687;GELU&#21644;SiLU&#36825;&#26679;&#30340;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#21487;&#20197;&#31283;&#20581;&#22320;&#20256;&#25773;&#26799;&#24230;&#65292;&#26377;&#21161;&#20110;&#20943;&#36731;&#26222;&#36941;&#23384;&#22312;&#20110;&#25152;&#26377;&#27169;&#25311;&#31995;&#32479;&#20013;&#30340;&#27169;&#25311;&#37327;&#21270;&#35823;&#24046;&#12290;&#25105;&#20204;&#22312;&#37327;&#21270;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20102;&#21367;&#31215;&#12289;&#32447;&#24615;&#21644;Transformer&#32593;&#32476;&#30340;&#20998;&#26512;&#21644;&#35757;&#32451;&#12290;&#25105;&#20204;&#33021;&#22815;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#20462;&#27491;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30456;&#27604;&#65292;&#36830;&#32493;&#21487;&#24494;&#28608;&#27963;&#20989;&#25968;&#22312;&#25239;&#22122;&#22768;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#19982;ReLU&#30456;&#27604;&#65292;&#22312;&#25509;&#36817;&#38646;&#26102;&#26799;&#24230;&#35823;&#24046;&#39640;&#20986;100&#20493;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#20026;&#36873;&#25321;&#36866;&#24403;&#30340;&#28608;&#27963;&#20989;&#25968;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20449;&#21495;&#22788;&#29702;&#31561;&#22810;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#20855;&#26377;&#39640;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#30340;&#30828;&#20214;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world analog systems intrinsically suffer from noise that can impede model convergence and accuracy on a variety of deep learning models. We demonstrate that differentiable activations like GELU and SiLU enable robust propagation of gradients which help to mitigate analog quantization error that is ubiquitous to all analog systems. We perform analysis and training of convolutional, linear, and transformer networks in the presence of quantized noise. Here, we are able to demonstrate that continuously differentiable activation functions are significantly more noise resilient over conventional rectified activations. As in the case of ReLU, the error in gradients are 100x higher than those in GELU near zero. Our findings provide guidance for selecting appropriate activations to realize performant and reliable hardware implementations across several machine learning domains such as computer vision, signal processing, and beyond.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.11655</link><description>&lt;p&gt;
&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bandits with Deterministically Evolving States. (arXiv:2307.11655v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11655
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#24102;&#26377;&#24378;&#30423;&#21453;&#39304;&#30340;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#65292;&#33021;&#20934;&#30830;&#35780;&#20272;&#22870;&#21169;&#19982;&#31995;&#32479;&#20581;&#24247;&#31243;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#19982;&#24378;&#30423;&#21453;&#39304;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#30830;&#23450;&#24615;&#28436;&#21270;&#21644;&#19981;&#21487;&#35266;&#27979;&#30340;&#29366;&#24577;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#20855;&#26377;&#30830;&#23450;&#24615;&#28436;&#21270;&#29366;&#24577;&#30340;&#24378;&#30423;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20027;&#35201;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#21644;&#22312;&#32447;&#24191;&#21578;&#30340;&#23398;&#20064;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#22312;&#27599;&#19968;&#36718;&#33719;&#24471;&#30340;&#22870;&#21169;&#26159;&#36873;&#25321;&#34892;&#21160;&#30340;&#30701;&#26399;&#22870;&#21169;&#21644;&#31995;&#32479;&#30340;&#8220;&#20581;&#24247;&#8221;&#31243;&#24230;&#65288;&#21363;&#36890;&#36807;&#20854;&#29366;&#24577;&#27979;&#37327;&#65289;&#30340;&#20989;&#25968;&#12290;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#24179;&#21488;&#20174;&#29992;&#25143;&#23545;&#29305;&#23450;&#31867;&#22411;&#20869;&#23481;&#30340;&#21442;&#19982;&#20013;&#33719;&#24471;&#30340;&#22870;&#21169;&#19981;&#20165;&#21462;&#20915;&#20110;&#20855;&#20307;&#20869;&#23481;&#30340;&#22266;&#26377;&#29305;&#24449;&#65292;&#36824;&#21462;&#20915;&#20110;&#29992;&#25143;&#19982;&#24179;&#21488;&#19978;&#20854;&#20182;&#31867;&#22411;&#20869;&#23481;&#20114;&#21160;&#21518;&#20854;&#20559;&#22909;&#30340;&#28436;&#21270;&#12290;&#25105;&#20204;&#30340;&#36890;&#29992;&#27169;&#22411;&#32771;&#34385;&#20102;&#29366;&#24577;&#28436;&#21270;&#30340;&#19981;&#21516;&#36895;&#29575;&#955;&#8712;[0,1]&#65288;&#20363;&#22914;&#65292;&#29992;&#25143;&#30340;&#20559;&#22909;&#22240;&#20808;&#21069;&#20869;&#23481;&#28040;&#36153;&#32780;&#24555;&#36895;&#21464;&#21270;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a model for learning with bandit feedback while accounting for deterministically evolving and unobservable states that we call Bandits with Deterministically Evolving States. The workhorse applications of our model are learning for recommendation systems and learning for online ads. In both cases, the reward that the algorithm obtains at each round is a function of the short-term reward of the action chosen and how ``healthy'' the system is (i.e., as measured by its state). For example, in recommendation systems, the reward that the platform obtains from a user's engagement with a particular type of content depends not only on the inherent features of the specific content, but also on how the user's preferences have evolved as a result of interacting with other types of content on the platform. Our general model accounts for the different rate $\lambda \in [0,1]$ at which the state evolves (e.g., how fast a user's preferences shift as a result of previous content consumption
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05772</link><description>&lt;p&gt;
&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38169;&#35823;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36825;&#31361;&#20986;&#20102;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#30830;&#23450;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25163;&#27573;&#65292;&#8220;&#30693;&#36947;&#19968;&#20010;&#27169;&#22411;&#19981;&#30693;&#36947;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#65292;&#20854;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20351;&#29992;&#38543;&#26426;&#38598;&#21512;&#30340;&#25968;&#23398;&#65292;&#21363;&#23545;&#26679;&#26412;&#31354;&#38388;&#30340;&#24130;&#38598;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38543;&#26426;&#38598;&#27169;&#22411;&#33021;&#22815;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#20013;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#8220;&#35748;&#35782;&#24615;&#8221;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#32622;&#20449;&#38598;&#30340;&#22823;&#23567;&#26469;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief func
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.16703</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304; -- &#19968;&#20010;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Sources of Uncertainty in Machine Learning -- A Statisticians' View. (arXiv:2305.16703v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21644;&#31867;&#22411;&#65292;&#20174;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#20986;&#21457;&#65292;&#20998;&#31867;&#21035;&#20171;&#32461;&#20102;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#21508;&#24322;&#65292;&#19981;&#21487;&#31616;&#21333;&#24402;&#20026;&#20004;&#31867;&#12290;&#21516;&#26102;&#65292;&#19982;&#32479;&#35745;&#23398;&#27010;&#24565;&#36827;&#34892;&#31867;&#27604;&#65292;&#25506;&#35752;&#19981;&#30830;&#23450;&#24615;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#25104;&#23601;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#22238;&#31572;&#20960;&#24180;&#21069;&#38590;&#20197;&#24819;&#35937;&#30340;&#38382;&#39064;&#12290;&#38500;&#20102;&#36825;&#20123;&#25104;&#21151;&#20043;&#22806;&#65292;&#36234;&#26469;&#36234;&#28165;&#26224;&#30340;&#26159;&#65292;&#22312;&#32431;&#39044;&#27979;&#20043;&#22806;&#65292;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#20063;&#26159;&#30456;&#20851;&#21644;&#24517;&#35201;&#30340;&#12290;&#34429;&#28982;&#36817;&#24180;&#26469;&#24050;&#32463;&#20986;&#29616;&#20102;&#36825;&#26041;&#38754;&#30340;&#31532;&#19968;&#25209;&#27010;&#24565;&#21644;&#24605;&#24819;&#65292;&#20294;&#26412;&#25991;&#37319;&#29992;&#20102;&#19968;&#20010;&#27010;&#24565;&#24615;&#30340;&#35270;&#35282;&#65292;&#24182;&#25506;&#35752;&#20102;&#21487;&#33021;&#30340;&#19981;&#30830;&#23450;&#24615;&#26469;&#28304;&#12290;&#36890;&#36807;&#37319;&#29992;&#32479;&#35745;&#23398;&#23478;&#30340;&#35270;&#35282;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19982;&#26426;&#22120;&#23398;&#20064;&#26356;&#24120;&#35265;&#30456;&#20851;&#30340;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#19981;&#30830;&#23450;&#24615;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#26088;&#22312;&#35268;&#33539;&#36825;&#20004;&#31181;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35777;&#26126;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#21508;&#24322;&#65292;&#24182;&#19988;&#19981;&#24635;&#26159;&#21487;&#20197;&#20998;&#35299;&#20026;&#38543;&#26426;&#24615;&#21644;&#35748;&#30693;&#24615;&#12290;&#36890;&#36807;&#23558;&#32479;&#35745;&#27010;&#24565;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#31867;&#27604;&#65292;&#25105;&#20204;&#20063;&#23637;&#31034;&#20102;&#32479;&#35745;&#23398;&#27010;&#24565;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the rol
&lt;/p&gt;</description></item><item><title>QuantumNAT&#26159;&#19968;&#20010;PQC&#29305;&#23450;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#25191;&#34892;&#22122;&#22768;&#24863;&#30693;&#20248;&#21270;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32531;&#35299;&#37327;&#23376;&#22122;&#22768;</title><link>http://arxiv.org/abs/2110.11331</link><description>&lt;p&gt;
QuantumNAT&#65306;&#27880;&#37325;&#37327;&#23376;&#22122;&#22768;&#30340;&#22122;&#22768;&#27880;&#20837;&#12289;&#37327;&#21270;&#21644;&#24402;&#19968;&#21270;&#30340;&#37327;&#23376;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
QuantumNAT: Quantum Noise-Aware Training with Noise Injection, Quantization and Normalization. (arXiv:2110.11331v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.11331
&lt;/p&gt;
&lt;p&gt;
QuantumNAT&#26159;&#19968;&#20010;PQC&#29305;&#23450;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#25191;&#34892;&#22122;&#22768;&#24863;&#30693;&#20248;&#21270;&#65292;&#25552;&#39640;&#40065;&#26834;&#24615;&#65292;&#32531;&#35299;&#37327;&#23376;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#26159;&#23454;&#29616;&#36817;&#26399;&#37327;&#23376;&#30828;&#20214;&#20248;&#21183;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#22312;&#36739;&#22823;&#30340;&#37327;&#23376;&#22122;&#22768;&#65288;&#35823;&#24046;&#65289;&#65292;&#22312;&#23454;&#38469;&#30340;&#37327;&#23376;&#35774;&#22791;&#19978;&#65292;PQC&#27169;&#22411;&#30340;&#24615;&#33021;&#20250;&#21463;&#21040;&#20005;&#37325;&#30340;&#38477;&#32423;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;QuantumNAT&#65292;&#19968;&#20010;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#38454;&#27573;&#25191;&#34892;&#22122;&#22768;&#24863;&#30693;&#20248;&#21270;&#30340;PQC&#29305;&#23450;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;&#20854;&#40065;&#26834;&#24615;&#12290;&#36890;&#36807;&#23454;&#39564;&#25105;&#20204;&#21457;&#29616;&#65292;&#37327;&#23376;&#22122;&#22768;&#23545;PQC&#27979;&#37327;&#32467;&#26524;&#30340;&#24433;&#21709;&#26159;&#20174;&#26080;&#22122;&#22768;&#32467;&#26524;&#32463;&#36807;&#19968;&#20010;&#32553;&#25918;&#21644;&#20559;&#31227;&#22240;&#23376;&#24471;&#21040;&#30340;&#32447;&#24615;&#26144;&#23556;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#27979;&#37327;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#29305;&#24449;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameterized Quantum Circuits (PQC) are promising towards quantum advantage on near-term quantum hardware. However, due to the large quantum noises (errors), the performance of PQC models has a severe degradation on real quantum devices. Take Quantum Neural Network (QNN) as an example, the accuracy gap between noise-free simulation and noisy results on IBMQ-Yorktown for MNIST-4 classification is over 60%. Existing noise mitigation methods are general ones without leveraging unique characteristics of PQC; on the other hand, existing PQC work does not consider noise effect. To this end, we present QuantumNAT, a PQC-specific framework to perform noise-aware optimizations in both training and inference stages to improve robustness. We experimentally observe that the effect of quantum noise to PQC measurement outcome is a linear map from noise-free outcome with a scaling and a shift factor. Motivated by that, we propose post-measurement normalization to mitigate the feature distribution di
&lt;/p&gt;</description></item></channel></rss>