<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ACFL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20043;&#21069;&#37319;&#29992;&#20010;&#24615;&#21270;&#30340;&#25968;&#25454;&#19978;&#20256;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#21407;&#26377;&#22266;&#23450;&#26435;&#37325;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14905</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#65306;&#38544;&#31169;&#20445;&#25252;&#19982;&#24930;&#33410;&#28857;&#32531;&#35299;
&lt;/p&gt;
&lt;p&gt;
Adaptive Coded Federated Learning: Privacy Preservation and Straggler Mitigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14905
&lt;/p&gt;
&lt;p&gt;
ACFL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#20043;&#21069;&#37319;&#29992;&#20010;&#24615;&#21270;&#30340;&#25968;&#25454;&#19978;&#20256;&#21040;&#20013;&#22830;&#26381;&#21153;&#22120;&#26469;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#65292;&#20197;&#35299;&#20915;&#21407;&#26377;&#22266;&#23450;&#26435;&#37325;&#29983;&#25104;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#26102;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#23384;&#22312;&#24930;&#33410;&#28857;&#24773;&#20917;&#19979;&#30340;&#32852;&#37030;&#23398;&#20064;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#20013;&#22830;&#26381;&#21153;&#22120;&#32858;&#21512;&#26469;&#33258;&#38750;&#24930;&#33410;&#28857;&#30340;&#26799;&#24230;&#21644;&#26469;&#33258;&#38544;&#31169;&#20445;&#25252;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#30340;&#26799;&#24230;&#65292;&#20197;&#20943;&#36731;&#24930;&#33410;&#28857;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#32858;&#21512;&#36825;&#20123;&#26799;&#24230;&#26102;&#65292;&#22266;&#23450;&#26435;&#37325;&#22312;&#36845;&#20195;&#20013;&#19968;&#30452;&#34987;&#24212;&#29992;&#65292;&#24573;&#30053;&#20102;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#30340;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;&#35757;&#32451;&#27169;&#22411;&#38543;&#30528;&#36845;&#20195;&#30340;&#21160;&#24577;&#24615;&#12290;&#36825;&#19968;&#30095;&#28431;&#21487;&#33021;&#23548;&#33268;&#23398;&#20064;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#33258;&#36866;&#24212;&#32534;&#30721;&#32852;&#37030;&#23398;&#20064;&#65288;ACFL&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;ACFL&#20013;&#65292;&#22312;&#35757;&#32451;&#20043;&#21069;&#65292;&#27599;&#20010;&#35774;&#22791;&#21521;&#20013;&#22830;&#26381;&#21153;&#22120;&#19978;&#20256;&#19968;&#20010;&#24102;&#26377;&#38468;&#21152;&#22122;&#22768;&#30340;&#32534;&#30721;&#26412;&#22320;&#25968;&#25454;&#38598;&#65292;&#20197;&#29983;&#25104;&#31526;&#21512;&#38544;&#31169;&#20445;&#25252;&#35201;&#27714;&#30340;&#20840;&#23616;&#32534;&#30721;&#25968;&#25454;&#38598;&#12290;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14905v1 Announce Type: cross  Abstract: In this article, we address the problem of federated learning in the presence of stragglers. For this problem, a coded federated learning framework has been proposed, where the central server aggregates gradients received from the non-stragglers and gradient computed from a privacy-preservation global coded dataset to mitigate the negative impact of the stragglers. However, when aggregating these gradients, fixed weights are consistently applied across iterations, neglecting the generation process of the global coded dataset and the dynamic nature of the trained model over iterations. This oversight may result in diminished learning performance. To overcome this drawback, we propose a new method named adaptive coded federated learning (ACFL). In ACFL, before the training, each device uploads a coded local dataset with additive noise to the central server to generate a global coded dataset under privacy preservation requirements. During
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03055</link><description>&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#23398;&#20064;&#20197;PAC-Bayes&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03055
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;PAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#32531;&#35299;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#12290;PAC&#36890;&#36807;&#23558;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#26080;&#32541;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28436;&#21592;&#35757;&#32451;&#20043;&#38388;&#30340;&#21160;&#24577;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;PAC&#31639;&#27861;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;Probably Approximately Correct-Bayesian&#65288;PAC-Bayes&#65289;&#20998;&#26512;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#25512;&#26029;&#35780;&#35770;&#23478;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#23545;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#30340;&#34701;&#20837;&#20351;PAC&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#35843;&#25972;&#20854;&#25506;&#32034;&#31574;&#30053;&#65292;&#25351;&#23548;&#28436;&#21592;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#20013;&#30340;&#22266;&#23450;&#25110;&#39044;&#23450;&#30340;&#25506;&#32034;&#26041;&#26696;&#30456;&#27604;&#65292;PAC&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;PAC-Bayes&#20998;&#26512;&#24341;&#23548;&#30340;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26159;&#21521;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26356;&#20855;&#33258;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#31574;&#30053;&#36808;&#20986;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement lear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#39118;&#38505;&#36866;&#24212;&#24615;&#35843;&#25972;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#35748;&#30693;&#39118;&#38505;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2310.05179</link><description>&lt;p&gt;
&#20855;&#26377;&#22312;&#32447;&#39118;&#38505;&#24863;&#30693;&#36866;&#24212;&#24615;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning with Online Risk-awareness Adaption
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#22312;&#32447;&#39118;&#38505;&#36866;&#24212;&#24615;&#35843;&#25972;&#26469;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#21160;&#24577;&#36873;&#25321;&#35748;&#30693;&#39118;&#38505;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#38656;&#35201;&#32771;&#34385;&#27425;&#20248;&#32467;&#26524;&#65292;&#36825;&#21462;&#20915;&#20110;&#20195;&#29702;&#20154;&#23545;&#19981;&#30830;&#23450;&#29615;&#22659;&#30340;&#29087;&#24713;&#31243;&#24230;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;Distributional RL with Online Risk Adaption&#65288;DRL-ORA&#65289;&#65292;&#21487;&#20197;&#32508;&#21512;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#24182;&#21160;&#24577;&#36873;&#25321;&#35748;&#30693;&#39118;&#38505;&#27700;&#24179;&#65292;&#36890;&#36807;&#22312;&#32447;&#35299;&#20915;&#24635;&#21464;&#24046;&#26368;&#23567;&#21270;&#38382;&#39064;&#12290;&#39118;&#38505;&#27700;&#24179;&#36873;&#25321;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;Follow-The-Leader&#31867;&#22411;&#31639;&#27861;&#36827;&#34892;&#32593;&#26684;&#25628;&#32034;&#26469;&#26377;&#25928;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05179v2 Announce Type: replace  Abstract: The use of reinforcement learning (RL) in practical applications requires considering sub-optimal outcomes, which depend on the agent's familiarity with the uncertain environment. Dynamically adjusting the level of epistemic risk over the course of learning can tactically achieve reliable optimal policy in safety-critical environments and tackle the sub-optimality of a static risk level. In this work, we introduce a novel framework, Distributional RL with Online Risk Adaption (DRL-ORA), which can quantify the aleatory and epistemic uncertainties compositely and dynamically select the epistemic risk levels via solving a total variation minimization problem online. The risk level selection can be efficiently achieved through grid search using a Follow-The-Leader type algorithm, and its offline oracle is related to "satisficing measure" (in the decision analysis community) under a special modification of the loss function. We show multi
&lt;/p&gt;</description></item><item><title>NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2401.13330</link><description>&lt;p&gt;
NACHOS: &#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
NACHOS: Neural Architecture Search for Hardware Constrained Early Exit Neural Networks. (arXiv:2401.13330v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13330
&lt;/p&gt;
&lt;p&gt;
NACHOS &#26159;&#19968;&#31181;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#21160;&#21270;&#35774;&#35745;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#24182;&#32771;&#34385;&#39592;&#24178;&#21644;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENNs&#65289;&#20026;&#26631;&#20934;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#37197;&#22791;&#26089;&#26399;&#36864;&#20986;&#20998;&#31867;&#22120;&#65288;EECs&#65289;&#65292;&#22312;&#22788;&#29702;&#30340;&#20013;&#38388;&#28857;&#19978;&#25552;&#20379;&#36275;&#22815;&#30340;&#20998;&#31867;&#32622;&#20449;&#24230;&#26102;&#36827;&#34892;&#39044;&#27979;&#12290;&#36825;&#22312;&#25928;&#26524;&#21644;&#25928;&#29575;&#26041;&#38754;&#24102;&#26469;&#20102;&#35768;&#22810;&#22909;&#22788;&#12290;&#30446;&#21069;&#65292;EENNs&#30340;&#35774;&#35745;&#26159;&#30001;&#19987;&#23478;&#25163;&#21160;&#23436;&#25104;&#30340;&#65292;&#36825;&#26159;&#19968;&#39033;&#22797;&#26434;&#21644;&#32791;&#26102;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#32771;&#34385;&#35768;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#27491;&#30830;&#30340;&#25918;&#32622;&#12289;&#38408;&#20540;&#35774;&#32622;&#21644;EECs&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#27491;&#22312;&#25506;&#32034;&#20351;&#29992;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#33258;&#21160;&#21270;&#35774;&#35745;EENNs&#12290;&#30446;&#21069;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#20010;&#23436;&#25972;&#30340;NAS&#35299;&#20915;&#26041;&#26696;&#29992;&#20110;EENNs&#65292;&#24182;&#19988;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#32508;&#21512;&#35774;&#35745;&#31574;&#30053;&#65292;&#21516;&#26102;&#32771;&#34385;&#39592;&#24178;&#21644;EECs&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#26412;&#30740;&#31350;&#21576;&#29616;&#20102;&#38754;&#21521;&#30828;&#20214;&#21463;&#38480;&#30340;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NACHOS&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early Exit Neural Networks (EENNs) endow astandard Deep Neural Network (DNN) with Early Exit Classifiers (EECs), to provide predictions at intermediate points of the processing when enough confidence in classification is achieved. This leads to many benefits in terms of effectiveness and efficiency. Currently, the design of EENNs is carried out manually by experts, a complex and time-consuming task that requires accounting for many aspects, including the correct placement, the thresholding, and the computational overhead of the EECs. For this reason, the research is exploring the use of Neural Architecture Search (NAS) to automatize the design of EENNs. Currently, few comprehensive NAS solutions for EENNs have been proposed in the literature, and a fully automated, joint design strategy taking into consideration both the backbone and the EECs remains an open problem. To this end, this work presents Neural Architecture Search for Hardware Constrained Early Exit Neural Networks (NACHOS),
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;Thompson&#37319;&#26679;&#19982;&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31574;&#30053;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#19968;&#33324;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00539</link><description>&lt;p&gt;
&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#19979;&#30340;Thompson&#25506;&#32034;&#22312;&#26368;&#20339;&#33218;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Thompson Exploration with Best Challenger Rule in Best Arm Identification. (arXiv:2310.00539v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;Thompson&#37319;&#26679;&#19982;&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31574;&#30053;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#19968;&#33324;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32463;&#20856;&#21333;&#21442;&#25968;&#25351;&#25968;&#27169;&#22411;&#19979;&#65292;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#21069;&#24050;&#26377;&#24456;&#22810;&#31574;&#30053;&#34987;&#25552;&#20986;&#65292;&#20294;&#22823;&#22810;&#25968;&#38656;&#35201;&#22312;&#27599;&#19968;&#36718;&#35299;&#20915;&#19968;&#20010;&#26368;&#20248;&#21270;&#38382;&#39064;&#21644;/&#25110;&#32773;&#38656;&#35201;&#25506;&#32034;&#19968;&#20010;&#33218;&#33267;&#23569;&#19968;&#23450;&#27425;&#25968;&#65292;&#38500;&#38750;&#26159;&#38024;&#23545;&#39640;&#26031;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;Thompson&#37319;&#26679;&#19982;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#8212;&#8212;&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#30456;&#32467;&#21512;&#12290;&#34429;&#28982;Thompson&#37319;&#26679;&#26368;&#21021;&#34987;&#32771;&#34385;&#29992;&#20110;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#23427;&#20063;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#20110;&#22312;BAI&#20013;&#25506;&#32034;&#33218;&#32780;&#19981;&#24378;&#36843;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#20219;&#24847;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#26159;&#28176;&#36817;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#19968;&#33324;&#30340;$K$&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#65288;$K\geq 3$&#65289;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K\geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.07886</link><description>&lt;p&gt;
&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#19982;&#20020;&#30028;&#28857;
&lt;/p&gt;
&lt;p&gt;
Symmetry &amp; Critical Points for Symmetric Tensor Decompositions Problems. (arXiv:2306.07886v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#23545;&#31216;&#32467;&#26500;&#65292;&#23548;&#20986;Puiseux&#32423;&#25968;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#20020;&#30028;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#20020;&#30028;&#20540;&#21644;Hessian&#35889;&#30340;&#31934;&#30830;&#20998;&#26512;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#21508;&#31181;&#20960;&#20309;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#26368;&#21518;&#65292;&#21033;&#29992;&#19968;&#20010;&#29275;&#39039;&#22810;&#38754;&#20307;&#35770;&#35777;&#20102;&#22266;&#23450;&#23545;&#31216;&#24615;&#30340;&#25152;&#26377;&#20020;&#30028;&#28857;&#30340;&#23436;&#20840;&#26522;&#20030;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38598;&#21512;&#30456;&#27604;&#65292;&#30001;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#20020;&#30028;&#28857;&#30340;&#38598;&#21512;&#21487;&#33021;&#20250;&#26174;&#31034;&#20986;&#32452;&#21512;&#30340;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the non-convex optimization problem associated with the decomposition of a real symmetric tensor into a sum of rank one terms. Use is made of the rich symmetry structure to derive Puiseux series representations of families of critical points, and so obtain precise analytic estimates on the critical values and the Hessian spectrum. The sharp results make possible an analytic characterization of various geometric obstructions to local optimization methods, revealing in particular a complex array of saddles and local minima which differ by their symmetry, structure and analytic properties. A desirable phenomenon, occurring for all critical points considered, concerns the index of a point, i.e., the number of negative Hessian eigenvalues, increasing with the value of the objective function. Lastly, a Newton polytope argument is used to give a complete enumeration of all critical points of fixed symmetry, and it is shown that contrarily to the set of global minima which remains 
&lt;/p&gt;</description></item></channel></rss>