<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;DualView&#65292;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#20248;&#36136;&#35780;&#20272;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12118</link><description>&lt;p&gt;
DualView&#65306;&#21452;&#37325;&#35270;&#35282;&#19979;&#30340;&#25968;&#25454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
DualView: Data Attribution from the Dual Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12118
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DualView&#65292;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#20855;&#26377;&#39640;&#25928;&#35745;&#31639;&#21644;&#20248;&#36136;&#35780;&#20272;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DualView&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#26367;&#20195;&#24314;&#27169;&#30340;&#21518;&#26399;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#33391;&#22909;&#30340;&#35780;&#20272;&#32467;&#26524;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#22312;&#19982;&#25991;&#29486;&#30456;&#20851;&#30340;&#36866;&#24403;&#23450;&#37327;&#35780;&#20272;&#31574;&#30053;&#19979;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#65292;&#27604;&#36739;&#20102;&#19982;&#30456;&#20851;&#20027;&#35201;&#26412;&#22320;&#25968;&#25454;&#24402;&#22240;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12118v1 Announce Type: cross  Abstract: Local data attribution (or influence estimation) techniques aim at estimating the impact that individual data points seen during training have on particular predictions of an already trained Machine Learning model during test time. Previous methods either do not perform well consistently across different evaluation criteria from literature, are characterized by a high computational demand, or suffer from both. In this work we present DualView, a novel method for post-hoc data attribution based on surrogate modelling, demonstrating both high computational efficiency, as well as good evaluation results. With a focus on neural networks, we evaluate our proposed technique using suitable quantitative evaluation strategies from the literature against related principal local data attribution methods. We find that DualView requires considerably lower computational resources than other methods, while demonstrating comparable performance to comp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#20013;&#24322;&#36136;&#24615;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;SCAFFLSA&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#20559;&#24046;&#12290;&#22312;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.04114</link><description>&lt;p&gt;
SCAFFLSA&#65306;&#37327;&#21270;&#21644;&#28040;&#38500;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#21644;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#30340;&#24322;&#36136;&#24615;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
SCAFFLSA: Quantifying and Eliminating Heterogeneity Bias in Federated Linear Stochastic Approximation and Temporal Difference Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#20013;&#24322;&#36136;&#24615;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;SCAFFLSA&#20316;&#20026;&#19968;&#31181;&#25913;&#36827;&#26041;&#27861;&#26469;&#28040;&#38500;&#27492;&#20559;&#24046;&#12290;&#22312;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#20013;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#31639;&#27861;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32852;&#37030;&#32447;&#24615;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#65288;FedLSA&#65289;&#36827;&#34892;&#20102;&#38750;&#28176;&#36827;&#20998;&#26512;&#12290;&#25105;&#20204;&#26126;&#30830;&#37327;&#21270;&#20102;&#24322;&#36136;&#20195;&#29702;&#26412;&#22320;&#35757;&#32451;&#24341;&#20837;&#30340;&#20559;&#24046;&#65292;&#24182;&#30740;&#31350;&#20102;&#35813;&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;FedLSA&#30340;&#36890;&#20449;&#22797;&#26434;&#24615;&#19982;&#25152;&#38656;&#31934;&#24230; $\epsilon$ &#21576;&#22810;&#39033;&#24335;&#20851;&#31995;&#65292;&#36825;&#38480;&#21046;&#20102;&#32852;&#37030;&#30340;&#22909;&#22788;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SCAFFLSA&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;FedLSA&#21464;&#20307;&#65292;&#23427;&#20351;&#29992;&#25511;&#21046;&#21464;&#37327;&#26469;&#26657;&#27491;&#26412;&#22320;&#35757;&#32451;&#30340;&#20559;&#24046;&#65292;&#24182;&#22312;&#19981;&#23545;&#32479;&#35745;&#24322;&#36136;&#24615;&#20570;&#20986;&#20219;&#20309;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#12290;&#25105;&#20204;&#23558;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#30340;&#32852;&#37030;&#26102;&#38388;&#24046;&#24322;&#23398;&#20064;&#65292;&#24182;&#20998;&#26512;&#20102;&#30456;&#24212;&#30340;&#22797;&#26434;&#24615;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we perform a non-asymptotic analysis of the federated linear stochastic approximation (FedLSA) algorithm. We explicitly quantify the bias introduced by local training with heterogeneous agents, and investigate the sample complexity of the algorithm. We show that the communication complexity of FedLSA scales polynomially with the desired precision $\epsilon$, which limits the benefits of federation. To overcome this, we propose SCAFFLSA, a novel variant of FedLSA, that uses control variates to correct the bias of local training, and prove its convergence without assumptions on statistical heterogeneity. We apply the proposed methodology to federated temporal difference learning with linear function approximation, and analyze the corresponding complexity improvements.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#26816;&#39564;&#26694;&#26550;&#65292;&#21033;&#29992;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#26469;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#23436;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#24314;&#27169;&#38656;&#27714;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2401.15502</link><description>&lt;p&gt;
&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Bayesian Tests. (arXiv:2401.15502v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#26816;&#39564;&#26694;&#26550;&#65292;&#21033;&#29992;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#26469;&#36827;&#34892;&#25512;&#26029;&#65292;&#24182;&#36991;&#20813;&#20102;&#23545;&#23436;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#30340;&#24314;&#27169;&#38656;&#27714;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20855;&#26377;&#23454;&#36136;&#24615;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21033;&#29992;&#26426;&#23494;&#25968;&#25454;&#36827;&#34892;&#31185;&#23398;&#20551;&#35774;&#26816;&#39564;&#30340;&#39046;&#22495;&#20013;&#65292;&#24046;&#20998;&#38544;&#31169;&#24050;&#32463;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#22522;&#30707;&#12290;&#22312;&#25253;&#21578;&#31185;&#23398;&#21457;&#29616;&#26102;&#65292;&#24191;&#27867;&#37319;&#29992;&#36125;&#21494;&#26031;&#26816;&#39564;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#25928;&#22320;&#36991;&#20813;&#20102;P&#20540;&#30340;&#20027;&#35201;&#25209;&#35780;&#65292;&#21363;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#21644;&#26080;&#27861;&#37327;&#21270;&#23545;&#31454;&#20105;&#20551;&#35774;&#30340;&#25903;&#25345;&#35777;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#20551;&#35774;&#26816;&#39564;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#22522;&#20110;&#35268;&#33539;&#21270;&#30340;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#22522;&#30784;&#19978;&#33258;&#28982;&#20135;&#29983;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#25512;&#26029;&#32467;&#26524;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19987;&#27880;&#20110;&#22522;&#20110;&#24191;&#27867;&#20351;&#29992;&#30340;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#24046;&#20998;&#38544;&#31169;&#36125;&#21494;&#26031;&#22240;&#23376;&#65292;&#25105;&#20204;&#36991;&#20813;&#20102;&#23545;&#23436;&#25972;&#25968;&#25454;&#29983;&#25104;&#26426;&#21046;&#24314;&#27169;&#30340;&#38656;&#27714;&#65292;&#24182;&#30830;&#20445;&#20102;&#23454;&#36136;&#24615;&#30340;&#35745;&#31639;&#20248;&#21183;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#32452;&#20805;&#20998;&#26465;&#20214;&#65292;&#20197;&#22312;&#25152;&#25552;&#26694;&#26550;&#19979;&#30830;&#31435;&#36125;&#21494;&#26031;&#22240;&#23376;&#19968;&#33268;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy has emerged as an significant cornerstone in the realm of scientific hypothesis testing utilizing confidential data. In reporting scientific discoveries, Bayesian tests are widely adopted since they effectively circumnavigate the key criticisms of P-values, namely, lack of interpretability and inability to quantify evidence in support of the competing hypotheses. We present a novel differentially private Bayesian hypotheses testing framework that arise naturally under a principled data generative mechanism, inherently maintaining the interpretability of the resulting inferences. Furthermore, by focusing on differentially private Bayes factors based on widely used test statistics, we circumvent the need to model the complete data generative mechanism and ensure substantial computational benefits. We also provide a set of sufficient conditions to establish results on Bayes factor consistency under the proposed framework. The utility of the devised technology is showc
&lt;/p&gt;</description></item></channel></rss>