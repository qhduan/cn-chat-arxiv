<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02115</link><description>&lt;p&gt;
GINopic&#65306;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GINopic: Topic Modeling with Graph Isomorphism Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02115
&lt;/p&gt;
&lt;p&gt;
GINopic&#26159;&#19968;&#31181;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#21516;&#26500;&#32593;&#32476;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#26356;&#22909;&#30340;&#26377;&#25928;&#24615;&#21644;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#39064;&#24314;&#27169;&#26159;&#20998;&#26512;&#21644;&#25506;&#32034;&#22823;&#22411;&#25991;&#26723;&#38598;&#21512;&#30340;&#24191;&#27867;&#20351;&#29992;&#26041;&#27861;&#12290; &#26368;&#36817;&#30340;&#30740;&#31350;&#24037;&#20316;&#23558;&#39044;&#35757;&#32451;&#30340;&#19978;&#19979;&#25991;&#21270;&#35821;&#35328;&#27169;&#22411;&#65292;&#22914;BERT&#23884;&#20837;&#65292;&#32435;&#20837;&#20027;&#39064;&#24314;&#27169;&#20013;&#12290; &#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#24573;&#30053;&#20102;&#21333;&#35789;&#20043;&#38388;&#30456;&#20114;&#20381;&#36182;&#20256;&#36798;&#30340;&#22266;&#26377;&#20449;&#24687;&#20215;&#20540;&#12290; &#26412;&#30740;&#31350;&#20171;&#32461;&#20102;GINopic&#65292;&#19968;&#31181;&#22522;&#20110;&#22270;&#21516;&#26500;&#32593;&#32476;&#30340;&#20027;&#39064;&#24314;&#27169;&#26694;&#26550;&#65292;&#20197;&#25429;&#25417;&#21333;&#35789;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290; &#36890;&#36807;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20869;&#22312;&#30340;&#65288;&#23450;&#37327;&#21644;&#23450;&#24615;&#65289;&#21644;&#22806;&#37096;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19982;&#29616;&#26377;&#20027;&#39064;&#27169;&#22411;&#30456;&#27604;&#65292;GINopic&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#25512;&#36827;&#20027;&#39064;&#24314;&#27169;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02115v1 Announce Type: new  Abstract: Topic modeling is a widely used approach for analyzing and exploring large document collections. Recent research efforts have incorporated pre-trained contextualized language models, such as BERT embeddings, into topic modeling. However, they often neglect the intrinsic informational value conveyed by mutual dependencies between words. In this study, we introduce GINopic, a topic modeling framework based on graph isomorphism networks to capture the correlation between words. By conducting intrinsic (quantitative as well as qualitative) and extrinsic evaluations on diverse benchmark datasets, we demonstrate the effectiveness of GINopic compared to existing topic models and highlight its potential for advancing topic modeling.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;</title><link>https://arxiv.org/abs/2403.17983</link><description>&lt;p&gt;
LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#27700;&#21360;&#25216;&#26415;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Watermarking LLM-Generated Code Robust?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#20316;&#21697;&#34920;&#26126;&#27700;&#21360;&#25216;&#26415;&#23545;&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#30340;&#36716;&#25442;&#24456;&#23481;&#26131;&#31227;&#38500;&#20195;&#30721;&#19978;&#30340;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17983v1 Announce Type: cross  Abstract: We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16970</link><description>&lt;p&gt;
&#32852;&#21512;&#33016;&#37096;X&#20809;&#35786;&#26029;&#21644;&#20020;&#24202;&#35270;&#35273;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20020;&#24202;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#39046;&#22495;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#22312;&#25918;&#23556;&#23398;&#31579;&#26597;&#36807;&#31243;&#20013;&#20020;&#24202;&#21307;&#29983;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#22270;&#20026;&#25552;&#20379;&#37325;&#35201;&#27934;&#23519;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#36164;&#20135;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#36825;&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#65292;&#21033;&#29992;&#20102;DenseNet201&#20027;&#24178;&#21644;&#22522;&#20110;&#27531;&#24046;&#21644;&#33192;&#32960;&#28608;&#21169;&#22359;&#30340;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#29992;&#20110;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#36827;&#34892;&#30142;&#30149;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
&lt;/p&gt;</description></item><item><title>GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16883</link><description>&lt;p&gt;
&#24102;&#25193;&#25955;&#26725;&#30340;&#31163;&#25955;&#28508;&#22312;&#22270;&#29983;&#25104;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Discrete Latent Graph Generative Modeling with Diffusion Bridges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16883
&lt;/p&gt;
&lt;p&gt;
GLAD&#26159;&#19968;&#20010;&#22312;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#36866;&#24212;&#25193;&#25955;&#26725;&#32467;&#26500;&#23398;&#20064;&#20854;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#65292;&#36991;&#20813;&#20102;&#20381;&#36182;&#20110;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#30340;&#20998;&#35299;&#65292;&#22312;&#22270;&#29983;&#25104;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#22270;&#29983;&#25104;&#27169;&#22411;&#30456;&#27604;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#19978;&#25805;&#20316;&#30340;&#27169;&#22411;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#65292;&#36804;&#20170;&#34920;&#29616;&#20986;&#30340;&#24615;&#33021;&#20047;&#21892;&#21487;&#38472;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GLAD&#65292;&#19968;&#20010;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#12290;&#19982;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#28508;&#22312;&#31354;&#38388;&#22270;&#29983;&#25104;&#27169;&#22411;&#19981;&#21516;&#65292;GLAD&#22312;&#20445;&#30041;&#22270;&#32467;&#26500;&#30340;&#31163;&#25955;&#24615;&#36136;&#26041;&#38754;&#36816;&#34892;&#65292;&#26080;&#38656;&#36827;&#34892;&#35832;&#22914;&#28508;&#22312;&#31354;&#38388;&#36830;&#32493;&#24615;&#31561;&#19981;&#33258;&#28982;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25193;&#25955;&#26725;&#35843;&#25972;&#21040;&#20854;&#32467;&#26500;&#65292;&#26469;&#23398;&#20064;&#25105;&#20204;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20808;&#39564;&#12290;&#36890;&#36807;&#22312;&#36866;&#24403;&#26500;&#24314;&#30340;&#28508;&#22312;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#25105;&#20204;&#36991;&#20813;&#20381;&#36182;&#20110;&#24120;&#29992;&#20110;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#25805;&#20316;&#30340;&#27169;&#22411;&#20013;&#30340;&#20998;&#35299;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#22270;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#26126;&#26174;&#23637;&#31034;&#20102;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#30340;&#20248;&#36234;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#29983;&#25104;&#24615;&#33021;&#65292;&#20351;GLA
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16883v1 Announce Type: new  Abstract: Learning graph generative models over latent spaces has received less attention compared to models that operate on the original data space and has so far demonstrated lacklustre performance. We present GLAD a latent space graph generative model. Unlike most previous latent space graph generative models, GLAD operates on a discrete latent space that preserves to a significant extent the discrete nature of the graph structures making no unnatural assumptions such as latent space continuity. We learn the prior of our discrete latent space by adapting diffusion bridges to its structure. By operating over an appropriately constructed latent space we avoid relying on decompositions that are often used in models that operate in the original data space. We present experiments on a series of graph benchmark datasets which clearly show the superiority of the discrete latent space and obtain state of the art graph generative performance, making GLA
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15245</link><description>&lt;p&gt;
&#35270;&#39057;&#30340;&#22686;&#24378;&#25512;&#29702;&#23545;&#35937;&#20013;&#24515;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reasoning-Enhanced Object-Centric Learning for Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15245
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#29702;&#27169;&#22359;STATM&#65292;&#21033;&#29992;&#35760;&#24518;&#32531;&#20914;&#21306;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#23398;&#20064;&#26088;&#22312;&#23558;&#22797;&#26434;&#30340;&#35270;&#35273;&#22330;&#26223;&#20998;&#35299;&#20026;&#26356;&#26131;&#22788;&#29702;&#30340;&#29289;&#20307;&#34920;&#31034;&#65292;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#29289;&#29702;&#19990;&#30028;&#30340;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#35270;&#39057;&#27169;&#22411;&#23637;&#29616;&#20986;&#22312;&#20998;&#21106;&#21644;&#36319;&#36394;&#29289;&#20307;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#65292;&#20294;&#24573;&#35270;&#20102;&#26377;&#25928;&#25512;&#29702;&#27169;&#22359;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;&#20855;&#26377;&#35760;&#24518;&#32531;&#20914;&#21306;&#30340;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#65288;STATM&#65289;&#30340;&#26032;&#22411;&#25512;&#29702;&#27169;&#22359;&#12290;&#35760;&#24518;&#32531;&#20914;&#21306;&#20027;&#35201;&#29992;&#20110;&#23384;&#20648;&#26469;&#33258;&#19978;&#28216;&#27169;&#22359;&#30340;&#27133;&#20301;&#20449;&#24687;&#65292;&#22522;&#20110;&#27133;&#20301;&#30340;&#26102;&#31354;&#21464;&#25442;&#22120;&#36890;&#36807;&#27133;&#20301;&#20026;&#22522;&#30784;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15245v1 Announce Type: cross  Abstract: Object-centric learning aims to break down complex visual scenes into more manageable object representations, enhancing the understanding and reasoning abilities of machine learning systems toward the physical world. Recently, slot-based video models have demonstrated remarkable proficiency in segmenting and tracking objects, but they overlook the importance of the effective reasoning module. In the real world, reasoning and predictive abilities play a crucial role in human perception and object tracking; in particular, these abilities are closely related to human intuitive physics. Inspired by this, we designed a novel reasoning module called the Slot-based Time-Space Transformer with Memory buffer (STATM) to enhance the model's perception ability in complex scenes. The memory buffer primarily serves as storage for slot information from upstream modules, the Slot-based Time-Space Transformer makes predictions through slot-based spatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;</title><link>https://arxiv.org/abs/2403.14404</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14404
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#21270;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#26694;&#26550;&#65292;&#21487;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#26045;&#21152;&#32422;&#26463;&#65292;&#20197;&#25913;&#21892;&#26679;&#26412;&#19982;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#24182;&#25552;&#20379;&#33258;&#28982;&#30340;&#27491;&#21017;&#21270;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22914;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#27491;&#24555;&#36895;&#25552;&#21319;&#20854;&#36924;&#36817;&#39640;&#24230;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#20063;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#36816;&#29992;&#20110;&#31185;&#23398;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#39044;&#26399;&#20174;&#38544;&#21547;&#25968;&#25454;&#20998;&#24067;&#20013;&#21462;&#26679;&#30340;&#26679;&#26412;&#23558;&#36981;&#23432;&#29305;&#23450;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#26399;&#38388;&#23545;&#29983;&#25104;&#26679;&#26412;&#30340;&#22522;&#30784;&#32422;&#26463;&#36827;&#34892;&#20449;&#24687;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#29983;&#25104;&#26679;&#26412;&#19982;&#26045;&#21152;&#32422;&#26463;&#30340;&#23545;&#40784;&#31243;&#24230;&#65292;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#32780;&#19981;&#24433;&#21709;&#25512;&#29702;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21152;&#20837;&#36825;&#20123;&#32422;&#26463;&#25552;&#20379;&#20102;&#33258;&#28982;&#30340;&#38450;&#27490;&#36807;&#25311;&#21512;&#30340;&#27491;&#21017;&#21270;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26131;&#20110;&#23454;&#29616;&#65292;&#36866;&#29992;&#24615;&#24191;&#27867;&#65292;&#21487;&#29992;&#20110;&#26045;&#21152;&#31561;&#24335;&#21644;&#19981;&#31561;&#24335;&#32422;&#26463;&#20197;&#21450;&#36741;&#21161;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14404v1 Announce Type: new  Abstract: Generative models such as denoising diffusion models are quickly advancing their ability to approximate highly complex data distributions. They are also increasingly leveraged in scientific machine learning, where samples from the implied data distribution are expected to adhere to specific governing equations. We present a framework to inform denoising diffusion models on underlying constraints on such generated samples during model training. Our approach improves the alignment of the generated samples with the imposed constraints and significantly outperforms existing methods without affecting inference speed. Additionally, our findings suggest that incorporating such constraints during training provides a natural regularization against overfitting. Our framework is easy to implement and versatile in its applicability for imposing equality and inequality constraints as well as auxiliary optimization objectives.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22788;&#29702;&#26465;&#20214;&#35774;&#32622;&#22312;EHR&#30340;&#20540;&#21644;&#27979;&#37327;&#19978;&#65292;&#20197;&#25972;&#21512;&#20020;&#24202;&#25104;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.13319</link><description>&lt;p&gt;
HyperFusion&#65306;&#19968;&#31181;&#29992;&#20110;&#39044;&#27979;&#24314;&#27169;&#30340;&#22810;&#27169;&#24577;&#25972;&#21512;&#34920;&#26684;&#21644;&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#30340;&#36229;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
HyperFusion: A Hypernetwork Approach to Multimodal Integration of Tabular and Medical Imaging Data for Predictive Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13319
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#22270;&#20687;&#22788;&#29702;&#26465;&#20214;&#35774;&#32622;&#22312;EHR&#30340;&#20540;&#21644;&#27979;&#37327;&#19978;&#65292;&#20197;&#25972;&#21512;&#20020;&#24202;&#25104;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#65292;&#26088;&#22312;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#20013;&#30340;&#20114;&#34917;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ARXIV: 2403.13319v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#25972;&#21512;&#21508;&#31181;&#20020;&#24202;&#27169;&#24335;&#65292;&#22914;&#21307;&#23398;&#25104;&#20687;&#21644;&#24739;&#32773;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#33719;&#24471;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#26159;&#29616;&#20195;&#21307;&#30103;&#20445;&#20581;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#22810;&#28304;&#25968;&#25454;&#30340;&#32508;&#21512;&#20998;&#26512;&#21487;&#20197;&#20840;&#38754;&#20102;&#35299;&#24739;&#32773;&#30340;&#29366;&#20917;&#65292;&#24182;&#21487;&#20197;&#22686;&#24378;&#35786;&#26029;&#21644;&#27835;&#30103;&#20915;&#31574;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21307;&#23398;&#39046;&#22495;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#19968;&#30452;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26377;&#25928;&#22320;&#23558;&#21307;&#23398;&#25104;&#20687;&#19982;&#20197;&#25968;&#23383;&#34920;&#26684;&#25968;&#25454;&#34920;&#31034;&#30340;&#20020;&#24202;&#12289;&#20154;&#21475;&#32479;&#35745;&#21644;&#36951;&#20256;&#20449;&#24687;&#36827;&#34892;&#34701;&#21512;&#30340;&#22797;&#26434;&#21162;&#21147;&#20173;&#28982;&#26159;&#19968;&#20010;&#39640;&#24230;&#27963;&#36291;&#30340;&#25345;&#32493;&#30740;&#31350;&#36861;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13319v1 Announce Type: cross  Abstract: The integration of diverse clinical modalities such as medical imaging and the tabular data obtained by the patients' Electronic Health Records (EHRs) is a crucial aspect of modern healthcare. The integrative analysis of multiple sources can provide a comprehensive understanding of a patient's condition and can enhance diagnoses and treatment decisions. Deep Neural Networks (DNNs) consistently showcase outstanding performance in a wide range of multimodal tasks in the medical domain. However, the complex endeavor of effectively merging medical imaging with clinical, demographic and genetic information represented as numerical tabular data remains a highly active and ongoing research pursuit.   We present a novel framework based on hypernetworks to fuse clinical imaging and tabular data by conditioning the image processing on the EHR's values and measurements. This approach aims to leverage the complementary information present in these
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.12326</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#23398;&#20064;&#25552;&#31034;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Removing Undesirable Concepts in Text-to-Image Generative Models with Learnable Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12326
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#25552;&#31034;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#26524;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#19978;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20869;&#23481;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26410;&#32463;&#31579;&#36873;&#30340;&#20114;&#32852;&#32593;&#25968;&#25454;&#19978;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#23398;&#20064;&#21644;&#38543;&#21518;&#20256;&#25773;&#19981;&#33391;&#27010;&#24565;&#65288;&#22914;&#21463;&#29256;&#26435;&#20445;&#25252;&#25110;&#19981;&#36947;&#24503;&#20869;&#23481;&#65289;&#30340;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#32467;&#21512;&#21040;&#20132;&#21449;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#65292;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#21435;&#38500;&#19981;&#33391;&#27010;&#24565;&#12290;&#36825;&#21487;&#23398;&#20064;&#25552;&#31034;&#20805;&#24403;&#38468;&#21152;&#20869;&#23384;&#65292;&#23558;&#19981;&#33391;&#27010;&#24565;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#20854;&#20013;&#65292;&#24182;&#20943;&#23569;&#36825;&#20123;&#27010;&#24565;&#23545;&#27169;&#22411;&#21442;&#25968;&#21644;&#30456;&#24212;&#25991;&#26412;&#36755;&#20837;&#30340;&#20381;&#36182;&#12290;&#30001;&#20110;&#36825;&#31181;&#30693;&#35782;&#36716;&#31227;&#21040;&#25552;&#31034;&#20013;&#65292;&#28040;&#38500;&#36825;&#20123;&#19981;&#33391;&#27010;&#24565;&#26356;&#21152;&#31283;&#23450;&#65292;&#24182;&#23545;&#20854;&#20182;&#27010;&#24565;&#24433;&#21709;&#26368;&#23567;&#12290;&#25105;&#20204;&#22312;&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12326v1 Announce Type: new  Abstract: Generative models have demonstrated remarkable potential in generating visually impressive content from textual descriptions. However, training these models on unfiltered internet data poses the risk of learning and subsequently propagating undesirable concepts, such as copyrighted or unethical content. In this paper, we propose a novel method to remove undesirable concepts from text-to-image generative models by incorporating a learnable prompt into the cross-attention module. This learnable prompt acts as additional memory to transfer the knowledge of undesirable concepts into it and reduce the dependency of these concepts on the model parameters and corresponding textual inputs. Because of this knowledge transfer into the prompt, erasing these undesirable concepts is more stable and has minimal negative impact on other concepts. We demonstrate the effectiveness of our method on the Stable Diffusion model, showcasing its superiority ov
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11624</link><description>&lt;p&gt;
&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual-Channel Multiplex Graph Neural Networks for Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11624
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#30340;&#26032;&#22411;&#25512;&#33616;&#26694;&#26550;&#65292;&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#29616;&#26377;&#25512;&#33616;&#26041;&#27861;&#20013;&#23384;&#22312;&#30340;&#22810;&#36890;&#36335;&#20851;&#31995;&#34892;&#20026;&#27169;&#24335;&#24314;&#27169;&#21644;&#23545;&#30446;&#26631;&#20851;&#31995;&#24433;&#21709;&#24573;&#30053;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#25512;&#33616;&#31995;&#32479;&#22312;&#20934;&#30830;&#25429;&#25417;&#21453;&#26144;&#20010;&#20154;&#20559;&#22909;&#30340;&#29992;&#25143;&#21644;&#39033;&#30446;&#23646;&#24615;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#19968;&#20123;&#29616;&#26377;&#30340;&#25512;&#33616;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#23558;&#37325;&#28857;&#36716;&#21521;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25512;&#33616;&#22330;&#26223;&#20013;&#23545;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#30340;&#21508;&#31181;&#31867;&#22411;&#20132;&#20114;&#20851;&#31995;&#36827;&#34892;&#24314;&#27169;&#65292;&#20363;&#22914;&#22312;&#32447;&#36141;&#29289;&#24179;&#21488;&#19978;&#30340;&#28857;&#20987;&#12289;&#26631;&#35760;&#25910;&#34255;&#21644;&#36141;&#20080;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#28982;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#30340;&#32570;&#28857;&#65306;(1) &#19981;&#36275;&#30340;&#24314;&#27169;&#21644;&#21033;&#29992;&#29992;&#25143;&#21644;&#39033;&#30446;&#20043;&#38388;&#22810;&#36890;&#36335;&#20851;&#31995;&#24418;&#25104;&#30340;&#21508;&#31181;&#34892;&#20026;&#27169;&#24335;&#23545;&#34920;&#31034;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;(2) &#24573;&#30053;&#20102;&#34892;&#20026;&#27169;&#24335;&#20013;&#19981;&#21516;&#20851;&#31995;&#23545;&#25512;&#33616;&#31995;&#32479;&#22330;&#26223;&#20013;&#30446;&#26631;&#20851;&#31995;&#30340;&#24433;&#21709;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25512;&#33616;&#26694;&#26550;&#65292;&#21363;&#21452;&#36890;&#36947;&#22810;&#37325;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DCMGNN&#65289;&#65292;&#35813;&#26694;&#26550;&#35299;&#20915;&#20102;&#19978;&#36848;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11624v1 Announce Type: cross  Abstract: Efficient recommender systems play a crucial role in accurately capturing user and item attributes that mirror individual preferences. Some existing recommendation techniques have started to shift their focus towards modeling various types of interaction relations between users and items in real-world recommendation scenarios, such as clicks, marking favorites, and purchases on online shopping platforms. Nevertheless, these approaches still grapple with two significant shortcomings: (1) Insufficient modeling and exploitation of the impact of various behavior patterns formed by multiplex relations between users and items on representation learning, and (2) ignoring the effect of different relations in the behavior patterns on the target relation in recommender system scenarios. In this study, we introduce a novel recommendation framework, Dual-Channel Multiplex Graph Neural Network (DCMGNN), which addresses the aforementioned challenges
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#23384;&#20648;&#22810;&#22788;&#29702;&#22120;&#19978;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#36817;&#20284;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#20013;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#30340;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.10332</link><description>&lt;p&gt;
GreedyML&#65306;&#19968;&#31181;&#29992;&#20110;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GreedyML: A Parallel Algorithm for Maximizing Submodular Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#23384;&#20648;&#22810;&#22788;&#29702;&#22120;&#19978;&#26368;&#22823;&#21270;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#36817;&#20284;&#31639;&#27861;&#65292;&#20197;&#35299;&#20915;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#20013;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#30340;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#20998;&#24067;&#24335;&#23384;&#20648;&#22810;&#22788;&#29702;&#22120;&#19978;&#26368;&#22823;&#21270;&#21333;&#35843;&#23376;&#27169;&#20989;&#25968;&#30340;&#24182;&#34892;&#36817;&#20284;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#21463;&#21040;&#22312;&#28023;&#37327;&#25968;&#25454;&#38598;&#19978;&#35299;&#20915;&#23376;&#27169;&#20248;&#21270;&#38382;&#39064;&#30340;&#38656;&#27714;&#30340;&#21551;&#21457;&#65292;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#39046;&#22495;&#65292;&#22914;&#25968;&#25454;&#25688;&#35201;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#22270;&#31232;&#30095;&#21270;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;Barbosa&#12289;Ene&#12289;Nguyen&#21644;Ward&#65288;2015&#65289;&#25552;&#20986;&#30340;&#38543;&#26426;&#20998;&#24067;&#24335;RandGreedI&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23558;&#25968;&#25454;&#38543;&#26426;&#20998;&#21306;&#21040;&#25152;&#26377;&#22788;&#29702;&#22120;&#20013;&#65292;&#28982;&#21518;&#20351;&#29992;&#21333;&#20010;&#32047;&#31215;&#27493;&#39588;&#35745;&#31639;&#20998;&#24067;&#24335;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#25152;&#26377;&#22788;&#29702;&#22120;&#23558;&#23427;&#20204;&#30340;&#37096;&#20998;&#35299;&#20915;&#26041;&#26696;&#21457;&#36865;&#32473;&#19968;&#20010;&#22788;&#29702;&#22120;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22823;&#38382;&#39064;&#65292;&#32047;&#31215;&#27493;&#39588;&#21487;&#33021;&#36229;&#36807;&#22788;&#29702;&#22120;&#19978;&#21487;&#29992;&#30340;&#20869;&#23384;&#65292;&#24182;&#19988;&#25191;&#34892;&#32047;&#31215;&#30340;&#22788;&#29702;&#22120;&#21487;&#33021;&#25104;&#20026;&#35745;&#31639;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10332v1 Announce Type: cross  Abstract: We describe a parallel approximation algorithm for maximizing monotone submodular functions subject to hereditary constraints on distributed memory multiprocessors. Our work is motivated by the need to solve submodular optimization problems on massive data sets, for practical applications in areas such as data summarization, machine learning, and graph sparsification. Our work builds on the randomized distributed RandGreedI algorithm, proposed by Barbosa, Ene, Nguyen, and Ward (2015). This algorithm computes a distributed solution by randomly partitioning the data among all the processors and then employing a single accumulation step in which all processors send their partial solutions to one processor. However, for large problems, the accumulation step could exceed the memory available on a processor, and the processor which performs the accumulation could become a computational bottleneck.   Here, we propose a generalization of the R
&lt;/p&gt;</description></item><item><title>Vlearn&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;Vlearn&#65292;&#23427;&#36890;&#36807;&#20165;&#21033;&#29992;&#29366;&#24577;&#20540;&#20989;&#25968;&#20316;&#20026;&#35780;&#35770;&#23478;&#65292;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.04453</link><description>&lt;p&gt;
Vlearn&#65306;&#20351;&#29992;&#39640;&#25928;&#29366;&#24577;&#20540;&#20989;&#25968;&#20272;&#35745;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Vlearn: Off-Policy Learning with Efficient State-Value Function Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04453
&lt;/p&gt;
&lt;p&gt;
Vlearn&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;Vlearn&#65292;&#23427;&#36890;&#36807;&#20165;&#21033;&#29992;&#29366;&#24577;&#20540;&#20989;&#25968;&#20316;&#20026;&#35780;&#35770;&#23478;&#65292;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#30340;&#38656;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#30340;&#31163;&#31574;&#30053;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#38656;&#35201;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#34920;&#31034;&#65292;&#36825;&#22312;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#20013;&#21464;&#24471;&#26840;&#25163;&#12290;&#36825;&#20123;&#31639;&#27861;&#32463;&#24120;&#36935;&#21040;&#25361;&#25112;&#65292;&#21363;&#23427;&#20204;&#22312;&#22788;&#29702;&#32500;&#24230;&#28798;&#38590;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#22312;&#36825;&#26679;&#30340;&#31354;&#38388;&#20013;&#32500;&#25252;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#21464;&#24471;&#25968;&#25454;&#25928;&#29575;&#20302;&#19979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Vlearn&#30340;&#26032;&#22411;&#31163;&#31574;&#30053;&#20449;&#20219;&#21306;&#22495;&#20248;&#21270;&#26041;&#27861;&#65292;&#23427;&#28040;&#38500;&#20102;&#23545;&#26126;&#30830;&#29366;&#24577;-&#21160;&#20316;-&#20540;&#20989;&#25968;&#30340;&#35201;&#27714;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#29366;&#24577;&#20540;&#20989;&#25968;&#20316;&#20026;&#35780;&#35770;&#23478;&#65292;&#20174;&#32780;&#20811;&#26381;&#29616;&#26377;&#26041;&#27861;&#30340;&#20960;&#20010;&#23616;&#38480;&#24615;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;Vlearn&#35299;&#20915;&#20102;&#39640;&#32500;&#21160;&#20316;&#31354;&#38388;&#25152;&#24102;&#26469;&#30340;&#35745;&#31639;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;Vlearn&#24341;&#20837;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#19982;&#32431;&#29366;&#24577;&#20540;&#20989;&#25968;&#23398;&#20064;&#30456;&#20851;&#30340;&#31163;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04453v1 Announce Type: new  Abstract: Existing off-policy reinforcement learning algorithms typically necessitate an explicit state-action-value function representation, which becomes problematic in high-dimensional action spaces. These algorithms often encounter challenges where they struggle with the curse of dimensionality, as maintaining a state-action-value function in such spaces becomes data-inefficient. In this work, we propose a novel off-policy trust region optimization approach, called Vlearn, that eliminates the requirement for an explicit state-action-value function. Instead, we demonstrate how to efficiently leverage just a state-value function as the critic, thus overcoming several limitations of existing methods. By doing so, Vlearn addresses the computational challenges posed by high-dimensional action spaces. Furthermore, Vlearn introduces an efficient approach to address the challenges associated with pure state-value function learning in the off-policy se
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.02957</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#28176;&#36817;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Asymptotic Mean Square Error Optimality of Diffusion Probabilistic Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#19968;&#20010;&#29305;&#23450;&#30340;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65292;&#31361;&#20986;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#65292;&#21516;&#26102;&#20855;&#26377;&#24378;&#22823;&#29983;&#25104;&#22120;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DPMs&#65289;&#22312;&#21435;&#22122;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#28508;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#24456;&#26377;&#29992;&#65292;&#20294;&#23427;&#20204;&#30340;&#29702;&#35770;&#29702;&#35299;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#36317;&#12290;&#26412;&#25991;&#36890;&#36807;&#20005;&#26684;&#35777;&#26126;&#29305;&#23450;DPM&#21435;&#22122;&#31574;&#30053;&#22312;&#22823;&#37327;&#25193;&#25955;&#27493;&#25968;&#19979;&#25910;&#25947;&#21040;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#26368;&#20248;&#26465;&#20214;&#22343;&#20540;&#20272;&#35745;&#22120;&#65288;CME&#65289;&#65292;&#20026;&#35813;&#39046;&#22495;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35265;&#35299;&#12290;&#30740;&#31350;&#30340;&#22522;&#20110;DPM&#30340;&#21435;&#22122;&#22120;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19982;DPMs&#20849;&#20139;&#65292;&#20294;&#22312;&#35757;&#32451;&#21518;&#30340;&#36870;&#25512;&#29702;&#36807;&#31243;&#20013;&#20165;&#20256;&#36882;&#26465;&#20214;&#22343;&#20540;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;DPM&#30001;&#28176;&#36817;&#26368;&#20248;&#30340;&#21435;&#22122;&#22120;&#32452;&#25104;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#21516;&#26102;&#36890;&#36807;&#22312;&#36870;&#36807;&#31243;&#20013;&#20999;&#25442;&#37325;&#26032;&#37319;&#26679;&#30340;&#26041;&#24335;&#32487;&#25215;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#29983;&#25104;&#22120;&#12290;&#36890;&#36807;&#25968;&#20540;&#32467;&#26524;&#39564;&#35777;&#20102;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02957v1 Announce Type: new  Abstract: Diffusion probabilistic models (DPMs) have recently shown great potential for denoising tasks. Despite their practical utility, there is a notable gap in their theoretical understanding. This paper contributes novel theoretical insights by rigorously proving the asymptotic convergence of a specific DPM denoising strategy to the mean square error (MSE)-optimal conditional mean estimator (CME) over a large number of diffusion steps. The studied DPM-based denoiser shares the training procedure of DPMs but distinguishes itself by forwarding only the conditional mean during the reverse inference process after training. We highlight the unique perspective that DPMs are composed of an asymptotically optimal denoiser while simultaneously inheriting a powerful generator by switching re-sampling in the reverse process on and off. The theoretical findings are validated by numerical results.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01643</link><description>&lt;p&gt;
&#24744;&#38656;&#35201;&#26356;&#22909;&#22320;&#20851;&#27880;&#20184;&#36153;
&lt;/p&gt;
&lt;p&gt;
You Need to Pay Better Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01643
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#20123;&#26426;&#21046;&#22312;&#25928;&#29575;&#21644;&#23398;&#20064;&#33021;&#21147;&#26041;&#38754;&#32988;&#36807;&#26631;&#20934;&#30340;&#22810;&#22836;&#27880;&#24847;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;Transformer&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#26356;&#24191;&#27867;&#30340;&#37096;&#32626;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;&#20248;&#21270;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#20284;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#23569;&#20102;&#22235;&#20998;&#20043;&#19977;&#65292;&#27599;&#20010;&#22836;&#37096;&#23569;&#20102;&#19968;&#20010;&#30697;&#38453;&#20056;&#27861;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#39640;&#25928;&#27880;&#24847;&#21147;&#65292;&#20854;&#24615;&#33021;&#19982;&#26631;&#20934;&#27880;&#24847;&#21147;&#30456;&#24403;&#65292;&#20294;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;&#19968;&#21322;&#65292;&#27599;&#20010;&#22836;&#37096;&#20943;&#23569;&#20102;&#20004;&#20010;&#30697;&#38453;&#20056;&#27861;&#65292;&#24182;&#19988;&#27604;&#26631;&#20934;&#27880;&#24847;&#21147;&#24555;&#20004;&#20493;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#32423;&#27880;&#24847;&#21147;&#65292;&#22312;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26126;&#26174;&#36229;&#36234;&#20102;&#26631;&#20934;&#27880;&#24847;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#23569;&#30340;&#21442;&#25968;&#21644;&#30697;&#38453;&#20056;&#27861;&#12290;&#38500;&#20102;&#25552;&#20379;&#20005;&#26684;&#30340;&#25968;&#23398;&#27604;&#36739;&#65292;&#25105;&#20204;&#22312;MN&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01643v1 Announce Type: cross  Abstract: We introduce three new attention mechanisms that outperform standard multi-head attention in terms of efficiency and learning capabilities, thereby improving the performance and broader deployability of Transformer models. Our first contribution is Optimised Attention, which performs similarly to standard attention, but has 3/4 as many parameters and one matrix multiplication fewer per head. Next, we introduce Efficient Attention, which performs on par with standard attention with only 1/2 as many parameters as many parameters and two matrix multiplications fewer per head and is up to twice as fast as standard attention. Lastly, we introduce Super Attention, which surpasses standard attention by a significant margin in both vision and natural language processing tasks while having fewer parameters and matrix multiplications. In addition to providing rigorous mathematical comparisons, we evaluate the presented attention mechanisms on MN
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#34892;&#38598;&#30340;&#26354;&#29575;&#65292;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.12868</link><description>&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21487;&#34892;&#38598;&#30340;&#26354;&#29575;&#65292;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#23454;&#29616;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;
&lt;/p&gt;
&lt;p&gt;
Fast Rates in Online Convex Optimization by Exploiting the Curvature of Feasible Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12868
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#34892;&#38598;&#30340;&#26354;&#29575;&#65292;&#22312;&#22312;&#32447;&#20984;&#20248;&#21270;&#20013;&#23454;&#29616;&#20102;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#32447;&#20984;&#20248;&#21270;&#65288;OCO&#65289;&#65292;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#21487;&#34892;&#38598;&#30340;&#26354;&#29575;&#25552;&#20379;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#30340;&#26032;&#20998;&#26512;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#65292;&#22914;&#26524;&#26368;&#20248;&#20915;&#31574;&#20301;&#20110;&#21487;&#34892;&#38598;&#30340;&#36793;&#30028;&#19978;&#19988;&#22522;&#30784;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#38750;&#38646;&#65292;&#21017;&#31639;&#27861;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#21487;&#20197;&#36798;&#21040;$O(\rho \log T)$&#30340;&#36951;&#25022;&#19978;&#30028;&#12290;&#20854;&#20013;&#65292;$\rho &gt; 0$&#26159;&#21253;&#21547;&#26368;&#20248;&#20915;&#31574;&#24182;&#22260;&#32469;&#21487;&#34892;&#38598;&#30340;&#26368;&#23567;&#29699;&#20307;&#30340;&#21322;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12868v1 Announce Type: new  Abstract: In this paper, we explore online convex optimization (OCO) and introduce a new analysis that provides fast rates by exploiting the curvature of feasible sets. In online linear optimization, it is known that if the average gradient of loss functions is larger than a certain value, the curvature of feasible sets can be exploited by the follow-the-leader (FTL) algorithm to achieve a logarithmic regret. This paper reveals that algorithms adaptive to the curvature of loss functions can also leverage the curvature of feasible sets. We first prove that if an optimal decision is on the boundary of a feasible set and the gradient of an underlying loss function is non-zero, then the algorithm achieves a regret upper bound of $O(\rho \log T)$ in stochastic environments. Here, $\rho &gt; 0$ is the radius of the smallest sphere that includes the optimal decision and encloses the feasible set. Our approach, unlike existing ones, can work directly with co
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#20998;&#35299;&#21644;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21516;&#26469;&#28304;&#65292;&#24182;&#28548;&#28165;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.10727</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20998;&#35299;&#23454;&#29616;&#20005;&#26684;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#30340;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Predictive Uncertainty Quantification via Risk Decompositions for Strictly Proper Scoring Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10727
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#20998;&#35299;&#21644;&#36866;&#24403;&#35780;&#20998;&#35268;&#21017;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#26469;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#19981;&#21516;&#26469;&#28304;&#65292;&#24182;&#28548;&#28165;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#39044;&#27979;&#27169;&#22411;&#24212;&#29992;&#20013;&#65292;&#21306;&#20998;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#26469;&#28304;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#35768;&#22810;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#20294;&#24182;&#27809;&#26377;&#20005;&#26684;&#30340;&#23450;&#20041;&#26469;&#35299;&#24320;&#23427;&#20204;&#12290;&#27492;&#22806;&#65292;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#25514;&#26045;&#20043;&#38388;&#30340;&#20851;&#31995;&#20173;&#28982;&#26377;&#20123;&#19981;&#28165;&#26224;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26681;&#26893;&#20110;&#32479;&#35745;&#25512;&#29702;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#19981;&#20165;&#20801;&#35768;&#21019;&#24314;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#65292;&#36824;&#28548;&#28165;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#32479;&#35745;&#39118;&#38505;&#26469;&#21306;&#20998;aleatoric&#21644;epistemic&#19981;&#30830;&#23450;&#24615;&#25104;&#20998;&#65292;&#24182;&#21033;&#29992;&#36866;&#24403;&#30340;&#35780;&#20998;&#35268;&#21017;&#23545;&#20854;&#36827;&#34892;&#37327;&#21270;&#12290;&#20026;&#20102;&#20351;&#20854;&#22312;&#23454;&#36341;&#20013;&#26131;&#20110;&#22788;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#36825;&#19968;&#26694;&#26550;&#20013;&#25972;&#21512;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#24819;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#25152;&#25552;&#36817;&#20284;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10727v1 Announce Type: cross  Abstract: Distinguishing sources of predictive uncertainty is of crucial importance in the application of forecasting models across various domains. Despite the presence of a great variety of proposed uncertainty measures, there are no strict definitions to disentangle them. Furthermore, the relationship between different measures of uncertainty quantification remains somewhat unclear. In this work, we introduce a general framework, rooted in statistical reasoning, which not only allows the creation of new uncertainty measures but also clarifies their interrelations. Our approach leverages statistical risk to distinguish aleatoric and epistemic uncertainty components and utilizes proper scoring rules to quantify them. To make it practically tractable, we propose an idea to incorporate Bayesian reasoning into this framework and discuss the properties of the proposed approximation.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09698</link><description>&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#36807;&#28388;&#22120;&#20013;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Combining Evidence Across Filtrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#20309;&#26102;&#21051;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#29702;&#20013;&#65292;&#24050;&#30693;&#20219;&#20309;&#21487;&#25509;&#21463;&#30340;&#25512;&#29702;&#26041;&#27861;&#24517;&#39035;&#22522;&#20110;&#27979;&#35797;&#38789;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#24191;&#20041;&#21270;&#65292;&#31216;&#20026;e&#36827;&#31243;&#65292;&#23427;&#20204;&#26159;&#38750;&#36127;&#36827;&#31243;&#65292;&#20854;&#22312;&#20219;&#20309;&#20219;&#24847;&#20572;&#26102;&#30340;&#26399;&#26395;&#19978;&#30028;&#19981;&#36229;&#36807;&#19968;&#12290;e&#36827;&#31243;&#37327;&#21270;&#20102;&#38024;&#23545;&#22797;&#21512;&#38646;&#20551;&#35774;&#30340;&#19968;&#31995;&#21015;&#32467;&#26524;&#30340;&#32047;&#31215;&#35777;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#20449;&#24687;&#38598;&#65288;&#21363;&#36807;&#28388;&#22120;&#65289;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#21512;&#24182;&#26041;&#27861;&#65292;&#38024;&#23545;&#19968;&#20010;&#38646;&#20551;&#35774;&#12290;&#23613;&#31649;&#22312;&#30456;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#21487;&#20197;&#36731;&#26494;&#22320;&#21512;&#24182;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#24179;&#22343;&#65289;&#65292;&#20294;&#22312;&#19981;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#19981;&#33021;&#37027;&#20040;&#23481;&#26131;&#22320;&#21512;&#24182;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36739;&#31895;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#19981;&#33021;&#36716;&#25442;&#20026;&#22312;&#26356;&#32454;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25991;&#29486;&#20013;&#19977;&#20010;&#20855;&#20307;&#20363;&#23376;&#65306;&#21487;&#20132;&#25442;&#24615;&#27979;&#35797;&#65292;&#29420;&#31435;&#24615;&#27979;&#35797;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09698v1 Announce Type: cross  Abstract: In anytime-valid sequential inference, it is known that any admissible inference procedure must be based on test martingales and their composite generalization, called e-processes, which are nonnegative processes whose expectation at any arbitrary stopping time is upper-bounded by one. An e-process quantifies the accumulated evidence against a composite null hypothesis over a sequence of outcomes. This paper studies methods for combining e-processes that are computed using different information sets, i.e., filtrations, for a null hypothesis. Even though e-processes constructed on the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed on different filtrations cannot be combined as easily because their validity in a coarser filtration does not translate to validity in a finer filtration. We discuss three concrete examples of such e-processes in the literature: exchangeability tests, independence te
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#30417;&#27979;&#38382;&#39064;&#20013;&#25506;&#32034;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#27491;&#21017;&#21270;&#22120;&#21487;&#20197;&#25552;&#39640;&#22312;&#38543;&#26426;&#21644;&#23545;&#25239;&#29615;&#22659;&#20013;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.08321</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#27491;&#21017;&#21270;&#22120;&#30340;&#20248;&#21270;&#25506;&#32034;&#65306;&#22312;&#37096;&#20998;&#30417;&#27979;&#20013;&#20855;&#26377;&#23545;&#25239;&#40065;&#26834;&#24615;&#30340;&#23545;&#25968;&#36951;&#25022;
&lt;/p&gt;
&lt;p&gt;
Exploration by Optimization with Hybrid Regularizers: Logarithmic Regret with Adversarial Robustness in Partial Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08321
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#37096;&#20998;&#30417;&#27979;&#38382;&#39064;&#20013;&#25506;&#32034;&#20248;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#28151;&#21512;&#27491;&#21017;&#21270;&#22120;&#21487;&#20197;&#25552;&#39640;&#22312;&#38543;&#26426;&#21644;&#23545;&#25239;&#29615;&#22659;&#20013;&#30340;&#36951;&#25022;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#20998;&#30417;&#27979;&#26159;&#19968;&#31181;&#20855;&#26377;&#26377;&#38480;&#35266;&#27979;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;&#20026;&#20102;&#20174;&#36825;&#31181;&#26377;&#38480;&#35266;&#27979;&#20013;&#20570;&#20986;&#20915;&#31574;&#65292;&#38656;&#35201;&#25214;&#21040;&#19968;&#20010;&#36866;&#24403;&#30340;&#25506;&#32034;&#20998;&#24067;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27492;&#30446;&#30340;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#20248;&#21270;&#36827;&#34892;&#25506;&#32034;&#65288;ExO&#65289;&#65292;&#23427;&#21033;&#29992;&#36861;&#36394;&#27491;&#21017;&#21270;&#26368;&#20248;&#26041;&#27861;&#65292;&#22312;&#24191;&#27867;&#30340;&#22312;&#32447;&#20915;&#31574;&#38382;&#39064;&#20013;&#23454;&#29616;&#23545;&#25239;&#29615;&#22659;&#19979;&#30340;&#26368;&#20248;&#30028;&#38480;&#12290;&#28982;&#32780;&#65292;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#32431;&#31929;&#24212;&#29992;ExO&#20250;&#26174;&#33879;&#38477;&#20302;&#36951;&#25022;&#30028;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#23616;&#37096;&#21487;&#35266;&#27979;&#28216;&#25103;&#20013;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;ExO&#19982;&#28151;&#21512;&#27491;&#21017;&#21270;&#22120;&#30340;&#26694;&#26550;&#21644;&#20998;&#26512;&#12290;&#36825;&#20010;&#21457;&#23637;&#20351;&#25105;&#20204;&#33021;&#22815;&#26174;&#33879;&#25913;&#36827;&#26368;&#20339;&#21452;&#36194;&#31639;&#27861;&#65288;BOBW&#65289;&#30340;&#29616;&#26377;&#36951;&#25022;&#30028;&#38480;&#65292;&#22312;&#38543;&#26426;&#21644;&#23545;&#25239;&#29615;&#22659;&#20013;&#37117;&#23454;&#29616;&#20102;&#20960;&#20046;&#26368;&#20248;&#30340;&#30028;&#38480;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#38543;&#26426;&#36951;&#25022;&#30028;&#38480;&#20026;$O(\sum_{a \neq a^*} k^2 m^2$
&lt;/p&gt;
&lt;p&gt;
Partial monitoring is a generic framework of online decision-making problems with limited observations. To make decisions from such limited observations, it is necessary to find an appropriate distribution for exploration. Recently, a powerful approach for this purpose, exploration by optimization (ExO), was proposed, which achieves the optimal bounds in adversarial environments with follow-the-regularized-leader for a wide range of online decision-making problems. However, a naive application of ExO in stochastic environments significantly degrades regret bounds. To resolve this problem in locally observable games, we first establish a novel framework and analysis for ExO with a hybrid regularizer. This development allows us to significantly improve the existing regret bounds of best-of-both-worlds (BOBW) algorithms, which achieves nearly optimal bounds both in stochastic and adversarial environments. In particular, we derive a stochastic regret bound of $O(\sum_{a \neq a^*} k^2 m^2 \
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21830;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;&#25968;&#25454;&#28304;&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#25512;&#33616;&#24341;&#25806;&#23545;&#29992;&#25143;&#20307;&#39564;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.08109</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#21040;&#20915;&#31574;&#65306;&#26426;&#22120;&#23398;&#20064;&#22312;&#21830;&#19994;&#25512;&#33616;&#20013;&#30340;&#36716;&#21464;&#21147;&#37327;
&lt;/p&gt;
&lt;p&gt;
From Data to Decisions: The Transformational Power of Machine Learning in Business Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#21830;&#19994;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20316;&#29992;&#65292;&#30528;&#37325;&#30740;&#31350;&#20102;&#25968;&#25454;&#28304;&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#35780;&#20272;&#25351;&#26631;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#31361;&#26174;&#20102;&#25512;&#33616;&#24341;&#25806;&#23545;&#29992;&#25143;&#20307;&#39564;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#26426;&#22120;&#23398;&#20064;&#23545;&#25512;&#33616;&#31995;&#32479;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#28436;&#21464;&#21644;&#26377;&#25928;&#24615;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#32972;&#26223;&#19979;&#12290;&#22312;&#26041;&#27861;&#35770;&#19978;&#65292;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#22609;&#36896;&#21644;&#25913;&#36827;&#30340;&#20316;&#29992;&#65292;&#30528;&#37325;&#30740;&#31350;&#25968;&#25454;&#26469;&#28304;&#12289;&#29305;&#24449;&#24037;&#31243;&#21644;&#35780;&#20272;&#25351;&#26631;&#30340;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#31361;&#26174;&#20102;&#22686;&#24378;&#25512;&#33616;&#31639;&#27861;&#30340;&#36845;&#20195;&#24615;&#36136;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#25512;&#33616;&#24341;&#25806;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#39640;&#32423;&#31639;&#27861;&#21644;&#25968;&#25454;&#20998;&#26512;&#39537;&#21160;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#23545;&#29992;&#25143;&#20307;&#39564;&#21644;&#20915;&#31574;&#36807;&#31243;&#30340;&#37325;&#35201;&#24433;&#21709;&#12290;&#36825;&#20123;&#24341;&#25806;&#19981;&#20165;&#31616;&#21270;&#20102;&#20449;&#24687;&#21457;&#29616;&#21644;&#22686;&#24378;&#20102;&#21327;&#20316;&#65292;&#36824;&#21152;&#24555;&#20102;&#30693;&#35782;&#33719;&#21462;&#65292;&#23545;&#20225;&#19994;&#22312;&#25968;&#23383;&#21270;&#39046;&#22495;&#20013;&#30340;&#23548;&#33322;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#20204;&#23545;&#38144;&#21806;&#12289;&#25910;&#20837;&#21644;&#20225;&#19994;&#31454;&#20105;&#20248;&#21183;&#30340;&#36129;&#29486;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
This research aims to explore the impact of Machine Learning (ML) on the evolution and efficacy of Recommendation Systems (RS), particularly in the context of their growing significance in commercial business environments. Methodologically, the study delves into the role of ML in crafting and refining these systems, focusing on aspects such as data sourcing, feature engineering, and the importance of evaluation metrics, thereby highlighting the iterative nature of enhancing recommendation algorithms. The deployment of Recommendation Engines (RE), driven by advanced algorithms and data analytics, is explored across various domains, showcasing their significant impact on user experience and decision-making processes. These engines not only streamline information discovery and enhance collaboration but also accelerate knowledge acquisition, proving vital in navigating the digital landscape for businesses. They contribute significantly to sales, revenue, and the competitive edge of enterpr
&lt;/p&gt;</description></item><item><title>&#22312;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#21487;&#20197;&#20135;&#29983;&#38750;&#22343;&#21248;&#20559;&#24046;&#65292;&#22240;&#27492;&#36890;&#24120;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20250;&#19982;&#31364;&#25945;&#24072;NN&#19968;&#26679;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2402.06323</link><description>&lt;p&gt;
&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#22914;&#20309;&#24341;&#36215;&#19981;&#22343;&#21248;&#20559;&#24046;&#65306;&#20856;&#22411;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#19982;&#31364;&#25945;&#24072;&#30340;&#26222;&#36941;&#24615;
&lt;/p&gt;
&lt;p&gt;
How Uniform Random Weights Induce Non-uniform Bias: Typical Interpolating Neural Networks Generalize with Narrow Teachers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06323
&lt;/p&gt;
&lt;p&gt;
&#22312;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#22343;&#21248;&#38543;&#26426;&#26435;&#37325;&#21487;&#20197;&#20135;&#29983;&#38750;&#22343;&#21248;&#20559;&#24046;&#65292;&#22240;&#27492;&#36890;&#24120;&#25554;&#20540;&#31070;&#32463;&#32593;&#32476;&#20250;&#19982;&#31364;&#25945;&#24072;NN&#19968;&#26679;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#29702;&#35770;&#38590;&#39064;&#26159;&#24403;&#31070;&#32463;&#32593;&#32476;&#34987;&#35757;&#32451;&#21040;&#38646;&#35823;&#24046;&#65288;&#21363;&#25554;&#20540;&#25968;&#25454;&#65289;&#26102;&#65292;&#20026;&#20160;&#20040;&#36229;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#33021;&#22815;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#36890;&#24120;&#65292;NN&#26159;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#25110;&#20854;&#21464;&#31181;&#20043;&#19968;&#35757;&#32451;&#30340;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#23454;&#35777;&#30740;&#31350;&#26816;&#39564;&#20102;&#20174;&#30475;&#20284;&#22343;&#21248;&#30340;&#21442;&#25968;&#20808;&#39564;&#20013;&#37319;&#26679;&#30340;&#38543;&#26426;NN&#23545;&#25968;&#25454;&#30340;&#27867;&#21270;&#33021;&#21147;&#65306;&#35813;NN&#23545;&#35757;&#32451;&#38598;&#36827;&#34892;&#20102;&#23436;&#32654;&#20998;&#31867;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#36825;&#26679;&#30340;NN&#26679;&#26412;&#36890;&#24120;&#20687;SGD&#35757;&#32451;&#30340;NN&#19968;&#26679;&#27867;&#21270;&#33391;&#22909;&#12290;&#36129;&#29486;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22914;&#26524;&#23384;&#22312;&#19982;&#26631;&#31614;&#19968;&#33268;&#30340;&#31364;&#8220;&#25945;&#24072;NN&#8221;&#65292;&#37027;&#20040;&#36825;&#26679;&#30340;&#38543;&#26426;NN&#25554;&#20540;&#22120;&#36890;&#24120;&#33021;&#24456;&#22909;&#22320;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;NN&#21442;&#25968;&#21270;&#20013;&#30340;&#8220;&#24179;&#22374;&#8221;&#20808;&#39564;&#36890;&#36807;NN&#32467;&#26500;&#20013;&#30340;&#20887;&#20313;&#24341;&#20837;&#20102;&#20016;&#23500;&#30340;NN&#20989;&#25968;&#20808;&#39564;&#12290;&#29305;&#21035;&#26159;&#65292;&#36825;&#20250;&#23545;&#36739;&#31616;&#21333;&#30340;&#20989;&#25968;&#20135;&#29983;&#20559;&#21521;&#65292;&#36825;&#20123;&#20989;&#25968;&#38656;&#35201;&#36739;&#23569;&#30340;&#30456;&#20851;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background. A main theoretical puzzle is why over-parameterized Neural Networks (NNs) generalize well when trained to zero loss (i.e., so they interpolate the data). Usually, the NN is trained with Stochastic Gradient Descent (SGD) or one of its variants. However, recent empirical work examined the generalization of a random NN that interpolates the data: the NN was sampled from a seemingly uniform prior over the parameters, conditioned on that the NN perfectly classifying the training set. Interestingly, such a NN sample typically generalized as well as SGD-trained NNs.   Contributions. We prove that such a random NN interpolator typically generalizes well if there exists an underlying narrow ``teacher NN" that agrees with the labels. Specifically, we show that such a `flat' prior over the NN parametrization induces a rich prior over the NN functions, due to the redundancy in the NN structure. In particular, this creates a bias towards simpler functions, which require less relevant pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28201;&#24230;&#32553;&#25918;&#23545;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#26657;&#20934;&#23545;&#33258;&#36866;&#24212;C&#26041;&#27861;&#20135;&#29983;&#20102;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.05806</link><description>&lt;p&gt;
&#20851;&#20110;&#28145;&#24230;&#20998;&#31867;&#22120;&#30340;&#26657;&#20934;&#21644;&#31526;&#21512;&#39044;&#27979;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Calibration and Conformal Prediction of Deep Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28201;&#24230;&#32553;&#25918;&#23545;&#31526;&#21512;&#39044;&#27979;&#26041;&#27861;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#65292;&#26657;&#20934;&#23545;&#33258;&#36866;&#24212;C&#26041;&#27861;&#20135;&#29983;&#20102;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#20998;&#31867;&#24212;&#29992;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22522;&#20110;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#38656;&#35201;&#20276;&#38543;&#19968;&#20123;&#32622;&#20449;&#24230;&#25351;&#31034;&#12290;&#38024;&#23545;&#36825;&#20010;&#30446;&#26631;&#65292;&#26377;&#20004;&#31181;&#27969;&#34892;&#30340;&#21518;&#22788;&#29702;&#26041;&#27861;&#65306;1&#65289;&#26657;&#20934;&#65306;&#20462;&#25913;&#20998;&#31867;&#22120;&#30340;softmax&#20540;&#65292;&#20351;&#20854;&#26368;&#22823;&#20540;&#65288;&#19982;&#39044;&#27979;&#30456;&#20851;&#65289;&#26356;&#22909;&#22320;&#20272;&#35745;&#27491;&#30830;&#27010;&#29575;&#65307;&#21644;2&#65289;&#31526;&#21512;&#39044;&#27979;&#65288;CP&#65289;&#65306;&#35774;&#35745;&#19968;&#20010;&#22522;&#20110;softmax&#20540;&#30340;&#20998;&#25968;&#65292;&#20174;&#20013;&#20135;&#29983;&#19968;&#32452;&#39044;&#27979;&#65292;&#20855;&#26377;&#29702;&#35770;&#19978;&#20445;&#35777;&#27491;&#30830;&#31867;&#21035;&#36793;&#38469;&#35206;&#30422;&#30340;&#29305;&#24615;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#20004;&#31181;&#25351;&#31034;&#37117;&#21487;&#33021;&#26159;&#38656;&#35201;&#30340;&#65292;&#20294;&#21040;&#30446;&#21069;&#20026;&#27490;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#23578;&#26410;&#24471;&#21040;&#30740;&#31350;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28201;&#24230;&#32553;&#25918;&#65292;&#36825;&#26159;&#26368;&#24120;&#35265;&#30340;&#26657;&#20934;&#25216;&#26415;&#65292;&#23545;&#37325;&#35201;&#30340;CP&#26041;&#27861;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#39318;&#20808;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#20854;&#20013;&#26174;&#31034;&#20102;&#19968;&#20123;&#37325;&#35201;&#30340;&#27934;&#23519;&#65292;&#20854;&#20013;&#21253;&#25324;&#20196;&#20154;&#24778;&#35766;&#30340;&#21457;&#29616;&#65292;&#21363;&#26657;&#20934;&#23545;&#27969;&#34892;&#30340;&#33258;&#36866;&#24212;C&#26041;&#27861;&#20135;&#29983;&#20102;&#26377;&#23475;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many classification applications, the prediction of a deep neural network (DNN) based classifier needs to be accompanied with some confidence indication. Two popular post-processing approaches for that aim are: 1) calibration: modifying the classifier's softmax values such that their maximum (associated with the prediction) better estimates the correctness probability; and 2) conformal prediction (CP): devising a score (based on the softmax values) from which a set of predictions with theoretically guaranteed marginal coverage of the correct class is produced. While in practice both types of indications can be desired, so far the interplay between them has not been investigated. Toward filling this gap, in this paper we study the effect of temperature scaling, arguably the most common calibration technique, on prominent CP methods. We start with an extensive empirical study that among other insights shows that, surprisingly, calibration has a detrimental effect on popular adaptive C
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#22312;2-Wasserstein&#36317;&#31163;&#20013;&#30340;&#19968;&#33324;&#31867;&#27010;&#29575;&#27969;ODE&#25277;&#26679;&#22120;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20551;&#35774;&#24471;&#20998;&#20272;&#35745;&#20934;&#30830;&#12290;</title><link>https://arxiv.org/abs/2401.17958</link><description>&lt;p&gt;
&#22312;Wasserstein&#36317;&#31163;&#20013;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#19968;&#33324;&#27010;&#29575;&#27969;ODE&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Convergence Analysis for General Probability Flow ODEs of Diffusion Models in Wasserstein Distances
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17958
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#22312;2-Wasserstein&#36317;&#31163;&#20013;&#30340;&#19968;&#33324;&#31867;&#27010;&#29575;&#27969;ODE&#25277;&#26679;&#22120;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#20551;&#35774;&#24471;&#20998;&#20272;&#35745;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#29575;&#27969;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#30340;&#22522;&#20110;&#24471;&#20998;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#24555;&#36895;&#30340;&#22522;&#20110;ODE&#30340;&#25277;&#26679;&#22120;&#24182;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#65292;&#20294;&#23545;&#27010;&#29575;&#27969;ODE&#30340;&#25910;&#25947;&#24615;&#23646;&#24615;&#30340;&#29702;&#35770;&#29702;&#35299;&#20173;&#28982;&#38750;&#24120;&#26377;&#38480;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36866;&#29992;&#20110;2-Wasserstein&#36317;&#31163;&#20013;&#30340;&#19968;&#33324;&#31867;&#27010;&#29575;&#27969;ODE&#25277;&#26679;&#22120;&#30340;&#39318;&#20010;&#38750;&#28176;&#36817;&#25910;&#25947;&#24615;&#20998;&#26512;&#32467;&#26524;&#65292;&#20551;&#35774;&#20934;&#30830;&#30340;&#24471;&#20998;&#20272;&#35745;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#21508;&#31181;&#31034;&#20363;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#24212;&#22522;&#20110;ODE&#30340;&#25277;&#26679;&#22120;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Score-based generative modeling with probability flow ordinary differential equations (ODEs) has achieved remarkable success in a variety of applications. While various fast ODE-based samplers have been proposed in the literature and employed in practice, the theoretical understandings about convergence properties of the probability flow ODE are still quite limited. In this paper, we provide the first non-asymptotic convergence analysis for a general class of probability flow ODE samplers in 2-Wasserstein distance, assuming accurate score estimates. We then consider various examples and establish results on the iteration complexity of the corresponding ODE-based samplers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;</title><link>https://arxiv.org/abs/2401.17809</link><description>&lt;p&gt;
SWEA:&#36890;&#36807;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#25913;&#21464;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20107;&#23454;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
SWEA: Changing Factual Knowledge in Large Language Models via Subject Word Embedding Altering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17809
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#26694;&#26550;&#65288;SWEA&#65289;&#65292;&#36890;&#36807;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#26469;&#32534;&#36753;&#30693;&#35782;&#65292;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#65292;&#36991;&#20813;&#19981;&#21487;&#36870;&#30340;&#25439;&#23475;&#21644;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#36817;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#27169;&#22411;&#32534;&#36753;&#26041;&#27861;&#20027;&#35201;&#28041;&#21450;&#20462;&#25913;&#27169;&#22411;&#21442;&#25968;&#25110;&#21521;&#29616;&#26377;&#27169;&#22411;&#28155;&#21152;&#38468;&#21152;&#27169;&#22359;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#20250;&#23545;LLM&#36896;&#25104;&#19981;&#21487;&#36870;&#30340;&#24433;&#21709;&#65292;&#32780;&#21518;&#32773;&#20250;&#20135;&#29983;&#39069;&#22806;&#30340;&#25512;&#29702;&#24320;&#38144;&#65292;&#24182;&#19988;&#27169;&#31946;&#30340;&#21521;&#37327;&#21305;&#37197;&#24182;&#19981;&#24635;&#26159;&#21487;&#38752;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65288;SWEA&#65289;&#26694;&#26550;&#65292;&#23427;&#22312;&#25512;&#29702;&#38454;&#27573;&#20462;&#25913;&#20027;&#39064;&#30340;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#32534;&#36753;&#30693;&#35782;&#30340;&#30446;&#26631;&#12290;SWEA&#22312;&#27169;&#22411;&#22806;&#37096;&#20351;&#29992;&#31934;&#30830;&#30340;&#20851;&#38190;&#21305;&#37197;&#65292;&#24182;&#36827;&#34892;&#21487;&#38752;&#30340;&#20027;&#39064;&#35789;&#23884;&#20837;&#20462;&#25913;&#65292;&#20174;&#32780;&#20445;&#25252;&#27169;&#22411;&#30340;&#21407;&#22987;&#26435;&#37325;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#24320;&#38144;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20248;&#21270;&#25233;&#21046;&#34701;&#21512;&#26041;&#27861;&#65292;&#39318;&#20808;&#20248;&#21270;&#32534;&#36753;&#30446;&#26631;&#30340;&#23884;&#20837;&#21521;&#37327;&#65292;&#28982;&#21518;&#25233;&#21046;&#30693;&#35782;&#23884;&#20837;&#32500;&#24230;&#65288;KED&#65289;&#20197;&#33719;&#24471;&#26368;&#32456;&#34701;&#21512;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#22240;&#27492;&#25552;&#20986;&#20102;SWEAOS&#20803;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Model editing has recently gained widespread attention. Current model editing methods primarily involve modifying model parameters or adding additional modules to the existing model. However, the former causes irreversible damage to LLMs, while the latter incurs additional inference overhead and fuzzy vector matching is not always reliable. To address these issues, we propose an expandable Subject Word Embedding Altering (SWEA) framework, which modifies the representation of subjects and achieve the goal of editing knowledge during the inference stage. SWEA uses precise key matching outside the model and performs reliable subject word embedding altering, thus protecting the original weights of the model without increasing inference overhead. We then propose optimizing then suppressing fusion method, which first optimizes the embedding vector for the editing target and then suppresses the Knowledge Embedding Dimension (KED) to obtain the final fused embedding. We thus propose SWEAOS met
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Fakepedia&#25968;&#25454;&#38598;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#33021;&#21147;&#21644;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#23384;&#20648;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.02073</link><description>&lt;p&gt;
&#19968;&#21058;&#30149;&#27602;&#65311;&#20351;&#29992;Fakepedia&#23450;&#20301;&#21644;&#26816;&#27979;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02073
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Fakepedia&#25968;&#25454;&#38598;&#30740;&#31350;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#33021;&#21147;&#21644;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#20197;&#35299;&#20915;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#23384;&#20648;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20855;&#26377;&#20174;&#20854;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#26032;&#39062;&#20449;&#24687;&#20013;&#33719;&#24471;&#30340;&#20986;&#33394;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23578;&#19981;&#28165;&#26970;&#22312;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#21442;&#25968;&#20013;&#23384;&#20648;&#30340;&#20107;&#23454;&#30693;&#35782;&#30456;&#30683;&#30462;&#30340;&#24773;&#20917;&#19979;&#65292;&#25903;&#25745;&#36825;&#31181;&#19978;&#19979;&#25991;&#22522;&#30784;&#30340;&#26426;&#21046;&#65292;LLMs&#22312;&#22238;&#24518;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#33394;&#12290;&#20559;&#22909;&#19978;&#19979;&#25991;&#20449;&#24687;&#23545;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26041;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#23558;&#19978;&#19979;&#25991;&#19982;&#26368;&#26032;&#20449;&#24687;&#20016;&#23500;&#65292;&#24076;&#26395;&#22522;&#30784;&#21487;&#20197;&#32416;&#27491;&#36807;&#26102;&#25110;&#26377;&#22122;&#22768;&#30340;&#23384;&#20648;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Fakepedia&#30340;&#26032;&#39062;&#26041;&#27861;&#26469;&#30740;&#31350;&#22522;&#30784;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#19982;&#27169;&#22411;&#20869;&#37096;&#21442;&#25968;&#30693;&#35782;&#20914;&#31361;&#30340;&#21453;&#20107;&#23454;&#25991;&#26412;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;Fakepedia&#23545;&#21508;&#31181;LLMs&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#65292;&#22522;&#20110;&#25105;&#20204;&#30340;&#36974;&#34109;&#20998;&#32452;&#22240;&#26524;&#36861;&#36394;&#65288;MGCT&#65289;&#65292;&#23545;&#22238;&#31572;Fakepedia&#26597;&#35810;&#26102;&#30340;LLM&#32452;&#20214;&#36827;&#34892;&#20998;&#26512;&#12290;&#22312;&#36825;&#20010;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#37492;&#21035;&#20986;d
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02073v2 Announce Type: replace  Abstract: Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a dataset of counterfactual texts constructed to clash with a model's internal parametric knowledge. We benchmark various LLMs with Fakepedia and then we conduct a causal mediation analysis, based on our Masked Grouped Causal Tracing (MGCT), on LLM components when answering Fakepedia queries. Within this analysis, we identify d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#25345;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#26469;&#35757;&#32451;&#33258;&#24341;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20803;&#23398;&#20064;&#33258;&#36523;&#30340;&#19978;&#19979;&#25991;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#35299;&#20915;&#8220;&#19978;&#19979;&#25991;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2312.00276</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Automating Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00276
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#25345;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#26469;&#35757;&#32451;&#33258;&#24341;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20803;&#23398;&#20064;&#33258;&#36523;&#30340;&#19978;&#19979;&#25991;&#25345;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#22312;&#35299;&#20915;&#8220;&#19978;&#19979;&#25991;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#26041;&#38754;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33324;&#29992;&#36884;&#30340;&#23398;&#20064;&#31995;&#32479;&#24212;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#20013;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#19981;&#26029;&#25913;&#36827;&#33258;&#36523;&#12290;&#28982;&#32780;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#20256;&#32479;&#23398;&#20064;&#31639;&#27861;&#24120;&#24120;&#36973;&#21463;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;-&#24403;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#20808;&#21069;&#33719;&#24471;&#30340;&#25216;&#33021;&#34987;&#36951;&#24536;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#21270;&#25345;&#32493;&#23398;&#20064;&#65288;ACL&#65289;&#26469;&#35757;&#32451;&#33258;&#24341;&#29992;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#20803;&#23398;&#20064;&#33258;&#36523;&#30340;&#19978;&#19979;&#25991;&#25345;&#32493;(meta-)&#23398;&#20064;&#31639;&#27861;&#12290;ACL&#23558;&#25152;&#26377;&#26399;&#26395;&#34920;&#29616;&#33391;&#22909;&#20110;&#26032;&#26087;&#20219;&#21153;&#30340;&#35201;&#27714;&#32534;&#30721;&#21040;&#20854;&#20803;&#23398;&#20064;&#30446;&#26631;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;ACL&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#8220;&#19978;&#19979;&#25991;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;; &#25105;&#20204;&#36890;&#36807;ACL&#23398;&#21040;&#30340;&#31639;&#27861;&#22312;&#26080;&#37325;&#25918;&#35774;&#32622;&#19979;&#20248;&#20110;&#25163;&#24037;&#21046;&#23450;&#30340;&#31639;&#27861;&#65292;&#20363;&#22914;&#22312;Split-MNIST&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#24182;&#19988;&#21487;&#20197;&#25345;&#32493;&#23398;&#20064;&#30001;&#22810;&#20010;&#23569;&#37327;&#31034;&#20363;&#21644;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#32452;&#25104;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00276v2 Announce Type: replace  Abstract: General-purpose learning systems should improve themselves in open-ended fashion in ever-changing environments. Conventional learning algorithms for neural networks, however, suffer from catastrophic forgetting (CF) -- previously acquired skills are forgotten when a new task is learned. Instead of hand-crafting new algorithms for avoiding CF, we propose Automated Continual Learning (ACL) to train self-referential neural networks to meta-learn their own in-context continual (meta-)learning algorithms. ACL encodes all desiderata -- good performance on both old and new tasks -- into its meta-learning objectives. Our experiments demonstrate that ACL effectively solves "in-context catastrophic forgetting"; our ACL-learned algorithms outperform hand-crafted ones, e.g., on the Split-MNIST benchmark in the replay-free setting, and enables continual learning of diverse tasks consisting of multiple few-shot and standard image classification da
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#23436;&#25972;&#25910;&#25947;&#29702;&#35770;&#20445;&#35777;&#65292;&#33719;&#24471;&#20102;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#21644;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#20248;&#19978;&#38480;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.13584</link><description>&lt;p&gt;
&#20851;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#21450;&#20854;&#35823;&#24046;&#30028;&#38480;&#65306;&#23436;&#20840;&#25910;&#25947;&#20272;&#35745;&#19979;&#30340;&#23545;&#25968;&#20985;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
On diffusion-based generative models and their error bounds: The log-concave case with full convergence estimates
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13584
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20110;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#19979;&#30340;&#23436;&#25972;&#25910;&#25947;&#29702;&#35770;&#20445;&#35777;&#65292;&#33719;&#24471;&#20102;&#23545;&#20110;&#21442;&#25968;&#20272;&#35745;&#21644;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#20248;&#19978;&#38480;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24378;&#23545;&#25968;&#20985;&#25968;&#25454;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#20026;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#25910;&#25947;&#34892;&#20026;&#25552;&#20379;&#20102;&#23436;&#25972;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#32780;&#25105;&#20204;&#29992;&#20110;&#24471;&#20998;&#20272;&#35745;&#30340;&#36924;&#36817;&#20989;&#25968;&#31867;&#30001;Lipschitz&#36830;&#32493;&#20989;&#25968;&#32452;&#25104;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#28608;&#21169;&#24615;&#20363;&#23376;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24378;&#22823;&#20043;&#22788;&#65292;&#21363;&#20174;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23545;&#30456;&#20851;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#24471;&#20998;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#26126;&#30830;&#30340;&#20272;&#35745;&#65292;&#21516;&#26102;&#23558;&#20854;&#19982;&#30456;&#24212;&#30340;&#37319;&#26679;&#20272;&#35745;&#32467;&#21512;&#36215;&#26469;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#24050;&#30693;&#19978;&#38480;&#20272;&#35745;&#65292;&#28041;&#21450;&#20851;&#38190;&#24863;&#20852;&#36259;&#30340;&#25968;&#37327;&#65292;&#22914;&#25968;&#25454;&#20998;&#24067;&#65288;&#20855;&#26377;&#26410;&#30693;&#22343;&#20540;&#30340;&#39640;&#26031;&#20998;&#24067;&#65289;&#19982;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20043;&#38388;&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#32500;&#24230;&#21644;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13584v2 Announce Type: replace  Abstract: We provide full theoretical guarantees for the convergence behaviour of diffusion-based generative models under the assumption of strongly log-concave data distributions while our approximating class of functions used for score estimation is made of Lipschitz continuous functions. We demonstrate via a motivating example, sampling from a Gaussian distribution with unknown mean, the powerfulness of our approach. In this case, explicit estimates are provided for the associated optimization problem, i.e. score approximation, while these are combined with the corresponding sampling estimates. As a result, we obtain the best known upper bound estimates in terms of key quantities of interest, such as the dimension and rates of convergence, for the Wasserstein-2 distance between the data distribution (Gaussian with unknown mean) and our sampling algorithm.   Beyond the motivating example and in order to allow for the use of a diverse range o
&lt;/p&gt;</description></item><item><title>Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.16316</link><description>&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#65306;&#23545;&#29305;&#24449;&#32452;&#30340;&#24544;&#23454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16316
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#20934;&#30830;&#21453;&#26144;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#21017;&#34987;&#35748;&#20026;&#26159;&#8220;&#24544;&#23454;&#8221;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#24402;&#22240;&#31561;&#35299;&#37322;&#24182;&#19981;&#33021;&#20445;&#35777;&#24544;&#23454;&#65292;&#26377;&#21487;&#33021;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sum-of-Parts&#65288;SOP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31867;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#20855;&#26377;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#24544;&#23454;&#30340;&#29305;&#24449;&#32452;&#24402;&#22240;&#12290;&#35813;&#27169;&#22411;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#27599;&#20010;&#20998;&#25968;&#30452;&#25509;&#24402;&#22240;&#20110;&#19968;&#32452;&#31232;&#30095;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#23545;SOP&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;SOP&#25552;&#20379;&#30340;&#24544;&#23454;&#35299;&#37322;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;Tabula&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#30740;&#31350;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.12746</link><description>&lt;p&gt;
TabuLa: &#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
TabuLa: Harnessing Language Models for Tabular Data Synthesis. (arXiv:2310.12746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;Tabula&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#30740;&#31350;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#34920;&#26684;&#25968;&#25454;&#22312;&#21508;&#34892;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#34920;&#26126;&#65292;&#21487;&#20197;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#30001;&#20110;LLMs&#23558;&#34920;&#26684;&#25968;&#25454;&#39044;&#22788;&#29702;&#20026;&#20840;&#25991;&#65292;&#23427;&#20204;&#20855;&#26377;&#36991;&#20813;&#39640;&#32500;&#24230;&#25968;&#25454;&#30340;&#29420;&#28909;&#32534;&#30721;&#25152;&#24102;&#26469;&#30340;&#32500;&#24230;&#28798;&#38590;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;&#22312;&#26032;&#20219;&#21153;&#19978;&#30340;&#21487;&#37325;&#29992;&#24615;&#26377;&#38480;&#65292;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#21462;&#20195;&#29616;&#26377;&#30340;&#34920;&#26684;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;Tabula&#65292;&#19968;&#31181;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#22120;&#12290;&#36890;&#36807;Tabula&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#37319;&#29992;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#19987;&#38376;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#23450;&#21046;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the ubiquitous use of tabular data in industries and the growing concerns in data privacy and security, tabular data synthesis emerges as a critical research area. The recent state-of-the-art methods show that large language models (LLMs) can be adopted to generate realistic tabular data. As LLMs pre-process tabular data as full text, they have the advantage of avoiding the curse of dimensionality associated with one-hot encoding high-dimensional data. However, their long training time and limited re-usability on new tasks prevent them from replacing exiting tabular generative models. In this paper, we propose Tabula, a tabular data synthesizer based on the language model structure. Through Tabula, we demonstrate the inherent limitation of employing pre-trained language models designed for natural language processing (NLP) in the context of tabular data synthesis. Our investigation delves into the development of a dedicated foundational model tailored specifically for tabular dat
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2309.06089</link><description>&lt;p&gt;
&#22312;&#36328;&#35821;&#35328;&#36716;&#31227;&#33539;&#24335;&#20013;&#27979;&#37327;&#28798;&#38590;&#24615;&#36951;&#24536;&#65306;&#25506;&#32034;&#35843;&#20248;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Measuring Catastrophic Forgetting in Cross-Lingual Transfer Paradigms: Exploring Tuning Strategies. (arXiv:2309.06089v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06089
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#24494;&#35843;&#21644;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#22312;&#35299;&#20915;&#36328;&#35821;&#35328;&#20219;&#21153;&#26102;&#30340;&#34920;&#29616;&#65292;&#35780;&#20272;&#20102;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#21644;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#35821;&#35328;&#36716;&#31227;&#26159;&#19968;&#31181;&#35299;&#20915;&#36164;&#28304;&#21294;&#20047;&#35821;&#35328;&#20219;&#21153;&#30340;&#26377;&#24076;&#26395;&#30340;&#25216;&#26415;&#12290;&#22312;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#19982;&#38646;&#23556;&#21644;&#20840;&#23556;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36328;&#35821;&#35328;&#35774;&#32622;&#19979;&#30340;&#24494;&#35843;&#26041;&#27861;&#12290;&#20316;&#20026;&#24494;&#35843;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#21442;&#25968;&#25928;&#29575;&#36866;&#37197;&#22120;&#26041;&#27861;&#19982;&#25152;&#26377;&#21442;&#25968;&#24494;&#35843;&#12290;&#20316;&#20026;&#36328;&#35821;&#35328;&#36716;&#31227;&#31574;&#30053;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20351;&#29992;&#27599;&#20010;&#35821;&#35328;&#20381;&#27425;&#30340;&#20013;&#38388;&#35757;&#32451;&#65288;IT&#65289;&#21644;&#22312;&#24494;&#35843;&#30340;&#39564;&#35777;&#38454;&#27573;&#24050;&#32463;&#20351;&#29992;&#30446;&#26631;&#35821;&#35328;&#30340;&#36328;&#35821;&#35328;&#39564;&#35777;&#65288;CLV&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#36716;&#31227;&#30340;&#25104;&#21151;&#31243;&#24230;&#20197;&#21450;&#28304;&#35821;&#35328;&#20013;&#30001;&#20110;&#36328;&#35821;&#35328;&#36716;&#31227;&#32780;&#23548;&#33268;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#31243;&#24230;&#65292;&#21363;&#22312;&#23398;&#20064;&#19981;&#21516;&#35821;&#35328;&#20013;&#30340;&#26032;&#20449;&#24687;&#26102;&#20043;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#25439;&#22833;&#20102;&#22810;&#23569;&#12290;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#20998;&#31867;&#38382;&#39064;&#19978;&#65292;&#21253;&#25324;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#21644;&#20135;&#21697;&#35780;&#35770;&#65292;&#20998;&#21035;&#21253;&#21547;&#20102;&#22810;&#20010;&#35821;&#31181;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cross-lingual transfer is a promising technique to solve tasks in less-resourced languages. In this empirical study, we compare two fine-tuning approaches combined with zero-shot and full-shot learning approaches for large language models in a cross-lingual setting. As fine-tuning strategies, we compare parameter-efficient adapter methods with fine-tuning of all parameters. As cross-lingual transfer strategies, we compare the intermediate-training (\textit{IT}) that uses each language sequentially and cross-lingual validation (\textit{CLV}) that uses a target language already in the validation phase of fine-tuning. We assess the success of transfer and the extent of catastrophic forgetting in a source language due to cross-lingual transfer, i.e., how much previously acquired knowledge is lost when we learn new information in a different language. The results on two different classification problems, hate speech detection and product reviews, each containing datasets in several lang
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#36941;&#21382;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20123;&#32463;&#39564;&#27861;&#21017;&#65292;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03888</link><description>&lt;p&gt;
&#20174;&#36941;&#21382;&#29702;&#35770;&#30340;&#35282;&#24230;&#30475;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks from the perspective of ergodic theory. (arXiv:2308.03888v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#36941;&#21382;&#29702;&#35770;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23558;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35270;&#20026;&#21160;&#21147;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20123;&#32463;&#39564;&#27861;&#21017;&#65292;&#35299;&#37322;&#20102;&#31070;&#32463;&#32593;&#32476;&#35774;&#35745;&#20013;&#30340;&#19968;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#35774;&#35745;&#26469;&#35828;&#65292;&#20173;&#28982;&#26356;&#22810;&#22320;&#26159;&#19968;&#38376;&#33402;&#26415;&#32780;&#19981;&#26159;&#31934;&#30830;&#30340;&#31185;&#23398;&#12290;&#36890;&#36807;&#23558;&#36941;&#21382;&#29702;&#35770;&#32771;&#34385;&#24341;&#20837;&#21040;&#23558;&#32593;&#32476;&#35270;&#20026;&#21160;&#21147;&#31995;&#32479;&#26102;&#38388;&#28436;&#21270;&#30340;&#35266;&#28857;&#19978;&#65292;&#20854;&#20013;&#27599;&#19968;&#23618;&#23545;&#24212;&#20110;&#19968;&#20010;&#26102;&#38388;&#23454;&#20363;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20123;&#32463;&#39564;&#27861;&#21017;&#21487;&#20197;&#24402;&#23646;&#20026;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#20174;&#32780;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#30340;&#31070;&#31192;&#24615;&#24471;&#20197;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of deep neural networks remains somewhat of an art rather than precise science. By tentatively adopting ergodic theory considerations on top of viewing the network as the time evolution of a dynamical system, with each layer corresponding to a temporal instance, we show that some rules of thumb, which might otherwise appear mysterious, can be attributed heuristics.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#33258;&#36866;&#24212;&#30340;&#20146;&#21644;&#30697;&#38453;&#65292;&#24182;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#19982;&#25289;&#26222;&#25289;&#26031;&#22411;&#31639;&#23376;&#19968;&#33268;&#12290;</title><link>http://arxiv.org/abs/2307.09816</link><description>&lt;p&gt;
&#29992;&#31232;&#30095;&#27491;&#21017;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Manifold Learning with Sparse Regularised Optimal Transport. (arXiv:2307.09816v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09816
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#31232;&#30095;&#27491;&#21017;&#26368;&#20248;&#20256;&#36755;&#36827;&#34892;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#33258;&#36866;&#24212;&#30340;&#20146;&#21644;&#30697;&#38453;&#65292;&#24182;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#19982;&#25289;&#26222;&#25289;&#26031;&#22411;&#31639;&#23376;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#26159;&#29616;&#20195;&#32479;&#35745;&#23398;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#20219;&#21153;&#12290;&#35768;&#22810;&#25968;&#25454;&#38598;&#65288;&#32454;&#32990;&#12289;&#25991;&#26723;&#12289;&#22270;&#20687;&#12289;&#20998;&#23376;&#65289;&#21487;&#20197;&#34987;&#34920;&#31034;&#20026;&#23884;&#20837;&#22312;&#39640;&#32500;&#29615;&#22659;&#31354;&#38388;&#20013;&#30340;&#28857;&#20113;&#65292;&#28982;&#32780;&#25968;&#25454;&#22266;&#26377;&#30340;&#33258;&#30001;&#24230;&#36890;&#24120;&#36828;&#36828;&#23569;&#20110;&#29615;&#22659;&#32500;&#24230;&#30340;&#25968;&#37327;&#12290;&#26816;&#27979;&#25968;&#25454;&#23884;&#20837;&#30340;&#28508;&#22312;&#27969;&#24418;&#26159;&#35768;&#22810;&#19979;&#28216;&#20998;&#26512;&#30340;&#20808;&#20915;&#26465;&#20214;&#12290;&#29616;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#32463;&#24120;&#21463;&#21040;&#22122;&#22768;&#35266;&#27979;&#21644;&#25277;&#26679;&#30340;&#24433;&#21709;&#65292;&#22240;&#27492;&#25552;&#21462;&#20851;&#20110;&#28508;&#22312;&#27969;&#24418;&#30340;&#20449;&#24687;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23545;&#31216;&#29256;&#26412;&#30340;&#26368;&#20248;&#20256;&#36755;&#21644;&#20108;&#27425;&#27491;&#21017;&#21270;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#23427;&#26500;&#24314;&#20102;&#19968;&#20010;&#31232;&#30095;&#33258;&#36866;&#24212;&#30340;&#20146;&#21644;&#30697;&#38453;&#65292;&#21487;&#20197;&#35299;&#37322;&#20026;&#21452;&#38543;&#26426;&#26680;&#24402;&#19968;&#21270;&#30340;&#25512;&#24191;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#36830;&#32493;&#26497;&#38480;&#19979;&#20135;&#29983;&#30340;&#26680;&#19982;&#25289;&#26222;&#25289;&#26031;&#22411;&#31639;&#23376;&#19968;&#33268;&#65292;&#24182;&#24314;&#31435;&#20102;&#35813;&#26041;&#27861;&#30340;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifold learning is a central task in modern statistics and data science. Many datasets (cells, documents, images, molecules) can be represented as point clouds embedded in a high dimensional ambient space, however the degrees of freedom intrinsic to the data are usually far fewer than the number of ambient dimensions. The task of detecting a latent manifold along which the data are embedded is a prerequisite for a wide family of downstream analyses. Real-world datasets are subject to noisy observations and sampling, so that distilling information about the underlying manifold is a major challenge. We propose a method for manifold learning that utilises a symmetric version of optimal transport with a quadratic regularisation that constructs a sparse and adaptive affinity matrix, that can be interpreted as a generalisation of the bistochastic kernel normalisation. We prove that the resulting kernel is consistent with a Laplace-type operator in the continuous limit, establish robustness
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.05772</link><description>&lt;p&gt;
&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Random-Set Convolutional Neural Network (RS-CNN) for Epistemic Deep Learning. (arXiv:2307.05772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05772
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#29992;&#20110;&#20998;&#31867;&#65292;&#36890;&#36807;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20197;&#34920;&#31034;&#27169;&#22411;&#30340;&#32622;&#20449;&#24230;&#21644;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#20272;&#35745;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#65292;&#23545;&#25239;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#38169;&#35823;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#36825;&#31361;&#20986;&#20102;&#23398;&#20064;&#31995;&#32479;&#38656;&#35201;&#33021;&#22815;&#30830;&#23450;&#27169;&#22411;&#23545;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#30340;&#25163;&#27573;&#65292;&#8220;&#30693;&#36947;&#19968;&#20010;&#27169;&#22411;&#19981;&#30693;&#36947;&#8221;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#20998;&#31867;&#30340;&#38543;&#26426;&#38598;&#21512;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;RS-CNN&#65289;&#65292;&#20854;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#32780;&#19981;&#26159;&#27010;&#29575;&#30690;&#37327;&#38598;&#21512;&#65292;&#20351;&#29992;&#38543;&#26426;&#38598;&#21512;&#30340;&#25968;&#23398;&#65292;&#21363;&#23545;&#26679;&#26412;&#31354;&#38388;&#30340;&#24130;&#38598;&#30340;&#20998;&#24067;&#12290;&#22522;&#20110;&#35748;&#35782;&#35770;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#38543;&#26426;&#38598;&#27169;&#22411;&#33021;&#22815;&#34920;&#31034;&#26426;&#22120;&#23398;&#20064;&#20013;&#30001;&#26377;&#38480;&#35757;&#32451;&#38598;&#24341;&#36215;&#30340;&#8220;&#35748;&#35782;&#24615;&#8221;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#36817;&#20284;&#39044;&#27979;&#20449;&#24565;&#20989;&#25968;&#30456;&#20851;&#32852;&#30340;&#32622;&#20449;&#38598;&#30340;&#22823;&#23567;&#26469;&#20272;&#35745;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning is increasingly deployed in safety-critical domains where robustness against adversarial attacks is crucial and erroneous predictions could lead to potentially catastrophic consequences. This highlights the need for learning systems to be equipped with the means to determine a model's confidence in its prediction and the epistemic uncertainty associated with it, 'to know when a model does not know'. In this paper, we propose a novel Random-Set Convolutional Neural Network (RS-CNN) for classification which predicts belief functions rather than probability vectors over the set of classes, using the mathematics of random sets, i.e., distributions over the power set of the sample space. Based on the epistemic deep learning approach, random-set models are capable of representing the 'epistemic' uncertainty induced in machine learning by limited training sets. We estimate epistemic uncertainty by approximating the size of credal sets associated with the predicted belief func
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20844;&#24179;&#39044;&#27979;&#30140;&#30171;&#29366;&#24577;&#65292;&#33268;&#21147;&#20110;&#20943;&#23569;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2307.05333</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#31359;&#25140;&#35774;&#22791;&#21644;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#26080;&#20559;&#35265;&#30340;&#30140;&#30171;&#35780;&#20272;&#65306;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach. (arXiv:2307.05333v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#36890;&#36807;&#32771;&#34385;&#24739;&#32773;&#25968;&#25454;&#20013;&#30340;&#25935;&#24863;&#23646;&#24615;&#65292;&#20844;&#24179;&#39044;&#27979;&#30140;&#30171;&#29366;&#24577;&#65292;&#33268;&#21147;&#20110;&#20943;&#23569;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26679;&#21270;&#30340;&#20581;&#24247;&#25968;&#25454;&#65288;&#29289;&#32852;&#32593;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#21644;&#20020;&#24202;&#35843;&#26597;&#65289;&#19982;&#21487;&#25193;&#23637;&#30340;&#36866;&#24212;&#24615;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#65292;&#24050;&#32463;&#23454;&#29616;&#20102;&#23545;&#30140;&#30171;&#29366;&#24577;&#30340;&#36523;&#20307;&#12289;&#34892;&#20026;&#21644;&#24515;&#29702;&#31038;&#20132;&#25351;&#26631;&#30340;&#21457;&#29616;&#12290;&#23613;&#31649;&#20197;&#25216;&#26415;&#36827;&#27493;&#25913;&#21464;&#21307;&#30103;&#31995;&#32479;&#30340;&#28909;&#24773;&#21644;&#25215;&#35834;&#65292;&#20294;&#20020;&#24202;&#30140;&#30171;&#35780;&#20272;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#21463;&#21040;&#20102;&#38382;&#39064;&#26412;&#36523;&#30340;&#22810;&#26679;&#24615;&#21644;&#20010;&#24615;&#21270;&#20197;&#21450;&#20844;&#24179;&#24615;&#31561;&#20854;&#20182;&#25361;&#25112;&#30340;&#38459;&#30861;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#65288;&#22914;&#26426;&#22120;&#23398;&#20064;&#25110;&#28145;&#24230;&#23398;&#20064;&#65289;&#27169;&#22411;&#26174;&#31034;&#20986;&#20559;&#35265;&#65292;&#24182;&#27495;&#35270;&#29305;&#23450;&#20154;&#32676;&#65288;&#22914;&#22522;&#20110;&#24615;&#21035;&#25110;&#31181;&#26063;&#65289;&#65292;&#36825;&#24341;&#36215;&#20102;&#21307;&#30103;&#19987;&#19994;&#20154;&#21592;&#23545;&#20154;&#24037;&#26234;&#33021;&#36866;&#24212;&#24615;&#30340;&#24576;&#30097;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23646;&#24615;&#20844;&#24179;&#25439;&#22833;&#30340;CNN&#27169;&#22411;&#65292;&#26088;&#22312;&#32771;&#34385;&#25968;&#25454;&#20013;&#21253;&#21547;&#30340;&#20219;&#20309;&#25935;&#24863;&#23646;&#24615;&#65292;&#24182;&#20844;&#24179;&#39044;&#27979;&#24739;&#32773;&#30340;&#30140;&#30171;&#29366;&#24577;&#65292;&#21516;&#26102;&#23613;&#37327;&#20943;&#23569;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The combination of diverse health data (IoT, EHR, and clinical surveys) and scalable-adaptable Artificial Intelligence (AI), has enabled the discovery of physical, behavioral, and psycho-social indicators of pain status. Despite the hype and promise to fundamentally alter the healthcare system with technological advancements, much AI adoption in clinical pain evaluation has been hampered by the heterogeneity of the problem itself and other challenges, such as personalization and fairness. Studies have revealed that many AI (i.e., machine learning or deep learning) models display biases and discriminate against specific population segments (such as those based on gender or ethnicity), which breeds skepticism among medical professionals about AI adaptability. In this paper, we propose a Multi-attribute Fairness Loss (MAFL) based CNN model that aims to account for any sensitive attributes included in the data and fairly predict patients' pain status while attempting to minimize the discre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#34701;&#20837;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09147</link><description>&lt;p&gt;
&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Learning of Multivariate Time Series with Temporal Irregularity. (arXiv:2306.09147v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09147
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20855;&#26377;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#34701;&#20837;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#36341;&#20013;&#25910;&#38598;&#30340;&#22810;&#20803;&#24207;&#21015;&#25968;&#25454;&#32463;&#24120;&#34920;&#29616;&#20986;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#21253;&#25324;&#38750;&#22343;&#21248;&#26102;&#38388;&#38388;&#38548;&#21644;&#32452;&#20214;&#38169;&#20301;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#19981;&#22343;&#21248;&#30340;&#38388;&#36317;&#21644;&#24322;&#27493;&#26159;&#25968;&#25454;&#20869;&#29983;&#29305;&#24449;&#32780;&#19981;&#26159;&#19981;&#36275;&#35266;&#23519;&#30340;&#32467;&#26524;&#65292;&#21017;&#36825;&#20123;&#19981;&#35268;&#21017;&#24615;&#30340;&#20449;&#24687;&#20869;&#23481;&#22312;&#34920;&#24449;&#22810;&#20803;&#20381;&#36182;&#32467;&#26500;&#26102;&#21457;&#25381;&#20915;&#23450;&#24615;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#27010;&#29575;&#39044;&#27979;&#26041;&#27861;&#35201;&#20040;&#24573;&#30053;&#20102;&#30001;&#27492;&#20135;&#29983;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#35201;&#20040;&#26131;&#21463;&#21040;&#25554;&#34917;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#35201;&#20040;&#23558;&#21442;&#25968;&#20551;&#35774;&#24378;&#21152;&#20110;&#25968;&#25454;&#20998;&#24067;&#19978;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#35266;&#23519;&#21040;&#36798;&#26102;&#38388;&#22312;&#27169;&#22411;&#26500;&#24314;&#20013;&#21457;&#25381;&#26680;&#24515;&#20316;&#29992;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#36825;&#26159;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#30340;&#26680;&#24515;&#12290;&#20026;&#20102;&#35748;&#35782;&#21040;&#26102;&#38388;&#30340;&#19981;&#35268;&#21017;&#24615;&#65292;&#25105;&#20204;&#39318;&#20808;&#20026;&#32452;&#20214;&#21551;&#29992;&#21807;&#19968;&#30340;&#38544;&#34255;&#29366;&#24577;&#65292;&#20197;&#20415;&#21040;&#36798;&#26102;&#38388;&#21487;&#20197;&#25351;&#23548;&#20309;&#26102;&#12289;&#22914;&#20309;&#21644;&#21738;&#20010;&#38544;&#34255;&#29366;&#24577;&#24212;&#35813;&#26356;&#26032;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#20808;&#39564;&#27169;&#22411;&#26126;&#30830;&#22320;&#34701;&#20837;&#20102;&#26102;&#38388;&#19981;&#35268;&#21017;&#24615;&#65292;&#35813;&#27169;&#22411;&#32852;&#21512;&#24314;&#27169;&#35266;&#27979;&#21644;&#38544;&#34255;&#29366;&#24577;&#65292;&#24182;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#20102;&#36328;&#32452;&#20214;&#30340;&#26102;&#38388;&#24322;&#36136;&#24615;&#21644;&#26465;&#20214;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate sequential data collected in practice often exhibit temporal irregularities, including nonuniform time intervals and component misalignment. However, if uneven spacing and asynchrony are endogenous characteristics of the data rather than a result of insufficient observation, the information content of these irregularities plays a defining role in characterizing the multivariate dependence structure. Existing approaches for probabilistic forecasting either overlook the resulting statistical heterogeneities, are susceptible to imputation biases, or impose parametric assumptions on the data distribution. This paper proposes an end-to-end solution that overcomes these limitations by allowing the observation arrival times to play the central role of model construction, which is at the core of temporal irregularities. To acknowledge temporal irregularities, we first enable unique hidden states for components so that the arrival times can dictate when, how, and which hidden state
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27714;&#35299;&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#21270;&#21040;&#20102; n^2k&#65292;&#26680;&#24515;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.04169</link><description>&lt;p&gt;
&#39640;&#25928;&#20132;&#26367;&#26368;&#23567;&#21270;&#21450;&#20854;&#22312;&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Efficient Alternating Minimization with Applications to Weighted Low Rank Approximation. (arXiv:2306.04169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#27714;&#35299;&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#38382;&#39064;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#36816;&#34892;&#26102;&#38388;&#20248;&#21270;&#21040;&#20102; n^2k&#65292;&#26680;&#24515;&#26041;&#27861;&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#26435;&#20302;&#31209;&#36924;&#36817;&#26159;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#40065;&#26834;&#30340;&#20132;&#26367;&#26368;&#23567;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#27714;&#35299;&#35813;&#38382;&#39064;&#65292;&#24182;&#23558;&#36816;&#34892;&#26102;&#38388;&#20174; n^2k^2 &#20248;&#21270;&#21040;&#20102; n^2k&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#39640;&#31934;&#24230;&#30340;&#22810;&#21709;&#24212;&#22238;&#24402;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Weighted low rank approximation is a fundamental problem in numerical linear algebra, and it has many applications in machine learning. Given a matrix $M \in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n \times n}$, a parameter $k$, the goal is to output two matrices $U, V \in \mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V) \|_F$ is minimized, where $\circ$ denotes the Hadamard product. Such a problem is known to be NP-hard and even hard to approximate [RSW16]. Meanwhile, alternating minimization is a good heuristic solution for approximating weighted low rank approximation. The work [LLR16] shows that, under mild assumptions, alternating minimization does provide provable guarantees. In this work, we develop an efficient and robust framework for alternating minimization. For weighted low rank approximation, this improves the runtime of [LLR16] from $n^2 k^2$ to $n^2k$. At the heart of our work framework is a high-accuracy multiple response regression
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.18413</link><description>&lt;p&gt;
&#20174;API&#23398;&#20064;&#23398;&#20064;&#65306;&#40657;&#30418;&#25968;&#25454;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#65288;DFML&#65289;&#26088;&#22312;&#36890;&#36807;&#20174;&#19968;&#32452;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20803;&#23398;&#20064;&#32780;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;DFML&#24037;&#20316;&#20165;&#33021;&#20174;&#65288;i&#65289;&#30333;&#30418;&#21644;&#65288;ii&#65289;&#23567;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;iii&#65289;&#30456;&#21516;&#30340;&#26550;&#26500;&#20013;&#20803;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#21363;&#29992;&#25143;&#20165;&#33021;&#36890;&#36807;&#20219;&#24847;&#27169;&#22411;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;API&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25968;&#25454;&#26080;&#20851;&#20803;&#30693;&#35782;&#33976;&#39311;&#65288;BiDf-MKD&#65289;&#26694;&#26550;&#65292;&#23558;&#26356;&#36890;&#29992;&#30340;&#20803;&#30693;&#35782;&#20174;&#19968;&#32452;&#40657;&#30418;API&#36716;&#31227;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;</title><link>http://arxiv.org/abs/2305.15557</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#23398;&#20064;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#29575;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence. (arXiv:2305.15557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#26469;&#35782;&#21035;&#38750;&#32447;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#33539;&#24335;&#20381;&#36182;&#20110;&#29366;&#24577;&#30340;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#30456;&#24212;&#30340;Fokker-Planck&#26041;&#31243;&#30340;&#22522;&#20110;RKHS&#30340;&#36817;&#20284;&#25311;&#21512;&#21040;&#36825;&#20123;&#35266;&#27979;&#20540;&#65292;&#20174;&#32780;&#24471;&#20986;&#29702;&#35770;&#23398;&#20064;&#36895;&#29575;&#30340;&#20272;&#35745;&#20540;&#65292;&#36825;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#24403;&#26410;&#30693;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#36234;&#39640;&#26102;&#65292;&#29702;&#35770;&#20272;&#35745;&#20540;&#36234;&#26469;&#36234;&#32039;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#20869;&#26680;&#30340;&#65292;&#22240;&#27492;&#31163;&#32447;&#39044;&#22788;&#29702;&#21487;&#20197;&#22312;&#21407;&#21017;&#19978;&#24471;&#21040;&#26377;&#25928;&#30340;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel non-parametric learning paradigm for the identification of drift and diffusion coefficients of non-linear stochastic differential equations, which relies upon discrete-time observations of the state. The key idea essentially consists of fitting a RKHS-based approximation of the corresponding Fokker-Planck equation to such observations, yielding theoretical estimates of learning rates which, unlike previous works, become increasingly tighter when the regularity of the unknown drift and diffusion coefficients becomes higher. Our method being kernel-based, offline pre-processing may in principle be profitably leveraged to enable efficient numerical implementation.
&lt;/p&gt;</description></item><item><title>&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11183</link><description>&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning. (arXiv:2303.11183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11183
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#30340;&#30446;&#30340;&#26159;&#20174;&#19968;&#32452;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20854;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#65292;&#24573;&#30053;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34164;&#21547;&#30340;&#20016;&#23500;&#25968;&#25454;&#30693;&#35782;&#65292;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#33021;&#20803;&#23398;&#20064;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#8212;&#8212;PURER&#65292;&#20854;&#20013;&#21253;&#21547;&#65306;&#65288;1&#65289;&#25968;&#25454;&#26080;&#20851;&#30340;&#20803;&#35757;&#32451;&#26399;&#38388;&#30340;&#33410;&#30446;&#35838;&#31243;&#21453;&#36716;&#65288;ECI&#65289;&#65307;&#65288;2&#65289;&#20803;&#27979;&#35797;&#26399;&#38388;&#20869;&#37096;&#24490;&#29615;&#21518;&#30340;&#21453;&#28436;&#26657;&#20934;&#65288;ICFIL&#65289;&#12290;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ECI&#26469;&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#65292;&#20197;&#20415;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ICFIL&#26469;&#26657;&#20934;&#21453;&#28436;&#26799;&#24230;&#65292;&#20197;&#20943;&#23569;&#22522;&#20110;&#21453;&#28436;&#30340;&#20248;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PURER&#21487;&#20197;&#26377;&#25928;&#22320;&#20803;&#23398;&#20064;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#22495;&#29978;&#33267;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes accord
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2303.09280</link><description>&lt;p&gt;
&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#25299;&#25169;&#20248;&#21270;&#65306;&#24212;&#29992;&#20110;&#38544;&#34255;&#20960;&#20309;&#32467;&#26500;&#30340;&#38750;&#20405;&#20837;&#24335;&#25506;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Topology optimization with physics-informed neural networks: application to noninvasive detection of hidden geometries. (arXiv:2303.09280v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09280
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#30340;&#25299;&#25169;&#20248;&#21270;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#26080;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#32467;&#26500;&#26816;&#27979;&#65292;&#36890;&#36807;&#26448;&#26009;&#23494;&#24230;&#22330;&#34920;&#31034;&#20219;&#24847;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;Eikonal&#27491;&#21017;&#21270;&#23454;&#29616;&#12290;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#30340;&#38750;&#20405;&#20837;&#24335;&#25104;&#20687;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#21644;&#24037;&#19994;&#24212;&#29992;&#20013;&#65292;&#36890;&#36807;&#30005;&#30913;&#12289;&#22768;&#23398;&#25110;&#26426;&#26800;&#36127;&#36733;&#20174;&#34920;&#38754;&#27979;&#37327;&#20013;&#26816;&#27979;&#38544;&#34255;&#30340;&#20960;&#20309;&#32467;&#26500;&#26159;&#38750;&#20405;&#20837;&#25104;&#20687;&#25216;&#26415;&#30340;&#30446;&#26631;&#12290;&#30001;&#20110;&#26410;&#30693;&#30340;&#25299;&#25169;&#21644;&#20960;&#20309;&#24418;&#29366;&#12289;&#25968;&#25454;&#30340;&#31232;&#30095;&#24615;&#20197;&#21450;&#29289;&#29702;&#35268;&#24459;&#30340;&#22797;&#26434;&#24615;&#65292;&#35299;&#20915;&#36870;&#38382;&#39064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29289;&#29702;&#30693;&#35782;&#31070;&#32463;&#32593;&#32476;&#24050;&#32463;&#34920;&#29616;&#20986;&#35768;&#22810;&#20248;&#28857;&#65292;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#38382;&#39064;&#21453;&#28436;&#24037;&#20855;&#65292;&#20294;&#23427;&#20204;&#23578;&#26410;&#24212;&#29992;&#20110;&#20855;&#26377;&#20808;&#39564;&#26410;&#30693;&#25299;&#25169;&#30340;&#19968;&#33324;&#38382;&#39064;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;PINNs&#30340;&#25299;&#25169;&#20248;&#21270;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35299;&#20915;&#27809;&#26377;&#24418;&#29366;&#25968;&#37327;&#25110;&#31867;&#22411;&#20808;&#39564;&#30693;&#35782;&#30340;&#20960;&#20309;&#26816;&#27979;&#38382;&#39064;&#12290;&#25105;&#20204;&#20801;&#35768;&#20219;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#25299;&#25169;&#65292;&#36890;&#36807;&#20351;&#29992;&#26448;&#26009;&#23494;&#24230;&#22330;&#26469;&#34920;&#31034;&#20960;&#20309;&#24418;&#29366;&#65292;&#24182;&#36890;&#36807;&#26032;&#30340;Eikonal&#27491;&#21017;&#21270;&#25509;&#36817;&#20108;&#36827;&#21046;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#26816;&#27979;&#38544;&#21547;&#34394;&#31354;&#21644;&#21253;&#21547;&#29289;&#30340;&#25968;&#37327;&#12289;&#20301;&#32622;&#21644;&#24418;&#29366;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Detecting hidden geometrical structures from surface measurements under electromagnetic, acoustic, or mechanical loading is the goal of noninvasive imaging techniques in medical and industrial applications. Solving the inverse problem can be challenging due to the unknown topology and geometry, the sparsity of the data, and the complexity of the physical laws. Physics-informed neural networks (PINNs) have shown promise as a simple-yet-powerful tool for problem inversion, but they have yet to be applied to general problems with a priori unknown topology. Here, we introduce a topology optimization framework based on PINNs that solves geometry detection problems without prior knowledge of the number or types of shapes. We allow for arbitrary solution topology by representing the geometry using a material density field that approaches binary values thanks to a novel eikonal regularization. We validate our framework by detecting the number, locations, and shapes of hidden voids and inclusio
&lt;/p&gt;</description></item><item><title>&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32852;&#21512;&#23398;&#20064;&#20026;&#26412;&#22320;&#23458;&#25143;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#22270;&#20197;&#21450;&#20849;&#35782;&#22270;&#65292;&#20197;&#25512;&#26029;&#28508;&#22312;&#22270;&#25299;&#25169;&#65292;&#21516;&#26102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2301.06662</link><description>&lt;p&gt;
&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#30340;&#22270;&#25299;&#25169;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Topology Learning Under Privacy Constraints. (arXiv:2301.06662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06662
&lt;/p&gt;
&lt;p&gt;
&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#32852;&#21512;&#23398;&#20064;&#20026;&#26412;&#22320;&#23458;&#25143;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#22270;&#20197;&#21450;&#20849;&#35782;&#22270;&#65292;&#20197;&#25512;&#26029;&#28508;&#22312;&#22270;&#25299;&#25169;&#65292;&#21516;&#26102;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#22788;&#29702;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#25968;&#25454;&#20998;&#24067;&#20110;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#19988;&#20855;&#26377;&#38544;&#31169;&#25935;&#24863;&#24615;&#30340;&#26032;&#39062;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#36890;&#36807;&#24179;&#28369;&#22270;&#20449;&#21495;&#25512;&#26029;&#28508;&#22312;&#22270;&#25299;&#25169;&#30340;&#38382;&#39064;&#12290;&#36825;&#20010;&#20219;&#21153;&#30340;&#20027;&#35201;&#22256;&#38590;&#22312;&#20110;&#22914;&#20309;&#22312;&#38544;&#31169;&#32422;&#26463;&#19979;&#21033;&#29992;&#25152;&#26377;&#29420;&#31435;&#23458;&#25143;&#31471;&#30340;&#28508;&#22312;&#24322;&#26500;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#20026;&#26412;&#22320;&#23458;&#25143;&#31471;&#23450;&#21046;&#30340;&#20010;&#24615;&#21270;&#22270;&#20197;&#21450;&#20849;&#35782;&#22270;&#12290;&#20010;&#24615;&#21270;&#22270;&#21305;&#37197;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#65292;&#20174;&#32780;&#20943;&#36731;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#65292;&#32780;&#20849;&#35782;&#22270;&#25429;&#25417;&#20840;&#23616;&#20449;&#24687;&#12290;&#25105;&#20204;&#25509;&#19979;&#26469;&#35774;&#35745;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#24341;&#20837;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#19981;&#36829;&#21453;&#38544;&#31169;&#32422;&#26463;&#65292;&#21363;&#25152;&#26377;&#30340;&#31169;&#26377;&#25968;&#25454;&#37117;&#22312;&#26412;&#22320;&#22788;&#29702;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#22686;&#24378;&#38544;&#31169;&#20445;&#25252;&#65292;&#25105;&#20204;&#23558;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#24341;&#20837;&#21040;&#25152;&#25552;&#31639;&#27861;&#20013;&#65292;&#22312;&#20256;&#36755;&#27169;&#22411;&#26356;&#26032;&#26102;&#25269;&#24481;&#38544;&#31169;&#25915;&#20987;&#12290;&#29702;&#35770;&#19978;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#21487;&#35777;&#26126;&#25910;&#25947;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of inferring the underlying graph topology from smooth graph signals in a novel but practical scenario where data are located in distributed clients and are privacy-sensitive. The main difficulty of this task lies in how to utilize the potentially heterogeneous data of all isolated clients under privacy constraints. Towards this end, we propose a framework where personalized graphs for local clients as well as a consensus graph are jointly learned. The personalized graphs match local data distributions, thereby mitigating data heterogeneity, while the consensus graph captures the global information. We next devise a tailored algorithm to solve the induced problem without violating privacy constraints, i.e., all private data are processed locally. To further enhance privacy protection, we introduce differential privacy (DP) into the proposed algorithm to resist privacy attacks when transmitting model updates. Theoretically, we establish provable convergence analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.01201</link><description>&lt;p&gt;
&#20154;&#31867;&#23545;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Human alignment of neural network representations. (arXiv:2211.01201v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#19982;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#30340;&#23545;&#40784;&#38382;&#39064;&#65292;&#21457;&#29616;&#27169;&#22411;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#23545;&#40784;&#20960;&#20046;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#37117;&#23545;&#23545;&#40784;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#33021;&#26174;&#33879;&#25552;&#39640;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#20154;&#31867;&#30456;&#20284;&#24615;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#20154;&#31867;&#25110;&#25509;&#36817;&#20154;&#31867;&#27700;&#24179;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#21644;&#23398;&#20064;&#31639;&#27861;&#19982;&#23548;&#33268;&#20154;&#31867;&#35270;&#35273;&#30340;&#26041;&#24335;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#20043;&#22788;&#12290;&#26412;&#25991;&#30740;&#31350;&#24433;&#21709;&#31070;&#32463;&#32593;&#32476;&#25152;&#23398;&#20064;&#30340;&#34920;&#31034;&#19982;&#36890;&#36807;&#34892;&#20026;&#21453;&#24212;&#25512;&#26029;&#20986;&#30340;&#20154;&#31867;&#24515;&#29702;&#34920;&#31034;&#20043;&#38388;&#23545;&#40784;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#35268;&#27169;&#21644;&#20307;&#31995;&#32467;&#26500;&#23545;&#19982;&#20154;&#31867;&#34892;&#20026;&#21453;&#24212;&#30340;&#23545;&#40784;&#22522;&#26412;&#19978;&#27809;&#26377;&#24433;&#21709;&#65292;&#32780;&#35757;&#32451;&#25968;&#25454;&#38598;&#21644;&#30446;&#26631;&#20989;&#25968;&#21017;&#20855;&#26377;&#26356;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#22312;&#20351;&#29992;&#20004;&#31181;&#19981;&#21516;&#20219;&#21153;&#25910;&#38598;&#30340;&#19977;&#20010;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#19968;&#33268;&#12290;&#20174;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#21478;&#22806;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#30340;&#20154;&#31867;&#30456;&#20284;&#24230;&#21028;&#26029;&#30340;&#23545;&#40784;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19968;&#20123;&#20154;&#31867;&#27010;&#24565;...
&lt;/p&gt;
&lt;p&gt;
Today's computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concept
&lt;/p&gt;</description></item></channel></rss>