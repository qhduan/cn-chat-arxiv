<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20351;&#29992;&#36716;&#25442;&#22120;&#27169;&#22411;&#36827;&#34892;T&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;&#65292;&#30740;&#31350;&#22810;&#22495;&#32467;&#26500;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#39046;&#22495;&#24863;&#30693;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.12117</link><description>&lt;p&gt;
&#36716;&#31227;&#23398;&#20064;&#29992;&#20110;T&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for T-Cell Response Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12117
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#36716;&#25442;&#22120;&#27169;&#22411;&#36827;&#34892;T&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;&#65292;&#30740;&#31350;&#22810;&#22495;&#32467;&#26500;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#25216;&#26415;&#65292;&#25552;&#20986;&#39046;&#22495;&#24863;&#30693;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#29305;&#23450;&#32473;&#23450;&#32957;&#27573;&#30340;T&#32454;&#32990;&#21709;&#24212;&#39044;&#27979;&#65292;&#36825;&#21487;&#20197;&#26159;&#21521;&#20010;&#24615;&#21270;&#30284;&#30151;&#30123;&#33495;&#21457;&#23637;&#36808;&#20986;&#37325;&#35201;&#19968;&#27493;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12117v1 Announce Type: cross  Abstract: We study the prediction of T-cell response for specific given peptides, which could, among other applications, be a crucial step towards the development of personalized cancer vaccines. It is a challenging task due to limited, heterogeneous training data featuring a multi-domain structure; such data entail the danger of shortcut learning, where models learn general characteristics of peptide sources, such as the source organism, rather than specific peptide characteristics associated with T-cell response.   Using a transformer model for T-cell response prediction, we show that the danger of inflated predictive performance is not merely theoretical but occurs in practice. Consequently, we propose a domain-aware evaluation scheme. We then study different transfer learning techniques to deal with the multi-domain structure and shortcut learning. We demonstrate a per-source fine tuning approach to be effective across a wide range of peptid
&lt;/p&gt;</description></item><item><title>ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09871</link><description>&lt;p&gt;
ThermoHands&#65306;&#19968;&#31181;&#29992;&#20110;&#20174;&#20027;&#35266;&#35270;&#35282;&#28909;&#22270;&#20013;&#20272;&#35745;3D&#25163;&#37096;&#23039;&#21183;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ThermoHands: A Benchmark for 3D Hand Pose Estimation from Egocentric Thermal Image
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09871
&lt;/p&gt;
&lt;p&gt;
ThermoHands&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;ThermoHands&#65292;&#26088;&#22312;&#35299;&#20915;&#28909;&#22270;&#20013;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#25361;&#25112;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#21452;transformer&#27169;&#22359;&#30340;&#23450;&#21046;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#34920;&#26126;&#28909;&#25104;&#20687;&#22312;&#24694;&#21155;&#26465;&#20214;&#19979;&#23454;&#29616;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ThermoHands&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#22522;&#20110;&#28909;&#22270;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#20811;&#26381;&#35832;&#22914;&#20809;&#29031;&#21464;&#21270;&#21644;&#36974;&#25377;&#65288;&#20363;&#22914;&#25163;&#37096;&#31359;&#25140;&#29289;&#65289;&#31561;&#25361;&#25112;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;28&#21517;&#20027;&#20307;&#36827;&#34892;&#25163;-&#29289;&#20307;&#21644;&#25163;-&#34394;&#25311;&#20132;&#20114;&#30340;&#22810;&#26679;&#25968;&#25454;&#38598;&#65292;&#32463;&#36807;&#33258;&#21160;&#21270;&#36807;&#31243;&#20934;&#30830;&#26631;&#27880;&#20102;3D&#25163;&#37096;&#23039;&#21183;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#23450;&#21046;&#30340;&#22522;&#32447;&#26041;&#27861;TheFormer&#65292;&#21033;&#29992;&#21452;transformer&#27169;&#22359;&#22312;&#28909;&#22270;&#20013;&#23454;&#29616;&#26377;&#25928;&#30340;&#20027;&#35266;&#35270;&#35282;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#31361;&#26174;&#20102;TheFormer&#30340;&#39046;&#20808;&#24615;&#33021;&#65292;&#24182;&#30830;&#35748;&#20102;&#28909;&#25104;&#20687;&#22312;&#23454;&#29616;&#24694;&#21155;&#26465;&#20214;&#19979;&#31283;&#20581;&#30340;3D&#25163;&#37096;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09871v1 Announce Type: cross  Abstract: In this work, we present ThermoHands, a new benchmark for thermal image-based egocentric 3D hand pose estimation, aimed at overcoming challenges like varying lighting and obstructions (e.g., handwear). The benchmark includes a diverse dataset from 28 subjects performing hand-object and hand-virtual interactions, accurately annotated with 3D hand poses through an automated process. We introduce a bespoken baseline method, TheFormer, utilizing dual transformer modules for effective egocentric 3D hand pose estimation in thermal imagery. Our experimental results highlight TheFormer's leading performance and affirm thermal imaging's effectiveness in enabling robust 3D hand pose estimation in adverse conditions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;meta-tasks&#20316;&#20026;&#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#21644;&#26032;&#39062;&#20219;&#21153;&#30340;&#27867;&#21270;&#65292;&#36991;&#20813;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#36739;&#20110;&#21407;&#22411;&#32593;&#32476;&#25552;&#39640;&#20102;3.9%&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.18599</link><description>&lt;p&gt;
Meta-Tasks: &#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#21478;&#19968;&#31181;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Meta-Tasks: An alternative view on Meta-Learning Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18599
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;meta-tasks&#20316;&#20026;&#20803;&#23398;&#20064;&#27491;&#21017;&#21270;&#30340;&#35270;&#35282;&#65292;&#23454;&#29616;&#20102;&#23545;&#35757;&#32451;&#21644;&#26032;&#39062;&#20219;&#21153;&#30340;&#27867;&#21270;&#65292;&#36991;&#20813;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#30340;&#22256;&#25200;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#36234;&#65292;&#30456;&#36739;&#20110;&#21407;&#22411;&#32593;&#32476;&#25552;&#39640;&#20102;3.9%&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Few-shot learning (FSL)&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#65292;&#22240;&#20026;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#12290;&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#27867;&#21270;&#21040;&#35757;&#32451;&#21644;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21516;&#26102;&#21033;&#29992;&#26410;&#26631;&#35760;&#26679;&#26412;&#12290;&#35813;&#26041;&#27861;&#22312;&#26356;&#26032;&#22806;&#23618;&#24490;&#29615;&#20043;&#21069;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#25216;&#26415;&#23545;&#23884;&#20837;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#65292;&#23558;&#20854;&#20316;&#20026;&#8220;&#20803;&#20219;&#21153;&#8221;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#26032;&#39062;&#21644;&#35757;&#32451;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#25910;&#25947;&#26356;&#24555;&#12289;&#26356;&#22909;&#65292;&#27867;&#21270;&#35823;&#24046;&#21644;&#26631;&#20934;&#24046;&#26356;&#20302;&#65292;&#34920;&#26126;&#20854;&#22312;FSL&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#28508;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#34920;&#29616;&#27604;&#21407;&#22411;&#32593;&#32476;&#39640;&#20986;3.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18599v1 Announce Type: cross  Abstract: Few-shot learning (FSL) is a challenging machine learning problem due to a scarcity of labeled data. The ability to generalize effectively on both novel and training tasks is a significant barrier to FSL. This paper proposes a novel solution that can generalize to both training and novel tasks while also utilizing unlabeled samples. The method refines the embedding model before updating the outer loop using unsupervised techniques as ``meta-tasks''. The experimental results show that our proposed method performs well on novel and training tasks, with faster and better convergence, lower generalization, and standard deviation error, indicating its potential for practical applications in FSL. The experimental results show that the proposed method outperforms prototypical networks by 3.9%.
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.17073</link><description>&lt;p&gt;
&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Graph Representation Learning Using Hyperdimensional Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17073
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#36827;&#34892;&#21333;&#27425;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#39640;&#32500;&#31354;&#38388;&#24182;&#21033;&#29992;HD&#36816;&#31639;&#31526;&#36827;&#34892;&#20449;&#24687;&#32858;&#21512;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#12289;&#31616;&#21333;&#12289;&#24555;&#36895;&#12289;&#39640;&#25928;&#30340;&#21322;&#30417;&#30563;&#22270;&#23398;&#20064;&#26041;&#27861;&#12290;&#25152;&#25552;&#26041;&#27861;&#21033;&#29992;&#36229;&#39640;&#32500;&#35745;&#31639;&#65292;&#23558;&#25968;&#25454;&#26679;&#26412;&#20351;&#29992;&#38543;&#26426;&#25237;&#24433;&#32534;&#30721;&#21040;&#39640;&#32500;&#31354;&#38388;&#65288;&#31616;&#31216;HD&#31354;&#38388;&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#33410;&#28857;&#34920;&#31034;&#30340;&#21333;&#23556;&#24615;&#36136;&#30340;&#36229;&#39640;&#32500;&#22270;&#23398;&#20064;&#65288;HDGL&#65289;&#31639;&#27861;&#12290;HDGL&#23558;&#33410;&#28857;&#29305;&#24449;&#26144;&#23556;&#21040;HD&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;HD&#36816;&#31639;&#31526;&#65288;&#22914;&#25414;&#32465;&#21644;&#32465;&#23450;&#65289;&#26469;&#32858;&#21512;&#27599;&#20010;&#33410;&#28857;&#30340;&#23616;&#37096;&#37051;&#22495;&#20449;&#24687;&#12290;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;HDGL&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30456;&#31454;&#20105;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#35745;&#31639;&#26114;&#36149;&#30340;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17073v1 Announce Type: cross  Abstract: We present a novel, simple, fast, and efficient approach for semi-supervised learning on graphs. The proposed approach takes advantage of hyper-dimensional computing which encodes data samples using random projections into a high dimensional space (HD space for short). Specifically, we propose a Hyper-dimensional Graph Learning (HDGL) algorithm that leverages the injectivity property of the node representations of a family of graph neural networks. HDGL maps node features to the HD space and then uses HD operators such as bundling and binding to aggregate information from the local neighborhood of each node. Results of experiments with widely used benchmark data sets show that HDGL achieves predictive performance that is competitive with the state-of-the-art deep learning methods, without the need for computationally expensive training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21644;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#20316;&#21697;&#65292;&#24182;&#25351;&#20986;&#33258;2015&#24180;&#20197;&#26469;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#24320;&#22987;&#20559;&#21521;&#20110;&#20351;&#29992;LLRMA&#65292;&#26174;&#31034;&#20854;&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14045</link><description>&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#25512;&#36827;&#20302;&#31209;&#21644;&#23616;&#37096;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65306;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Advancing Low-Rank and Local Low-Rank Matrix Approximation in Medical Imaging: A Systematic Literature Review and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14045
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#32508;&#36848;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21644;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#20316;&#21697;&#65292;&#24182;&#25351;&#20986;&#33258;2015&#24180;&#20197;&#26469;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#24320;&#22987;&#20559;&#21521;&#20110;&#20351;&#29992;LLRMA&#65292;&#26174;&#31034;&#20854;&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#25968;&#25454;&#38598;&#30340;&#22823;&#23481;&#37327;&#21644;&#22797;&#26434;&#24615;&#26159;&#23384;&#20648;&#12289;&#20256;&#36755;&#21644;&#22788;&#29702;&#30340;&#29942;&#39048;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#20302;&#31209;&#30697;&#38453;&#36924;&#36817;&#65288;LRMA&#65289;&#21450;&#20854;&#27966;&#29983;&#29289;&#23616;&#37096;LRMA&#65288;LLRMA&#65289;&#30340;&#24212;&#29992;&#24050;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#23637;&#31034;&#20102;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#24212;&#29992;LRMA&#21644;LLRMA&#30340;&#20316;&#21697;&#12290;&#25991;&#29486;&#30340;&#35814;&#32454;&#20998;&#26512;&#30830;&#35748;&#20102;&#24212;&#29992;&#20110;&#21508;&#31181;&#25104;&#20687;&#27169;&#24577;&#30340;LRMA&#21644;LLRMA&#26041;&#27861;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#29616;&#26377;LRMA&#21644;LLRMA&#26041;&#27861;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#33258;2015&#24180;&#20197;&#26469;&#65292;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26126;&#26174;&#20559;&#21521;&#20110;LLRMA&#65292;&#26174;&#31034;&#20102;&#30456;&#23545;&#20110;LRMA&#22312;&#25429;&#33719;&#21307;&#23398;&#25968;&#25454;&#20013;&#22797;&#26434;&#32467;&#26500;&#26041;&#38754;&#30340;&#28508;&#21147;&#21644;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;LLRMA&#25152;&#20351;&#29992;&#30340;&#27973;&#23618;&#30456;&#20284;&#24615;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#20808;&#36827;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#26469;&#22788;&#29702;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14045v1 Announce Type: cross  Abstract: The large volume and complexity of medical imaging datasets are bottlenecks for storage, transmission, and processing. To tackle these challenges, the application of low-rank matrix approximation (LRMA) and its derivative, local LRMA (LLRMA) has demonstrated potential.   This paper conducts a systematic literature review to showcase works applying LRMA and LLRMA in medical imaging. A detailed analysis of the literature identifies LRMA and LLRMA methods applied to various imaging modalities. This paper addresses the challenges and limitations associated with existing LRMA and LLRMA methods.   We note a significant shift towards a preference for LLRMA in the medical imaging field since 2015, demonstrating its potential and effectiveness in capturing complex structures in medical data compared to LRMA. Acknowledging the limitations of shallow similarity methods used with LLRMA, we suggest advanced semantic image segmentation for similarit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;</title><link>https://arxiv.org/abs/2402.12365</link><description>&lt;p&gt;
&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Universal Physics Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12365
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#36825;&#19968;&#26032;&#39062;&#23398;&#20064;&#33539;&#24335;&#65292;&#33021;&#22815;&#27169;&#25311;&#24191;&#27867;&#30340;&#26102;&#31354;&#38382;&#39064;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#65292;&#26377;&#25928;&#22320;&#20256;&#25773;&#21160;&#24577;&#24182;&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#26367;&#20195;&#32773;&#36817;&#26469;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#31867;&#20284;&#20110;&#23427;&#20204;&#30340;&#25968;&#20540;&#23545;&#24212;&#29289;&#65292;&#22312;&#19981;&#21516;&#24212;&#29992;&#20013;&#20351;&#29992;&#19981;&#21516;&#30340;&#25216;&#26415;&#65292;&#21363;&#20351;&#31995;&#32479;&#30340;&#22522;&#30784;&#21160;&#24577;&#30456;&#20284;&#12290;&#19968;&#20010;&#33879;&#21517;&#30340;&#20363;&#23376;&#26159;&#22312;&#35745;&#31639;&#27969;&#20307;&#21160;&#21147;&#23398;&#20013;&#30340;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#34920;&#36848;&#65292;&#36825;&#20026;&#31070;&#32463;&#32593;&#32476;&#26377;&#25928;&#22320;&#24314;&#27169;&#22522;&#20110;&#31890;&#23376;&#32780;&#19981;&#26159;&#32593;&#26684;&#30340;&#21160;&#24577;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36890;&#29992;&#29289;&#29702;&#21464;&#21387;&#22120;&#65288;UPTs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#27169;&#25311;&#20102;&#19968;&#31995;&#21015;&#26102;&#31354;&#38382;&#39064; - &#23545;&#25289;&#26684;&#26391;&#26085;&#21644;&#27431;&#25289;&#31163;&#25955;&#21270;&#26041;&#26696;&#12290;UPTs&#22312;&#27809;&#26377;&#22522;&#20110;&#32593;&#26684;&#25110;&#22522;&#20110;&#31890;&#23376;&#30340;&#28508;&#22312;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#36816;&#34892;&#65292;&#20174;&#32780;&#22312;&#32593;&#26684;&#21644;&#31890;&#23376;&#20043;&#38388;&#23454;&#29616;&#20102;&#28789;&#27963;&#24615;&#12290;UPTs&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#39640;&#25928;&#20256;&#25773;&#21160;&#24577;&#65292;&#24378;&#35843;&#20102;&#36870;&#32534;&#30721;&#21644;&#35299;&#30721;&#25216;&#26415;&#12290;&#26368;&#21518;&#65292;UPTs&#20801;&#35768;&#26597;&#35810;&#28508;&#22312;&#31354;&#38388;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12365v1 Announce Type: cross  Abstract: Deep neural network based surrogates for partial differential equations have recently gained increased interest. However, akin to their numerical counterparts, different techniques are used across applications, even if the underlying dynamics of the systems are similar. A prominent example is the Lagrangian and Eulerian specification in computational fluid dynamics, posing a challenge for neural networks to effectively model particle- as opposed to grid-based dynamics. We introduce Universal Physics Transformers (UPTs), a novel learning paradigm which models a wide range of spatio-temporal problems - both for Lagrangian and Eulerian discretization schemes. UPTs operate without grid- or particle-based latent structures, enabling flexibility across meshes and particles. UPTs efficiently propagate dynamics in the latent space, emphasized by inverse encoding and decoding techniques. Finally, UPTs allow for queries of the latent space repre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2402.05966</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#21644;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rethink Model Re-Basin and the Linear Mode Connectivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#29616;&#35937;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#19981;&#36275;&#12290;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#65292;&#25105;&#20204;&#25913;&#36827;&#20102;&#21305;&#37197;&#31639;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#23427;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#20026;&#21098;&#26525;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35299;&#65292;&#25512;&#21160;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23545;&#20110;&#36275;&#22815;&#23485;&#30340;&#27169;&#22411;&#26469;&#35828;&#65292;&#22823;&#37096;&#20998;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#35299;&#21487;&#20197;&#25910;&#25947;&#21040;&#30456;&#21516;&#30340;&#22522;&#24213;&#65292;&#21482;&#26159;&#39034;&#24207;&#21487;&#33021;&#19981;&#21516;&#12290;&#36825;&#31181;&#29616;&#35937;&#34987;&#31216;&#20026;&#27169;&#22411;&#37325;&#26032;&#22522;&#24213;&#30340;&#38454;&#27573;&#65292;&#23545;&#20110;&#27169;&#22411;&#24179;&#22343;&#21270;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#37325;&#26032;&#22522;&#24213;&#31574;&#30053;&#22312;&#25928;&#26524;&#19978;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23545;&#24213;&#23618;&#26426;&#21046;&#30340;&#29702;&#35299;&#19981;&#22815;&#20840;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#26032;&#23457;&#35270;&#20102;&#26631;&#20934;&#20570;&#27861;&#65292;&#24182;&#25581;&#31034;&#20102;&#29616;&#26377;&#21305;&#37197;&#31639;&#27861;&#30340;&#39057;&#32321;&#19981;&#36275;&#20043;&#22788;&#65292;&#25105;&#20204;&#36890;&#36807;&#36866;&#24403;&#30340;&#37325;&#24402;&#19968;&#21270;&#26469;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#12290;&#36890;&#36807;&#24341;&#20837;&#26356;&#30452;&#25509;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21305;&#37197;&#31639;&#27861;&#19982;&#37325;&#24402;&#19968;&#21270;&#36807;&#31243;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#36825;&#31181;&#35266;&#28857;&#19981;&#20165;&#28548;&#28165;&#21644;&#25913;&#36827;&#20102;&#20197;&#21069;&#30340;&#30740;&#31350;&#32467;&#26524;&#65292;&#36824;&#20419;&#36827;&#20102;&#26032;&#30340;&#27934;&#35265;&#12290;&#20363;&#22914;&#65292;&#23427;&#23558;&#32447;&#24615;&#27169;&#24577;&#36830;&#25509;&#24615;&#19982;&#21098;&#26525;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#28608;&#21457;&#20102;&#19968;&#31181;&#36731;&#37327;&#19988;&#26377;&#25928;&#30340;&#21518;&#21098;&#26525;&#25554;&#20214;&#65292;&#21487;&#20197;&#30452;&#25509;&#19982;&#20219;&#20309;&#29616;&#26377;&#30340;&#21098;&#26525;&#25216;&#26415;&#21512;&#24182;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies suggest that with sufficiently wide models, most SGD solutions can, up to permutation, converge into the same basin. This phenomenon, known as the model re-basin regime, has significant implications for model averaging. However, current re-basin strategies are limited in effectiveness due to a lack of comprehensive understanding of underlying mechanisms. Addressing this gap, our work revisits standard practices and uncovers the frequent inadequacies of existing matching algorithms, which we show can be mitigated through proper re-normalization. By introducing a more direct analytical approach, we expose the interaction between matching algorithms and re-normalization processes. This perspective not only clarifies and refines previous findings but also facilitates novel insights. For instance, it connects the linear mode connectivity to pruning, motivating a lightweight yet effective post-pruning plug-in that can be directly merged with any existing pruning techniques. Ou
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;ER SDE&#65289;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#31934;&#30830;&#35299;&#21644;&#39640;&#38454;&#36817;&#20284;&#35299;&#65292;&#24182;&#35299;&#37322;&#20102;&#22312;&#24555;&#36895;&#37319;&#26679;&#26041;&#38754;ODE&#27714;&#35299;&#22120;&#20248;&#20110;SDE&#27714;&#35299;&#22120;&#30340;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06169</link><description>&lt;p&gt;
&#38416;&#26126;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#35299;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Elucidating the solution space of extended reverse-time SDE for diffusion models. (arXiv:2309.06169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06169
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;ER SDE&#65289;&#29992;&#20110;&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#37319;&#26679;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#31934;&#30830;&#35299;&#21644;&#39640;&#38454;&#36817;&#20284;&#35299;&#65292;&#24182;&#35299;&#37322;&#20102;&#22312;&#24555;&#36895;&#37319;&#26679;&#26041;&#38754;ODE&#27714;&#35299;&#22120;&#20248;&#20110;SDE&#27714;&#35299;&#22120;&#30340;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#21508;&#31181;&#29983;&#25104;&#24314;&#27169;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#24378;&#22823;&#30340;&#22270;&#20687;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#37319;&#26679;&#36895;&#24230;&#36739;&#24930;&#65292;&#38656;&#35201;&#36890;&#36807;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#30334;&#25110;&#25968;&#21315;&#27425;&#36830;&#32493;&#20989;&#25968;&#35780;&#20272;&#25165;&#33021;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#12290;&#20174;&#25193;&#25955;&#27169;&#22411;&#20013;&#37319;&#26679;&#21487;&#20197;&#30475;&#20316;&#26159;&#35299;&#30456;&#24212;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#25110;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;&#37319;&#26679;&#36807;&#31243;&#24418;&#24335;&#21270;&#20026;&#25193;&#23637;&#21453;&#21521;&#26102;&#38388; SDE&#65288;ER SDE&#65289;&#65292;&#23558;&#20043;&#21069;&#23545;ODE&#21644;SDE&#30340;&#25506;&#32034;&#32479;&#19968;&#36215;&#26469;&#12290;&#21033;&#29992;ER SDE&#35299;&#30340;&#21322;&#32447;&#24615;&#32467;&#26500;&#65292;&#25105;&#20204;&#20026;VP SDE&#25552;&#20379;&#20102;&#31934;&#30830;&#35299;&#21644;&#20219;&#24847;&#39640;&#38454;&#36817;&#20284;&#35299;&#65292;&#20026;VE SDE&#25552;&#20379;&#20102;&#39640;&#38454;&#36817;&#20284;&#35299;&#12290;&#22522;&#20110;ER SDE&#30340;&#35299;&#31354;&#38388;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;ODE&#27714;&#35299;&#22120;&#22312;&#24555;&#36895;&#37319;&#26679;&#26041;&#38754;&#20248;&#20110;SDE&#27714;&#35299;&#22120;&#30340;&#25968;&#23398;&#27934;&#23519;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25581;&#31034;&#20102;VP SDE&#27714;&#35299;&#22120;&#19982;&#20854;VE SDE&#27714;&#35299;&#22120;&#22312;&#24615;&#33021;&#19978;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models (DMs) demonstrate potent image generation capabilities in various generative modeling tasks. Nevertheless, their primary limitation lies in slow sampling speed, requiring hundreds or thousands of sequential function evaluations through large neural networks to generate high-quality images. Sampling from DMs can be seen as solving corresponding stochastic differential equations (SDEs) or ordinary differential equations (ODEs). In this work, we formulate the sampling process as an extended reverse-time SDE (ER SDE), unifying prior explorations into ODEs and SDEs. Leveraging the semi-linear structure of ER SDE solutions, we offer exact solutions and arbitrarily high-order approximate solutions for VP SDE and VE SDE, respectively. Based on the solution space of the ER SDE, we yield mathematical insights elucidating the superior performance of ODE solvers over SDE solvers in terms of fast sampling. Additionally, we unveil that VP SDE solvers stand on par with their VE SDE c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25237;&#24433;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#23558;&#30697;&#38453;&#25237;&#24433;&#21040; $\ell_{1,\infty}$ &#29699;&#38754;&#12290;&#35813;&#31639;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#12290;&#21516;&#26102;&#65292;&#23558;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#20013;&#21487;&#20197;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#21644;&#26435;&#37325;&#30340;&#31232;&#30095;&#21270;&#12290;</title><link>http://arxiv.org/abs/2307.09836</link><description>&lt;p&gt;
&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#25237;&#24433;&#21040; $\ell_{1,\infty}$ &#29699;&#38754;&#65307;&#31232;&#30095;&#33258;&#32534;&#30721;&#22120;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Near-Linear Time Projection onto the $\ell_{1,\infty}$ Ball; Application to Sparse Autoencoders. (arXiv:2307.09836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25237;&#24433;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#20960;&#20046;&#32447;&#24615;&#26102;&#38388;&#20869;&#23558;&#30697;&#38453;&#25237;&#24433;&#21040; $\ell_{1,\infty}$ &#29699;&#38754;&#12290;&#35813;&#31639;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#12290;&#21516;&#26102;&#65292;&#23558;&#35813;&#31639;&#27861;&#24212;&#29992;&#20110;&#33258;&#32534;&#30721;&#22120;&#35757;&#32451;&#20013;&#21487;&#20197;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#21644;&#26435;&#37325;&#30340;&#31232;&#30095;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#22312;&#23547;&#25214;&#31232;&#30095;&#24615;&#23545;&#20110;&#21152;&#36895;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#33267;&#20851;&#37325;&#35201;&#12290;&#25237;&#24433;&#21040; $\ell_{1,2}$ &#21644; $\ell_{1,\infty}$ &#26159;&#31232;&#30095;&#21270;&#21644;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#25972;&#20307;&#25104;&#26412;&#30340;&#26368;&#39640;&#25928;&#25216;&#26415;&#20043;&#19968;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340; $\ell_{1,\infty}$ &#33539;&#25968;&#29699;&#38754;&#30340;&#25237;&#24433;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#30340;&#26368;&#22351;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026; $\mathcal{O}\big(nm+J\log(nm)\big)$&#65292;&#20854;&#20013;&#30697;&#38453;&#20026; $\mathbb{R}^{n\times m}$&#12290;$J$ &#26159;&#19968;&#20010;&#22312;&#31232;&#30095;&#24615;&#39640;&#26102;&#36235;&#36817;&#20110;0&#65292;&#22312;&#31232;&#30095;&#24615;&#20302;&#26102;&#36235;&#36817;&#20110; $nm$ &#30340;&#39033;&#12290;&#35813;&#31639;&#27861;&#26131;&#20110;&#23454;&#29616;&#65292;&#24182;&#20445;&#35777;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#25910;&#25947;&#21040;&#31934;&#30830;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#35757;&#32451;&#33258;&#32534;&#30721;&#22120;&#26102;&#23558; $\ell_{1,\infty}$ &#29699;&#38754;&#25237;&#24433;&#32435;&#20837;&#20854;&#20013;&#65292;&#20197;&#24378;&#21046;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#26435;&#37325;&#30340;&#31232;&#30095;&#21270;&#12290;&#22312;&#25105;&#20204;&#30340;&#29983;&#29289;&#23398;&#24212;&#29992;&#20013;&#65292;&#31232;&#30095;&#21270;&#20027;&#35201;&#20986;&#29616;&#22312;&#32534;&#30721;&#22120;&#20013;&#65292;&#20197;&#23454;&#29616;&#29305;&#24449;&#36873;&#25321;&#65292;&#22240;&#20026;&#21482;&#26377;&#38750;&#24120;&#23567;&#30340;&#19968;&#37096;&#20998;&#25968;&#25454;&#65288;&lt;2%&#65289;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Looking for sparsity is nowadays crucial to speed up the training of large-scale neural networks. Projections onto the $\ell_{1,2}$ and $\ell_{1,\infty}$ are among the most efficient techniques to sparsify and reduce the overall cost of neural networks. In this paper, we introduce a new projection algorithm for the $\ell_{1,\infty}$ norm ball. The worst-case time complexity of this algorithm is $\mathcal{O}\big(nm+J\log(nm)\big)$ for a matrix in $\mathbb{R}^{n\times m}$. $J$ is a term that tends to 0 when the sparsity is high, and to $nm$ when the sparsity is low. Its implementation is easy and it is guaranteed to converge to the exact solution in a finite time. Moreover, we propose to incorporate the $\ell_{1,\infty}$ ball projection while training an autoencoder to enforce feature selection and sparsity of the weights. Sparsification appears in the encoder to primarily do feature selection due to our application in biology, where only a very small part ($&lt;2\%$) of the data is relevan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20511;&#37492;&#20102;&#22810;&#20010;&#25991;&#29486;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38236;&#20687;&#19979;&#38477;&#21644;&#20849;&#36717;&#26799;&#24230;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35745;&#31639;Wasserstein&#36317;&#31163;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#27604;&#20854;&#20182;&#31639;&#27861;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.08507</link><description>&lt;p&gt;
&#20511;&#37492;&#29109;&#26368;&#20248;&#36755;&#36816;&#12289;&#38236;&#20687;&#19979;&#38477;&#21644;&#20849;&#36717;&#26799;&#24230;&#30340;&#25991;&#29486;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#20934;&#30830;&#30340;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient and Accurate Optimal Transport with Mirror Descent and Conjugate Gradients. (arXiv:2307.08507v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.08507
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#20511;&#37492;&#20102;&#22810;&#20010;&#25991;&#29486;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38236;&#20687;&#19979;&#38477;&#21644;&#20849;&#36717;&#26799;&#24230;&#30340;&#25216;&#26415;&#65292;&#33021;&#22815;&#39640;&#25928;&#20934;&#30830;&#22320;&#35745;&#31639;Wasserstein&#36317;&#31163;&#65292;&#24182;&#19988;&#22312;&#39640;&#32500;&#38382;&#39064;&#19978;&#27604;&#20854;&#20182;&#31639;&#27861;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26368;&#20248;&#36755;&#36816;&#31639;&#27861;&#65292;&#36890;&#36807;&#20511;&#37492;&#29109;&#26368;&#20248;&#36755;&#36816;&#12289;&#38236;&#20687;&#19979;&#38477;&#21644;&#20849;&#36717;&#26799;&#24230;&#30340;&#25991;&#29486;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#25193;&#23637;&#19988;&#21487;&#22312;GPU&#19978;&#24182;&#34892;&#35745;&#31639;&#65292;&#33021;&#22815;&#20197;&#26497;&#39640;&#30340;&#31934;&#24230;&#35745;&#31639;Wasserstein&#36317;&#31163;&#65292;&#20351;&#30456;&#23545;&#35823;&#24046;&#36798;&#21040;$10^{-8}$&#65292;&#24182;&#19988;&#27809;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#35777;&#19978;&#65292;&#19982;&#21253;&#25324;&#23545;&#25968;&#22495;&#31283;&#23450;Sinkhorn&#31639;&#27861;&#22312;&#20869;&#30340;&#22810;&#31181;&#31639;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#36798;&#21040;&#39640;&#31934;&#24230;&#35299;&#65292;&#20855;&#26377;&#26356;&#30701;&#30340;&#22681;&#38047;&#26102;&#38388;&#12290;&#25105;&#20204;&#35814;&#32454;&#22320;&#20998;&#26512;&#20102;&#31639;&#27861;&#21644;&#38382;&#39064;&#21442;&#25968;&#65292;&#24182;&#22312;MNIST&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#19982;&#21508;&#31181;&#26368;&#26032;&#30340;&#39640;&#32500;&#38382;&#39064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#31639;&#27861;&#21487;&#20197;&#25104;&#20026;&#20174;&#19994;&#20154;&#21592;&#26368;&#20248;&#36755;&#36816;&#24037;&#20855;&#21253;&#20013;&#26377;&#29992;&#30340;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
We design a novel algorithm for optimal transport by drawing from the entropic optimal transport, mirror descent and conjugate gradients literatures. Our scalable and GPU parallelizable algorithm is able to compute the Wasserstein distance with extreme precision, reaching relative error rates of $10^{-8}$ without numerical stability issues. Empirically, the algorithm converges to high precision solutions more quickly in terms of wall-clock time than a variety of algorithms including log-domain stabilized Sinkhorn's Algorithm. We provide careful ablations with respect to algorithm and problem parameters, and present benchmarking over upsampled MNIST images, comparing to various recent algorithms over high-dimensional problems. The results suggest that our algorithm can be a useful addition to the practitioner's optimal transport toolkit.
&lt;/p&gt;</description></item><item><title>FedICT&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20010;&#24615;&#21270;&#26381;&#21153;&#21644;&#24322;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00389</link><description>&lt;p&gt;
FedICT:&#29992;&#20110;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
FedICT: Federated Multi-task Distillation for Multi-access Edge Computing. (arXiv:2301.00389v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00389
&lt;/p&gt;
&lt;p&gt;
FedICT&#26159;&#19968;&#31181;&#29992;&#20110;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#20010;&#24615;&#21270;&#26381;&#21153;&#21644;&#24322;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#36890;&#20449;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#31227;&#21160;&#35774;&#22791;&#26234;&#33021;&#26381;&#21153;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#32852;&#37030;&#23398;&#20064;&#22312;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#65288;MEC&#65289;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;&#22810;&#26679;&#30340;&#29992;&#25143;&#34892;&#20026;&#35201;&#27714;&#22312;&#19981;&#21516;&#35774;&#22791;&#19978;&#20351;&#29992;&#20010;&#24615;&#21270;&#26381;&#21153;&#21644;&#24322;&#26500;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#25552;&#20986;&#20102;&#32852;&#37030;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;FMTL&#65289;&#26469;&#20026;&#19981;&#21516;&#35774;&#22791;&#35757;&#32451;&#30456;&#20851;&#20294;&#20010;&#24615;&#21270;&#30340;ML&#27169;&#22411;&#65292;&#28982;&#32780;&#20043;&#21069;&#30340;&#24037;&#20316;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#23384;&#22312;&#36807;&#22810;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#24573;&#35270;&#20102;MEC&#20013;&#35774;&#22791;&#20043;&#38388;&#30340;&#27169;&#22411;&#24322;&#26500;&#24615;&#12290;&#23558;&#30693;&#35782;&#33976;&#39311;&#24341;&#20837;FMTL&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#39640;&#25928;&#30340;&#36890;&#20449;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#27169;&#22411;&#24322;&#26500;&#24615;&#65292;&#32780;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#36825;&#22312;&#23454;&#38469;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22256;&#22659;&#65292;&#25552;&#20986;&#20102;&#29992;&#20110;&#22810;&#25509;&#20837;&#36793;&#32536;&#35745;&#31639;&#30340;&#32852;&#37030;&#22810;&#20219;&#21153;&#33976;&#39311;&#65288;FedICT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing interest in intelligent services and privacy protection for mobile devices has given rise to the widespread application of federated learning in Multi-access Edge Computing (MEC). Diverse user behaviors call for personalized services with heterogeneous Machine Learning (ML) models on different devices. Federated Multi-task Learning (FMTL) is proposed to train related but personalized ML models for different devices, whereas previous works suffer from excessive communication overhead during training and neglect the model heterogeneity among devices in MEC. Introducing knowledge distillation into FMTL can simultaneously enable efficient communication and model heterogeneity among clients, whereas existing methods rely on a public dataset, which is impractical in reality. To tackle this dilemma, Federated MultI-task Distillation for Multi-access Edge CompuTing (FedICT) is proposed. FedICT direct local-global knowledge aloof during bi-directional distillation processes between 
&lt;/p&gt;</description></item></channel></rss>