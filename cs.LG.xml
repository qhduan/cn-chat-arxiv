<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>Serpent&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#24674;&#22797;&#26550;&#26500;&#65292;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#20840;&#23616;&#24863;&#21463;&#37326;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#20294;&#35745;&#31639;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2403.17902</link><description>&lt;p&gt;
Serpent&#65306;&#36890;&#36807;&#22810;&#23610;&#24230;&#32467;&#26500;&#21270;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#23454;&#29616;&#21487;&#25193;&#23637;&#39640;&#25928;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Serpent: Scalable and Efficient Image Restoration via Multi-scale Structured State Space Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17902
&lt;/p&gt;
&lt;p&gt;
Serpent&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#24674;&#22797;&#26550;&#26500;&#65292;&#21033;&#29992;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#22312;&#20840;&#23616;&#24863;&#21463;&#37326;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#20294;&#35745;&#31639;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22270;&#20687;&#24674;&#22797;&#26550;&#26500;&#30340;&#35745;&#31639;&#24314;&#31569;&#22359;&#39046;&#22495;&#65292;&#20027;&#35201;&#30001;&#21367;&#31215;&#22788;&#29702;&#21644;&#21508;&#31181;&#27880;&#24847;&#26426;&#21046;&#30340;&#32452;&#21512;&#25152;&#20027;&#23548;&#12290;&#28982;&#32780;&#65292;&#21367;&#31215;&#28388;&#27874;&#22120;&#26412;&#36136;&#19978;&#26159;&#23616;&#37096;&#30340;&#65292;&#22240;&#27492;&#22312;&#24314;&#27169;&#22270;&#20687;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#24615;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#27880;&#24847;&#26426;&#21046;&#25797;&#38271;&#25429;&#33719;&#20219;&#24847;&#22270;&#20687;&#21306;&#22495;&#20043;&#38388;&#30340;&#20840;&#23616;&#30456;&#20114;&#20316;&#29992;&#65292;&#20294;&#23545;&#22270;&#20687;&#23610;&#23544;&#30340;&#20108;&#27425;&#25104;&#26412;&#36739;&#39640;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Serpent&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#26368;&#36817;&#22312;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#26041;&#38754;&#30340;&#36827;&#23637;&#20316;&#20026;&#20854;&#26680;&#24515;&#35745;&#31639;&#27169;&#22359;&#30340;&#26550;&#26500;&#12290;SSMs&#26368;&#21021;&#29992;&#20110;&#24207;&#21015;&#24314;&#27169;&#65292;&#21487;&#20197;&#36890;&#36807;&#26377;&#21033;&#30340;&#36755;&#20837;&#23610;&#23544;&#30340;&#32447;&#24615;&#32553;&#25918;&#26469;&#32500;&#25345;&#20840;&#23616;&#24863;&#21463;&#37326;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;Serpent&#21487;&#20197;&#23454;&#29616;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#24403;&#30340;&#37325;&#24314;&#36136;&#37327;&#65292;&#21516;&#26102;&#38656;&#35201;&#25968;&#37327;&#32423;&#30340;&#35745;&#31639;&#37327;&#36739;&#23569;&#65288;&#22312;FLOPS&#19978;&#39640;&#36798;150&#20493;&#30340;&#20943;&#23569;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17902v1 Announce Type: cross  Abstract: The landscape of computational building blocks of efficient image restoration architectures is dominated by a combination of convolutional processing and various attention mechanisms. However, convolutional filters are inherently local and therefore struggle at modeling long-range dependencies in images. On the other hand, attention excels at capturing global interactions between arbitrary image regions, however at a quadratic cost in image dimension. In this work, we propose Serpent, an architecture that leverages recent advances in state space models (SSMs) in its core computational block. SSMs, originally introduced for sequence modeling, can maintain a global receptive field with a favorable linear scaling in input size. Our preliminary results demonstrate that Serpent can achieve reconstruction quality on par with state-of-the-art techniques, while requiring orders of magnitude less compute (up to $150$ fold reduction in FLOPS) an
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26159;&#20851;&#20110;&#22312;&#23391;&#21152;&#25289;&#22269;&#32972;&#26223;&#19979;&#23545;&#30007;&#24615;&#23478;&#24237;&#26292;&#21147;&#36827;&#34892;&#24320;&#21019;&#24615;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#30007;&#24615;&#21463;&#23475;&#32773;&#30340;&#23384;&#22312;&#12289;&#27169;&#24335;&#21644;&#28508;&#22312;&#22240;&#32032;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#23545;&#30007;&#24615;&#21463;&#23475;&#32773;&#30740;&#31350;&#31354;&#30333;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15594</link><description>&lt;p&gt;
&#36890;&#36807;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#21644;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#27934;&#35265;&#20998;&#26512;&#30007;&#24615;&#23478;&#24237;&#26292;&#21147;
&lt;/p&gt;
&lt;p&gt;
Analyzing Male Domestic Violence through Exploratory Data Analysis and Explainable Machine Learning Insights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15594
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26159;&#20851;&#20110;&#22312;&#23391;&#21152;&#25289;&#22269;&#32972;&#26223;&#19979;&#23545;&#30007;&#24615;&#23478;&#24237;&#26292;&#21147;&#36827;&#34892;&#24320;&#21019;&#24615;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#30007;&#24615;&#21463;&#23475;&#32773;&#30340;&#23384;&#22312;&#12289;&#27169;&#24335;&#21644;&#28508;&#22312;&#22240;&#32032;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#25991;&#29486;&#23545;&#30007;&#24615;&#21463;&#23475;&#32773;&#30740;&#31350;&#31354;&#30333;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23478;&#24237;&#26292;&#21147;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#20010;&#20851;&#20110;&#22899;&#24615;&#21463;&#23475;&#32773;&#30340;&#24615;&#21035;&#38382;&#39064;&#65292;&#22312;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#23613;&#31649;&#26377;&#36825;&#31181;&#20851;&#27880;&#65292;&#23391;&#21152;&#25289;&#22269;&#29305;&#21035;&#26159;&#30007;&#24615;&#21463;&#23475;&#32773;&#20173;&#28982;&#20027;&#35201;&#34987;&#24573;&#35270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20195;&#34920;&#20102;&#22312;&#23391;&#21152;&#25289;&#22269;&#32972;&#26223;&#19979;&#23545;&#30007;&#24615;&#23478;&#24237;&#26292;&#21147;&#65288;MDV&#65289;&#36825;&#19968;&#26410;&#34987;&#20805;&#20998;&#25506;&#35752;&#39046;&#22495;&#30340;&#24320;&#21019;&#24615;&#25506;&#32034;&#65292;&#25581;&#31034;&#20102;&#20854;&#26222;&#36941;&#24615;&#12289;&#27169;&#24335;&#21644;&#28508;&#22312;&#22240;&#32032;&#12290;&#29616;&#26377;&#25991;&#29486;&#20027;&#35201;&#24378;&#35843;&#23478;&#24237;&#26292;&#21147;&#24773;&#22659;&#20013;&#22899;&#24615;&#30340;&#21463;&#23475;&#65292;&#23548;&#33268;&#23545;&#30007;&#24615;&#21463;&#23475;&#32773;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#20174;&#23391;&#21152;&#25289;&#22269;&#20027;&#35201;&#22478;&#24066;&#25910;&#38598;&#20102;&#25968;&#25454;&#65292;&#24182;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#20197;&#20102;&#35299;&#28508;&#22312;&#21160;&#24577;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;11&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65288;&#21253;&#25324;&#40664;&#35748;&#21644;&#20248;&#21270;&#30340;&#36229;&#21442;&#25968;&#65289;&#12289;2&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;4&#31181;&#38598;&#25104;&#27169;&#22411;&#12290;&#23613;&#31649;&#37319;&#29992;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;CatBoost&#30001;&#20110;&#20854;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15594v1 Announce Type: cross  Abstract: Domestic violence, which is often perceived as a gendered issue among female victims, has gained increasing attention in recent years. Despite this focus, male victims of domestic abuse remain primarily overlooked, particularly in Bangladesh. Our study represents a pioneering exploration of the underexplored realm of male domestic violence (MDV) within the Bangladeshi context, shedding light on its prevalence, patterns, and underlying factors. Existing literature predominantly emphasizes female victimization in domestic violence scenarios, leading to an absence of research on male victims. We collected data from the major cities of Bangladesh and conducted exploratory data analysis to understand the underlying dynamics. We implemented 11 traditional machine learning models with default and optimized hyperparameters, 2 deep learning, and 4 ensemble models. Despite various approaches, CatBoost has emerged as the top performer due to its 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.15048</link><description>&lt;p&gt;
&#21345;&#36890;&#24187;&#35273;&#26816;&#27979;: &#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#25104;&#39046;&#22495;&#20013;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#24187;&#35273;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39118;&#26684;&#22914;&#21345;&#36890;&#20154;&#29289;&#20013;&#21253;&#21547;&#20102;&#24863;&#30693;&#19978;&#20851;&#38190;&#30340;&#32570;&#38519;&#65292;&#20381;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#26816;&#27979;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#30340;&#35270;&#35273;&#24187;&#35273;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#65288;PA-ICVL&#65289;&#19982;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#21516;&#26102;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#12290;&#36890;&#36807;&#20174;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#23039;&#21183;&#20272;&#35745;&#22120;&#20013;&#33719;&#24471;&#23039;&#21183;&#25351;&#23548;&#65292;&#25105;&#20204;&#20351;VLM&#33021;&#22815;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35782;&#21035;&#35270;&#35273;&#24187;&#35273;&#26041;&#38754;&#65292;&#19982;&#20165;&#20381;&#36182;&#20110;RGB&#22270;&#20687;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20943;&#36731;&#35270;&#35273;&#24187;&#35273;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15048v1 Announce Type: cross  Abstract: Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13847</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Domain Adaptation through Gaussian Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31561;&#20215;&#30340;&#31163;&#25955;&#38382;&#39064;&#35299;&#20915;&#36830;&#32493;&#26368;&#20248;&#36755;&#36816;&#12290;&#26368;&#20248;&#36755;&#36816;&#35299;&#20915;&#26041;&#26696;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#21305;&#37197;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22495;&#20043;&#38388;&#26144;&#23556;&#25968;&#25454;&#28857;&#65292;&#25110;&#32773;&#23558;&#26631;&#31614;&#20174;&#28304;&#22495;&#32452;&#20214;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#22312;&#22833;&#25928;&#35786;&#26029;&#30340;&#20004;&#20010;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13847v1 Announce Type: cross  Abstract: In this paper we explore domain adaptation through optimal transport. We propose a novel approach, where we model the data distributions through Gaussian mixture models. This strategy allows us to solve continuous optimal transport through an equivalent discrete problem. The optimal transport solution gives us a matching between source and target domain mixture components. From this matching, we can map data points between domains, or transfer the labels from the source domain components towards the target domain. We experiment with 2 domain adaptation benchmarks in fault diagnosis, showing that our methods have state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#36866;&#24212;&#35821;&#20041;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#36793;&#32536;&#20869;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24573;&#35270;&#12290;</title><link>https://arxiv.org/abs/2403.05559</link><description>&lt;p&gt;
&#29992;&#33258;&#36866;&#24212;&#20851;&#31995;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Improving Cognitive Diagnosis Models with Adaptive Relational Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05559
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#36866;&#24212;&#35821;&#20041;&#24863;&#30693;&#22270;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;&#35748;&#30693;&#35786;&#26029;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#20013;&#23545;&#36793;&#32536;&#20869;&#30340;&#24322;&#36136;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#30340;&#24573;&#35270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#35786;&#26029;&#65288;CD&#65289;&#31639;&#27861;&#22312;&#26234;&#33021;&#25945;&#32946;&#20013;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20851;&#27880;&#12290;&#20856;&#22411;&#30340;CD&#31639;&#27861;&#36890;&#36807;&#25512;&#26029;&#23398;&#29983;&#30340;&#33021;&#21147;&#65288;&#21363;&#20182;&#20204;&#22312;&#21508;&#31181;&#30693;&#35782;&#27010;&#24565;&#19978;&#30340;&#29087;&#32451;&#27700;&#24179;&#65289;&#26469;&#24110;&#21161;&#23398;&#29983;&#12290;&#36825;&#31181;&#29087;&#32451;&#27700;&#24179;&#21487;&#20197;&#36827;&#19968;&#27493;&#23454;&#29616;&#38024;&#23545;&#24615;&#30340;&#25216;&#33021;&#35757;&#32451;&#21644;&#20010;&#24615;&#21270;&#30340;&#32451;&#20064;&#24314;&#35758;&#65292;&#20174;&#32780;&#20419;&#36827;&#22312;&#32447;&#25945;&#32946;&#20013;&#23398;&#29983;&#30340;&#23398;&#20064;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#24314;&#31435;&#21644;&#25972;&#21512;&#23398;&#29983;-&#32451;&#20064;&#20108;&#37096;&#22270;&#23545;&#20110;&#22686;&#24378;&#35786;&#26029;&#24615;&#33021;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#30340;&#30740;&#31350;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#19968;&#26041;&#38754;&#65292;&#30740;&#31350;&#20154;&#21592;&#24573;&#35270;&#20102;&#36793;&#32536;&#20869;&#30340;&#24322;&#36136;&#24615;&#65292;&#21363;&#21487;&#33021;&#23384;&#22312;&#27491;&#30830;&#21644;&#38169;&#35823;&#30340;&#31572;&#26696;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20182;&#20204;&#24573;&#35270;&#20102;&#36793;&#32536;&#20869;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20363;&#22914;&#65292;&#27491;&#30830;&#30340;&#31572;&#26696;&#21487;&#33021;&#34920;&#31034;&#30495;&#27491;&#25484;&#25569;&#25110;&#24184;&#36816;&#29468;&#27979;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#35821;&#20041;&#24863;&#30693;&#22270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05559v1 Announce Type: cross  Abstract: Cognitive Diagnosis (CD) algorithms receive growing research interest in intelligent education. Typically, these CD algorithms assist students by inferring their abilities (i.e., their proficiency levels on various knowledge concepts). The proficiency levels can enable further targeted skill training and personalized exercise recommendations, thereby promoting students' learning efficiency in online education. Recently, researchers have found that building and incorporating a student-exercise bipartite graph is beneficial for enhancing diagnostic performance. However, there are still limitations in their studies. On one hand, researchers overlook the heterogeneity within edges, where there can be both correct and incorrect answers. On the other hand, they disregard the uncertainty within edges, e.g., a correct answer can indicate true mastery or fortunate guessing. To address the limitations, we propose Adaptive Semantic-aware Graph-ba
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#20989;&#25968;&#30340;&#24555;&#36895;&#36941;&#21382;&#25628;&#32034;&#26041;&#27861;&#65292;&#20854;&#22312;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#19978;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26446;&#32676;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27979;&#35797;&#23637;&#31034;&#27604;&#29616;&#26377;&#31639;&#27861;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2403.01536</link><description>&lt;p&gt;
&#20351;&#29992;&#26680;&#20989;&#25968;&#30340;&#24555;&#36895;&#36941;&#21382;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Fast Ergodic Search with Kernel Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01536
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26680;&#20989;&#25968;&#30340;&#24555;&#36895;&#36941;&#21382;&#25628;&#32034;&#26041;&#27861;&#65292;&#20854;&#22312;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#19978;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#21487;&#20197;&#25512;&#24191;&#21040;&#26446;&#32676;&#65292;&#24182;&#19988;&#36890;&#36807;&#25968;&#20540;&#27979;&#35797;&#23637;&#31034;&#27604;&#29616;&#26377;&#31639;&#27861;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36941;&#21382;&#25628;&#32034;&#20351;&#24471;&#23545;&#20449;&#24687;&#20998;&#24067;&#36827;&#34892;&#26368;&#20339;&#25506;&#32034;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#23545;&#25628;&#32034;&#31354;&#38388;&#30340;&#28176;&#36817;&#35206;&#30422;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#22312;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#19978;&#20855;&#26377;&#25351;&#25968;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#24182;&#19988;&#23616;&#38480;&#20110;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35745;&#31639;&#39640;&#25928;&#30340;&#36941;&#21382;&#25628;&#32034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26159;&#21452;&#37325;&#30340;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#26680;&#30340;&#36941;&#21382;&#24230;&#37327;&#65292;&#24182;&#23558;&#20854;&#20174;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#25512;&#24191;&#21040;&#26446;&#32676;&#19978;&#12290;&#25105;&#20204;&#27491;&#24335;&#35777;&#26126;&#20102;&#25152;&#24314;&#35758;&#30340;&#24230;&#37327;&#19982;&#26631;&#20934;&#36941;&#21382;&#24230;&#37327;&#19968;&#33268;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#22312;&#25628;&#32034;&#31354;&#38388;&#32500;&#24230;&#19978;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#26680;&#36941;&#21382;&#24230;&#37327;&#30340;&#19968;&#38454;&#26368;&#20248;&#24615;&#26465;&#20214;&#65292;&#36825;&#20351;&#24471;&#36712;&#36857;&#20248;&#21270;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;&#20840;&#38754;&#30340;&#25968;&#20540;&#22522;&#20934;&#27979;&#35797;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33267;&#23569;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#31639;&#27861;&#24555;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01536v1 Announce Type: cross  Abstract: Ergodic search enables optimal exploration of an information distribution while guaranteeing the asymptotic coverage of the search space. However, current methods typically have exponential computation complexity in the search space dimension and are restricted to Euclidean space. We introduce a computationally efficient ergodic search method. Our contributions are two-fold. First, we develop a kernel-based ergodic metric and generalize it from Euclidean space to Lie groups. We formally prove the proposed metric is consistent with the standard ergodic metric while guaranteeing linear complexity in the search space dimension. Secondly, we derive the first-order optimality condition of the kernel ergodic metric for nonlinear systems, which enables efficient trajectory optimization. Comprehensive numerical benchmarks show that the proposed method is at least two orders of magnitude faster than the state-of-the-art algorithm. Finally, we d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;T-CUR&#20998;&#35299;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2401.16836</link><description>&lt;p&gt;
&#22522;&#20110;T-CUR&#20998;&#35299;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Coseparable Nonnegative Tensor Factorization With T-CUR Decomposition. (arXiv:2401.16836v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;T-CUR&#20998;&#35299;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#32500;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;(NMF)&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#22312;&#22810;&#39033;&#24335;&#26102;&#38388;&#26694;&#26550;&#20869;&#35299;&#20915;NMF&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#24341;&#20837;&#20102;&#21487;&#20998;&#31163;&#24615;&#20551;&#35774;&#65292;&#26368;&#36817;&#28436;&#21464;&#20026;&#21487;&#20998;&#31163;&#30340;&#27010;&#24565;&#12290;&#36825;&#19968;&#36827;&#23637;&#20026;&#21407;&#22987;&#25968;&#25454;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26680;&#24515;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#25968;&#25454;&#26356;&#33258;&#28982;&#22320;&#34987;&#34920;&#31034;&#20026;&#22810;&#32500;&#25968;&#32452;&#65292;&#22914;&#22270;&#20687;&#25110;&#35270;&#39057;&#12290;&#23558;NMF&#24212;&#29992;&#20110;&#39640;&#32500;&#25968;&#25454;&#28041;&#21450;&#21521;&#37327;&#21270;&#65292;&#20250;&#23548;&#33268;&#20002;&#22833;&#20851;&#38190;&#30340;&#22810;&#32500;&#24230;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#20445;&#30041;&#25968;&#25454;&#20013;&#36825;&#20123;&#22266;&#26377;&#30340;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#36716;&#21521;&#24352;&#37327;(&#22810;&#32500;&#25968;&#32452;)&#24182;&#21033;&#29992;&#24352;&#37327;t&#20056;&#31215;&#12290;&#36825;&#31181;&#26041;&#27861;&#23558;&#21487;&#20998;&#31163;&#30340;NMF&#25193;&#23637;&#21040;&#24352;&#37327;&#35774;&#32622;&#65292;&#20174;&#32780;&#21019;&#24314;&#20102;&#25105;&#20204;&#25152;&#31216;&#30340;&#21487;&#20998;&#31163;&#38750;&#36127;&#24352;&#37327;&#20998;&#35299;(NTF)&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#20132;&#26367;&#32034;&#24341;&#36873;&#25321;&#26041;&#27861;&#26469;&#36873;&#25321;cos
&lt;/p&gt;
&lt;p&gt;
Nonnegative Matrix Factorization (NMF) is an important unsupervised learning method to extract meaningful features from data. To address the NMF problem within a polynomial time framework, researchers have introduced a separability assumption, which has recently evolved into the concept of coseparability. This advancement offers a more efficient core representation for the original data. However, in the real world, the data is more natural to be represented as a multi-dimensional array, such as images or videos. The NMF's application to high-dimensional data involves vectorization, which risks losing essential multi-dimensional correlations. To retain these inherent correlations in the data, we turn to tensors (multidimensional arrays) and leverage the tensor t-product. This approach extends the coseparable NMF to the tensor setting, creating what we term coseparable Nonnegative Tensor Factorization (NTF). In this work, we provide an alternating index selection method to select the cos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.18449</link><description>&lt;p&gt;
&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#20855;&#26377;&#38544;&#34255;&#32422;&#26463;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization with Hidden Constraints via Latent Decision Models. (arXiv:2310.18449v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18449
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#21644;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#23454;&#29616;&#20102;&#21452;&#21521;&#26144;&#23556;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#38544;&#34255;&#32422;&#26463;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#22797;&#26434;&#20915;&#31574;&#38382;&#39064;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#23588;&#20854;&#22312;&#20844;&#20849;&#25919;&#31574;&#39046;&#22495;&#22914;&#35686;&#23519;&#21010;&#21306;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23450;&#20041;&#21487;&#34892;&#21306;&#22495;&#30340;&#22797;&#26434;&#24615;&#21644;&#20915;&#31574;&#30340;&#39640;&#32500;&#24230;&#65292;&#20854;&#22312;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38459;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#38544;&#34255;&#32422;&#26463;&#28508;&#22312;&#31354;&#38388;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;HC-LSBO&#65289;&#65292;&#35813;&#26041;&#27861;&#38598;&#25104;&#20102;&#28508;&#22312;&#20915;&#31574;&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#26469;&#23398;&#20064;&#21487;&#34892;&#20915;&#31574;&#30340;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#21407;&#22987;&#20915;&#31574;&#31354;&#38388;&#19982;&#36739;&#20302;&#32500;&#24230;&#30340;&#28508;&#22312;&#31354;&#38388;&#20043;&#38388;&#30340;&#21452;&#21521;&#26144;&#23556;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;HC-LSBO&#25429;&#25417;&#20102;&#20844;&#20849;&#20915;&#31574;&#21046;&#23450;&#20013;&#22266;&#26377;&#30340;&#38544;&#34255;&#32422;&#26463;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#30340;&#21516;&#26102;&#65292;&#22312;&#21407;&#22987;&#31354;&#38388;&#20013;&#35780;&#20272;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#36827;&#34892;&#25968;&#20540;&#23454;&#39564;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#22823;&#35268;&#27169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimization (BO) has emerged as a potent tool for addressing intricate decision-making challenges, especially in public policy domains such as police districting. However, its broader application in public policymaking is hindered by the complexity of defining feasible regions and the high-dimensionality of decisions. This paper introduces the Hidden-Constrained Latent Space Bayesian Optimization (HC-LSBO), a novel BO method integrated with a latent decision model. This approach leverages a variational autoencoder to learn the distribution of feasible decisions, enabling a two-way mapping between the original decision space and a lower-dimensional latent space. By doing so, HC-LSBO captures the nuances of hidden constraints inherent in public policymaking, allowing for optimization in the latent space while evaluating objectives in the original space. We validate our method through numerical experiments on both synthetic and real data sets, with a specific focus on large-scal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#24182;&#23548;&#20986;&#20102;&#21518;&#24724;&#19978;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#32771;&#34385;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#26102;&#32988;&#36807;&#20854;&#20182;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13604</link><description>&lt;p&gt;
&#24102;&#26377;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#36172;&#24466;&#21453;&#39304;&#30340;&#38543;&#26426;&#27425;&#27169;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback. (arXiv:2303.13604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#24182;&#23548;&#20986;&#20102;&#21518;&#24724;&#19978;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#32771;&#34385;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#26102;&#32988;&#36807;&#20854;&#20182;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26399;&#26395;&#19979;&#30340;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#65292;&#24310;&#36831;&#21453;&#39304;&#34987;&#20551;&#23450;&#20026;&#32452;&#21512;&#21644;&#21311;&#21517;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24310;&#36831;&#21453;&#39304;&#26159;&#30001;&#36807;&#21435;&#34892;&#21160;&#30340;&#22870;&#21169;&#32452;&#25104;&#30340;&#65292;&#36825;&#20123;&#22870;&#21169;&#30001;&#23376;&#32452;&#20214;&#26500;&#25104;&#65292;&#20854;&#26410;&#30693;&#30340;&#20998;&#37197;&#26041;&#24335;&#12290;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#65306;&#26377;&#30028;&#23545;&#25239;&#27169;&#22411;&#12289;&#38543;&#26426;&#29420;&#31435;&#27169;&#22411;&#21644;&#38543;&#26426;&#26465;&#20214;&#29420;&#31435;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#27599;&#31181;&#24310;&#36831;&#27169;&#22411;&#23548;&#20986;&#20102;&#21518;&#24724;&#30028;&#12290;&#24573;&#30053;&#38382;&#39064;&#30456;&#20851;&#21442;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#24310;&#36831;&#27169;&#22411;&#30340;&#21518;&#24724;&#30028;&#20026; $\tilde{O}(T^{2/3} + T^{1/3} \nu)$&#65292;&#20854;&#20013; $T$ &#26159;&#26102;&#38388;&#33539;&#22260;&#65292;$\nu$ &#26159;&#19977;&#31181;&#24773;&#20917;&#19979;&#19981;&#21516;&#23450;&#20041;&#30340;&#24310;&#36831;&#21442;&#25968;&#65292;&#22240;&#27492;&#23637;&#31034;&#20102;&#24102;&#26377;&#24310;&#36831;&#30340;&#34917;&#20607;&#39033;&#12290;&#25152;&#32771;&#34385;&#30340;&#31639;&#27861;&#34987;&#35777;&#26126;&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;&#32771;&#34385;&#20102;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#30340;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the sub-components. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models. The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedbac
&lt;/p&gt;</description></item></channel></rss>