<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ImageNot&#25968;&#25454;&#38598;&#19982;ImageNet&#24418;&#25104;&#23545;&#27604;&#65292;&#23637;&#31034;&#20102;&#20851;&#38190;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25490;&#21517;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#30456;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#30340;&#25913;&#36827;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#20102;ImageNot&#23545;&#20110;&#36801;&#31227;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#25928;&#29992;&#20110;ImageNet&#12290;</title><link>https://arxiv.org/abs/2404.02112</link><description>&lt;p&gt;
ImageNot&#65306;&#19982;ImageNet&#24418;&#25104;&#23545;&#27604;&#65292;&#20445;&#25345;&#27169;&#22411;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
ImageNot: A contrast with ImageNet preserves model rankings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02112
&lt;/p&gt;
&lt;p&gt;
ImageNot&#25968;&#25454;&#38598;&#19982;ImageNet&#24418;&#25104;&#23545;&#27604;&#65292;&#23637;&#31034;&#20102;&#20851;&#38190;&#27169;&#22411;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#25490;&#21517;&#19968;&#33268;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#30456;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#30340;&#25913;&#36827;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#20855;&#26377;&#24378;&#30456;&#20851;&#24615;&#65292;&#34920;&#26126;&#20102;ImageNot&#23545;&#20110;&#36801;&#31227;&#23398;&#20064;&#20855;&#26377;&#31867;&#20284;&#25928;&#29992;&#20110;ImageNet&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;ImageNot&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#19982;ImageNet&#22312;&#35268;&#27169;&#19978;&#21305;&#37197;&#20294;&#22312;&#20854;&#20182;&#26041;&#38754;&#26377;&#30528;&#26174;&#33879;&#24046;&#24322;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#24180;&#26469;&#20026;ImageNet&#24320;&#21457;&#30340;&#20851;&#38190;&#27169;&#22411;&#26550;&#26500;&#22312;ImageNot&#19978;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#65292;&#20854;&#25490;&#21517;&#19982;&#23427;&#20204;&#22312;ImageNet&#19978;&#30340;&#25490;&#21517;&#23436;&#20840;&#30456;&#21516;&#12290;&#26080;&#35770;&#26159;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#36824;&#26159;&#24494;&#35843;&#27169;&#22411;&#65292;&#36825;&#19968;&#28857;&#37117;&#25104;&#31435;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#27169;&#22411;&#30456;&#23545;&#20110;&#20808;&#21069;&#27169;&#22411;&#30340;&#25913;&#36827;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#37117;&#26377;&#24456;&#24378;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;ImageNot&#22312;&#36801;&#31227;&#23398;&#20064;&#30446;&#30340;&#19978;&#20855;&#26377;&#31867;&#20284;&#30340;&#25928;&#29992;&#20110;ImageNet&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23637;&#31034;&#20102;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30456;&#23545;&#24615;&#33021;&#30340;&#24778;&#20154;&#22806;&#37096;&#26377;&#25928;&#24615;&#12290;&#36825;&#19982;&#36890;&#24120;&#22312;&#25968;&#25454;&#38598;&#21457;&#29983;&#23567;&#21464;&#21270;&#26102;&#29978;&#33267;&#32454;&#24494;&#25913;&#21464;&#26102;&#32477;&#23545;&#20934;&#30830;&#24230;&#25968;&#23383;&#20250;&#24613;&#21095;&#19979;&#38477;&#24418;&#25104;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02112v1 Announce Type: new  Abstract: We introduce ImageNot, a dataset designed to match the scale of ImageNet while differing drastically in other aspects. We show that key model architectures developed for ImageNet over the years rank identically when trained and evaluated on ImageNot to how they rank on ImageNet. This is true when training models from scratch or fine-tuning them. Moreover, the relative improvements of each model over earlier models strongly correlate in both datasets. We further give evidence that ImageNot has a similar utility as ImageNet for transfer learning purposes. Our work demonstrates a surprising degree of external validity in the relative performance of image classification models. This stands in contrast with absolute accuracy numbers that typically drop sharply even under small changes to a dataset.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.02514</link><description>&lt;p&gt;
&#20026;&#24320;&#25918;&#24335;&#23398;&#20064;&#26426;&#22120;&#20154;&#35774;&#23450;&#30446;&#30340;&#65306;&#19968;&#20010;&#35745;&#31639;&#20998;&#31867;&#12289;&#23450;&#20041;&#21644;&#25805;&#20316;&#21270;
&lt;/p&gt;
&lt;p&gt;
Purpose for Open-Ended Learning Robots: A Computational Taxonomy, Definition, and Operationalisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02514
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35774;&#23450;&#26426;&#22120;&#20154;&#30446;&#30340;&#30340;&#27010;&#24565;&#65292;&#20197;&#24110;&#21161;&#26426;&#22120;&#20154;&#26356;&#21152;&#20851;&#27880;&#33719;&#21462;&#19982;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#20027;&#24320;&#25918;&#24335;&#23398;&#20064;(OEL)&#26426;&#22120;&#20154;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#30452;&#25509;&#20132;&#20114;&#32047;&#31215;&#33719;&#21462;&#26032;&#25216;&#33021;&#21644;&#30693;&#35782;&#65292;&#20363;&#22914;&#20381;&#38752;&#20869;&#22312;&#21160;&#26426;&#21644;&#33258;&#21160;&#29983;&#25104;&#30340;&#30446;&#26631;&#30340;&#25351;&#23548;&#12290;OEL&#26426;&#22120;&#20154;&#23545;&#24212;&#29992;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20851;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#20351;&#29992;&#33258;&#20027;&#33719;&#21462;&#30340;&#30693;&#35782;&#26469;&#23436;&#25104;&#23545;&#20154;&#31867;&#29992;&#25143;&#26377;&#20851;&#30340;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;OEL&#26426;&#22120;&#20154;&#38754;&#20020;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#36825;&#21487;&#33021;&#23548;&#33268;&#33719;&#21462;&#30340;&#30693;&#35782;&#23545;&#23436;&#25104;&#29992;&#25143;&#20219;&#21153;&#24182;&#19981;&#37027;&#20040;&#37325;&#35201;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#20010;&#21487;&#33021;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#22260;&#32469;&#8220;&#30446;&#30340;&#8221;&#36825;&#19968;&#26032;&#27010;&#24565;&#23637;&#24320;&#12290;&#30446;&#30340;&#34920;&#31034;&#35774;&#35745;&#32773;&#21644;/&#25110;&#29992;&#25143;&#24076;&#26395;&#26426;&#22120;&#20154;&#20174;&#20013;&#33719;&#24471;&#20160;&#20040;&#12290;&#26426;&#22120;&#20154;&#24212;&#20351;&#29992;&#30446;&#30340;&#30340;&#20869;&#37096;&#34920;&#24449;&#65292;&#36825;&#37324;&#31216;&#20026;&#8220;&#24895;&#26395;&#8221;&#65292;&#26469;&#23558;&#20854;&#24320;&#25918;&#24335;&#25506;&#32034;&#38598;&#20013;&#20110;&#33719;&#21462;&#19982;&#20854;&#23436;&#25104;&#30446;&#30340;&#30456;&#20851;&#30340;&#30693;&#35782;&#12290;&#36825;&#39033;&#24037;&#20316;&#26377;&#21161;&#20110;&#21457;&#23637;&#19968;&#20010;&#20849;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02514v1 Announce Type: cross  Abstract: Autonomous open-ended learning (OEL) robots are able to cumulatively acquire new skills and knowledge through direct interaction with the environment, for example relying on the guidance of intrinsic motivations and self-generated goals. OEL robots have a high relevance for applications as they can use the autonomously acquired knowledge to accomplish tasks relevant for their human users. OEL robots, however, encounter an important limitation: this may lead to the acquisition of knowledge that is not so much relevant to accomplish the users' tasks. This work analyses a possible solution to this problem that pivots on the novel concept of `purpose'. Purposes indicate what the designers and/or users want from the robot. The robot should use internal representations of purposes, called here `desires', to focus its open-ended exploration towards the acquisition of knowledge relevant to accomplish them. This work contributes to develop a co
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.12462</link><description>&lt;p&gt;
&#23454;&#29616;&#31471;&#21040;&#31471;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards an end-to-end artificial intelligence driven global weather forecasting system
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12462
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65292;&#36890;&#36807;&#23558;AI&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#25454;&#21516;&#21270;&#21644;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25968;&#25454;&#22788;&#29702;&#21040;&#39044;&#27979;&#20840;&#36807;&#31243;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#23545;&#31185;&#23398;&#21644;&#31038;&#20250;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#23558;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24212;&#29992;&#20110;&#20013;&#26399;&#22825;&#27668;&#39044;&#25253;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#23601;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#20381;&#36182;&#20110;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#65288;NWP&#65289;&#31995;&#32479;&#30340;&#20998;&#26512;&#25110;&#20877;&#20998;&#26512;&#20135;&#21697;&#20316;&#20026;&#39044;&#27979;&#30340;&#21021;&#22987;&#26465;&#20214;&#12290;&#21021;&#22987;&#29366;&#24577;&#36890;&#24120;&#30001;&#20256;&#32479;&#25968;&#25454;&#21516;&#21270;&#32452;&#20214;&#29983;&#25104;&#65292;&#36825;&#26159;&#35745;&#31639;&#26114;&#36149;&#19988;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#25968;&#25454;&#21516;&#21270;&#27169;&#22411;&#65292;&#21363;Adas&#65292;&#29992;&#20110;&#20840;&#29699;&#22825;&#27668;&#21464;&#37327;&#12290;&#25105;&#20204;&#23558;Adas&#19982;&#20808;&#36827;&#30340;&#22522;&#20110;AI&#30340;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65288;&#21363;FengWu&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#26500;&#24314;&#20102;&#31532;&#19968;&#20010;&#31471;&#21040;&#31471;&#22522;&#20110;AI&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#31995;&#32479;&#65306;FengWu-Adas&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Adas&#33021;&#22815;&#21516;&#21270;&#31232;&#30095;&#30340;&#20840;&#29699;&#35266;&#27979;&#25968;&#25454;&#65292;&#20135;&#29983;&#39640;&#36136;&#37327;&#30340;&#20998;&#26512;&#32467;&#26524;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#31283;&#23450;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12462v2 Announce Type: replace-cross  Abstract: The weather forecasting system is important for science and society, and significant achievements have been made in applying artificial intelligence (AI) to medium-range weather forecasting. However, existing AI-based weather forecasting models rely on analysis or reanalysis products from the traditional numerical weather prediction (NWP) systems as initial conditions for making predictions. Initial states are typically generated by traditional data assimilation component, which is computational expensive and time-consuming. Here we present an AI-based data assimilation model, i.e., Adas, for global weather variables. And we combine Adas with the advanced AI-based weather forecasting model (i.e., FengWu) to construct the first end-to-end AI-based global weather forecasting system: FengWu-Adas. We demonstrate that Adas can assimilate sparse global observations to produce high-quality analysis, enabling the system operate stably 
&lt;/p&gt;</description></item></channel></rss>