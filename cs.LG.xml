<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27425;&#20248;&#24615;&#30028;&#38480;&#26041;&#27861;&#20197;&#35299;&#20915;&#26799;&#24230;&#19979;&#38477;&#38382;&#39064;&#65292;&#21033;&#29992;&#26041;&#21521;&#24179;&#28369;&#24230;&#24320;&#21457;&#19978;&#30028;&#24182;&#33719;&#24471;&#25910;&#25947;&#20445;&#35777;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;L&#24179;&#28369;&#24230;&#30340;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.04081</link><description>&lt;p&gt;
&#26041;&#21521;&#24179;&#28369;&#24230;&#19982;&#26799;&#24230;&#26041;&#27861;&#65306;&#25910;&#25947;&#24615;&#21644;&#33258;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Directional Smoothness and Gradient Methods: Convergence and Adaptivity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04081
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27425;&#20248;&#24615;&#30028;&#38480;&#26041;&#27861;&#20197;&#35299;&#20915;&#26799;&#24230;&#19979;&#38477;&#38382;&#39064;&#65292;&#21033;&#29992;&#26041;&#21521;&#24179;&#28369;&#24230;&#24320;&#21457;&#19978;&#30028;&#24182;&#33719;&#24471;&#25910;&#25947;&#20445;&#35777;&#65292;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20248;&#20110;&#20256;&#32479;&#22522;&#20110;L&#24179;&#28369;&#24230;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20026;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#27425;&#20248;&#24615;&#30028;&#38480;&#65292;&#20854;&#21462;&#20915;&#20110;&#27839;&#20248;&#21270;&#36335;&#24452;&#30340;&#30446;&#26631;&#26465;&#20214;&#24615;&#65292;&#32780;&#19981;&#26159;&#20840;&#23616;&#26368;&#22351;&#24773;&#20917;&#24120;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#30340;&#20851;&#38190;&#26159;&#26041;&#21521;&#24179;&#28369;&#24230;&#65292;&#36825;&#26159;&#25105;&#20204;&#29992;&#26469;&#24320;&#21457;&#30446;&#26631;&#19978;&#30028;&#30340;&#26799;&#24230;&#21464;&#21270;&#24230;&#37327;&#12290;&#26368;&#23567;&#21270;&#36825;&#20123;&#19978;&#30028;&#38656;&#35201;&#35299;&#20915;&#38544;&#24335;&#26041;&#31243;&#20197;&#33719;&#24471;&#19968;&#31995;&#21015;&#24378;&#36866;&#24212;&#30340;&#27493;&#38271;&#65307;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#20984;&#20108;&#27425;&#20989;&#25968;, &#36825;&#20123;&#26041;&#31243;&#24456;&#23481;&#26131;&#27714;&#35299;&#65292;&#24182;&#20026;&#20004;&#31181;&#32463;&#20856;&#27493;&#38271;&#25552;&#20379;&#20102;&#26032;&#30340;&#20445;&#35777;&#12290;&#23545;&#20110;&#19968;&#33324;&#20989;&#25968;, &#25105;&#20204;&#35777;&#26126;Polyak&#27493;&#38271;&#21644;&#24402;&#19968;&#21270;GD&#33719;&#24471;&#24555;&#36895;&#30340;&#12289;&#36335;&#24452;&#30456;&#20851;&#30340;&#36895;&#29575;&#65292;&#23613;&#31649;&#19981;&#20351;&#29992;&#26041;&#21521;&#24179;&#28369;&#24230;&#30340;&#20219;&#20309;&#30693;&#35782;&#12290; Logistic&#22238;&#24402;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25910;&#25947;&#20445;&#35777;&#27604;&#22522;&#20110;L&#24179;&#28369;&#24230;&#30340;&#32463;&#20856;&#29702;&#35770;&#26356;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04081v1 Announce Type: new  Abstract: We develop new sub-optimality bounds for gradient descent (GD) that depend on the conditioning of the objective along the path of optimization, rather than on global, worst-case constants. Key to our proofs is directional smoothness, a measure of gradient variation that we use to develop upper-bounds on the objective. Minimizing these upper-bounds requires solving implicit equations to obtain a sequence of strongly adapted step-sizes; we show that these equations are straightforward to solve for convex quadratics and lead to new guarantees for two classical step-sizes. For general functions, we prove that the Polyak step-size and normalized GD obtain fast, path-dependent rates despite using no knowledge of the directional smoothness. Experiments on logistic regression show our convergence guarantees are tighter than the classical theory based on L-smoothness.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;</title><link>https://arxiv.org/abs/2403.02774</link><description>&lt;p&gt;
&#24555;&#36895;&#12289;&#33258;&#36866;&#24212;&#23610;&#24230;&#21644;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#22330;&#38477;&#23610;&#24230;&#19982;&#29983;&#25104;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Fast, Scale-Adaptive, and Uncertainty-Aware Downscaling of Earth System Model Fields with Generative Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02774
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#19968;&#33268;&#24615;&#27169;&#22411;&#65292;&#22312;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#39640;&#25928;&#12289;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#27169;&#25311;&#65292;&#24182;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21644;&#39640;&#20998;&#36776;&#29575;&#30340;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;(ESM)&#27169;&#25311;&#23545;&#20110;&#35780;&#20272;&#20154;&#20026;&#27668;&#20505;&#21464;&#21270;&#23545;&#29983;&#24577;&#21644;&#31038;&#20250;&#32463;&#27982;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36807;&#39640;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;ESM&#27169;&#25311;&#30340;&#38477;&#23610;&#24230;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#32479;&#35745;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23545;&#27599;&#20010;ESM&#37117;&#38656;&#35201;&#35745;&#31639;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#19988;&#22312;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#27668;&#20505;&#39044;&#27979;&#25928;&#26524;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#19968;&#33268;&#24615;&#27169;&#22411;(CM)&#65292;&#20197;&#38646;&#26679;&#26412;&#26041;&#24335;&#39640;&#25928;&#20934;&#30830;&#22320;&#38477;&#23610;&#24230;&#20219;&#24847;ESM&#27169;&#25311;&#26469;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#12290;&#25105;&#20204;&#30340;&#22522;&#30784;&#27169;&#22411;&#26041;&#27861;&#20197;&#21482;&#21463;&#35266;&#27979;&#21442;&#32771;&#25968;&#25454;&#38480;&#21046;&#30340;&#20998;&#36776;&#29575;&#20135;&#29983;&#27010;&#29575;&#24615;&#38477;&#23610;&#24230;&#22330;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CM&#22312;&#32500;&#25345;&#39640;&#21487;&#25511;&#24615;&#30340;&#21516;&#26102;&#20197;&#36739;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02774v1 Announce Type: cross  Abstract: Accurate and high-resolution Earth system model (ESM) simulations are essential to assess the ecological and socio-economic impacts of anthropogenic climate change, but are computationally too expensive. Recent machine learning approaches have shown promising results in downscaling ESM simulations, outperforming state-of-the-art statistical approaches. However, existing methods require computationally costly retraining for each ESM and extrapolate poorly to climates unseen during training. We address these shortcomings by learning a consistency model (CM) that efficiently and accurately downscales arbitrary ESM simulations without retraining in a zero-shot manner. Our foundation model approach yields probabilistic downscaled fields at resolution only limited by the observational reference data. We show that the CM outperforms state-of-the-art diffusion models at a fraction of computational cost while maintaining high controllability on
&lt;/p&gt;</description></item><item><title>KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.02648</link><description>&lt;p&gt;
&#31227;&#38500;&#24179;&#26041;&#26681;&#65306;&#19968;&#31181;&#26032;&#30340;&#39640;&#25928;&#26631;&#24230;&#19981;&#21464;&#29256;&#26412;&#30340;AdaGrad
&lt;/p&gt;
&lt;p&gt;
Remove that Square Root: A New Efficient Scale-Invariant Version of AdaGrad
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02648
&lt;/p&gt;
&lt;p&gt;
KATE&#26159;&#19968;&#31181;&#26032;&#30340;&#20248;&#21270;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;AdaGrad&#26631;&#24230;&#19981;&#21464;&#30340;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#21644;&#19968;&#33324;&#30340;&#38750;&#20984;&#38382;&#39064;&#20013;&#35777;&#26126;&#20102;&#20854;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#25968;&#20540;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;KATE&#22312;&#21508;&#31181;&#22330;&#26223;&#20013;&#22343;&#20248;&#20110;AdaGrad&#24182;&#19982;Adam&#24615;&#33021;&#21305;&#37197;/&#36229;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#38750;&#24120;&#27969;&#34892;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38477;&#20302;&#23398;&#20064;&#36895;&#29575;&#35843;&#25972;&#30340;&#25104;&#26412;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;KATE&#30340;&#26032;&#22411;&#20248;&#21270;&#31639;&#27861;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;AdaGrad&#31639;&#27861;&#30340;&#26631;&#24230;&#19981;&#21464;&#36866;&#24212;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KATE&#22312;&#24191;&#20041;&#32447;&#24615;&#27169;&#22411;&#26696;&#20363;&#20013;&#30340;&#26631;&#24230;&#19981;&#21464;&#24615;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#19968;&#33324;&#30340;&#20809;&#28369;&#38750;&#20984;&#38382;&#39064;&#65292;&#25105;&#20204;&#20026;KATE&#24314;&#31435;&#20102;&#19968;&#20010;&#25910;&#25947;&#36895;&#29575;&#20026;$O \left(\frac{\log T}{\sqrt{T}} \right)$&#65292;&#19982;AdaGrad&#21644;Adam&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#30456;&#21305;&#37197;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#19981;&#21516;&#38382;&#39064;&#30340;&#25968;&#20540;&#23454;&#39564;&#23558;KATE&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#33258;&#36866;&#24212;&#31639;&#27861;Adam&#21644;AdaGrad&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21253;&#25324;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#21644;&#25991;&#26412;&#20998;&#31867;&#31561;&#22797;&#26434;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25152;&#26377;&#32771;&#34385;&#21040;&#30340;&#22330;&#26223;&#20013;&#65292;KATE&#22987;&#32456;&#32988;&#36807;AdaGrad&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#19978;&#21305;&#37197;/&#36229;&#36234;Adam&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02648v1 Announce Type: cross  Abstract: Adaptive methods are extremely popular in machine learning as they make learning rate tuning less expensive. This paper introduces a novel optimization algorithm named KATE, which presents a scale-invariant adaptation of the well-known AdaGrad algorithm. We prove the scale-invariance of KATE for the case of Generalized Linear Models. Moreover, for general smooth non-convex problems, we establish a convergence rate of $O \left(\frac{\log T}{\sqrt{T}} \right)$ for KATE, matching the best-known ones for AdaGrad and Adam. We also compare KATE to other state-of-the-art adaptive algorithms Adam and AdaGrad in numerical experiments with different problems, including complex machine learning tasks like image classification and text classification on real data. The results indicate that KATE consistently outperforms AdaGrad and matches/surpasses the performance of Adam in all considered scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.00376</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#27867;&#21270;&#30340;&#19981;&#21464;&#27979;&#35797;&#26102;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Invariant Test-Time Adaptation for Vision-Language Model Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00376
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#36890;&#36807;&#20248;&#21270;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#36843;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#65292;&#20197;&#35299;&#20915;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#29305;&#23450;&#20219;&#21153;&#38656;&#27714;&#19978;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#22312;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#20351;&#20854;&#22312;&#20247;&#22810;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#38271;&#23614;&#20219;&#21153;&#65288;&#22914;&#32454;&#31890;&#24230;&#22270;&#20687;&#20998;&#31867;&#65289;&#26102;&#26174;&#31034;&#20986;&#26126;&#26174;&#23616;&#38480;&#65292;&#36825;&#26159;&#30001;&#20110;&#8220;&#20915;&#31574;&#25463;&#24452;&#8221;&#23548;&#33268;&#20102;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#21463;&#38480;&#12290;&#26412;&#25991;&#21457;&#29616;CLIP&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#29305;&#24449;&#38598;&#65292;&#28085;&#30422;&#20102;&#26082;&#26377;&#30340;\textit{&#26399;&#26395;&#19981;&#21464;&#22240;&#26524;&#29305;&#24449;}&#21448;&#26377;&#30340;\textit{&#19981;&#24076;&#26395;&#30340;&#20915;&#31574;&#25463;&#24452;}&#12290;&#27492;&#22806;&#65292;CLIP&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#28304;&#33258;&#20854;&#26080;&#27861;&#26377;&#25928;&#21033;&#29992;&#39044;&#35757;&#32451;&#29305;&#24449;&#20197;&#31526;&#21512;&#29305;&#23450;&#20219;&#21153;&#35201;&#27714;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#31181;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#20248;&#33539;&#24335;&#65292;&#20248;&#21270;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#65292;&#20174;&#32780;&#20419;&#20351;&#27169;&#22411;&#21033;&#29992;&#30495;&#27491;&#30340;&#22240;&#26524;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00376v1 Announce Type: cross  Abstract: Vision-language foundation models have exhibited remarkable success across a multitude of downstream tasks due to their scalability on extensive image-text paired datasets. However, these models display significant limitations when applied to long-tail tasks, such as fine-grained image classification, as a result of "decision shortcuts" that hinders their generalization capabilities. In this work, we find that the CLIP model possesses a rich set of features, encompassing both \textit{desired invariant causal features} and \textit{undesired decision shortcuts}. Moreover, the underperformance of CLIP on downstream tasks originates from its inability to effectively utilize pre-trained features in accordance with specific task requirements. To address this challenge, this paper introduces a test-time prompt tuning paradigm that optimizes a learnable prompt, thereby compelling the model to exploit genuine causal invariant features while dis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#19981;&#21516;&#25512;&#26029;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#65292;&#24182;&#23545;&#36807;&#21435;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;</title><link>https://arxiv.org/abs/2402.05098</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#25955;&#25512;&#26029;&#27169;&#22411;&#30340;&#25193;&#25955;&#27169;&#22411;&#65306;&#22522;&#20934;&#27979;&#35797;&#21644;&#25913;&#36827;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
On diffusion models for amortized inference: Benchmarking and improving stochastic control and sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05098
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;&#38543;&#26426;&#25511;&#21046;&#21644;&#37319;&#26679;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#36890;&#36807;&#22522;&#20934;&#27979;&#35797;&#27604;&#36739;&#20102;&#19981;&#21516;&#25512;&#26029;&#26041;&#27861;&#30340;&#30456;&#23545;&#20248;&#21155;&#65292;&#24182;&#23545;&#36807;&#21435;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#20197;&#20174;&#32473;&#23450;&#30340;&#38750;&#26631;&#20934;&#21270;&#23494;&#24230;&#25110;&#33021;&#37327;&#20989;&#25968;&#20998;&#24067;&#20013;&#37319;&#26679;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#20960;&#31181;&#25193;&#25955;&#32467;&#26500;&#25512;&#26029;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#21253;&#25324;&#22522;&#20110;&#27169;&#25311;&#30340;&#21464;&#20998;&#26041;&#27861;&#21644;&#31163;&#31574;&#30053;&#26041;&#27861;&#65288;&#36830;&#32493;&#29983;&#25104;&#27969;&#32593;&#32476;&#65289;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#29616;&#26377;&#31639;&#27861;&#30340;&#30456;&#23545;&#20248;&#21183;&#65292;&#21516;&#26102;&#23545;&#36807;&#21435;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#36136;&#30097;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#31574;&#30053;&#26041;&#27861;&#25506;&#32034;&#31574;&#30053;&#65292;&#22522;&#20110;&#30446;&#26631;&#31354;&#38388;&#20013;&#30340;&#23616;&#37096;&#25628;&#32034;&#21644;&#22238;&#25918;&#32531;&#20914;&#21306;&#30340;&#20351;&#29992;&#65292;&#24182;&#35777;&#26126;&#23427;&#21487;&#20197;&#25913;&#21892;&#21508;&#31181;&#30446;&#26631;&#20998;&#24067;&#19978;&#30340;&#26679;&#26412;&#36136;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#30340;&#20195;&#30721;&#24050;&#20844;&#24320;&#22312;https://github.com/GFNOrg/gfn-diffusion&#65292;&#20316;&#20026;&#26410;&#26469;&#22312;&#20998;&#25955;&#25512;&#26029;&#27169;&#22411;&#19978;&#24037;&#20316;&#30340;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of training diffusion models to sample from a distribution with a given unnormalized density or energy function. We benchmark several diffusion-structured inference methods, including simulation-based variational approaches and off-policy methods (continuous generative flow networks). Our results shed light on the relative advantages of existing algorithms while bringing into question some claims from past work. We also propose a novel exploration strategy for off-policy methods, based on local search in the target space with the use of a replay buffer, and show that it improves the quality of samples on a variety of target distributions. Our code for the sampling methods and benchmarks studied is made public at https://github.com/GFNOrg/gfn-diffusion as a base for future work on diffusion models for amortized inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#37319;&#29992;&#38543;&#26426;&#30697;&#38453;&#26041;&#27861;&#65292;&#22312;&#20302;&#22810;&#32447;&#24615;&#31209;&#24352;&#37327;&#36924;&#36817;&#20013;&#23637;&#31034;&#20102;&#23545;&#31181;&#26893;&#30340;&#20302;&#31209;&#20449;&#21495;&#30340;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#22823;&#32500;&#35889;&#34892;&#20026;&#21644;&#20449;&#22122;&#27604;&#20934;&#30830;&#39044;&#27979;&#20102;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#32473;&#20986;&#20102;HOOI&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>https://arxiv.org/abs/2402.03169</link><description>&lt;p&gt;
&#20302;&#22810;&#32447;&#24615;&#31209;&#24352;&#37327;&#36924;&#36817;&#30340;&#38543;&#26426;&#30697;&#38453;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Random Matrix Approach to Low-Multilinear-Rank Tensor Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03169
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#37319;&#29992;&#38543;&#26426;&#30697;&#38453;&#26041;&#27861;&#65292;&#22312;&#20302;&#22810;&#32447;&#24615;&#31209;&#24352;&#37327;&#36924;&#36817;&#20013;&#23637;&#31034;&#20102;&#23545;&#31181;&#26893;&#30340;&#20302;&#31209;&#20449;&#21495;&#30340;&#20272;&#35745;&#65292;&#24182;&#26681;&#25454;&#22823;&#32500;&#35889;&#34892;&#20026;&#21644;&#20449;&#22122;&#27604;&#20934;&#30830;&#39044;&#27979;&#20102;&#37325;&#24314;&#24615;&#33021;&#65292;&#24182;&#32473;&#20986;&#20102;HOOI&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#35745;&#31639;&#38408;&#20540;&#38468;&#36817;&#30340;&#19968;&#33324;&#23574;&#23792;&#24352;&#37327;&#27169;&#22411;&#65292;&#23545;&#31181;&#26893;&#30340;&#20302;&#31209;&#20449;&#21495;&#20272;&#35745;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35748;&#35782;&#12290;&#20381;&#38752;&#22823;&#22411;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#30340;&#26631;&#20934;&#24037;&#20855;&#65292;&#25105;&#20204;&#34920;&#24449;&#20102;&#25968;&#25454;&#24352;&#37327;&#30340;&#23637;&#24320;&#30340;&#22823;&#32500;&#35889;&#34892;&#20026;&#65292;&#24182;&#23637;&#31034;&#20102;&#20915;&#23450;&#20027;&#35201;&#20449;&#21495;&#26041;&#21521;&#21487;&#26816;&#27979;&#24615;&#30340;&#30456;&#20851;&#20449;&#22122;&#27604;&#12290;&#36825;&#20123;&#32467;&#26524;&#21487;&#20197;&#20934;&#30830;&#22320;&#39044;&#27979;&#22312;&#38750;&#24179;&#20961;&#21306;&#22495;&#30340;&#25130;&#26029;&#22810;&#32447;&#24615;&#22855;&#24322;&#20540;&#20998;&#35299;(MLSVD)&#30340;&#37325;&#24314;&#24615;&#33021;&#12290;&#36825;&#19968;&#28857;&#23588;&#20854;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20316;&#20026;&#26356;&#39640;&#38454;&#27491;&#20132;&#36845;&#20195;(HOOI)&#26041;&#26696;&#30340;&#21021;&#22987;&#21270;&#65292;&#20854;&#25910;&#25947;&#21040;&#26368;&#20339;&#20302;&#22810;&#32447;&#24615;&#31209;&#36924;&#36817;&#23436;&#20840;&#21462;&#20915;&#20110;&#20854;&#21021;&#22987;&#21270;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;HOOI&#25910;&#25947;&#30340;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#35777;&#26126;&#22312;&#22823;&#32500;&#26497;&#38480;&#19979;&#25910;&#25947;&#21069;&#30340;&#36845;&#20195;&#27425;&#25968;&#36235;&#20110;1&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a comprehensive understanding of the estimation of a planted low-rank signal from a general spiked tensor model near the computational threshold. Relying on standard tools from the theory of large random matrices, we characterize the large-dimensional spectral behavior of the unfoldings of the data tensor and exhibit relevant signal-to-noise ratios governing the detectability of the principal directions of the signal. These results allow to accurately predict the reconstruction performance of truncated multilinear SVD (MLSVD) in the non-trivial regime. This is particularly important since it serves as an initialization of the higher-order orthogonal iteration (HOOI) scheme, whose convergence to the best low-multilinear-rank approximation depends entirely on its initialization. We give a sufficient condition for the convergence of HOOI and show that the number of iterations before convergence tends to $1$ in the large-dimensional limit.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.09982</link><description>&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#65306;AI-Enabled Compiler-driven Program Optimization&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;LLVM&#25552;&#20379;&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#19981;&#21516;&#30340;&#20248;&#21270;&#36890;&#36335;&#20013;&#33719;&#30410;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;ACPO&#30340;&#39640;&#23618;&#35270;&#22270;&#12289;&#31867;&#23618;&#27425;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#24490;&#29615;&#23637;&#24320;&#21644;&#20989;&#25968;&#20869;&#32852;&#20256;&#36882;&#30340;ML&#20351;&#33021;&#21270;&#65292;&#23637;&#31034;&#20102;ACPO&#30340;&#19968;&#20123;&#29992;&#20363;&#65292;&#25551;&#36848;&#20102;ACPO&#22914;&#20309;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09982v2 Announce Type: replace-cross  Abstract: The key to performance optimization of a program is to decide correctly when a certain transformation should be applied by a compiler. This is an ideal opportunity to apply machine-learning models to speed up the tuning process; while this realization has been around since the late 90s, only recent advancements in ML enabled a practical application of ML to compilers as an end-to-end framework.   This paper presents ACPO: \textbf{\underline{A}}I-Enabled \textbf{\underline{C}}ompiler-driven \textbf{\underline{P}}rogram \textbf{\underline{O}}ptimization; a novel framework to provide LLVM with simple and comprehensive tools to benefit from employing ML models for different optimization passes. We first showcase the high-level view, class hierarchy, and functionalities of ACPO and subsequently, demonstrate a couple of use cases of ACPO by ML-enabling the Loop Unroll and Function Inlining passes and describe how ACPO can be leverage
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.09308</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#33041;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Divergences between Language Models and Human Brains
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09308
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21644;&#20154;&#31867;&#22823;&#33041;&#22312;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#21457;&#29616;&#22312;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#39046;&#22495;&#65292;LMs&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#65292;&#20294;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21644;&#20154;&#31867;&#26159;&#21542;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#22788;&#29702;&#35821;&#35328;&#65311;&#26368;&#36817;&#30340;&#30740;&#31350;&#26263;&#31034;&#32943;&#23450;&#65292;&#21457;&#29616;&#22823;&#33041;&#20449;&#21495;&#21487;&#20197;&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#20869;&#37096;&#34920;&#31034;&#26377;&#25928;&#22320;&#36827;&#34892;&#39044;&#27979;&#12290;&#23613;&#31649;&#36825;&#26679;&#30340;&#32467;&#26524;&#34987;&#35748;&#20026;&#21453;&#26144;&#20102;LMs&#21644;&#20154;&#31867;&#22823;&#33041;&#20043;&#38388;&#30340;&#20849;&#20139;&#35745;&#31639;&#21407;&#29702;&#65292;&#20294;LMs&#21644;&#20154;&#31867;&#22312;&#35821;&#35328;&#34920;&#31034;&#21644;&#20351;&#29992;&#19978;&#20063;&#23384;&#22312;&#26126;&#26174;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#26816;&#26597;LM&#34920;&#31034;&#21644;&#20154;&#31867;&#22823;&#33041;&#23545;&#35821;&#35328;&#30340;&#21709;&#24212;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36890;&#36807;&#37319;&#29992;&#20004;&#20010;&#25968;&#25454;&#38598;&#23545;&#21463;&#35797;&#32773;&#38405;&#35835;&#21644;&#21548;&#21465;&#36848;&#25925;&#20107;&#30340;&#26041;&#24335;&#65292;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#35821;&#35328;&#22788;&#29702;&#20043;&#38388;&#30340;&#20998;&#27495;&#12290;&#36890;&#36807;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20004;&#20010;&#39046;&#22495;&#65292;&#21363;&#31038;&#20132;/&#24773;&#24863;&#26234;&#33021;&#21644;&#29289;&#29702;&#24120;&#35782;&#65292;&#36825;&#20123;&#39046;&#22495;&#22312;LMs&#20013;&#26080;&#27861;&#24456;&#22909;&#22320;&#25429;&#25417;&#21040;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20154;&#31867;&#34892;&#20026;&#23454;&#39564;&#39564;&#35777;&#20102;&#36825;&#20123;&#39046;&#22495;&#65292;&#24182;&#35777;&#26126;&#22312;&#36825;&#20123;&#39046;&#22495;&#23545;LMs&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#25913;&#21892;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2401.14961</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
End-To-End Set-Based Training for Neural Network Verification. (arXiv:2401.14961v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14961
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#65292;&#24182;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#31616;&#21270;&#39564;&#35777;&#36807;&#31243;&#24182;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#21363;&#24494;&#23567;&#30340;&#36755;&#20837;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#31070;&#32463;&#32593;&#32476;&#36755;&#20986;&#20135;&#29983;&#37325;&#22823;&#21464;&#21270;&#12290;&#23433;&#20840;&#20851;&#38190;&#29615;&#22659;&#38656;&#35201;&#23545;&#36755;&#20837;&#25200;&#21160;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#39318;&#27425;&#37319;&#29992;&#31471;&#21040;&#31471;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#35813;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#35757;&#32451;&#20986;&#21487;&#36827;&#34892;&#24418;&#24335;&#21270;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#35757;&#32451;&#26041;&#27861;&#33021;&#22815;&#22823;&#22823;&#31616;&#21270;&#24050;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#32493;&#24418;&#24335;&#21270;&#40065;&#26834;&#24615;&#39564;&#35777;&#36807;&#31243;&#12290;&#30456;&#27604;&#20110;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;&#38598;&#21512;&#30340;&#35745;&#31639;&#26469;&#35757;&#32451;&#25972;&#20010;&#25200;&#21160;&#36755;&#20837;&#38598;&#21512;&#19978;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#22522;&#20110;&#38598;&#21512;&#30340;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#35757;&#32451;&#20986;&#26131;&#20110;&#39564;&#35777;&#30340;&#40065;&#26834;&#24615;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are vulnerable to adversarial attacks, i.e., small input perturbations can result in substantially different outputs of a neural network. Safety-critical environments require neural networks that are robust against input perturbations. However, training and formally verifying robust neural networks is challenging. We address this challenge by employing, for the first time, a end-to-end set-based training procedure that trains robust neural networks for formal verification. Our training procedure drastically simplifies the subsequent formal robustness verification of the trained neural network. While previous research has predominantly focused on augmenting neural network training with adversarial attacks, our approach leverages set-based computing to train neural networks with entire sets of perturbed inputs. Moreover, we demonstrate that our set-based training procedure effectively trains robust neural networks, which are easier to verify. In many cases, set-based trai
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;AUROC&#21644;AUPRC&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#30456;&#27604;&#20110;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;AUPRC&#20248;&#36234;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#39044;&#26399;&#30340;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25991;&#29486;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2401.06091</link><description>&lt;p&gt;
AUROC&#21644;AUPRC&#22312;&#31867;&#19981;&#24179;&#34913;&#19979;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at AUROC and AUPRC under Class Imbalance. (arXiv:2401.06091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06091
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#30740;&#31350;&#21457;&#29616;AUROC&#21644;AUPRC&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#24773;&#20917;&#19979;&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#30456;&#27604;&#20110;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;AUPRC&#20248;&#36234;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#39044;&#26399;&#30340;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#12290;&#30740;&#31350;&#36824;&#36890;&#36807;&#20998;&#26512;&#22823;&#37327;&#25991;&#29486;&#39564;&#35777;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#24191;&#27867;&#30340;&#35266;&#28857;&#26159;&#65292;&#22312;&#20108;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#38754;&#31215;&#21463;&#38480;&#21046;&#30340;&#20934;&#30830;&#29575;&#26354;&#32447;&#65288;AUPRC&#65289;&#27604;&#21463;&#35797;&#32773;&#24037;&#20316;&#29305;&#24449;&#26354;&#32447;&#19979;&#30340;&#38754;&#31215;&#65288;AUROC&#65289;&#26356;&#22909;&#22320;&#29992;&#20110;&#27169;&#22411;&#27604;&#36739;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#12290;&#26412;&#25991;&#36890;&#36807;&#26032;&#39062;&#30340;&#25968;&#23398;&#20998;&#26512;&#25361;&#25112;&#20102;&#36825;&#19968;&#35266;&#28857;&#65292;&#24182;&#35828;&#26126;&#20102;AUROC&#21644;AUPRC&#21487;&#20197;&#20197;&#27010;&#29575;&#26415;&#35821;&#31616;&#27905;&#22320;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;AUPRC&#24182;&#19981;&#22914;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#30340;&#22312;&#31867;&#21035;&#19981;&#24179;&#34913;&#30340;&#24773;&#20917;&#19979;&#26356;&#20248;&#65292;&#29978;&#33267;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#23475;&#30340;&#25351;&#26631;&#65292;&#22240;&#20026;&#23427;&#20542;&#21521;&#20110;&#36807;&#20998;&#20559;&#21521;&#20110;&#22312;&#27491;&#26679;&#26412;&#36739;&#20026;&#39057;&#32321;&#30340;&#23376;&#32676;&#20013;&#25913;&#21892;&#27169;&#22411;&#12290;&#36825;&#31181;&#20559;&#24046;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#22686;&#21152;&#31639;&#27861;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#20123;&#27934;&#35265;&#30340;&#25512;&#21160;&#19979;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#22238;&#39038;&#65292;&#24182;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;arXiv&#19978;&#30340;150&#22810;&#19975;&#31687;&#35770;&#25991;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#37325;&#28857;&#26159;&#39564;&#35777;&#21644;&#35777;&#26126;&#22768;&#31216;&#30340;AUPRC&#20248;&#36234;&#24615;&#30340;&#26222;&#36941;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In machine learning (ML), a widespread adage is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for binary classification tasks with class imbalance. This paper challenges this notion through novel mathematical analysis, illustrating that AUROC and AUPRC can be concisely related in probabilistic terms. We demonstrate that AUPRC, contrary to popular belief, is not superior in cases of class imbalance and might even be a harmful metric, given its inclination to unduly favor model improvements in subpopulations with more frequent positive labels. This bias can inadvertently heighten algorithmic disparities. Prompted by these insights, a thorough review of existing ML literature was conducted, utilizing large language models to analyze over 1.5 million papers from arXiv. Our investigation focused on the prevalence and substantiation of the purported AUPRC superiority. The result
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.16625</link><description>&lt;p&gt;
&#38598;&#21512;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Set-based Neural Network Encoding. (arXiv:2305.16625v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16625
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#38598;&#21512;&#21270;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#26469;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#39640;&#25928;&#32534;&#30721;&#22788;&#29702;&#65292;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#38598;&#21512;&#21040;&#38598;&#21512;&#21644;&#38598;&#21512;&#21040;&#21521;&#37327;&#20989;&#25968;&#26469;&#26377;&#25928;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#65292;&#36827;&#34892;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#32534;&#30721;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#38656;&#35201;&#23545;&#19981;&#21516;&#26550;&#26500;&#32534;&#20889;&#33258;&#23450;&#20041;&#32534;&#30721;&#27169;&#22411;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#23545;&#28151;&#21512;&#26550;&#26500;&#21644;&#19981;&#21516;&#21442;&#25968;&#22823;&#23567;&#30340;&#27169;&#22411;&#21160;&#24577;&#32534;&#30721;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340; SNE&#65288;&#38598;&#21512;&#21270;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#22120;&#65289;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#36880;&#23618;&#32534;&#30721;&#26041;&#26696;&#65292;&#32771;&#34385;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#23618;&#35745;&#31639;&#32467;&#26500;&#12290;&#26368;&#32456;&#23558;&#25152;&#26377;&#23618;&#27425;&#32534;&#30721;&#21512;&#24182;&#21040;&#19968;&#36215;&#65292;&#20197;&#33719;&#21462;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#30690;&#37327;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#8220;pad-chunk-encode&#8221;&#27969;&#27700;&#32447;&#26469;&#26377;&#25928;&#22320;&#32534;&#30721;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#35813;&#27969;&#27700;&#32447;&#21487;&#26681;&#25454;&#35745;&#31639;&#21644;&#20869;&#23384;&#38480;&#21046;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#20004;&#20010;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#24615;&#33021;&#39044;&#27979;&#30340;&#26032;&#20219;&#21153;&#65306;&#36328;&#25968;&#25454;&#38598;&#21644;&#26550;&#26500;&#36866;&#24212;&#24615;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose an approach to neural network weight encoding for generalization performance prediction that utilizes set-to-set and set-to-vector functions to efficiently encode neural network parameters. Our approach is capable of encoding neural networks in a modelzoo of mixed architecture and different parameter sizes as opposed to previous approaches that require custom encoding models for different architectures. Furthermore, our \textbf{S}et-based \textbf{N}eural network \textbf{E}ncoder (SNE) takes into consideration the hierarchical computational structure of neural networks by utilizing a layer-wise encoding scheme that culminates to encoding all layer-wise encodings to obtain the neural network encoding vector. Additionally, we introduce a \textit{pad-chunk-encode} pipeline to efficiently encode neural network layers that is adjustable to computational and memory constraints. We also introduce two new tasks for neural network generalization performance prediction: cross-dataset a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#36827;&#34892;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#65292;&#24182;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.05223</link><description>&lt;p&gt;
&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#19981;&#22343;&#21248;&#22270;&#36235;&#21183;&#36807;&#28388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inhomogeneous graph trend filtering via a l2,0 cardinality penalty. (arXiv:2304.05223v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05223
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;L2&#65292;0&#22522;&#25968;&#24809;&#32602;&#30340;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#21487;&#21516;&#26102;&#36827;&#34892;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#65292;&#24182;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#20272;&#35745;&#20998;&#27573;&#24179;&#28369;&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;$\ell_{2,0}$-&#33539;&#25968;&#24809;&#32602;&#22270;&#36235;&#21183;&#36807;&#28388;&#65288;GTF&#65289;&#27169;&#22411;&#65292;&#20197;&#20272;&#35745;&#22312;&#33410;&#28857;&#20043;&#38388;&#20855;&#26377;&#19981;&#22343;&#21248;&#24179;&#28369;&#27700;&#24179;&#30340;&#20998;&#27573;&#24179;&#28369;&#22270;&#20449;&#21495;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#21516;&#26102;&#26159;&#22522;&#20110;&#33410;&#28857;&#19978;&#30340;&#20449;&#21495;&#30340;k-means&#32858;&#31867;&#21644;&#22522;&#20110;&#22270;&#30340;&#26368;&#23567;&#21106;&#65292;&#20854;&#20013;&#32858;&#31867;&#21644;&#21106;&#20849;&#20139;&#30456;&#21516;&#30340;&#20998;&#37197;&#30697;&#38453;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#65306;&#19968;&#31181;&#26159;&#22522;&#20110;&#35889;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;&#27169;&#25311;&#36864;&#28779;&#30340;&#26041;&#27861;&#12290;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;GTF&#27169;&#22411;&#22312;&#38477;&#22122;&#12289;&#25903;&#25345;&#24674;&#22797;&#21644;&#21322;&#30417;&#30563;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#19988;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#39640;&#25928;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study estimation of piecewise smooth signals over a graph. We propose a $\ell_{2,0}$-norm penalized Graph Trend Filtering (GTF) model to estimate piecewise smooth graph signals that exhibits inhomogeneous levels of smoothness across the nodes. We prove that the proposed GTF model is simultaneously a k-means clustering on the signal over the nodes and a minimum graph cut on the edges of the graph, where the clustering and the cut share the same assignment matrix. We propose two methods to solve the proposed GTF model: a spectral decomposition method and a method based on simulated annealing. In the experiment on synthetic and real-world datasets, we show that the proposed GTF model has a better performances compared with existing approaches on the tasks of denoising, support recovery and semi-supervised classification. We also show that the proposed GTF model can be solved more efficiently than existing models for the dataset with a large edge set.
&lt;/p&gt;</description></item></channel></rss>