<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#28857;&#20113;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#20843;&#21449;&#26641;&#27010;&#29575;&#22330;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28210;&#26579;&#21644;&#20445;&#30041;&#32454;&#33268;&#20960;&#20309;&#32454;&#33410;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#20960;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16862</link><description>&lt;p&gt;
INPC&#65306;&#29992;&#20110;&#36752;&#23556;&#22330;&#28210;&#26579;&#30340;&#38544;&#24335;&#31070;&#32463;&#28857;&#20113;
&lt;/p&gt;
&lt;p&gt;
INPC: Implicit Neural Point Clouds for Radiance Field Rendering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16862
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#24335;&#28857;&#20113;&#34920;&#31034;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36830;&#32493;&#20843;&#21449;&#26641;&#27010;&#29575;&#22330;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#28210;&#26579;&#21644;&#20445;&#30041;&#32454;&#33268;&#20960;&#20309;&#32454;&#33410;&#30340;&#20248;&#21183;&#65292;&#24182;&#19988;&#22312;&#20960;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#37325;&#24314;&#21644;&#21512;&#25104;&#26080;&#36793;&#30028;&#30340;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#12290;&#19982;&#20197;&#24448;&#20351;&#29992;&#20307;&#31215;&#22330;&#12289;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#25110;&#31163;&#25955;&#28857;&#20113;&#20195;&#29702;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#22330;&#26223;&#34920;&#31034;&#65292;&#23427;&#22312;&#36830;&#32493;&#20843;&#21449;&#26641;&#27010;&#29575;&#22330;&#21644;&#22810;&#20998;&#36776;&#29575;&#21704;&#24076;&#32593;&#26684;&#20013;&#38544;&#21547;&#22320;&#32534;&#30721;&#28857;&#20113;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#21183;&#65292;&#20445;&#30041;&#20102;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#26377;&#21033;&#30340;&#34892;&#20026;&#65306;&#25105;&#20204;&#30340;&#26032;&#39062;&#38544;&#24335;&#28857;&#20113;&#34920;&#31034;&#21644;&#21487;&#24494;&#30340;&#21452;&#32447;&#24615;&#20809;&#26629;&#21270;&#22120;&#23454;&#29616;&#20102;&#24555;&#36895;&#28210;&#26579;&#65292;&#21516;&#26102;&#20445;&#30041;&#20102;&#32454;&#24494;&#30340;&#20960;&#20309;&#32454;&#33410;&#65292;&#32780;&#26080;&#38656;&#20381;&#36182;&#20110;&#20687;&#32467;&#26500;&#36816;&#21160;&#28857;&#20113;&#36825;&#26679;&#30340;&#21021;&#22987;&#20808;&#39564;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#24120;&#35265;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#24555;&#36895;&#25512;&#29702;&#65292;&#21487;&#20132;&#20114;&#24103;&#36895;&#29575;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#21462;&#26174;&#24335;&#28857;&#20113;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16862v1 Announce Type: cross  Abstract: We introduce a new approach for reconstruction and novel-view synthesis of unbounded real-world scenes. In contrast to previous methods using either volumetric fields, grid-based models, or discrete point cloud proxies, we propose a hybrid scene representation, which implicitly encodes a point cloud in a continuous octree-based probability field and a multi-resolution hash grid. In doing so, we combine the benefits of both worlds by retaining favorable behavior during optimization: Our novel implicit point cloud representation and differentiable bilinear rasterizer enable fast rendering while preserving fine geometric detail without depending on initial priors like structure-from-motion point clouds. Our method achieves state-of-the-art image quality on several common benchmark datasets. Furthermore, we achieve fast inference at interactive frame rates, and can extract explicit point clouds to further enhance performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#20056;&#23376;&#35825;&#23548;&#30340;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13728</link><description>&lt;p&gt;
M-HOF-Opt: &#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#65306;&#22522;&#20110;&#20056;&#23376;&#35825;&#23548;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
M-HOF-Opt: Multi-Objective Hierarchical Output Feedback Optimization via Multiplier Induced Loss Landscape Scheduling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#30446;&#26631;&#20998;&#23618;&#36755;&#20986;&#21453;&#39304;&#20248;&#21270;&#30340;&#26041;&#24335;&#65292;&#21033;&#29992;&#20056;&#23376;&#35825;&#23548;&#30340;&#25439;&#22833;&#26223;&#35266;&#35843;&#24230;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#22797;&#26434;&#25439;&#22833;&#20989;&#25968;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#25439;&#22833;&#20989;&#25968;&#30001;&#35768;&#22810;&#39033;&#32452;&#25104;&#26102;&#65292;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#23545;&#26435;&#37325;&#20056;&#23376;&#30340;&#32452;&#21512;&#36873;&#25321;&#24418;&#25104;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#65292;&#29992;&#20110;&#32852;&#21512;&#27169;&#22411;&#21442;&#25968;&#21644;&#20056;&#23376;&#28436;&#21270;&#36807;&#31243;&#65292;&#20855;&#26377;&#22522;&#20110;&#36229;&#20307;&#31215;&#30340;&#20284;&#28982;&#65292;&#20419;&#36827;&#27599;&#20010;&#25439;&#22833;&#39033;&#30340;&#22810;&#30446;&#26631;&#19979;&#38477;&#12290;&#30456;&#24212;&#30340;&#21442;&#25968;&#21644;&#20056;&#23376;&#20272;&#35745;&#20316;&#20026;&#19968;&#20010;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#34987;&#36716;&#21270;&#20026;&#19968;&#20010;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#65292;&#20854;&#20013;&#22810;&#30446;&#26631;&#19979;&#38477;&#30446;&#26631;&#34987;&#20998;&#23618;&#22320;&#20998;&#27966;&#21040;&#19968;&#31995;&#21015;&#32422;&#26463;&#20248;&#21270;&#23376;&#38382;&#39064;&#20013;&#12290;&#23376;&#38382;&#39064;&#32422;&#26463;&#26681;&#25454;&#24085;&#32047;&#25176;&#25903;&#37197;&#33258;&#21160;&#36866;&#24212;&#24182;&#20316;&#20026;&#20302;&#23618;&#20056;&#23376;&#25511;&#21046;&#22120;&#35843;&#24230;&#25439;&#22833;&#26223;&#35266;&#30340;&#35774;&#23450;&#28857;&#65292;&#36890;&#36807;&#27599;&#20010;&#25439;&#22833;&#39033;&#30340;&#36755;&#20986;&#21453;&#39304;&#26469;&#36816;&#34892;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26080;&#20056;&#23376;&#30340;&#65292;&#24182;&#19988;&#22312;&#26102;&#20195;&#23610;&#24230;&#19978;&#36816;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13728v1 Announce Type: new  Abstract: When a neural network parameterized loss function consists of many terms, the combinatorial choice of weight multipliers during the optimization process forms a challenging problem. To address this, we proposed a probabilistic graphical model (PGM) for the joint model parameter and multiplier evolution process, with a hypervolume based likelihood that promotes multi-objective descent of each loss term. The corresponding parameter and multiplier estimation as a sequential decision process is then cast into an optimal control problem, where the multi-objective descent goal is dispatched hierarchically into a series of constraint optimization sub-problems. The sub-problem constraint automatically adapts itself according to Pareto dominance and serves as the setpoint for the low level multiplier controller to schedule loss landscapes via output feedback of each loss term. Our method is multiplier-free and operates at the timescale of epochs,
&lt;/p&gt;</description></item><item><title>ZIP-DL&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.11795</link><description>&lt;p&gt;
&#20302;&#25104;&#26412;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Low-Cost Privacy-Aware Decentralized Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11795
&lt;/p&gt;
&lt;p&gt;
ZIP-DL&#26159;&#19968;&#31181;&#20302;&#25104;&#26412;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#65292;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#36739;&#39640;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38544;&#31169;&#24863;&#30693;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#65288;DL&#65289;&#31639;&#27861;ZIP-DL&#65292;&#35813;&#31639;&#27861;&#20381;&#36182;&#20110;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#21521;&#27599;&#20010;&#27169;&#22411;&#26356;&#26032;&#28155;&#21152;&#30456;&#20851;&#22122;&#22768;&#12290;&#36825;&#31181;&#25216;&#26415;&#30830;&#20445;&#20102;&#30001;&#20110;&#20854;&#30456;&#20851;&#24615;&#65292;&#22312;&#32858;&#21512;&#36807;&#31243;&#20013;&#28155;&#21152;&#30340;&#22122;&#22768;&#20960;&#20046;&#30456;&#20114;&#25269;&#28040;&#65292;&#20174;&#32780;&#26368;&#23567;&#21270;&#23545;&#27169;&#22411;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;ZIP-DL&#19981;&#38656;&#35201;&#22810;&#27425;&#36890;&#20449;&#36718;&#36827;&#34892;&#22122;&#22768;&#25269;&#28040;&#65292;&#35299;&#20915;&#20102;&#38544;&#31169;&#20445;&#25252;&#19982;&#36890;&#20449;&#24320;&#38144;&#20043;&#38388;&#30340;&#24120;&#35265;&#26435;&#34913;&#12290;&#25105;&#20204;&#20026;&#25910;&#25947;&#36895;&#24230;&#21644;&#38544;&#31169;&#20445;&#35777;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20174;&#32780;&#20351;ZIP-DL&#21487;&#24212;&#29992;&#20110;&#23454;&#38469;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;ZIP-DL&#22312;&#26131;&#21463;&#25915;&#20987;&#24615;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#21462;&#24471;&#20102;&#26368;&#20339;&#26435;&#34913;&#12290;&#29305;&#21035;&#26159;&#65292;&#19982;&#22522;&#32447;DL&#30456;&#27604;&#65292;ZIP-DL&#65288;i&#65289;&#23558;&#21487;&#36861;&#36394;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#38477;&#20302;&#20102;&#22810;&#36798;52&#20010;&#28857;&#65292;&#65288;ii&#65289;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;37&#20010;&#30334;&#20998;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11795v1 Announce Type: new  Abstract: This paper introduces ZIP-DL, a novel privacy-aware decentralized learning (DL) algorithm that relies on adding correlated noise to each model update during the model training process. This technique ensures that the added noise almost neutralizes itself during the aggregation process due to its correlation, thus minimizing the impact on model accuracy. In addition, ZIP-DL does not require multiple communication rounds for noise cancellation, addressing the common trade-off between privacy protection and communication overhead. We provide theoretical guarantees for both convergence speed and privacy guarantees, thereby making ZIP-DL applicable to practical scenarios. Our extensive experimental study shows that ZIP-DL achieves the best trade-off between vulnerability and accuracy. In particular, ZIP-DL (i) reduces the effectiveness of a linkability attack by up to 52 points compared to baseline DL, and (ii) achieves up to 37 more accuracy
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#21040;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#30830;&#20445;&#31283;&#20581;&#24615;&#65292;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#31639;&#27861;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#65292;&#31616;&#21333;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#65292;&#39564;&#35777;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#30340;&#25512;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.01865</link><description>&lt;p&gt;
&#36890;&#36807;&#38170;&#22810;&#20803;&#20998;&#26512;&#25913;&#21892;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving generalisation via anchor multivariate analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01865
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#21040;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#65292;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#30830;&#20445;&#31283;&#20581;&#24615;&#65292;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#31639;&#27861;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#65292;&#31616;&#21333;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#65292;&#39564;&#35777;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#23545;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#30340;&#25512;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#38170;&#22238;&#24402;&#65288;AR&#65289;&#20013;&#24341;&#20837;&#22240;&#26524;&#27491;&#21017;&#21270;&#25193;&#23637;&#65292;&#20197;&#25913;&#21892;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19982;&#38170;&#26694;&#26550;&#30456;&#21305;&#37197;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#30830;&#20445;&#23545;&#20998;&#24067;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#12290;&#21508;&#31181;&#22810;&#20803;&#20998;&#26512;&#65288;MVA&#65289;&#31639;&#27861;&#65292;&#22914;&#65288;&#27491;&#20132;&#21270;&#65289;PLS&#12289;RRR&#21644;MLR&#65292;&#22343;&#22312;&#38170;&#26694;&#26550;&#20869;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#31616;&#21333;&#30340;&#27491;&#21017;&#21270;&#22686;&#24378;&#20102;OOD&#35774;&#32622;&#20013;&#30340;&#31283;&#20581;&#24615;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#30340;&#27668;&#20505;&#31185;&#23398;&#38382;&#39064;&#20013;&#65292;&#20026;&#25152;&#36873;&#31639;&#27861;&#25552;&#20379;&#20102;&#20272;&#35745;&#22120;&#65292;&#23637;&#31034;&#20102;&#20854;&#19968;&#33268;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;&#32463;&#39564;&#39564;&#35777;&#31361;&#26174;&#20102;&#38170;&#27491;&#21017;&#21270;&#30340;&#22810;&#21151;&#33021;&#24615;&#65292;&#24378;&#35843;&#20854;&#19982;MVA&#26041;&#27861;&#30340;&#20860;&#23481;&#24615;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#22686;&#24378;&#21487;&#22797;&#21046;&#24615;&#30340;&#21516;&#26102;&#25269;&#24481;&#20998;&#24067;&#36716;&#31227;&#20013;&#30340;&#20316;&#29992;&#12290;&#25193;&#23637;&#30340;AR&#26694;&#26550;&#25512;&#36827;&#20102;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#35770;&#65292;&#35299;&#20915;&#20102;&#21487;&#38752;OOD&#27867;&#21270;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01865v1 Announce Type: cross  Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#32676;&#23545;&#31216;&#24615;&#31561;&#39069;&#22806;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#23545;&#31216;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;</title><link>https://arxiv.org/abs/2402.19369</link><description>&lt;p&gt;
&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Structure Preserving Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#21487;&#20197;&#23398;&#20064;&#20855;&#26377;&#32676;&#23545;&#31216;&#24615;&#31561;&#39069;&#22806;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31995;&#21015;&#23545;&#31216;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25193;&#25955;&#27169;&#22411;&#24050;&#25104;&#20026;&#20027;&#35201;&#30340;&#20998;&#24067;&#23398;&#20064;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32467;&#26500;&#20445;&#25345;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#31867;&#29992;&#20110;&#23398;&#20064;&#20855;&#26377;&#39069;&#22806;&#32467;&#26500;&#65288;&#22914;&#32676;&#23545;&#31216;&#24615;&#65289;&#30340;&#20998;&#24067;&#30340;&#25193;&#25955;&#36807;&#31243;&#65292;&#36890;&#36807;&#21046;&#23450;&#25193;&#25955;&#36716;&#25442;&#27493;&#39588;&#20445;&#25345;&#23545;&#31216;&#24615;&#30340;&#29702;&#35770;&#26465;&#20214;&#12290;&#38500;&#20102;&#23454;&#29616;&#31561;&#21464;&#25968;&#25454;&#37319;&#26679;&#36712;&#36857;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#19968;&#31995;&#21015;&#19981;&#21516;&#30340;&#23545;&#31216;&#31561;&#21464;&#25193;&#25955;&#27169;&#22411;&#26469;&#35828;&#26126;&#36825;&#20123;&#32467;&#26524;&#65292;&#36825;&#20123;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#22266;&#26377;&#23545;&#31216;&#30340;&#20998;&#24067;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#35777;&#30740;&#31350;&#39564;&#35777;&#25152;&#24320;&#21457;&#30340;&#27169;&#22411;&#31526;&#21512;&#25552;&#20986;&#30340;&#29702;&#35770;&#65292;&#24182;&#22312;&#26679;&#26412;&#22343;&#31561;&#24615;&#26041;&#38754;&#33021;&#22815;&#32988;&#36807;&#29616;&#26377;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#25552;&#20986;&#30340;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#20445;&#35777;&#30340;&#31561;&#21464;&#22270;&#20687;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19369v1 Announce Type: new  Abstract: Diffusion models have become the leading distribution-learning method in recent years. Herein, we introduce structure-preserving diffusion processes, a family of diffusion processes for learning distributions that possess additional structure, such as group symmetries, by developing theoretical conditions under which the diffusion transition steps preserve said symmetry. While also enabling equivariant data sampling trajectories, we exemplify these results by developing a collection of different symmetry equivariant diffusion models capable of learning distributions that are inherently symmetric. Empirical studies, over both synthetic and real-world datasets, are used to validate the developed models adhere to the proposed theory and are capable of achieving improved performance over existing methods in terms of sample equality. We also show how the proposed models can be used to achieve theoretically guaranteed equivariant image noise r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#23545;&#40784;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#39640;&#27010;&#29575;&#24674;&#22797;&#27491;&#30830;&#30340;&#39030;&#28857;&#23545;&#40784;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#29305;&#24449;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#27700;&#24179;&#26465;&#20214;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#30452;&#25509;&#21305;&#37197;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2402.07340</link><description>&lt;p&gt;
&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#38543;&#26426;&#20960;&#20309;&#22270;&#36827;&#34892;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Random Geometric Graph Alignment with Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#23545;&#40784;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#39640;&#27010;&#29575;&#24674;&#22797;&#27491;&#30830;&#30340;&#39030;&#28857;&#23545;&#40784;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#29305;&#24449;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#27700;&#24179;&#26465;&#20214;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19982;&#30452;&#25509;&#21305;&#37197;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#39030;&#28857;&#29305;&#24449;&#20449;&#24687;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#23545;&#40784;&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20004;&#20010;&#29420;&#31435;&#25200;&#21160;&#30340;&#21333;&#20010;&#38543;&#26426;&#20960;&#20309;&#22270;&#20197;&#21450;&#22122;&#22768;&#31232;&#30095;&#29305;&#24449;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#21153;&#26159;&#24674;&#22797;&#20004;&#20010;&#22270;&#30340;&#39030;&#28857;&#20043;&#38388;&#30340;&#26410;&#30693;&#19968;&#23545;&#19968;&#26144;&#23556;&#20851;&#31995;&#12290;&#25105;&#20204;&#35777;&#26126;&#22312;&#29305;&#24449;&#21521;&#37327;&#30340;&#31232;&#30095;&#24615;&#21644;&#22122;&#22768;&#27700;&#24179;&#28385;&#36275;&#19968;&#23450;&#26465;&#20214;&#30340;&#24773;&#20917;&#19979;&#65292;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21333;&#23618;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#22312;&#24456;&#39640;&#30340;&#27010;&#29575;&#19979;&#36890;&#36807;&#22270;&#32467;&#26500;&#26469;&#24674;&#22797;&#27491;&#30830;&#30340;&#39030;&#28857;&#23545;&#40784;&#12290;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#22122;&#22768;&#27700;&#24179;&#30340;&#26465;&#20214;&#19978;&#30028;&#65292;&#20165;&#23384;&#22312;&#23545;&#25968;&#22240;&#23376;&#24046;&#36317;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#19982;&#30452;&#25509;&#22312;&#22122;&#22768;&#39030;&#28857;&#29305;&#24449;&#19978;&#27714;&#35299;&#20998;&#37197;&#38382;&#39064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#22122;&#22768;&#27700;&#24179;&#33267;&#23569;&#20026;&#24120;&#25968;&#26102;&#65292;&#36825;&#31181;&#30452;&#25509;&#21305;&#37197;&#20250;&#23548;&#33268;&#24674;&#22797;&#19981;&#23436;&#20840;&#65292;&#32780;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#23481;&#24525;n
&lt;/p&gt;
&lt;p&gt;
We characterize the performance of graph neural networks for graph alignment problems in the presence of vertex feature information. More specifically, given two graphs that are independent perturbations of a single random geometric graph with noisy sparse features, the task is to recover an unknown one-to-one mapping between the vertices of the two graphs. We show under certain conditions on the sparsity and noise level of the feature vectors, a carefully designed one-layer graph neural network can with high probability recover the correct alignment between the vertices with the help of the graph structure. We also prove that our conditions on the noise level are tight up to logarithmic factors. Finally we compare the performance of the graph neural network to directly solving an assignment problem on the noisy vertex features. We demonstrate that when the noise level is at least constant this direct matching fails to have perfect recovery while the graph neural network can tolerate n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;</title><link>https://arxiv.org/abs/2402.04412</link><description>&lt;p&gt;
VampPrior&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
The VampPrior Mixture Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#23427;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#65292;&#21487;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#38598;&#25104;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#25913;&#21892;&#24403;&#21069;&#32858;&#31867;&#20808;&#39564;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28165;&#26224;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#24378;&#22823;&#30340;&#32858;&#31867;&#24615;&#33021;&#65292;&#23558;VMM&#19982;scVI&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#29992;&#20110;&#28145;&#24230;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;DLVMs&#65289;&#30340;&#32858;&#31867;&#20808;&#39564;&#38656;&#35201;&#39044;&#20808;&#23450;&#20041;&#32858;&#31867;&#30340;&#25968;&#37327;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#36739;&#24046;&#30340;&#21021;&#22987;&#21270;&#30340;&#24433;&#21709;&#12290;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#36890;&#36807;&#21516;&#26102;&#25191;&#34892;&#38598;&#25104;&#21644;&#32858;&#31867;&#30340;&#26041;&#24335;&#26497;&#22823;&#22320;&#25913;&#36827;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;scRNA-seq&#20998;&#26512;&#12290;&#25105;&#20204;&#23558;VampPrior&#65288;Tomczak&#21644;Welling&#65292;2018&#65289;&#35843;&#25972;&#20026;Dirichlet&#36807;&#31243;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#24471;&#21040;VampPrior&#28151;&#21512;&#27169;&#22411;&#65288;VMM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;DLVM&#20808;&#39564;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25512;&#29702;&#36807;&#31243;&#65292;&#20132;&#26367;&#20351;&#29992;&#21464;&#20998;&#25512;&#29702;&#21644;&#32463;&#39564;&#36125;&#21494;&#26031;&#65292;&#20197;&#28165;&#26970;&#22320;&#21306;&#20998;&#21464;&#20998;&#21644;&#20808;&#39564;&#21442;&#25968;&#12290;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;VMM&#30340;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#33719;&#24471;&#20102;&#26497;&#20855;&#31454;&#20105;&#21147;&#30340;&#32858;&#31867;&#24615;&#33021;&#12290;&#23558;VMM&#19982;&#24191;&#21463;&#27426;&#36814;&#30340;scRNA-seq&#38598;&#25104;&#26041;&#27861;scVI&#65288;Lopez&#31561;&#65292;2018&#65289;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#20854;&#24615;&#33021;&#65292;&#24182;&#33258;&#21160;&#23558;&#32454;&#32990;&#20998;&#32452;&#20026;&#20855;&#26377;&#29983;&#29289;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current clustering priors for deep latent variable models (DLVMs) require defining the number of clusters a-priori and are susceptible to poor initializations. Addressing these deficiencies could greatly benefit deep learning-based scRNA-seq analysis by performing integration and clustering simultaneously. We adapt the VampPrior (Tomczak &amp; Welling, 2018) into a Dirichlet process Gaussian mixture model, resulting in the VampPrior Mixture Model (VMM), a novel prior for DLVMs. We propose an inference procedure that alternates between variational inference and Empirical Bayes to cleanly distinguish variational and prior parameters. Using the VMM in a Variational Autoencoder attains highly competitive clustering performance on benchmark datasets. Augmenting scVI (Lopez et al., 2018), a popular scRNA-seq integration method, with the VMM significantly improves its performance and automatically arranges cells into biologically meaningful clusters.
&lt;/p&gt;</description></item><item><title>&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>https://arxiv.org/abs/2402.02746</link><description>&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#36275;&#20197;&#24212;&#23545;
&lt;/p&gt;
&lt;p&gt;
Standard Gaussian Process is All You Need for High-Dimensional Bayesian Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02746
&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934; Gaussian &#36807;&#31243;&#22312;&#39640;&#32500;&#36125;&#21494;&#26031;&#20248;&#21270;&#20013;&#34920;&#29616;&#20248;&#31168;&#65292;&#32463;&#39564;&#35777;&#25454;&#26174;&#31034;&#20854;&#22312;&#20989;&#25968;&#20272;&#35745;&#21644;&#21327;&#26041;&#24046;&#24314;&#27169;&#20013;&#20811;&#26381;&#20102;&#39640;&#32500;&#36755;&#20837;&#22256;&#38590;&#65292;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#20154;&#20204;&#26222;&#36941;&#35748;&#20026;&#20351;&#29992;&#26631;&#20934; Gaussian &#36807;&#31243;&#65288;GP&#65289;&#36827;&#34892;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;BO&#65289;&#65292;&#21363;&#26631;&#20934; BO&#65292;&#22312;&#39640;&#32500;&#20248;&#21270;&#38382;&#39064;&#20013;&#25928;&#26524;&#19981;&#20339;&#12290;&#36825;&#31181;&#35266;&#24565;&#21487;&#20197;&#37096;&#20998;&#24402;&#22240;&#20110; Gaussian &#36807;&#31243;&#22312;&#21327;&#26041;&#24046;&#24314;&#27169;&#21644;&#20989;&#25968;&#20272;&#35745;&#20013;&#23545;&#39640;&#32500;&#36755;&#20837;&#30340;&#22256;&#38590;&#12290;&#34429;&#28982;&#36825;&#20123;&#25285;&#24551;&#30475;&#36215;&#26469;&#21512;&#29702;&#65292;&#20294;&#32570;&#20047;&#25903;&#25345;&#36825;&#31181;&#35266;&#28857;&#30340;&#32463;&#39564;&#35777;&#25454;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#30740;&#31350;&#20102;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22522;&#20934;&#38382;&#39064;&#19978;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#22238;&#24402;&#36827;&#34892;&#39640;&#32500;&#20248;&#21270;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#26631;&#20934; GP &#30340;&#34920;&#29616;&#22987;&#32456;&#20301;&#20110;&#26368;&#20339;&#33539;&#22260;&#20869;&#65292;&#24448;&#24448;&#27604;&#19987;&#38376;&#20026;&#39640;&#32500;&#20248;&#21270;&#35774;&#35745;&#30340;&#29616;&#26377; BO &#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#19982;&#21051;&#26495;&#21360;&#35937;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#26631;&#20934; GP &#21487;&#20197;&#20316;&#20026;&#23398;&#20064;&#39640;&#32500;&#30446;&#26631;&#20989;&#25968;&#30340;&#33021;&#21147;&#24378;&#22823;&#30340;&#20195;&#29702;&#12290;&#22312;&#27809;&#26377;&#24378;&#32467;&#26500;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#26631;&#20934; GP &#36827;&#34892; BO &#21487;&#20197;&#33719;&#24471;&#38750;&#24120;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been a long-standing and widespread belief that Bayesian Optimization (BO) with standard Gaussian process (GP), referred to as standard BO, is ineffective in high-dimensional optimization problems. This perception may partly stem from the intuition that GPs struggle with high-dimensional inputs for covariance modeling and function estimation. While these concerns seem reasonable, empirical evidence supporting this belief is lacking. In this paper, we systematically investigated BO with standard GP regression across a variety of synthetic and real-world benchmark problems for high-dimensional optimization. Surprisingly, the performance with standard GP consistently ranks among the best, often outperforming existing BO methods specifically designed for high-dimensional optimization by a large margin. Contrary to the stereotype, we found that standard GP can serve as a capable surrogate for learning high-dimensional target functions. Without strong structural assumptions, BO wit
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hypergraph-MLP&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#36229;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#30417;&#30563;&#20013;&#38598;&#25104;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#32780;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#26102;&#20943;&#23569;&#36807;&#24230;&#24179;&#28369;&#21644;&#32467;&#26500;&#25200;&#21160;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.09778</link><description>&lt;p&gt;
&#36229;&#22270;-MLP&#65306;&#22312;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#30340;&#36229;&#22270;&#19978;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Hypergraph-MLP: Learning on Hypergraphs without Message Passing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09778
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hypergraph-MLP&#30340;&#26032;&#22411;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#36229;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#30417;&#30563;&#20013;&#38598;&#25104;&#36229;&#22270;&#32467;&#26500;&#20449;&#24687;&#32780;&#26080;&#38656;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#26102;&#20943;&#23569;&#36807;&#24230;&#24179;&#28369;&#21644;&#32467;&#26500;&#25200;&#21160;&#24341;&#36215;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#22312;&#24314;&#27169;&#21253;&#21547;&#20004;&#20010;&#20197;&#19978;&#23454;&#20307;&#30340;&#39640;&#38454;&#20851;&#31995;&#25968;&#25454;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#36234;&#26469;&#36234;&#21463;&#37325;&#35270;&#12290;&#35768;&#22810;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#21033;&#29992;&#22312;&#36229;&#22270;&#32467;&#26500;&#19978;&#30340;&#28040;&#24687;&#20256;&#36882;&#26469;&#22686;&#24378;&#33410;&#28857;&#34920;&#24449;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#36229;&#22270;&#33410;&#28857;&#20998;&#31867;&#31561;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#30340;&#27169;&#22411;&#38754;&#20020;&#30528;&#36807;&#24230;&#24179;&#28369;&#20197;&#21450;&#22312;&#25512;&#29702;&#26102;&#23545;&#32467;&#26500;&#25200;&#21160;&#30340;&#39640;&#24310;&#36831;&#21644;&#25935;&#24863;&#24615;&#31561;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21478;&#31867;&#26041;&#27861;&#65292;&#21363;&#23558;&#20851;&#20110;&#36229;&#22270;&#32467;&#26500;&#30340;&#20449;&#24687;&#38598;&#25104;&#21040;&#35757;&#32451;&#30417;&#30563;&#20013;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#22312;&#25512;&#29702;&#26102;&#20063;&#28040;&#38500;&#20102;&#23545;&#20854;&#30340;&#20381;&#36182;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Hypergraph-MLP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#36229;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#20854;&#20013;&#23398;&#20064;&#27169;&#22411;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09778v2 Announce Type: replace  Abstract: Hypergraphs are vital in modelling data with higher-order relations containing more than two entities, gaining prominence in machine learning and signal processing. Many hypergraph neural networks leverage message passing over hypergraph structures to enhance node representation learning, yielding impressive performances in tasks like hypergraph node classification. However, these message-passing-based models face several challenges, including oversmoothing as well as high latency and sensitivity to structural perturbations at inference time. To tackle those challenges, we propose an alternative approach where we integrate the information about hypergraph structures into training supervision without explicit message passing, thus also removing the reliance on it at inference. Specifically, we introduce Hypergraph-MLP, a novel learning framework for hypergraph-structured data, where the learning model is a straightforward multilayer p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#20998;&#24067;&#24335;&#22870;&#21169;&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#30495;&#23454;&#22870;&#21169;&#65292;&#24182;&#22312;&#22810;&#20010;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#36739;&#39640;&#30340;&#22238;&#25253;&#12290;</title><link>http://arxiv.org/abs/2401.05710</link><description>&lt;p&gt;
&#23545;&#25200;&#21160;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#24335;&#22870;&#21169;&#35780;&#35770;&#23478;&#26550;&#26500;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Distributional Reward Critic Architecture for Perturbed-Reward Reinforcement Learning. (arXiv:2401.05710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05710
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#36866;&#24212;&#24615;&#20998;&#24067;&#24335;&#22870;&#21169;&#35780;&#35770;&#23478;&#26550;&#26500;&#65292;&#33021;&#22815;&#22312;&#26410;&#30693;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#24674;&#22797;&#30495;&#23454;&#22870;&#21169;&#65292;&#24182;&#22312;&#22810;&#20010;&#25511;&#21046;&#20219;&#21153;&#20013;&#21462;&#24471;&#36739;&#39640;&#30340;&#22238;&#25253;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#26410;&#30693;&#22870;&#21169;&#25200;&#21160;&#30340;&#24773;&#20917;&#19979;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#23545;&#36825;&#20010;&#38382;&#39064;&#20570;&#20986;&#20102;&#24378;&#22823;&#30340;&#20551;&#35774;&#65292;&#21253;&#25324;&#22870;&#21169;&#24179;&#28369;&#24615;&#12289;&#24050;&#30693;&#25200;&#21160;&#21644;/&#25110;&#19981;&#20250;&#25913;&#21464;&#26368;&#20248;&#31574;&#30053;&#30340;&#25200;&#21160;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#26410;&#30693;&#20219;&#24847;&#25200;&#21160;&#30340;&#24773;&#20917;&#65292;&#36825;&#20123;&#25200;&#21160;&#23545;&#22870;&#21169;&#31354;&#38388;&#36827;&#34892;&#20102;&#31163;&#25955;&#21270;&#21644;&#27927;&#29260;&#65292;&#20294;&#22312;&#25200;&#21160;&#21518;&#65292;&#30495;&#23454;&#22870;&#21169;&#23646;&#20110;&#26368;&#39057;&#32321;&#35266;&#23519;&#21040;&#30340;&#31867;&#21035;&#12290;&#36825;&#31867;&#25200;&#21160;&#27867;&#21270;&#20102;&#29616;&#26377;&#30340;&#31867;&#21035;&#65288;&#24182;&#22312;&#26497;&#38480;&#24773;&#20917;&#19979;&#27867;&#21270;&#20102;&#25152;&#26377;&#36830;&#32493;&#26377;&#30028;&#25200;&#21160;&#65289;&#65292;&#24182;&#25112;&#32988;&#20102;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20998;&#24067;&#24335;&#22870;&#21169;&#35780;&#35770;&#23478;&#65292;&#24182;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#22312;&#25216;&#26415;&#26465;&#20214;&#19979;&#23427;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#22870;&#21169;&#12290;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#30446;&#26631;&#25200;&#21160;&#19979;&#65292;&#25105;&#20204;&#22312;40/57&#20010;&#29615;&#22659;&#20013;&#36194;&#21033;/&#24179;&#23616;&#65288;&#30456;&#23545;&#20110;&#26368;&#20339;&#22522;&#32447;&#30340;16/57&#65289;&#12290;&#21363;&#20351;&#22312;&#38750;&#30446;&#26631;&#25200;&#21160;&#19979;&#65292;&#25105;&#20204;&#20173;&#28982;&#32988;&#36807;&#35774;&#35745;&#20026;&#24102;&#26377;&#30446;&#26631;&#25200;&#21160;&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study reinforcement learning in the presence of an unknown reward perturbation. Existing methodologies for this problem make strong assumptions including reward smoothness, known perturbations, and/or perturbations that do not modify the optimal policy. We study the case of unknown arbitrary perturbations that discretize and shuffle reward space, but have the property that the true reward belongs to the most frequently observed class after perturbation. This class of perturbations generalizes existing classes (and, in the limit, all continuous bounded perturbations) and defeats existing methods. We introduce an adaptive distributional reward critic and show theoretically that it can recover the true rewards under technical conditions. Under the targeted perturbation in discrete and continuous control tasks, we win/tie the highest return in 40/57 settings (compared to 16/57 for the best baseline). Even under the untargeted perturbation, we still win an edge over the baseline designed
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20272;&#35745;&#26469;&#35299;&#20915;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#20449;&#36947;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#26469;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20256;&#36755;&#31526;&#21495;&#20272;&#35745;&#12290;</title><link>http://arxiv.org/abs/2311.00226</link><description>&lt;p&gt;
Transformers&#26159;&#26080;&#32447;&#36890;&#20449;&#20013;&#39640;&#25928;&#30340;&#19978;&#19979;&#25991;&#20272;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Transformers are Efficient In-Context Estimators for Wireless Communication. (arXiv:2311.00226v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00226
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#19978;&#19979;&#25991;&#20272;&#35745;&#26469;&#35299;&#20915;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#20449;&#36947;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#32780;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;Transformers&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#26469;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#20256;&#36755;&#31526;&#21495;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;Transformers&#21487;&#20197;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#22312;&#21482;&#26377;&#23569;&#37327;&#25552;&#31034;&#30340;&#24773;&#20917;&#19979;&#65292;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26174;&#24335;&#30340;&#27169;&#22411;&#20248;&#21270;&#12290;&#21463;&#21040;&#36825;&#20010;&#23646;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#19978;&#19979;&#25991;&#20272;&#35745;&#65292;&#29992;&#20110;&#20272;&#35745;&#20174;&#25509;&#25910;&#21040;&#30340;&#31526;&#21495;&#20013;&#30340;&#20256;&#36755;&#31526;&#21495;&#30340;&#32463;&#20856;&#36890;&#20449;&#38382;&#39064;&#12290;&#36890;&#20449;&#20449;&#36947;&#26412;&#36136;&#19978;&#26159;&#19968;&#20010;&#23558;&#20256;&#36755;&#31526;&#21495;&#26144;&#23556;&#21040;&#25509;&#25910;&#31526;&#21495;&#30340;&#22122;&#22768;&#20989;&#25968;&#65292;&#36825;&#20010;&#20989;&#25968;&#21487;&#20197;&#30001;&#19968;&#20010;&#26410;&#30693;&#21442;&#25968;&#34920;&#31034;&#65292;&#20854;&#32479;&#35745;&#25968;&#25454;&#20381;&#36182;&#20110;&#19968;&#20010;&#65288;&#20063;&#26159;&#26410;&#30693;&#30340;&#65289;&#28508;&#22312;&#19978;&#19979;&#25991;&#12290;&#20256;&#32479;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#31181;&#23618;&#27425;&#32467;&#26500;&#65292;&#21482;&#26159;&#35797;&#22270;&#20351;&#29992;&#24050;&#30693;&#30340;&#20256;&#36755;&#20449;&#21495;&#36827;&#34892;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#65292;&#28982;&#21518;&#29992;&#20110;&#20272;&#35745;&#36830;&#32493;&#30340;&#26410;&#30693;&#20256;&#36755;&#31526;&#21495;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#22522;&#26412;&#32852;&#31995;&#65292;&#21363;Transformers&#22312;&#23569;&#37327;&#25552;&#31034;&#19979;&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#19978;&#19979;&#25991;&#24207;&#21015;&#23436;&#25104;&#33021;&#21147;&#65292;&#22240;&#27492;&#23427;&#20204;&#24212;&#35813;&#33021;&#22815;&#38544;&#24335;&#30830;&#23450;...
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers can perform in-context learning, where they adapt to a new task using only a small number of prompts without any explicit model optimization. Inspired by this attribute, we propose a novel approach, called in-context estimation, for the canonical communication problem of estimating transmitted symbols from received symbols. A communication channel is essentially a noisy function that maps transmitted symbols to received symbols, and this function can be represented by an unknown parameter whose statistics depend on an (also unknown) latent context. Conventional approaches ignore this hierarchical structure and simply attempt to use known transmissions, called pilots, to perform a least-squares estimate of the channel parameter, which is then used to estimate successive, unknown transmitted symbols. We make the basic connection that transformers show excellent contextual sequence completion with a few prompts, and so they should be able to implicitly determine t
&lt;/p&gt;</description></item><item><title>&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.12862</link><description>&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Associative Transformer Is A Sparse Representation Learner. (arXiv:2309.12862v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12862
&lt;/p&gt;
&lt;p&gt;
&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#26159;&#19968;&#31181;&#37319;&#29992;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#65292;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#23454;&#29616;&#27169;&#22359;&#29305;&#21270;&#21644;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24418;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20256;&#32479;&#30340;Transformer&#27169;&#22411;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22522;&#20110;&#31232;&#30095;&#20132;&#20114;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36825;&#31181;&#26426;&#21046;&#19982;&#29983;&#29289;&#21407;&#29702;&#26356;&#20026;&#25509;&#36817;&#12290;&#21253;&#25324;Set Transformer&#21644;Perceiver&#22312;&#20869;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#19982;&#26377;&#38480;&#33021;&#21147;&#30340;&#28508;&#22312;&#31354;&#38388;&#30456;&#32467;&#21512;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22522;&#20110;&#26368;&#36817;&#23545;&#20840;&#23616;&#24037;&#20316;&#31354;&#38388;&#29702;&#35770;&#21644;&#20851;&#32852;&#35760;&#24518;&#30340;&#31070;&#32463;&#31185;&#23398;&#30740;&#31350;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#32852;&#21464;&#25442;&#22120;&#65288;AiT&#65289;&#12290;AiT&#24341;&#20837;&#20102;&#20302;&#31209;&#26174;&#24335;&#35760;&#24518;&#65292;&#26082;&#21487;&#20197;&#20316;&#20026;&#20808;&#39564;&#26469;&#25351;&#23548;&#20849;&#20139;&#24037;&#20316;&#31354;&#38388;&#30340;&#29942;&#39048;&#27880;&#24847;&#21147;&#65292;&#21448;&#21487;&#20197;&#20316;&#20026;&#20851;&#32852;&#35760;&#24518;&#30340;&#21560;&#24341;&#23376;&#12290;&#36890;&#36807;&#32852;&#21512;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#36825;&#20123;&#20808;&#39564;&#33258;&#28982;&#22320;&#21457;&#23637;&#20986;&#27169;&#22359;&#30340;&#29305;&#21270;&#65292;&#27599;&#20010;&#27169;&#22359;&#23545;&#24418;&#25104;&#27880;&#24847;&#21147;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#26377;&#25152;&#36129;&#29486;&#12290;&#29942;&#39048;&#21487;&#20197;&#20419;&#36827;&#36755;&#20837;&#20043;&#38388;&#20026;&#23558;&#20449;&#24687;&#20889;&#20837;&#20869;&#23384;&#32780;&#36827;&#34892;&#31454;&#20105;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AiT&#26159;&#19968;&#31181;&#31232;&#30095;&#34920;&#31034;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emerging from the monolithic pairwise attention mechanism in conventional Transformer models, there is a growing interest in leveraging sparse interactions that align more closely with biological principles. Approaches including the Set Transformer and the Perceiver employ cross-attention consolidated with a latent space that forms an attention bottleneck with limited capacity. Building upon recent neuroscience studies of Global Workspace Theory and associative memory, we propose the Associative Transformer (AiT). AiT induces low-rank explicit memory that serves as both priors to guide bottleneck attention in the shared workspace and attractors within associative memory of a Hopfield network. Through joint end-to-end training, these priors naturally develop module specialization, each contributing a distinct inductive bias to form attention bottlenecks. A bottleneck can foster competition among inputs for writing information into the memory. We show that AiT is a sparse representation 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.14172</link><description>&lt;p&gt;
&#20174;&#25968;&#25454;&#20013;&#22522;&#20110;&#20809;&#28369;&#24615;&#20808;&#39564;&#25512;&#26029;&#36229;&#22270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Hypergraph Structure Inference From Data Under Smoothness Prior. (arXiv:2308.14172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14172
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20809;&#28369;&#24615;&#20808;&#39564;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#36229;&#22270;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#35813;&#26041;&#27861;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#22312;&#22788;&#29702;&#28041;&#21450;&#22810;&#20010;&#23454;&#20307;&#30340;&#39640;&#38454;&#20851;&#31995;&#25968;&#25454;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#27809;&#26377;&#26126;&#30830;&#36229;&#22270;&#21487;&#29992;&#30340;&#24773;&#20917;&#19979;&#65292;&#24076;&#26395;&#33021;&#22815;&#20174;&#33410;&#28857;&#29305;&#24449;&#20013;&#25512;&#26029;&#20986;&#26377;&#24847;&#20041;&#30340;&#36229;&#22270;&#32467;&#26500;&#65292;&#20197;&#25429;&#25417;&#25968;&#25454;&#20869;&#22312;&#30340;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#37319;&#29992;&#31616;&#21333;&#39044;&#23450;&#20041;&#30340;&#35268;&#21017;&#65292;&#19981;&#33021;&#31934;&#30830;&#25429;&#25417;&#28508;&#22312;&#36229;&#22270;&#32467;&#26500;&#30340;&#20998;&#24067;&#65292;&#35201;&#20040;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#21644;&#33410;&#28857;&#29305;&#24449;&#20043;&#38388;&#30340;&#26144;&#23556;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65288;&#21363;&#39044;&#20808;&#23384;&#22312;&#30340;&#36229;&#22270;&#32467;&#26500;&#65289;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#23616;&#38480;&#20110;&#23454;&#38469;&#24773;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20809;&#28369;&#24615;&#20808;&#39564;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#35745;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#27809;&#26377;&#26631;&#35760;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#27599;&#20010;&#28508;&#22312;&#36229;&#36793;&#30340;&#27010;&#29575;&#12290;&#25152;&#25552;&#20986;&#30340;&#20808;&#39564;&#34920;&#31034;&#36229;&#36793;&#20013;&#30340;&#33410;&#28857;&#29305;&#24449;&#19982;&#21253;&#21547;&#35813;&#36229;&#36793;&#30340;&#36229;&#36793;&#30340;&#29305;&#24449;&#39640;&#24230;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraphs are important for processing data with higher-order relationships involving more than two entities. In scenarios where explicit hypergraphs are not readily available, it is desirable to infer a meaningful hypergraph structure from the node features to capture the intrinsic relations within the data. However, existing methods either adopt simple pre-defined rules that fail to precisely capture the distribution of the potential hypergraph structure, or learn a mapping between hypergraph structures and node features but require a large amount of labelled data, i.e., pre-existing hypergraph structures, for training. Both restrict their applications in practical scenarios. To fill this gap, we propose a novel smoothness prior that enables us to design a method to infer the probability for each potential hyperedge without labelled data as supervision. The proposed prior indicates features of nodes in a hyperedge are highly correlated by the features of the hyperedge containing th
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.05239</link><description>&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS).
&lt;/p&gt;
&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS). (arXiv:2308.05239v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05239
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#36719;&#20214;&#12289;&#31995;&#32479;&#21644;&#20225;&#19994;&#30340;&#26550;&#26500;&#26694;&#26550;&#12290;&#23427;&#20204;&#35782;&#21035;&#20102;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#24182;&#23450;&#20041;&#20102;&#26550;&#26500;&#30340;&#35266;&#28857;&#21644;&#35270;&#22270;&#65292;&#20197;&#26694;&#26550;&#21644;&#35299;&#20915;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20851;&#27880;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#26550;&#26500;&#26694;&#26550;&#20013;&#65292;&#23578;&#26410;&#21253;&#25324;&#19982;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#25968;&#25454;&#24037;&#31243;&#24072;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26410;&#33021;&#35299;&#20915;&#21709;&#24212;&#25968;&#25454;&#31185;&#23398;&#31038;&#21306;&#20851;&#27880;&#30340;&#26550;&#26500;&#35270;&#28857;&#21644;&#35270;&#22270;&#12290;&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#36866;&#29992;&#20110;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#30340;&#26550;&#26500;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#26222;&#36941;&#23384;&#22312;&#19988;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;CPS&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#32452;&#36866;&#24212;CPS&#39640;&#25928;&#24320;&#21457;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#20248;&#28857;&#26631;&#20934;&#65292;&#21363;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;CPS&#30340;&#26631;&#20934;&#65292;
&lt;/p&gt;
&lt;p&gt;
Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and
&lt;/p&gt;</description></item><item><title>SketchOGD&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.16424</link><description>&lt;p&gt;
SketchOGD&#65306;&#20869;&#23384;&#39640;&#25928;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SketchOGD: Memory-Efficient Continual Learning. (arXiv:2305.16424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16424
&lt;/p&gt;
&lt;p&gt;
SketchOGD&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#20219;&#21153;&#19978;&#25345;&#32493;&#35757;&#32451;&#26102;&#65292;&#23427;&#20204;&#23481;&#26131;&#24536;&#35760;&#20808;&#21069;&#20219;&#21153;&#19978;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#65292;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#24448;&#24448;&#28041;&#21450;&#23384;&#20648;&#36807;&#21435;&#20219;&#21153;&#30340;&#20449;&#24687;&#65292;&#36825;&#24847;&#21619;&#30528;&#20869;&#23384;&#20351;&#29992;&#26159;&#30830;&#23450;&#23454;&#29992;&#24615;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20869;&#23384;&#39640;&#25928;&#30340;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#19968;&#31181;&#24050;&#26377;&#30340;&#31639;&#27861;&#8212;&#8212;&#27491;&#20132;&#26799;&#24230;&#19979;&#38477;&#65288;OGD&#65289;&#12290;OGD&#21033;&#29992;&#20808;&#21069;&#27169;&#22411;&#26799;&#24230;&#26469;&#25214;&#21040;&#32500;&#25345;&#20808;&#21069;&#25968;&#25454;&#28857;&#24615;&#33021;&#30340;&#26435;&#37325;&#26356;&#26032;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23384;&#20648;&#20808;&#21069;&#27169;&#22411;&#26799;&#24230;&#30340;&#20869;&#23384;&#25104;&#26412;&#38543;&#31639;&#27861;&#36816;&#34892;&#26102;&#38388;&#22686;&#38271;&#32780;&#22686;&#21152;&#65292;&#22240;&#27492;OGD&#19981;&#36866;&#29992;&#20110;&#20219;&#24847;&#38271;&#26102;&#38388;&#36328;&#24230;&#30340;&#36830;&#32493;&#23398;&#20064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;SketchOGD&#12290;SketchOGD&#37319;&#29992;&#22312;&#32447;&#33609;&#22270;&#31639;&#27861;&#65292;&#23558;&#27169;&#22411;&#26799;&#24230;&#21387;&#32553;&#20026;&#22266;&#23450;&#22823;&#23567;&#30340;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;
When machine learning models are trained continually on a sequence of tasks, they are liable to forget what they learned on previous tasks -- a phenomenon known as catastrophic forgetting. Proposed solutions to catastrophic forgetting tend to involve storing information about past tasks, meaning that memory usage is a chief consideration in determining their practicality. This paper proposes a memory-efficient solution to catastrophic forgetting, improving upon an established algorithm known as orthogonal gradient descent (OGD). OGD utilizes prior model gradients to find weight updates that preserve performance on prior datapoints. However, since the memory cost of storing prior model gradients grows with the runtime of the algorithm, OGD is ill-suited to continual learning over arbitrarily long time horizons. To address this problem, this paper proposes SketchOGD. SketchOGD employs an online sketching algorithm to compress model gradients as they are encountered into a matrix of a fix
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32676;&#19981;&#21464;GAN&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32676;&#19981;&#21464;&#20998;&#24067;&#26102;&#65292;&#32676;&#19981;&#21464;GAN&#25152;&#38656;&#26679;&#26412;&#25968;&#20250;&#25353;&#32676;&#20307;&#22823;&#23567;&#30340;&#24130;&#27604;&#20363;&#20943;&#23569;&#12290;</title><link>http://arxiv.org/abs/2305.13517</link><description>&lt;p&gt;
Group-Invariant GAN&#30340;&#32479;&#35745;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Statistical Guarantees of Group-Invariant GANs. (arXiv:2305.13517v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#32676;&#19981;&#21464;GAN&#30340;&#32479;&#35745;&#20445;&#35777;&#65292;&#21457;&#29616;&#24403;&#23398;&#20064;&#32676;&#19981;&#21464;&#20998;&#24067;&#26102;&#65292;&#32676;&#19981;&#21464;GAN&#25152;&#38656;&#26679;&#26412;&#25968;&#20250;&#25353;&#32676;&#20307;&#22823;&#23567;&#30340;&#24130;&#27604;&#20363;&#20943;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Group-Invariant&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;(GAN)&#26159;&#19968;&#31181;GAN&#65292;&#20854;&#20013;&#29983;&#25104;&#22120;&#21644;&#21028;&#21035;&#22120;&#20855;&#26377;&#30828;&#24615;&#38598;&#22242;&#23545;&#31216;&#24615;&#12290;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#20123;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#20855;&#26377;&#26174;&#30528;&#25913;&#36827;&#25968;&#25454;&#25928;&#29575;&#30340;&#38598;&#22242;&#19981;&#21464;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#20998;&#26512;&#32676;&#19981;&#21464;GAN&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#20943;&#23569;&#26469;&#20005;&#26684;&#37327;&#21270;&#36825;&#31181;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23398;&#20064;&#32676;&#19981;&#21464;&#20998;&#24067;&#26102;&#65292;&#32676;&#19981;&#21464;GAN&#25152;&#38656;&#26679;&#26412;&#25968;&#25353;&#29031;&#32676;&#20307;&#22823;&#23567;&#30340;&#24130;&#27604;&#20363;&#20943;&#23569;&#65292;&#36825;&#20010;&#24130;&#21462;&#20915;&#20110;&#20998;&#24067;&#25903;&#25345;&#30340;&#26412;&#36136;&#32500;&#24230;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#20010;&#20026;&#32676;&#19981;&#21464;&#29983;&#25104;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;GAN&#25552;&#20379;&#32479;&#35745;&#20272;&#35745;&#30340;&#24037;&#20316;&#65292;&#24182;&#21487;&#20197;&#20026;&#20854;&#20182;&#32676;&#19981;&#21464;&#29983;&#25104;&#27169;&#22411;&#30340;&#30740;&#31350;&#25552;&#20379;&#20511;&#37492;&#12290;
&lt;/p&gt;
&lt;p&gt;
Group-invariant generative adversarial networks (GANs) are a type of GANs in which the generators and discriminators are hardwired with group symmetries. Empirical studies have shown that these networks are capable of learning group-invariant distributions with significantly improved data efficiency. In this study, we aim to rigorously quantify this improvement by analyzing the reduction in sample complexity for group-invariant GANs. Our findings indicate that when learning group-invariant distributions, the number of samples required for group-invariant GANs decreases proportionally with a power of the group size, and this power depends on the intrinsic dimension of the distribution's support. To our knowledge, this work presents the first statistical estimation for group-invariant generative models, specifically for GANs, and it may shed light on the study of other group-invariant generative models.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;</title><link>http://arxiv.org/abs/2305.11857</link><description>&lt;p&gt;
Q-malizing&#27969;&#21644;&#26080;&#31351;&#23567;&#23494;&#24230;&#27604;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Q-malizing flow and infinitesimal density ratio estimation. (arXiv:2305.11857v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11857
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#20174;&#19968;&#20010;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#20219;&#24847;&#35775;&#38382;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#30340;Q&#30340;&#27969;&#27169;&#22411;&#12290;&#36825;&#20010;&#27169;&#22411;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#21487;&#20197;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#30340;&#27491;&#21017;&#21270;&#27969;&#22312;&#29983;&#25104;&#20219;&#21153;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20854;&#20013;&#27969;&#32593;&#32476;&#20174;&#25968;&#25454;&#20998;&#24067;P&#20256;&#36755;&#21040;&#27491;&#24577;&#20998;&#24067;&#12290;&#19968;&#31181;&#33021;&#22815;&#20174;P&#20256;&#36755;&#21040;&#20219;&#24847;Q&#30340;&#27969;&#27169;&#22411;&#65292;&#20854;&#20013;P&#21644;Q&#37117;&#21487;&#36890;&#36807;&#26377;&#38480;&#26679;&#26412;&#35775;&#38382;&#65292;&#23558;&#22312;&#21508;&#31181;&#24212;&#29992;&#20852;&#36259;&#20013;&#20351;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26368;&#36817;&#24320;&#21457;&#30340;&#26395;&#36828;&#38236;&#23494;&#24230;&#27604;&#20272;&#35745;&#20013;&#65288;DRE&#65289;&#65292;&#23427;&#38656;&#35201;&#26500;&#24314;&#20013;&#38388;&#23494;&#24230;&#20197;&#22312;P&#21644;Q&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#26679;&#30340;&#8220;Q-malizing&#27969;&#8221;&#65292;&#36890;&#36807;&#31070;&#32463;ODE&#27169;&#22411;&#36827;&#34892;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#32463;&#39564;&#26679;&#26412;&#30340;&#21487;&#36870;&#20256;&#36755;&#20174;P&#21040;Q&#65288;&#21453;&#20043;&#20134;&#28982;&#65289;&#65292;&#24182;&#36890;&#36807;&#26368;&#23567;&#21270;&#20256;&#36755;&#25104;&#26412;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#35757;&#32451;&#22909;&#30340;&#27969;&#27169;&#22411;&#20351;&#25105;&#20204;&#33021;&#22815;&#27839;&#19982;&#26102;&#38388;&#21442;&#25968;&#21270;&#30340;log&#23494;&#24230;&#36827;&#34892;&#26080;&#31351;&#23567;DRE&#65292;&#36890;&#36807;&#35757;&#32451;&#38468;&#21152;&#30340;&#36830;&#32493;&#26102;&#38388;&#27969;&#32593;&#32476;&#20351;&#29992;&#20998;&#31867;&#25439;&#22833;&#26469;&#20272;&#35745;log&#23494;&#24230;&#30340;&#26102;&#38388;&#20559;&#23548;&#25968;&#12290;&#36890;&#36807;&#31215;&#20998;&#26102;&#38388;&#24471;&#20998;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Continuous normalizing flows are widely used in generative tasks, where a flow network transports from a data distribution $P$ to a normal distribution. A flow model that can transport from $P$ to an arbitrary $Q$, where both $P$ and $Q$ are accessible via finite samples, would be of various application interests, particularly in the recently developed telescoping density ratio estimation (DRE) which calls for the construction of intermediate densities to bridge between $P$ and $Q$. In this work, we propose such a ``Q-malizing flow'' by a neural-ODE model which is trained to transport invertibly from $P$ to $Q$ (and vice versa) from empirical samples and is regularized by minimizing the transport cost. The trained flow model allows us to perform infinitesimal DRE along the time-parametrized $\log$-density by training an additional continuous-time flow network using classification loss, which estimates the time-partial derivative of the $\log$-density. Integrating the time-score network
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#24179;&#28369;&#20808;&#39564;&#30340;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20174;&#35266;&#23519;&#21040;&#30340;&#20449;&#21495;&#20013;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#20197;&#25429;&#33719;&#23454;&#20307;&#38388;&#30340;&#20869;&#22312;&#39640;&#38454;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2211.01717</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#24179;&#28369;&#20808;&#39564;&#23398;&#20064;&#20449;&#21495;&#30340;&#36229;&#22270;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning Hypergraphs From Signals With Dual Smoothness Prior. (arXiv:2211.01717v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#24179;&#28369;&#20808;&#39564;&#30340;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20174;&#35266;&#23519;&#21040;&#30340;&#20449;&#21495;&#20013;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#20197;&#25429;&#33719;&#23454;&#20307;&#38388;&#30340;&#20869;&#22312;&#39640;&#38454;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#26159;&#20174;&#35266;&#23519;&#21040;&#30340;&#20449;&#21495;&#20013;&#23398;&#20064;&#36229;&#22270;&#32467;&#26500;&#65292;&#20197;&#25429;&#25417;&#23454;&#20307;&#20043;&#38388;&#20869;&#22312;&#30340;&#39640;&#38454;&#20851;&#31995;&#65292;&#24403;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#21487;&#29992;&#30340;&#36229;&#22270;&#25299;&#25169;&#32467;&#26500;&#26102;&#65292;&#36825;&#21464;&#24471;&#38750;&#24120;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21452;&#37325;&#24179;&#28369;&#20808;&#39564;&#30340;&#36229;&#22270;&#32467;&#26500;&#23398;&#20064;&#26694;&#26550;HGSL&#65292;&#36890;&#36807;&#25226;&#27599;&#20010;&#36229;&#36793;&#19982;&#20855;&#26377;&#33410;&#28857;&#20449;&#21495;&#24179;&#28369;&#24615;&#21644;&#36793;&#36830;&#25509;&#24615;&#30340;&#23376;&#22270;&#23545;&#24212;&#36215;&#26469;&#65292;&#25581;&#31034;&#20102;&#35266;&#23519;&#21040;&#30340;&#33410;&#28857;&#20449;&#21495;&#21644;&#36229;&#22270;&#32467;&#26500;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hypergraph structure learning, which aims to learn the hypergraph structures from the observed signals to capture the intrinsic high-order relationships among the entities, becomes crucial when a hypergraph topology is not readily available in the datasets. There are two challenges that lie at the heart of this problem: 1) how to handle the huge search space of potential hyperedges, and 2) how to define meaningful criteria to measure the relationship between the signals observed on nodes and the hypergraph structure. In this paper, for the first challenge, we adopt the assumption that the ideal hypergraph structure can be derived from a learnable graph structure that captures the pairwise relations within signals. Further, we propose a hypergraph structure learning framework HGSL with a novel dual smoothness prior that reveals a mapping between the observed node signals and the hypergraph structure, whereby each hyperedge corresponds to a subgraph with both node signal smoothness and e
&lt;/p&gt;</description></item></channel></rss>