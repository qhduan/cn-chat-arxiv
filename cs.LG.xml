<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;</title><link>https://arxiv.org/abs/2403.13501</link><description>&lt;p&gt;
VSTAR&#65306;&#29992;&#20110;&#29983;&#25104;&#38271;&#21160;&#24577;&#35270;&#39057;&#21512;&#25104;&#30340;&#26102;&#38388;&#25252;&#29702;
&lt;/p&gt;
&lt;p&gt;
VSTAR: Generative Temporal Nursing for Longer Dynamic Video Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;VSTAR&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#24182;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#20174;&#32780;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25991;&#26412;&#21040;&#35270;&#39057;&#65288;T2V&#65289;&#21512;&#25104;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#65292;&#20294;&#24320;&#28304;&#30340;T2V&#25193;&#25955;&#27169;&#22411;&#38590;&#20197;&#29983;&#25104;&#20855;&#26377;&#21160;&#24577;&#21464;&#21270;&#21644;&#19981;&#26029;&#36827;&#21270;&#20869;&#23481;&#30340;&#36739;&#38271;&#35270;&#39057;&#12290;&#23427;&#20204;&#24448;&#24448;&#21512;&#25104;&#20934;&#38745;&#24577;&#35270;&#39057;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#25552;&#31034;&#20013;&#28041;&#21450;&#30340;&#24517;&#35201;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#35270;&#35273;&#21464;&#21270;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#25193;&#23637;&#21040;&#23454;&#29616;&#26356;&#38271;&#12289;&#26356;&#21160;&#24577;&#30340;&#35270;&#39057;&#21512;&#25104;&#24448;&#24448;&#22312;&#35745;&#31639;&#19978;&#38590;&#20197;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#29983;&#25104;&#26102;&#24207;&#25252;&#29702;&#65288;GTN&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21363;&#26102;&#25913;&#21464;&#29983;&#25104;&#36807;&#31243;&#65292;&#20197;&#25913;&#21892;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#25511;&#21046;&#65292;&#24182;&#23454;&#29616;&#29983;&#25104;&#26356;&#38271;&#30340;&#35270;&#39057;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;GTN&#26041;&#27861;&#65292;&#21517;&#20026;VSTAR&#65292;&#23427;&#21253;&#25324;&#20004;&#20010;&#20851;&#38190;&#35201;&#32032;&#65306;1&#65289;&#35270;&#39057;&#26775;&#27010;&#25552;&#31034;&#65288;VSP&#65289;-&#22522;&#20110;&#21407;&#22987;&#21333;&#20010;&#25552;&#31034;&#33258;&#21160;&#29983;&#25104;&#35270;&#39057;&#26775;&#27010;&#65292;&#21033;&#29992;LLMs&#25552;&#20379;&#20934;&#30830;&#30340;&#25991;&#26412;&#25351;&#23548;&#65292;&#20197;&#23454;&#29616;&#23545;&#26102;&#24207;&#21160;&#24577;&#30340;&#31934;&#30830;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13501v1 Announce Type: cross  Abstract: Despite tremendous progress in the field of text-to-video (T2V) synthesis, open-sourced T2V diffusion models struggle to generate longer videos with dynamically varying and evolving content. They tend to synthesize quasi-static videos, ignoring the necessary visual change-over-time implied in the text prompt. At the same time, scaling these models to enable longer, more dynamic video synthesis often remains computationally intractable. To address this challenge, we introduce the concept of Generative Temporal Nursing (GTN), where we aim to alter the generative process on the fly during inference to improve control over the temporal dynamics and enable generation of longer videos. We propose a method for GTN, dubbed VSTAR, which consists of two key ingredients: 1) Video Synopsis Prompting (VSP) - automatic generation of a video synopsis based on the original single prompt leveraging LLMs, which gives accurate textual guidance to differe
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#27700;&#24179;&#38598;&#20256;&#36755;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#27700;&#24179;&#38598;&#20256;&#36755;&#21487;&#20197;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#38656;&#35201;Hessian-vector products&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#35813;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03362</link><description>&lt;p&gt;
&#27700;&#24179;&#38598;&#20256;&#36755;&#65306;&#20248;&#21270;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Level Set Teleportation: An Optimization Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03362
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20174;&#20248;&#21270;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#27700;&#24179;&#38598;&#20256;&#36755;&#65292;&#35777;&#26126;&#20102;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#27700;&#24179;&#38598;&#20256;&#36755;&#21487;&#20197;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#21482;&#38656;&#35201;Hessian-vector products&#30340;&#26041;&#27861;&#39564;&#35777;&#20102;&#35813;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#27700;&#24179;&#38598;&#20256;&#36755;&#65292;&#36825;&#26159;&#19968;&#31181;&#20248;&#21270;&#23376;&#31243;&#24207;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#30446;&#26631;&#20989;&#25968;&#30340;&#27700;&#24179;&#38598;&#19978;&#26368;&#22823;&#21270;&#26799;&#24230;&#33539;&#25968;&#26469;&#21152;&#36895;&#26799;&#24230;&#26041;&#27861;&#12290;&#30001;&#20110;&#19979;&#38477;&#24341;&#29702;&#26263;&#31034;&#26799;&#24230;&#19979;&#38477;&#65288;GD&#65289;&#20351;&#30446;&#26631;&#20989;&#25968;&#25353;&#26799;&#24230;&#30340;&#24179;&#26041;&#33539;&#25968;&#19979;&#38477;&#65292;&#27700;&#24179;&#38598;&#20256;&#36755;&#26368;&#22823;&#21270;&#20102;&#36825;&#19968;&#27493;&#36827;&#20445;&#35777;&#12290;&#23545;&#20110;&#28385;&#36275;Hessian&#31283;&#23450;&#24615;&#30340;&#20984;&#20989;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;GD&#19982;&#27700;&#24179;&#38598;&#20256;&#36755;&#33719;&#24471;&#20102;&#32508;&#21512;&#30340;&#27425;&#32447;&#24615;/&#32447;&#24615;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#27604;&#22312;&#26368;&#20248;&#24615;&#24046;&#36317;&#36739;&#23567;&#26102;&#26631;&#20934;GD&#35201;&#24555;&#24471;&#22810;&#12290;&#19982;&#26631;&#20934;&#65288;&#24378;&#65289;&#20984;&#35774;&#32622;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27700;&#24179;&#38598;&#20256;&#36755;&#26082;&#19981;&#25913;&#21892;&#20063;&#19981;&#24694;&#21270;&#25910;&#25947;&#36895;&#24230;&#12290;&#20026;&#20102;&#23454;&#38469;&#35780;&#20272;&#20256;&#36755;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21482;&#38656;&#35201;Hessian-&#21521;&#37327;&#20056;&#31215;&#30340;&#25237;&#24433;&#26799;&#24230;&#31867;&#22411;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#26174;&#31034;&#65292;&#22914;&#26524;&#25552;&#20379;&#20102;&#26799;&#24230;&#35775;&#38382;&#26435;&#38480;&#65292;&#27700;&#24179;&#38598;&#20256;&#36755;&#26799;&#24230;&#26041;&#27861;&#20855;&#26377;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03362v1 Announce Type: new  Abstract: We study level set teleportation, an optimization sub-routine which seeks to accelerate gradient methods by maximizing the gradient norm on a level-set of the objective function. Since the descent lemma implies that gradient descent (GD) decreases the objective proportional to the squared norm of the gradient, level-set teleportation maximizes this one-step progress guarantee. For convex functions satisfying Hessian stability, we prove that GD with level-set teleportation obtains a combined sub-linear/linear convergence rate which is strictly faster than standard GD when the optimality gap is small. This is in sharp contrast to the standard (strongly) convex setting, where we show level-set teleportation neither improves nor worsens convergence rates. To evaluate teleportation in practice, we develop a projected-gradient-type method requiring only Hessian-vector products. We use this method to show that gradient methods with access to a 
&lt;/p&gt;</description></item><item><title>Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16562</link><description>&lt;p&gt;
Q-FOX&#23398;&#20064;&#65306;&#39072;&#35206;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-FOX Learning: Breaking Tradition in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16562
&lt;/p&gt;
&lt;p&gt;
Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20339;&#21160;&#20316;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#25110;&#30452;&#25509;&#30417;&#30563;&#30340;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-FOX&#30340;&#26032;&#39062;&#33258;&#21160;&#35843;&#21442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;FOX&#20248;&#21270;&#22120;&#21644;&#24120;&#29992;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;RL Q-learning&#31639;&#27861;&#35299;&#20915;&#20102;&#35843;&#21442;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#22870;&#21169;&#25918;&#22312;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#23398;&#20064;&#26102;&#38388;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Concrete&#20998;&#24067;&#20316;&#20026;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#20445;&#25345;&#31934;&#24230;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#20855;&#26377;&#26368;&#20248;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.13765</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#32479;&#35745;&#24314;&#27169;&#23454;&#29616;&#20445;&#25345;&#31934;&#24230;&#30340;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Accuracy-Preserving Calibration via Statistical Modeling on Probability Simplex
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13765
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;Concrete&#20998;&#24067;&#20316;&#20026;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#20445;&#25345;&#31934;&#24230;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20854;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#20855;&#26377;&#26368;&#20248;&#24615;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26679;&#26412;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#20998;&#31867;&#27169;&#22411;&#24517;&#39035;&#36827;&#34892;&#26657;&#20934;&#65292;&#20197;&#35780;&#20272;&#39044;&#27979;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#26657;&#20934;&#26041;&#27861;&#37319;&#29992;&#20102;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26657;&#20934;&#26041;&#27861;&#26080;&#27861;&#20445;&#25345;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#21363;&#20351;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#24456;&#39640;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Concrete&#20998;&#24067;&#20316;&#20026;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#30340;&#27010;&#29575;&#27169;&#22411;&#30340;&#20445;&#25345;&#31934;&#24230;&#30340;&#26657;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#65292;&#22312;&#20132;&#21449;&#29109;&#25439;&#22833;&#19978;&#35757;&#32451;&#30340;DNN&#27169;&#22411;&#20855;&#26377;Concrete&#20998;&#24067;&#21442;&#25968;&#30340;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#25104;&#29983;&#25104;&#26679;&#26412;&#65292;&#29992;&#20110;&#22312;&#27010;&#29575;&#21333;&#32431;&#24418;&#19978;&#35757;&#32451;&#27010;&#29575;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#20445;&#25345;&#26657;&#20934;&#20219;&#21153;&#19978;&#21487;&#20197;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13765v1 Announce Type: new  Abstract: Classification models based on deep neural networks (DNNs) must be calibrated to measure the reliability of predictions. Some recent calibration methods have employed a probabilistic model on the probability simplex. However, these calibration methods cannot preserve the accuracy of pre-trained models, even those with a high classification accuracy. We propose an accuracy-preserving calibration method using the Concrete distribution as the probabilistic model on the probability simplex. We theoretically prove that a DNN model trained on cross-entropy loss has optimality as the parameter of the Concrete distribution. We also propose an efficient method that synthetically generates samples for training probabilistic models on the probability simplex. We demonstrate that the proposed method can outperform previous methods in accuracy-preserving calibration tasks using benchmarks.
&lt;/p&gt;</description></item><item><title>StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2310.17042</link><description>&lt;p&gt;
StochGradAdam: &#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
StochGradAdam: Accelerating Neural Networks Training with Stochastic Gradient Sampling. (arXiv:2310.17042v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17042
&lt;/p&gt;
&lt;p&gt;
StochGradAdam&#26159;&#19968;&#31181;&#21033;&#29992;&#38543;&#26426;&#26799;&#24230;&#25277;&#26679;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#65292;&#33021;&#22815;&#31283;&#23450;&#25910;&#25947;&#65292;&#25552;&#21319;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;StochGradAdam&#20248;&#21270;&#22120;&#65292;&#36825;&#26159;&#23545;&#24191;&#21463;&#36190;&#35465;&#30340;Adam&#31639;&#27861;&#30340;&#26032;&#39062;&#25913;&#36827;&#12290;StochGradAdam&#30340;&#26680;&#24515;&#26159;&#20854;&#26799;&#24230;&#25277;&#26679;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#19981;&#20165;&#30830;&#20445;&#31283;&#23450;&#25910;&#25947;&#65292;&#32780;&#19988;&#21033;&#29992;&#36873;&#25321;&#24615;&#26799;&#24230;&#32771;&#34385;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#20943;&#36731;&#22122;&#22768;&#25110;&#24322;&#24120;&#25968;&#25454;&#30340;&#24433;&#21709;&#21644;&#22686;&#24378;&#25439;&#22833;&#20989;&#25968;&#31354;&#38388;&#30340;&#25506;&#32034;&#65292;&#25552;&#21319;&#20102;&#40065;&#26834;&#35757;&#32451;&#12290;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#20998;&#21106;&#20219;&#21153;&#20013;&#65292;StochGradAdam&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;Adam&#20248;&#21270;&#22120;&#30340;&#24615;&#33021;&#12290;&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#31934;&#24515;&#36873;&#25321;&#19968;&#37096;&#20998;&#26799;&#24230;&#36827;&#34892;&#25277;&#26679;&#65292;&#35813;&#20248;&#21270;&#22120;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#27169;&#22411;&#30340;&#31649;&#29702;&#12290;&#26412;&#25991;&#20174;&#25968;&#23398;&#22522;&#30784;&#21040;&#20559;&#24046;&#26657;&#27491;&#31574;&#30053;&#20840;&#38754;&#25506;&#35752;&#20102;StochGradAdam&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#25216;&#26415;&#30340;&#21487;&#26399;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing domain of deep learning optimization, this paper unveils the StochGradAdam optimizer, a novel adaptation of the well-regarded Adam algorithm. Central to StochGradAdam is its gradient sampling technique. This method not only ensures stable convergence but also leverages the advantages of selective gradient consideration, fostering robust training by potentially mitigating the effects of noisy or outlier data and enhancing the exploration of the loss landscape for more dependable convergence. In both image classification and segmentation tasks, StochGradAdam has demonstrated superior performance compared to the traditional Adam optimizer. By judiciously sampling a subset of gradients at each iteration, the optimizer is optimized for managing intricate models. The paper provides a comprehensive exploration of StochGradAdam's methodology, from its mathematical foundations to bias correction strategies, heralding a promising advancement in deep learning training tec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2310.04264</link><description>&lt;p&gt;
C(NN)FD -- &#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#27668;&#21160;&#24615;&#33021;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
C(NN)FD -- deep learning predictions of tip clearance variations on multi-stage axial compressors aerodynamic performance. (arXiv:2310.04264v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04264
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#39044;&#27979;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#24433;&#21709;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#65292;&#26041;&#20415;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36804;&#20170;&#20026;&#27490;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#35832;&#22914;CFD&#65288;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;&#65289;&#31561;&#29289;&#29702;&#27169;&#25311;&#22312;&#24037;&#19994;&#19978;&#30340;&#37325;&#35201;&#24615;&#26377;&#38480;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#32423;&#36724;&#21521;&#21387;&#32553;&#26426;&#22312;&#29123;&#27668;&#36718;&#26426;&#20013;&#23574;&#38388;&#38553;&#21464;&#21270;&#23545;&#27668;&#21160;&#24615;&#33021;&#30340;&#23454;&#26102;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#24320;&#21457;&#21644;&#24212;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;C(NN)FD&#26550;&#26500;&#32463;&#35777;&#26126;&#21487;&#25193;&#23637;&#33267;&#24037;&#19994;&#24212;&#29992;&#65292;&#24182;&#36798;&#21040;&#19982;CFD&#22522;&#20934;&#30456;&#23218;&#32654;&#30340;&#23454;&#26102;&#20934;&#30830;&#24615;&#12290;&#37096;&#32626;&#30340;&#27169;&#22411;&#21487;&#36731;&#26494;&#38598;&#25104;&#21040;&#29123;&#27668;&#36718;&#26426;&#30340;&#21046;&#36896;&#21644;&#26500;&#24314;&#36807;&#31243;&#20013;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#20998;&#26512;&#35780;&#20272;&#24615;&#33021;&#24433;&#21709;&#24182;&#28508;&#22312;&#20943;&#23569;&#26114;&#36149;&#29289;&#29702;&#27979;&#35797;&#35201;&#27714;&#30340;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application of deep learning methods to physical simulations such as CFD (Computational Fluid Dynamics), have been so far of limited industrial relevance. This paper demonstrates the development and application of a deep learning framework for real-time predictions of the impact of tip clearance variations on the aerodynamic performance of multi-stage axial compressors in gas turbines. The proposed C(NN)FD architecture is proven to be scalable to industrial applications, and achieves in real-time accuracy comparable to the CFD benchmark. The deployed model, is readily integrated within the manufacturing and build process of gas turbines, thus providing the opportunity to analytically assess the impact on performance and potentially reduce requirements for expensive physical tests.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30830;&#31435;&#20102;&#20854;&#39640;&#25928;&#21435;&#37327;&#21270;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;PQC&#26550;&#26500;&#35774;&#35745;&#24314;&#35758;&#21644;&#35782;&#21035;&#20102;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#30340;&#24517;&#35201;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.11647</link><description>&lt;p&gt;
&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Potential and limitations of random Fourier features for dequantizing quantum machine learning. (arXiv:2309.11647v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30830;&#31435;&#20102;&#20854;&#39640;&#25928;&#21435;&#37327;&#21270;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;PQC&#26550;&#26500;&#35774;&#35745;&#24314;&#35758;&#21644;&#35782;&#21035;&#20102;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#30340;&#24517;&#35201;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#26368;&#24191;&#27867;&#25506;&#32034;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#30446;&#21069;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#20854;&#20013;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#34987;&#29992;&#20316;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#33021;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#36827;&#34892;&#39640;&#25928;&#30340;&#21435;&#37327;&#21270;&#12290;&#26412;&#25991;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30830;&#31435;&#20102;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#25552;&#20379;&#39640;&#25928;&#21435;&#37327;&#21270;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#26550;&#26500;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#21450;&#35782;&#21035;&#20102;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#21462;&#24471;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#30340;&#24517;&#35201;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is arguably one of the most explored applications of near-term quantum devices. Much focus has been put on notions of variational quantum machine learning where parameterized quantum circuits (PQCs) are used as learning models. These PQC models have a rich structure which suggests that they might be amenable to efficient dequantization via random Fourier features (RFF). In this work, we establish necessary and sufficient conditions under which RFF does indeed provide an efficient dequantization of variational quantum machine learning for regression. We build on these insights to make concrete suggestions for PQC architecture design, and to identify structures which are necessary for a regression problem to admit a potential quantum advantage via PQC based optimization.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11256</link><description>&lt;p&gt;
&#22312;&#27714;&#35299;&#21338;&#24328;&#20013;&#30340;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Efficient Last-iterate Convergence Algorithms in Solving Games. (arXiv:2308.11256v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11256
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#27714;&#35299;&#21338;&#24328;&#20013;&#39640;&#25928;&#25910;&#25947;&#31639;&#27861;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#20998;&#26512;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#31639;&#27861;&#65292;&#20197;&#21450;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#24724;&#31639;&#27861;&#22312;&#23398;&#20064;&#20004;&#20154;&#38646;&#21644;&#26631;&#20934;&#22411;&#28216;&#25103;&#21644;&#25193;&#23637;&#22411;&#28216;&#25103;&#30340;&#32435;&#20160;&#22343;&#34913;&#20013;&#24456;&#21463;&#27426;&#36814;&#12290;&#26368;&#36817;&#30340;&#35768;&#22810;&#30740;&#31350;&#32771;&#34385;&#20102;&#26368;&#21518;&#19968;&#27425;&#36845;&#20195;&#25910;&#25947;&#30340;&#26080;&#24724;&#31639;&#27861;&#12290;&#20854;&#20013;&#65292;&#26368;&#26377;&#21517;&#30340;&#20004;&#20010;&#31639;&#27861;&#26159;&#20048;&#35266;&#26799;&#24230;&#19979;&#38477;&#19978;&#21319;&#65288;OGDA&#65289;&#21644;&#20048;&#35266;&#20056;&#27861;&#26435;&#37325;&#26356;&#26032;&#65288;OMWU&#65289;&#12290;&#28982;&#32780;&#65292;OGDA&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#24456;&#39640;&#12290;OMWU&#20855;&#26377;&#36739;&#20302;&#30340;&#27599;&#27425;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20294;&#23454;&#39564;&#24615;&#33021;&#36739;&#24046;&#65292;&#24182;&#19988;&#23427;&#30340;&#25910;&#25947;&#20165;&#22312;&#32435;&#20160;&#22343;&#34913;&#21807;&#19968;&#26102;&#25104;&#31435;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22870;&#21169;&#36716;&#21270;&#65288;RT&#65289;&#26694;&#26550;&#29992;&#20110;MWU&#65292;&#23427;&#28040;&#38500;&#20102;&#21807;&#19968;&#24615;&#26465;&#20214;&#65292;&#24182;&#19988;&#22312;&#19982;OMWU&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22522;&#20110;RT&#30340;&#31639;&#27861;&#22312;&#30456;&#21516;&#36845;&#20195;&#27425;&#25968;&#19979;&#34920;&#29616;&#19981;&#22914;OGDA&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#25910;&#25947;&#20445;&#35777;&#22522;&#20110;&#36830;&#32493;&#26102;&#38388;&#21453;&#39304;&#20551;&#35774;&#65292;&#36825;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#23545;RT&#26694;&#26550;&#36827;&#34892;&#20102;&#26356;&#35814;&#32454;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
No-regret algorithms are popular for learning Nash equilibrium (NE) in two-player zero-sum normal-form games (NFGs) and extensive-form games (EFGs). Many recent works consider the last-iterate convergence no-regret algorithms. Among them, the two most famous algorithms are Optimistic Gradient Descent Ascent (OGDA) and Optimistic Multiplicative Weight Update (OMWU). However, OGDA has high per-iteration complexity. OMWU exhibits a lower per-iteration complexity but poorer empirical performance, and its convergence holds only when NE is unique. Recent works propose a Reward Transformation (RT) framework for MWU, which removes the uniqueness condition and achieves competitive performance with OMWU. Unfortunately, RT-based algorithms perform worse than OGDA under the same number of iterations, and their convergence guarantee is based on the continuous-time feedback assumption, which does not hold in most scenarios. To address these issues, we provide a closer analysis of the RT framework, w
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#20195;&#29702;&#21464;&#37327;&#30340;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#21644;&#26368;&#22823;&#30697;&#32422;&#26463;&#30340;&#26041;&#27861;&#21487;&#20197;&#19968;&#33268;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2308.04585</link><description>&lt;p&gt;
&#20915;&#23450;&#24615;&#28151;&#28102;&#19979;&#30340;&#20869;&#26680;&#21333;&#19968;&#20195;&#29702;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Kernel Single Proxy Control for Deterministic Confounding. (arXiv:2308.04585v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#22312;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21333;&#19968;&#20195;&#29702;&#21464;&#37327;&#30340;&#20869;&#26680;&#26041;&#27861;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#22238;&#24402;&#21644;&#26368;&#22823;&#30697;&#32422;&#26463;&#30340;&#26041;&#27861;&#21487;&#20197;&#19968;&#33268;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#25104;&#21151;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#26410;&#35266;&#27979;&#28151;&#28102;&#22240;&#32032;&#30340;&#22240;&#26524;&#25928;&#24212;&#20272;&#35745;&#38382;&#39064;&#65292;&#20854;&#20013;&#25105;&#20204;&#35266;&#27979;&#21040;&#19982;&#28151;&#28102;&#22240;&#32032;&#30456;&#20851;&#30340;&#20195;&#29702;&#21464;&#37327;&#12290;&#23613;&#31649;&#20195;&#29702;&#22240;&#26524;&#23398;&#20064;&#65288;PCL&#65289;&#20351;&#29992;&#20004;&#20010;&#20195;&#29702;&#21464;&#37327;&#26469;&#24674;&#22797;&#30495;&#23454;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#25105;&#20204;&#35777;&#26126;&#22914;&#26524;&#32467;&#26524;&#26159;&#30830;&#23450;&#24615;&#29983;&#25104;&#30340;&#65292;&#21017;&#20351;&#29992;&#21333;&#20010;&#20195;&#29702;&#21464;&#37327;&#23601;&#36275;&#20197;&#36827;&#34892;&#22240;&#26524;&#20272;&#35745;&#65292;&#24182;&#27010;&#25324;&#20102;&#25511;&#21046;&#32467;&#26524;&#26657;&#20934;&#27861;&#65288;COCA&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#20869;&#26680;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#20004;&#38454;&#27573;&#22238;&#24402;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#26368;&#22823;&#30697;&#32422;&#26463;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#19968;&#33268;&#22320;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#36890;&#36807;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#23454;&#39564;&#25104;&#21151;&#22320;&#24674;&#22797;&#20102;&#22240;&#26524;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of causal effect estimation with an unobserved confounder, where we observe a proxy variable that is associated with the confounder. Although Proxy Causal Learning (PCL) uses two proxy variables to recover the true causal effect, we show that a single proxy variable is sufficient for causal estimation if the outcome is generated deterministically, generalizing Control Outcome Calibration Approach (COCA). We propose two kernel-based methods for this setting: the first based on the two-stage regression approach, and the second based on a maximum moment restriction approach. We prove that both approaches can consistently estimate the causal effect, and we empirically demonstrate that we can successfully recover the causal effect on a synthetic dataset.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2308.02000</link><description>&lt;p&gt;
&#20174;&#31070;&#32463;&#34920;&#31034;&#21040;&#31526;&#21495;&#30693;&#35782;&#30340;&#36807;&#28193;
&lt;/p&gt;
&lt;p&gt;
On the Transition from Neural Representation to Symbolic Knowledge. (arXiv:2308.02000v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02000
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#24605;&#32500;&#36827;&#34892;&#32467;&#21512;&#12290;&#36890;&#36807;&#23398;&#20064;&#36807;&#28193;&#34920;&#31034;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#65292;&#20197;&#21450;&#36890;&#36807;&#21338;&#24328;&#21644;&#24378;&#21270;&#23398;&#20064;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#23454;&#29616;&#23545;&#39640;&#32500;&#20449;&#24687;&#30340;&#21387;&#32553;&#21644;&#31526;&#21495;&#34920;&#31034;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#21512;&#31070;&#32463;&#34920;&#31034;&#19982;&#31526;&#21495;&#34920;&#31034;&#20043;&#38388;&#30340;&#24040;&#22823;&#24046;&#36317;&#21487;&#33021;&#20351;&#31526;&#21495;&#24605;&#32500;&#20174;&#26412;&#36136;&#19978;&#34701;&#20837;&#31070;&#32463;&#32593;&#32476;&#12290;&#21463;&#20154;&#31867;&#22914;&#20309;&#36880;&#28176;&#20174;&#36890;&#36807;&#30693;&#35273;&#21644;&#29615;&#22659;&#20132;&#20114;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#31526;&#21495;&#26500;&#24314;&#22797;&#26434;&#30340;&#31526;&#21495;&#34920;&#31034;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31070;&#32463;-&#31526;&#21495;&#36807;&#28193;&#23383;&#20856;&#23398;&#20064;&#65288;TDL&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;EM&#31639;&#27861;&#23398;&#20064;&#25968;&#25454;&#30340;&#36807;&#28193;&#34920;&#31034;&#65292;&#23558;&#36755;&#20837;&#30340;&#39640;&#32500;&#35270;&#35273;&#37096;&#20998;&#20449;&#24687;&#21387;&#32553;&#21040;&#19968;&#32452;&#24352;&#37327;&#20316;&#20026;&#31070;&#32463;&#21464;&#37327;&#65292;&#24182;&#33258;&#30417;&#30563;&#22320;&#21457;&#29616;&#38544;&#21547;&#30340;&#35859;&#35789;&#32467;&#26500;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#36755;&#20837;&#20998;&#35299;&#35270;&#20026;&#21512;&#20316;&#21338;&#24328;&#26469;&#23454;&#29616;&#26694;&#26550;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#35859;&#35789;&#65292;&#24182;&#36890;&#36807;RL&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#36827;&#19968;&#27493;&#35843;&#25972;&#23398;&#20064;&#21040;&#30340;&#21407;&#22411;&#65292;&#20197;&#34701;&#20837;&#20027;&#35266;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bridging the huge disparity between neural and symbolic representation can potentially enable the incorporation of symbolic thinking into neural networks from essence. Motivated by how human gradually builds complex symbolic representation from the prototype symbols that are learned through perception and environmental interactions. We propose a Neural-Symbolic Transitional Dictionary Learning (TDL) framework that employs an EM algorithm to learn a transitional representation of data that compresses high-dimension information of visual parts of an input into a set of tensors as neural variables and discover the implicit predicate structure in a self-supervised way. We implement the framework with a diffusion model by regarding the decomposition of input as a cooperative game, then learn predicates by prototype clustering. We additionally use RL enabled by the Markovian of diffusion models to further tune the learned prototypes by incorporating subjective factors. Extensive experiments 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#26694;&#26550;&#8212;&#8212;&#32771;&#34385;&#25919;&#31574;&#20248;&#21270;&#21738;&#20010;&#25928;&#29992;&#65292;&#23450;&#20041;&#20102;&#20449;&#24687;&#20215;&#20540;&#20844;&#24179;&#65292;&#25552;&#20986;&#19981;&#24212;&#20351;&#29992;&#19981;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#23454;&#29992;&#31243;&#24207;&#65292;&#24182;&#25506;&#35752;&#20102;&#20462;&#25913;&#23454;&#29992;&#31243;&#24207;&#20197;&#28385;&#36275;&#27492;&#20844;&#24179;&#26631;&#20934;&#21487;&#33021;&#23545;&#26368;&#20248;&#25919;&#31574;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2306.00636</link><description>&lt;p&gt;
&#19981;&#20844;&#24179;&#30340;&#23454;&#29992;&#31243;&#24207;&#21450;&#20854;&#25913;&#36827;&#30340;&#31532;&#19968;&#27493;
&lt;/p&gt;
&lt;p&gt;
Unfair Utilities and First Steps Towards Improving Them. (arXiv:2306.00636v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20844;&#24179;&#26694;&#26550;&#8212;&#8212;&#32771;&#34385;&#25919;&#31574;&#20248;&#21270;&#21738;&#20010;&#25928;&#29992;&#65292;&#23450;&#20041;&#20102;&#20449;&#24687;&#20215;&#20540;&#20844;&#24179;&#65292;&#25552;&#20986;&#19981;&#24212;&#20351;&#29992;&#19981;&#28385;&#36275;&#36825;&#19968;&#26631;&#20934;&#30340;&#23454;&#29992;&#31243;&#24207;&#65292;&#24182;&#25506;&#35752;&#20102;&#20462;&#25913;&#23454;&#29992;&#31243;&#24207;&#20197;&#28385;&#36275;&#27492;&#20844;&#24179;&#26631;&#20934;&#21487;&#33021;&#23545;&#26368;&#20248;&#25919;&#31574;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20844;&#24179;&#26631;&#20934;&#23545;&#25919;&#31574;&#25110;&#39044;&#27979;&#22120;&#30340;&#36873;&#25321;&#36827;&#34892;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#21516;&#30340;&#24605;&#32771;&#20844;&#24179;&#30340;&#26694;&#26550;&#65306;&#25105;&#20204;&#32771;&#34385;&#25919;&#31574;&#27491;&#22312;&#20248;&#21270;&#21738;&#20010;&#25928;&#29992;&#65292;&#32780;&#19981;&#26159;&#38480;&#21046;&#25919;&#31574;&#25110;&#39044;&#27979;&#22120;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#20449;&#24687;&#20215;&#20540;&#20844;&#24179;&#65292;&#24182;&#24314;&#35758;&#19981;&#20351;&#29992;&#19981;&#28385;&#36275;&#27492;&#26631;&#20934;&#30340;&#23454;&#29992;&#31243;&#24207;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#20462;&#25913;&#23454;&#29992;&#31243;&#24207;&#20197;&#28385;&#36275;&#36825;&#31181;&#20844;&#24179;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#36825;&#21487;&#33021;&#23545;&#30456;&#24212;&#30340;&#26368;&#20248;&#25919;&#31574;&#20135;&#29983;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many fairness criteria constrain the policy or choice of predictors. In this work, we propose a different framework for thinking about fairness: Instead of constraining the policy or choice of predictors, we consider which utility a policy is optimizing for. We define value of information fairness and propose to not use utilities that do not satisfy this criterion. We describe how to modify a utility to satisfy this fairness criterion and discuss the consequences this might have on the corresponding optimal policies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#22312;ReLU&#32593;&#32476;&#20013;&#28155;&#21152;&#32447;&#24615;&#23618;&#26377;&#21161;&#20110;&#36924;&#36817;&#20855;&#26377;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20302;&#34920;&#31034;&#25104;&#26412;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2305.15598</link><description>&lt;p&gt;
&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#23618;&#20419;&#36827;&#23398;&#20064;&#21333;&#25351;&#25968;&#21644;&#22810;&#25351;&#25968;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Linear Neural Network Layers Promote Learning Single- and Multiple-Index Models. (arXiv:2305.15598v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20559;&#35265;&#65292;&#21457;&#29616;&#22312;ReLU&#32593;&#32476;&#20013;&#28155;&#21152;&#32447;&#24615;&#23618;&#26377;&#21161;&#20110;&#36924;&#36817;&#20855;&#26377;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20302;&#34920;&#31034;&#25104;&#26412;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#28145;&#24230;&#22823;&#20110;&#20004;&#23618;&#30340;&#36807;&#24230;&#21442;&#25968;&#21270;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#21547;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#19968;&#31867;&#28145;&#24230;&#19981;&#21516;&#20294;&#23481;&#37327;&#30456;&#21516;&#30340;&#32593;&#32476;&#65292;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#30340;&#26174;&#24335;&#23450;&#20041;&#30340;&#34920;&#31034;&#25104;&#26412;&#12290;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35825;&#23548;&#30340;&#20989;&#25968;&#30340;&#34920;&#31034;&#25104;&#26412;&#26159;&#32593;&#32476;&#34920;&#31034;&#35813;&#20989;&#25968;&#25152;&#38656;&#30340;&#24179;&#26041;&#26435;&#37325;&#20043;&#21644;&#30340;&#26368;&#23567;&#20540;&#65307;&#23427;&#21453;&#26144;&#20102;&#19982;&#35813;&#26550;&#26500;&#30456;&#20851;&#30340;&#20989;&#25968;&#31354;&#38388;&#20559;&#24046;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23558;&#32447;&#24615;&#23618;&#28155;&#21152;&#21040;ReLU&#32593;&#32476;&#20250;&#20135;&#29983;&#19968;&#20010;&#34920;&#31034;&#25104;&#26412;&#65292;&#36825;&#26377;&#21033;&#20110;&#20351;&#29992;&#20004;&#23618;&#32593;&#32476;&#26469;&#36924;&#36817;&#30001;&#20302;&#31209;&#32447;&#24615;&#31639;&#23376;&#21644;&#20855;&#26377;&#20302;&#34920;&#31034;&#25104;&#26412;&#30340;&#20989;&#25968;&#32452;&#25104;&#30340;&#20989;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20197;&#26368;&#23567;&#30340;&#34920;&#31034;&#25104;&#26412;&#25311;&#21512;&#35757;&#32451;&#25968;&#25454;&#20250;&#24471;&#21040;&#19968;&#20010;&#19982;&#20302;&#32500;&#23376;&#31354;&#38388;&#22402;&#30452;&#26041;&#21521;&#36817;&#20046;&#24658;&#23450;&#30340;&#25554;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the implicit bias of overparameterized neural networks of depth greater than two layers. Our framework considers a family of networks of varying depths that all have the same capacity but different implicitly defined representation costs. The representation cost of a function induced by a neural network architecture is the minimum sum of squared weights needed for the network to represent the function; it reflects the function space bias associated with the architecture. Our results show that adding linear layers to a ReLU network yields a representation cost that favors functions that can be approximated by a low-rank linear operator composed with a function with low representation cost using a two-layer network. Specifically, using a neural network to fit training data with minimum representation cost yields an interpolating function that is nearly constant in directions orthogonal to a low-dimensional subspace. This means that the learned network will approximate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#37319;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38480;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NEOLITHIC&#30340;&#26032;&#22411;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#32553;&#23567;&#19979;&#38480;&#21644;&#29616;&#26377;&#31639;&#27861;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2305.07612</link><description>&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#19979;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#20013;&#30340;&#19979;&#38480;&#21644;&#21152;&#36895;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lower Bounds and Accelerated Algorithms in Distributed Stochastic Optimization with Communication Compression. (arXiv:2305.07612v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07612
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37319;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38480;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NEOLITHIC&#30340;&#26032;&#22411;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#32553;&#23567;&#19979;&#38480;&#21644;&#29616;&#26377;&#31639;&#27861;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#21387;&#32553;&#26159;&#20943;&#36731;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#20013;&#35745;&#31639;&#33410;&#28857;&#38388;&#20449;&#24687;&#20132;&#25442;&#37327;&#30340;&#37325;&#35201;&#31574;&#30053;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;&#36890;&#20449;&#21387;&#32553;&#30340;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#19979;&#38480;&#65292;&#24182;&#20851;&#27880;&#20004;&#31181;&#20027;&#35201;&#31867;&#22411;&#30340;&#21387;&#32553;&#22120;&#65306;&#26080;&#20559;&#21644;&#21387;&#32553;&#22411;&#65292;&#24182;&#35299;&#20915;&#20102;&#21487;&#20197;&#36890;&#36807;&#36825;&#20123;&#21387;&#32553;&#22120;&#33719;&#24471;&#30340;&#26368;&#20339;&#25910;&#25947;&#36895;&#29575;&#38382;&#39064;&#12290;&#26412;&#25991;&#38024;&#23545;&#20845;&#31181;&#19981;&#21516;&#35774;&#32622;&#65292;&#32467;&#21512;&#24378;&#20984;&#12289;&#19968;&#33324;&#20984;&#25110;&#38750;&#20984;&#20989;&#25968;&#65292;&#24182;&#29992;&#26080;&#20559;&#25110;&#21387;&#32553;&#22411;&#21387;&#32553;&#22120;&#24314;&#31435;&#20102;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#30340;&#25910;&#25947;&#36895;&#29575;&#19979;&#38480;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NEOLITHIC&#30340;&#26032;&#22411;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#65292;&#36890;&#36807;&#21152;&#36895;&#25910;&#25947;&#36895;&#29575;&#30456;&#27604;&#32463;&#20856;&#26041;&#27861;&#65292;&#32553;&#23567;&#20102;&#19979;&#38480;&#21644;&#29616;&#26377;&#31639;&#27861;&#30340;&#24046;&#36317;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#25968;&#20540;&#23454;&#39564;&#25552;&#20379;&#20102;&#20851;&#20110;&#20998;&#24067;&#24335;&#38543;&#26426;&#20248;&#21270;&#20013;&#36890;&#20449;&#21387;&#32553;&#31639;&#27861;&#30340;&#26368;&#20248;&#24615;&#33021;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication compression is an essential strategy for alleviating communication overhead by reducing the volume of information exchanged between computing nodes in large-scale distributed stochastic optimization. Although numerous algorithms with convergence guarantees have been obtained, the optimal performance limit under communication compression remains unclear.  In this paper, we investigate the performance limit of distributed stochastic optimization algorithms employing communication compression. We focus on two main types of compressors, unbiased and contractive, and address the best-possible convergence rates one can obtain with these compressors. We establish the lower bounds for the convergence rates of distributed stochastic optimization in six different settings, combining strongly-convex, generally-convex, or non-convex functions with unbiased or contractive compressor types. To bridge the gap between lower bounds and existing algorithms' rates, we propose NEOLITHIC, a n
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#23558;&#22240;&#26524;&#24615;&#21407;&#21017;&#34701;&#20837;DGM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;</title><link>http://arxiv.org/abs/2301.12351</link><description>&lt;p&gt;
&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Emerging Synergies in Causality and Deep Generative Models: A Survey. (arXiv:2301.12351v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12351
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20043;&#38388;&#30340;&#26032;&#20852;&#21327;&#21516;&#20316;&#29992;&#65292;&#38416;&#26126;&#20102;&#23558;&#22240;&#26524;&#24615;&#21407;&#21017;&#34701;&#20837;DGM&#20013;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#24212;&#29992;&#22240;&#26524;&#24615;&#30340;&#30740;&#31350;&#21069;&#27839;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#20102;&#35299;&#21644;&#24314;&#27169;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65288;DGP&#65289;&#30340;&#36861;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65288;DGM&#65289;&#22312;&#25429;&#25417;&#22797;&#26434;&#25968;&#25454;&#20998;&#24067;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36890;&#24120;&#22312;&#27867;&#21270;&#33021;&#21147;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#34920;&#29616;&#19981;&#36275;&#12290;&#32780;&#22240;&#26524;&#24615;&#21017;&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#30340;&#26041;&#27861;&#26469;&#29702;&#35299;&#39537;&#21160;&#25968;&#25454;&#29983;&#25104;&#30340;&#26426;&#21046;&#65292;&#24182;&#31361;&#26174;&#20102;&#36825;&#20123;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#22240;&#26524;&#25928;&#24212;&#21160;&#21147;&#23398;&#12290;&#34429;&#28982;&#22240;&#26524;&#24615;&#22312;&#21487;&#35299;&#37322;&#24615;&#21644;&#22806;&#25512;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#21364;&#38754;&#20020;&#30528;&#39640;&#32500;&#31354;&#38388;&#20013;&#30340;&#22797;&#26434;&#24615;&#12290;&#24847;&#35782;&#21040;&#23427;&#20204;&#20043;&#38388;&#30340;&#21327;&#21516;&#28508;&#21147;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#22240;&#26524;&#24615;&#21644;DGM&#30340;&#20132;&#27719;&#28857;&#12290;&#25105;&#20204;&#38416;&#26126;&#20102;&#22240;&#26524;&#24615;&#21407;&#21017;&#22312;DGM&#20013;&#30340;&#25972;&#21512;&#65292;&#25506;&#35752;&#20102;&#20351;&#29992;DGM&#36827;&#34892;&#22240;&#26524;&#35782;&#21035;&#30340;&#26041;&#27861;&#65292;&#24182;&#23545;&#22240;&#26524;&#24615;&#22312;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26032;&#20852;&#30740;&#31350;&#21069;&#27839;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20013;&#30340;&#29983;&#25104;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#26041;&#27861;&#35770;&#65292;&#31361;&#20986;&#20102;&#24320;&#25918;&#30340;&#25361;&#25112;&#21644;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of artificial intelligence (AI), the quest to understand and model data-generating processes (DGPs) is of paramount importance. Deep generative models (DGMs) have proven adept in capturing complex data distributions but often fall short in generalization and interpretability. On the other hand, causality offers a structured lens to comprehend the mechanisms driving data generation and highlights the causal-effect dynamics inherent in these processes. While causality excels in interpretability and the ability to extrapolate, it grapples with intricacies of high-dimensional spaces. Recognizing the synergistic potential, we delve into the confluence of causality and DGMs. We elucidate the integration of causal principles within DGMs, investigate causal identification using DGMs, and navigate an emerging research frontier of causality in large-scale generative models, particularly generative large language models (LLMs). We offer insights into methodologies, highlight open cha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2208.06648</link><description>&lt;p&gt;
&#22312;&#20020;&#24202;&#23384;&#22312;&#19979;&#30340;&#22635;&#34917;&#31574;&#30053;&#65306;&#23545;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness. (arXiv:2208.06648v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06648
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21487;&#33021;&#20250;&#24378;&#21270;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#65292;&#32780;&#25105;&#20204;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#25552;&#20986;&#65292;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#20869;&#23481;&#20063;&#20250;&#20135;&#29983;&#20559;&#35265;&#12290;&#22312;&#21307;&#30103;&#39046;&#22495;&#65292;&#20559;&#35265;&#24050;&#32463;&#22312;&#21307;&#30103;&#21382;&#21490;&#19978;&#30041;&#19979;&#20102;&#28145;&#28145;&#30340;&#28889;&#21360;&#65292;&#23548;&#33268;&#36793;&#32536;&#21270;&#32676;&#20307;&#21463;&#21040;&#19981;&#24179;&#31561;&#30340;&#25252;&#29702;&#12290;&#32570;&#22833;&#25968;&#25454;&#20013;&#30340;&#27169;&#24335;&#36890;&#24120;&#21453;&#26144;&#20102;&#36825;&#20123;&#32676;&#20307;&#30340;&#24046;&#24322;&#65292;&#20294;&#26159;&#29305;&#23450;&#32676;&#20307;&#32570;&#22833;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#24433;&#21709;&#36824;&#19981;&#22826;&#28165;&#26970;&#12290;&#23613;&#31649;&#20854;&#28508;&#22312;&#24433;&#21709;&#24040;&#22823;&#65292;&#20294;&#22635;&#34917;&#24448;&#24448;&#34987;&#24573;&#35270;&#20026;&#19968;&#20010;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#32780;&#20851;&#27880;&#28857;&#25918;&#22312;&#20102;&#37325;&#24314;&#35823;&#24046;&#30340;&#20943;&#23569;&#21644;&#25972;&#20307;&#24615;&#33021;&#19978;&#65292;&#24573;&#30053;&#20102;&#22635;&#34917;&#22914;&#20309;&#23545;&#19981;&#21516;&#32676;&#20307;&#20135;&#29983;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#22635;&#34917;&#36873;&#25321;&#23545;&#19981;&#21516;&#32676;&#20307;&#30340;&#37325;&#24314;&#35823;&#24046;&#21644;&#19979;&#28216;&#39044;&#27979;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#23646;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning risks reinforcing biases present in data, and, as we argue in this work, in what is absent from data. In healthcare, biases have marked medical history, leading to unequal care affecting marginalised groups. Patterns in missing data often reflect these group discrepancies, but the algorithmic fairness implications of group-specific missingness are not well understood. Despite its potential impact, imputation is often an overlooked preprocessing step, with attention placed on the reduction of reconstruction error and overall performance, ignoring how imputation can affect groups differently. Our work studies how imputation choices affect reconstruction errors across groups and algorithmic fairness properties of downstream predictions.
&lt;/p&gt;</description></item></channel></rss>