<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16677</link><description>&lt;p&gt;
FOOL: &#29992;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#35299;&#20915;&#21355;&#26143;&#35745;&#31639;&#20013;&#30340;&#19979;&#34892;&#29942;&#39048;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16677
&lt;/p&gt;
&lt;p&gt;
FOOL&#26159;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12289;&#23884;&#20837;&#19978;&#19979;&#25991;&#21644;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20256;&#24863;&#22120;&#30340;&#32435;&#21355;&#26143;&#26143;&#24231;&#25429;&#33719;&#22823;&#33539;&#22260;&#22320;&#29702;&#21306;&#22495;&#65292;&#20026;&#22320;&#29699;&#35266;&#27979;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#12290;&#38543;&#30528;&#26143;&#24231;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#32593;&#32476;&#20105;&#29992;&#24418;&#25104;&#20102;&#19979;&#34892;&#29942;&#39048;&#12290;&#36712;&#36947;&#36793;&#32536;&#35745;&#31639;&#65288;OEC&#65289;&#21033;&#29992;&#26377;&#38480;&#30340;&#26426;&#36733;&#35745;&#31639;&#36164;&#28304;&#36890;&#36807;&#22312;&#28304;&#22836;&#22788;&#29702;&#21407;&#22987;&#25429;&#33719;&#26469;&#20943;&#23569;&#20256;&#36755;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#31895;&#31961;&#30340;&#36807;&#28388;&#26041;&#27861;&#25110;&#36807;&#20998;&#20248;&#20808;&#32771;&#34385;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#65292;&#30446;&#21069;&#30340;&#35299;&#20915;&#26041;&#26696;&#20855;&#26377;&#26377;&#38480;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FOOL&#65292;&#19968;&#31181;OEC&#26412;&#22320;&#21644;&#20219;&#21153;&#19981;&#21487;&#30693;&#30340;&#29305;&#24449;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20445;&#30041;&#39044;&#27979;&#24615;&#33021;&#12290;FOOL&#23558;&#39640;&#20998;&#36776;&#29575;&#21355;&#26143;&#22270;&#20687;&#36827;&#34892;&#20998;&#21306;&#65292;&#20197;&#26368;&#22823;&#21270;&#21534;&#21520;&#37327;&#12290;&#27492;&#22806;&#65292;&#23427;&#23884;&#20837;&#19978;&#19979;&#25991;&#24182;&#21033;&#29992;&#29943;&#30742;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#20197;&#36739;&#20302;&#30340;&#24320;&#38144;&#38477;&#20302;&#20256;&#36755;&#25104;&#26412;&#12290;&#34429;&#28982;FOOL&#26159;&#19968;&#31181;&#29305;&#24449;&#21387;&#32553;&#22120;&#65292;&#20294;&#23427;&#21487;&#20197;&#22312;&#20302;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16677v1 Announce Type: new  Abstract: Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks.   This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on perceptual quality measures at low
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27010;&#29575;&#22635;&#20805;&#25935;&#24863;&#29305;&#24449;&#65292;&#32852;&#21512;&#23398;&#20064;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#27010;&#29575;&#65292;&#22686;&#24378;&#19968;&#33324;&#20844;&#24179;&#39118;&#38505;&#65292;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#25913;&#36827;&#24179;&#34913;</title><link>https://arxiv.org/abs/2402.13393</link><description>&lt;p&gt;
&#38024;&#23545;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#30340;&#20844;&#24179;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Fairness Risks for Group-conditionally Missing Demographics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13393
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#22635;&#20805;&#25935;&#24863;&#29305;&#24449;&#65292;&#32852;&#21512;&#23398;&#20064;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#27010;&#29575;&#65292;&#22686;&#24378;&#19968;&#33324;&#20844;&#24179;&#39118;&#38505;&#65292;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#25913;&#36827;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20844;&#24179;&#24847;&#35782;&#30340;&#20998;&#31867;&#27169;&#22411;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#22240;&#20026;&#23545;&#26576;&#20123;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#30340;&#27495;&#35270;&#38382;&#39064;&#26085;&#30410;&#24341;&#36215;&#25285;&#24551;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#35201;&#27714;&#23436;&#20840;&#20102;&#35299;&#25935;&#24863;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#30001;&#20110;&#38544;&#31169;&#12289;&#27861;&#24459;&#38382;&#39064;&#21644;&#20010;&#20154;&#23545;&#27495;&#35270;&#30340;&#24656;&#24807;&#32780;&#19981;&#20999;&#23454;&#38469;&#12290;&#25105;&#20204;&#23558;&#35299;&#20915;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#19981;&#21487;&#29992;&#24615;&#30340;&#32676;&#20307;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#65292;&#26576;&#20123;&#24180;&#40836;&#33539;&#22260;&#30340;&#20154;&#21487;&#33021;&#26356;&#19981;&#24895;&#36879;&#38706;&#20182;&#20204;&#30340;&#24180;&#40836;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#36807;&#23545;&#25935;&#24863;&#29305;&#24449;&#36827;&#34892;&#27010;&#29575;&#22635;&#20805;&#65292;&#21516;&#26102;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#20013;&#32852;&#21512;&#23398;&#20064;&#32676;&#20307;&#26465;&#20214;&#24615;&#32570;&#22833;&#30340;&#27010;&#29575;&#65292;&#23558;&#19968;&#33324;&#20844;&#24179;&#39118;&#38505;&#19982;&#20043;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22270;&#20687;&#21644;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#25928;&#24615;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#20043;&#38388;&#30340;&#25913;&#36827;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13393v1 Announce Type: new  Abstract: Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual's fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07033</link><description>&lt;p&gt;
Fiddler&#65306;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#24555;&#36895;&#25512;&#26029;&#30340;CPU-GPU&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Mixture-of-Experts&#65288;MoE&#65289;&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#65292;&#21363;GPU&#20869;&#23384;&#36164;&#28304;&#19981;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23558;&#27169;&#22411;&#26435;&#37325;&#21368;&#36733;&#21040;CPU&#20869;&#23384;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#39057;&#32321;&#22320;&#22312;CPU&#21644;GPU&#20043;&#38388;&#31227;&#21160;&#25968;&#25454;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;MoE&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#23454;&#29616;&#20102;CPU-GPU&#32534;&#25490;&#12290;Fiddler&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;CPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#26368;&#23567;&#21270;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Fiddler&#33021;&#22815;&#22312;&#21333;&#20010;&#20855;&#26377;24GB&#20869;&#23384;&#30340;GPU&#19978;&#36816;&#34892;&#26410;&#21387;&#32553;&#30340;Mixtral-8x7B&#27169;&#22411;&#65288;&#21442;&#25968;&#36229;&#36807;90GB&#65289;&#65292;&#27599;&#31186;&#29983;&#25104;&#36229;&#36807;3&#20010;token&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;Fiddler&#30340;&#20195;&#30721;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/efeslab/fiddler}
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;</title><link>https://arxiv.org/abs/2311.06840</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#25512;&#26029;&#20013;&#30340;&#36951;&#28431;&#26631;&#31614;: &#19968;&#39033;&#20851;&#20110;&#24726;&#35770;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Omitted Labels in Causality: A Study of Paradoxes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25152;&#35859;&#30340;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#24182;&#21033;&#29992;&#24726;&#35770;&#23637;&#31034;&#20102;&#22312;&#36825;&#31181;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#21457;&#29616;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#24517;&#39035;&#20351;&#29992;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#36827;&#34892;&#27491;&#30830;&#30340;&#26657;&#27491;&#12290;&#27492;&#22806;&#65292;&#30740;&#31350;&#36824;&#21457;&#29616;&#20102;&#32467;&#35770;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#20043;&#38388;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#25105;&#20204;&#25152;&#31216;&#20043;&#20026;&#8220;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#8221;&#30340;&#27010;&#24565;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#20165;&#38480;&#20110;&#21487;&#33021;&#26631;&#31614;&#30340;&#19968;&#20010;&#23376;&#38598;&#12290;&#36825;&#31181;&#35774;&#32622;&#22312;&#19987;&#19994;&#20154;&#22763;&#25110;&#29305;&#23450;&#30340;&#19987;&#27880;&#30740;&#31350;&#20013;&#38750;&#24120;&#26222;&#36941;&#12290;&#25105;&#20204;&#21033;&#29992;&#24050;&#24191;&#27867;&#30740;&#31350;&#30340;&#24726;&#35770;&#65288;&#36763;&#26222;&#26862;&#24726;&#35770;&#21644;&#24247;&#22810;&#22622;&#24726;&#35770;&#65289;&#26469;&#35828;&#26126;&#22312;&#36951;&#28431;&#26631;&#31614;&#19978;&#19979;&#25991;&#20013;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#30340;&#26356;&#26222;&#36941;&#22256;&#38590;&#12290;&#19982;&#22240;&#26524;&#25512;&#26029;&#22522;&#26412;&#21407;&#29702;&#30456;&#21453;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#8220;&#27491;&#30830;&#8221;&#30340;&#26657;&#27491;&#26377;&#26102;&#38656;&#35201;&#38750;&#21487;&#20132;&#25442;&#30340;&#22788;&#29702;&#32452;&#21644;&#23545;&#29031;&#32452;&#12290;&#36825;&#20123;&#38519;&#38449;&#24341;&#23548;&#25105;&#20204;&#30740;&#31350;&#19981;&#21516;&#19978;&#19979;&#25991;&#20013;&#24471;&#20986;&#30340;&#32467;&#35770;&#32593;&#32476;&#21644;&#20854;&#24418;&#25104;&#30340;&#32467;&#26500;&#65292;&#20174;&#32780;&#35777;&#26126;&#20102;&#36825;&#20123;&#32593;&#32476;&#19982;&#31038;&#20250;&#36873;&#25321;&#29702;&#35770;&#30340;&#26377;&#36259;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore what we call ``omitted label contexts,'' in which training data is limited to a subset of the possible labels. This setting is common among specialized human experts or specific focused studies. We lean on well-studied paradoxes (Simpson's and Condorcet) to illustrate the more general difficulties of causal inference in omitted label contexts. Contrary to the fundamental principles on which much of causal inference is built, we show that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. These pitfalls lead us to the study networks of conclusions drawn from different contexts and the structures the form, proving an interesting connection between these networks and social choice theory.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21512;&#25104;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#26102;&#36827;&#34892;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#12290;</title><link>http://arxiv.org/abs/2312.12028</link><description>&lt;p&gt;
EyePreserve: &#20445;&#25345;&#36523;&#20221;&#30340;&#34425;&#33180;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
EyePreserve: Identity-Preserving Iris Synthesis. (arXiv:2312.12028v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.12028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#65292;&#33021;&#22815;&#21512;&#25104;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#26102;&#36827;&#34892;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#30643;&#23380;&#23610;&#23544;&#33539;&#22260;&#20869;&#20445;&#25345;&#36523;&#20221;&#30340;&#21516;&#36523;&#20221;&#29983;&#29289;&#29305;&#24449;&#34425;&#33180;&#22270;&#20687;&#30340;&#21512;&#25104;&#26159;&#22797;&#26434;&#30340;&#65292;&#22240;&#20026;&#23427;&#28041;&#21450;&#21040;&#34425;&#33180;&#32908;&#32905;&#25910;&#32553;&#26426;&#21046;&#65292;&#38656;&#35201;&#23558;&#34425;&#33180;&#38750;&#32447;&#24615;&#32441;&#29702;&#21464;&#24418;&#27169;&#22411;&#23884;&#20837;&#21040;&#21512;&#25104;&#27969;&#31243;&#20013;&#12290;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23436;&#20840;&#25968;&#25454;&#39537;&#21160;&#30340;&#12289;&#20445;&#25345;&#36523;&#20221;&#30340;&#12289;&#30643;&#23380;&#23610;&#23544;&#21464;&#21270;&#30340;&#34425;&#33180;&#22270;&#20687;&#21512;&#25104;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#21512;&#25104;&#20855;&#26377;&#19981;&#21516;&#30643;&#23380;&#23610;&#23544;&#30340;&#34425;&#33180;&#22270;&#20687;&#65292;&#20195;&#34920;&#19981;&#23384;&#22312;&#30340;&#36523;&#20221;&#65292;&#24182;&#33021;&#22815;&#22312;&#32473;&#23450;&#30446;&#26631;&#34425;&#33180;&#22270;&#20687;&#30340;&#20998;&#21106;&#25513;&#33180;&#19979;&#38750;&#32447;&#24615;&#22320;&#21464;&#24418;&#29616;&#26377;&#20027;&#20307;&#30340;&#34425;&#33180;&#22270;&#20687;&#32441;&#29702;&#12290;&#34425;&#33180;&#35782;&#21035;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#21464;&#24418;&#27169;&#22411;&#19981;&#20165;&#22312;&#25913;&#21464;&#30643;&#23380;&#23610;&#23544;&#26102;&#20445;&#25345;&#36523;&#20221;&#65292;&#32780;&#19988;&#22312;&#30643;&#23380;&#23610;&#23544;&#26377;&#26174;&#33879;&#24046;&#24322;&#30340;&#21516;&#36523;&#20221;&#34425;&#33180;&#26679;&#26412;&#20043;&#38388;&#25552;&#20379;&#26356;&#22909;&#30340;&#30456;&#20284;&#24230;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#32447;&#24615;&#26041;&#27861;&#30456;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthesis of same-identity biometric iris images, both for existing and non-existing identities while preserving the identity across a wide range of pupil sizes, is complex due to intricate iris muscle constriction mechanism, requiring a precise model of iris non-linear texture deformations to be embedded into the synthesis pipeline. This paper presents the first method of fully data-driven, identity-preserving, pupil size-varying s ynthesis of iris images. This approach is capable of synthesizing images of irises with different pupil sizes representing non-existing identities as well as non-linearly deforming the texture of iris images of existing subjects given the segmentation mask of the target iris image. Iris recognition experiments suggest that the proposed deformation model not only preserves the identity when changing the pupil size but offers better similarity between same-identity iris samples with significant differences in pupil size, compared to state-of-the-art linear an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEXR&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#31639;&#27861;&#22312;&#35299;&#20915;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#26102;&#20855;&#26377;&#20248;&#36234;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2312.02277</link><description>&lt;p&gt;
ALEXR:&#19968;&#31181;&#29992;&#20110;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#30340;&#26368;&#20248;&#21333;&#24490;&#29615;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
ALEXR: An Optimal Single-Loop Algorithm for Convex Finite-Sum Coupled Compositional Stochastic Optimization. (arXiv:2312.02277v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02277
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ALEXR&#30340;&#39640;&#25928;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#38382;&#39064;&#12290;&#27492;&#31639;&#27861;&#22312;&#35299;&#20915;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#38382;&#39064;&#26102;&#20855;&#26377;&#20248;&#36234;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#19988;&#21487;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#12289;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#12289;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#19968;&#31867;&#20855;&#26377;&#22810;&#20010;&#24212;&#29992;&#30340;&#20984;&#26377;&#38480;&#21644;&#32806;&#21512;&#32452;&#25104;&#38543;&#26426;&#20248;&#21270;&#65288;cFCCO&#65289;&#38382;&#39064;&#65292;&#21253;&#25324;&#32452;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65288;GDRO&#65289;&#65292;&#19981;&#24179;&#34913;&#25968;&#25454;&#23398;&#20064;&#65292;&#24378;&#21270;&#23398;&#20064;&#21644;&#25490;&#24207;&#23398;&#20064;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#24490;&#29615;&#21407;&#22987;-&#23545;&#20598;&#22359;&#22352;&#26631;&#36817;&#31471;&#31639;&#27861;&#65292;&#31216;&#20026;ALEXR&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#22359;&#22352;&#26631;&#38543;&#26426;&#38236;&#20687;&#19978;&#21319;&#26356;&#26032;&#23545;&#20598;&#21464;&#37327;&#21644;&#38543;&#26426;&#36817;&#31471;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#21407;&#22987;&#21464;&#37327;&#12290;&#25105;&#20204;&#22312;&#24179;&#28369;&#21644;&#38750;&#24179;&#28369;&#20989;&#25968;&#26465;&#20214;&#19979;&#24314;&#31435;&#20102;ALEXR&#22312;&#20984;&#21644;&#24378;&#20984;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#36825;&#19981;&#20165;&#25913;&#36827;&#20102;&#20197;&#21069;&#22312;&#24179;&#28369;cFCCO&#38382;&#39064;&#19978;&#30340;&#26368;&#20339;&#36895;&#24230;&#65292;&#36824;&#25193;&#23637;&#20102;cFCCO&#30340;&#33539;&#22260;&#65292;&#29992;&#20110;&#35299;&#20915;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38750;&#24179;&#28369;&#38382;&#39064;&#65292;&#22914;GDRO&#30340;&#23545;&#20598;&#24418;&#24335;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36739;&#20302;&#30340;&#22797;&#26434;&#24615;&#19979;&#30028;&#65292;&#20197;&#35777;&#26126;&#31639;&#27861;&#20855;&#26377;&#24456;&#24378;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper revisits a class of convex Finite-Sum Coupled Compositional Stochastic Optimization (cFCCO) problems with many applications, including group distributionally robust optimization (GDRO), learning with imbalanced data, reinforcement learning, and learning to rank. To better solve these problems, we introduce an efficient single-loop primal-dual block-coordinate proximal algorithm, dubbed ALEXR. This algorithm leverages block-coordinate stochastic mirror ascent updates for the dual variable and stochastic proximal gradient descent updates for the primal variable. We establish the convergence rates of ALEXR in both convex and strongly convex cases under smoothness and non-smoothness conditions of involved functions, which not only improve the best rates in previous works on smooth cFCCO problems but also expand the realm of cFCCO for solving more challenging non-smooth problems such as the dual form of GDRO. Finally, we present lower complexity bounds to demonstrate that the con
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20248;&#28857;&#26159;&#36991;&#20813;&#20102;&#22312;&#25972;&#20010;&#21487;&#34892;&#38598;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#19988;&#21033;&#29992;&#36895;&#24230;&#26469;&#34920;&#36798;&#32422;&#26463;&#65292;&#20351;&#24471;&#31639;&#27861;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#21644;&#32422;&#26463;&#25968;&#37327;&#19978;&#30340;&#22797;&#26434;&#24230;&#22686;&#38271;&#36866;&#24230;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.00316</link><description>&lt;p&gt;
&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accelerated First-Order Optimization under Nonlinear Constraints. (arXiv:2302.00316v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00316
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#65292;&#20854;&#20248;&#28857;&#26159;&#36991;&#20813;&#20102;&#22312;&#25972;&#20010;&#21487;&#34892;&#38598;&#19978;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#19988;&#21033;&#29992;&#36895;&#24230;&#26469;&#34920;&#36798;&#32422;&#26463;&#65292;&#20351;&#24471;&#31639;&#27861;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#21644;&#32422;&#26463;&#25968;&#37327;&#19978;&#30340;&#22797;&#26434;&#24230;&#22686;&#38271;&#36866;&#24230;&#65292;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21033;&#29992;&#32422;&#26463;&#20248;&#21270;&#21644;&#38750;&#20809;&#28369;&#21160;&#21147;&#31995;&#32479;&#20043;&#38388;&#30340;&#31867;&#27604;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#21152;&#36895;&#38750;&#32447;&#24615;&#32422;&#26463;&#19979;&#30340;&#19968;&#38454;&#20248;&#21270;&#31639;&#27861;&#12290;&#19982;Frank-Wolfe&#25110;&#25237;&#24433;&#26799;&#24230;&#19981;&#21516;&#65292;&#36825;&#20123;&#31639;&#27861;&#36991;&#20813;&#20102;&#27599;&#27425;&#36845;&#20195;&#22312;&#25972;&#20010;&#21487;&#34892;&#38598;&#19978;&#36827;&#34892;&#20248;&#21270;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38750;&#20984;&#35774;&#32622;&#20013;&#30340;&#25910;&#25947;&#24615;&#65292;&#24182;&#25512;&#23548;&#20102;&#22312;&#36830;&#32493;&#26102;&#38388;&#21644;&#31163;&#25955;&#26102;&#38388;&#20013;&#30340;&#20984;&#35774;&#32622;&#30340;&#21152;&#36895;&#29575;&#12290;&#36825;&#20123;&#31639;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#29305;&#24615;&#26159;&#20351;&#29992;&#36895;&#24230;&#32780;&#19981;&#26159;&#20301;&#32622;&#26469;&#34920;&#36798;&#32422;&#26463;&#65292;&#36825;&#33258;&#28982;&#22320;&#23548;&#33268;&#21487;&#34892;&#38598;&#30340;&#31232;&#30095;&#12289;&#23616;&#37096;&#21644;&#20984;&#36817;&#20284;&#65288;&#21363;&#20351;&#21487;&#34892;&#38598;&#26159;&#38750;&#20984;&#30340;&#65289;&#12290;&#22240;&#27492;&#65292;&#22797;&#26434;&#24230;&#22312;&#20915;&#31574;&#21464;&#37327;&#25968;&#37327;&#21644;&#32422;&#26463;&#25968;&#37327;&#19978;&#36866;&#24230;&#22686;&#38271;&#65292;&#20351;&#24471;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#21387;&#32553;&#24863;&#30693;&#21644;&#31232;&#30095;&#183;&#183;&#183;
&lt;/p&gt;
&lt;p&gt;
We exploit analogies between first-order algorithms for constrained optimization and non-smooth dynamical systems to design a new class of accelerated first-order algorithms for constrained optimization. Unlike Frank-Wolfe or projected gradients, these algorithms avoid optimization over the entire feasible set at each iteration. We prove convergence to stationary points even in a nonconvex setting and we derive accelerated rates for the convex setting both in continuous time, as well as in discrete time. An important property of these algorithms is that constraints are expressed in terms of velocities instead of positions, which naturally leads to sparse, local and convex approximations of the feasible set (even if the feasible set is nonconvex). Thus, the complexity tends to grow mildly in the number of decision variables and in the number of constraints, which makes the algorithms suitable for machine learning applications. We apply our algorithms to a compressed sensing and a sparse
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2211.09619</link><description>&lt;p&gt;
&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Introduction to Online Nonstochastic Control. (arXiv:2211.09619v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09619
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#21160;&#24577;&#31995;&#32479;&#25511;&#21046;&#19982;&#21487;&#24494;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#65292;&#24182;&#24212;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#21644;&#20984;&#26494;&#24347;&#25216;&#26415;&#24471;&#21040;&#20102;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#26368;&#20339;&#21644;&#40065;&#26834;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#19982;&#20854;&#20182;&#26694;&#26550;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#22312;&#26080;&#27861;&#39044;&#27979;&#25200;&#21160;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.  The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.  This objective suggests the use of the decision making frame
&lt;/p&gt;</description></item></channel></rss>