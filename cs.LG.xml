<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#38024;&#23545;&#21160;&#24577;&#22270;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;GreeDy&#21644;CoDy&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoDy&#22312;&#23547;&#25214;&#37325;&#35201;&#21453;&#20107;&#23454;&#36755;&#20837;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;59%&#12290;</title><link>https://arxiv.org/abs/2403.16846</link><description>&lt;p&gt;
GreeDy&#21644;CoDy&#65306;&#21160;&#24577;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#38024;&#23545;&#21160;&#24577;&#22270;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;GreeDy&#21644;CoDy&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoDy&#22312;&#23547;&#25214;&#37325;&#35201;&#21453;&#20107;&#23454;&#36755;&#20837;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;59%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNNs&#65289;&#23545;&#20110;&#24314;&#27169;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#20132;&#20114;&#30340;&#21160;&#24577;&#22270;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#30740;&#31350;&#36755;&#20837;&#22270;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340; TGNNs &#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;GreeDy&#65288;&#21160;&#24577;&#22270;&#30340;&#36138;&#24515;&#35299;&#37322;&#22120;&#65289;&#21644; CoDy&#65288;&#21160;&#24577;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65289;&#12290;&#23427;&#20204;&#23558;&#35299;&#37322;&#35270;&#20026;&#19968;&#20010;&#25628;&#32034;&#38382;&#39064;&#65292;&#23547;&#25214;&#25913;&#21464;&#27169;&#22411;&#39044;&#27979;&#30340;&#36755;&#20837;&#22270;&#20462;&#25913;&#12290;GreeDy &#20351;&#29992;&#31616;&#21333;&#30340;&#36138;&#24515;&#26041;&#27861;&#65292;&#32780; CoDy &#20351;&#29992;&#22797;&#26434;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#26377;&#25928;&#29983;&#25104;&#28165;&#26224;&#30340;&#35299;&#37322;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CoDy &#30340;&#24615;&#33021;&#20248;&#20110; GreeDy &#21644;&#29616;&#26377;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#23547;&#25214;&#21040;&#37325;&#35201;&#30340;&#21453;&#20107;&#23454;&#36755;&#20837;&#30340;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;&#39640;&#36798; 59\%&#12290;&#36825;&#31361;&#20986;&#20102; CoDy &#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16846v1 Announce Type: cross  Abstract: Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs with time-varying interactions, face a significant challenge in explainability due to their complex model structure. Counterfactual explanations, crucial for understanding model decisions, examine how input graph changes affect outcomes. This paper introduces two novel counterfactual explanation methods for TGNNs: GreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer for Dynamic Graphs). They treat explanations as a search problem, seeking input graph alterations that alter model predictions. GreeDy uses a simple, greedy approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm. Experiments show both methods effectively generate clear explanations. Notably, CoDy outperforms GreeDy and existing factual methods, with up to 59\% higher success rate in finding significant counterfactual inputs. This highlights CoDy's p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13847</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Domain Adaptation through Gaussian Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31561;&#20215;&#30340;&#31163;&#25955;&#38382;&#39064;&#35299;&#20915;&#36830;&#32493;&#26368;&#20248;&#36755;&#36816;&#12290;&#26368;&#20248;&#36755;&#36816;&#35299;&#20915;&#26041;&#26696;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#21305;&#37197;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22495;&#20043;&#38388;&#26144;&#23556;&#25968;&#25454;&#28857;&#65292;&#25110;&#32773;&#23558;&#26631;&#31614;&#20174;&#28304;&#22495;&#32452;&#20214;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#22312;&#22833;&#25928;&#35786;&#26029;&#30340;&#20004;&#20010;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13847v1 Announce Type: cross  Abstract: In this paper we explore domain adaptation through optimal transport. We propose a novel approach, where we model the data distributions through Gaussian mixture models. This strategy allows us to solve continuous optimal transport through an equivalent discrete problem. The optimal transport solution gives us a matching between source and target domain mixture components. From this matching, we can map data points between domains, or transfer the labels from the source domain components towards the target domain. We experiment with 2 domain adaptation benchmarks in fault diagnosis, showing that our methods have state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.04945</link><description>&lt;p&gt;
&#20026;&#25253;&#21578;&#29983;&#25104;&#35843;&#20248;&#24515;&#30005;&#22270;&#25351;&#23548;
&lt;/p&gt;
&lt;p&gt;
Electrocardiogram Instruction Tuning for Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04945
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#65292;&#24182;&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35780;&#20272;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#20316;&#20026;&#24515;&#33039;&#30149;&#24773;&#30417;&#27979;&#30340;&#20027;&#35201;&#38750;&#20405;&#20837;&#24615;&#35786;&#26029;&#24037;&#20855;&#65292;&#23545;&#20110;&#21327;&#21161;&#20020;&#24202;&#21307;&#29983;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#20351;&#29992;ECG&#25968;&#25454;&#23545;&#24515;&#33039;&#30149;&#24773;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#24573;&#30053;&#20102;ECG&#25253;&#21578;&#29983;&#25104;&#65292;&#36825;&#19981;&#20165;&#32791;&#26102;&#65292;&#32780;&#19988;&#38656;&#35201;&#20020;&#24202;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;ECG&#25253;&#21578;&#29983;&#25104;&#24182;&#30830;&#20445;&#20854;&#22810;&#21151;&#33021;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Multimodal ECG Instruction Tuning&#65288;MEIT&#65289;&#26694;&#26550;&#65292;&#36825;&#26159;\textit{&#39318;&#27425;}&#23581;&#35797;&#20351;&#29992;LLMs&#21644;&#22810;&#27169;&#24577;&#25351;&#23548;&#26469;&#35299;&#20915;ECG&#25253;&#21578;&#29983;&#25104;&#38382;&#39064;&#12290;&#20026;&#20102;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#22522;&#20934;&#26469;&#35780;&#20272;MEIT&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;ECG&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#21508;&#31181;LLM&#39592;&#24178;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#29420;&#29305;&#22320;&#23545;&#40784;&#20102;ECG&#20449;&#21495;&#21644;&#25253;&#21578;&#30340;&#34920;&#31034;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#26469;&#35780;&#20272;MEIT&#19982;&#20061;&#20010;&#24320;&#28304;LLMs&#65292;&#20351;&#29992;&#20102;&#36229;&#36807;80&#19975;&#20010;ECG&#25253;&#21578;&#12290;MEIT&#30340;&#32467;&#26524;&#20984;&#26174;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04945v1 Announce Type: new  Abstract: Electrocardiogram (ECG) serves as the primary non-invasive diagnostic tool for cardiac conditions monitoring, are crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is not only time-consuming but also requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the \textit{first} attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open source LLMs, using more than 800,000 ECG reports. MEIT's results underscore the superior p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.01671</link><description>&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#65306;&#32479;&#35745;&#26816;&#39564;&#12289;&#24230;&#37327;&#29109;&#20013;&#30340;&#38477;&#32500;&#21644;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Permutation invariant functions: statistical tests, dimension reduction in metric entropy and estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01671
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#12289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#20197;&#21450;&#20998;&#26512;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#27604;&#36739;&#20102;&#23427;&#20204;&#19982;&#27809;&#26377;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20989;&#25968;&#31867;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25490;&#21015;&#19981;&#21464;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#20197;&#21033;&#29992;&#26469;&#31616;&#21270;&#22797;&#26434;&#38382;&#39064;&#30340;&#26368;&#24120;&#35265;&#30340;&#23545;&#31216;&#24615;&#20043;&#19968;&#12290;&#36817;&#24180;&#26469;&#20851;&#20110;&#26500;&#24314;&#25490;&#21015;&#19981;&#21464;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#30340;&#30740;&#31350;&#27963;&#21160;&#28608;&#22686;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#20803;&#27010;&#29575;&#20998;&#24067;&#20013;&#30340;&#21464;&#37327;&#22914;&#20309;&#32479;&#35745;&#27979;&#35797;&#25490;&#21015;&#19981;&#21464;&#24615;&#21364;&#40092;&#26377;&#30740;&#31350;&#65292;&#20854;&#20013;&#26679;&#26412;&#37327;&#20801;&#35768;&#38543;&#30528;&#32500;&#25968;&#30340;&#22686;&#38271;&#12290;&#27492;&#22806;&#65292;&#22312;&#32479;&#35745;&#29702;&#35770;&#26041;&#38754;&#65292;&#20851;&#20110;&#25490;&#21015;&#19981;&#21464;&#24615;&#22914;&#20309;&#24110;&#21161;&#20272;&#35745;&#20013;&#38477;&#32500;&#30340;&#30693;&#35782;&#29978;&#23569;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20960;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#22238;&#39038;&#24182;&#25506;&#35752;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;i&#65289;&#27979;&#35797;&#22810;&#20803;&#20998;&#24067;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#20551;&#35774;&#65307;&#65288;ii&#65289;&#20272;&#35745;&#25490;&#21015;&#19981;&#21464;&#23494;&#24230;&#65307;&#65288;iii&#65289;&#20998;&#26512;&#20809;&#28369;&#25490;&#21015;&#19981;&#21464;&#20989;&#25968;&#31867;&#30340;&#24230;&#37327;&#29109;&#65292;&#24182;&#23558;&#20854;&#19982;&#26410;&#24378;&#21152;&#25490;&#21015;&#19981;&#21464;&#24615;&#30340;&#23545;&#24212;&#20989;&#25968;&#31867;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01671v1 Announce Type: new  Abstract: Permutation invariance is among the most common symmetry that can be exploited to simplify complex problems in machine learning (ML). There has been a tremendous surge of research activities in building permutation invariant ML architectures. However, less attention is given to how to statistically test for permutation invariance of variables in a multivariate probability distribution where the dimension is allowed to grow with the sample size. Also, in terms of a statistical theory, little is known about how permutation invariance helps with estimation in reducing dimensions. In this paper, we take a step back and examine these questions in several fundamental problems: (i) testing the assumption of permutation invariance of multivariate distributions; (ii) estimating permutation invariant densities; (iii) analyzing the metric entropy of smooth permutation invariant function classes and compare them with their counterparts without impos
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.14609</link><description>&lt;p&gt;
&#32852;&#37030;&#24335;&#22797;&#26434;&#26597;&#35810;&#31572;&#26696;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Federated Complex Qeury Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#30340;&#32852;&#37030;&#24335;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#38544;&#31169;&#20445;&#25252;&#21644;&#31572;&#26696;&#26816;&#32034;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22797;&#26434;&#36923;&#36753;&#26597;&#35810;&#31572;&#26696;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#25191;&#34892;&#22797;&#26434;&#36923;&#36753;&#25512;&#29702;&#30340;&#33021;&#21147;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#22522;&#20110;&#22270;&#25512;&#29702;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#36923;&#36753;&#26597;&#35810;&#34920;&#31034;&#20026;&#23884;&#20837;&#21521;&#37327;&#65292;&#24182;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25214;&#21040;&#36923;&#36753;&#26597;&#35810;&#30340;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#26597;&#35810;&#21333;&#20010;&#30693;&#35782;&#22270;&#35889;&#19978;&#65292;&#24182;&#19981;&#33021;&#24212;&#29992;&#20110;&#22810;&#20010;&#22270;&#24418;&#12290;&#27492;&#22806;&#65292;&#30452;&#25509;&#20849;&#20139;&#24102;&#26377;&#25935;&#24863;&#20449;&#24687;&#30340;&#30693;&#35782;&#22270;&#35889;&#21487;&#33021;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#65292;&#20351;&#24471;&#20849;&#20139;&#21644;&#26500;&#24314;&#19968;&#20010;&#32858;&#21512;&#30693;&#35782;&#22270;&#35889;&#29992;&#20110;&#25512;&#29702;&#20197;&#26816;&#32034;&#26597;&#35810;&#31572;&#26696;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#22240;&#27492;&#65292;&#30446;&#21069;&#20173;&#28982;&#19981;&#28165;&#26970;&#22914;&#20309;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#26597;&#35810;&#12290;&#19968;&#20010;&#23454;&#20307;&#21487;&#33021;&#28041;&#21450;&#21040;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#65292;&#23545;&#22810;&#20010;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#22312;&#22810;&#28304;&#30693;&#35782;&#22270;&#35889;&#19978;&#22238;&#31572;&#22797;&#26434;&#26597;&#35810;&#23545;&#20110;&#21457;&#29616;&#30693;&#35782;&#26159;&#37325;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14609v1 Announce Type: cross  Abstract: Complex logical query answering is a challenging task in knowledge graphs (KGs) that has been widely studied. The ability to perform complex logical reasoning is essential and supports various graph reasoning-based downstream tasks, such as search engines. Recent approaches are proposed to represent KG entities and logical queries into embedding vectors and find answers to logical queries from the KGs. However, existing proposed methods mainly focus on querying a single KG and cannot be applied to multiple graphs. In addition, directly sharing KGs with sensitive information may incur privacy risks, making it impractical to share and construct an aggregated KG for reasoning to retrieve query answers. Thus, it remains unknown how to answer queries on multi-source KGs. An entity can be involved in various knowledge graphs and reasoning on multiple KGs and answering complex queries on multi-source KGs is important in discovering knowledge 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;TopCoW&#25361;&#25112;&#65292;&#36890;&#36807;&#21457;&#24067;&#20855;&#26377;13&#31181;&#34880;&#31649;&#32452;&#20998;&#27880;&#37322;&#30340;Willis&#24490;&#29615;&#65288;CoW&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#25216;&#26415;&#36827;&#34892;&#25299;&#25169;&#24863;&#30693;&#35299;&#21078;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;CoW&#34920;&#24449;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2312.17670</link><description>&lt;p&gt;
TopCoW&#65306;&#22522;&#20110;&#25299;&#25169;&#24863;&#30693;&#35299;&#21078;&#20998;&#21106;&#30340;Willis&#24490;&#29615;&#65288;CoW&#65289;&#22312;CTA&#21644;MRA&#20013;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
TopCoW: Benchmarking Topology-Aware Anatomical Segmentation of the Circle of Willis (CoW) for CTA and MRA. (arXiv:2312.17670v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;TopCoW&#25361;&#25112;&#65292;&#36890;&#36807;&#21457;&#24067;&#20855;&#26377;13&#31181;&#34880;&#31649;&#32452;&#20998;&#27880;&#37322;&#30340;Willis&#24490;&#29615;&#65288;CoW&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#25216;&#26415;&#36827;&#34892;&#25299;&#25169;&#24863;&#30693;&#35299;&#21078;&#20998;&#21106;&#65292;&#35299;&#20915;&#20102;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;CoW&#34920;&#24449;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Willis&#24490;&#29615;&#65288;CoW&#65289;&#26159;&#36830;&#25509;&#22823;&#33041;&#20027;&#35201;&#24490;&#29615;&#30340;&#37325;&#35201;&#21160;&#33033;&#32593;&#32476;&#12290;&#20854;&#34880;&#31649;&#32467;&#26500;&#34987;&#35748;&#20026;&#24433;&#21709;&#30528;&#20005;&#37325;&#31070;&#32463;&#34880;&#31649;&#30142;&#30149;&#30340;&#39118;&#38505;&#12289;&#20005;&#37325;&#31243;&#24230;&#21644;&#20020;&#24202;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23545;&#39640;&#24230;&#21464;&#21270;&#30340;CoW&#35299;&#21078;&#36827;&#34892;&#34920;&#24449;&#20173;&#28982;&#26159;&#19968;&#39033;&#38656;&#35201;&#25163;&#21160;&#21644;&#32791;&#26102;&#30340;&#19987;&#23478;&#20219;&#21153;&#12290;CoW&#36890;&#24120;&#36890;&#36807;&#20004;&#31181;&#34880;&#31649;&#36896;&#24433;&#25104;&#20687;&#27169;&#24335;&#36827;&#34892;&#25104;&#20687;&#65292;&#21363;&#30913;&#20849;&#25391;&#34880;&#31649;&#25104;&#20687;&#65288;MRA&#65289;&#21644;&#35745;&#31639;&#26426;&#26029;&#23618;&#34880;&#31649;&#36896;&#24433;&#65288;CTA&#65289;&#65292;&#20294;&#26159;&#20851;&#20110;CTA&#30340;CoW&#35299;&#21078;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#21450;&#20854;&#27880;&#37322;&#38750;&#24120;&#26377;&#38480;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#22312;2023&#24180;&#32452;&#32455;&#20102;TopCoW&#25361;&#25112;&#36187;&#65292;&#24182;&#21457;&#24067;&#20102;&#19968;&#20010;&#24102;&#26377;&#27880;&#37322;&#30340;CoW&#25968;&#25454;&#38598;&#12290;TopCoW&#25968;&#25454;&#38598;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;13&#31181;&#21487;&#33021;&#30340;CoW&#34880;&#31649;&#32452;&#20998;&#30340;&#20307;&#32032;&#32423;&#27880;&#37322;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#25216;&#26415;&#23454;&#29616;&#12290;&#23427;&#20063;&#26159;&#31532;&#19968;&#20010;&#24102;&#26377;&#26469;&#33258;&#21516;&#19968;&#24739;&#32773;&#30340;&#25104;&#23545;MRA&#21644;CTA&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;TopCoW&#25361;&#25112;&#23558;CoW&#34920;&#24449;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#22810;&#31867;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Circle of Willis (CoW) is an important network of arteries connecting major circulations of the brain. Its vascular architecture is believed to affect the risk, severity, and clinical outcome of serious neuro-vascular diseases. However, characterizing the highly variable CoW anatomy is still a manual and time-consuming expert task. The CoW is usually imaged by two angiographic imaging modalities, magnetic resonance angiography (MRA) and computed tomography angiography (CTA), but there exist limited public datasets with annotations on CoW anatomy, especially for CTA. Therefore we organized the TopCoW Challenge in 2023 with the release of an annotated CoW dataset. The TopCoW dataset was the first public dataset with voxel-level annotations for thirteen possible CoW vessel components, enabled by virtual-reality (VR) technology. It was also the first large dataset with paired MRA and CTA from the same patients. TopCoW challenge formalized the CoW characterization problem as a multiclas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#30701;&#26102;&#38388;&#20869;&#26631;&#35760;8000&#20010;&#33145;&#37096;CT&#25195;&#25551;&#20013;&#30340;8&#20010;&#22120;&#23448;&#65292;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#22120;&#23448;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2305.09666</link><description>&lt;p&gt;
&#22312;&#19977;&#21608;&#20869;&#20026;8,000&#20010;&#33145;&#37096;CT&#25195;&#25551;&#26631;&#27880;&#22810;&#22120;&#23448;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Annotating 8,000 Abdominal CT Volumes for Multi-Organ Segmentation in Three Weeks. (arXiv:2305.09666v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#22312;&#30701;&#26102;&#38388;&#20869;&#26631;&#35760;8000&#20010;&#33145;&#37096;CT&#25195;&#25551;&#20013;&#30340;8&#20010;&#22120;&#23448;&#65292;&#24314;&#31435;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#22810;&#22120;&#23448;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#24433;&#20687;&#26631;&#27880;&#65292;&#29305;&#21035;&#26159;&#22120;&#23448;&#20998;&#21106;&#65292;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#22120;&#23448;&#20998;&#21106;&#30340;&#26631;&#27880;&#36807;&#31243;&#12290;&#25105;&#20204;&#26631;&#27880;&#20102;8,448&#20010;&#33145;&#37096;CT&#25195;&#25551;&#65292;&#26631;&#35760;&#20102;&#33086;&#33039;&#12289;&#32925;&#33039;&#12289;&#32958;&#33039;&#12289;&#32963;&#12289;&#32966;&#22218;&#12289;&#33008;&#33146;&#12289;&#20027;&#21160;&#33033;&#21644;&#19979;&#33108;&#38745;&#33033;&#12290;&#20256;&#32479;&#30340;&#26631;&#27880;&#26041;&#27861;&#38656;&#35201;&#19968;&#20301;&#32463;&#39564;&#20016;&#23500;&#30340;&#26631;&#27880;&#21592;1600&#21608;&#65292;&#32780;&#25105;&#20204;&#30340;&#26631;&#27880;&#26041;&#27861;&#20165;&#29992;&#20102;&#19977;&#21608;&#12290;
&lt;/p&gt;
&lt;p&gt;
Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30-60 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes a systematic and efficient method to expedite the annotation process for organ segmentation. We have created the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation method has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintain
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#20989;&#25968;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#26469;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#34920;&#29616;&#26469;&#28304;&#65292;&#32467;&#26524;&#34920;&#26126;DNNs&#20043;&#25152;&#20197;&#25104;&#21151;&#65292;&#26159;&#22240;&#20026;&#23427;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#20855;&#22791;&#19968;&#31181;&#20869;&#22312;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36275;&#20197;&#25269;&#28040;&#20989;&#25968;&#25968;&#37327;&#21450;&#22797;&#26434;&#24230;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;</title><link>http://arxiv.org/abs/2304.06670</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#20855;&#22791;&#20869;&#32622;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do deep neural networks have an inbuilt Occam's razor?. (arXiv:2304.06670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06670
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#20989;&#25968;&#20808;&#39564;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#26469;&#30740;&#31350;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#34920;&#29616;&#26469;&#28304;&#65292;&#32467;&#26524;&#34920;&#26126;DNNs&#20043;&#25152;&#20197;&#25104;&#21151;&#65292;&#26159;&#22240;&#20026;&#23427;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#20855;&#22791;&#19968;&#31181;&#20869;&#22312;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#36275;&#20197;&#25269;&#28040;&#20989;&#25968;&#25968;&#37327;&#21450;&#22797;&#26434;&#24230;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#21331;&#36234;&#24615;&#33021;&#24517;&#39035;&#28304;&#33258;&#20110;&#32593;&#32476;&#26550;&#26500;&#12289;&#35757;&#32451;&#31639;&#27861;&#21644;&#25968;&#25454;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#20102;&#21306;&#20998;&#36825;&#19977;&#20010;&#37096;&#20998;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#22522;&#20110;DNN&#25152;&#34920;&#36798;&#30340;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#35270;&#35282;&#26469;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#12290;&#32463;&#36807;&#32593;&#32476;&#30830;&#23450;&#30340;&#20989;&#25968;&#20808;&#39564;&#36890;&#36807;&#21033;&#29992;&#26377;&#24207;&#21644;&#28151;&#27788;&#29366;&#24577;&#20043;&#38388;&#30340;&#36716;&#21464;&#32780;&#21464;&#21270;&#12290;&#23545;&#20110;&#24067;&#23572;&#20989;&#25968;&#20998;&#31867;&#65292;&#25105;&#20204;&#21033;&#29992;&#20989;&#25968;&#30340;&#35823;&#24046;&#35889;&#22312;&#25968;&#25454;&#19978;&#36827;&#34892;&#21487;&#33021;&#24615;&#30340;&#36817;&#20284;&#12290;&#24403;&#19982;&#20808;&#39564;&#30456;&#32467;&#21512;&#26102;&#65292;&#23427;&#21487;&#20197;&#31934;&#30830;&#22320;&#39044;&#27979;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;DNN&#30340;&#21518;&#39564;&#27010;&#29575;&#12290;&#35813;&#20998;&#26512;&#25581;&#31034;&#20102;&#32467;&#26500;&#21270;&#25968;&#25454;&#65292;&#20197;&#21450;&#20869;&#22312;&#30340;&#22885;&#21345;&#22982;&#21059;&#20992;&#24335;&#24402;&#32435;&#20559;&#24046;&#65292;&#21363;&#36275;&#20197;&#25269;&#28040;&#22797;&#26434;&#24230;&#38543;&#20989;&#25968;&#25968;&#37327;&#21576;&#25351;&#25968;&#22686;&#38271;&#32780;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#26159;DNNs&#25104;&#21151;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable performance of overparameterized deep neural networks (DNNs) must arise from an interplay between network architecture, training algorithms, and structure in the data. To disentangle these three components, we apply a Bayesian picture, based on the functions expressed by a DNN, to supervised learning. The prior over functions is determined by the network, and is varied by exploiting a transition between ordered and chaotic regimes. For Boolean function classification, we approximate the likelihood using the error spectrum of functions on data. When combined with the prior, this accurately predicts the posterior, measured for DNNs trained with stochastic gradient descent. This analysis reveals that structured data, combined with an intrinsic Occam's razor-like inductive bias towards (Kolmogorov) simple functions that is strong enough to counteract the exponential growth of the number of functions with complexity, is a key to the success of DNNs.
&lt;/p&gt;</description></item></channel></rss>