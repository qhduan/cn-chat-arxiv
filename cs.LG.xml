<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10786</link><description>&lt;p&gt;
ContourDiff&#65306;&#24102;&#36718;&#24275;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#26080;&#37197;&#23545;&#22270;&#20687;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
ContourDiff: Unpaired Image Translation with Contour-Guided Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10786
&lt;/p&gt;
&lt;p&gt;
ContourDiff&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#65292;&#26088;&#22312;&#24110;&#21161;&#20934;&#30830;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#24182;&#20445;&#25345;&#20854;&#35299;&#21078;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#22320;&#22312;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#32763;&#35793;&#21307;&#23398;&#22270;&#20687;&#65288;&#20363;&#22914;&#20174;CT&#21040;MRI&#65289;&#23545;&#20110;&#35768;&#22810;&#20020;&#24202;&#21644;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ContourDiff&#30340;&#26032;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22270;&#20687;&#30340;&#39046;&#22495;&#19981;&#21464;&#35299;&#21078;&#36718;&#24275;&#34920;&#31034;&#12290;&#36825;&#20123;&#34920;&#31034;&#26131;&#20110;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#65292;&#20294;&#23545;&#20854;&#35299;&#21078;&#20869;&#23481;&#24418;&#25104;&#31934;&#30830;&#30340;&#31354;&#38388;&#32422;&#26463;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#26469;&#33258;&#20219;&#24847;&#36755;&#20837;&#39046;&#22495;&#30340;&#22270;&#20687;&#30340;&#36718;&#24275;&#34920;&#31034;&#36716;&#25442;&#20026;&#36755;&#20986;&#39046;&#22495;&#20013;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10786v1 Announce Type: cross  Abstract: Accurately translating medical images across different modalities (e.g., CT to MRI) has numerous downstream clinical and machine learning applications. While several methods have been proposed to achieve this, they often prioritize perceptual quality with respect to output domain features over preserving anatomical fidelity. However, maintaining anatomy during translation is essential for many tasks, e.g., when leveraging masks from the input domain to develop a segmentation model with images translated to the output domain. To address these challenges, we propose ContourDiff, a novel framework that leverages domain-invariant anatomical contour representations of images. These representations are simple to extract from images, yet form precise spatial constraints on their anatomical content. We introduce a diffusion model that converts contour representations of images from arbitrary input domains into images in the output domain of in
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20165;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#20840;&#21442;&#25968;&#37325;&#26032;&#35757;&#32451;&#30340;&#20570;&#27861;&#65292;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#25110;&#29978;&#33267;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;PERP&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2312.15230</link><description>&lt;p&gt;
PERP: &#22312;LLMs&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#20462;&#21098;-&#37325;&#26032;&#35757;&#32451;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20165;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#20840;&#21442;&#25968;&#37325;&#26032;&#35757;&#32451;&#30340;&#20570;&#27861;&#65292;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#25110;&#29978;&#33267;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;PERP&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#21387;&#32553;&#65292;&#26174;&#33879;&#20943;&#23569;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#20687;&#36845;&#20195;&#24133;&#20540;&#20462;&#21098;&#65288;IMP&#65292;Han&#31561;&#65292;2015&#65289;&#36825;&#26679;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#21487;&#20197;&#21435;&#38500;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#65292;&#24182;&#38656;&#35201;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#20197;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#30001;&#20110;&#20869;&#23384;&#21644;&#35745;&#31639;&#38480;&#21046;&#65292;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#21482;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#36890;&#24120;&#36275;&#20197;&#24674;&#22797;&#29978;&#33267;&#25552;&#39640;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20165;&#37325;&#26032;&#35757;&#32451;GPT-&#32467;&#26500;&#30340;0.27%-0.35%&#30340;&#21442;&#25968;&#21363;&#21487;&#22312;&#19981;&#21516;&#31232;&#30095;&#27700;&#24179;&#19978;&#23454;&#29616;&#19982;&#19968;&#27425;&#24615;IMP&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#20462;&#21098;&#21518;&#21442;&#25968;&#39640;&#25928;&#37325;&#26032;&#35757;&#32451;&#65288;PERP&#65289;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Networks can be efficiently compressed through pruning, significantly reducing storage and computational demands while maintaining predictive performance. Simple yet effective methods like Iterative Magnitude Pruning (IMP, Han et al., 2015) remove less important parameters and require a costly retraining procedure to recover performance after pruning. However, with the rise of Large Language Models (LLMs), full retraining has become infeasible due to memory and compute constraints. In this study, we challenge the practice of retraining all parameters by demonstrating that updating only a small subset of highly expressive parameters is often sufficient to recover or even improve performance compared to full retraining. Surprisingly, retraining as little as 0.27%-0.35% of the parameters of GPT-architectures achieves comparable performance to One Shot IMP across various sparsity levels. Our approach, Parameter-Efficient Retraining after Pruning (PERP), drastically reduces compute a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2310.11590</link><description>&lt;p&gt;
&#25506;&#32034;&#22312;&#23548;&#33322;&#22330;&#26223;&#19979;&#25512;&#26029;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;
&lt;/p&gt;
&lt;p&gt;
Towards Inferring Users' Impressions of Robot Performance in Navigation Scenarios. (arXiv:2310.11590v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25311;&#36890;&#36807;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#21644;&#20998;&#26512;&#32467;&#26524;&#65292;&#21457;&#29616;&#22312;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;&#21360;&#35937;&#36890;&#24120;&#36890;&#36807;&#35843;&#26597;&#38382;&#21367;&#26469;&#34913;&#37327;&#12290;&#20316;&#20026;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#26356;&#39640;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#38750;&#35821;&#35328;&#34892;&#20026;&#25552;&#31034;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#39044;&#27979;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#34892;&#20026;&#21360;&#35937;&#30340;&#21487;&#33021;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#20102;SEAN TOGETHER&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#25324;&#22312;&#34394;&#25311;&#29616;&#23454;&#27169;&#25311;&#20013;&#20154;&#19982;&#31227;&#21160;&#26426;&#22120;&#20154;&#30456;&#20114;&#20316;&#29992;&#30340;&#35266;&#23519;&#32467;&#26524;&#65292;&#20197;&#21450;&#29992;&#25143;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#30340;5&#28857;&#37327;&#34920;&#35780;&#20215;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;&#20154;&#31867;&#21644;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#22914;&#20309;&#22522;&#20110;&#19981;&#21516;&#30340;&#35266;&#23519;&#31867;&#22411;&#65288;&#20363;&#22914;&#38754;&#37096;&#12289;&#31354;&#38388;&#21644;&#22320;&#22270;&#29305;&#24449;&#65289;&#26469;&#39044;&#27979;&#24863;&#30693;&#21040;&#30340;&#26426;&#22120;&#20154;&#24615;&#33021;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20165;&#20165;&#38754;&#37096;&#34920;&#24773;&#23601;&#33021;&#25552;&#20379;&#20851;&#20110;&#20154;&#20204;&#23545;&#26426;&#22120;&#20154;&#24615;&#33021;&#21360;&#35937;&#30340;&#26377;&#29992;&#20449;&#24687;&#65307;&#20294;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;&#23548;&#33322;&#22330;&#26223;&#20013;&#65292;&#31354;&#38388;&#29305;&#24449;&#26159;&#36825;&#31181;&#25512;&#26029;&#20219;&#21153;&#26368;&#20851;&#38190;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human impressions of robot performance are often measured through surveys. As a more scalable and cost-effective alternative, we study the possibility of predicting people's impressions of robot behavior using non-verbal behavioral cues and machine learning techniques. To this end, we first contribute the SEAN TOGETHER Dataset consisting of observations of an interaction between a person and a mobile robot in a Virtual Reality simulation, together with impressions of robot performance provided by users on a 5-point scale. Second, we contribute analyses of how well humans and supervised learning techniques can predict perceived robot performance based on different combinations of observation types (e.g., facial, spatial, and map features). Our results show that facial expressions alone provide useful information about human impressions of robot performance; but in the navigation scenarios we tested, spatial features are the most critical piece of information for this inference task. Als
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.15039</link><description>&lt;p&gt;
&#32467;&#21512;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#21033;&#29992;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#36827;&#34892;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Combining Survival Analysis and Machine Learning for Mass Cancer Risk Prediction using EHR data. (arXiv:2309.15039v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15039
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992; EHR &#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21482;&#38656;&#21033;&#29992;&#21382;&#21490;&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#20449;&#24687;&#26469;&#23454;&#29616;&#26368;&#23567;&#21270;&#30340;&#25968;&#25454;&#38656;&#27714;&#65292;&#36890;&#36807;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#22312;&#22823;&#35268;&#27169;&#24212;&#29992;&#20013;&#23454;&#29616;&#23545;&#24739;&#32773;&#30284;&#30151;&#39118;&#38505;&#30340;&#20010;&#24615;&#21270;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32431;&#31929;&#30340;&#21307;&#23398;&#32959;&#30244;&#31579;&#26597;&#26041;&#27861;&#36890;&#24120;&#36153;&#29992;&#39640;&#26114;&#12289;&#32791;&#26102;&#38271;&#65292;&#24182;&#19988;&#20165;&#36866;&#29992;&#20110;&#22823;&#35268;&#27169;&#24212;&#29992;&#12290;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#22312;&#30284;&#30151;&#26816;&#27979;&#26041;&#38754;&#21457;&#25381;&#20102;&#24040;&#22823;&#20316;&#29992;&#65292;&#20294;&#38656;&#35201;&#29305;&#23450;&#25110;&#28145;&#20837;&#30340;&#21307;&#23398;&#25968;&#25454;&#12290;&#36825;&#20123;&#26041;&#38754;&#24433;&#21709;&#20102;&#30284;&#30151;&#31579;&#26597;&#26041;&#27861;&#30340;&#22823;&#35268;&#27169;&#23454;&#26045;&#12290;&#22240;&#27492;&#65292;&#22522;&#20110;&#24050;&#26377;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#25968;&#25454;&#23545;&#24739;&#32773;&#36827;&#34892;&#22823;&#35268;&#27169;&#20010;&#24615;&#21270;&#30284;&#30151;&#39118;&#38505;&#35780;&#20272;&#24212;&#29992;AI&#26041;&#27861;&#26159;&#19968;&#31181;&#39072;&#35206;&#24615;&#30340;&#25913;&#21464;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;EHR&#25968;&#25454;&#36827;&#34892;&#22823;&#35268;&#27169;&#32959;&#30244;&#39118;&#38505;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#30340;&#25968;&#25454;&#36138;&#23146;&#31574;&#30053;&#33073;&#39062;&#32780;&#20986;&#65292;&#20165;&#38656;&#35201;&#26469;&#33258;EHR&#30340;&#21307;&#30103;&#26381;&#21153;&#20195;&#30721;&#21644;&#35786;&#26029;&#21382;&#21490;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#20108;&#20998;&#31867;&#38382;&#39064;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;175441&#21517;&#19981;&#35760;&#21517;&#30340;&#24739;&#32773;&#65288;&#20854;&#20013;2861&#21517;&#34987;&#35786;&#26029;&#20026;&#30284;&#30151;&#65289;&#12290;&#20316;&#20026;&#22522;&#20934;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#22522;&#20110;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#23384;&#27963;&#20998;&#26512;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;
&lt;/p&gt;
&lt;p&gt;
Purely medical cancer screening methods are often costly, time-consuming, and weakly applicable on a large scale. Advanced Artificial Intelligence (AI) methods greatly help cancer detection but require specific or deep medical data. These aspects affect the mass implementation of cancer screening methods. For these reasons, it is a disruptive change for healthcare to apply AI methods for mass personalized assessment of the cancer risk among patients based on the existing Electronic Health Records (EHR) volume.  This paper presents a novel method for mass cancer risk prediction using EHR data. Among other methods, our one stands out by the minimum data greedy policy, requiring only a history of medical service codes and diagnoses from EHR. We formulate the problem as a binary classification. This dataset contains 175 441 de-identified patients (2 861 diagnosed with cancer). As a baseline, we implement a solution based on a recurrent neural network (RNN). We propose a method that combine
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2202.09134</link><description>&lt;p&gt;
&#22312;&#27424;&#21442;&#25968;&#21270;&#21644;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#24335;&#20013;&#30340;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Data Augmentation in the Underparameterized and Overparameterized Regimes. (arXiv:2202.09134v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09134
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#30830;&#20999;&#37327;&#21270;&#32467;&#26524;&#65292;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#19988;&#20854;&#25928;&#26524;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#36890;&#36807;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#36827;&#34892;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#30830;&#20999;&#37327;&#21270;&#25968;&#25454;&#22686;&#24378;&#22914;&#20309;&#24433;&#21709;&#20272;&#35745;&#30340;&#26041;&#24046;&#21644;&#26497;&#38480;&#20998;&#24067;&#30340;&#32467;&#26524;&#65292;&#24182;&#35814;&#32454;&#20998;&#26512;&#20102;&#20960;&#20010;&#20855;&#20307;&#27169;&#22411;&#12290;&#32467;&#26524;&#35777;&#23454;&#20102;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#20013;&#30340;&#19968;&#20123;&#35266;&#23519;&#65292;&#20294;&#20063;&#24471;&#20986;&#20102;&#24847;&#22806;&#30340;&#21457;&#29616;&#65306;&#25968;&#25454;&#22686;&#24378;&#21487;&#33021;&#20250;&#22686;&#21152;&#32780;&#19981;&#26159;&#20943;&#23569;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#27604;&#22914;&#32463;&#39564;&#39044;&#27979;&#39118;&#38505;&#12290;&#23427;&#21487;&#20197;&#20805;&#24403;&#27491;&#21017;&#21270;&#22120;&#65292;&#20294;&#22312;&#26576;&#20123;&#39640;&#32500;&#38382;&#39064;&#20013;&#21364;&#26080;&#27861;&#23454;&#29616;&#65292;&#24182;&#19988;&#21487;&#33021;&#20250;&#25913;&#21464;&#32463;&#39564;&#39118;&#38505;&#30340;&#21452;&#37325;&#19979;&#38477;&#23792;&#20540;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#20998;&#26512;&#34920;&#26126;&#25968;&#25454;&#22686;&#24378;&#34987;&#36171;&#20104;&#30340;&#20960;&#20010;&#23646;&#24615;&#35201;&#20040;&#26159;&#30495;&#30340;&#65292;&#35201;&#20040;&#26159;&#20551;&#30340;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#22810;&#20010;&#22240;&#32032;&#30340;&#32452;&#21512;-&#29305;&#21035;&#26159;&#25968;&#25454;&#20998;&#24067;&#65292;&#20272;&#35745;&#22120;&#30340;&#23646;&#24615;&#20197;&#21450;&#26679;&#26412;&#22823;&#23567;&#65292;&#22686;&#24378;&#25968;&#37327;&#21644;&#32500;&#25968;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#29702;&#35770;&#24037;&#20855;&#26159;&#38543;&#26426;&#36716;&#25442;&#30340;&#39640;&#32500;&#38543;&#26426;&#21521;&#37327;&#30340;&#20989;&#25968;&#30340;&#26497;&#38480;&#23450;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide results that exactly quantify how data augmentation affects the variance and limiting distribution of estimates, and analyze several specific models in detail. The results confirm some observations made in machine learning practice, but also lead to unexpected findings: Data augmentation may increase rather than decrease the uncertainty of estimates, such as the empirical prediction risk. It can act as a regularizer, but fails to do so in certain high-dimensional problems, and it may shift the double-descent peak of an empirical risk. Overall, the analysis shows that several properties data augmentation has been attributed with are not either true or false, but rather depend on a combination of factors -- notably the data distribution, the properties of the estimator, and the interplay of sample size, number of augmentations, and dimension. Our main theoretical tool is a limit theorem for functions of randomly transformed, high-dimensional random vectors. The proof draws on 
&lt;/p&gt;</description></item></channel></rss>