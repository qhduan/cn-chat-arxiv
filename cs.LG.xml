<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;Brenier&#30340;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#31070;&#32463;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#28508;&#22312;&#20989;&#25968;$u$&#65292;&#20174;&#26368;&#26032;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;</title><link>https://arxiv.org/abs/2403.03071</link><description>&lt;p&gt;
&#35770;Brenier&#30340;&#26497;&#20998;&#35299;&#30340;&#31070;&#32463;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
On a Neural Implementation of Brenier's Polar Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03071
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Brenier&#30340;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#31070;&#32463;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#28508;&#22312;&#20989;&#25968;$u$&#65292;&#20174;&#26368;&#26032;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#36827;&#23637;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;1991&#24180;&#65292;Brenier&#35777;&#26126;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#23558;$QR$&#20998;&#35299;&#65288;&#20998;&#20026;&#21322;&#27491;&#23450;&#30697;&#38453;$\times$&#37193;&#30697;&#38453;&#65289;&#25512;&#24191;&#21040;&#20219;&#24847;&#30690;&#37327;&#22330;$F:\mathbb{R}^d\rightarrow \mathbb{R}^d$&#12290;&#36825;&#20010;&#34987;&#31216;&#20026;&#26497;&#20998;&#35299;&#23450;&#29702;&#30340;&#23450;&#29702;&#34920;&#26126;&#65292;&#20219;&#24847;&#22330;$F$&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#20984;&#20989;&#25968;$u$&#30340;&#26799;&#24230;&#19982;&#20445;&#27979;&#24230;&#26144;&#23556;$M$&#30340;&#22797;&#21512;&#65292;&#21363;$F=\nabla u \circ M$&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#19968;&#20855;&#26377;&#28145;&#36828;&#29702;&#35770;&#24847;&#20041;&#30340;&#32467;&#26524;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#21487;&#33021;&#30340;&#24212;&#29992;&#12290;&#35813;&#23450;&#29702;&#19982;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#29702;&#35770;&#23494;&#20999;&#30456;&#20851;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#31070;&#32463;&#26368;&#20248;&#36755;&#36816;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23558;&#28508;&#22312;&#20989;&#25968;$u$&#21442;&#25968;&#21270;&#20026;&#36755;&#20837;&#20984;&#31070;&#32463;&#32593;&#32476;&#12290;&#26144;&#23556;$M$&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;$u^*$&#65292;&#21363;$u$&#30340;&#20984;&#20849;&#36717;&#65292;&#36880;&#28857;&#35745;&#31639;&#24471;&#21040;&#65292;&#21363;$M=\nabla u^* \circ F$&#65292;&#25110;&#32773;&#20316;&#20026;&#36741;&#21161;&#32593;&#32476;&#23398;&#20064;&#24471;&#21040;&#12290;&#22240;&#20026;$M$&#22312;&#22522;&#22240;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03071v1 Announce Type: cross  Abstract: In 1991, Brenier proved a theorem that generalizes the $QR$ decomposition for square matrices -- factored as PSD $\times$ unitary -- to any vector field $F:\mathbb{R}^d\rightarrow \mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\nabla u \circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\nabla u^* \circ F$, or learned as an auxiliary network. Because $M$ is, in gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;Halpern&#36845;&#20195;&#31639;&#27861;&#30340;&#19981;&#31934;&#30830;&#21464;&#31181;&#65292;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#19981;&#31934;&#30830;&#30340;&#23481;&#24046;&#65292;&#36825;&#20123;&#21464;&#31181;&#23637;&#29616;&#20986;O(k^-1)&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;&#24182;&#19988;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#31867;&#25968;&#25454;&#39537;&#21160;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22312;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#20013;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#19981;&#31934;&#30830;&#35745;&#31639;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06033</link><description>&lt;p&gt;
&#19981;&#31934;&#30830;&#30340;Halpern&#36845;&#20195;&#31639;&#27861;&#21450;&#20854;&#22312;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
An Inexact Halpern Iteration for with Application to Distributionally Robust Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;Halpern&#36845;&#20195;&#31639;&#27861;&#30340;&#19981;&#31934;&#30830;&#21464;&#31181;&#65292;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#19981;&#31934;&#30830;&#30340;&#23481;&#24046;&#65292;&#36825;&#20123;&#21464;&#31181;&#23637;&#29616;&#20986;O(k^-1)&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#24615;&#30340;&#25910;&#25947;&#29305;&#24615;&#12290;&#24182;&#19988;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20004;&#31867;&#25968;&#25454;&#39537;&#21160;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#21450;&#22312;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#20013;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#19981;&#31934;&#30830;&#35745;&#31639;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Halpern&#36845;&#20195;&#31639;&#27861;&#22240;&#20854;&#31616;&#21333;&#24418;&#24335;&#21644;&#21560;&#24341;&#20154;&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#36817;&#24180;&#26469;&#22312;&#35299;&#20915;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#29615;&#22659;&#19979;&#35813;&#26041;&#26696;&#30340;&#19981;&#31934;&#30830;&#21464;&#31181;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#65292;&#24182;&#34920;&#26126;&#36890;&#36807;&#36866;&#24403;&#36873;&#25321;&#19981;&#31934;&#30830;&#30340;&#23481;&#24046;&#65292;&#19981;&#31934;&#30830;&#26041;&#26696;&#22312;&#65288;&#26399;&#26395;&#30340;&#65289;&#27531;&#24046;&#33539;&#25968;&#19978;&#20855;&#26377;O(k^-1)&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25918;&#23485;&#20102;&#25991;&#29486;&#20013;&#37319;&#29992;&#30340;&#26368;&#26032;&#19981;&#31934;&#30830;&#24615;&#26465;&#20214;&#65292;&#21516;&#26102;&#20855;&#26377;&#30456;&#21516;&#30340;&#31454;&#20105;&#24615;&#25910;&#25947;&#29305;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35299;&#20915;&#20004;&#31867;&#20855;&#26377;&#20984;&#20985;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#37325;&#26500;&#30340;&#25968;&#25454;&#39537;&#21160;Wasserstein&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#20854;&#22312;&#20351;&#29992;&#38543;&#26426;&#19968;&#38454;&#26041;&#27861;&#36827;&#34892;&#20998;&#24067;&#40065;&#26834;&#23398;&#20064;&#20013;&#30340;&#19981;&#31934;&#30830;&#35745;&#31639;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Halpern iteration for solving monotone inclusion problems has gained increasing interests in recent years due to its simple form and appealing convergence properties. In this paper, we investigate the inexact variants of the scheme in both deterministic and stochastic settings. We conduct extensive convergence analysis and show that by choosing the inexactness tolerances appropriately, the inexact schemes admit an $O(k^{-1})$ convergence rate in terms of the (expected) residue norm. Our results relax the state-of-the-art inexactness conditions employed in the literature while sharing the same competitive convergence properties. We then demonstrate how the proposed methods can be applied for solving two classes of data-driven Wasserstein distributionally robust optimization problems that admit convex-concave min-max optimization reformulations. We highlight its capability of performing inexact computations for distributionally robust learning with stochastic first-order methods.
&lt;/p&gt;</description></item><item><title>&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.04068</link><description>&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65306;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;&#39537;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Retrieve to Explain: Evidence-driven Predictions with Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04068
&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;R2E&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;Shapley&#20540;&#30830;&#23450;&#35777;&#25454;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#20174;&#32780;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36890;&#36807;&#24212;&#29992;&#20110;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#20219;&#21153;&#20013;&#65292;R2E&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#27169;&#22411;&#65292;&#24448;&#24448;&#38590;&#20197;&#28145;&#20837;&#20998;&#26512;&#12290;&#40657;&#30418;&#27169;&#22411;&#21487;&#33021;&#25513;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#38382;&#39064;&#21644;&#26377;&#23475;&#20559;&#24046;&#12290;&#23545;&#20110;&#20154;&#26426;&#21327;&#20316;&#36807;&#31243;&#26469;&#35828;&#65292;&#19981;&#36879;&#26126;&#30340;&#39044;&#27979;&#21487;&#33021;&#23548;&#33268;&#32570;&#20047;&#20449;&#20219;&#65292;&#38480;&#21046;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21363;&#20351;&#27169;&#22411;&#30340;&#24615;&#33021;&#24456;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26816;&#32034;&#20197;&#35299;&#37322;&#65288;Retrieve to Explain&#65292;&#31616;&#31216;R2E&#65289;&#12290;R2E&#26159;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#26681;&#25454;&#25991;&#26723;&#35821;&#26009;&#24211;&#20013;&#30340;&#35777;&#25454;&#65292;&#20351;&#29992;Shapley&#20540;&#26469;&#30830;&#23450;&#35777;&#25454;&#23545;&#26368;&#32456;&#39044;&#27979;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#27169;&#26495;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#20854;&#20013;&#12290;R2E&#33021;&#22815;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#30340;&#35777;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#27169;&#26495;&#21270;&#23558;&#32467;&#26500;&#21270;&#25968;&#25454;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#20013;&#12290;&#25105;&#20204;&#22312;&#36890;&#36807;&#20998;&#26512;&#24050;&#21457;&#34920;&#30340;&#31185;&#23398;&#25991;&#29486;&#36827;&#34892;&#33647;&#29289;&#38774;&#28857;&#37492;&#23450;&#30340;&#23454;&#38469;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#35813;&#27169;&#22411;&#22312;&#39044;&#27979;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#26041;&#38754;&#20248;&#20110;&#34892;&#19994;&#26631;&#20934;&#30340;&#22522;&#22240;&#23398;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning models, particularly language models, are notoriously difficult to introspect. Black-box models can mask both issues in model training and harmful biases. For human-in-the-loop processes, opaque predictions can drive lack of trust, limiting a model's impact even when it performs effectively. To address these issues, we introduce Retrieve to Explain (R2E). R2E is a retrieval-based language model that prioritizes amongst a pre-defined set of possible answers to a research question based on the evidence in a document corpus, using Shapley values to identify the relative importance of pieces of evidence to the final prediction. R2E can adapt to new evidence without retraining, and incorporate structured data through templating into natural language. We assess on the use case of drug target identification from published scientific literature, where we show that the model outperforms an industry-standard genetics-based approach on predicting clinical trial outcomes.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#22791;&#36873;&#26041;&#26696;&#38598;&#21512;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#21464;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#65292;&#33021;&#22815;&#36991;&#20813;&#32452;&#21512;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#39640;&#25928;&#32431;&#25506;&#32034;&#65292;&#20174;&#32780;&#20934;&#30830;&#22238;&#31572;&#26597;&#35810;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.19319</link><description>&lt;p&gt;
&#39640;&#25928;&#32431;&#25506;&#32034;&#30340;&#21452;&#21521;&#31639;&#27861;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Dual-Directed Algorithm Design for Efficient Pure Exploration. (arXiv:2310.19319v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19319
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#22791;&#36873;&#26041;&#26696;&#38598;&#21512;&#20013;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#12290;&#36890;&#36807;&#20351;&#29992;&#23545;&#20598;&#21464;&#37327;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#65292;&#33021;&#22815;&#36991;&#20813;&#32452;&#21512;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#23454;&#29616;&#39640;&#25928;&#32431;&#25506;&#32034;&#65292;&#20174;&#32780;&#20934;&#30830;&#22238;&#31572;&#26597;&#35810;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#26377;&#38480;&#30340;&#22791;&#36873;&#26041;&#26696;&#38598;&#21512;&#20013;&#30340;&#38543;&#26426;&#39034;&#24207;&#33258;&#36866;&#24212;&#23454;&#39564;&#30340;&#32431;&#25506;&#32034;&#38382;&#39064;&#12290;&#20915;&#31574;&#32773;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#26368;&#23567;&#30340;&#27979;&#37327;&#24037;&#20316;&#20197;&#39640;&#32622;&#20449;&#24230;&#20934;&#30830;&#22238;&#31572;&#19982;&#22791;&#36873;&#26041;&#26696;&#30456;&#20851;&#30340;&#26597;&#35810;&#38382;&#39064;&#12290;&#19968;&#20010;&#20856;&#22411;&#30340;&#26597;&#35810;&#38382;&#39064;&#26159;&#30830;&#23450;&#34920;&#29616;&#26368;&#20339;&#30340;&#22791;&#36873;&#26041;&#26696;&#65292;&#36825;&#22312;&#25490;&#21517;&#21644;&#36873;&#25321;&#38382;&#39064;&#20197;&#21450;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#31216;&#20026;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#22266;&#23450;&#31934;&#24230;&#30340;&#35774;&#23450;&#65292;&#24182;&#23548;&#20986;&#20102;&#19968;&#20010;&#19982;&#26679;&#26412;&#26368;&#20248;&#20998;&#37197;&#26377;&#24378;&#25910;&#25947;&#24615;&#27010;&#24565;&#30456;&#20851;&#30340;&#20248;&#21270;&#26465;&#20214;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#20351;&#29992;&#23545;&#20598;&#21464;&#37327;&#65292;&#25105;&#20204;&#21051;&#30011;&#20102;&#19968;&#20010;&#20998;&#37197;&#26159;&#21542;&#26368;&#20248;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#23545;&#20598;&#21464;&#37327;&#30340;&#20351;&#29992;&#20351;&#25105;&#20204;&#33021;&#22815;&#32469;&#36807;&#23436;&#20840;&#20381;&#36182;&#20110;&#21407;&#22987;&#21464;&#37327;&#30340;&#26368;&#20248;&#26465;&#20214;&#30340;&#32452;&#21512;&#32467;&#26500;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20123;&#26368;&#20248;&#26465;&#20214;&#20351;&#24471;&#21452;&#21521;&#31639;&#27861;&#35774;&#35745;&#21407;&#21017;&#30340;&#25193;&#23637;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider pure-exploration problems in the context of stochastic sequential adaptive experiments with a finite set of alternative options. The goal of the decision-maker is to accurately answer a query question regarding the alternatives with high confidence with minimal measurement efforts. A typical query question is to identify the alternative with the best performance, leading to ranking and selection problems, or best-arm identification in the machine learning literature. We focus on the fixed-precision setting and derive a sufficient condition for optimality in terms of a notion of strong convergence to the optimal allocation of samples. Using dual variables, we characterize the necessary and sufficient conditions for an allocation to be optimal. The use of dual variables allow us to bypass the combinatorial structure of the optimality conditions that relies solely on primal variables. Remarkably, these optimality conditions enable an extension of top-two algorithm design princ
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#24066;&#22330;&#65292;&#29992;&#20110;&#36830;&#25509;&#26426;&#22120;&#23398;&#20064;&#30340;&#20379;&#27714;&#21305;&#37197;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#24066;&#22330;&#35774;&#35745;&#20013;&#30340;&#20004;&#20010;&#26680;&#24515;&#25361;&#25112;&#30340;&#26032;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2310.17843</link><description>&lt;p&gt;
&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#26426;&#22120;&#23398;&#20064;&#22312;&#32447;&#24066;&#22330;&#65306;&#20174;&#21457;&#29616;&#21040;&#23450;&#20215;
&lt;/p&gt;
&lt;p&gt;
A Data-Centric Online Market for Machine Learning: From Discovery to Pricing. (arXiv:2310.17843v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17843
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#22312;&#32447;&#24066;&#22330;&#65292;&#29992;&#20110;&#36830;&#25509;&#26426;&#22120;&#23398;&#20064;&#30340;&#20379;&#27714;&#21305;&#37197;&#65292;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#36825;&#20010;&#24066;&#22330;&#35774;&#35745;&#20013;&#30340;&#20004;&#20010;&#26680;&#24515;&#25361;&#25112;&#30340;&#26032;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#30340;&#21160;&#21147; - &#20016;&#23500;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#35201;&#23558;&#26426;&#22120;&#23398;&#20064;&#20174;&#23569;&#25968;&#22823;&#22411;&#20844;&#21496;&#20043;&#38388;&#30340;&#31454;&#36187;&#36716;&#21464;&#20026;&#20026;&#20247;&#22810;&#26222;&#36890;&#29992;&#25143;&#30340;&#25968;&#25454;&#20998;&#26512;&#35831;&#27714;&#26381;&#21153;&#30340;&#21487;&#35775;&#38382;&#25216;&#26415;&#65292;&#20173;&#28982;&#23384;&#22312;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#19968;&#20010;&#24046;&#36317;&#26159;&#65292;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#29992;&#25143;&#21487;&#20197;&#20174;&#20854;&#20182;&#25968;&#25454;&#25152;&#26377;&#32773;&#25317;&#26377;&#30340;&#26032;&#25968;&#25454;&#20013;&#21463;&#30410;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#25152;&#26377;&#32773;&#21364;&#22352;&#22312;&#19968;&#22534;&#25968;&#25454;&#19978;&#65292;&#19981;&#30693;&#36947;&#35841;&#21487;&#20197;&#21463;&#30410;&#20110;&#23427;&#12290;&#36825;&#31181;&#24046;&#36317;&#20026;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#33258;&#21160;&#36830;&#25509;&#20379;&#27714;&#30340;&#22312;&#32447;&#24066;&#22330;&#21019;&#36896;&#20102;&#26426;&#20250;&#12290;&#34429;&#28982;&#22312;&#32447;&#21305;&#37197;&#24066;&#22330;&#24456;&#24120;&#35265;&#65288;&#20363;&#22914;&#65292;&#25171;&#36710;&#31995;&#32479;&#65289;&#65292;&#20294;&#20026;&#26426;&#22120;&#23398;&#20064;&#35774;&#35745;&#19968;&#20010;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#30340;&#24066;&#22330;&#38754;&#20020;&#35768;&#22810;&#21069;&#25152;&#26410;&#26377;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#26032;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#24066;&#22330;&#20013;&#30340;&#20004;&#20010;&#26680;&#24515;&#25361;&#25112;&#65306;&#65288;a&#65289;&#20026;&#20102;&#39640;&#25928;&#22320;&#23558;&#38656;&#27714;&#19982;&#20379;&#24212;&#21305;&#37197;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#20174;&#25968;&#21315;&#20010;&#25968;&#25454;&#27744;&#20013;&#33258;&#21160;&#21457;&#29616;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#25152;&#38656;&#30340;&#26377;&#29992;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data fuels machine learning (ML) - rich and high-quality training data is essential to the success of ML. However, to transform ML from the race among a few large corporations to an accessible technology that serves numerous normal users' data analysis requests, there still exist important challenges. One gap we observed is that many ML users can benefit from new data that other data owners possess, whereas these data owners sit on piles of data without knowing who can benefit from it. This gap creates the opportunity for building an online market that can automatically connect supply with demand. While online matching markets are prevalent (e.g., ride-hailing systems), designing a data-centric market for ML exhibits many unprecedented challenges.  This paper develops new techniques to tackle two core challenges in designing such a market: (a) to efficiently match demand with supply, we design an algorithm to automatically discover useful data for any ML task from a pool of thousands o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.11684</link><description>&lt;p&gt;
&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#30340;&#37327;&#23376;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
Quantum Acceleration of Infinite Horizon Average-Reward Reinforcement Learning. (arXiv:2310.11684v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#24378;&#21270;&#23398;&#20064;&#20013;&#37327;&#23376;&#21152;&#36895;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#65292;&#23454;&#29616;&#20102;&#25351;&#25968;&#32423;&#25913;&#36827;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#30456;&#36739;&#20110;&#32463;&#20856;&#31639;&#27861;&#65292;&#22312;&#36951;&#25022;&#30028;&#38480;&#19978;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#37327;&#23376;&#21152;&#36895;&#22312;&#35299;&#20915;&#26080;&#38480;&#26102;&#22495;Markov&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20013;&#25552;&#39640;&#24179;&#22343;&#22870;&#21169;&#32467;&#26524;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376;&#26694;&#26550;&#65292;&#29992;&#20110;&#20195;&#29702;&#19982;&#26410;&#30693;MDP&#30340;&#20114;&#21160;&#65292;&#25193;&#23637;&#20102;&#20256;&#32479;&#30340;&#20132;&#20114;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#35774;&#35745;&#19968;&#31181;&#22522;&#20110;&#20048;&#35266;&#20027;&#23548;&#30340;&#20855;&#26377;&#37327;&#23376;&#20449;&#21495;&#30340;&#34920;&#26684;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#39640;&#25928;&#30340;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#25216;&#26415;&#33719;&#21462;&#20195;&#29702;&#33719;&#21462;&#30340;&#37327;&#23376;&#20449;&#21495;&#12290;&#36890;&#36807;&#28145;&#20837;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#37327;&#23376;&#22343;&#20540;&#20272;&#35745;&#30340;&#20248;&#21183;&#33021;&#22815;&#22312;&#26080;&#38480;&#26102;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#23548;&#33268;&#36951;&#25022;&#20445;&#35777;&#30340;&#25351;&#25968;&#36827;&#23637;&#12290;&#20855;&#20307;&#22320;&#65292;&#25152;&#25552;&#20986;&#30340;&#37327;&#23376;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#36951;&#25022;&#30028;&#20026;$\tilde{\mathcal{O}}(1)$&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#30456;&#23545;&#20110;&#32463;&#20856;&#23545;&#24212;&#31639;&#27861;&#25152;&#23637;&#31034;&#30340;$\tilde{\mathcal{O}}(\sqrt{T})$&#30028;&#38480;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the potential of quantum acceleration in addressing infinite horizon Markov Decision Processes (MDPs) to enhance average reward outcomes. We introduce an innovative quantum framework for the agent's engagement with an unknown MDP, extending the conventional interaction paradigm. Our approach involves the design of an optimism-driven tabular Reinforcement Learning algorithm that harnesses quantum signals acquired by the agent through efficient quantum mean estimation techniques. Through thorough theoretical analysis, we demonstrate that the quantum advantage in mean estimation leads to exponential advancements in regret guarantees for infinite horizon Reinforcement Learning. Specifically, the proposed Quantum algorithm achieves a regret bound of $\tilde{\mathcal{O}}(1)$, a significant improvement over the $\tilde{\mathcal{O}}(\sqrt{T})$ bound exhibited by classical counterparts.
&lt;/p&gt;</description></item><item><title>CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.03635</link><description>&lt;p&gt;
CLEVRER-Humans: &#29992;&#20154;&#31867;&#30340;&#26041;&#24335;&#25551;&#36848;&#29289;&#29702;&#21644;&#22240;&#26524;&#20107;&#20214;
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans: Describing Physical and Causal Events the Human Way. (arXiv:2310.03635v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03635
&lt;/p&gt;
&lt;p&gt;
CLEVRER-Humans&#26159;&#19968;&#20010;&#29992;&#20110;&#22240;&#26524;&#21028;&#26029;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#26469;&#35299;&#20915;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#35821;&#35328;&#25551;&#36848;&#30340;&#32570;&#20047;&#22810;&#26679;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#21644;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#33021;&#22815;&#25512;&#29702;&#29289;&#29702;&#20107;&#20214;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#30340;&#26426;&#22120;&#23545;&#20110;&#19982;&#29289;&#29702;&#19990;&#30028;&#36827;&#34892;&#28789;&#27963;&#20114;&#21160;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22823;&#22810;&#25968;&#29289;&#29702;&#21644;&#22240;&#26524;&#25512;&#29702;&#22522;&#20934;&#37117;&#20165;&#22522;&#20110;&#21512;&#25104;&#20107;&#20214;&#21644;&#21512;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#36825;&#31181;&#35774;&#35745;&#23384;&#22312;&#20004;&#20010;&#38382;&#39064;&#65306;&#19968;&#26159;&#20107;&#20214;&#31867;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#32570;&#20047;&#22810;&#26679;&#24615;&#65307;&#20108;&#26159;&#22522;&#20110;&#25163;&#21160;&#23450;&#20041;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#30340;&#22240;&#26524;&#20851;&#31995;&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CLEVRER-Humans&#22522;&#20934;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20154;&#24037;&#26631;&#27880;&#30340;&#35270;&#39057;&#25512;&#29702;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#23545;&#29289;&#29702;&#20107;&#20214;&#30340;&#22240;&#26524;&#21028;&#26029;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#20004;&#31181;&#25216;&#26415;&#26469;&#25552;&#39640;&#25968;&#25454;&#25910;&#38598;&#25928;&#29575;&#65306;&#39318;&#20808;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#36845;&#20195;&#20107;&#20214;&#22635;&#31354;&#20219;&#21153;&#65292;&#20197; eliciting &#35270;&#39057;&#20013;&#20107;&#20214;&#30340;&#26032;&#34920;&#31034;&#26041;&#24335;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#22240;&#26524;&#20107;&#20214;&#22270; (CEGs)&#65307;&#20854;&#27425;&#65292;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00369</link><description>&lt;p&gt;
&#25552;&#28860;&#24402;&#32435;&#20559;&#24046;&#65306;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Distilling Inductive Bias: Knowledge Distillation Beyond Model Compression. (arXiv:2310.00369v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00369
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36229;&#36234;&#27169;&#22411;&#21387;&#32553;&#30340;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#65292;&#20351;Vision Transformers (ViTs) &#30340;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21253;&#25324;&#20351;&#29992;&#19968;&#32452;&#19981;&#21516;&#26550;&#26500;&#30340;&#25945;&#24072;&#27169;&#22411;&#26469;&#25351;&#23548;&#23398;&#29983;Transformer&#65292;&#20174;&#32780;&#26377;&#25928;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;Vision Transformers (ViTs) &#25552;&#20379;&#20102;&#22312;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20013;&#23454;&#29616;&#32479;&#19968;&#20449;&#24687;&#22788;&#29702;&#30340;&#35825;&#20154;&#21069;&#26223;&#12290;&#20294;&#26159;&#30001;&#20110;ViTs&#32570;&#20047;&#22266;&#26377;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#23427;&#20204;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#20351;&#23427;&#20204;&#30340;&#24212;&#29992;&#23454;&#38469;&#21487;&#34892;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#22522;&#20110;&#38598;&#25104;&#30340;&#33976;&#39311;&#26041;&#27861;&#65292;&#20174;&#36731;&#37327;&#32423;&#30340;&#25945;&#24072;&#27169;&#22411;&#20013;&#25552;&#21462;&#24402;&#32435;&#20559;&#24046;&#12290;&#20197;&#21069;&#30340;&#31995;&#32479;&#20165;&#20381;&#38752;&#22522;&#20110;&#21367;&#31215;&#30340;&#25945;&#23398;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23558;&#19968;&#32452;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#20542;&#21521;&#30340;&#36731;&#37327;&#32423;&#25945;&#24072;&#27169;&#22411;&#65288;&#20363;&#22914;&#21367;&#31215;&#21644;&#38750;&#32447;&#24615;&#21367;&#31215;&#65289;&#21516;&#26102;&#29992;&#20110;&#25351;&#23548;&#23398;&#29983;Transformer&#12290;&#30001;&#20110;&#36825;&#20123;&#29420;&#29305;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#25945;&#24072;&#27169;&#22411;&#21487;&#20197;&#20174;&#21508;&#31181;&#23384;&#20648;&#25968;&#25454;&#38598;&#20013;&#33719;&#24471;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#23398;&#29983;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#36824;&#28041;&#21450;&#39044;&#20808;&#35745;&#31639;&#21644;&#23384;&#20648;logits&#65292;&#20174;&#26681;&#26412;&#19978;&#23454;&#29616;&#20102;&#38750;&#24402;&#19968;&#21270;&#30340;&#29366;&#24577;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of computer vision, Vision Transformers (ViTs) offer the tantalizing prospect of unified information processing across visual and textual domains. But due to the lack of inherent inductive biases in ViTs, they require enormous amount of data for training. To make their applications practical, we introduce an innovative ensemble-based distillation approach distilling inductive bias from complementary lightweight teacher models. Prior systems relied solely on convolution-based teaching. However, this method incorporates an ensemble of light teachers with different architectural tendencies, such as convolution and involution, to instruct the student transformer jointly. Because of these unique inductive biases, instructors can accumulate a wide range of knowledge, even from readily identifiable stored datasets, which leads to enhanced student performance. Our proposed framework also involves precomputing and storing logits in advance, essentially the unnormalize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09924</link><description>&lt;p&gt;
&#22522;&#20110;&#28909;&#21644;&#27874;&#21160;&#21160;&#21147;&#23398;&#29305;&#24449;&#30340;&#22270;&#25299;&#25169;&#23646;&#24615;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Graph topological property recovery with heat and wave dynamics-based features on graphsD. (arXiv:2309.09924v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09924
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#29305;&#24449;&#26469;&#24674;&#22797;&#22270;&#30340;&#25299;&#25169;&#23646;&#24615;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#20248;&#31168;&#30340;&#34920;&#29616;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#20063;&#23637;&#29616;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22270;&#24494;&#20998;&#26041;&#31243;&#32593;&#32476;&#65288;GDeNet&#65289;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22270;&#19978;&#30340;PDE&#35299;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#20026;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#33719;&#24471;&#36830;&#32493;&#30340;&#33410;&#28857;&#21644;&#22270;&#32423;&#34920;&#31034;&#12290;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#28909;&#21644;&#27874;&#21160;&#26041;&#31243;&#21160;&#21147;&#23398;&#19982;&#22270;&#30340;&#35889;&#29305;&#24615;&#20197;&#21450;&#36830;&#32493;&#26102;&#38388;&#38543;&#26426;&#28216;&#36208;&#22312;&#22270;&#19978;&#34892;&#20026;&#20043;&#38388;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#38543;&#26426;&#22270;&#29983;&#25104;&#21442;&#25968;&#12289;Ricci&#26354;&#29575;&#21644;&#25345;&#20037;&#21516;&#35843;&#31561;&#26041;&#24335;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20123;&#21160;&#21147;&#23398;&#33021;&#22815;&#25429;&#25417;&#21040;&#22270;&#24418;&#20960;&#20309;&#21644;&#25299;&#25169;&#30340;&#26174;&#33879;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;GDeNet&#22312;&#21253;&#25324;&#24341;&#29992;&#22270;&#12289;&#33647;&#29289;&#20998;&#23376;&#21644;&#34507;&#30333;&#36136;&#22312;&#20869;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose Graph Differential Equation Network (GDeNet), an approach that harnesses the expressive power of solutions to PDEs on a graph to obtain continuous node- and graph-level representations for various downstream tasks. We derive theoretical results connecting the dynamics of heat and wave equations to the spectral properties of the graph and to the behavior of continuous-time random walks on graphs. We demonstrate experimentally that these dynamics are able to capture salient aspects of graph geometry and topology by recovering generating parameters of random graphs, Ricci curvature, and persistent homology. Furthermore, we demonstrate the superior performance of GDeNet on real-world datasets including citation graphs, drug-like molecules, and proteins.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#20248;&#21270;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#25928;&#26524;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#22320;&#22270;&#23450;&#20301;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#22312;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08731</link><description>&lt;p&gt;
&#25351;&#24341;&#30340;&#26041;&#27861;&#65306;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#25913;&#36827;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Pointing the Way: Refining Radar-Lidar Localization Using Learned ICP Weights. (arXiv:2309.08731v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;ICP&#26435;&#37325;&#20248;&#21270;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#30340;&#23450;&#20301;&#65292;&#20174;&#32780;&#25913;&#21892;&#20102;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#25928;&#26524;&#12290;&#36825;&#19968;&#26041;&#27861;&#22312;&#20445;&#25345;&#39640;&#36136;&#37327;&#22320;&#22270;&#23450;&#20301;&#24615;&#33021;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#22312;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#23450;&#20301;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25913;&#36827;&#38647;&#36798;&#27979;&#37327;&#23545;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#30340;&#23450;&#20301;&#12290;&#34429;&#28982;&#30446;&#21069;&#23450;&#20301;&#30340;&#25216;&#26415;&#27700;&#24179;&#26159;&#23558;&#28608;&#20809;&#38647;&#36798;&#25968;&#25454;&#19982;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#36827;&#34892;&#21305;&#37197;&#65292;&#20294;&#26159;&#38647;&#36798;&#34987;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#23545;&#38477;&#27700;&#21644;&#22823;&#38654;&#31561;&#24694;&#21155;&#22825;&#27668;&#20855;&#26377;&#26356;&#24378;&#30340;&#38887;&#24615;&#12290;&#20026;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;&#39640;&#36136;&#37327;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#65292;&#21516;&#26102;&#22312;&#24694;&#21155;&#22825;&#27668;&#19979;&#20445;&#25345;&#24615;&#33021;&#65292;&#23558;&#38647;&#36798;&#25968;&#25454;&#19982;&#28608;&#20809;&#38647;&#36798;&#22320;&#22270;&#36827;&#34892;&#21305;&#37197;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38647;&#36798;&#27979;&#37327;&#20013;&#23384;&#22312;&#30340;&#29420;&#29305;&#20266;&#24433;&#65292;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;&#19968;&#30452;&#38590;&#20197;&#36798;&#21040;&#19982;&#28608;&#20809;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#31995;&#32479;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#26080;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#12290;&#26412;&#24037;&#20316;&#22312;&#22522;&#20110;ICP&#30340;&#38647;&#36798;-&#28608;&#20809;&#38647;&#36798;&#23450;&#20301;&#31995;&#32479;&#22522;&#30784;&#19978;&#65292;&#21253;&#25324;&#19968;&#20010;&#23398;&#20064;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#26681;&#25454;&#39640;&#23618;&#27425;&#30340;&#25195;&#25551;&#20449;&#24687;&#23545;&#38647;&#36798;&#28857;&#36827;&#34892;&#21152;&#26435;&#12290;&#23558;&#32463;&#36807;&#39564;&#35777;&#30340;&#20998;&#26512;&#26041;&#27861;&#19982;&#23398;&#20064;&#21040;&#30340;&#26435;&#37325;&#30456;&#32467;&#21512;&#65292;&#20943;&#23567;&#20102;&#38647;&#36798;&#23450;&#20301;&#20013;&#30340;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel deep-learning-based approach to improve localizing radar measurements against lidar maps. Although the state of the art for localization is matching lidar data to lidar maps, radar has been considered as a promising alternative, as it is potentially more resilient against adverse weather such as precipitation and heavy fog. To make use of existing high-quality lidar maps, while maintaining performance in adverse weather, matching radar data to lidar maps is of interest. However, owing in part to the unique artefacts present in radar measurements, radar-lidar localization has struggled to achieve comparable performance to lidar-lidar systems, preventing it from being viable for autonomous driving. This work builds on an ICP-based radar-lidar localization system by including a learned preprocessing step that weights radar points based on high-level scan information. Combining a proven analytical approach with a learned weight reduces localization errors in rad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#21152;&#36895;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#21644;&#32858;&#31867;&#36136;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08710</link><description>&lt;p&gt;
&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Clustered Multi-Agent Linear Bandits. (arXiv:2309.08710v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#26469;&#21152;&#36895;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#36951;&#25022;&#26368;&#23567;&#21270;&#21644;&#32858;&#31867;&#36136;&#37327;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#38543;&#26426;&#36172;&#21338;&#38382;&#39064;&#30340;&#19968;&#20010;&#29305;&#23450;&#23454;&#20363;&#65292;&#21363;&#38598;&#32676;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#32447;&#24615;&#36172;&#21338;&#26426;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#26469;&#21152;&#36895;&#25972;&#20307;&#20248;&#21270;&#38382;&#39064;&#12290;&#22312;&#36825;&#19968;&#36129;&#29486;&#20013;&#65292;&#32593;&#32476;&#25511;&#21046;&#22120;&#36127;&#36131;&#20272;&#35745;&#32593;&#32476;&#30340;&#22522;&#26412;&#38598;&#32676;&#32467;&#26500;&#24182;&#20248;&#21270;&#21516;&#19968;&#32452;&#20013;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#32463;&#39564;&#20998;&#20139;&#12290;&#25105;&#20204;&#23545;&#36951;&#25022;&#26368;&#23567;&#21270;&#38382;&#39064;&#21644;&#32858;&#31867;&#36136;&#37327;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#19982;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#25105;&#20204;&#30340;&#31639;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#36951;&#25022;&#26368;&#23567;&#21270;&#65292;&#24182;&#25104;&#21151;&#24674;&#22797;&#20102;&#30495;&#23454;&#30340;&#22522;&#26412;&#38598;&#32676;&#21010;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We address in this paper a particular instance of the multi-agent linear stochastic bandit problem, called clustered multi-agent linear bandits. In this setting, we propose a novel algorithm leveraging an efficient collaboration between the agents in order to accelerate the overall optimization problem. In this contribution, a network controller is responsible for estimating the underlying cluster structure of the network and optimizing the experiences sharing among agents within the same groups. We provide a theoretical analysis for both the regret minimization problem and the clustering quality. Through empirical evaluation against state-of-the-art algorithms on both synthetic and real data, we demonstrate the effectiveness of our approach: our algorithm significantly improves regret minimization while managing to recover the true underlying cluster partitioning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2306.07566</link><description>&lt;p&gt;
&#23398;&#20064;&#36873;&#25321;&#26631;&#31614;&#19979;&#30340;&#24322;&#36136;&#20915;&#31574;&#32773;&#65306;&#19968;&#31181;&#24037;&#20855;&#21464;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Learning under Selective Labels with Heterogeneous Decision-makers: An Instrumental Variable Approach. (arXiv:2306.07566v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#31181;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#36873;&#25321;&#24615;&#26631;&#35760;&#25968;&#25454;&#19979;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#36825;&#31181;&#38382;&#39064;&#22312;&#21382;&#21490;&#20915;&#31574;&#23548;&#33268;&#32467;&#26524;&#20165;&#37096;&#20998;&#26631;&#35760;&#26102;&#20986;&#29616;&#12290;&#26631;&#35760;&#25968;&#25454;&#20998;&#24067;&#21487;&#33021;&#19982;&#25972;&#20307;&#20154;&#32676;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#24403;&#21382;&#21490;&#20915;&#31574;&#21644;&#30446;&#26631;&#32467;&#26524;&#21487;&#20197;&#21516;&#26102;&#21463;&#26576;&#20123;&#26410;&#35266;&#23519;&#21040;&#30340;&#22240;&#32032;&#24433;&#21709;&#26102;&#12290;&#22240;&#27492;&#65292;&#20165;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#25972;&#20307;&#20154;&#32676;&#20013;&#30340;&#20005;&#37325;&#20559;&#24046;&#12290;&#25105;&#20204;&#30340;&#35770;&#25991;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#24212;&#29992;&#20013;&#21382;&#21490;&#20915;&#31574;&#30001;&#19968;&#32452;&#24322;&#36136;&#20915;&#31574;&#32773;&#20570;&#20986;&#30340;&#20107;&#23454;&#26469;&#35299;&#20915;&#27492;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#19968;&#20010;&#26377;&#21407;&#29702;&#30340;&#24037;&#20855;&#21464;&#37327;&#26694;&#26550;&#19979;&#20998;&#26512;&#20102;&#36825;&#31181;&#35774;&#32622;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#28385;&#36275;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#26102;&#20219;&#20309;&#32473;&#23450;&#39044;&#27979;&#35268;&#21017;&#30340;&#20840;&#20307;&#39118;&#38505;&#30340;&#28857;&#35782;&#21035;&#26465;&#20214;&#65292;&#24182;&#22312;&#28857;&#35782;&#21035;&#22833;&#36133;&#26102;&#25552;&#20379;&#20102;&#23574;&#38160;&#30340;&#39118;&#38505;&#30028;&#38480;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of learning with selectively labeled data, which arises when outcomes are only partially labeled due to historical decision-making. The labeled data distribution may substantially differ from the full population, especially when the historical decisions and the target outcome can be simultaneously affected by some unobserved factors. Consequently, learning with only the labeled data may lead to severely biased results when deployed to the full population. Our paper tackles this challenge by exploiting the fact that in many applications the historical decisions were made by a set of heterogeneous decision-makers. In particular, we analyze this setup in a principled instrumental variable (IV) framework. We establish conditions for the full-population risk of any given prediction rule to be point-identified from the observed data and provide sharp risk bounds when the point identification fails. We further propose a weighted learning approach that learns prediction ru
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20351;&#29992;InceptionTime&#21644;ROCKET&#26041;&#27861;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#20197;&#30417;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#25163;&#33109;&#36816;&#21160;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#26377;&#26041;&#27861;&#37117;&#36866;&#29992;&#20110;&#20272;&#35745;&#38663;&#39076;&#20005;&#37325;&#31243;&#24230;&#21644;&#32908;&#32905;&#24378;&#30452;&#30340;&#23384;&#22312;&#65292;&#20294;&#22312;&#26816;&#27979;&#36816;&#21160;&#38556;&#30861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#30340;InceptionTime&#26041;&#27861;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26174;&#31034;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;PD&#30151;&#29366;&#30417;&#27979;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11265</link><description>&lt;p&gt;
&#25163;&#33109;&#21160;&#20316;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#29992;&#20110;&#24085;&#37329;&#26862;&#30149;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Time Series Classification for Detecting Parkinson's Disease from Wrist Motions. (arXiv:2304.11265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11265
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20351;&#29992;InceptionTime&#21644;ROCKET&#26041;&#27861;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#65292;&#20197;&#30417;&#27979;&#24085;&#37329;&#26862;&#30149;&#24739;&#32773;&#30340;&#25163;&#33109;&#36816;&#21160;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#25152;&#26377;&#26041;&#27861;&#37117;&#36866;&#29992;&#20110;&#20272;&#35745;&#38663;&#39076;&#20005;&#37325;&#31243;&#24230;&#21644;&#32908;&#32905;&#24378;&#30452;&#30340;&#23384;&#22312;&#65292;&#20294;&#22312;&#26816;&#27979;&#36816;&#21160;&#38556;&#30861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#30340;InceptionTime&#26041;&#27861;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#26174;&#31034;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;PD&#30151;&#29366;&#30417;&#27979;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24085;&#37329;&#26862;&#30149;&#26159;&#19968;&#31181;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20855;&#26377;&#39057;&#32321;&#21464;&#21270;&#30340;&#36816;&#21160;&#30151;&#29366;&#65292;&#25345;&#32493;&#30340;&#30151;&#29366;&#30417;&#27979;&#21487;&#20197;&#23454;&#29616;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#27835;&#30103;&#12290;&#20256;&#32479;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#22312;&#20351;&#29992;&#21487;&#31359;&#25140;&#21152;&#36895;&#24230;&#35745;&#25968;&#25454;&#36827;&#34892;PD&#30151;&#29366;&#30417;&#27979;&#26102;&#24615;&#33021;&#26377;&#38480;&#65292;&#22240;&#20026;PD&#36816;&#21160;&#27169;&#24335;&#20855;&#26377;&#22797;&#26434;&#24615;&#65292;&#20294;&#25968;&#25454;&#38598;&#24456;&#23567;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;InceptionTime&#21644;RandOm&#21367;&#31215;&#26680;&#21464;&#25442;&#65288;ROCKET&#65289;&#65292;&#22240;&#20026;&#23427;&#20204;&#26159;TSC&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#24182;&#19988;&#23545;&#20110;PD&#30151;&#29366;&#30417;&#27979;&#38750;&#24120;&#26377;&#21069;&#26223;&#65306;InceptionTime&#30340;&#39640;&#23398;&#20064;&#33021;&#21147;&#36866;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#36816;&#21160;&#27169;&#24335;&#65292;&#32780;ROCKET&#36866;&#29992;&#20110;&#23567;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#38543;&#26426;&#25628;&#32034;&#25214;&#21040;&#20102;&#26368;&#39640;&#24471;&#20998;&#30340;InceptionTime&#32467;&#26500;&#65292;&#24182;&#23558;&#20854;&#19982;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#30340;ROCKET&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#29992;&#20110;PD&#24739;&#32773;&#30340;&#25163;&#33109;&#36816;&#21160;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#26377;&#26041;&#27861;&#37117;&#36866;&#29992;&#20110;&#20272;&#35745;&#38663;&#39076;&#20005;&#37325;&#31243;&#24230;&#21644;&#32908;&#32905;&#24378;&#30452;&#30340;&#23384;&#22312;&#65292;&#20294;&#22312;&#26816;&#27979;&#36816;&#21160;&#38556;&#30861;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20855;&#26377;&#23725;&#20998;&#31867;&#22120;&#30340;InceptionTime&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;TSC&#22312;&#22522;&#20110;&#21487;&#31359;&#25140;&#35774;&#22791;&#30340;PD&#30151;&#29366;&#30417;&#27979;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parkinson's disease (PD) is a neurodegenerative disease with frequently changing motor symptoms where continuous symptom monitoring enables more targeted treatment. Classical time series classification (TSC) and deep learning techniques have limited performance for PD symptom monitoring using wearable accelerometer data because PD movement patterns are complex, but datasets are small. We investigate InceptionTime and RandOm Convolutional KErnel Transform (ROCKET) because they are state-of-the-art for TSC and promising for PD symptom monitoring: InceptionTime's high learning capacity is suited to modeling complex movement patterns while ROCKET is suited to small datasets. We used a random search to find the highest-scoring InceptionTime architecture and compared it to ROCKET with a ridge classifier and a multi-layer perceptron (MLP) on wrist motions of PD patients. We find that all approaches are suitable for estimating tremor severity and bradykinesia presence but struggle with detecti
&lt;/p&gt;</description></item></channel></rss>