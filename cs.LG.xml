<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32452;&#20985;&#27491;&#21017;&#21270;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#31232;&#30095;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#29305;&#24449;&#24182;&#20445;&#25345;&#31283;&#23450;&#30340;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.00344</link><description>&lt;p&gt;
&#20351;&#29992;&#32452;&#20985;&#27491;&#21017;&#21270;&#30340;&#31232;&#30095;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Sparse-Input Neural Network using Group Concave Regularization. (arXiv:2307.00344v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00344
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32452;&#20985;&#27491;&#21017;&#21270;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#31232;&#30095;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#36873;&#25321;&#37325;&#35201;&#30340;&#29305;&#24449;&#24182;&#20445;&#25345;&#31283;&#23450;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#21644;&#38750;&#32447;&#24615;&#20989;&#25968;&#20272;&#35745;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#20854;&#20013;&#21464;&#37327;&#30340;&#25968;&#37327;&#36229;&#36807;&#20102;&#24314;&#27169;&#20013;&#21487;&#29992;&#30340;&#26679;&#26412;&#22823;&#23567;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;&#34429;&#28982;&#32452;LASSO&#24050;&#32463;&#34987;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#20013;&#36873;&#25321;&#21464;&#37327;&#65292;&#20294;&#23427;&#20542;&#21521;&#20110;&#36873;&#25321;&#26080;&#20851;&#32039;&#35201;&#30340;&#21464;&#37327;&#26469;&#24357;&#34917;&#36807;&#24230;&#32553;&#20943;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31232;&#30095;&#36755;&#20837;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#20351;&#29992;&#32452;&#20985;&#27491;&#21017;&#21270;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#65292;&#36866;&#29992;&#20110;&#20302;&#32500;&#21644;&#39640;&#32500;&#35774;&#32622;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#23545;&#27599;&#20010;&#36755;&#20837;&#33410;&#28857;&#30340;&#25152;&#26377;&#20986;&#31449;&#36830;&#25509;&#30340;&#26435;&#37325;&#30340;l2&#33539;&#25968;&#24212;&#29992;&#36866;&#24403;&#30340;&#20985;&#24809;&#32602;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#21482;&#20351;&#29992;&#21407;&#22987;&#21464;&#37327;&#30340;&#19968;&#20010;&#23567;&#23376;&#38598;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22522;&#20110;&#21521;&#21518;&#36335;&#24452;&#20248;&#21270;&#24320;&#21457;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#33719;&#24471;&#31283;&#23450;&#30340;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simultaneous feature selection and non-linear function estimation are challenging, especially in high-dimensional settings where the number of variables exceeds the available sample size in modeling. In this article, we investigate the problem of feature selection in neural networks. Although the group LASSO has been utilized to select variables for learning with neural networks, it tends to select unimportant variables into the model to compensate for its over-shrinkage. To overcome this limitation, we propose a framework of sparse-input neural networks using group concave regularization for feature selection in both low-dimensional and high-dimensional settings. The main idea is to apply a proper concave penalty to the $l_2$ norm of weights from all outgoing connections of each input node, and thus obtain a neural net that only uses a small subset of the original variables. In addition, we develop an effective algorithm based on backward path-wise optimization to yield stable solutio
&lt;/p&gt;</description></item></channel></rss>