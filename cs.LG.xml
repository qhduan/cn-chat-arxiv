<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#35299;&#37322;&#38169;&#35823;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#35299;&#37322;&#31639;&#27861;&#38656;&#35201;&#26126;&#30830;&#20854;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#24182;&#28548;&#28165;&#21487;&#20197;&#21644;&#19981;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02870</link><description>&lt;p&gt;
&#27809;&#26377;&#35299;&#37322;&#30340;&#32479;&#35745;&#23398;&#65306;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#20919;&#38745;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Statistics without Interpretation: A Sober Look at Explainable Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02870
&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#35299;&#37322;&#38169;&#35823;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#35299;&#37322;&#31639;&#27861;&#38656;&#35201;&#26126;&#30830;&#20854;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#24182;&#28548;&#28165;&#21487;&#20197;&#21644;&#19981;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#20110;&#35299;&#37322;&#31639;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#25991;&#29486;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#24448;&#24448;&#19981;&#28165;&#26970;&#25152;&#29992;&#20110;&#20309;&#22788;&#21450;&#20854;&#20351;&#29992;&#26041;&#24335;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#22312;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#28165;&#26224;&#35299;&#37322;&#30340;&#22797;&#26434;&#32479;&#35745;&#26041;&#27861;&#24456;&#21487;&#33021;&#23548;&#33268;&#35299;&#37322;&#30340;&#38169;&#35823;&#65292;&#36825;&#19968;&#20107;&#23454;&#22312;&#25991;&#29486;&#20013;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#20851;&#20110;&#35299;&#37322;&#31639;&#27861;&#30340;&#35770;&#25991;&#24212;&#26126;&#30830;&#35299;&#37322;&#31639;&#27861;&#30340;&#36755;&#20986;&#22914;&#20309;&#35299;&#37322;&#12290;&#20182;&#20204;&#36824;&#24212;&#28548;&#28165;&#22312;&#32473;&#20986;&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22238;&#31572;&#21738;&#20123;&#20851;&#20110;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21738;&#20123;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#23427;&#20204;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#23427;&#36824;&#20381;&#36182;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used. We argue that this is because explanation algorithms are often mathematically complex but don't admit a clear interpretation. Unfortunately, complex statistical methods that don't have a clear interpretation are bound to lead to errors in interpretation, a fact that has become increasingly apparent in the literature. In order to move forward, papers on explanation algorithms should make clear how precisely the output of the algorithms should be interpreted. They should also clarify what questions about the function can and cannot be answered given the explanations. Our argument is based on the distinction between statistics and their interpretation. It also relies on parallels between explainable machine learning and applied statistics.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;</title><link>https://arxiv.org/abs/2402.02242</link><description>&lt;p&gt;
&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Fine-Tuning for Pre-Trained Vision Models: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02242
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#30740;&#20102;&#38754;&#21521;&#39044;&#35757;&#32451;&#35270;&#35273;&#27169;&#22411;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#24182;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#27169;&#22411;&#65288;PVMs&#65289;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#26368;&#20808;&#36827;&#30340;PVMs&#36798;&#21040;&#25968;&#21313;&#20159;&#29978;&#33267;&#25968;&#19975;&#20159;&#20010;&#21442;&#25968;&#65292;&#26631;&#20934;&#30340;&#20840;&#38754;&#24494;&#35843;&#33539;&#24335;&#30001;&#20110;&#39640;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#21464;&#24471;&#19981;&#21487;&#25345;&#32493;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#65292;&#26088;&#22312;&#20197;&#26368;&#23567;&#21442;&#25968;&#20462;&#25913;&#36229;&#36234;&#20840;&#38754;&#24494;&#35843;&#30340;&#24615;&#33021;&#12290;&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#35270;&#35273;PEFT&#30340;&#20840;&#38754;&#27010;&#36848;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#23545;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;PEFT&#30340;&#27491;&#24335;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#29616;&#26377;&#26041;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#22522;&#20110;&#28155;&#21152;&#30340;&#12289;&#22522;&#20110;&#37096;&#20998;&#30340;&#21644;&#22522;&#20110;&#32479;&#19968;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#28508;&#22312;&#30340;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;&#35813;&#32508;&#36848;&#36824;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#36164;&#28304;&#25910;&#34255;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained vision models (PVMs) have shown great potential for adaptability across various downstream vision tasks. However, with state-of-the-art PVMs growing to billions or even trillions of parameters, the standard full fine-tuning paradigm is becoming unsustainable due to high computational and storage demands. In response, researchers are exploring parameter-efficient fine-tuning (PEFT), which seeks to exceed the performance of full fine-tuning with minimal parameter modifications. This survey provides a comprehensive overview and future directions for visual PEFT, offering a systematic review of the latest advancements. First, we provide a formal definition of PEFT and discuss model pre-training methods. We then categorize existing methods into three categories: addition-based, partial-based, and unified-based. Finally, we introduce the commonly used datasets and applications and suggest potential future research challenges. A comprehensive collection of resources is
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaskSub&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36974;&#32617;&#23376;&#27169;&#22411;&#21644;&#25918;&#26494;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24378;&#21270;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36974;&#32617;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2306.11339</link><description>&lt;p&gt;
&#36974;&#32617;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Masking Augmentation for Supervised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.11339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MaskSub&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36974;&#32617;&#23376;&#27169;&#22411;&#21644;&#25918;&#26494;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#24378;&#21270;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#36974;&#32617;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#24182;&#21152;&#36895;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#38543;&#26426;&#36974;&#32617;&#36827;&#34892;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#25216;&#26415;&#20013;&#30340;&#26032;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;&#23398;&#20064;&#22312;&#37319;&#29992;&#36974;&#32617;&#22686;&#24378;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#19981;&#31283;&#23450;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28041;&#21450;&#36974;&#32617;&#22686;&#24378;&#30340;&#26032;&#26041;&#27861;&#65292;&#31216;&#20026;Masked Sub-model (MaskSub)&#12290;MaskSub&#30001;&#20027;&#27169;&#22411;&#21644;&#23376;&#27169;&#22411;&#32452;&#25104;&#65307;&#21069;&#32773;&#20139;&#21463;&#20256;&#32479;&#35757;&#32451;&#26041;&#27861;&#65292;&#32780;&#21518;&#32773;&#21033;&#29992;&#24378;&#22823;&#30340;&#36974;&#32617;&#22686;&#24378;&#26469;&#35757;&#32451;&#12290;MaskSub&#36890;&#36807;&#32531;&#35299;&#31867;&#20284;&#20110;&#33258;&#33976;&#39311;&#25439;&#22833;&#30340;&#25918;&#26494;&#25439;&#22833;&#20989;&#25968;&#26469;&#35299;&#20915;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;MaskSub&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#35757;&#32451;&#25439;&#22833;&#30340;&#25910;&#25947;&#36895;&#24230;&#29978;&#33267;&#27604;&#24120;&#35268;&#35757;&#32451;&#26356;&#24555;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;MaskSub&#22312;&#21508;&#31181;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;DeiT-III&#65292;MAE&#24494;&#35843;&#65292;CLIP&#24494;&#35843;&#65292;ResNet&#21644;Swin T&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.11339v2 Announce Type: replace-cross  Abstract: Pre-training using random masking has emerged as a novel trend in training techniques. However, supervised learning faces a challenge in adopting masking augmentations, primarily due to unstable training. In this paper, we propose a novel way to involve masking augmentations dubbed Masked Sub-model (MaskSub). MaskSub consists of the main-model and sub-model; while the former enjoys conventional training recipes, the latter leverages the benefit of strong masking augmentations in training. MaskSub addresses the challenge by mitigating adverse effects through a relaxed loss function similar to a self-distillation loss. Our analysis shows that MaskSub improves performance, with the training loss converging even faster than regular training, which suggests our method facilitates training. We further validate MaskSub across diverse training recipes and models, including DeiT-III, MAE fine-tuning, CLIP fine-tuning, ResNet, and Swin T
&lt;/p&gt;</description></item><item><title>DistDNAS&#26159;&#19968;&#31181;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#25628;&#32034;&#29305;&#24449;&#20132;&#20114;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#25628;&#32034;&#21644;&#36873;&#25321;&#26368;&#20339;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#21152;&#36895;&#24182;&#23558;&#25628;&#32034;&#26102;&#38388;&#20174;2&#22825;&#32553;&#30701;&#21040;2&#23567;&#26102;&#12290;</title><link>http://arxiv.org/abs/2311.00231</link><description>&lt;p&gt;
DistDNAS: &#22312;2&#23567;&#26102;&#20869;&#39640;&#25928;&#25628;&#32034;&#29305;&#24449;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
DistDNAS: Search Efficient Feature Interactions within 2 Hours. (arXiv:2311.00231v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00231
&lt;/p&gt;
&lt;p&gt;
DistDNAS&#26159;&#19968;&#31181;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39640;&#25928;&#25628;&#32034;&#29305;&#24449;&#20132;&#20114;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20998;&#24067;&#24335;&#25628;&#32034;&#21644;&#36873;&#25321;&#26368;&#20339;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#24040;&#22823;&#30340;&#21152;&#36895;&#24182;&#23558;&#25628;&#32034;&#26102;&#38388;&#20174;2&#22825;&#32553;&#30701;&#21040;2&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#65292;&#25628;&#32034;&#25928;&#29575;&#21644;&#26381;&#21153;&#25928;&#29575;&#26159;&#26500;&#24314;&#29305;&#24449;&#20132;&#20114;&#21644;&#21152;&#24555;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#30340;&#20004;&#20010;&#20027;&#35201;&#26041;&#38754;&#12290;&#22312;&#22823;&#35268;&#27169;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#30001;&#20110;&#22823;&#37327;&#25968;&#25454;&#19978;&#30340;&#39034;&#24207;&#24037;&#20316;&#27969;&#31243;&#65292;&#25628;&#32034;&#26368;&#20339;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#38656;&#35201;&#20184;&#20986;&#24040;&#22823;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#34701;&#21512;&#21508;&#31181;&#26469;&#28304;&#12289;&#39034;&#24207;&#21644;&#25968;&#23398;&#36816;&#31639;&#30340;&#20132;&#20114;&#20250;&#24341;&#20837;&#28508;&#22312;&#30340;&#20914;&#31361;&#21644;&#39069;&#22806;&#30340;&#20887;&#20313;&#65292;&#23548;&#33268;&#24615;&#33021;&#21644;&#26381;&#21153;&#25104;&#26412;&#30340;&#27425;&#20248;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DistDNAS&#20316;&#20026;&#19968;&#31181;&#31616;&#27905;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#39640;&#25928;&#22320;&#36827;&#34892;&#29305;&#24449;&#20132;&#20114;&#35774;&#35745;&#12290;DistDNAS&#25552;&#20986;&#20102;&#19968;&#20010;&#36229;&#32423;&#32593;&#32476;&#65292;&#23558;&#19981;&#21516;&#39034;&#24207;&#21644;&#31867;&#22411;&#30340;&#20132;&#20114;&#27169;&#22359;&#20316;&#20026;&#25628;&#32034;&#31354;&#38388;&#36827;&#34892;&#25972;&#21512;&#12290;&#20026;&#20102;&#20248;&#21270;&#25628;&#32034;&#25928;&#29575;&#65292;DistDNAS&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#26085;&#26399;&#19978;&#20998;&#24067;&#24335;&#25628;&#32034;&#24182;&#27719;&#24635;&#36873;&#25321;&#26368;&#20339;&#30340;&#20132;&#20114;&#27169;&#22359;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;25&#20493;&#30340;&#21152;&#36895;&#65292;&#23558;&#25628;&#32034;&#25104;&#26412;&#20174;2&#22825;&#20943;&#23569;&#21040;2&#23567;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Search efficiency and serving efficiency are two major axes in building feature interactions and expediting the model development process in recommender systems. On large-scale benchmarks, searching for the optimal feature interaction design requires extensive cost due to the sequential workflow on the large volume of data. In addition, fusing interactions of various sources, orders, and mathematical operations introduces potential conflicts and additional redundancy toward recommender models, leading to sub-optimal trade-offs in performance and serving cost. In this paper, we present DistDNAS as a neat solution to brew swift and efficient feature interaction design. DistDNAS proposes a supernet to incorporate interaction modules of varying orders and types as a search space. To optimize search efficiency, DistDNAS distributes the search and aggregates the choice of optimal interaction modules on varying data dates, achieving over 25x speed-up and reducing search cost from 2 days to 2 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#39046;&#22495;&#20132;&#20114;&#20449;&#24687;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#26469;&#36827;&#34892;&#26032;&#39046;&#22495;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20010;AI&#21161;&#25163;&#24212;&#29992;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06302</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#30693;&#35782;&#22686;&#24378;&#22312;AI&#21161;&#25163;&#24212;&#29992;&#20013;&#30340;&#38646;&#26679;&#26412;&#21644;&#22810;&#39046;&#22495;&#25512;&#33616;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Knowledge Enhancement for Zero-Shot and Multi-Domain Recommendation in an AI Assistant Application. (arXiv:2306.06302v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22810;&#39046;&#22495;&#20132;&#20114;&#20449;&#24687;&#21644;&#22806;&#37096;&#30693;&#35782;&#22270;&#26469;&#36827;&#34892;&#26032;&#39046;&#22495;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#19968;&#20010;AI&#21161;&#25163;&#24212;&#29992;&#20013;&#65292;&#20197;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#21830;&#19994;&#19978;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#38590;&#20197;&#23558;&#26032;&#29992;&#25143;&#25972;&#21512;&#36827;&#21435;&#12290;&#30001;&#20110;&#29992;&#25143;&#32463;&#24120;&#22312;&#19981;&#21516;&#39046;&#22495;&#19982;&#20869;&#23481;&#36827;&#34892;&#20132;&#20114;&#65292;&#22240;&#27492;&#21487;&#20197;&#21033;&#29992;&#29992;&#25143;&#22312;&#20043;&#21069;&#30340;&#39046;&#22495;&#20013;&#30340;&#20132;&#20114;&#26469;&#25913;&#21892;&#20854;&#22312;&#26032;&#39046;&#22495;&#20013;&#30340;&#25512;&#33616;&#65288;&#22810;&#39046;&#22495;&#25512;&#33616;&#65289;&#12290;&#30693;&#35782;&#22270;&#22686;&#24378;&#30340;&#21333;&#19968;&#39046;&#22495;&#25512;&#33616;&#65288;&#30693;&#35782;&#22270;&#22686;&#24378;&#65289;&#30340;&#30740;&#31350;&#32447;&#31243;&#29420;&#31435;&#20110;&#27492;&#20351;&#29992;&#22806;&#37096;&#30693;&#35782;&#22270;&#26469;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#23558;&#36825;&#20123;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65306;&#21033;&#29992;&#20854;&#20182;&#39046;&#22495;&#20013;&#30340;&#20132;&#20114;&#20449;&#24687;&#20197;&#21450;&#22806;&#37096;&#30693;&#35782;&#22270;&#26469;&#36827;&#34892;&#26032;&#39046;&#22495;&#30340;&#25512;&#33616;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#24819;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#20174;&#25968;&#30334;&#19975;&#29992;&#25143;&#35831;&#27714;&#30340;&#35270;&#39057;&#12289;&#38899;&#20048;&#21644;&#20070;&#31821;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;&#35813;&#25968;&#25454;&#38598;&#29992;&#20110;&#19968;&#20010;AI&#21161;&#25163;&#24212;&#29992;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have found significant commercial success but still struggle with integrating new users. Since users often interact with content in different domains, it is possible to leverage a user's interactions in previous domains to improve that user's recommendations in a new one (multi-domain recommendation). A separate research thread on knowledge graph enhancement uses external knowledge graphs to improve single domain recommendations (knowledge graph enhancement). Both research threads incorporate related information to improve predictions in a new domain. We propose in this work to unify these approaches: Using information from interactions in other domains as well as external knowledge graphs to make predictions in a new domain that would be impossible with either information source alone. We apply these ideas to a dataset derived from millions of users' requests for content across three domains (videos, music, and books) in a live virtual assistant application. We dem
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.18593</link><description>&lt;p&gt;
&#20851;&#20110;&#25193;&#25955;&#24314;&#27169;&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On Diffusion Modeling for Anomaly Detection. (arXiv:2305.18593v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18593
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25193;&#25955;&#24314;&#27169;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#34920;&#29616;&#24456;&#22909;&#20294;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#22240;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#8212;&#8212;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#65292;&#24182;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#20197;&#20854;&#22312;&#29983;&#25104;&#24314;&#27169;&#20013;&#30340;&#20248;&#24322;&#24615;&#33021;&#32780;&#38395;&#21517;&#65292;&#25104;&#20026;&#22522;&#20110;&#23494;&#24230;&#30340;&#24322;&#24120;&#26816;&#27979;&#30340;&#26377;&#21560;&#24341;&#21147;&#30340;&#20505;&#36873;&#31639;&#27861;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21508;&#31181;&#25193;&#25955;&#24314;&#27169;&#26041;&#27861;&#22312;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#12290;&#23588;&#20854;&#26159;&#21457;&#29616;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#22312;&#24322;&#24120;&#26816;&#27979;&#26041;&#38754;&#20855;&#22791;&#24456;&#22909;&#30340;&#34920;&#29616;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#36739;&#39640;&#12290;&#36890;&#36807;&#31616;&#21270;DDPM&#22312;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#24212;&#29992;&#65292;&#25105;&#20204;&#33258;&#28982;&#22320;&#24341;&#20986;&#21478;&#19968;&#31181;&#31216;&#20026;&#25193;&#25955;&#26102;&#38388;&#27010;&#29575;&#27169;&#22411;&#65288;DTPM&#65289;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;DTPM&#20272;&#35745;&#32473;&#23450;&#36755;&#20837;&#30340;&#25193;&#25955;&#26102;&#38388;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#33021;&#22815;&#36890;&#36807;&#36739;&#22823;&#30340;&#26102;&#38388;&#27493;&#38271;&#19978;&#30340;&#39640;&#21518;&#39564;&#23494;&#24230;&#35782;&#21035;&#24322;&#24120;&#12290;&#25105;&#20204;&#23548;&#20986;&#20102;&#27492;&#21518;&#39564;&#20998;&#24067;&#30340;&#35299;&#26512;&#24418;&#24335;&#65292;&#24182;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25552;&#39640;&#25512;&#29702;&#25928;&#29575;&#12290;&#36890;&#36807;&#22312;ADBenh&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Known for their impressive performance in generative modeling, diffusion models are attractive candidates for density-based anomaly detection. This paper investigates different variations of diffusion modeling for unsupervised and semi-supervised anomaly detection. In particular, we find that Denoising Diffusion Probability Models (DDPM) are performant on anomaly detection benchmarks yet computationally expensive. By simplifying DDPM in application to anomaly detection, we are naturally led to an alternative approach called Diffusion Time Probabilistic Model (DTPM). DTPM estimates the posterior distribution over diffusion time for a given input, enabling the identification of anomalies due to their higher posterior density at larger timesteps. We derive an analytical form for this posterior density and leverage a deep neural network to improve inference efficiency. Through empirical evaluations on the ADBench benchmark, we demonstrate that all diffusion-based anomaly detection methods 
&lt;/p&gt;</description></item><item><title>Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.12693</link><description>&lt;p&gt;
Phylo2Vec: &#19968;&#31181;&#20108;&#21449;&#26641;&#30340;&#21521;&#37327;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec: a vector representation for binary trees. (arXiv:2304.12693v1 [q-bio.PE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12693
&lt;/p&gt;
&lt;p&gt;
Phylo2Vec&#26159;&#19968;&#31181;&#26032;&#30340;&#20108;&#21449;&#26641;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#36731;&#26494;&#37319;&#26679;&#20108;&#21449;&#26641;&#65292;&#24182;&#20197;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#36825;&#31181;&#26041;&#27861;&#29992;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#34507;&#30333;&#36136;&#31867;&#21035;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#29289;&#25968;&#25454;&#25512;&#26029;&#24471;&#21040;&#30340;&#20108;&#21449;&#36827;&#21270;&#26641;&#23545;&#20110;&#29702;&#35299;&#29983;&#29289;&#20043;&#38388;&#20849;&#20139;&#30340;&#36827;&#21270;&#21382;&#21490;&#33267;&#20851;&#37325;&#35201;&#12290;&#26681;&#25454;&#26368;&#22823;&#20284;&#28982;&#31561;&#26576;&#20010;&#26368;&#20248;&#24615;&#20934;&#21017;&#25512;&#26029;&#20986;&#26641;&#20013;&#28508;&#22312;&#33410;&#28857;&#30340;&#20301;&#32622;&#26159;NP-hard&#38382;&#39064;&#65292;&#36825;&#25512;&#21160;&#20102;&#22823;&#37327;&#21551;&#21457;&#24335;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#21551;&#21457;&#24335;&#26041;&#27861;&#36890;&#24120;&#32570;&#20047;&#19968;&#31181;&#31995;&#32479;&#24615;&#30340;&#26041;&#27861;&#26469;&#22343;&#21248;&#37319;&#26679;&#38543;&#26426;&#26641;&#25110;&#26377;&#25928;&#22320;&#25506;&#32034;&#25351;&#25968;&#32423;&#22686;&#38271;&#30340;&#26641;&#31354;&#38388;&#65292;&#36825;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#31561;&#20248;&#21270;&#38382;&#39064;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Phylo2Vec&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#31616;&#26126;&#34920;&#31034;&#26041;&#27861;&#26469;&#34920;&#31034;&#36827;&#21270;&#26641;&#12290;Phylo2Vec&#23558;&#20219;&#20309;&#20855;&#26377;n&#20010;&#21494;&#23376;&#30340;&#20108;&#21449;&#26641;&#26144;&#23556;&#21040;&#38271;&#24230;&#20026;n&#30340;&#25972;&#25968;&#21521;&#37327;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#22312;&#31354;&#38388;&#20013;&#26082;&#26159;&#33391;&#23450;&#30340;&#21448;&#26159;&#21452;&#23556;&#30340;&#12290;Phylo2Vec&#30340;&#20248;&#28857;&#26159;&#65306;i&#65289;&#36731;&#26494;&#22343;&#21248;&#37319;&#26679;&#20108;&#21449;&#26641;&#65307;ii&#65289;&#20197;&#38750;&#24120;&#22823;&#25110;&#23567;&#30340;&#27493;&#38271;&#31995;&#32479;&#22320;&#36941;&#21382;&#26641;&#31354;&#38388;&#12290;&#20316;&#20026;&#27010;&#24565;&#39564;&#35777;&#65292;&#25105;&#20204;&#20351;&#29992;Phylo2Vec&#26500;&#24314;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#20174;&#27688;&#22522;&#37240;&#24207;&#21015;&#39044;&#27979;&#34507;&#30333;&#36136;&#31867;&#21035;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;Phylo2Vec&#26174;&#33879;&#25552;&#39640;&#20102;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20043;&#21069;&#30340;&#26368;&#20248;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Binary phylogenetic trees inferred from biological data are central to understanding the shared evolutionary history of organisms. Inferring the placement of latent nodes in a tree by any optimality criterion (e.g., maximum likelihood) is an NP-hard problem, propelling the development of myriad heuristic approaches. Yet, these heuristics often lack a systematic means of uniformly sampling random trees or effectively exploring a tree space that grows factorially, which are crucial to optimisation problems such as machine learning. Accordingly, we present Phylo2Vec, a new parsimonious representation of a phylogenetic tree. Phylo2Vec maps any binary tree with $n$ leaves to an integer vector of length $n$. We prove that Phylo2Vec is both well-defined and bijective to the space of phylogenetic trees. The advantages of Phylo2Vec are twofold: i) easy uniform sampling of binary trees and ii) systematic ability to traverse tree space in very large or small jumps. As a proof of concept, we use P
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#23481;&#37327;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#37325;&#35201;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22810;&#20041;&#24615;&#29616;&#35937;&#22312;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20998;&#22359;&#21322;&#27491;&#20132;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2210.01892</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#21644;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Polysemanticity and Capacity in Neural Networks. (arXiv:2210.01892v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01892
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#20998;&#26512;&#29305;&#24449;&#23481;&#37327;&#26469;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#22810;&#20041;&#24615;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#19979;&#65292;&#31070;&#32463;&#32593;&#32476;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#37325;&#35201;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65292;&#24182;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#22810;&#20041;&#24615;&#29616;&#35937;&#22312;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#21457;&#29616;&#20102;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#20998;&#22359;&#21322;&#27491;&#20132;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21333;&#20010;&#31070;&#32463;&#20803;&#36890;&#24120;&#20195;&#34920;&#26080;&#20851;&#29305;&#24449;&#30340;&#28151;&#21512;&#12290;&#36825;&#31181;&#29616;&#35937;&#31216;&#20026;&#22810;&#20041;&#24615;&#65292;&#20351;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29702;&#35299;&#20854;&#21407;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#29305;&#24449;&#23481;&#37327;&#30340;&#35270;&#35282;&#26469;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#29305;&#24449;&#23481;&#37327;&#26159;&#27599;&#20010;&#29305;&#24449;&#22312;&#23884;&#20837;&#31354;&#38388;&#20013;&#21344;&#29992;&#30340;&#20998;&#24418;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19968;&#20010;&#29609;&#20855;&#27169;&#22411;&#20013;&#65292;&#26368;&#20248;&#30340;&#23481;&#37327;&#20998;&#37197;&#20542;&#21521;&#20110;&#21333;&#20041;&#22320;&#34920;&#31034;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#65292;&#22810;&#20041;&#22320;&#34920;&#31034;&#27425;&#37325;&#35201;&#29305;&#24449;&#65288;&#19982;&#20854;&#23545;&#25439;&#22833;&#30340;&#24433;&#21709;&#25104;&#27604;&#20363;&#65289;&#65292;&#24182;&#23436;&#20840;&#24573;&#30053;&#26368;&#19981;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;&#24403;&#36755;&#20837;&#20855;&#26377;&#26356;&#39640;&#30340;&#23792;&#24230;&#25110;&#31232;&#30095;&#24615;&#26102;&#65292;&#22810;&#20041;&#24615;&#26356;&#20026;&#26222;&#36941;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#20307;&#31995;&#32467;&#26500;&#20013;&#27604;&#20854;&#20182;&#20307;&#31995;&#32467;&#26500;&#26356;&#20026;&#26222;&#36941;&#12290;&#22312;&#24471;&#21040;&#26368;&#20248;&#23481;&#37327;&#20998;&#37197;&#21518;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20102;&#23884;&#20837;&#31354;&#38388;&#30340;&#20960;&#20309;&#32467;&#26500;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#20998;&#22359;&#21322;&#27491;&#20132;&#30340;&#32467;&#26500;&#65292;&#19981;&#21516;&#27169;&#22411;&#20013;&#30340;&#20998;&#22359;&#22823;&#23567;&#19981;&#21516;&#65292;&#31361;&#20986;&#20102;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individual neurons in neural networks often represent a mixture of unrelated features. This phenomenon, called polysemanticity, can make interpreting neural networks more difficult and so we aim to understand its causes. We propose doing so through the lens of feature \emph{capacity}, which is the fractional dimension each feature consumes in the embedding space. We show that in a toy model the optimal capacity allocation tends to monosemantically represent the most important features, polysemantically represent less important features (in proportion to their impact on the loss), and entirely ignore the least important features. Polysemanticity is more prevalent when the inputs have higher kurtosis or sparsity and more prevalent in some architectures than others. Given an optimal allocation of capacity, we go on to study the geometry of the embedding space. We find a block-semi-orthogonal structure, with differing block sizes in different models, highlighting the impact of model archit
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;</title><link>http://arxiv.org/abs/2209.12835</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#23454;&#29616;&#26377;&#38024;&#23545;&#24615;&#30340;&#20998;&#31163;&#19982;&#25910;&#25947;
&lt;/p&gt;
&lt;p&gt;
Targeted Separation and Convergence with Kernel Discrepancies. (arXiv:2209.12835v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.12835
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26680;&#24046;&#24322;&#24230;&#37327;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#65292;&#23454;&#29616;&#20102;&#23558;&#30446;&#26631;&#20998;&#31163;&#20986;&#26469;&#65292;&#20197;&#21450;&#25511;&#21046;&#23545;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;$\mathbb{R}^d$&#19978;&#20351;&#29992;&#20102;&#36825;&#20123;&#32467;&#26524;&#26469;&#25193;&#23637;&#20102;&#26680;Stein&#24046;&#24322;&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#30446;&#26631;&#30340;&#24369;&#25910;&#25947;&#24615;&#30340;&#26680;&#24046;&#24322;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMDs&#65289;&#22914;&#26680;Stein&#24046;&#24322;&#65288;KSD&#65289;&#24050;&#32463;&#25104;&#20026;&#24191;&#27867;&#24212;&#29992;&#30340;&#20013;&#24515;&#65292;&#21253;&#25324;&#20551;&#35774;&#26816;&#39564;&#12289;&#37319;&#26679;&#22120;&#36873;&#25321;&#12289;&#20998;&#24067;&#36817;&#20284;&#21644;&#21464;&#20998;&#25512;&#26029;&#12290;&#22312;&#27599;&#20010;&#35774;&#32622;&#20013;&#65292;&#36825;&#20123;&#22522;&#20110;&#26680;&#30340;&#24046;&#24322;&#24230;&#37327;&#38656;&#35201;&#23454;&#29616;&#65288;i&#65289;&#23558;&#30446;&#26631;P&#19982;&#20854;&#20182;&#27010;&#29575;&#27979;&#24230;&#20998;&#31163;&#65292;&#29978;&#33267;&#65288;ii&#65289;&#25511;&#21046;&#23545;P&#30340;&#24369;&#25910;&#25947;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#30830;&#20445;&#65288;i&#65289;&#21644;&#65288;ii&#65289;&#30340;&#26032;&#30340;&#20805;&#20998;&#24517;&#35201;&#26465;&#20214;&#12290;&#23545;&#20110;&#21487;&#20998;&#30340;&#24230;&#37327;&#31354;&#38388;&#19978;&#30340;MMDs&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#20998;&#31163;Bochner&#21487;&#23884;&#20837;&#27979;&#24230;&#30340;&#26680;&#65292;&#24182;&#24341;&#20837;&#31616;&#21333;&#30340;&#26465;&#20214;&#26469;&#20998;&#31163;&#25152;&#26377;&#20855;&#26377;&#26080;&#30028;&#26680;&#30340;&#27979;&#24230;&#21644;&#29992;&#26377;&#30028;&#26680;&#26469;&#25511;&#21046;&#25910;&#25947;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#22312;$\mathbb{R}^d$&#19978;&#22823;&#22823;&#25193;&#23637;&#20102;KSD&#20998;&#31163;&#21644;&#25910;&#25947;&#25511;&#21046;&#30340;&#24050;&#30693;&#26465;&#20214;&#65292;&#24182;&#24320;&#21457;&#20102;&#39318;&#20010;&#33021;&#22815;&#31934;&#30830;&#24230;&#37327;&#23545;P&#30340;&#24369;&#25910;&#25947;&#30340;KSDs&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancies (MMDs) like the kernel Stein discrepancy (KSD) have grown central to a wide range of applications, including hypothesis testing, sampler selection, distribution approximation, and variational inference. In each setting, these kernel-based discrepancy measures are required to (i) separate a target P from other probability measures or even (ii) control weak convergence to P. In this article we derive new sufficient and necessary conditions to ensure (i) and (ii). For MMDs on separable metric spaces, we characterize those kernels that separate Bochner embeddable measures and introduce simple conditions for separating all measures with unbounded kernels and for controlling convergence with bounded kernels. We use these results on $\mathbb{R}^d$ to substantially broaden the known conditions for KSD separation and convergence control and to develop the first KSDs known to exactly metrize weak convergence to P. Along the way, we highlight the implications of our res
&lt;/p&gt;</description></item></channel></rss>