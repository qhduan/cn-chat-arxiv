<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title /><link>https://arxiv.org/abs/2402.07127</link><description>&lt;p&gt;
&#35266;&#23519;&#23398;&#20064;&#65306;&#22522;&#20110;&#35270;&#39057;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#23398;&#20064;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Learning by Watching: A Review of Video-based Learning Approaches for Robot Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07127
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#25805;&#20316;&#25216;&#33021;&#21463;&#21040;&#22810;&#26679;&#21270;&#12289;&#26080;&#20559;&#30340;&#25968;&#25454;&#38598;&#30340;&#31232;&#32570;&#24615;&#30340;&#24433;&#21709;&#12290;&#23613;&#31649;&#31574;&#21010;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#24110;&#21161;&#35299;&#20915;&#38382;&#39064;&#65292;&#20294;&#22312;&#27867;&#21270;&#24615;&#21644;&#29616;&#23454;&#19990;&#30028;&#30340;&#36716;&#31227;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#8220;&#37326;&#22806;&#8221;&#35270;&#39057;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#23384;&#22312;&#36890;&#36807;&#33258;&#30417;&#30563;&#25216;&#26415;&#25512;&#21160;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36827;&#23637;&#12290;&#23558;&#36825;&#19968;&#28857;&#24212;&#29992;&#21040;&#26426;&#22120;&#20154;&#39046;&#22495;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#36890;&#36807;&#34987;&#21160;&#35266;&#23519;&#26469;&#23398;&#20064;&#20016;&#23500;&#30340;&#22312;&#32447;&#35270;&#39057;&#20013;&#30340;&#25805;&#20316;&#25216;&#33021;&#12290;&#36825;&#31181;&#22522;&#20110;&#35270;&#39057;&#30340;&#23398;&#20064;&#33539;&#24335;&#26174;&#31034;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#23427;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#26041;&#27861;&#65292;&#21516;&#26102;&#38477;&#20302;&#20102;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#12290;&#26412;&#32508;&#36848;&#22238;&#39038;&#20102;&#35270;&#39057;&#29305;&#24449;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#12289;&#29289;&#20307;&#21487;&#34892;&#24615;&#29702;&#35299;&#12289;&#19977;&#32500;&#25163;&#37096;/&#36523;&#20307;&#24314;&#27169;&#21644;&#22823;&#35268;&#27169;&#26426;&#22120;&#20154;&#36164;&#28304;&#31561;&#22522;&#30784;&#30693;&#35782;&#65292;&#20197;&#21450;&#20174;&#19981;&#21463;&#25511;&#21046;&#30340;&#35270;&#39057;&#28436;&#31034;&#20013;&#33719;&#21462;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#26032;&#20852;&#25216;&#26415;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20165;&#20174;&#35266;&#23519;&#22823;&#35268;&#27169;&#20154;&#31867;&#35270;&#39057;&#20013;&#23398;&#20064;&#22914;&#20309;&#22686;&#24378;&#26426;&#22120;&#20154;&#30340;&#27867;&#21270;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot learning of manipulation skills is hindered by the scarcity of diverse, unbiased datasets. While curated datasets can help, challenges remain in generalizability and real-world transfer. Meanwhile, large-scale "in-the-wild" video datasets have driven progress in computer vision through self-supervised techniques. Translating this to robotics, recent works have explored learning manipulation skills by passively watching abundant videos sourced online. Showing promising results, such video-based learning paradigms provide scalable supervision while reducing dataset bias. This survey reviews foundations such as video feature representation learning techniques, object affordance understanding, 3D hand/body modeling, and large-scale robot resources, as well as emerging techniques for acquiring robot manipulation skills from uncontrolled video demonstrations. We discuss how learning only from observing large-scale human videos can enhance generalization and sample efficiency for roboti
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20107;&#25925;&#21457;&#29983;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;GraphSAGE&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#19978;&#30340;&#20107;&#25925;&#25968;&#37327;&#65292;&#24182;&#21028;&#26029;&#20107;&#25925;&#26159;&#21542;&#20250;&#21457;&#29983;&#12290;</title><link>http://arxiv.org/abs/2311.00164</link><description>&lt;p&gt;
&#36947;&#36335;&#23433;&#20840;&#24314;&#27169;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#29992;&#20110;&#20107;&#25925;&#20998;&#26512;&#30340;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis. (arXiv:2311.00164v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00164
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#39044;&#27979;&#20107;&#25925;&#21457;&#29983;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;GraphSAGE&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#19978;&#30340;&#20107;&#25925;&#25968;&#37327;&#65292;&#24182;&#21028;&#26029;&#20107;&#25925;&#26159;&#21542;&#20250;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22522;&#20110;&#36947;&#36335;&#32593;&#32476;&#36830;&#25509;&#21644;&#20132;&#36890;&#27969;&#37327;&#30340;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20132;&#36890;&#20107;&#25925;&#20998;&#26512;&#38382;&#39064;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#20351;&#29992;&#21382;&#21490;&#35760;&#24405;&#35774;&#35745;&#20102;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#20132;&#36890;&#20107;&#25925;&#30340;&#21457;&#29983;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#32570;&#20047;&#20849;&#35782;&#65292;&#24182;&#19988;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#32570;&#20047;&#20844;&#20849;&#20107;&#25925;&#25968;&#25454;&#38598;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#12289;&#32479;&#19968;&#30340;&#36947;&#36335;&#20132;&#36890;&#20107;&#25925;&#35760;&#24405;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;&#32654;&#22269;&#21508;&#24030;&#23448;&#26041;&#25253;&#21578;&#30340;900&#19975;&#26465;&#35760;&#24405;&#65292;&#20197;&#21450;&#36947;&#36335;&#32593;&#32476;&#21644;&#20132;&#36890;&#27969;&#37327;&#25253;&#21578;&#12290;&#21033;&#29992;&#36825;&#20010;&#26032;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#39044;&#27979;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#20107;&#25925;&#21457;&#29983;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#20687;GraphSAGE&#36825;&#26679;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#36947;&#36335;&#19978;&#30340;&#20107;&#25925;&#25968;&#37327;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#19981;&#36229;&#36807;&#23454;&#38469;&#25968;&#30446;&#30340;22%&#65292;&#24182;&#33021;&#22815;&#21028;&#26029;&#20107;&#25925;&#26159;&#21542;&#20250;&#21457;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not w
&lt;/p&gt;</description></item></channel></rss>