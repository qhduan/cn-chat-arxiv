<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#65288;NoisySAN&#65289;&#20197;&#35299;&#20915;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#25506;&#32034;&#20013;&#30340;&#24369;&#28857;&#65292;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.04162</link><description>&lt;p&gt;
&#29992;&#20110;&#25506;&#32034;&#30340;&#22122;&#22768;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Noisy Spiking Actor Network for Exploration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04162
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#65288;NoisySAN&#65289;&#20197;&#35299;&#20915;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#25506;&#32034;&#20013;&#30340;&#24369;&#28857;&#65292;&#24182;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#32034;&#30340;&#19968;&#31181;&#36890;&#29992;&#26041;&#27861;&#65292;NoisyNet&#33021;&#22815;&#20135;&#29983;&#29305;&#23450;&#20110;&#38382;&#39064;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;&#30001;&#20110;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#20855;&#26377;&#20108;&#36827;&#21046;&#21457;&#25918;&#26426;&#21046;&#65292;&#23545;&#20110;&#22122;&#22768;&#20855;&#26377;&#24456;&#24378;&#30340;&#40065;&#26834;&#24615;&#65292;&#22240;&#27492;&#38590;&#20197;&#36890;&#36807;&#23616;&#37096;&#24178;&#25200;&#23454;&#29616;&#39640;&#25928;&#25506;&#32034;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25506;&#32034;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24341;&#20837;&#26102;&#38388;&#30456;&#20851;&#22122;&#22768;&#30340;&#22122;&#22768;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#65288;NoisySAN&#65289;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22122;&#22768;&#20943;&#23569;&#26041;&#27861;&#26469;&#20026;&#20195;&#29702;&#25214;&#21040;&#31283;&#23450;&#31574;&#30053;&#12290;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26469;&#33258;OpenAI gym&#30340;&#24191;&#27867;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04162v1 Announce Type: new  Abstract: As a general method for exploration in deep reinforcement learning (RL), NoisyNet can produce problem-specific exploration strategies. Spiking neural networks (SNNs), due to their binary firing mechanism, have strong robustness to noise, making it difficult to realize efficient exploration with local disturbances. To solve this exploration problem, we propose a noisy spiking actor network (NoisySAN) that introduces time-correlated noise during charging and transmission. Moreover, a noise reduction method is proposed to find a stable policy for the agent. Extensive experimental results demonstrate that our method outperforms the state-of-the-art performance on a wide range of continuous control tasks from OpenAI gym.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;MAPPOHR&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#12289;&#32463;&#39564;&#35268;&#21017;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35268;&#21010;&#25928;&#29575;&#21644;&#36991;&#30896;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.01270</link><description>&lt;p&gt;
&#32452;&#21512;&#21551;&#21457;&#24335;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Multi-Robot Path Planning Combining Heuristics and Multi-Agent Reinforcement Learning. (arXiv:2306.01270v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;MAPPOHR&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#12289;&#32463;&#39564;&#35268;&#21017;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#35268;&#21010;&#25928;&#29575;&#21644;&#36991;&#30896;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#29615;&#22659;&#19979;&#30340;&#22810;&#26426;&#22120;&#20154;&#36335;&#24452;&#35268;&#21010;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#32463;&#20856;&#38382;&#39064;&#12290;&#22312;&#31227;&#21160;&#36807;&#31243;&#20013;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#36991;&#20813;&#19982;&#20854;&#20182;&#31227;&#21160;&#26426;&#22120;&#20154;&#21457;&#29983;&#30896;&#25758;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23427;&#20204;&#30340;&#34892;&#39542;&#36317;&#31163;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#35201;&#20040;&#20351;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#19981;&#26029;&#37325;&#26032;&#35268;&#21010;&#36335;&#24452;&#20197;&#36991;&#20813;&#20914;&#31361;&#65292;&#35201;&#20040;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#36873;&#25321;&#36866;&#24403;&#30340;&#36991;&#30896;&#31574;&#30053;&#12290;&#21069;&#32773;&#21487;&#33021;&#30001;&#20110;&#39057;&#32321;&#30340;&#37325;&#26032;&#35268;&#21010;&#23548;&#33268;&#34892;&#39542;&#36317;&#31163;&#36739;&#38271;&#65292;&#32780;&#21518;&#32773;&#21487;&#33021;&#30001;&#20110;&#20302;&#26679;&#26412;&#25506;&#32034;&#21644;&#21033;&#29992;&#29575;&#32780;&#23548;&#33268;&#23398;&#20064;&#25928;&#29575;&#20302;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#30340;&#35757;&#32451;&#25104;&#26412;&#36739;&#39640;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;MAPPOHR&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#25628;&#32034;&#12289;&#32463;&#39564;&#35268;&#21017;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26041;&#27861;&#21253;&#21547;&#20004;&#20010;&#23618;&#27425;&#65306;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;MAPPO&#30340;&#23454;&#26102;&#35268;&#21010;&#22120;&#65292;&#20854;&#23558;&#32463;&#39564;&#35268;&#21017;&#23884;&#20837;&#21040;&#21160;&#20316;&#36755;&#20986;&#23618;&#21644;&#22870;&#21169;&#20989;&#25968;&#20013;&#65307;&#20197;&#21450;&#19968;&#20010;&#21551;&#21457;&#24335;&#35268;&#21010;&#22120;&#65292;&#23427;&#29983;&#25104;&#21021;&#22987;&#36335;&#24452;&#24182;&#21521;MAPPO&#35268;&#21010;&#22120;&#28155;&#21152;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35268;&#21010;&#25928;&#29575;&#21644;&#36991;&#30896;&#33021;&#21147;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-robot path finding in dynamic environments is a highly challenging classic problem. In the movement process, robots need to avoid collisions with other moving robots while minimizing their travel distance. Previous methods for this problem either continuously replan paths using heuristic search methods to avoid conflicts or choose appropriate collision avoidance strategies based on learning approaches. The former may result in long travel distances due to frequent replanning, while the latter may have low learning efficiency due to low sample exploration and utilization, and causing high training costs for the model. To address these issues, we propose a path planning method, MAPPOHR, which combines heuristic search, empirical rules, and multi-agent reinforcement learning. The method consists of two layers: a real-time planner based on the multi-agent reinforcement learning algorithm, MAPPO, which embeds empirical rules in the action output layer and reward functions, and a heuri
&lt;/p&gt;</description></item></channel></rss>