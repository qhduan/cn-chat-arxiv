<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#27604;&#35270;&#22270;&#22686;&#24378;&#31574;&#30053;&#12289;&#20301;&#32622;&#24863;&#30693;&#21644;&#35821;&#20041;&#24863;&#30693;&#27491;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#20811;&#26381;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.02810</link><description>&lt;p&gt;
&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Generative-Contrastive Heterogeneous Graph Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#23545;&#27604;&#35270;&#22270;&#22686;&#24378;&#31574;&#30053;&#12289;&#20301;&#32622;&#24863;&#30693;&#21644;&#35821;&#20041;&#24863;&#30693;&#27491;&#26679;&#26412;&#37319;&#26679;&#31574;&#30053;&#20197;&#21450;&#20998;&#23618;&#23545;&#27604;&#23398;&#20064;&#31574;&#30053;&#26469;&#20811;&#26381;&#22270;&#25968;&#25454;&#22686;&#24378;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#34920;&#36798;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#20851;&#31995;&#65292;&#21253;&#25324;&#22810;&#31181;&#31867;&#22411;&#30340;&#33410;&#28857;&#21644;&#36793;&#12290;&#21463;&#33258;&#30417;&#30563;&#23398;&#20064;&#21551;&#21457;&#65292;&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(HGNNs)&#21033;&#29992;&#25968;&#25454;&#22686;&#24378;&#21644;&#36776;&#21035;&#22120;&#23637;&#29616;&#20102;&#24040;&#22823;&#28508;&#21147;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#22270;&#30340;&#31163;&#25955;&#21644;&#25277;&#35937;&#29305;&#24615;&#65292;&#25968;&#25454;&#22686;&#24378;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;\textit{&#29983;&#25104;-&#23545;&#27604;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;(GC-HGNN)}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02810v1 Announce Type: new  Abstract: Heterogeneous Graphs (HGs) can effectively model complex relationships in the real world by multi-type nodes and edges. In recent years, inspired by self-supervised learning, contrastive Heterogeneous Graphs Neural Networks (HGNNs) have shown great potential by utilizing data augmentation and discriminators for downstream tasks. However, data augmentation is still limited due to the discrete and abstract nature of graphs. To tackle the above limitations, we propose a novel \textit{Generative-Contrastive Heterogeneous Graph Neural Network (GC-HGNN)}. Specifically, we first propose a heterogeneous graph generative learning enhanced contrastive paradigm. This paradigm includes: 1) A contrastive view augmentation strategy by using masked autoencoder. 2) Position-aware and semantics-aware positive sample sampling strategy for generate hard negative samples. 3) A hierarchical contrastive learning strategy for capturing local and global informa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.16933</link><description>&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#22823;&#33041;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Backpropagation through space, time, and the brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38656;&#35201;&#26681;&#25454;&#23427;&#20204;&#23545;&#35299;&#20915;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26469;&#35843;&#25972;&#21333;&#20010;&#31361;&#35302;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#36824;&#26159;&#20154;&#24037;&#30340;&#29289;&#29702;&#31070;&#32463;&#31995;&#32479;&#37117;&#21463;&#21040;&#26102;&#31354;&#23616;&#38480;&#12290;&#36825;&#26679;&#30340;&#32593;&#32476;&#22914;&#20309;&#25191;&#34892;&#39640;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38169;&#35823;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#20960;&#20046;&#26222;&#36941;&#34987;&#31354;&#38388;&#65288;BP&#65289;&#21644;&#26102;&#38388;&#65288;BPTT&#65289;&#20004;&#31181;&#26041;&#24335;&#32473;&#20986;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;BP(TT)&#34987;&#24191;&#27867;&#35748;&#20026;&#20381;&#36182;&#20110;&#19981;&#20855;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#20551;&#35774;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#26102;&#31354;&#23616;&#38480;&#24615;&#65292;&#32780;&#27491;&#21521;&#20256;&#25773;&#27169;&#22411;&#65292;&#22914;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#65292;&#21017;&#21463;&#21040;&#20869;&#23384;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#28508;&#22312;&#24179;&#34913;&#65288;GLE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31070;&#32463;&#20803;&#29289;&#29702;&#21160;&#24577;&#32593;&#32476;&#23436;&#20840;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#20174;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16933v1 Announce Type: cross  Abstract: Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by 
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.10266</link><description>&lt;p&gt;
DSP&#65306;&#22810;&#32500;Transformer&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10266
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#24615;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26681;&#25454;&#24403;&#21069;&#35745;&#31639;&#38454;&#27573;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#24615;&#32500;&#24230;&#65292;&#21033;&#29992;&#22810;&#32500;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#29305;&#24615;&#12290;&#36825;&#31181;&#21160;&#24577;&#32500;&#24230;&#20999;&#25442;&#20351;&#24471;&#24207;&#21015;&#24182;&#34892;&#24615;&#22312;&#22810;&#32500;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10266v1 Announce Type: cross  Abstract: Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#36741;&#21161;&#20449;&#24687;&#23545;&#27492;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.07569</link><description>&lt;p&gt;
&#25506;&#31350;&#21333;&#21488;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#30340;&#28145;&#24230;&#23398;&#20064;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Exploring Challenges in Deep Learning of Single-Station Ground Motion Records
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#23398;&#20064;&#30340;&#25928;&#26524;&#65292;&#24182;&#25506;&#35752;&#36741;&#21161;&#20449;&#24687;&#23545;&#27492;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#22320;&#38663;&#23398;&#21644;&#22320;&#38663;&#24037;&#31243;&#30340;&#21508;&#31181;&#24212;&#29992;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#27169;&#22411;&#20027;&#35201;&#20381;&#36182;&#21033;&#29992;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#36827;&#34892;&#22320;&#38663;&#20107;&#20214;&#20998;&#31867;&#12289;&#23450;&#20301;&#12289;&#22320;&#38663;&#26089;&#26399;&#39044;&#35686;&#31995;&#32479;&#21644;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#36825;&#20123;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#20449;&#21495;&#20013;&#26377;&#25928;&#23398;&#20064;&#30340;&#31243;&#24230;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#36741;&#21161;&#20449;&#24687;&#65288;&#22914;&#22320;&#38663;&#30456;&#21040;&#36798;&#26102;&#38388;&#25110;&#32593;&#32476;&#20869;&#22320;&#38663;&#21488;&#31449;&#20998;&#24067;&#65289;&#22312;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20027;&#23548;&#31243;&#24230;&#65292;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#36229;&#21442;&#25968;&#25628;&#32034;&#65292;&#35780;&#20272;&#23427;&#20204;&#22312;&#20174;&#22330;&#22320;&#36816;&#21160;&#35760;&#24405;&#20013;&#36827;&#34892;&#28145;&#24230;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#65292;&#21516;&#26102;&#26816;&#26597;&#36741;&#21161;&#20449;&#24687;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07569v1 Announce Type: cross  Abstract: Contemporary deep learning models have demonstrated promising results across various applications within seismology and earthquake engineering. These models rely primarily on utilizing ground motion records for tasks such as earthquake event classification, localization, earthquake early warning systems, and structural health monitoring. However, the extent to which these models effectively learn from these complex time-series signals has not been thoroughly analyzed. In this study, our objective is to evaluate the degree to which auxiliary information, such as seismic phase arrival times or seismic station distribution within a network, dominates the process of deep learning from ground motion records, potentially hindering its effectiveness. We perform a hyperparameter search on two deep learning models to assess their effectiveness in deep learning from ground motion records while also examining the impact of auxiliary information o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.06869</link><description>&lt;p&gt;
&#22312;&#26377;&#22122;&#22768;&#22522;&#30784;&#27169;&#22411;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning with Noisy Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06869
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30340;&#22122;&#22768;&#24615;&#36136;&#65292;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#35843;&#25972;&#26469;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#24448;&#24448;&#26080;&#27861;&#33719;&#21462;&#25110;&#25104;&#26412;&#36807;&#39640;&#65292;&#21487;&#33021;&#21253;&#21547;&#26631;&#31614;&#22122;&#22768;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#36896;&#25104;&#19981;&#21033;&#24433;&#21709;&#65292;&#24182;&#24102;&#26469;&#24847;&#24819;&#19981;&#21040;&#30340;&#39118;&#38505;&#12290;&#26412;&#25991;&#26159;&#39318;&#20010;&#20840;&#38754;&#20102;&#35299;&#21644;&#20998;&#26512;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#22122;&#22768;&#24615;&#36136;&#65292;&#24182;&#26377;&#25928;&#20943;&#36731;&#20854;&#23545;&#19979;&#28216;&#20219;&#21153;&#24433;&#21709;&#30340;&#24037;&#20316;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#22312;&#21512;&#25104;&#26377;&#22122;&#22768;&#30340;ImageNet-1K&#12289;YFCC15M&#21644;CC12M&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23436;&#20840;&#30417;&#30563;&#21644;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#65292;&#23613;&#31649;&#39044;&#35757;&#32451;&#20013;&#30340;&#36731;&#24494;&#22122;&#22768;&#21487;&#20197;&#20351;&#21516;&#39046;&#22495;&#65288;ID&#65289;&#24615;&#33021;&#21463;&#30410;&#65292;&#21363;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20849;&#20139;&#31867;&#20284;&#20998;&#24067;&#65292;&#20294;&#23427;&#24635;&#26159;&#20250;&#30772;&#22351;&#36328;&#39046;&#22495;&#65288;OOD&#65289;&#24615;&#33021;&#65292;&#22312;&#37027;&#37324;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#26126;&#26174;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06869v1 Announce Type: cross  Abstract: Foundation models are usually pre-trained on large-scale datasets and then adapted to downstream tasks through tuning. However, the large-scale pre-training datasets, often inaccessible or too expensive to handle, can contain label noise that may adversely affect the generalization of the model and pose unexpected risks. This paper stands out as the first work to comprehensively understand and analyze the nature of noise in pre-training datasets and then effectively mitigate its impacts on downstream tasks. Specifically, through extensive experiments of fully-supervised and image-text contrastive pre-training on synthetic noisy ImageNet-1K, YFCC15M, and CC12M datasets, we demonstrate that, while slight noise in pre-training can benefit in-domain (ID) performance, where the training and testing data share a similar distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing distributions are signific
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013; JEPA &#26550;&#26500;&#21551;&#21457;&#30340; Spatio-Temporal Joint Embedding Masked Autoencoder&#65288;ST-JEMA&#65289;&#29992;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.06432</link><description>&lt;p&gt;
&#20154;&#31867;&#22823;&#33041;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#32852;&#21512;&#23884;&#20837;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Joint-Embedding Masked Autoencoder for Self-supervised Learning of Dynamic Functional Connectivity from the Human Brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06432
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013; JEPA &#26550;&#26500;&#21551;&#21457;&#30340; Spatio-Temporal Joint Embedding Masked Autoencoder&#65288;ST-JEMA&#65289;&#29992;&#20110;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06432v1 &#36890;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#23398;&#20064;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#21487;&#20197;&#21306;&#20998;&#20154;&#33041;&#32593;&#32476;&#20013;&#30340;&#34920;&#29616;&#22411;&#12290;&#28982;&#32780;&#65292;&#33719;&#24471;&#29992;&#20110;&#35757;&#32451;&#30340;&#22823;&#37327;&#26631;&#35760;&#20020;&#24202;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#36164;&#28304;&#23494;&#38598;&#24615;&#65292;&#36825;&#20351;&#24471;&#23454;&#38469;&#24212;&#29992;&#21464;&#24471;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#22312;&#26631;&#31614;&#31232;&#32570;&#35774;&#32622;&#20013;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23545;&#20110;&#34920;&#31034;&#23398;&#20064;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29983;&#25104;&#24335;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#21160;&#24577;&#22270;&#24418;&#19978;&#30340;&#24212;&#29992;&#20197;&#21450;&#21160;&#24577;&#21151;&#33021;&#36830;&#25509;&#26041;&#38754;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#65292;&#38754;&#20020;&#30528;&#25429;&#25417;&#39640;&#32423;&#35821;&#20041;&#34920;&#31034;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#26102;&#31354;&#32852;&#21512;&#23884;&#20837;&#25513;&#34109;&#33258;&#32534;&#30721;&#22120;&#65288;ST-JEMA&#65289;&#65292;&#21463;&#21040;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#30340;&#21551;&#21457;&#12290;ST-JEMA&#37319;&#29992;&#20102;&#19968;&#31181;&#21463;JEPA&#21551;&#21457;&#30340;&#31574;&#30053;&#26469;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06432v1 Announce Type: new  Abstract: Graph Neural Networks (GNNs) have shown promise in learning dynamic functional connectivity for distinguishing phenotypes from human brain networks. However, obtaining extensive labeled clinical data for training is often resource-intensive, making practical application difficult. Leveraging unlabeled data thus becomes crucial for representation learning in a label-scarce setting. Although generative self-supervised learning techniques, especially masked autoencoders, have shown promising results in representation learning in various domains, their application to dynamic graphs for dynamic functional connectivity remains underexplored, facing challenges in capturing high-level semantic representations. Here, we introduce the Spatio-Temporal Joint Embedding Masked Autoencoder (ST-JEMA), drawing inspiration from the Joint Embedding Predictive Architecture (JEPA) in computer vision. ST-JEMA employs a JEPA-inspired strategy for reconstructin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2402.16310</link><description>&lt;p&gt;
REPLAY: &#23545;&#31232;&#30095;&#36712;&#36857;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
REPLAY: Modeling Time-Varying Temporal Regularities of Human Mobility for Location Prediction over Sparse Trajectories
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16310
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;REPLAY&#27169;&#22411;&#65292;&#21033;&#29992;&#19968;&#33324;RNN&#26550;&#26500;&#26469;&#23398;&#20064;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#29992;&#20110;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20301;&#32622;&#39044;&#27979;&#26159;&#26681;&#25454;&#21382;&#21490;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#26469;&#39044;&#27979;&#29992;&#25143;&#20301;&#32622;&#30340;&#25216;&#26415;&#12290;&#20026;&#20102;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#29992;&#25143;&#31227;&#21160;&#36712;&#36857;&#30340;&#22266;&#26377;&#31232;&#30095;&#38382;&#39064;&#65292;&#26102;&#31354;&#19978;&#19979;&#25991;&#34987;&#35777;&#26126;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#26159;&#23558;&#20301;&#32622;&#20043;&#38388;&#30340;&#26102;&#31354;&#36317;&#31163;&#32435;&#20837;&#21040;&#31227;&#21160;&#36712;&#36857;&#20013;&#65292;&#35201;&#20040;&#36890;&#36807;&#23558;&#20854;&#20316;&#20026;&#38468;&#21152;&#36755;&#20837;&#25552;&#20379;&#32473;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#65292;&#35201;&#20040;&#36890;&#36807;&#21033;&#29992;&#23427;&#20204;&#26469;&#23547;&#25214;&#26377;&#20449;&#24687;&#30340;&#36807;&#21435;&#38544;&#34255;&#29366;&#24577;&#36827;&#34892;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#26410;&#33021;&#25429;&#25417;&#20154;&#31867;&#31227;&#21160;&#30340;&#26102;&#38388;&#21464;&#21270;&#35268;&#24459;&#65292;&#20363;&#22914;&#65292;&#20154;&#31867;&#31227;&#21160;&#22312;&#26089;&#26216;&#36890;&#24120;&#27604;&#20854;&#20182;&#26102;&#38388;&#26356;&#26377;&#35268;&#24459;&#65307;&#36825;&#26263;&#31034;&#20102;&#23454;&#38469;&#26102;&#38388;&#25139;&#30340;&#26377;&#29992;&#24615;&#12290;&#22522;&#20110;&#36825;&#19968;&#32972;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;REPLAY&#65292;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;RNN&#26550;&#26500;&#65292;&#26088;&#22312;&#25429;&#25417;&#26102;&#38388;&#21464;&#21270;&#30340;&#20154;&#31867;&#31227;&#21160;&#26102;&#38388;&#35268;&#24459;&#20197;&#36827;&#34892;&#20301;&#32622;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16310v1 Announce Type: cross  Abstract: Location prediction forecasts a user's location based on historical user mobility traces. To tackle the intrinsic sparsity issue of real-world user mobility traces, spatiotemporal contexts have been shown as significantly useful. Existing solutions mostly incorporate spatiotemporal distances between locations in mobility traces, either by feeding them as additional inputs to Recurrent Neural Networks (RNNs) or by using them to search for informative past hidden states for prediction. However, such distance-based methods fail to capture the time-varying temporal regularities of human mobility, where human mobility is often more regular in the morning than in other periods, for example; this suggests the usefulness of the actual timestamps besides the temporal distances. Against this background, we propose REPLAY, a general RNN architecture learning to capture the time-varying temporal regularities for location prediction. Specifically, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.15290</link><description>&lt;p&gt;
&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Linear Dynamics-embedded Neural Network for Long-Sequence Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15290
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#30340;&#23646;&#24615;&#21644;&#20248;&#21270;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#25512;&#26029;&#21644;&#39640;&#25928;&#35757;&#32451;&#65292;&#26368;&#32456;&#22312;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#19978;&#21462;&#24471;&#20102;&#26377;&#25928;&#19988;&#39046;&#20808;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#20013;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#20043;&#38388;&#30340;&#26435;&#34913;&#25104;&#20026;&#29942;&#39048;&#65292;&#21463;&#21040;&#25511;&#21046;&#29702;&#35770;&#20013;&#20855;&#26377;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23884;&#20837;&#32447;&#24615;&#21160;&#21147;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;LDNN&#65289;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#12290; SSM&#30340;&#36830;&#32493;&#12289;&#31163;&#25955;&#21644;&#21367;&#31215;&#23646;&#24615;&#20351;LDNN&#20855;&#26377;&#23569;&#37327;&#21442;&#25968;&#12289;&#28789;&#27963;&#30340;&#25512;&#26029;&#21644;&#22312;&#38271;&#24207;&#21015;&#20219;&#21153;&#20013;&#39640;&#25928;&#35757;&#32451;&#30340;&#29305;&#28857;&#12290; &#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#26377;&#25928;&#31574;&#30053;&#65292;&#23545;&#35282;&#21270;&#21644;&#8220;&#35299;&#32806;&#28982;&#21518;&#24555;&#36895;&#20613;&#31435;&#21494;&#21464;&#25442;&#65288;FFT&#65289;&#8221;&#65292;&#20197;&#23558;&#21367;&#31215;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#20174;$O(LNH\max\{L, N\})$&#38477;&#20302;&#21040;$O(LN\max\{H, \log L\})$&#12290; &#25105;&#20204;&#36890;&#36807;&#21452;&#21521;&#38750;&#22240;&#26524;&#21644;&#22810;&#22836;&#35774;&#32622;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;LDNN&#65292;&#20197;&#36866;&#24212;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290; &#23545;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;LDNN&#30340;&#26377;&#25928;&#24615;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15290v1 Announce Type: cross  Abstract: The trade-off between performance and computational efficiency in long-sequence modeling becomes a bottleneck for existing models. Inspired by the continuous state space models (SSMs) with multi-input and multi-output in control theory, we propose a new neural network called Linear Dynamics-embedded Neural Network (LDNN). SSMs' continuous, discrete, and convolutional properties enable LDNN to have few parameters, flexible inference, and efficient training in long-sequence tasks. Two efficient strategies, diagonalization and $'\text{Disentanglement then Fast Fourier Transform (FFT)}'$, are developed to reduce the time complexity of convolution from $O(LNH\max\{L, N\})$ to $O(LN\max \{H, \log L\})$. We further improve LDNN through bidirectional noncausal and multi-head settings to accommodate a broader range of applications. Extensive experiments on the Long Range Arena (LRA) demonstrate the effectiveness and state-of-the-art performance
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;</title><link>https://arxiv.org/abs/2402.14264</link><description>&lt;p&gt;
&#21452;&#31283;&#20581;&#23398;&#20064;&#22312;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#20013;&#30340;&#32467;&#26500;&#19981;&#21487;&#30693;&#24615;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;
Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14264
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#32467;&#26500;&#19981;&#21487;&#30693;&#30340;&#32479;&#35745;&#19979;&#30028;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#22312;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#26041;&#38754;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#26159;&#22240;&#26524;&#25512;&#26029;&#20013;&#26368;&#26680;&#24515;&#30340;&#38382;&#39064;&#65292;&#24212;&#29992;&#24191;&#27867;&#12290;&#34429;&#28982;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#35768;&#22810;&#20272;&#35745;&#31574;&#30053;&#65292;&#26368;&#36817;&#36824;&#32435;&#20837;&#20102;&#36890;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#20272;&#35745;&#22120;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#37319;&#29992;&#26368;&#36817;&#24341;&#20837;&#30340;&#32479;&#35745;&#19979;&#30028;&#32467;&#26500;&#19981;&#21487;&#30693;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23545;&#24178;&#25200;&#20989;&#25968;&#27809;&#26377;&#32467;&#26500;&#24615;&#36136;&#20551;&#35774;&#65292;&#38500;&#20102;&#35775;&#38382;&#40657;&#30418;&#20272;&#35745;&#22120;&#20197;&#36798;&#21040;&#23567;&#35823;&#24046;&#65307;&#24403;&#21482;&#24895;&#24847;&#32771;&#34385;&#20351;&#29992;&#38750;&#21442;&#25968;&#22238;&#24402;&#21644;&#20998;&#31867;&#31070;&#35861;&#20316;&#20026;&#40657;&#30418;&#23376;&#36807;&#31243;&#30340;&#20272;&#35745;&#31574;&#30053;&#26102;&#65292;&#36825;&#19968;&#28857;&#23588;&#20854;&#21560;&#24341;&#20154;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#31283;&#20581;&#20272;&#35745;&#22120;&#23545;&#20110;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#21644;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#30340;&#32479;&#35745;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14264v1 Announce Type: cross  Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, recently also incorporating generic machine learning estimators, the statistical optimality of these methods has still remained an open area of investigation. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that attain small errors; which is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as a black-box sub-process. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Avera
&lt;/p&gt;</description></item><item><title>&#21487;&#23481;&#35768;&#39044;&#27979;&#35268;&#21010;&#65288;CPP&#65289;&#26159;&#19968;&#31181;&#35299;&#20915;&#21463;&#20219;&#24847;&#38543;&#26426;&#21442;&#25968;&#24433;&#21709;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26679;&#26412;&#21644;&#37327;&#23376;&#24341;&#29702;&#23558;&#26426;&#36935;&#21463;&#38480;&#20248;&#21270;&#65288;CCO&#65289;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20855;&#22791;&#36793;&#38469;&#27010;&#29575;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2402.07407</link><description>&lt;p&gt;
&#21487;&#23481;&#35768;&#39044;&#27979;&#35268;&#21010;&#29992;&#20110;&#26426;&#36935;&#21463;&#38480;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Conformal Predictive Programming for Chance Constrained Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07407
&lt;/p&gt;
&lt;p&gt;
&#21487;&#23481;&#35768;&#39044;&#27979;&#35268;&#21010;&#65288;CPP&#65289;&#26159;&#19968;&#31181;&#35299;&#20915;&#21463;&#20219;&#24847;&#38543;&#26426;&#21442;&#25968;&#24433;&#21709;&#30340;&#20248;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#26679;&#26412;&#21644;&#37327;&#23376;&#24341;&#29702;&#23558;&#26426;&#36935;&#21463;&#38480;&#20248;&#21270;&#65288;CCO&#65289;&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20855;&#22791;&#36793;&#38469;&#27010;&#29575;&#21487;&#34892;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#39044;&#27979;&#35268;&#21010;&#65288;CP&#65289;&#30340;&#36827;&#23637;&#30340;&#28608;&#21169;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#23481;&#35768;&#39044;&#27979;&#35268;&#21010;&#65288;CPP&#65289;&#65292;&#19968;&#31181;&#35299;&#20915;&#26426;&#36935;&#21463;&#38480;&#20248;&#21270;&#65288;CCO&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#21363;&#21463;&#20219;&#24847;&#38543;&#26426;&#21442;&#25968;&#24433;&#21709;&#30340;&#38750;&#32447;&#24615;&#32422;&#26463;&#20989;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;CPP&#21033;&#29992;&#36825;&#20123;&#38543;&#26426;&#21442;&#25968;&#30340;&#26679;&#26412;&#20197;&#21450;&#37327;&#23376;&#24341;&#29702;&#65288;CP&#30340;&#26680;&#24515;&#65289;&#23558;CCO&#38382;&#39064;&#36716;&#21270;&#20026;&#30830;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#65306;&#65288;1&#65289;&#23558;&#37327;&#23376;&#34920;&#31034;&#20026;&#32447;&#24615;&#35268;&#21010;&#20197;&#21450;&#20854;KKT&#26465;&#20214;&#65288;CPP-KKT&#65289;&#65307;&#65288;2&#65289;&#20351;&#29992;&#28151;&#21512;&#25972;&#25968;&#35268;&#21010;&#65288;CPP-MIP&#65289;&#26469;&#21576;&#29616;CPP&#30340;&#20004;&#31181;&#26131;&#20110;&#22788;&#29702;&#30340;&#25913;&#36827;&#12290;CPP&#20855;&#22791;&#23545;CCO&#38382;&#39064;&#36827;&#34892;&#36793;&#38469;&#27010;&#29575;&#21487;&#34892;&#24615;&#20445;&#35777;&#65292;&#36825;&#19982;&#29616;&#26377;&#26041;&#27861;&#65288;&#20363;&#22914;&#26679;&#26412;&#36924;&#36817;&#21644;&#22330;&#26223;&#26041;&#27861;&#65289;&#22312;&#27010;&#24565;&#19978;&#26377;&#25152;&#19981;&#21516;&#12290;&#23613;&#31649;&#25105;&#20204;&#25506;&#35752;&#20102;&#19982;&#26679;&#26412;&#36924;&#36817;&#26041;&#27861;&#30340;&#31639;&#27861;&#30456;&#20284;&#20043;&#22788;&#65292;&#20294;&#25105;&#20204;&#24378;&#35843;CPP&#30340;&#20248;&#21183;&#22312;&#20110;&#26131;&#20110;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Motivated by the advances in conformal prediction (CP), we propose conformal predictive programming (CPP), an approach to solve chance constrained optimization (CCO) problems, i.e., optimization problems with nonlinear constraint functions affected by arbitrary random parameters. CPP utilizes samples from these random parameters along with the quantile lemma -- which is central to CP -- to transform the CCO problem into a deterministic optimization problem. We then present two tractable reformulations of CPP by: (1) writing the quantile as a linear program along with its KKT conditions (CPP-KKT), and (2) using mixed integer programming (CPP-MIP). CPP comes with marginal probabilistic feasibility guarantees for the CCO problem that are conceptually different from existing approaches, e.g., the sample approximation and the scenario approach. While we explore algorithmic similarities with the sample approximation approach, we emphasize that the strength of CPP is that it can easily be ext
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCEO&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;&#30456;&#20114;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2402.07002</link><description>&lt;p&gt;
&#23458;&#25143;&#31471;&#21327;&#20316;&#65306;&#20855;&#26377;&#20445;&#35777;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#25913;&#36827;&#30340;&#28789;&#27963;&#24046;&#20998;&#38544;&#31169;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clients Collaborate: Flexible Differentially Private Federated Learning with Guaranteed Improvement of Utility-Privacy Trade-off
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07002
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCEO&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#20445;&#25252;&#29992;&#25143;&#38544;&#31169;&#30340;&#21516;&#26102;&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;&#30456;&#20114;&#21327;&#20316;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#25928;&#29992;&#21644;&#38544;&#31169;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#22312;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#38450;&#27490;&#29992;&#25143;&#25968;&#25454;&#30340;&#38544;&#31169;&#27844;&#28431;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#24046;&#20998;&#38544;&#31169;&#65292;&#20294;&#23427;&#24182;&#19981;&#26159;&#20813;&#36153;&#30340;&#12290;&#22122;&#22768;&#30340;&#28155;&#21152;&#20250;&#38543;&#26426;&#24178;&#25200;&#27169;&#22411;&#30340;&#35821;&#20041;&#23436;&#25972;&#24615;&#65292;&#24182;&#19988;&#36825;&#31181;&#24178;&#25200;&#20250;&#38543;&#30528;&#36890;&#20449;&#36718;&#27425;&#30340;&#22686;&#21152;&#32780;&#32047;&#31215;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20855;&#26377;&#20005;&#26684;&#38544;&#31169;&#20445;&#35777;&#30340;&#26032;&#22411;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;FedCEO&#65292;&#36890;&#36807;&#35753;&#23458;&#25143;&#31471;"&#30456;&#20114;&#21327;&#20316;"&#65292;&#26088;&#22312;&#22312;&#27169;&#22411;&#25928;&#29992;&#21644;&#29992;&#25143;&#38544;&#31169;&#20043;&#38388;&#25214;&#21040;&#19968;&#31181;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;&#26381;&#21153;&#22120;&#19978;&#23545;&#22534;&#21472;&#30340;&#26412;&#22320;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#20102;&#39640;&#25928;&#30340;&#24352;&#37327;&#20302;&#31209;&#36817;&#31471;&#20248;&#21270;&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#20809;&#35889;&#31354;&#38388;&#20013;&#28789;&#27963;&#25130;&#26029;&#39640;&#39057;&#32452;&#20998;&#30340;&#33021;&#21147;&#12290;&#36825;&#24847;&#21619;&#30528;&#25105;&#20204;&#30340;FedCEO&#33021;&#22815;&#36890;&#36807;&#24179;&#28369;&#20840;&#23616;&#35821;&#20041;&#31354;&#38388;&#26469;&#26377;&#25928;&#24674;&#22797;&#34987;&#25171;&#26029;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#38544;&#31169;&#35774;&#32622;&#21644;&#25345;&#32493;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;SOTA&#30340;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#36793;&#30028;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
To defend against privacy leakage of user data, differential privacy is widely used in federated learning, but it is not free. The addition of noise randomly disrupts the semantic integrity of the model and this disturbance accumulates with increased communication rounds. In this paper, we introduce a novel federated learning framework with rigorous privacy guarantees, named FedCEO, designed to strike a trade-off between model utility and user privacy by letting clients ''Collaborate with Each Other''. Specifically, we perform efficient tensor low-rank proximal optimization on stacked local model parameters at the server, demonstrating its capability to flexibly truncate high-frequency components in spectral space. This implies that our FedCEO can effectively recover the disrupted semantic information by smoothing the global semantic space for different privacy settings and continuous training processes. Moreover, we improve the SOTA utility-privacy trade-off bound by an order of $\sqr
&lt;/p&gt;</description></item><item><title>&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;</title><link>https://arxiv.org/abs/2311.02181</link><description>&lt;p&gt;
&#23398;&#20064;&#22810;&#20010;&#21160;&#24577;&#31995;&#32479;&#20013;&#30340;&#32852;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Joint Problems in Learning Multiple Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02181
&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26102;&#38388;&#24207;&#21015;&#30340;&#26032;&#38382;&#39064;&#65292;&#25552;&#20986;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#26368;&#23567;&#21270;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#30340;&#32858;&#31867;&#26159;&#19968;&#20010;&#32463;&#36807;&#20805;&#20998;&#30740;&#31350;&#30340;&#38382;&#39064;&#65292;&#20854;&#24212;&#29992;&#33539;&#22260;&#20174;&#36890;&#36807;&#20195;&#35874;&#20135;&#29289;&#27987;&#24230;&#33719;&#24471;&#30340;&#23450;&#37327;&#20010;&#24615;&#21270;&#20195;&#35874;&#27169;&#22411;&#21040;&#37327;&#23376;&#20449;&#24687;&#29702;&#35770;&#20013;&#30340;&#29366;&#24577;&#21028;&#21035;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21464;&#31181;&#65292;&#21363;&#32473;&#23450;&#19968;&#32452;&#36712;&#36857;&#21644;&#19968;&#20123;&#37096;&#20998;&#65292;&#25105;&#20204;&#32852;&#21512;&#21010;&#20998;&#36712;&#36857;&#38598;&#24182;&#23398;&#20064;&#27599;&#20010;&#37096;&#20998;&#30340;&#32447;&#24615;&#21160;&#24577;&#31995;&#32479;&#65288;LDS&#65289;&#27169;&#22411;&#65292;&#20197;&#20351;&#24471;&#25152;&#26377;&#27169;&#22411;&#30340;&#26368;&#22823;&#35823;&#24046;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20840;&#23616;&#25910;&#25947;&#30340;&#26041;&#27861;&#21644;EM&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#24182;&#38468;&#19978;&#20102;&#26377;&#21069;&#26223;&#30340;&#35745;&#31639;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02181v2 Announce Type: replace-cross  Abstract: Clustering of time series is a well-studied problem, with applications ranging from quantitative, personalized models of metabolism obtained from metabolite concentrations to state discrimination in quantum information theory. We consider a variant, where given a set of trajectories and a number of parts, we jointly partition the set of trajectories and learn linear dynamical system (LDS) models for each part, so as to minimize the maximum error across all the models. We present globally convergent methods and EM heuristics, accompanied by promising computational results.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06417</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#23398;&#20064;&#20013;&#30340;&#25299;&#25169;&#27010;&#25324;&#30340;&#27969;&#21160;&#25193;&#25955;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Advective Diffusion Transformers for Topological Generalization in Graph Learning. (arXiv:2310.06417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#25955;&#26041;&#31243;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#19988;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#20998;&#26512;GNN&#21160;&#21147;&#23398;&#12289;&#24418;&#24335;&#21270;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#35777;&#26126;&#26550;&#26500;&#36873;&#25321;&#30340;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#12290;&#22270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;GNN&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;&#24403;&#21069;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#20551;&#35774;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#22270;&#25299;&#25169;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#22270;&#25193;&#25955;&#26041;&#31243;&#22312;&#19981;&#21516;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#30340;&#22806;&#25512;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#36808;&#20986;&#20102;&#35299;&#26512;GNN&#27010;&#25324;&#24615;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#19978;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#36825;&#26159;&#30001;&#20110;&#23545;&#25299;&#25169;&#21464;&#21270;&#30340;&#25351;&#25968;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#12290;&#38543;&#21518;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#65292;&#23427;&#20513;&#23548;&#23545;&#23436;&#20840;&#36830;&#25509;&#30340;&#28508;&#22312;&#22270;&#36827;&#34892;&#29305;&#24449;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent gr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#24615;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;TAB&#27169;&#22411;&#65292;&#36890;&#36807;&#34913;&#37327;&#30446;&#26631;&#19982;&#28304;&#22238;&#24402;&#20989;&#25968;&#20043;&#38388;&#30340;&#27169;&#31946;&#24230;&#27700;&#24179;&#26469;&#25913;&#21892;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36991;&#20813;&#36127;&#36801;&#31227;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;TAB&#27169;&#22411;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#21644;&#36923;&#36753;&#22238;&#24402;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.04606</link><description>&lt;p&gt;
&#20855;&#26377;&#19981;&#21487;&#38752;&#28304;&#25968;&#25454;&#30340;&#40065;&#26834;&#24615;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Transfer Learning with Unreliable Source Data. (arXiv:2310.04606v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04606
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#24615;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;TAB&#27169;&#22411;&#65292;&#36890;&#36807;&#34913;&#37327;&#30446;&#26631;&#19982;&#28304;&#22238;&#24402;&#20989;&#25968;&#20043;&#38388;&#30340;&#27169;&#31946;&#24230;&#27700;&#24179;&#26469;&#25913;&#21892;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#36991;&#20813;&#36127;&#36801;&#31227;&#12290;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;TAB&#27169;&#22411;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#21644;&#36923;&#36753;&#22238;&#24402;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#40065;&#26834;&#24615;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#36125;&#21494;&#26031;&#20998;&#31867;&#22120;&#30340;&#27169;&#31946;&#24615;&#21644;&#30446;&#26631;&#19982;&#28304;&#20998;&#24067;&#20043;&#38388;&#30340;&#24369;&#21487;&#36716;&#31227;&#20449;&#21495;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#65292;&#31216;&#20026;&#8220;&#27169;&#31946;&#24230;&#27700;&#24179;&#8221;&#65292;&#29992;&#20110;&#34913;&#37327;&#30446;&#26631;&#19982;&#28304;&#22238;&#24402;&#20989;&#25968;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#19968;&#33324;&#23450;&#29702;&#65292;&#35828;&#26126;&#20102;&#36825;&#20010;&#26032;&#37327;&#19982;&#23398;&#20064;&#30340;&#36801;&#31227;&#24615;&#22312;&#39118;&#38505;&#25913;&#21892;&#26041;&#38754;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#36793;&#30028;&#21608;&#22260;&#36716;&#31227;&#8221;(Transfer Around Boundary, TAB)&#27169;&#22411;&#36890;&#36807;&#22312;&#30446;&#26631;&#25968;&#25454;&#21644;&#28304;&#25968;&#25454;&#24615;&#33021;&#20043;&#38388;&#36827;&#34892;&#24179;&#34913;&#30340;&#38408;&#20540;&#65292;&#26082;&#39640;&#25928;&#21448;&#40065;&#26834;&#65292;&#33021;&#22815;&#25913;&#21892;&#20998;&#31867;&#24182;&#36991;&#20813;&#36127;&#36801;&#31227;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;TAB&#27169;&#22411;&#22312;&#38750;&#21442;&#25968;&#20998;&#31867;&#21644;&#36923;&#36753;&#22238;&#24402;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#36798;&#21040;&#20102;&#26368;&#20248;&#30340;&#19978;&#30028;&#65292;&#21482;&#26377;&#23545;&#25968;&#22240;&#23376;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#20223;&#30495;&#30740;&#31350;&#36827;&#19968;&#27493;&#25903;&#25345;&#20102;TAB&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses challenges in robust transfer learning stemming from ambiguity in Bayes classifiers and weak transferable signals between the target and source distribution. We introduce a novel quantity called the ''ambiguity level'' that measures the discrepancy between the target and source regression functions, propose a simple transfer learning procedure, and establish a general theorem that shows how this new quantity is related to the transferability of learning in terms of risk improvements. Our proposed ''Transfer Around Boundary'' (TAB) model, with a threshold balancing the performance of target and source data, is shown to be both efficient and robust, improving classification while avoiding negative transfer. Moreover, we demonstrate the effectiveness of the TAB model on non-parametric classification and logistic regression tasks, achieving upper bounds which are optimal up to logarithmic factors. Simulation studies lend further support to the effectiveness of TAB. We 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#20869;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23398;&#20064;&#22120;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#26159;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02075</link><description>&lt;p&gt;
&#20351;&#29992;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Quantum Processes with Quantum Statistical Queries. (arXiv:2310.02075v1 [quant-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#27169;&#22411;&#20869;&#23398;&#20064;&#37327;&#23376;&#36807;&#31243;&#30340;&#26694;&#26550;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#23398;&#20064;&#22120;&#21644;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#25581;&#31034;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#30340;&#33030;&#24369;&#24615;&#65292;&#36825;&#26159;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#19968;&#20010;&#37325;&#35201;&#30340;&#24320;&#25918;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22797;&#26434;&#30340;&#37327;&#23376;&#36807;&#31243;&#26159;&#37327;&#23376;&#35745;&#31639;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30340;&#35768;&#22810;&#39046;&#22495;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#65292;&#24212;&#29992;&#20110;&#37327;&#23376;&#22522;&#20934;&#27979;&#35797;&#12289;&#23494;&#30721;&#20998;&#26512;&#21644;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#37327;&#23376;&#32479;&#35745;&#26597;&#35810;&#65288;QSQ&#65289;&#27169;&#22411;&#20869;&#30740;&#31350;&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23545;&#37327;&#23376;&#36807;&#31243;&#65288;QPSQs&#65289;&#36827;&#34892;&#32479;&#35745;&#26597;&#35810;&#30340;&#31532;&#19968;&#20010;&#27491;&#24335;&#23450;&#20041;&#12290;&#35813;&#26694;&#26550;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19968;&#31181;&#39640;&#25928;&#30340;QPSQ&#23398;&#20064;&#22120;&#65292;&#36866;&#29992;&#20110;&#20219;&#24847;&#37327;&#23376;&#36807;&#31243;&#65292;&#24182;&#38468;&#24102;&#21487;&#35777;&#26126;&#30340;&#24615;&#33021;&#20445;&#35777;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25968;&#20540;&#27169;&#25311;&#26469;&#23637;&#31034;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#36890;&#36807;&#22312;&#23494;&#30721;&#20998;&#26512;&#20013;&#24212;&#29992;&#35813;&#26694;&#26550;&#65292;&#31361;&#20986;&#20102;&#32463;&#20856;&#35835;&#20986;&#37327;&#23376;&#29289;&#29702;&#19981;&#21487;&#20811;&#38534;&#20989;&#25968;&#65288;CR-QPUFs&#65289;&#30340;&#33030;&#24369;&#24615;&#65292;&#35299;&#20915;&#20102;&#37327;&#23376;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#12290;&#36825;&#39033;&#24037;&#20316;&#26159;&#26397;&#30528;&#28145;&#20837;&#29702;&#35299;&#37327;&#23376;&#36807;&#31243;&#23398;&#20064;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning complex quantum processes is a central challenge in many areas of quantum computing and quantum machine learning, with applications in quantum benchmarking, cryptanalysis, and variational quantum algorithms. This paper introduces the first learning framework for studying quantum process learning within the Quantum Statistical Query (QSQ) model, providing the first formal definition of statistical queries to quantum processes (QPSQs). The framework allows us to propose an efficient QPSQ learner for arbitrary quantum processes accompanied by a provable performance guarantee. We also provide numerical simulations to demonstrate the efficacy of this algorithm. The practical relevance of this framework is exemplified through application in cryptanalysis, highlighting vulnerabilities of Classical-Readout Quantum Physical Unclonable Functions (CR-QPUFs), addressing an important open question in the field of quantum hardware security. This work marks a significant step towards underst
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#20197;&#21450;BERT&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2309.15670</link><description>&lt;p&gt;
MONOVAB: &#29992;&#20110;&#23391;&#21152;&#25289;&#35821;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
MONOVAB : An Annotated Corpus for Bangla Multi-label Emotion Detection. (arXiv:2309.15670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#26500;&#24314;&#20102;&#19968;&#20010;&#22522;&#20110;&#23391;&#21152;&#25289;&#35821;&#30340;&#27880;&#37322;&#35821;&#26009;&#24211;&#65292;&#29992;&#20110;&#22810;&#26631;&#31614;&#24773;&#24863;&#26816;&#27979;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#20197;&#21450;BERT&#27169;&#22411;&#65292;&#22635;&#34917;&#20102;&#36825;&#19968;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24773;&#24863;&#20998;&#26512;(SA)&#21644;&#24773;&#24863;&#35782;&#21035;(ER)&#22312;&#23391;&#21152;&#25289;&#35821;&#20013;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#23391;&#21152;&#25289;&#35821;&#26159;&#19990;&#30028;&#19978;&#31532;&#19971;&#22823;&#20351;&#29992;&#20154;&#25968;&#26368;&#22810;&#30340;&#35821;&#35328;&#12290;&#28982;&#32780;&#65292;&#23391;&#21152;&#25289;&#35821;&#30340;&#32467;&#26500;&#22797;&#26434;&#65292;&#36825;&#20351;&#24471;&#20934;&#30830;&#25552;&#21462;&#24773;&#32490;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#19968;&#20123;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#25552;&#21462;&#31215;&#26497;&#21644;&#28040;&#26497;&#24773;&#24863;&#20197;&#21450;&#22810;&#31867;&#24773;&#32490;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#35821;&#35328;&#20013;&#25552;&#21462;&#22810;&#31181;&#24773;&#32490;&#20960;&#20046;&#26159;&#26410;&#24320;&#21457;&#30340;&#39046;&#22495;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#19968;&#27573;&#25991;&#26412;&#35782;&#21035;&#20986;&#22810;&#31181;&#24773;&#24863;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#20174;Facebook&#19978;&#25235;&#21462;&#30340;&#25968;&#25454;&#26500;&#24314;&#27880;&#37322;&#35821;&#26009;&#24211;&#30340;&#35814;&#32454;&#26041;&#27861;&#65292;&#20197;&#22635;&#34917;&#36825;&#20010;&#23398;&#31185;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#20811;&#26381;&#25361;&#25112;&#12290;&#20026;&#20102;&#20351;&#36825;&#31181;&#27880;&#37322;&#26356;&#26377;&#25104;&#26524;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#26041;&#27861;&#12290;&#36716;&#25442;&#22120;&#20013;&#30340;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;(BERT)&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Sentiment Analysis (SA) and Emotion Recognition (ER) have been increasingly popular in the Bangla language, which is the seventh most spoken language throughout the entire world. However, the language is structurally complicated, which makes this field arduous to extract emotions in an accurate manner. Several distinct approaches such as the extraction of positive and negative sentiments as well as multiclass emotions, have been implemented in this field of study. Nevertheless, the extraction of multiple sentiments is an almost untouched area in this language. Which involves identifying several feelings based on a single piece of text. Therefore, this study demonstrates a thorough method for constructing an annotated corpus based on scrapped data from Facebook to bridge the gaps in this subject area to overcome the challenges. To make this annotation more fruitful, the context-based approach has been used. Bidirectional Encoder Representations from Transformers (BERT),
&lt;/p&gt;</description></item><item><title>&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2306.15546</link><description>&lt;p&gt;
&#24403;&#22522;&#30784;&#27169;&#22411;&#36935;&#21040;&#32852;&#37030;&#23398;&#20064;&#65306;&#21160;&#26426;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions. (arXiv:2306.15546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15546
&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#19982;&#32852;&#37030;&#23398;&#20064;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#25193;&#23637;&#20102;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;&#27169;&#22411;&#21457;&#23637;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65288;FM&#65289;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#20132;&#21449;&#25552;&#20379;&#20102;&#30456;&#20114;&#30340;&#22909;&#22788;&#65292;&#22312;AI&#30740;&#31350;&#20013;&#25552;&#20379;&#20102;&#35299;&#38145;&#26032;&#21487;&#33021;&#24615;&#30340;&#29420;&#29305;&#26426;&#20250;&#65292;&#35299;&#20915;&#20102;AI&#21644;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;FL&#25193;&#23637;&#20102;FM&#30340;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#35745;&#31639;&#20849;&#20139;&#65292;&#20998;&#25955;&#20102;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#20943;&#36731;&#20102;FL&#21442;&#19982;&#32773;&#30340;&#36127;&#25285;&#12290;&#23427;&#20419;&#36827;&#20102;&#21327;&#20316;&#24335;FM&#21457;&#23637;&#65292;&#27665;&#20027;&#21270;&#20102;&#36825;&#19968;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#21253;&#23481;&#24615;&#21644;&#21019;&#26032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;FM&#20197;&#20854;&#24222;&#22823;&#30340;&#35268;&#27169;&#12289;&#39044;&#35757;&#32451;&#30340;&#30693;&#35782;&#21644;&#20986;&#33394;&#30340;&#24615;&#33021;&#65292;&#20026;FL&#25552;&#20379;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#36215;&#28857;&#65292;&#20419;&#36827;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#26356;&#24555;&#30340;&#25910;&#25947;&#21644;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21033;&#29992;FM&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21487;&#20197;&#20016;&#23500;&#25968;&#25454;&#22810;&#26679;&#24615;&#65292;&#20943;&#23569;&#36807;&#25311;&#21512;&#65292;&#20445;&#25252;&#38544;&#31169;&#12290;&#36890;&#36807;&#30740;&#31350;FL&#21644;FM&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#26412;&#25991;&#26088;&#22312;&#21152;&#28145;&#23545;&#23427;&#20204;&#21327;&#21516;&#20851;&#31995;&#30340;&#29702;&#35299;&#65292;&#24378;&#35843;&#21160;&#26426;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intersection of the Foundation Model (FM) and Federated Learning (FL) provides mutual benefits, presents a unique opportunity to unlock new possibilities in AI research, and address critical challenges in AI and real-world applications. FL expands the availability of data for FMs and enables computation sharing, distributing the training process and reducing the burden on FL participants. It promotes collaborative FM development, democratizing the process and fostering inclusivity and innovation. On the other hand, FM, with its enormous size, pre-trained knowledge, and exceptional performance, serves as a robust starting point for FL, facilitating faster convergence and better performance under non-iid data. Additionally, leveraging FM to generate synthetic data enriches data diversity, reduces overfitting, and preserves privacy. By examining the interplay between FL and FM, this paper aims to deepen the understanding of their synergistic relationship, highlighting the motivations,
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#21644;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#20013;&#21463;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#24212;&#29992;&#65292;&#25506;&#35752;&#23558;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#33267;&#27979;&#35797;&#24037;&#20855;&#20013;&#20197;&#25552;&#39640;&#25968;&#23383;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#24377;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.07546</link><description>&lt;p&gt;
&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#21450;&#20854;&#22312;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#20013;&#30340;&#24212;&#29992;&#65306;&#31995;&#32479;&#32508;&#36848;&#65288;arXiv:2303.07546v1 [cs.SE]&#65289;
&lt;/p&gt;
&lt;p&gt;
Constrained Adversarial Learning and its applicability to Automated Software Testing: a systematic review. (arXiv:2303.07546v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#21644;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#20013;&#21463;&#38480;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#24212;&#29992;&#65292;&#25506;&#35752;&#23558;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#33267;&#27979;&#35797;&#24037;&#20855;&#20013;&#20197;&#25552;&#39640;&#25968;&#23383;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#21644;&#24377;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27599;&#31181;&#26032;&#25216;&#26415;&#37117;&#20250;&#22686;&#21152;&#38544;&#21547;&#30340;&#28431;&#27934;&#65292;&#35753;&#36234;&#26469;&#36234;&#22810;&#30340;&#32593;&#32476;&#25915;&#20987;&#32773;&#21033;&#29992;&#12290;&#33258;&#21160;&#21270;&#36719;&#20214;&#27979;&#35797;&#21487;&#20197;&#25104;&#20026;&#24555;&#36895;&#20998;&#26512;&#25968;&#21315;&#34892;&#20195;&#30721;&#30340;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#30053;&#24494;&#20462;&#25913;&#21151;&#33021;&#29305;&#23450;&#30340;&#27979;&#35797;&#25968;&#25454;&#26469;&#36935;&#21040;&#22810;&#20010;&#28431;&#27934;&#21644;&#25915;&#20987;&#21521;&#37327;&#12290;&#36825;&#20010;&#36807;&#31243;&#19982;&#21463;&#38480;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#29983;&#25104;&#30340;&#21463;&#38480;&#24615;&#23545;&#25239;&#24615;&#31034;&#20363;&#30456;&#20284;&#65292;&#22240;&#27492;&#23558;&#36825;&#20123;&#26041;&#27861;&#25972;&#21512;&#21040;&#33258;&#21160;&#21270;&#27979;&#35797;&#24037;&#20855;&#20013;&#21487;&#33021;&#20250;&#26377;&#26174;&#30528;&#30340;&#22909;&#22788;&#12290;&#22240;&#27492;&#65292;&#26412;&#31995;&#32479;&#32508;&#36848;&#20391;&#37325;&#20110;&#38480;&#21046;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#22312;&#23545;&#25239;&#23398;&#20064;&#21644;&#36719;&#20214;&#27979;&#35797;&#20013;&#30340;&#24212;&#29992;&#30340;&#24403;&#21069;&#26368;&#26032;&#25216;&#26415;&#65292;&#26088;&#22312;&#25351;&#23548;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#20351;&#29992;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#22686;&#24378;&#27979;&#35797;&#24037;&#20855;&#65292;&#25552;&#39640;&#25968;&#23383;&#31995;&#32479;&#30340;&#24377;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#23545;&#20110;&#23545;&#25239;&#26426;&#22120;&#23398;&#20064;&#30340;&#21457;&#29616;&#21463;&#38480;&#21046;&#30340;&#25968;&#25454;&#29983;&#25104;&#24212;&#29992;&#26159;&#31995;&#32479;&#21270;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every novel technology adds hidden vulnerabilities ready to be exploited by a growing number of cyber-attacks. Automated software testing can be a promising solution to quickly analyze thousands of lines of code by generating and slightly modifying function-specific testing data to encounter a multitude of vulnerabilities and attack vectors. This process draws similarities to the constrained adversarial examples generated by adversarial learning methods, so there could be significant benefits to the integration of these methods in automated testing tools. Therefore, this systematic review is focused on the current state-of-the-art of constrained data generation methods applied for adversarial learning and software testing, aiming to guide researchers and developers to enhance testing tools with adversarial learning methods and improve the resilience and robustness of their digital systems. The found constrained data generation applications for adversarial machine learning were systemat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2302.06375</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#26102;&#38388;&#24207;&#21015;&#30340;Transformer&#65306;&#34920;&#31034;&#21644;&#35757;&#32451;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
One Transformer for All Time Series: Representing and Training with Time-Dependent Heterogeneous Tabular Data. (arXiv:2302.06375v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#65292;&#29992;&#20110;&#34920;&#31034;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#30340;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#65292;&#36890;&#36807;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#26469;&#34920;&#31034;&#25968;&#20540;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23558;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20197;&#22797;&#21046;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22312;&#36825;&#19968;&#32467;&#26500;&#21270;&#39046;&#22495;&#30340;&#25104;&#21151;&#12290;&#29305;&#21035;&#26377;&#36259;&#30340;&#26159;&#65292;&#34920;&#26684;&#25968;&#25454;&#20855;&#26377;&#26102;&#38388;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#37329;&#34701;&#20132;&#26131;&#12290;&#28982;&#32780;&#65292;&#34920;&#26684;&#20540;&#30340;&#24322;&#36136;&#24615;&#65292;&#20854;&#20013;&#31867;&#21035;&#20803;&#32032;&#19982;&#25968;&#20540;&#39033;&#28151;&#21512;&#65292;&#20351;&#24471;&#36825;&#31181;&#36866;&#24212;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Transformer&#26550;&#26500;&#26469;&#34920;&#31034;&#24322;&#26500;&#30340;&#26102;&#38388;&#30456;&#20851;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#25968;&#20540;&#29305;&#24449;&#20351;&#29992;&#19968;&#32452;&#39057;&#29575;&#20989;&#25968;&#34920;&#31034;&#65292;&#24182;&#19988;&#25972;&#20010;&#32593;&#32476;&#20351;&#29992;&#21807;&#19968;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#32479;&#19968;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a recent growing interest in applying Deep Learning techniques to tabular data, in order to replicate the success of other Artificial Intelligence areas in this structured domain. Specifically interesting is the case in which tabular data have a time dependence, such as, for instance financial transactions. However, the heterogeneity of the tabular values, in which categorical elements are mixed with numerical items, makes this adaptation difficult. In this paper we propose a Transformer architecture to represent heterogeneous time-dependent tabular data, in which numerical features are represented using a set of frequency functions and the whole network is uniformly trained with a unique loss function.
&lt;/p&gt;</description></item></channel></rss>