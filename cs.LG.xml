<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;</title><link>https://rss.arxiv.org/abs/2402.00976</link><description>&lt;p&gt;
&#20855;&#26377;&#21160;&#24577;&#20572;&#27490;&#30340;&#24490;&#29615;Transformer
&lt;/p&gt;
&lt;p&gt;
Recurrent Transformers with Dynamic Halt
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.00976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#30340;&#20004;&#31181;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25193;&#23637;&#21644;&#32452;&#21512;&#26041;&#27861;&#12290;&#22312;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#20013;&#36827;&#34892;&#27604;&#36739;&#65292;&#25506;&#32034;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#20027;&#35201;&#26041;&#27861;&#22312;&#22686;&#24378;Transformer&#19982;&#24490;&#29615;&#26426;&#21046;&#26041;&#38754;&#30340;&#24402;&#32435;&#20559;&#22909;&#8212;&#8212;&#65288;1&#65289;&#31867;&#20284;&#20110;Universal Transformers&#30340;&#28145;&#24230;&#36880;&#23618;&#24490;&#29615;&#26041;&#27861;&#65307;&#21644;&#65288;2&#65289;&#31867;&#20284;&#20110;Temporal Latent Bottleneck&#30340;&#20998;&#22359;&#26102;&#24577;&#24490;&#29615;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#30740;&#31350;&#20102;&#25193;&#23637;&#21644;&#32452;&#21512;&#19978;&#36848;&#26041;&#27861;&#30340;&#26032;&#26041;&#24335;&#65292;&#20363;&#22914;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20840;&#23616;&#22343;&#20540;&#30340;Universal Transformer&#21160;&#24577;&#20572;&#27490;&#26426;&#21046;&#65292;&#24182;&#23558;Universal Transformer&#30340;&#20803;&#32032;&#34701;&#20837;&#21040;Temporal Latent Bottleneck&#20013;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#20010;&#35786;&#26029;&#20219;&#21153;&#65288;&#22914;Long Range Arena&#65288;LRA&#65289;&#65292;&#32763;&#36716;-&#32763;&#36716;&#35821;&#35328;&#24314;&#27169;&#65292;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#27604;&#36739;&#20102;&#27169;&#22411;&#24182;&#25506;&#32034;&#20102;&#23427;&#20204;&#30340;&#24402;&#32435;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the inductive biases of two major approaches to augmenting Transformers with a recurrent mechanism - (1) the approach of incorporating a depth-wise recurrence similar to Universal Transformers; and (2) the approach of incorporating a chunk-wise temporal recurrence like Temporal Latent Bottleneck. Furthermore, we propose and investigate novel ways to extend and combine the above methods - for example, we propose a global mean-based dynamic halting mechanism for Universal Transformer and an augmentation of Temporal Latent Bottleneck with elements from Universal Transformer. We compare the models and probe their inductive biases in several diagnostic tasks such as Long Range Arena (LRA), flip-flop language modeling, ListOps, and Logical Inference.
&lt;/p&gt;</description></item><item><title>&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.00204</link><description>&lt;p&gt;
&#22522;&#20110;PPO&#30340;DRL&#33258;&#35843;PID&#38750;&#32447;&#24615;&#26080;&#20154;&#26426;&#25511;&#21046;&#22120;&#29992;&#20110;&#31283;&#20581;&#33258;&#20027;&#39134;&#34892;
&lt;/p&gt;
&lt;p&gt;
A PPO-based DRL Auto-Tuning Nonlinear PID Drone Controller for Robust Autonomous Flights
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#24341;&#20837;&#26080;&#20154;&#26426;&#25511;&#21046;&#20013;&#65292;&#21462;&#20195;&#20256;&#32479;&#32447;&#24615;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#20102;&#26080;&#32541;&#36807;&#28193;&#12289;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#32467;&#21512;PPO&#31574;&#30053;&#35757;&#32451;DRL&#20195;&#29702;&#65292;&#24182;&#21033;&#29992;&#39640;&#31934;&#24230;&#36319;&#36394;&#31995;&#32479;&#25552;&#39640;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#23558;&#38750;&#32447;&#24615;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#20195;&#29702;&#20316;&#20026;&#20256;&#32479;&#32447;&#24615;&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#30340;&#26367;&#20195;&#21697;&#65292;&#20174;&#32780;&#24443;&#24213;&#25913;&#21464;&#26080;&#20154;&#26426;&#39134;&#34892;&#25511;&#21046;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#25163;&#21160;&#21644;&#33258;&#20027;&#27169;&#24335;&#20043;&#38388;&#23454;&#29616;&#26080;&#32541;&#36807;&#28193;&#65292;&#25552;&#39640;&#21709;&#24212;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#22312;Gazebo&#27169;&#25311;&#22120;&#20013;&#21033;&#29992;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;PPO&#65289;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#26469;&#35757;&#32451;DRL&#20195;&#29702;&#12290;&#28155;&#21152;20000&#32654;&#20803;&#30340;&#23460;&#20869;Vicon&#36319;&#36394;&#31995;&#32479;&#25552;&#20379;&lt;1mm&#30340;&#23450;&#20301;&#31934;&#24230;&#65292;&#26174;&#30528;&#25552;&#39640;&#20102;&#33258;&#20027;&#39134;&#34892;&#31934;&#24230;&#12290;&#20026;&#20102;&#22312;&#26368;&#30701;&#30340;&#26080;&#30896;&#25758;&#36712;&#36857;&#20013;&#23548;&#33322;&#26080;&#20154;&#26426;&#65292;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#19977;&#32500;A*&#36335;&#24452;&#35268;&#21010;&#22120;&#24182;&#25104;&#21151;&#22320;&#23558;&#20854;&#23454;&#26045;&#21040;&#23454;&#38469;&#39134;&#34892;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00204v1 Announce Type: cross  Abstract: This project aims to revolutionize drone flight control by implementing a nonlinear Deep Reinforcement Learning (DRL) agent as a replacement for traditional linear Proportional Integral Derivative (PID) controllers. The primary objective is to seamlessly transition drones between manual and autonomous modes, enhancing responsiveness and stability. We utilize the Proximal Policy Optimization (PPO) reinforcement learning strategy within the Gazebo simulator to train the DRL agent. Adding a $20,000 indoor Vicon tracking system offers &lt;1mm positioning accuracy, which significantly improves autonomous flight precision. To navigate the drone in the shortest collision-free trajectory, we also build a 3 dimensional A* path planner and implement it into the real flight successfully.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#25913;&#36827;Hadamard&#21464;&#25442;&#21644;&#35774;&#35745;&#22359;MHT&#23618;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.18846</link><description>&lt;p&gt;
&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
The Blind Normalized Stein Variational Gradient Descent-Based Detection for Intelligent Massive Random Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18846
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;&#30340;&#26816;&#27979;&#22120;&#65292;&#29992;&#20110;&#35299;&#20915;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;&#20013;&#30340;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#24320;&#21457;&#25913;&#36827;Hadamard&#21464;&#25442;&#21644;&#35774;&#35745;&#22359;MHT&#23618;&#26469;&#25552;&#39640;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#20047;&#39640;&#25928;&#30340;&#21069;&#23548;&#26816;&#27979;&#31639;&#27861;&#20173;&#28982;&#26159;&#35299;&#20915;&#23454;&#38469;&#36890;&#20449;&#22330;&#26223;&#20013;&#26234;&#33021;&#22823;&#35268;&#27169;&#38543;&#26426;&#25509;&#20837;(RA)&#20013;&#21069;&#23548;&#30896;&#25758;&#38382;&#39064;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;(MLE)&#27169;&#22411;&#30340;&#26089;&#26399;&#21069;&#23548;&#26816;&#27979;&#26041;&#26696;&#65292;&#22312;&#25480;&#20104;&#24335;RA&#27969;&#31243;&#30340;&#31532;&#19968;&#27493;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30450;&#24402;&#19968;&#21270;&#26031;&#22374;&#21464;&#20998;&#26799;&#24230;&#19979;&#38477;(SVGD)&#30340;&#26816;&#27979;&#22120;&#65292;&#20197;&#33719;&#24471;MLE&#27169;&#22411;&#30340;&#36817;&#20284;&#35299;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#25506;&#32034;Hadamard&#21464;&#25442;&#21644;&#23567;&#27874;&#21464;&#25442;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#25913;&#36827;Hadamard&#21464;&#25442;(MHT)&#65292;&#20351;&#29992;&#20108;&#38454;&#23548;&#25968;&#28388;&#27874;&#22120;&#23558;&#39640;&#39057;&#20998;&#31163;&#20986;&#37325;&#35201;&#37096;&#20998;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#28040;&#38500;SVGD&#26816;&#27979;&#22120;&#20013;&#30340;&#22122;&#22768;&#24182;&#20943;&#36731;&#26799;&#24230;&#28040;&#22833;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#22522;&#20110;MHT&#30340;&#22359;MHT&#23618;&#65292;&#35813;&#23618;&#22522;&#20110;MHT&#12289;&#32553;&#25918;&#23618;&#12289;&#36719;&#38408;&#20540;&#23618;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18846v1 Announce Type: cross  Abstract: The lack of an efficient preamble detection algorithm remains a challenge for solving preamble collision problems in intelligent massive random access (RA) in practical communication scenarios. To solve this problem, we present a novel early preamble detection scheme based on a maximum likelihood estimation (MLE) model at the first step of the grant-based RA procedure. A novel blind normalized Stein variational gradient descent (SVGD)-based detector is proposed to obtain an approximate solution to the MLE model. First, by exploring the relationship between the Hadamard transform and wavelet transform, a new modified Hadamard transform (MHT) is developed to separate high-frequencies from important components using the second-order derivative filter. Next, to eliminate noise and mitigate the vanishing gradients problem in the SVGD-based detectors, the block MHT layer is designed based on the MHT, scaling layer, soft-thresholding layer, i
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#30340;&#22810;&#23610;&#24230;&#32441;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26356;&#22909;&#22320;&#25429;&#25417;&#22797;&#26434;&#30340;&#22270;&#20687;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.16640</link><description>&lt;p&gt;
&#20351;&#29992;GAN&#36827;&#34892;CT&#21435;&#22122;&#30340;&#22810;&#23610;&#24230;&#32441;&#29702;&#25439;&#22833;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Texture Loss for CT denoising with GANs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16640
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#30340;&#22810;&#23610;&#24230;&#32441;&#29702;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#24110;&#21161;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26356;&#22909;&#22320;&#25429;&#25417;&#22797;&#26434;&#30340;&#22270;&#20687;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#21435;&#22122;&#24212;&#29992;&#20013;&#26159;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;GAN&#30340;&#21435;&#22122;&#31639;&#27861;&#20173;&#28982;&#23384;&#22312;&#25429;&#25417;&#22270;&#20687;&#20869;&#22797;&#26434;&#20851;&#31995;&#30340;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25484;&#25569;&#39640;&#24230;&#22797;&#26434;&#21644;&#38750;&#32447;&#24615;&#30340;&#32441;&#29702;&#20851;&#31995;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28784;&#24230;&#20849;&#29983;&#30697;&#38453;&#65288;GLCM&#65289;&#22266;&#26377;&#30340;&#22810;&#23610;&#24230;&#24615;&#36136;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#22312;&#20998;&#31867;&#21644;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#25105;&#20204;&#20551;&#35774;&#23558;&#20854;&#20449;&#24687;&#20869;&#23481;&#25972;&#21512;&#21040;GANs&#30340;&#35757;&#32451;&#20013;&#20250;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#22522;&#20110;&#26799;&#24230;&#20248;&#21270;&#30340;GLCM&#30340;&#21487;&#24494;&#20998;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16640v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have proved as a powerful framework for denoising applications in medical imaging. However, GAN-based denoising algorithms still suffer from limitations in capturing complex relationships within the images. In this regard, the loss function plays a crucial role in guiding the image generation process, encompassing how much a synthetic image differs from a real image. To grasp highly complex and non-linear textural relationships in the training process, this work presents a loss function that leverages the intrinsic multi-scale nature of the Gray-Level-Co-occurrence Matrix (GLCM). Although the recent advances in deep learning have demonstrated superior performance in classification and detection tasks, we hypothesize that its information content can be valuable when integrated into GANs' training. To this end, we propose a differentiable implementation of the GLCM suited for gradient-based optimiza
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24212;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;LSTM&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#21463;&#34920;&#38754;&#24352;&#21147;&#24433;&#21709;&#30340;&#27969;&#20307;&#27969;&#21160;&#20013;&#30340;&#33021;&#37327;&#39044;&#31639;&#12290;</title><link>https://arxiv.org/abs/2403.16144</link><description>&lt;p&gt;
&#39044;&#27979;&#28082;&#28404;&#21160;&#21147;&#23398;&#20013;&#30340;&#33021;&#37327;&#39044;&#31639;&#65306;&#19968;&#31181;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Predicting Energy Budgets in Droplet Dynamics: A Recurrent Neural Network Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16144
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24212;&#29992;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;LSTM&#27169;&#22411;&#65292;&#26469;&#39044;&#27979;&#21463;&#34920;&#38754;&#24352;&#21147;&#24433;&#21709;&#30340;&#27969;&#20307;&#27969;&#21160;&#20013;&#30340;&#33021;&#37327;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16144v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#27969;&#20307;&#21147;&#23398;&#20013;&#30340;&#31070;&#32463;&#32593;&#32476;&#20026;&#25506;&#32034;&#22797;&#26434;&#27969;&#21160;&#65288;&#21253;&#25324;&#22810;&#30456;&#21644;&#33258;&#30001;&#34920;&#38754;&#27969;&#21160;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#26041;&#27861;&#12290; &#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#23545;&#23398;&#20064;&#20174;&#30636;&#24577;&#36755;&#20837;&#21040;&#21160;&#24577;&#36755;&#20986;&#30340;&#26144;&#23556;&#20855;&#26377;&#21560;&#24341;&#21147;&#12290; &#26412;&#30740;&#31350;&#24212;&#29992;LSTM&#26469;&#39044;&#27979;&#21463;&#34920;&#38754;&#24352;&#21147;&#24433;&#21709;&#30340;&#27969;&#20307;&#27969;&#21160;&#30340;&#30636;&#24577;&#21644;&#38745;&#24577;&#36755;&#20986;&#12290; &#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#28082;&#28404;&#21160;&#21147;&#23398;&#22330;&#26223;&#65306;&#20855;&#26377;&#19981;&#21516;&#21021;&#22987;&#24418;&#29366;&#30340;&#28082;&#28404;&#19982;&#22266;&#20307;&#34920;&#38754;&#30896;&#25758;&#65292;&#20197;&#21450;&#20004;&#20010;&#28082;&#28404;&#30896;&#25758;&#21518;&#20957;&#32858;&#12290; &#20165;&#20351;&#29992;&#26080;&#37327;&#32434;&#25968;&#21644;&#26469;&#33258;&#25968;&#20540;&#27169;&#25311;&#30340;&#20960;&#20309;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;LSTM&#21487;&#20197;&#39044;&#27979;&#33021;&#37327;&#39044;&#31639;&#12290; &#37319;&#29992;&#26631;&#35760;&#26684;&#28857;&#27861;&#21069;&#21521;&#36319;&#36394;&#26041;&#27861;&#32467;&#21512;&#26631;&#35760;&#26684;&#28857;&#27861;&#26377;&#38480;&#24046;&#20998;&#31574;&#30053;&#26469;&#27169;&#25311;&#28082;&#28404;&#21160;&#21147;&#23398;&#12290; &#20351;&#29992;&#39304;&#36865;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16144v1 Announce Type: cross  Abstract: Neural networks in fluid mechanics offer an efficient approach for exploring complex flows, including multiphase and free surface flows. The recurrent neural network, particularly the Long Short-Term Memory (LSTM) model, proves attractive for learning mappings from transient inputs to dynamic outputs. This study applies LSTM to predict transient and static outputs for fluid flows under surface tension effects. Specifically, we explore two distinct droplet dynamic scenarios: droplets with diverse initial shapes impacting with solid surfaces, as well as the coalescence of two droplets following collision. Using only dimensionless numbers and geometric time series data from numerical simulations, LSTM predicts the energy budget. The marker-and-cell front-tracking methodology combined with a marker-and-cell finite-difference strategy is adopted for simulating the droplet dynamics. Using a recurrent neural network (RNN) architecture fed wit
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#30830;&#23450;&#24615;&#38382;&#39064;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#22411;&#29305;&#24449;&#20540;&#35745;&#31639;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#12290;</title><link>https://arxiv.org/abs/2403.12206</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#25311;&#21512;&#30340;&#23454;&#29992;&#32039;&#20945;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Useful Compact Representations for Data-Fitting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12206
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#32039;&#20945;&#34920;&#31034;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#30830;&#23450;&#24615;&#38382;&#39064;&#30340;&#36719;&#20214;&#23454;&#29616;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#22823;&#22411;&#29305;&#24449;&#20540;&#35745;&#31639;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;2&#38454;&#23548;&#25968;&#20449;&#24687;&#30340;&#26368;&#23567;&#21270;&#38382;&#39064;&#20013;&#65292;&#20272;&#35745;Hessian&#30697;&#38453;&#30340;&#26041;&#27861;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#25216;&#26415;&#29983;&#25104;&#30340;&#31264;&#23494;&#30697;&#38453;&#23545;&#20110;&#22823;&#22411;&#38382;&#39064;&#26159;&#38590;&#20197;&#25215;&#21463;&#30340;&#12290;&#26377;&#38480;&#20869;&#23384;&#32039;&#20945;&#34920;&#31034;&#23558;&#31264;&#23494;&#25968;&#32452;&#34920;&#31034;&#20026;&#20302;&#31209;&#34920;&#31034;&#65292;&#24050;&#25104;&#20026;&#22823;&#22411;&#30830;&#23450;&#24615;&#38382;&#39064;&#36719;&#20214;&#23454;&#29616;&#30340;&#26368;&#26032;&#25216;&#26415;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#30001;&#21521;&#37327;&#36873;&#25321;&#21442;&#25968;&#21270;&#30340;&#26032;&#32039;&#20945;&#34920;&#31034;&#65292;&#24182;&#19988;&#23545;&#20110;&#29305;&#23450;&#36873;&#25321;&#65292;&#23427;&#20204;&#21487;&#20197;&#31616;&#21270;&#20026;&#29616;&#26377;&#30340;&#33879;&#21517;&#20844;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#32039;&#20945;&#34920;&#31034;&#22312;&#22823;&#22411;&#29305;&#24449;&#20540;&#35745;&#31639;&#12289;&#24352;&#37327;&#20998;&#35299;&#21644;&#38750;&#32447;&#24615;&#22238;&#24402;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12206v1 Announce Type: cross  Abstract: For minimization problems without 2nd derivative information, methods that estimate Hessian matrices can be very effective. However, conventional techniques generate dense matrices that are prohibitive for large problems. Limited-memory compact representations express the dense arrays in terms of a low rank representation and have become the state-of-the-art for software implementations on large deterministic problems. We develop new compact representations that are parameterized by a choice of vectors and that reduce to existing well known formulas for special choices. We demonstrate effectiveness of the compact representations for large eigenvalue computations, tensor factorizations and nonlinear regressions.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26426;&#21046;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#22312;&#38750;iid&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.11464</link><description>&lt;p&gt;
FedSPU&#65306;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#30340;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedSPU: Personalized Federated Learning for Resource-constrained Devices with Stochastic Parameter Update
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11464
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26426;&#21046;&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#22312;&#38750;iid&#25968;&#25454;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;PFL&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#37327;&#30340;&#38750;iid&#23458;&#25143;&#31471;&#25968;&#25454;&#65292;&#21516;&#26102;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#23458;&#25143;&#25317;&#26377;&#30340;&#24322;&#26500;&#36793;&#32536;&#35774;&#22791;&#21487;&#33021;&#26045;&#21152;&#19981;&#21516;&#31243;&#24230;&#30340;&#36164;&#28304;&#32422;&#26463;&#65292;&#32473;PFL&#36896;&#25104;&#35745;&#31639;&#21644;&#36890;&#20449;&#29942;&#39048;&#12290;&#32852;&#37030;Dropout&#24050;&#25104;&#20026;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#19968;&#31181;&#27969;&#34892;&#31574;&#30053;&#65292;&#20854;&#20013;&#20165;&#22312;&#23458;&#25143;&#31471;&#35774;&#22791;&#19978;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#30340;&#19968;&#20010;&#23376;&#27169;&#22411;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;Dropout&#30340;&#27169;&#22411;&#20462;&#21098;&#31574;&#30053;&#21487;&#33021;&#24341;&#20837;&#20559;&#24046;&#65292;&#29305;&#21035;&#26159;&#23545;&#38750;iid&#26412;&#22320;&#25968;&#25454;&#12290;&#24403;&#26377;&#20559;&#35265;&#30340;&#23376;&#27169;&#22411;&#21560;&#25910;&#26469;&#33258;&#20854;&#20182;&#23458;&#25143;&#31471;&#30340;&#39640;&#24230;&#20998;&#25955;&#21442;&#25968;&#26102;&#65292;&#24615;&#33021;&#19979;&#38477;&#26159;&#19981;&#21487;&#36991;&#20813;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#26377;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FedSPU&#65289;&#12290;&#19982;&#19987;&#38376;&#20026;&#23567;&#22411;&#26412;&#22320;&#23376;&#27169;&#22411;&#23450;&#21046;&#20840;&#23616;&#27169;&#22411;&#30340;Dropout&#19981;&#21516;&#65292;FedSPU&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#30340;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26426;&#21046;&#65292;&#20197;&#22312;&#20445;&#25345;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#21644;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11464v1 Announce Type: new  Abstract: Personalized Federated Learning (PFL) is widely employed in IoT applications to handle high-volume, non-iid client data while ensuring data privacy. However, heterogeneous edge devices owned by clients may impose varying degrees of resource constraints, causing computation and communication bottlenecks for PFL. Federated Dropout has emerged as a popular strategy to address this challenge, wherein only a subset of the global model, i.e. a \textit{sub-model}, is trained on a client's device, thereby reducing computation and communication overheads. Nevertheless, the dropout-based model-pruning strategy may introduce bias, particularly towards non-iid local data. When biased sub-models absorb highly divergent parameters from other clients, performance degradation becomes inevitable. In response, we propose federated learning with stochastic parameter update (FedSPU). Unlike dropout that tailors the global model to small-size local sub-model
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20013;&#30340;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.06402</link><description>&lt;p&gt;
&#19968;&#20992;&#20999;&#19981;&#36866;&#29992;&#65306;&#23398;&#20064;&#22312;&#25991;&#26412;&#20998;&#31867;&#20013;&#20351;&#29992;&#22810;&#23569;&#20363;&#20026;&#20102;&#25913;&#36827;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
'One size doesn't fit all': Learning how many Examples to use for In-Context Learning for Improved Text Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06402
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#33258;&#36866;&#24212;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#65292;&#36890;&#36807;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#26469;&#25552;&#39640;&#25991;&#26412;&#20998;&#31867;&#30340;&#24615;&#33021;&#65292;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20013;&#30340;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06402v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#26032; Abstract: &#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#39044;&#27979;&#27169;&#22411;&#24050;&#32463;&#20174;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#21040;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#31181;&#24494;&#35843;&#30340;&#26497;&#31471;&#24418;&#24335;&#28041;&#21450;&#21040;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#20854;&#20013;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#30340;&#36755;&#20986;&#65288;&#20923;&#32467;&#30340;&#35299;&#30721;&#22120;&#21442;&#25968;&#65289;&#21482;&#21463;&#21040;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#21464;&#21270;&#65288;&#31216;&#20026;&#25351;&#20196;&#25110;&#25552;&#31034;&#65289;&#30340;&#25511;&#21046;&#12290;ICL&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#22312;&#25552;&#31034;&#20013;&#20351;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#23454;&#20363;&#20316;&#20026;&#31034;&#20363;&#12290;&#23613;&#31649;&#29616;&#26377;&#24037;&#20316;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20026;&#27599;&#20010;&#25968;&#25454;&#23454;&#20363;&#20351;&#29992;&#38745;&#24577;&#25968;&#37327;&#30340;&#31034;&#20363;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#35843;&#25972;&#31034;&#20363;&#25968;&#37327;&#30340;&#26032;&#26041;&#27861;&#12290;&#36825;&#31867;&#20284;&#20110;k&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20013;&#20351;&#29992;&#21487;&#21464;&#22823;&#23567;&#37051;&#22495;&#30340;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;ICL&#65288;AICL&#65289;&#30340;&#24037;&#20316;&#27969;&#31243;&#20013;&#65292;&#23545;&#20110;&#29305;&#23450;&#25968;&#25454;&#23454;&#20363;&#36827;&#34892;&#25512;&#29702;&#26102;&#20351;&#29992;&#30340;&#28436;&#31034;&#25968;&#37327;&#26159;&#21160;&#24577;&#35843;&#25972;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06402v1 Announce Type: new  Abstract: Predictive models in natural language processing (NLP) have evolved from training models from scratch to fine-tuning pre-trained models with labelled data. An extreme form of this fine-tuning involves in-context learning (ICL), where the output of a pre-trained generative model (frozen decoder parameters) is controlled only with variations in the input strings (called instructions or prompts). An important component of ICL is the use of a small number of labelled data instances as examples in the prompt. While existing work uses a static number of examples during inference for each data instance, in this paper we propose a novel methodology of dynamically adapting the number of examples as per the data. This is analogous to the use of a variable-sized neighborhood in k-nearest neighbors (k-NN) classifier. In our proposed workflow of adaptive ICL (AICL), the number of demonstrations to employ during the inference on a particular data inst
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#20445;&#30041;&#20102;&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#36991;&#20813;&#20102;&#32047;&#31215;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06328</link><description>&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#23454;&#29616;&#21487;&#36801;&#31227;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferable Reinforcement Learning via Generalized Occupancy Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#31867;&#21035;&#65292;&#20445;&#30041;&#20102;&#27169;&#22411;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#36890;&#29992;&#24615;&#65292;&#24182;&#36991;&#20813;&#20102;&#32047;&#31215;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#20195;&#29702;&#24517;&#39035;&#26159;&#36890;&#29992;&#30340; - &#20855;&#26377;&#24555;&#36895;&#36866;&#24212;&#21644;&#27010;&#25324;&#21040;&#19981;&#21516;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26694;&#26550;&#20869;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#31639;&#27861;&#23398;&#20064;&#19990;&#30028;&#30340;&#20219;&#21153;&#19981;&#21487;&#30693;&#21160;&#24577;&#27169;&#22411;&#65292;&#21407;&#21017;&#19978;&#20351;&#23427;&#20204;&#33021;&#22815;&#27010;&#25324;&#21040;&#20219;&#24847;&#22870;&#21169;&#12290;&#28982;&#32780;&#65292;&#19968;&#27493;&#27169;&#22411;&#33258;&#28982;&#20250;&#21463;&#21040;&#32047;&#31215;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#20351;&#23427;&#20204;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#36328;&#24230;&#21644;&#22823;&#29366;&#24577;&#31354;&#38388;&#30340;&#38382;&#39064;&#19978;&#22833;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31867;&#26032;&#22411;&#27169;&#22411; - &#24191;&#20041;&#21344;&#26377;&#27169;&#22411;&#65288;GOMs&#65289;&#65292;&#20445;&#30041;&#20102;&#22522;&#20110;&#27169;&#22411;&#30340;RL&#30340;&#36890;&#29992;&#24615;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32047;&#31215;&#24615;&#38169;&#35823;&#12290;GOMs&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#22312;&#19968;&#20010;&#22266;&#23450;&#25968;&#25454;&#38598;&#30340;&#35206;&#30422;&#19979;&#65292;&#24314;&#27169;&#32473;&#23450;&#29366;&#24577;&#30340;&#25152;&#26377;&#21487;&#33021;&#38271;&#26399;&#32467;&#26524;&#30340;&#20998;&#24067;&#65292;&#20197;&#21450;&#23454;&#29616;&#32473;&#23450;&#29366;&#24577;&#30340;&#29305;&#23450;&#32467;&#26524;&#30340;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#36805;&#36895;&#29992;&#20110;&#20026;&#20219;&#24847;&#26032;&#20219;&#21153;&#36873;&#25321;&#26368;&#20248;&#25805;&#20316;&#65292;&#32780;&#26080;&#38656;&#25285;&#24515;&#32047;&#31215;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06328v1 Announce Type: new  Abstract: Intelligent agents must be generalists - showing the ability to quickly adapt and generalize to varying tasks. Within the framework of reinforcement learning (RL), model-based RL algorithms learn a task-agnostic dynamics model of the world, in principle allowing them to generalize to arbitrary rewards. However, one-step models naturally suffer from compounding errors, making them ineffective for problems with long horizons and large state spaces. In this work, we propose a novel class of models - generalized occupancy models (GOMs) - that retain the generality of model-based RL while avoiding compounding error. The key idea behind GOMs is to model the distribution of all possible long-term outcomes from a given state under the coverage of a stationary dataset, along with a policy that realizes a particular outcome from the given state. These models can then quickly be used to select the optimal action for arbitrary new tasks, without hav
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02302</link><description>&lt;p&gt;
&#36229;&#36234;&#19987;&#19994;&#21270;&#65306;&#35780;&#20272;MLLMs&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21464;&#24471;&#24322;&#24120;&#27969;&#34892;&#12290;&#20687;ChatGPT-4V&#21644;Gemini&#36825;&#26679;&#21151;&#33021;&#24378;&#22823;&#30340;&#21830;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#20687;LLaVA&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#26412;&#36136;&#19978;&#37117;&#26159;&#36890;&#29992;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22914;&#27492;&#24378;&#22823;&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22788;&#29702;&#29978;&#33267;&#26410;&#32463;&#19987;&#38376;&#35757;&#32451;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#30340;MLLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;ShareGPT4V&#12289;ChatGPT&#12289;LLaVA-Next &#36827;&#34892;&#20102;&#19987;&#38376;&#20219;&#21153;&#30340;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#65292;&#19982;&#25105;&#20204;&#30340;&#26368;&#26032;&#19987;&#19994;&#21270;&#27169;&#22411;MiVOLO&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;MiVOLO&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;&#35814;&#32454;&#20449;&#24687;&#21644;&#26032;&#30340;&#25351;&#26631;&#12290;&#36825;&#31181;&#27604;&#36739;&#20135;&#29983;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#32467;&#26524;&#21644;&#20851;&#20110;&#21442;&#19982;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
&lt;/p&gt;</description></item><item><title>Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16562</link><description>&lt;p&gt;
Q-FOX&#23398;&#20064;&#65306;&#39072;&#35206;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Q-FOX Learning: Breaking Tradition in Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16562
&lt;/p&gt;
&lt;p&gt;
Q-FOX&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;FOX&#20248;&#21270;&#22120;&#21644;Q-learning&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20132;&#20114;&#26469;&#23398;&#20064;&#26368;&#20339;&#21160;&#20316;&#65292;&#22240;&#27492;&#36866;&#29992;&#20110;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#25110;&#30452;&#25509;&#30417;&#30563;&#30340;&#20219;&#21153;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Q-FOX&#30340;&#26032;&#39062;&#33258;&#21160;&#35843;&#21442;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;FOX&#20248;&#21270;&#22120;&#21644;&#24120;&#29992;&#30340;&#26131;&#20110;&#23454;&#29616;&#30340;RL Q-learning&#31639;&#27861;&#35299;&#20915;&#20102;&#35843;&#21442;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#23558;&#22870;&#21169;&#25918;&#22312;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;&#23398;&#20064;&#26102;&#38388;&#20043;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16562v2 Announce Type: replace-cross  Abstract: Reinforcement learning (RL) is a subset of artificial intelligence (AI) where agents learn the best action by interacting with the environment, making it suitable for tasks that do not require labeled data or direct supervision. Hyperparameters (HP) tuning refers to choosing the best parameter that leads to optimal solutions in RL algorithms. Manual or random tuning of the HP may be a crucial process because variations in this parameter lead to changes in the overall learning aspects and different rewards. In this paper, a novel and automatic HP-tuning method called Q-FOX is proposed. This uses both the FOX optimizer, a new optimization method inspired by nature that mimics red foxes' hunting behavior, and the commonly used, easy-to-implement RL Q-learning algorithm to solve the problem of HP tuning. Moreover, a new objective function is proposed which prioritizes the reward over the mean squared error (MSE) and learning time (
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32771;&#34385;&#36716;&#31227;&#23433;&#20840;&#30340;&#20840;&#23616;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#23433;&#20840;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#28304;&#32452;&#20214;&#26469;&#20943;&#23569;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;</title><link>https://arxiv.org/abs/2402.14402</link><description>&lt;p&gt;
&#20840;&#23616;&#23433;&#20840;&#39034;&#24207;&#23398;&#20064;&#36890;&#36807;&#39640;&#25928;&#30693;&#35782;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Global Safe Sequential Learning via Efficient Knowledge Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14402
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32771;&#34385;&#36716;&#31227;&#23433;&#20840;&#30340;&#20840;&#23616;&#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#23433;&#20840;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#39044;&#20808;&#35745;&#31639;&#28304;&#32452;&#20214;&#26469;&#20943;&#23569;&#39069;&#22806;&#30340;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14402v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: &#39034;&#24207;&#23398;&#20064;&#26041;&#27861;&#20363;&#22914;&#20027;&#21160;&#23398;&#20064;&#21644;&#36125;&#21494;&#26031;&#20248;&#21270;&#36873;&#25321;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#25968;&#25454;&#26469;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#12290;&#22312;&#35768;&#22810;&#21307;&#23398;&#25110;&#24037;&#31243;&#24212;&#29992;&#20013;&#65292;&#25968;&#25454;&#36873;&#25321;&#21463;&#20808;&#39564;&#26410;&#30693;&#30340;&#23433;&#20840;&#26465;&#20214;&#38480;&#21046;&#12290;&#19968;&#26465;&#26377;&#21069;&#36884;&#30340;&#23433;&#20840;&#23398;&#20064;&#26041;&#27861;&#21033;&#29992;&#39640;&#26031;&#36807;&#31243;&#65288;GPs&#65289;&#26469;&#24314;&#27169;&#23433;&#20840;&#27010;&#29575;&#65292;&#24182;&#22312;&#20855;&#26377;&#36739;&#39640;&#23433;&#20840;&#32622;&#20449;&#24230;&#30340;&#21306;&#22495;&#20013;&#36827;&#34892;&#25968;&#25454;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#20934;&#30830;&#30340;&#23433;&#20840;&#24314;&#27169;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#25110;&#28040;&#32791;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#23433;&#20840;&#32622;&#20449;&#24230;&#38598;&#20013;&#22312;&#32473;&#23450;&#30340;&#35266;&#27979;&#20540;&#21608;&#22260;&#65292;&#23548;&#33268;&#23616;&#37096;&#25506;&#32034;&#12290;&#30001;&#20110;&#22312;&#23433;&#20840;&#20851;&#38190;&#23454;&#39564;&#20013;&#36890;&#24120;&#23384;&#22312;&#21487;&#36716;&#31227;&#30340;&#28304;&#30693;&#35782;&#65292;&#25105;&#20204;&#25552;&#20986;&#32771;&#34385;&#36716;&#31227;&#23433;&#20840;&#39034;&#24207;&#23398;&#20064;&#26469;&#21152;&#36895;&#23433;&#20840;&#23398;&#20064;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#32771;&#34385;&#20808;&#35745;&#31639;&#28304;&#32452;&#20214;&#65292;&#20197;&#20943;&#23569;&#24341;&#20837;&#28304;&#25968;&#25454;&#24102;&#26469;&#30340;&#39069;&#22806;&#35745;&#31639;&#36127;&#36733;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14402v1 Announce Type: new  Abstract: Sequential learning methods such as active learning and Bayesian optimization select the most informative data to learn about a task. In many medical or engineering applications, the data selection is constrained by a priori unknown safety conditions. A promissing line of safe learning methods utilize Gaussian processes (GPs) to model the safety probability and perform data selection in areas with high safety confidence. However, accurate safety modeling requires prior knowledge or consumes data. In addition, the safety confidence centers around the given observations which leads to local exploration. As transferable source knowledge is often available in safety critical experiments, we propose to consider transfer safe sequential learning to accelerate the learning of safety. We further consider a pre-computation of source components to reduce the additional computational load that is introduced by incorporating source data. In this pap
&lt;/p&gt;</description></item><item><title>UniGraph&#26694;&#26550;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#22270;&#21644;&#20219;&#21153;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;</title><link>https://arxiv.org/abs/2402.13630</link><description>&lt;p&gt;
UniGraph: &#20174;&#33258;&#28982;&#35821;&#35328;&#20013;&#23398;&#20064;&#36328;&#39046;&#22495;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13630
&lt;/p&gt;
&lt;p&gt;
UniGraph&#26694;&#26550;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#22270;&#21644;&#20219;&#21153;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13630v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#25688;&#35201;: ChatGPT &#21644; GPT-4 &#31561;&#22522;&#30784;&#27169;&#22411;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20154;&#24037;&#26234;&#33021;&#65292;&#23637;&#31034;&#20986;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#24212;&#29992;&#20013;&#27867;&#21270;&#30340;&#26174;&#33879;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#23427;&#20204;&#26368;&#21021;&#30340;&#35757;&#32451;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#36825;&#20010;&#27010;&#24565;&#24212;&#29992;&#20110;&#22270;&#23398;&#20064;&#26102;&#65292;&#20986;&#29616;&#20102;&#40092;&#26126;&#30340;&#23545;&#27604;&#12290;&#22270;&#23398;&#20064;&#20027;&#35201;&#38598;&#20013;&#22312;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#25110;&#25968;&#25454;&#38598;&#23450;&#21046;&#30340;&#21333;&#20010;&#22270;&#27169;&#22411;&#19978;&#65292;&#32570;&#20047;&#23558;&#23398;&#21040;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110;&#22270;&#32467;&#26500;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#65292;&#20197;&#21450;&#29305;&#23450;&#20110;&#22270;&#25968;&#25454;&#30340;&#19981;&#21516;&#29305;&#24449;&#21644;&#26631;&#31614;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;UniGraph&#26694;&#26550;&#65292;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#21040;&#19981;&#21516;&#39046;&#22495;&#30340;&#26410;&#35265;&#22270;&#21644;&#20219;&#21153;&#30340;&#22270;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13630v1 Announce Type: new  Abstract: Foundation models like ChatGPT and GPT-4 have revolutionized artificial intelligence, exhibiting remarkable abilities to generalize across a wide array of tasks and applications beyond their initial training objectives. However, when this concept is applied to graph learning, a stark contrast emerges. Graph learning has predominantly focused on single-graph models, tailored to specific tasks or datasets, lacking the ability to transfer learned knowledge to different domains. This limitation stems from the inherent complexity and diversity of graph structures, along with the different feature and label spaces specific to graph data. In this paper, we present our UniGraph framework, designed to train a graph foundation model capable of generalizing to unseen graphs and tasks across diverse domains. Unlike single-graph models that use pre-computed node features of varying dimensions as input, our approach leverages Text-Attributed Graphs (T
&lt;/p&gt;</description></item><item><title>GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2402.12566</link><description>&lt;p&gt;
GenAudit&#65306;&#21033;&#29992;&#35777;&#25454;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
GenAudit: Fixing Factual Errors in Language Model Outputs with Evidence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12566
&lt;/p&gt;
&lt;p&gt;
GenAudit&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#29486;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#24110;&#21161;&#20462;&#22797;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#21363;&#20351;&#21487;&#20197;&#35775;&#38382;&#21442;&#32771;&#25991;&#26723;&#65292;&#20063;&#21487;&#33021;&#29983;&#25104;&#20107;&#23454;&#19981;&#20934;&#30830;&#30340;&#38472;&#36848;&#12290;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#22522;&#20110;&#25991;&#26723;&#30340;&#21307;&#30103;&#20445;&#20581;&#25110;&#37329;&#34701;&#38382;&#31572;&#65289;&#65292;&#36825;&#26679;&#30340;&#38169;&#35823;&#21487;&#33021;&#20855;&#26377;&#21361;&#38505;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenAudit -- &#19968;&#20010;&#26088;&#22312;&#24110;&#21161;&#26816;&#26597;&#22522;&#20110;&#25991;&#26723;&#20219;&#21153;&#35821;&#35328;&#27169;&#22411;&#21709;&#24212;&#30340;&#24037;&#20855;&#12290;GenAudit&#36890;&#36807;&#20462;&#35746;&#25110;&#21024;&#38500;&#26410;&#34987;&#21442;&#32771;&#25991;&#26723;&#25903;&#25345;&#30340;&#22768;&#26126;&#65292;&#21516;&#26102;&#20026;&#30475;&#20284;&#34987;&#35777;&#25454;&#25903;&#25345;&#30340;&#20107;&#23454;&#25552;&#20379;&#26469;&#33258;&#21442;&#32771;&#25991;&#29486;&#30340;&#35777;&#25454;&#65292;&#26469;&#24314;&#35758;&#20462;&#25913;LLM&#21709;&#24212;&#12290;&#25105;&#20204;&#35757;&#32451;&#27169;&#22411;&#26469;&#25191;&#34892;&#36825;&#20123;&#20219;&#21153;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20132;&#20114;&#30028;&#38754;&#65292;&#21521;&#29992;&#25143;&#21576;&#29616;&#24314;&#35758;&#30340;&#20462;&#25913;&#21644;&#35777;&#25454;&#12290;&#36890;&#36807;&#20154;&#24037;&#35780;&#20998;&#21592;&#30340;&#20840;&#38754;&#35780;&#20272;&#26174;&#31034;&#65292;GenAudit&#22312;&#24635;&#32467;&#19981;&#21516;&#39046;&#22495;&#25991;&#26723;&#26102;&#33021;&#22815;&#26816;&#27979;&#20986;8&#31181;&#19981;&#21516;&#30340;LLM&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#20026;&#30830;&#20445;&#31995;&#32479;&#33021;&#22815;&#26631;&#35760;&#22823;&#22810;&#25968;&#38169;&#35823;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#38169;&#35823;&#21484;&#22238;&#29575;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#23545;&#39044;&#22788;&#29702;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12566v1 Announce Type: new  Abstract: LLMs can generate factually incorrect statements even when provided access to reference documents. Such errors can be dangerous in high-stakes applications (e.g., document-grounded QA for healthcare or finance). We present GenAudit -- a tool intended to assist fact-checking LLM responses for document-grounded tasks. GenAudit suggests edits to the LLM response by revising or removing claims that are not supported by the reference document, and also presents evidence from the reference for facts that do appear to have support. We train models to execute these tasks, and design an interactive interface to present suggested edits and evidence to users. Comprehensive evaluation by human raters shows that GenAudit can detect errors in 8 different LLM outputs when summarizing documents from diverse domains. To ensure that most errors are flagged by the system, we propose a method that can increase the error recall while minimizing impact on pre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.00162</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#25506;&#32034;&#32972;&#21518;&#30340;&#31070;&#35805;
&lt;/p&gt;
&lt;p&gt;
Behind the Myth of Exploration in Policy Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#26159;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25511;&#21046;&#38382;&#39064;&#30340;&#26377;&#25928;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#35745;&#31639;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#22312;&#23454;&#36341;&#20013;&#24517;&#39035;&#22312;&#23398;&#20064;&#30446;&#26631;&#20013;&#21253;&#21547;&#25506;&#32034;&#39033;&#12290;&#23613;&#31649;&#36825;&#20123;&#39033;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#36890;&#36807;&#23545;&#25506;&#32034;&#29615;&#22659;&#30340;&#20869;&#22312;&#38656;&#27714;&#36827;&#34892;&#35777;&#26126;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#20004;&#31181;&#19981;&#21516;&#21547;&#20041;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20351;&#24471;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;&#20445;&#25345;&#20840;&#23616;&#26368;&#22823;&#20540;&#30340;&#21516;&#26102;&#28040;&#38500;&#20102;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#20462;&#25913;&#20102;&#26799;&#24230;&#20272;&#35745;&#65292;&#22686;&#21152;&#20102;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26368;&#32456;&#25552;&#20379;&#26368;&#20248;&#31574;&#30053;&#30340;&#27010;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#25928;&#24212;&#65292;&#25105;&#20204;&#35752;&#35770;&#24182;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#31361;&#20986;&#20102;&#20854;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#36825;&#20123;&#31574;&#30053;&#30340;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2302.00284</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Selective Uncertainty Propagation in Offline RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.00284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#26377;&#25928;&#22320;&#22788;&#29702;&#20102;&#23454;&#38469;&#38382;&#39064;&#20013;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#26377;&#38480;&#26102;&#38388;&#27573;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#24773;&#26223;&#65292;&#30446;&#26631;&#22312;&#20110;&#24212;&#23545;&#21160;&#24577;&#35268;&#21010;&#31639;&#27861;&#20013;&#27599;&#19968;&#27493;&#31574;&#30053;&#23398;&#20064;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#35780;&#20272;&#31163;&#24320;&#34892;&#20026;&#31574;&#30053;&#22312;&#31532;h&#27493;&#26102;&#30340;&#22788;&#29702;&#25928;&#26524;&#65292;&#23601;&#21487;&#20197;&#23398;&#20064;&#21040;&#36825;&#19968;&#27493;&#30340;&#31574;&#30053;&#12290;&#30001;&#20110;&#27599;&#19968;&#27493;&#31574;&#30053;&#37117;&#20250;&#24433;&#21709;&#19979;&#19968;&#29366;&#24577;&#30340;&#20998;&#24067;&#65292;&#30456;&#20851;&#30340;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#20351;&#24471;&#36825;&#19968;&#38382;&#39064;&#22312;&#32479;&#35745;&#23398;&#19978;&#27604;&#38543;&#26426;&#24773;&#22659;&#25361;&#25112;&#19979;&#30340;&#22788;&#29702;&#25928;&#26524;&#20272;&#35745;&#26356;&#21152;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#29616;&#23454;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#38590;&#24230;&#20171;&#20110;&#36825;&#20004;&#31181;&#24773;&#22659;&#20043;&#38388;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#36873;&#25321;&#24615;&#19981;&#30830;&#23450;&#24615;&#20256;&#25773;&#65292;&#29992;&#20110;&#24314;&#31435;&#32622;&#20449;&#21306;&#38388;&#65292;&#24182;&#26681;&#25454;&#30456;&#20851;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#30340;&#38590;&#24230;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#29609;&#20855;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#21183;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#25216;&#26415;&#22312;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the finite-horizon offline reinforcement learning (RL) setting, and are motivated by the challenge of learning the policy at any step h in dynamic programming (DP) algorithms. To learn this, it is sufficient to evaluate the treatment effect of deviating from the behavioral policy at step h after having optimized the policy for all future steps. Since the policy at any step can affect next-state distributions, the related distributional shift challenges can make this problem far more statistically hard than estimating such treatment effects in the stochastic contextual bandit setting. However, the hardness of many real-world RL instances lies between the two regimes. We develop a flexible and general method called selective uncertainty propagation for confidence interval construction that adapts to the hardness of the associated distribution shift challenges. We show benefits of our approach on toy environments and demonstrate the benefits of these techniques for offline pol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#30340;&#28508;&#22312;&#34920;&#31034;&#22312;2&#32500;&#21333;&#20301;&#21452;&#26354;&#32447;&#19978;&#30340;&#22352;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32423;&#34920;&#31034;&#30340;&#26377;&#25928;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2309.16014</link><description>&lt;p&gt;
&#29992;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#36827;&#34892;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph-level Representation Learning with Joint-Embedding Predictive Architectures. (arXiv:2309.16014v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#32423;&#34920;&#31034;&#23398;&#20064;&#30340;&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPA&#65289;&#65292;&#36890;&#36807;&#39044;&#27979;&#36755;&#20837;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#30340;&#28508;&#22312;&#34920;&#31034;&#22312;2&#32500;&#21333;&#20301;&#21452;&#26354;&#32447;&#19978;&#30340;&#22352;&#26631;&#65292;&#23454;&#29616;&#20102;&#23545;&#22270;&#32423;&#34920;&#31034;&#30340;&#26377;&#25928;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#21512;&#23884;&#20837;&#39044;&#27979;&#26550;&#26500;&#65288;JEPAs&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#32780;&#24378;&#22823;&#30340;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#25216;&#26415;&#26368;&#36817;&#20986;&#29616;&#12290;&#23427;&#20204;&#26088;&#22312;&#36890;&#36807;&#20174;&#19978;&#19979;&#25991;&#20449;&#21495;x&#20013;&#39044;&#27979;&#30446;&#26631;&#20449;&#21495;y&#30340;&#28508;&#22312;&#34920;&#31034;&#26469;&#23398;&#20064;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#12290;JEPAs&#32469;&#36807;&#20102;&#23545;&#25968;&#25454;&#22686;&#24378;&#21644;&#36127;&#26679;&#26412;&#30340;&#38656;&#27714;&#65292;&#36825;&#36890;&#24120;&#26159;&#23545;&#27604;&#23398;&#20064;&#25152;&#35201;&#27714;&#30340;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#19982;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#30456;&#20851;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#33539;&#24335;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#22270;&#32423;&#34920;&#31034;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#20102;Graph-JEPA&#65292;&#36825;&#26159;&#22270;&#39046;&#22495;&#30340;&#31532;&#19968;&#20010;JEPA&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#37319;&#29992;&#25513;&#30721;&#24314;&#27169;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#36755;&#20837;&#22270;&#30340;&#19981;&#21516;&#23376;&#22270;&#30340;&#23884;&#20837;&#12290;&#20026;&#20102;&#36171;&#20104;&#34920;&#31034;&#38544;&#21547;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26367;&#20195;&#24615;&#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#35813;&#30446;&#26631;&#26159;&#39044;&#27979;&#32534;&#30721;&#23376;&#22270;&#22312;2&#32500;&#21333;&#20301;&#21452;&#26354;&#32447;&#19978;&#30340;&#22352;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Joint-Embedding Predictive Architectures (JEPAs) have recently emerged as a novel and powerful technique for self-supervised representation learning. They aim to learn an energy-based model by predicting the latent representation of a target signal $y$ from a context signal $x$. JEPAs bypass the need for data augmentation and negative samples, which are typically required by contrastive learning, while avoiding the overfitting issues associated with generative-based pretraining. In this paper, we show that graph-level representations can be effectively modeled using this paradigm and propose Graph-JEPA, the first JEPA for the graph domain. In particular, we employ masked modeling to learn embeddings for different subgraphs of the input graph. To endow the representations with the implicit hierarchy that is often present in graph-level concepts, we devise an alternative training objective that consists of predicting the coordinates of the encoded subgraphs on the unit hyperbola in the 2
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2308.12420</link><description>&lt;p&gt;
ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65306;&#23545;&#25991;&#29486;&#36827;&#34892;NLP&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Evolution of ESG-focused DLT Research: An NLP Analysis of the Literature. (arXiv:2308.12420v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;NLP&#20998;&#26512;&#20102;ESG&#20027;&#23548;&#30340;DLT&#30740;&#31350;&#30340;&#28436;&#21270;&#65292;&#36890;&#36807;&#26500;&#24314;&#24341;&#29992;&#32593;&#32476;&#21644;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#21457;&#23637;&#36827;&#34892;&#20102;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#36134;&#26412;&#25216;&#26415;(DLT)&#36805;&#36895;&#21457;&#23637;&#65292;&#38656;&#35201;&#20840;&#38754;&#20102;&#35299;&#20854;&#21508;&#20010;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;DLT&#30340;&#29615;&#22659;&#12289;&#21487;&#25345;&#32493;&#24615;&#21644;&#27835;&#29702;(ESG)&#32452;&#25104;&#37096;&#20998;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#36824;&#19981;&#36275;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;107&#31687;&#31181;&#23376;&#25991;&#29486;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;63,083&#20010;&#21442;&#32771;&#25991;&#29486;&#30340;&#24341;&#29992;&#32593;&#32476;&#65292;&#24182;&#23558;&#20854;&#31934;&#28860;&#20026;24,539&#31687;&#25991;&#29486;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#20998;&#26512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19968;&#20010;&#24050;&#24314;&#31435;&#30340;&#25216;&#26415;&#20998;&#31867;&#27861;&#20174;46&#31687;&#35770;&#25991;&#20013;&#26631;&#35760;&#20102;&#21629;&#21517;&#23454;&#20307;&#65292;&#24182;&#36890;&#36807;&#25214;&#20986;DLT&#30340;ESG&#35201;&#32032;&#26469;&#23436;&#21892;&#36825;&#20010;&#20998;&#31867;&#27861;&#12290;&#21033;&#29992;&#22522;&#20110;transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#32454;&#21270;&#35843;&#25972;&#65292;&#29992;&#20110;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#65292;&#20351;&#29992;&#25105;&#20204;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21033;&#29992;&#25105;&#20204;&#35843;&#25972;&#21518;&#30340;&#35821;&#35328;&#27169;&#22411;&#23545;&#35821;&#26009;&#24211;&#36827;&#34892;&#20102;&#31934;&#31616;&#65292;&#24471;&#21040;&#20102;505&#31687;&#20851;&#38190;&#35770;&#25991;&#65292;&#36890;&#36807;&#21629;&#21517;&#23454;&#20307;&#21644;&#26102;&#38388;&#22270;&#20998;&#26512;&#65292;&#20419;&#36827;&#20102;&#23545;DLT&#22312;ESG&#32972;&#26223;&#19979;&#30340;&#28436;&#21270;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed Ledger Technologies (DLTs) have rapidly evolved, necessitating comprehensive insights into their diverse components. However, a systematic literature review that emphasizes the Environmental, Sustainability, and Governance (ESG) components of DLT remains lacking. To bridge this gap, we selected 107 seed papers to build a citation network of 63,083 references and refined it to a corpus of 24,539 publications for analysis. Then, we labeled the named entities in 46 papers according to twelve top-level categories derived from an established technology taxonomy and enhanced the taxonomy by pinpointing DLT's ESG elements. Leveraging transformer-based language models, we fine-tuned a pre-trained language model for a Named Entity Recognition (NER) task using our labeled dataset. We used our fine-tuned language model to distill the corpus to 505 key papers, facilitating a literature review via named entities and temporal graph analysis on DLT evolution in the context of ESG. Our con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09254</link><description>&lt;p&gt;
&#29992;&#20110;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PAC Neural Prediction Set Learning to Quantify the Uncertainty of Generative Language Models. (arXiv:2307.09254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;PAC&#31070;&#32463;&#39044;&#27979;&#38598;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22810;&#31181;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#37327;&#21270;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#26159;&#22686;&#24378;&#27169;&#22411;&#21487;&#20449;&#24230;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#30001;&#20110;&#23545;&#29983;&#25104;&#34394;&#26500;&#20107;&#23454;&#30340;&#25285;&#24551;&#65292;&#26368;&#36817;&#20852;&#36215;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;GLM&#65289;&#29305;&#21035;&#24378;&#35843;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#39044;&#27979;&#38598;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20197;&#21487;&#33021;&#36817;&#20284;&#27491;&#30830;&#65288;PAC&#65289;&#30340;&#26041;&#24335;&#37327;&#21270;GLM&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#19982;&#29616;&#26377;&#30340;&#39044;&#27979;&#38598;&#27169;&#22411;&#36890;&#36807;&#26631;&#37327;&#20540;&#21442;&#25968;&#21270;&#19981;&#21516;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#39044;&#27979;&#38598;&#65292;&#23454;&#29616;&#26356;&#31934;&#30830;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65292;&#20294;&#20173;&#28385;&#36275;PAC&#20445;&#35777;&#12290;&#36890;&#36807;&#22312;&#22235;&#31181;&#31867;&#22411;&#30340;&#35821;&#35328;&#25968;&#25454;&#38598;&#21644;&#20845;&#31181;&#31867;&#22411;&#30340;&#27169;&#22411;&#19978;&#23637;&#31034;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#26631;&#20934;&#22522;&#20934;&#26041;&#27861;&#24179;&#22343;&#25552;&#39640;&#20102;63&#65285;&#30340;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty learning and quantification of models are crucial tasks to enhance the trustworthiness of the models. Importantly, the recent surge of generative language models (GLMs) emphasizes the need for reliable uncertainty quantification due to the concerns on generating hallucinated facts. In this paper, we propose to learn neural prediction set models that comes with the probably approximately correct (PAC) guarantee for quantifying the uncertainty of GLMs. Unlike existing prediction set models, which are parameterized by a scalar value, we propose to parameterize prediction sets via neural networks, which achieves more precise uncertainty quantification but still satisfies the PAC guarantee. We demonstrate the efficacy of our method on four types of language datasets and six types of models by showing that our method improves the quantified uncertainty by $63\%$ on average, compared to a standard baseline method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#27714;&#35299;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.07343</link><description>&lt;p&gt;
MaxMin-L2-SVC-NCH:&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MaxMin-L2-SVC-NCH: A New Method to Train Support Vector Classifier with the Selection of Model's Parameters. (arXiv:2307.07343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#12290;&#36890;&#36807;&#24314;&#27169;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#21033;&#29992;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#27714;&#35299;&#65292;&#23454;&#29616;&#20102;&#26356;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#21442;&#25968;&#30340;&#36873;&#25321;&#22312;&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#65288;SVC&#65289;&#30340;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#24120;&#29992;&#30340;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#30340;&#26041;&#27861;&#26159;k&#25240;&#20132;&#21449;&#39564;&#35777;&#19982;&#26684;&#28857;&#25628;&#32034;&#65288;CV&#65289;&#12290;&#30001;&#20110;&#38656;&#35201;&#35757;&#32451;&#22823;&#37327;&#30340;SVC&#27169;&#22411;&#65292;&#36825;&#20010;&#26041;&#27861;&#38750;&#24120;&#32791;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;SVC&#24182;&#36873;&#25321;&#27169;&#22411;&#21442;&#25968;&#12290;&#39318;&#20808;&#65292;&#23558;&#20855;&#26377;&#27169;&#22411;&#21442;&#25968;&#36873;&#25321;&#30340;SVC&#35757;&#32451;&#24314;&#27169;&#20026;&#26497;&#23567;&#21270;&#26497;&#22823;&#20248;&#21270;&#38382;&#39064;&#65288;MaxMin-L2-SVC-NCH&#65289;&#65292;&#20854;&#20013;&#26497;&#23567;&#21270;&#38382;&#39064;&#26159;&#23547;&#25214;&#20004;&#20010;&#27491;&#24120;&#20984;&#22771;&#20043;&#38388;&#26368;&#25509;&#36817;&#28857;&#30340;&#20248;&#21270;&#38382;&#39064;&#65288;L2-SVC-NCH&#65289;&#65292;&#32780;&#26497;&#22823;&#21270;&#38382;&#39064;&#26159;&#23547;&#25214;&#26368;&#20248;&#27169;&#22411;&#21442;&#25968;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;MaxMin-L2-SVC-NCH&#20855;&#26377;&#36739;&#20302;&#30340;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#22240;&#20026;&#25918;&#24323;&#20102;CV&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26799;&#24230;&#30340;&#31639;&#27861;&#26469;&#27714;&#35299;MaxMin-L2-SVC-NCH&#65292;&#20854;&#20013;L2-SVC-NCH&#36890;&#36807;&#25237;&#24433;&#26799;&#24230;&#31639;&#27861;&#27714;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The selection of model's parameters plays an important role in the application of support vector classification (SVC). The commonly used method of selecting model's parameters is the k-fold cross validation with grid search (CV). It is extremely time-consuming because it needs to train a large number of SVC models. In this paper, a new method is proposed to train SVC with the selection of model's parameters. Firstly, training SVC with the selection of model's parameters is modeled as a minimax optimization problem (MaxMin-L2-SVC-NCH), in which the minimization problem is an optimization problem of finding the closest points between two normal convex hulls (L2-SVC-NCH) while the maximization problem is an optimization problem of finding the optimal model's parameters. A lower time complexity can be expected in MaxMin-L2-SVC-NCH because CV is abandoned. A gradient-based algorithm is then proposed to solve MaxMin-L2-SVC-NCH, in which L2-SVC-NCH is solved by a projected gradient algorithm 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#38543;&#26426;Bregman&#36817;&#31471;&#26799;&#24230;&#27861;&#65288;SBPG&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Bregman&#36817;&#20284;&#27979;&#24230;&#26367;&#20195;&#20102;&#38543;&#26426;&#26799;&#24230;&#27861;&#20013;&#30340;&#19978;&#20108;&#27425;&#36924;&#36817;&#65292;&#24182;&#22312;&#25429;&#25417;&#38750;Lipschitz&#26799;&#24230;&#30340;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#24471;&#21040;&#26356;&#22909;&#30340;&#36817;&#20284;&#27169;&#22411;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;SBPG&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#31216;&#20026;MSBPG&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2306.14522</link><description>&lt;p&gt;
&#38750;&#20984;&#38543;&#26426; Bregman &#36817;&#31471;&#26799;&#24230;&#27861;&#21450;&#20854;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Nonconvex Stochastic Bregman Proximal Gradient Method with Application to Deep Learning. (arXiv:2306.14522v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.14522
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#20984;&#38543;&#26426;Bregman&#36817;&#31471;&#26799;&#24230;&#27861;&#65288;SBPG&#65289;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;Bregman&#36817;&#20284;&#27979;&#24230;&#26367;&#20195;&#20102;&#38543;&#26426;&#26799;&#24230;&#27861;&#20013;&#30340;&#19978;&#20108;&#27425;&#36924;&#36817;&#65292;&#24182;&#22312;&#25429;&#25417;&#38750;Lipschitz&#26799;&#24230;&#30340;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#26041;&#38754;&#24471;&#21040;&#26356;&#22909;&#30340;&#36817;&#20284;&#27169;&#22411;&#12290;&#35770;&#25991;&#35777;&#26126;&#20102;SBPG&#30340;&#25910;&#25947;&#24615;&#36136;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#37327;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#31216;&#20026;MSBPG&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20855;&#26377;&#26356;&#22909;&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#27867;&#20351;&#29992;&#30340;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#29992;&#20110;&#26368;&#23567;&#21270;&#38750;&#20984;&#22797;&#21512;&#30446;&#26631;&#20989;&#25968;&#26102;&#38656;&#35201;&#21487;&#24494;&#37096;&#20998;&#30340;Lipschitz&#24179;&#28369;&#24615;, &#20294;&#36825;&#19968;&#35201;&#27714;&#23545;&#20110;&#21253;&#25324;&#20108;&#27425;&#36870;&#38382;&#39064;&#21644;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#22312;&#20869;&#30340;&#38382;&#39064;&#31867;&#21035;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#26063;&#38543;&#26426; Bregman &#36817;&#31471;&#26799;&#24230; (SBPG) &#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#21482;&#38656;&#35201;&#21487;&#24494;&#37096;&#20998;&#30340;&#24179;&#28369;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The widely used stochastic gradient methods for minimizing nonconvex composite objective functions require the Lipschitz smoothness of the differentiable part. But the requirement does not hold true for problem classes including quadratic inverse problems and training neural networks. To address this issue, we investigate a family of stochastic Bregman proximal gradient (SBPG) methods, which only require smooth adaptivity of the differentiable part. SBPG replaces the upper quadratic approximation used in SGD with the Bregman proximity measure, resulting in a better approximation model that captures the non-Lipschitz gradients of the nonconvex objective. We formulate the vanilla SBPG and establish its convergence properties under nonconvex setting without finite-sum structure. Experimental results on quadratic inverse problems testify the robustness of SBPG. Moreover, we propose a momentum-based version of SBPG (MSBPG) and prove it has improved convergence properties. We apply MSBPG to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;PGMs&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26102;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#65292;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.17583</link><description>&lt;p&gt;
&#20851;&#20110;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#26080;&#38480;&#26641;&#29366;&#27010;&#29575;&#22270;&#27169;&#22411;&#30340;&#35770;&#25991;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Neural Networks as Infinite Tree-Structured Probabilistic Graphical Models. (arXiv:2305.17583v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;PGMs&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#30340;&#38382;&#39064;&#12290;&#30740;&#31350;&#21457;&#29616;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26102;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#65292;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#32570;&#20047;&#27010;&#29575;&#22270;&#27169;&#22411;(PGMs)&#30340;&#31934;&#30830;&#35821;&#20041;&#21644;&#26126;&#30830;&#23450;&#20041;&#30340;&#27010;&#29575;&#35299;&#37322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#31070;&#32463;&#32593;&#32476;&#23436;&#20840;&#23545;&#24212;&#30340;&#26080;&#38480;&#26641;&#29366;PGMs&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;DNNs&#22312;&#21069;&#21521;&#20256;&#25773;&#26399;&#38388;&#30830;&#23454;&#25191;&#34892;PGM&#25512;&#26029;&#30340;&#36817;&#20284;&#65292;&#36825;&#19982;&#26366;&#32463;&#30340;&#31070;&#32463;&#32593;&#32476;&#25551;&#36848;&#20026;&#26680;&#26426;&#22120;&#25110;&#26080;&#38480;&#22823;&#23567;&#30340;&#39640;&#26031;&#36807;&#31243;&#30340;&#29616;&#26377;&#30740;&#31350;&#19981;&#21516;&#65292;&#23427;&#38416;&#26126;&#20102;DNNs&#23545;PGMs&#20013;&#30340;&#31934;&#30830;&#25512;&#29702;&#30340;&#26356;&#30452;&#25509;&#36817;&#20284;&#12290;&#28508;&#22312;&#30340;&#22909;&#22788;&#21253;&#25324;&#25913;&#36827;DNNs&#30340;&#25945;&#23398;&#21644;&#35299;&#37322;&#65292;&#20197;&#21450;&#33021;&#22815;&#21512;&#24182;PGMs&#21644;DNNs&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) lack the precise semantics and definitive probabilistic interpretation of probabilistic graphical models (PGMs). In this paper, we propose an innovative solution by constructing infinite tree-structured PGMs that correspond exactly to neural networks. Our research reveals that DNNs, during forward propagation, indeed perform approximations of PGM inference that are precise in this alternative PGM structure. Not only does our research complement existing studies that describe neural networks as kernel machines or infinite-sized Gaussian processes, it also elucidates a more direct approximation that DNNs make to exact inference in PGMs. Potential benefits include improved pedagogy and interpretation of DNNs, and algorithms that can merge the strengths of PGMs and DNNs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#20013;&#25552;&#21462;&#20854;&#32479;&#35745;&#30452;&#35273;&#30340;&#20107;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#22120;&#20197;&#21382;&#21490;&#35266;&#23519;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#21487;&#33021;&#21457;&#29983;&#30340;&#39640;&#36136;&#37327;&#38543;&#21518;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#12289;&#28789;&#27963;&#21644;&#34920;&#31034;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2305.12569</link><description>&lt;p&gt;
&#26377;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#26159;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#30340;&#24517;&#22791;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conditional Generative Modeling is All You Need for Marked Temporal Point Processes. (arXiv:2305.12569v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12569
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#20013;&#25552;&#21462;&#20854;&#32479;&#35745;&#30452;&#35273;&#30340;&#20107;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#22120;&#20197;&#21382;&#21490;&#35266;&#23519;&#20316;&#20026;&#36755;&#20837;&#65292;&#29983;&#25104;&#21487;&#33021;&#21457;&#29983;&#30340;&#39640;&#36136;&#37327;&#38543;&#21518;&#20107;&#20214;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#12289;&#28789;&#27963;&#21644;&#34920;&#31034;&#33021;&#21147;&#31561;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24314;&#27169;&#30340;&#36827;&#27493;&#20351;&#24471;&#20174;&#19978;&#19979;&#25991;&#20449;&#24687;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#20869;&#23481;&#25104;&#20026;&#21487;&#33021;&#65292;&#20294;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65306;&#22914;&#20309;&#25945;&#27169;&#22411;&#30693;&#36947;&#20309;&#26102;&#29983;&#25104;&#20869;&#23481;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20107;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#20174;&#26631;&#35760;&#26102;&#38388;&#28857;&#36807;&#31243;&#20013;&#25552;&#21462;&#20854;&#32479;&#35745;&#30452;&#35273;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#24178;&#20928;&#12289;&#28789;&#27963;&#21644;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36866;&#29992;&#20110;&#28041;&#21450;&#22810;&#32500;&#26631;&#35760;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#25105;&#20204;&#26088;&#22312;&#25429;&#25417;&#28857;&#36807;&#31243;&#30340;&#20998;&#24067;&#32780;&#19981;&#38656;&#26126;&#30830;&#25351;&#23450;&#26465;&#20214;&#24378;&#24230;&#25110;&#27010;&#29575;&#23494;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#22120;&#65292;&#20197;&#20107;&#20214;&#21382;&#21490;&#20026;&#36755;&#20837;&#24182;&#29983;&#25104;&#22312;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20107;&#20214;&#19979;&#65292;&#21487;&#33021;&#21457;&#29983;&#30340;&#39640;&#36136;&#37327;&#38543;&#21518;&#20107;&#20214;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#21033;&#30410;&#65292;&#21253;&#25324;&#22312;&#23398;&#20064;&#27169;&#22411;&#21644;&#29983;&#25104;&#26679;&#26412;&#26041;&#38754;&#30340;&#24322;&#24120;&#25928;&#29575;&#20197;&#21450;&#30456;&#24403;&#22823;&#30340;&#34920;&#31034;&#33021;&#21147;&#26469;&#25429;&#25417;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in generative modeling have made it possible to generate high-quality content from context information, but a key question remains: how to teach models to know when to generate content? To answer this question, this study proposes a novel event generative model that draws its statistical intuition from marked temporal point processes, and offers a clean, flexible, and computationally efficient solution for a wide range of applications involving multi-dimensional marks. We aim to capture the distribution of the point process without explicitly specifying the conditional intensity or probability density. Instead, we use a conditional generator that takes the history of events as input and generates the high-quality subsequent event that is likely to occur given the prior observations. The proposed framework offers a host of benefits, including exceptional efficiency in learning the model and generating samples, as well as considerable representational power to capture
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24930;&#26597;&#35810;&#28857;&#25216;&#26415;&#25913;&#36827;Local-SGD&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25239;&#24322;&#26500;&#24773;&#20917;&#19979;&#26412;&#22320;&#26356;&#26032;&#30340;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2304.04169</link><description>&lt;p&gt;
SLowcal-SGD&#65306;&#24930;&#26597;&#35810;&#28857;&#25552;&#39640;&#20102;&#38543;&#26426;&#20984;&#20248;&#21270;&#30340;Local-SGD&#31639;&#27861;&#65288;arXiv:2304.04169v1 [cs.LG]&#65289;
&lt;/p&gt;
&lt;p&gt;
SLowcal-SGD: Slow Query Points Improve Local-SGD for Stochastic Convex Optimization. (arXiv:2304.04169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24930;&#26597;&#35810;&#28857;&#25216;&#26415;&#25913;&#36827;Local-SGD&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23545;&#25239;&#24322;&#26500;&#24773;&#20917;&#19979;&#26412;&#22320;&#26356;&#26032;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#22330;&#26223;&#20013;&#65292;M&#21488;&#35745;&#31639;&#26426;&#19982;&#21442;&#25968;&#26381;&#21153;&#22120;&#20132;&#20114;&#65292;&#36890;&#36807;&#22810;&#20010;&#36890;&#20449;&#36718;&#27425;&#26469;&#26368;&#23567;&#21270;&#32852;&#21512;&#30446;&#26631;&#20989;&#25968;&#12290;&#25105;&#20204;&#20851;&#27880;&#24322;&#26500;&#24773;&#20917;&#65292;&#21363;&#19981;&#21516;&#35745;&#31639;&#26426;&#21487;&#33021;&#20174;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#20013;&#25277;&#21462;&#26679;&#26412;&#65292;&#35774;&#35745;&#20102;&#31532;&#19968;&#31181;&#21487;&#20197;&#35777;&#26126;&#20248;&#20110;&#20004;&#20010;&#26368;&#31361;&#20986;&#30340;&#20998;&#24067;&#24335;&#22522;&#20934;&#32447;&#8212;&#8212;Minibatch-SGD&#21644;Local-SGD&#30340;&#26412;&#22320;&#26356;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#22312;&#20110;&#24930;&#26597;&#35810;&#25216;&#26415;&#65292;&#25105;&#20204;&#23558;&#20854;&#23450;&#21046;&#32473;&#20998;&#24067;&#24335;&#35774;&#32622;&#20351;&#29992;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#20943;&#36731;&#26412;&#22320;&#26356;&#26032;&#24102;&#26469;&#30340;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider distributed learning scenarios where M machines interact with a parameter server along several communication rounds in order to minimize a joint objective function. Focusing on the heterogeneous case, where different machines may draw samples from different data-distributions, we design the first local update method that provably benefits over the two most prominent distributed baselines: namely Minibatch-SGD and Local-SGD. Key to our approach is a slow querying technique that we customize to the distributed setting, which in turn enables a better mitigation of the bias caused by local updates.
&lt;/p&gt;</description></item><item><title>XPER&#26041;&#27861;&#33021;&#34913;&#37327;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20855;&#20307;&#36129;&#29486;&#65292;&#24182;&#21487;&#29992;&#20110;&#22788;&#29702;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#26500;&#24314;&#21516;&#36136;&#21270;&#20010;&#20307;&#32676;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2212.05866</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24615;&#33021;&#65306;&#34913;&#37327;&#39044;&#27979;&#24615;&#33021;&#30340;&#39537;&#21160;&#21147;
&lt;/p&gt;
&lt;p&gt;
Explainable Performance: Measuring the Driving Forces of Predictive Performance. (arXiv:2212.05866v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05866
&lt;/p&gt;
&lt;p&gt;
XPER&#26041;&#27861;&#33021;&#34913;&#37327;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20855;&#20307;&#36129;&#29486;&#65292;&#24182;&#21487;&#29992;&#20110;&#22788;&#29702;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#26500;&#24314;&#21516;&#36136;&#21270;&#20010;&#20307;&#32676;&#20307;&#65292;&#20174;&#32780;&#25552;&#39640;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;XPER&#65288;eXplainable PERformance&#65289;&#26041;&#27861;&#26469;&#34913;&#37327;&#36755;&#20837;&#29305;&#24449;&#23545;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#30340;&#20855;&#20307;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#29702;&#35770;&#19978;&#22522;&#20110;Shapley&#20540;&#65292;&#26082;&#19981;&#20381;&#36182;&#20110;&#27169;&#22411;&#65292;&#20063;&#19981;&#20381;&#36182;&#20110;&#24615;&#33021;&#24230;&#37327;&#12290;&#27492;&#22806;&#65292;XPER&#21487;&#22312;&#27169;&#22411;&#32423;&#21035;&#25110;&#20010;&#20307;&#32423;&#21035;&#23454;&#29616;&#12290;&#25105;&#20204;&#35777;&#26126;XPER&#20855;&#26377;&#26631;&#20934;&#35299;&#37322;&#24615;&#26041;&#27861;&#65288;SHAP&#65289;&#30340;&#29305;&#27530;&#24773;&#20917;&#12290;&#22312;&#36151;&#27454;&#36829;&#32422;&#39044;&#27979;&#24212;&#29992;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;XPER&#22788;&#29702;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#22806;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#22522;&#20110;&#20010;&#20307;XPER&#20540;&#23545;&#20182;&#20204;&#36827;&#34892;&#32858;&#31867;&#26469;&#26500;&#24314;&#21516;&#36136;&#21270;&#30340;&#20010;&#20307;&#32676;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#20272;&#35745;&#32676;&#20307;&#29305;&#23450;&#30340;&#27169;&#22411;&#27604;&#19968;&#20010;&#27169;&#22411;&#36866;&#29992;&#20110;&#25152;&#26377;&#20010;&#20307;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the XPER (eXplainable PERformance) methodology to measure the specific contribution of the input features to the predictive performance of a model. Our methodology is theoretically grounded on Shapley values and is both model-agnostic and performance metric-agnostic. Furthermore, XPER can be implemented either at the model level or at the individual level. We demonstrate that XPER has as a special case the standard explainability method in machine learning (SHAP). In a loan default forecasting application, we show how XPER can be used to deal with heterogeneity issues and significantly boost out-of-sample performance. To do so, we build homogeneous groups of individuals by clustering them based on their individual XPER values. We find that estimating group-specific models yields a much higher predictive accuracy than with a one-fits-all model.
&lt;/p&gt;</description></item></channel></rss>