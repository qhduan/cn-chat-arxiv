<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;CDR&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#65292;&#36991;&#20813;&#20256;&#32479;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.20298</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#26354;&#23884;&#20837;&#21644;&#23618;&#27425;&#24863;&#30693;&#22495;&#35299;&#32806;&#30340;&#22522;&#20110;&#35780;&#35770;&#30340;&#36328;&#39046;&#22495;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Review-Based Cross-Domain Recommendation via Hyperbolic Embedding and Hierarchy-Aware Domain Disentanglement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26354;CDR&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#25968;&#25454;&#31232;&#30095;&#24615;&#25361;&#25112;&#65292;&#36991;&#20813;&#20256;&#32479;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#31232;&#30095;&#24615;&#38382;&#39064;&#23545;&#25512;&#33616;&#31995;&#32479;&#26500;&#25104;&#20102;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#30340;&#31639;&#27861;&#65292;&#20197;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#36328;&#39046;&#22495;&#25512;&#33616;&#65288;CDR&#65289;&#21560;&#24341;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#25429;&#25417;&#21487;&#22312;&#39046;&#22495;&#38388;&#20849;&#20139;&#30340;&#30693;&#35782;&#65292;&#24182;&#23558;&#20854;&#20174;&#26356;&#20016;&#23500;&#30340;&#39046;&#22495;&#65288;&#28304;&#39046;&#22495;&#65289;&#36716;&#31227;&#21040;&#26356;&#31232;&#30095;&#30340;&#39046;&#22495;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#27431;&#20960;&#37324;&#24503;&#23884;&#20837;&#31354;&#38388;&#65292;&#22312;&#20934;&#30830;&#34920;&#31034;&#26356;&#20016;&#23500;&#30340;&#25991;&#26412;&#20449;&#24687;&#21644;&#22788;&#29702;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#22797;&#26434;&#20132;&#20114;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#20513;&#23548;&#19968;&#31181;&#22522;&#20110;&#35780;&#35770;&#25991;&#26412;&#30340;&#21452;&#26354;CDR&#26041;&#27861;&#26469;&#24314;&#27169;&#29992;&#25143;-&#29289;&#21697;&#20851;&#31995;&#12290;&#39318;&#20808;&#24378;&#35843;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#39046;&#22495;&#23545;&#40784;&#25216;&#26415;&#21487;&#33021;&#20250;&#23548;&#33268;&#38382;&#39064;&#65292;&#22240;&#20026;&#22312;&#21452;&#26354;&#20960;&#20309;&#20013;&#23545;&#23567;&#20462;&#25913;&#36896;&#25104;&#30340;&#24178;&#25200;&#20250;&#34987;&#25918;&#22823;&#65292;&#26368;&#32456;&#23548;&#33268;&#23618;&#27425;&#24615;&#23849;&#28291;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20298v1 Announce Type: cross  Abstract: The issue of data sparsity poses a significant challenge to recommender systems. In response to this, algorithms that leverage side information such as review texts have been proposed. Furthermore, Cross-Domain Recommendation (CDR), which captures domain-shareable knowledge and transfers it from a richer domain (source) to a sparser one (target), has received notable attention. Nevertheless, the majority of existing methodologies assume a Euclidean embedding space, encountering difficulties in accurately representing richer text information and managing complex interactions between users and items. This paper advocates a hyperbolic CDR approach based on review texts for modeling user-item relationships. We first emphasize that conventional distance-based domain alignment techniques may cause problems because small modifications in hyperbolic geometry result in magnified perturbations, ultimately leading to the collapse of hierarchical 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31526;&#21495;&#22270;&#30340;&#21487;&#36890;&#20449;&#24615;&#20960;&#20309;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#20854;&#24230;&#37327;&#26159;&#27431;&#20960;&#37324;&#24503;&#30340;&#21644;&#29699;&#24418;&#30340;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#35299;&#20915;&#31526;&#21495;&#22270;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07493</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#36890;&#20449;&#24615;&#20960;&#20309;&#23398;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#31526;&#21495;&#22270;
&lt;/p&gt;
&lt;p&gt;
Signed graphs in data sciences via communicability geometry
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07493
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31526;&#21495;&#22270;&#30340;&#21487;&#36890;&#20449;&#24615;&#20960;&#20309;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#20854;&#24230;&#37327;&#26159;&#27431;&#20960;&#37324;&#24503;&#30340;&#21644;&#29699;&#24418;&#30340;&#65292;&#28982;&#21518;&#24212;&#29992;&#20110;&#35299;&#20915;&#31526;&#21495;&#22270;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#22810;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22270;&#26159;&#34920;&#31034;&#22810;&#31181;&#23384;&#22312;&#20914;&#31361;&#20132;&#20114;&#30340;&#25968;&#25454;&#30340;&#26032;&#20852;&#26041;&#24335;&#65292;&#21253;&#25324;&#26469;&#33258;&#29983;&#29289;&#23398;&#12289;&#29983;&#24577;&#23398;&#21644;&#31038;&#20250;&#31995;&#32479;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#20102;&#31526;&#21495;&#22270;&#30340;&#21487;&#36890;&#20449;&#24615;&#20960;&#20309;&#27010;&#24565;&#65292;&#35777;&#26126;&#20102;&#22312;&#36825;&#20010;&#31354;&#38388;&#20013;&#30340;&#24230;&#37327;&#65292;&#27604;&#22914;&#21487;&#36890;&#20449;&#24615;&#36317;&#31163;&#21644;&#35282;&#24230;&#65292;&#26159;&#27431;&#20960;&#37324;&#24503;&#30340;&#21644;&#29699;&#24418;&#30340;&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;&#36825;&#20123;&#24230;&#37327;&#24212;&#29992;&#20110;&#20197;&#32479;&#19968;&#26041;&#24335;&#35299;&#20915;&#31526;&#21495;&#22270;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#20960;&#20010;&#38382;&#39064;&#65292;&#21253;&#25324;&#31526;&#21495;&#22270;&#30340;&#20998;&#21306;&#12289;&#32500;&#24230;&#32422;&#31616;&#12289;&#25214;&#21040;&#31526;&#21495;&#32593;&#32476;&#20013;&#30340;&#32852;&#30431;&#31561;&#32423;&#20197;&#21450;&#37327;&#21270;&#31995;&#32479;&#20013;&#29616;&#26377;&#27966;&#31995;&#20043;&#38388;&#26497;&#21270;&#31243;&#24230;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07493v1 Announce Type: cross  Abstract: Signed graphs are an emergent way of representing data in a variety of contexts were conflicting interactions exist. These include data from biological, ecological, and social systems. Here we propose the concept of communicability geometry for signed graphs, proving that metrics in this space, such as the communicability distance and angles, are Euclidean and spherical. We then apply these metrics to solve several problems in data analysis of signed graphs in a unified way. They include the partitioning of signed graphs, dimensionality reduction, finding hierarchies of alliances in signed networks as well as the quantification of the degree of polarization between the existing factions in systems represented by this type of graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25104;&#21151;&#23545;&#25239;&#26679;&#26412;&#27010;&#29575;&#19978;&#38480;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#21462;&#20915;&#20110;&#25200;&#21160;&#33539;&#25968;&#12289;&#26680;&#20989;&#25968;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26368;&#25509;&#36817;&#30340;&#19981;&#21516;&#26631;&#31614;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01896</link><description>&lt;p&gt;
&#25104;&#21151;&#23545;&#25239;&#26679;&#26412;&#30340;&#24378;&#40065;&#26834;&#24615;&#30028;&#38480;&#65306;&#29702;&#35770;&#19982;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Robustness Bounds on the Successful Adversarial Examples: Theory and Practice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25104;&#21151;&#23545;&#25239;&#26679;&#26412;&#27010;&#29575;&#19978;&#38480;&#30340;&#29702;&#35770;&#30028;&#38480;&#65292;&#21462;&#20915;&#20110;&#25200;&#21160;&#33539;&#25968;&#12289;&#26680;&#20989;&#25968;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#26368;&#25509;&#36817;&#30340;&#19981;&#21516;&#26631;&#31614;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#29702;&#35770;&#32467;&#26524;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#65288;AE&#65289;&#26159;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#30340;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#28155;&#21152;&#19981;&#21487;&#24863;&#30693;&#30340;&#25200;&#21160;&#26469;&#35825;&#20351;&#38169;&#20998;&#12290;&#26412;&#25991;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#65288;GP&#65289;&#20998;&#31867;&#65292;&#30740;&#31350;&#20102;&#25104;&#21151;AE&#30340;&#27010;&#29575;&#19978;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#26032;&#30340;&#19978;&#30028;&#65292;&#21462;&#20915;&#20110;AE&#30340;&#25200;&#21160;&#33539;&#25968;&#12289;GP&#20013;&#20351;&#29992;&#30340;&#26680;&#20989;&#25968;&#20197;&#21450;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#20855;&#26377;&#19981;&#21516;&#26631;&#31614;&#30340;&#26368;&#25509;&#36817;&#23545;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#35813;&#19978;&#38480;&#19981;&#21463;&#26679;&#26412;&#25968;&#25454;&#38598;&#20998;&#24067;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;ImageNet&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25913;&#21464;&#26680;&#20989;&#25968;&#21442;&#25968;&#20250;&#23548;&#33268;&#25104;&#21151;AE&#27010;&#29575;&#19978;&#38480;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01896v1 Announce Type: new  Abstract: Adversarial example (AE) is an attack method for machine learning, which is crafted by adding imperceptible perturbation to the data inducing misclassification. In the current paper, we investigated the upper bound of the probability of successful AEs based on the Gaussian Process (GP) classification. We proved a new upper bound that depends on AE's perturbation norm, the kernel function used in GP, and the distance of the closest pair with different labels in the training dataset. Surprisingly, the upper bound is determined regardless of the distribution of the sample dataset. We showed that our theoretical result was confirmed through the experiment using ImageNet. In addition, we showed that changing the parameters of the kernel function induces a change of the upper bound of the probability of successful AEs.
&lt;/p&gt;</description></item><item><title>&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.13213</link><description>&lt;p&gt;
&#36719;&#26368;&#22823;&#27010;&#29575;&#65288;&#22823;&#37096;&#20998;&#26102;&#20505;&#65289;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#39044;&#27979;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Softmax Probabilities (Mostly) Predict Large Language Model Correctness on Multiple-Choice Q&amp;A
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13213
&lt;/p&gt;
&lt;p&gt;
&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#22522;&#20110;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#30340;&#27169;&#22411;&#39044;&#27979;&#26041;&#27861;&#26377;&#21161;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#27491;&#30830;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26681;&#25454;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#30340;&#31574;&#30053;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36807;&#24230;&#33258;&#20449;&#20173;&#28982;&#26159;&#19968;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#35774;&#22312;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#20013;&#65292;&#38169;&#35823;&#31572;&#26696;&#23558;&#19982;&#26368;&#22823;softmax&#27010;&#29575;&#65288;MSPs&#65289;&#36739;&#23567;&#30456;&#20851;&#65292;&#30456;&#27604;&#20043;&#19979;&#27491;&#30830;&#31572;&#26696;&#36739;&#22823;&#12290;&#25105;&#20204;&#22312;&#21313;&#20010;&#24320;&#28304;LLMs&#21644;&#20116;&#20010;&#25968;&#25454;&#38598;&#19978;&#20840;&#38754;&#35780;&#20272;&#20102;&#36825;&#19968;&#20551;&#35774;&#65292;&#22312;&#34920;&#29616;&#33391;&#22909;&#30340;&#21407;&#22987;&#38382;&#31572;&#20219;&#21153;&#20013;&#21457;&#29616;&#20102;&#23545;&#25105;&#20204;&#20551;&#35774;&#30340;&#24378;&#26377;&#21147;&#35777;&#25454;&#12290;&#23545;&#20110;&#34920;&#29616;&#26368;&#20339;&#30340;&#20845;&#20010;LLMs&#65292;&#20174;MSP&#23548;&#20986;&#30340;AUROC&#22312;59/60&#20010;&#23454;&#20363;&#20013;&#37117;&#20248;&#20110;&#38543;&#26426;&#26426;&#20250;&#65292;p &lt; 10^{-4}&#12290;&#22312;&#36825;&#20845;&#20010;LLMs&#20013;&#65292;&#24179;&#22343;AUROC&#33539;&#22260;&#22312;60%&#33267;69%&#20043;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24102;&#26377;&#24323;&#26435;&#36873;&#39033;&#30340;&#22810;&#39033;&#36873;&#25321;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#23637;&#31034;&#36890;&#36807;&#26681;&#25454;&#21021;&#22987;&#27169;&#22411;&#21709;&#24212;&#30340;MSP&#26377;&#36873;&#25321;&#22320;&#24323;&#26435;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#29992;&#39044;softmax logits&#32780;&#19981;&#26159;softmax&#36827;&#34892;&#20102;&#30456;&#21516;&#30340;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13213v1 Announce Type: cross  Abstract: Although large language models (LLMs) perform impressively on many tasks, overconfidence remains a problem. We hypothesized that on multiple-choice Q&amp;A tasks, wrong answers would be associated with smaller maximum softmax probabilities (MSPs) compared to correct answers. We comprehensively evaluate this hypothesis on ten open-source LLMs and five datasets, and find strong evidence for our hypothesis among models which perform well on the original Q&amp;A task. For the six LLMs with the best Q&amp;A performance, the AUROC derived from the MSP was better than random chance with p &lt; 10^{-4} in 59/60 instances. Among those six LLMs, the average AUROC ranged from 60% to 69%. Leveraging these findings, we propose a multiple-choice Q&amp;A task with an option to abstain and show that performance can be improved by selectively abstaining based on the MSP of the initial model response. We also run the same experiments with pre-softmax logits instead of sof
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09122</link><description>&lt;p&gt;
&#28151;&#21512;&#36755;&#20986;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Mixed-Output Gaussian Process Latent Variable Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09122
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#20449;&#21495;&#20998;&#31163;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#21253;&#21547;&#32431;&#32452;&#20998;&#20449;&#21495;&#21152;&#26435;&#21644;&#30340;&#24773;&#20917;&#65292;&#36866;&#29992;&#20110;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#30340;&#20449;&#21495;&#20998;&#31163;&#26041;&#27861;&#65292;&#20854;&#20013;&#20449;&#21495;&#21487;&#20197;&#26681;&#25454;&#28508;&#21464;&#37327;&#21464;&#21270;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22686;&#21152;&#20102;&#39640;&#26031;&#36807;&#31243;&#28508;&#21464;&#37327;&#27169;&#22411;&#65288;GPLVMs&#65289;&#65292;&#20197;&#21253;&#25324;&#27599;&#20010;&#25968;&#25454;&#28857;&#30001;&#24050;&#30693;&#25968;&#37327;&#30340;&#32431;&#32452;&#20998;&#20449;&#21495;&#30340;&#21152;&#26435;&#21644;&#32452;&#25104;&#30340;&#24773;&#20917;&#65292;&#24182;&#35266;&#23519;&#22810;&#20010;&#36755;&#20837;&#20301;&#32622;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20351;&#29992;&#21508;&#31181;&#20851;&#20110;&#27599;&#20010;&#35266;&#27979;&#26435;&#37325;&#30340;&#20808;&#39564;&#12290;&#36825;&#31181;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#21253;&#25324;&#29992;&#20110;&#20272;&#35745;&#20998;&#25968;&#32452;&#25104;&#30340;&#24635;&#21644;&#20026;&#19968;&#32422;&#26463;&#21644;&#29992;&#20110;&#20998;&#31867;&#30340;&#20108;&#36827;&#21046;&#26435;&#37325;&#30340;&#29992;&#20363;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#23545;&#20110;&#20809;&#35889;&#23398;&#23588;&#20854;&#30456;&#20851;&#65292;&#22240;&#20026;&#25913;&#21464;&#26465;&#20214;&#21487;&#33021;&#23548;&#33268;&#22522;&#30784;&#32431;&#32452;&#20998;&#20449;&#21495;&#22312;&#26679;&#26412;&#20043;&#38388;&#21464;&#21270;&#12290;&#20026;&#20102;&#23637;&#31034;&#23545;&#20809;&#35889;&#23398;&#21644;&#20854;&#20182;&#39046;&#22495;&#30340;&#36866;&#29992;&#24615;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#20010;&#24212;&#29992;&#65306;&#19968;&#20010;&#20855;&#26377;&#19981;&#21516;&#28201;&#24230;&#30340;&#36817;&#32418;&#22806;&#20809;&#35889;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09122v1 Announce Type: cross Abstract: This work develops a Bayesian non-parametric approach to signal separation where the signals may vary according to latent variables. Our key contribution is to augment Gaussian Process Latent Variable Models (GPLVMs) to incorporate the case where each data point comprises the weighted sum of a known number of pure component signals, observed across several input locations. Our framework allows the use of a range of priors for the weights of each observation. This flexibility enables us to represent use cases including sum-to-one constraints for estimating fractional makeup, and binary weights for classification. Our contributions are particularly relevant to spectroscopy, where changing conditions may cause the underlying pure component signals to vary from sample to sample. To demonstrate the applicability to both spectroscopy and other domains, we consider several applications: a near-infrared spectroscopy data set with varying temper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2311.01434</link><description>&lt;p&gt;
&#36890;&#36807;&#26680;&#25197;&#26354;&#20989;&#25968;&#23450;&#21046;Mixup&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Tailoring Mixup to Data using Kernel Warping functions. (arXiv:2311.01434v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26680;&#25197;&#26354;&#20989;&#25968;&#23545;Mixup&#25968;&#25454;&#36827;&#34892;&#20010;&#24615;&#21270;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#26469;&#23454;&#29616;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#30456;&#20284;&#25968;&#25454;&#28857;&#12290;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#23398;&#20064;&#39640;&#25928;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#22522;&#30784;&#12290;&#22312;&#25152;&#26377;&#25552;&#20986;&#30340;&#22686;&#24378;&#25216;&#26415;&#20013;&#65292;&#32447;&#24615;&#25554;&#20540;&#35757;&#32451;&#25968;&#25454;&#28857;&#65288;&#20063;&#31216;&#20026;Mixup&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#36873;&#25321;&#21512;&#36866;&#30340;&#28857;&#36827;&#34892;&#28151;&#21512;&#65292;&#25110;&#32773;&#24212;&#29992;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#25554;&#20540;&#65292;&#32780;&#25105;&#20204;&#21017;&#23545;&#26356;&#30456;&#20284;&#30340;&#28857;&#36827;&#34892;&#26356;&#39057;&#32321;&#21644;&#26356;&#24378;&#28872;&#30340;&#28151;&#21512;&#24863;&#20852;&#36259;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#25197;&#26354;&#20989;&#25968;&#21160;&#24577;&#25913;&#21464;&#25554;&#20540;&#31995;&#25968;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#21462;&#20915;&#20110;&#35201;&#32452;&#21512;&#30340;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#39640;&#25928;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20197;&#36991;&#20813;&#22810;&#26679;&#24615;&#30340;&#25439;&#22833;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26082;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#21448;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ENSTA-U2IS/torch-uncertainty&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is an essential building block for learning efficient deep learning models. Among all augmentation techniques proposed so far, linear interpolation of training data points, also called mixup, has found to be effective for a large panel of applications. While the majority of works have focused on selecting the right points to mix, or applying complex non-linear interpolation, we are interested in mixing similar points more frequently and strongly than less similar ones. To this end, we propose to dynamically change the underlying distribution of interpolation coefficients through warping functions, depending on the similarity between data points to combine. We define an efficient and flexible framework to do so without losing in diversity. We provide extensive experiments for classification and regression tasks, showing that our proposed method improves both performance and calibration of models. Code available in https://github.com/ENSTA-U2IS/torch-uncertainty
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#20132;&#36890;&#29702;&#35770;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#36710;&#36742;&#32534;&#38431;&#21644;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20316;&#20026;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24182;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21327;&#35843;&#65292;&#20197;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#21644;&#32531;&#35299;&#22478;&#24066;&#25317;&#22581;&#12290;</title><link>http://arxiv.org/abs/2310.10948</link><description>&lt;p&gt;
&#36890;&#36807;&#21327;&#20316;&#35299;&#20915;&#22478;&#24066;&#25317;&#22581;&#65306;&#22522;&#20110;&#24322;&#26500;GNN&#30340;&#21327;&#35843;&#32534;&#38431;&#21644;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Combat Urban Congestion via Collaboration: Heterogeneous GNN-based MARL for Coordinated Platooning and Traffic Signal Control. (arXiv:2310.10948v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24322;&#26500;&#22270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#20132;&#36890;&#29702;&#35770;&#30340;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#23558;&#36710;&#36742;&#32534;&#38431;&#21644;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20316;&#20026;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#24182;&#32467;&#21512;&#22270;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#21327;&#35843;&#65292;&#20197;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#21644;&#32531;&#35299;&#22478;&#24066;&#25317;&#22581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29420;&#31435;&#25110;&#20998;&#23618;&#26041;&#24335;&#24320;&#21457;&#20449;&#21495;&#25511;&#21046;&#21644;&#36710;&#36742;&#32534;&#38431;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#26102;&#20013;&#32852;&#21512;&#25511;&#21046;&#36825;&#20004;&#32773;&#20197;&#20943;&#36731;&#20132;&#36890;&#25317;&#22581;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22914;&#20449;&#21495;&#25511;&#21046;&#21644;&#32534;&#38431;&#20043;&#38388;&#22266;&#26377;&#30340;&#29289;&#29702;&#21644;&#34892;&#20026;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#20043;&#38388;&#30340;&#21327;&#35843;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#22522;&#20110;&#24322;&#26500;&#22270;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21644;&#20132;&#36890;&#29702;&#35770;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#65306;1&#65289;&#23558;&#32534;&#38431;&#21644;&#20449;&#21495;&#25511;&#21046;&#35774;&#35745;&#20026;&#19981;&#21516;&#30340;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#65292;&#20855;&#26377;&#33258;&#24049;&#30340;&#35266;&#27979;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#20248;&#21270;&#20132;&#36890;&#27969;&#37327;&#65307;2&#65289;&#36890;&#36807;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#26469;&#35774;&#35745;&#21327;&#35843;&#65292;&#20197;&#20419;&#36827;&#21306;&#22495;&#33539;&#22260;&#20869;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#26080;&#32541;&#20449;&#24687;&#20132;&#25442;&#12290;&#25105;&#20204;&#36890;&#36807;SUMO&#27169;&#25311;&#29615;&#22659;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, reinforcement learning has emerged as a popular approach to develop signal control and vehicle platooning strategies either independently or in a hierarchical way. However, jointly controlling both in real-time to alleviate traffic congestion presents new challenges, such as the inherent physical and behavioral heterogeneity between signal control and platooning, as well as coordination between them. This paper proposes an innovative solution to tackle these challenges based on heterogeneous graph multi-agent reinforcement learning and traffic theories. Our approach involves: 1) designing platoon and signal control as distinct reinforcement learning agents with their own set of observations, actions, and reward functions to optimize traffic flow; 2) designing coordination by incorporating graph neural networks within multi-agent reinforcement learning to facilitate seamless information exchange among agents on a regional scale. We evaluate our approach through SUMO simu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HASTE&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#25216;&#26415;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#25110;&#31934;&#35843;&#21363;&#21487;&#23454;&#26102;&#38477;&#20302;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;&#21387;&#32553;&#29305;&#24449;&#22270;&#26102;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17211</link><description>&lt;p&gt;
&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#22312;CNN&#20013;&#23454;&#29616;&#21363;&#26102;&#22797;&#26434;&#24230;&#38477;&#20302;
&lt;/p&gt;
&lt;p&gt;
Instant Complexity Reduction in CNNs using Locality-Sensitive Hashing. (arXiv:2309.17211v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17211
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;HASTE&#30340;&#27169;&#22359;&#65292;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#25216;&#26415;&#65292;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#25110;&#31934;&#35843;&#21363;&#21487;&#23454;&#26102;&#38477;&#20302;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#19988;&#22312;&#21387;&#32553;&#29305;&#24449;&#22270;&#26102;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#35774;&#22791;&#19978;&#38477;&#20302;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#32467;&#26500;&#21270;&#21098;&#26525;&#26041;&#27861;&#24050;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#22312;&#19981;&#22826;&#22823;&#31243;&#24230;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#22823;&#22823;&#20943;&#23569;&#20102;&#28014;&#28857;&#36816;&#31639;&#65288;FLOPs&#65289;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#26041;&#27861;&#35201;&#27714;&#36827;&#34892;&#31934;&#35843;&#25110;&#29305;&#23450;&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20197;&#23454;&#29616;&#22312;&#20445;&#30041;&#20934;&#30830;&#24615;&#21644;&#38477;&#20302;FLOPs&#20043;&#38388;&#21512;&#29702;&#25240;&#34935;&#12290;&#36825;&#24341;&#20837;&#20102;&#35745;&#31639;&#24320;&#38144;&#30340;&#39069;&#22806;&#25104;&#26412;&#65292;&#24182;&#38656;&#35201;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HASTE&#65288;Hashing for Tractable Efficiency&#65289;&#65292;&#23427;&#26159;&#19968;&#20010;&#26080;&#38656;&#21442;&#25968;&#21644;&#26080;&#38656;&#25968;&#25454;&#30340;&#27169;&#22359;&#65292;&#21487;&#20197;&#20316;&#20026;&#20219;&#20309;&#24120;&#35268;&#21367;&#31215;&#27169;&#22359;&#30340;&#21363;&#25554;&#21363;&#29992;&#26367;&#20195;&#21697;&#12290;&#23427;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#35757;&#32451;&#25110;&#31934;&#35843;&#30340;&#24773;&#20917;&#19979;&#21363;&#26102;&#38477;&#20302;&#32593;&#32476;&#30340;&#27979;&#35797;&#25512;&#29702;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;&#23616;&#37096;&#25935;&#24863;&#21704;&#24076;&#65288;LSH&#65289;&#26469;&#26816;&#27979;&#29305;&#24449;&#22270;&#20013;&#30340;&#20887;&#20313;&#65292;&#25105;&#20204;&#33021;&#22815;&#22823;&#24133;&#21387;&#32553;&#28508;&#22312;&#29305;&#24449;&#22270;&#32780;&#20960;&#20046;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To reduce the computational cost of convolutional neural networks (CNNs) for usage on resource-constrained devices, structured pruning approaches have shown promising results, drastically reducing floating-point operations (FLOPs) without substantial drops in accuracy. However, most recent methods require fine-tuning or specific training procedures to achieve a reasonable trade-off between retained accuracy and reduction in FLOPs. This introduces additional cost in the form of computational overhead and requires training data to be available. To this end, we propose HASTE (Hashing for Tractable Efficiency), a parameter-free and data-free module that acts as a plug-and-play replacement for any regular convolution module. It instantly reduces the network's test-time inference cost without requiring any training or fine-tuning. We are able to drastically compress latent feature maps without sacrificing much accuracy by using locality-sensitive hashing (LSH) to detect redundancies in the c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2207.04053</link><description>&lt;p&gt;
&#35770;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#22240;&#26524;&#20851;&#31995;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Need and Applicability of Causality for Fair Machine Learning. (arXiv:2207.04053v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20851;&#31995;&#22312;&#20844;&#24179;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24517;&#35201;&#24615;&#21644;&#36866;&#29992;&#24615;&#65292;&#24378;&#35843;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#21644;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#20381;&#36182;&#20110;&#22240;&#26524;&#20027;&#24352;&#12290;&#21516;&#26102;&#35752;&#35770;&#20102;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#22312;&#27969;&#34892;&#30149;&#23398;&#12289;&#25919;&#27835;&#21644;&#31038;&#20250;&#31185;&#23398;&#20013;&#30340;&#24120;&#35265;&#24212;&#29992;&#26696;&#20363;&#22806;&#65292;&#20107;&#23454;&#35777;&#26126;&#22240;&#26524;&#20851;&#31995;&#22312;&#35780;&#20272;&#33258;&#21160;&#20915;&#31574;&#30340;&#20844;&#27491;&#24615;&#26041;&#38754;&#21313;&#20998;&#37325;&#35201;&#65292;&#26080;&#35770;&#26159;&#22312;&#27861;&#24459;&#19978;&#36824;&#26159;&#26085;&#24120;&#29983;&#27963;&#20013;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#20026;&#20309;&#22240;&#26524;&#20851;&#31995;&#23545;&#20844;&#24179;&#24615;&#35780;&#20272;&#23588;&#20026;&#37325;&#35201;&#30340;&#35770;&#28857;&#21644;&#31034;&#20363;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#38750;&#22240;&#26524;&#39044;&#27979;&#30340;&#31038;&#20250;&#24433;&#21709;&#20197;&#21450;&#20381;&#36182;&#22240;&#26524;&#20027;&#24352;&#30340;&#27861;&#24459;&#21453;&#27495;&#35270;&#36807;&#31243;&#12290;&#25105;&#20204;&#26368;&#21518;&#35752;&#35770;&#20102;&#24212;&#29992;&#22240;&#26524;&#20851;&#31995;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#25361;&#25112;&#21644;&#23616;&#38480;&#24615;&#65292;&#20197;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Besides its common use cases in epidemiology, political, and social sciences, causality turns out to be crucial in evaluating the fairness of automated decisions, both in a legal and everyday sense. We provide arguments and examples, of why causality is particularly important for fairness evaluation. In particular, we point out the social impact of non-causal predictions and the legal anti-discrimination process that relies on causal claims. We conclude with a discussion about the challenges and limitations of applying causality in practical scenarios as well as possible solutions.
&lt;/p&gt;</description></item></channel></rss>