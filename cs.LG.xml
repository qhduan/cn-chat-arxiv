<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35838;&#31243;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;</title><link>https://arxiv.org/abs/2205.10016</link><description>&lt;p&gt;
&#23398;&#20064;&#36827;&#24230;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;&#35838;&#31243;
&lt;/p&gt;
&lt;p&gt;
Learning Progress Driven Multi-Agent Curriculum
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.10016
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#20197;&#35299;&#20915;&#24403;&#21069;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#35838;&#31243;&#29983;&#25104;&#30340;&#38382;&#39064;&#65292;&#20248;&#20808;&#32771;&#34385;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35838;&#31243;&#24378;&#21270;&#23398;&#20064;&#65288;CRL&#65289;&#26088;&#22312;&#36890;&#36807;&#36880;&#28176;&#22686;&#21152;&#20219;&#21153;&#30340;&#38590;&#24230;&#65288;&#36890;&#24120;&#30001;&#21487;&#23454;&#29616;&#30340;&#39044;&#26399;&#22238;&#25253;&#37327;&#21270;&#65289;&#26469;&#21152;&#24555;&#23398;&#20064;&#36895;&#24230;&#12290;&#21463;CRL&#22312;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#23558;CRL&#24212;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#65292;&#20351;&#29992;&#26234;&#33021;&#20307;&#25968;&#37327;&#26469;&#25511;&#21046;&#20219;&#21153;&#38590;&#24230;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20351;&#29992;&#25163;&#21160;&#23450;&#20041;&#30340;&#35838;&#31243;&#65292;&#22914;&#32447;&#24615;&#26041;&#26696;&#12290;&#26412;&#25991;&#39318;&#20808;&#23558;&#26368;&#20808;&#36827;&#30340;&#21333;&#26234;&#33021;&#20307;&#33258;&#20027;&#24335;CRL&#24212;&#29992;&#20110;&#31232;&#30095;&#22870;&#21169;MARL&#12290;&#34429;&#28982;&#34920;&#29616;&#20196;&#20154;&#28385;&#24847;&#65292;&#20294;&#25105;&#20204;&#30830;&#23450;&#20102;&#29616;&#26377;&#22522;&#20110;&#22870;&#21169;&#30340;CRL&#26041;&#27861;&#29983;&#25104;&#30340;&#35838;&#31243;&#23384;&#22312;&#20004;&#20010;&#28508;&#22312;&#32570;&#38519;&#65306;&#65288;1&#65289;&#39640;&#22238;&#25253;&#30340;&#20219;&#21153;&#21487;&#33021;&#19981;&#25552;&#20379;&#20449;&#24687;&#37327;&#22823;&#30340;&#23398;&#20064;&#20449;&#21495;&#65292;&#65288;2&#65289;&#22312;&#22810;&#26234;&#33021;&#20307;&#20135;&#29983;&#26356;&#39640;&#22238;&#25253;&#30340;&#20219;&#21153;&#20013;&#65292;&#21152;&#21095;&#20102;&#23398;&#20998;&#20998;&#37197;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#33258;&#20027;&#24335;MARL&#65288;SPMARL&#65289;&#65292;&#20197;&#22522;&#20110;&#20219;&#21153;&#30340;&#20248;&#20808;&#32423;&#36827;&#34892;&#23433;&#25490;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.10016v2 Announce Type: replace  Abstract: Curriculum reinforcement learning (CRL) aims to speed up learning by gradually increasing the difficulty of a task, usually quantified by the achievable expected return. Inspired by the success of CRL in single-agent settings, a few works have attempted to apply CRL to multi-agent reinforcement learning (MARL) using the number of agents to control task difficulty. However, existing works typically use manually defined curricula such as a linear scheme. In this paper, we first apply state-of-the-art single-agent self-paced CRL to sparse reward MARL. Although with satisfying performance, we identify two potential flaws of the curriculum generated by existing reward-based CRL methods: (1) tasks with high returns may not provide informative learning signals and (2) the exacerbated credit assignment difficulty in tasks where more agents yield higher returns. Thereby, we further propose self-paced MARL (SPMARL) to prioritize tasks based on
&lt;/p&gt;</description></item><item><title>&#36825;&#26159;&#19968;&#20010;&#23398;&#20064;&#22686;&#24378;&#30340;B&#26641;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22797;&#21512;&#20248;&#20808;&#32423;&#30340;Treaps&#65292;&#27599;&#20010;&#39033;&#30446;&#30340;&#28145;&#24230;&#30001;&#20854;&#39044;&#27979;&#26435;&#37325;&#30830;&#23450;&#65292;&#25512;&#24191;&#20102;&#26368;&#36817;&#30340;&#23398;&#20064;&#22686;&#24378;BST&#65292;&#24182;&#19988;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#35775;&#38382;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#24615;&#30340;B&#26641;&#25968;&#25454;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2211.09251</link><description>&lt;p&gt;
&#23398;&#20064;&#22686;&#24378;&#30340;B&#26641;
&lt;/p&gt;
&lt;p&gt;
Learning-Augmented B-Trees. (arXiv:2211.09251v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09251
&lt;/p&gt;
&lt;p&gt;
&#36825;&#26159;&#19968;&#20010;&#23398;&#20064;&#22686;&#24378;&#30340;B&#26641;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22797;&#21512;&#20248;&#20808;&#32423;&#30340;Treaps&#65292;&#27599;&#20010;&#39033;&#30446;&#30340;&#28145;&#24230;&#30001;&#20854;&#39044;&#27979;&#26435;&#37325;&#30830;&#23450;&#65292;&#25512;&#24191;&#20102;&#26368;&#36817;&#30340;&#23398;&#20064;&#22686;&#24378;BST&#65292;&#24182;&#19988;&#26159;&#31532;&#19968;&#20010;&#21487;&#20197;&#21033;&#29992;&#35775;&#38382;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#24615;&#30340;B&#26641;&#25968;&#25454;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22797;&#21512;&#20248;&#20808;&#32423;&#30340;Treaps&#26469;&#30740;&#31350;&#23398;&#20064;&#22686;&#24378;&#30340;&#20108;&#21449;&#25628;&#32034;&#26641;&#65288;BST&#65289;&#21644;B&#26641;&#12290;&#32467;&#26524;&#26159;&#19968;&#20010;&#31616;&#21333;&#30340;&#25628;&#32034;&#26641;&#65292;&#20854;&#20013;&#27599;&#20010;&#39033;&#30446;&#30340;&#28145;&#24230;&#30001;&#20854;&#39044;&#27979;&#26435;&#37325;$w_x$&#30830;&#23450;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#32467;&#26524;&#65292;&#27599;&#20010;&#39033;&#30446;$x$&#37117;&#26377;&#20854;&#22797;&#21512;&#20248;&#20808;&#32423;$-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$&#65292;&#20854;&#20013;$U(0, 1)$&#26159;&#22343;&#21248;&#20998;&#24067;&#30340;&#38543;&#26426;&#21464;&#37327;&#12290;&#36825;&#23558;&#26368;&#36817;&#30340;&#23398;&#20064;&#22686;&#24378;BST&#65288;Lin-Luo-Woodruff ICML`22&#65289;&#25512;&#24191;&#21040;&#20219;&#24847;&#36755;&#20837;&#21644;&#39044;&#27979;&#65292;&#32780;&#19981;&#20165;&#20165;&#36866;&#29992;&#20110;Zipfian&#20998;&#24067;&#12290;&#23427;&#36824;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#26681;&#25454;&#35775;&#38382;&#24207;&#21015;&#20013;&#30340;&#23616;&#37096;&#24615;&#36827;&#34892;&#22312;&#32447;&#33258;&#25105;&#37325;&#32452;&#30340;B&#26641;&#25968;&#25454;&#32467;&#26500;&#12290;&#35813;&#25968;&#25454;&#32467;&#26500;&#23545;&#20110;&#39044;&#27979;&#38169;&#35823;&#26159;&#20581;&#22766;&#30340;&#65292;&#21487;&#20197;&#22788;&#29702;&#25554;&#20837;&#12289;&#21024;&#38500;&#20197;&#21450;&#39044;&#27979;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study learning-augmented binary search trees (BSTs) and B-Trees via Treaps with composite priorities. The result is a simple search tree where the depth of each item is determined by its predicted weight $w_x$. To achieve the result, each item $x$ has its composite priority $-\lfloor\log\log(1/w_x)\rfloor + U(0, 1)$ where $U(0, 1)$ is the uniform random variable. This generalizes the recent learning-augmented BSTs [Lin-Luo-Woodruff ICML`22], which only work for Zipfian distributions, to arbitrary inputs and predictions. It also gives the first B-Tree data structure that can provably take advantage of localities in the access sequence via online self-reorganization. The data structure is robust to prediction errors and handles insertions, deletions, as well as prediction updates.
&lt;/p&gt;</description></item></channel></rss>