<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#65292;&#20998;&#21035;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#21644;&#20449;&#24687;&#35770;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03587</link><description>&lt;p&gt;
&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#30340;&#26377;&#25928;&#33719;&#21462;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Effective Acquisition Functions for Active Correlation Clustering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#20027;&#21160;&#30456;&#20851;&#32858;&#31867;&#65292;&#20998;&#21035;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#21644;&#20449;&#24687;&#35770;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#32858;&#31867;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#20363;&#65292;&#25903;&#25345;&#27491;&#21644;&#36127;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#20551;&#35774;&#30456;&#20284;&#24615;&#20107;&#20808;&#26410;&#30693;&#65292;&#32780;&#26159;&#37319;&#29992;&#20027;&#21160;&#23398;&#20064;&#20197;&#19968;&#31181;&#25104;&#26412;&#26377;&#25928;&#30340;&#26041;&#24335;&#36845;&#20195;&#22320;&#26597;&#35810;&#30456;&#20284;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19977;&#31181;&#26377;&#25928;&#30340;&#33719;&#21462;&#20989;&#25968;&#29992;&#20110;&#22312;&#27492;&#35774;&#32622;&#19979;&#20351;&#29992;&#12290;&#20854;&#20013;&#19968;&#31181;&#22522;&#20110;&#19981;&#19968;&#33268;&#24615;&#27010;&#24565;&#65288;&#21363;&#24403;&#30456;&#20284;&#24615;&#36829;&#21453;&#20256;&#36882;&#24615;&#26102;&#65289;&#12290;&#20854;&#20313;&#20004;&#20010;&#22522;&#20110;&#20449;&#24687;&#35770;&#37327;&#65292;&#21363;&#29109;&#21644;&#20449;&#24687;&#22686;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlation clustering is a powerful unsupervised learning paradigm that supports positive and negative similarities. In this paper, we assume the similarities are not known in advance. Instead, we employ active learning to iteratively query similarities in a cost-efficient way. In particular, we develop three effective acquisition functions to be used in this setting. One is based on the notion of inconsistency (i.e., when similarities violate the transitive property). The remaining two are based on information-theoretic quantities, i.e., entropy and information gain.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;</title><link>http://arxiv.org/abs/2310.16152</link><description>&lt;p&gt;
FLTrojan: &#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#23545;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#27844;&#38706;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering. (arXiv:2310.16152v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#27491;&#25104;&#20026;&#35768;&#22810;&#25216;&#26415;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#65292;&#20854;&#20013;&#20010;&#20307;FL&#21442;&#19982;&#32773;&#22312;&#20854;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#24448;&#24448;&#20855;&#26377;&#25935;&#24863;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#31243;&#24230;&#24182;&#19981;&#31616;&#21333;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#26159;&#35797;&#22270;&#25552;&#21462;&#25968;&#25454;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#25110;&#22825;&#30495;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#30340;&#20004;&#20010;&#26032;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#27604;&#26368;&#32456;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#36896;&#25104;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#65292;&#36825;&#20123;&#26435;&#37325;&#29305;&#21035;&#36127;&#36131;&#35760;&#24518;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24694;&#24847;&#23458;&#25143;&#31471;&#22914;&#20309;&#22312;FL&#20013;&#27844;&#38706;&#20854;&#20182;&#29992;&#25143;&#30340;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19600</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#19979;&#30340;&#24322;&#26500;&#25968;&#25454;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Learning on Heterogeneous Data via Adaptive Self-Distillation. (arXiv:2305.19600v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#65292;&#35813;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#30446;&#21069;&#27969;&#34892;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#23427;&#20351;&#24471;&#23458;&#25143;&#26426;&#21487;&#20197;&#32858;&#21512;&#26412;&#22320;&#35757;&#32451;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#20219;&#20309;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#20174;&#32780;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23454;&#36341;&#20013;&#21457;&#29616;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#35266;&#23519;&#21040;&#30340;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#22343;&#21248;&#24615;&#65288;&#20363;&#22914;&#31867;&#21035;&#19981;&#24179;&#34913;&#65289;&#12290;&#22312;&#36825;&#31181;&#19981;&#22343;&#21248;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#32852;&#37030;&#23398;&#20064;&#20250;&#20986;&#29616;&#8220;&#23458;&#25143;&#26426;&#28418;&#31227;&#8221;&#38382;&#39064;&#65292;&#23548;&#33268;&#27599;&#20010;&#23458;&#25143;&#31471;&#25910;&#25947;&#21040;&#20854;&#33258;&#24049;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#36825;&#20250;&#38477;&#20302;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#24182;&#38477;&#20302;&#27169;&#22411;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#33258;&#33976;&#39311;&#30340;&#26032;&#22411;&#27491;&#21017;&#21270;&#25216;&#26415;&#26469;&#35757;&#32451;&#23458;&#25143;&#31471;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#22522;&#20110;&#23458;&#25143;&#31471;&#26412;&#22320;&#27169;&#22411;&#39044;&#27979;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#30456;&#20284;&#24615;&#20197;&#21450;&#23458;&#25143;&#31471;&#30340;&#26631;&#31614;&#20998;&#24067;&#26469;&#33258;&#36866;&#24212;&#22320;&#35843;&#25972;&#23458;&#25143;&#31471;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#27491;&#21017;&#21270;&#25216;&#26415;&#21487;&#20197;&#36731;&#26494;&#22320;&#38598;&#25104;&#22312;&#29616;&#26377;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20043;&#19978;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#23458;&#25143;&#31471;&#25110;&#26381;&#21153;&#22120;&#20195;&#30721;&#36827;&#34892;&#20219;&#20309;&#26356;&#25913;&#65292;&#22240;&#27492;&#20855;&#26377;&#39640;&#24230;&#30340;&#21487;&#37096;&#32626;&#24615;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#19979;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning paradigm that enables clients to jointly train a global model by aggregating the locally trained models without sharing any local training data. In practice, there can often be substantial heterogeneity (e.g., class imbalance) across the local data distributions observed by each of these clients. Under such non-iid data distributions across clients, FL suffers from the 'client-drift' problem where every client converges to its own local optimum. This results in slower convergence and poor performance of the aggregated model. To address this limitation, we propose a novel regularization technique based on adaptive self-distillation (ASD) for training models on the client side. Our regularization scheme adaptively adjusts to the client's training data based on: (1) the closeness of the local model's predictions with that of the global model and (2) the client's label distribution. The proposed regularization can be easily integrated atop exis
&lt;/p&gt;</description></item></channel></rss>