<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;</title><link>https://arxiv.org/abs/2403.10444</link><description>&lt;p&gt;
&#29992;&#20110;&#21152;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#26368;&#20339;&#22359;&#32423;&#33609;&#31295;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Optimal Block-Level Draft Verification for Accelerating Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10444
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#22909;&#30340;&#33609;&#31295;&#39564;&#35777;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39069;&#22806;&#30340;&#22681;&#38047;&#36895;&#24230;&#25552;&#21319;&#65292;&#32780;&#19981;&#22686;&#21152;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#33609;&#31295;&#26631;&#35760;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#24050;&#34987;&#35777;&#26126;&#26159;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26080;&#25439;&#21152;&#36895;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290; &#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#31639;&#27861;&#39318;&#20808;&#20351;&#29992;&#19968;&#20010;&#36739;&#23567;&#30340;&#27169;&#22411;&#36215;&#33609;&#19968;&#22359;&#26631;&#35760;&#12290;&#36825;&#20123;&#26631;&#35760;&#28982;&#21518;&#30001;&#22823;&#22411;&#27169;&#22411;&#24182;&#34892;&#39564;&#35777;&#65292;&#21482;&#26377;&#19968;&#37096;&#20998;&#26631;&#35760;&#23558;&#34987;&#20445;&#30041;&#65292;&#20197;&#30830;&#20445;&#26368;&#32456;&#36755;&#20986;&#36981;&#24490;&#22823;&#22411;&#27169;&#22411;&#30340;&#20998;&#24067;&#12290; &#22312;&#20197;&#24448;&#30340;&#25152;&#26377;&#25512;&#27979;&#35299;&#30721;&#24037;&#20316;&#20013;&#65292;&#36215;&#33609;&#39564;&#35777;&#26159;&#29420;&#31435;&#22320;&#36880;&#20010;&#26631;&#35760;&#25191;&#34892;&#30340;&#12290; &#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#22909;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#21487;&#25552;&#20379;&#39069;&#22806;&#30340;&#22681;&#38047;&#21152;&#36895;&#65292;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#21644;&#36215;&#33609;&#26631;&#35760;&#12290; &#25105;&#20204;&#39318;&#20808;&#23558;&#36215;&#33609;&#39564;&#35777;&#27493;&#39588;&#21046;&#23450;&#20026;&#19968;&#20010;&#22359;&#32423;&#26368;&#20248;&#20256;&#36755;&#38382;&#39064;&#12290; &#22359;&#32423;&#21046;&#23450;&#20801;&#35768;&#25105;&#20204;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#36215;&#33609;&#39564;&#35777;&#31639;&#27861;&#65292;&#24182;&#22312;&#19968;&#20010;&#36215;&#33609;&#20013;&#39044;&#26399;&#33719;&#24471;&#26356;&#22810;&#25509;&#21463;&#30340;&#26631;&#35760;&#25968;&#37327;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10444v1 Announce Type: cross  Abstract: Speculative decoding has shown to be an effective method for lossless acceleration of large language models (LLMs) during inference. In each iteration, the algorithm first uses a smaller model to draft a block of tokens. The tokens are then verified by the large model in parallel and only a subset of tokens will be kept to guarantee that the final output follows the distribution of the large model. In all of the prior speculative decoding works, the draft verification is performed token-by-token independently. In this work, we propose a better draft verification algorithm that provides additional wall-clock speedup without incurring additional computation cost and draft tokens. We first formulate the draft verification step as a block-level optimal transport problem. The block-level formulation allows us to consider a wider range of draft verification algorithms and obtain a higher number of accepted tokens in expectation in one draft 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;KRR&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#21576;&#25351;&#25968;&#24555;&#36895;&#34928;&#20943;&#26102;&#65292;KRR&#23454;&#29616;&#20102;&#35889;&#31934;&#24230;&#12290;&#23545;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#20102;&#23545;&#20598;&#20998;&#26512;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#25193;&#23637;&#30340;&#23545;&#20598;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#36229;&#20986;&#26412;&#24037;&#20316;&#33539;&#22260;&#30340;&#26680;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.15718</link><description>&lt;p&gt;
&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#23545;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#23545;&#20598;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Duality Analysis of Kernel Ridge Regression in the Noiseless Regime
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;KRR&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#29305;&#21035;&#26159;&#22312;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#21576;&#25351;&#25968;&#24555;&#36895;&#34928;&#20943;&#26102;&#65292;KRR&#23454;&#29616;&#20102;&#35889;&#31934;&#24230;&#12290;&#23545;&#26680;&#23725;&#22238;&#24402;&#36827;&#34892;&#20102;&#23545;&#20598;&#20998;&#26512;&#65292;&#21033;&#29992;&#20102;&#19968;&#31181;&#26032;&#22411;&#25193;&#23637;&#30340;&#23545;&#20598;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#36229;&#20986;&#26412;&#24037;&#20316;&#33539;&#22260;&#30340;&#26680;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#22312;&#26080;&#22122;&#22768;&#24773;&#20917;&#19979;&#26680;&#23725;&#22238;&#24402;&#65288;KRR&#65289;&#30340;&#27867;&#21270;&#29305;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#36825;&#23545;&#20110;&#31185;&#23398;&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#25968;&#25454;&#32463;&#24120;&#26159;&#36890;&#36807;&#35745;&#31639;&#26426;&#27169;&#25311;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;KRR&#21487;&#20197;&#36798;&#21040;&#26368;&#23567;&#21270;&#26368;&#20248;&#29575;&#65292;&#36825;&#21462;&#20915;&#20110;&#30456;&#20851;&#26680;&#30340;&#29305;&#24449;&#20540;&#34928;&#20943;&#21644;&#30446;&#26631;&#20989;&#25968;&#30340;&#30456;&#23545;&#24179;&#28369;&#31243;&#24230;&#12290;&#29305;&#21035;&#26159;&#65292;&#24403;&#29305;&#24449;&#20540;&#30340;&#34928;&#20943;&#21576;&#25351;&#25968;&#24555;&#36895;&#34928;&#20943;&#26102;&#65292;KRR&#23454;&#29616;&#20102;&#35889;&#31934;&#24230;&#65292;&#21363;&#25910;&#25947;&#36895;&#24230;&#24555;&#20110;&#20219;&#20309;&#22810;&#39033;&#24335;&#12290;&#27492;&#22806;&#65292;&#25968;&#20540;&#23454;&#39564;&#24456;&#22909;&#22320;&#35777;&#23454;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#21033;&#29992;&#20102;&#38472;&#31561;&#20154;&#65288;2023&#24180;&#65289;&#24341;&#20837;&#30340;&#23545;&#20598;&#26694;&#26550;&#30340;&#19968;&#31181;&#26032;&#22411;&#25193;&#23637;&#65292;&#36825;&#23545;&#20998;&#26512;&#36229;&#20986;&#26412;&#24037;&#20316;&#33539;&#22260;&#30340;&#22522;&#20110;&#26680;&#30340;&#26041;&#27861;&#21487;&#33021;&#24456;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15718v1 Announce Type: new  Abstract: In this paper, we conduct a comprehensive analysis of generalization properties of Kernel Ridge Regression (KRR) in the noiseless regime, a scenario crucial to scientific computing, where data are often generated via computer simulations. We prove that KRR can attain the minimax optimal rate, which depends on both the eigenvalue decay of the associated kernel and the relative smoothness of target functions. Particularly, when the eigenvalue decays exponentially fast, KRR achieves the spectral accuracy, i.e., a convergence rate faster than any polynomial. Moreover, the numerical experiments well corroborate our theoretical findings. Our proof leverages a novel extension of the duality framework introduced by Chen et al. (2023), which could be useful in analyzing kernel-based methods beyond the scope of this work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;Moreau&#21253;&#32476;&#26469;&#23545;&#27979;&#24230;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;Wasserstein&#26799;&#24230;&#27969;&#12290;</title><link>https://arxiv.org/abs/2402.04613</link><description>&lt;p&gt;
&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;Moreau&#21253;&#32476;&#30340;f-&#24046;&#24322;&#30340;Wasserstein&#26799;&#24230;&#27969;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Gradient Flows for Moreau Envelopes of f-Divergences in Reproducing Kernel Hilbert Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#20351;&#29992;Moreau&#21253;&#32476;&#26469;&#23545;&#27979;&#24230;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#35813;&#26041;&#27861;&#20998;&#26512;&#20102;Wasserstein&#26799;&#24230;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#24120;&#29992;&#30340;&#27979;&#24230;f-&#24046;&#24322;&#65292;&#20363;&#22914;Kullback-Leibler&#24046;&#24322;&#65292;&#23545;&#20110;&#25152;&#28041;&#21450;&#30340;&#27979;&#24230;&#30340;&#25903;&#25345;&#23384;&#22312;&#38480;&#21046;&#12290;&#35299;&#20915;&#21150;&#27861;&#26159;&#36890;&#36807;&#19982;&#29305;&#24449;&#26680;K&#30456;&#20851;&#30340;&#24179;&#26041;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;(MMD)&#23545;f-&#24046;&#24322;&#36827;&#34892;&#27491;&#21017;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#25152;&#35859;&#30340;&#26680;&#22343;&#20540;&#23884;&#20837;&#26469;&#26174;&#31034;&#30456;&#24212;&#30340;&#27491;&#21017;&#21270;&#21487;&#20197;&#37325;&#20889;&#20026;&#19982;K&#30456;&#20851;&#30340;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#26576;&#20123;&#20989;&#25968;&#30340;Moreau&#21253;&#32476;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#20851;&#20110;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;Moreau&#21253;&#32476;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#32467;&#26524;&#26469;&#35777;&#26126;MMD&#27491;&#21017;&#21270;&#30340;f-&#24046;&#24322;&#21450;&#20854;&#26799;&#24230;&#30340;&#23646;&#24615;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26469;&#20998;&#26512;&#21463;MMD&#27491;&#21017;&#21270;&#30340;f-&#24046;&#24322;&#30340;Wasserstein&#26799;&#24230;&#27969;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#32463;&#39564;&#27979;&#24230;&#24320;&#22987;&#30340;Wasserstein&#26799;&#24230;&#27969;&#65292;&#24182;&#25552;&#20379;&#20351;&#29992;Tsallis-$\alpha$&#24046;&#24322;&#30340;&#27010;&#24565;&#24615;&#25968;&#20540;&#31034;&#20363;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most commonly used $f$-divergences of measures, e.g., the Kullback-Leibler divergence, are subject to limitations regarding the support of the involved measures. A remedy consists of regularizing the $f$-divergence by a squared maximum mean discrepancy (MMD) associated with a characteristic kernel $K$. In this paper, we use the so-called kernel mean embedding to show that the corresponding regularization can be rewritten as the Moreau envelope of some function in the reproducing kernel Hilbert space associated with $K$. Then, we exploit well-known results on Moreau envelopes in Hilbert spaces to prove properties of the MMD-regularized $f$-divergences and, in particular, their gradients. Subsequently, we use our findings to analyze Wasserstein gradient flows of MMD-regularized $f$-divergences. Finally, we consider Wasserstein gradient flows starting from empirical measures and provide proof-of-the-concept numerical examples with Tsallis-$\alpha$ divergences.
&lt;/p&gt;</description></item></channel></rss>