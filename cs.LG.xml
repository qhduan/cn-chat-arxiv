<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#35770;&#23478;&#30340;&#22343;&#26041;&#35823;&#24046;&#19978;&#30028;&#20026;$\epsilon$&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.08})$&#65292;&#20248;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01371</link><description>&lt;p&gt;
Critic-Actor&#31639;&#27861;&#22312;&#24179;&#22343;&#22870;&#21169;MDPs&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65306;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Critic-Actor for Average Reward MDPs with Function Approximation: A Finite-Time Analysis
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#20013;&#30340;&#20989;&#25968;&#36924;&#36817;&#38382;&#39064;&#65292;&#24182;&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#22312;&#35780;&#35770;&#23478;&#30340;&#22343;&#26041;&#35823;&#24046;&#19978;&#30028;&#20026;$\epsilon$&#30340;&#24773;&#20917;&#19979;&#65292;&#33719;&#24471;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.08})$&#65292;&#20248;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20851;&#20110;&#20004;&#20010;&#26102;&#38388;&#23610;&#24230;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#28176;&#36817;&#21644;&#38750;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#30340;&#30740;&#31350;&#24037;&#20316;&#38750;&#24120;&#27963;&#36291;&#65292;&#20854;&#20013;&#28436;&#21592;&#30340;&#26356;&#26032;&#36895;&#24230;&#27604;&#35780;&#35770;&#23478;&#24930;&#12290;&#22312;&#26368;&#36817;&#30340;&#19968;&#39033;&#24037;&#20316;&#20013;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#25240;&#25187;&#25104;&#26412;&#35774;&#32622;&#20013;&#30340;&#26597;&#25214;&#34920;&#24773;&#20917;&#65292;&#20854;&#20013;&#28436;&#21592;&#21644;&#35780;&#35770;&#23478;&#30340;&#26102;&#38388;&#23610;&#24230;&#30456;&#21453;&#65292;&#24182;&#32473;&#20986;&#20102;&#28176;&#36817;&#25910;&#25947;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20989;&#25968;&#36924;&#36817;&#30340;&#35780;&#35770;&#23478;-&#28436;&#21592;&#31639;&#27861;&#65292;&#24182;&#22312;&#38271;&#26399;&#24179;&#22343;&#22870;&#21169;&#35774;&#32622;&#20013;&#36827;&#34892;&#20102;&#39318;&#27425;&#26377;&#38480;&#26102;&#38388;&#65288;&#38750;&#28176;&#36817;&#65289;&#20998;&#26512;&#12290;&#25105;&#20204;&#24471;&#21040;&#20102;&#26368;&#20248;&#30340;&#23398;&#20064;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20174;&#35780;&#35770;&#23478;&#30340;&#22343;&#26041;&#35823;&#24046;&#19978;&#30028;&#20026;$\epsilon$&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026;$\mathcal{\tilde{O}}(\epsilon^{-2.08})$&#65292;&#27492;&#32467;&#26524;&#27604;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#33719;&#24471;&#30340;&#32467;&#26524;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, there has been a lot of research work activity focused on carrying out asymptotic and non-asymptotic convergence analyses for two-timescale actor critic algorithms where the actor updates are performed on a timescale that is slower than that of the critic. In a recent work, the critic-actor algorithm has been presented for the infinite horizon discounted cost setting in the look-up table case where the timescales of the actor and the critic are reversed and asymptotic convergence analysis has been presented. In our work, we present the first critic-actor algorithm with function approximation and in the long-run average reward setting and present the first finite-time (non-asymptotic) analysis of such a scheme. We obtain optimal learning rates and prove that our algorithm achieves a sample complexity of $\mathcal{\tilde{O}}(\epsilon^{-2.08})$ for the mean squared error of the critic to be upper bounded by $\epsilon$ which is better than the one obtained for actor-critic
&lt;/p&gt;</description></item><item><title>PyTorch Frame&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#30340;PyTorch&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#25277;&#35937;&#21644;&#22806;&#37096;&#22522;&#30784;&#27169;&#22411;&#25972;&#21512;&#31561;&#21151;&#33021;&#65292;&#23454;&#29616;&#20102;&#27169;&#22359;&#21270;&#30340;&#34920;&#26684;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#25104;&#21151;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2404.00776</link><description>&lt;p&gt;
PyTorch Frame: &#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#34920;&#26684;&#23398;&#20064;&#30340;&#27169;&#22359;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PyTorch Frame: A Modular Framework for Multi-Modal Tabular Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00776
&lt;/p&gt;
&lt;p&gt;
PyTorch Frame&#26159;&#19968;&#20010;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#30340;PyTorch&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#20379;&#25968;&#25454;&#32467;&#26500;&#12289;&#27169;&#22411;&#25277;&#35937;&#21644;&#22806;&#37096;&#22522;&#30784;&#27169;&#22411;&#25972;&#21512;&#31561;&#21151;&#33021;&#65292;&#23454;&#29616;&#20102;&#27169;&#22359;&#21270;&#30340;&#34920;&#26684;&#27169;&#22411;&#23454;&#29616;&#65292;&#24182;&#25104;&#21151;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PyTorch Frame&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22788;&#29702;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#12290;PyTorch Frame&#36890;&#36807;&#25552;&#20379;&#22522;&#20110;PyTorch&#30340;&#25968;&#25454;&#32467;&#26500;&#26469;&#22788;&#29702;&#22797;&#26434;&#30340;&#34920;&#26684;&#25968;&#25454;&#65292;&#24341;&#20837;&#27169;&#22411;&#25277;&#35937;&#20197;&#23454;&#29616;&#34920;&#26684;&#27169;&#22411;&#30340;&#27169;&#22359;&#21270;&#23454;&#29616;&#65292;&#24182;&#20801;&#35768;&#25972;&#21512;&#22806;&#37096;&#22522;&#30784;&#27169;&#22411;&#26469;&#22788;&#29702;&#22797;&#26434;&#21015;&#65288;&#20363;&#22914;&#65292;&#29992;&#20110;&#25991;&#26412;&#21015;&#30340;LLMs&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#20197;&#27169;&#22359;&#21270;&#26041;&#24335;&#23454;&#29616;&#22810;&#26679;&#30340;&#34920;&#26684;&#27169;&#22411;&#65292;&#25104;&#21151;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#34920;&#26684;&#25968;&#25454;&#65292;&#24182;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#19982;PyTorch Geometric&#38598;&#25104;&#65292;PyTorch Geometric&#26159;&#19968;&#20010;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;PyTorch&#24211;&#65292;&#20197;&#23454;&#29616;&#23545;&#20851;&#31995;&#25968;&#25454;&#24211;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00776v1 Announce Type: new  Abstract: We present PyTorch Frame, a PyTorch-based framework for deep learning over multi-modal tabular data. PyTorch Frame makes tabular deep learning easy by providing a PyTorch-based data structure to handle complex tabular data, introducing a model abstraction to enable modular implementation of tabular models, and allowing external foundation models to be incorporated to handle complex columns (e.g., LLMs for text columns). We demonstrate the usefulness of PyTorch Frame by implementing diverse tabular models in a modular way, successfully applying these models to complex multi-modal tabular data, and integrating our framework with PyTorch Geometric, a PyTorch library for Graph Neural Networks (GNNs), to perform end-to-end learning over relational databases.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#26041;&#21521;&#24863;&#30693;&#25439;&#22833;&#39033;&#65292;&#24378;&#35843;&#25968;&#25454;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#24341;&#23548;&#20248;&#21270;&#21644;&#32467;&#26524;&#23884;&#20837;&#20197;&#23637;&#31034;&#21487;&#33021;&#34987;&#24573;&#30053;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.19040</link><description>&lt;p&gt;
&#20351;&#29992;&#26041;&#21521;&#24863;&#30693;t-SNE&#21487;&#35270;&#21270;&#39640;&#32500;&#26102;&#38388;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Visualizing High-Dimensional Temporal Data Using Direction-Aware t-SNE
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19040
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#26041;&#21521;&#24863;&#30693;&#25439;&#22833;&#39033;&#65292;&#24378;&#35843;&#25968;&#25454;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#24341;&#23548;&#20248;&#21270;&#21644;&#32467;&#26524;&#23884;&#20837;&#20197;&#23637;&#31034;&#21487;&#33021;&#34987;&#24573;&#30053;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21253;&#21547;&#26102;&#38388;&#32452;&#20214;&#25110;&#28041;&#21450;&#20174;&#19968;&#20010;&#29366;&#24577;&#21040;&#21478;&#19968;&#20010;&#29366;&#24577;&#30340;&#36716;&#21464;&#12290;&#20026;&#20102;&#36827;&#34892;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#36825;&#20123;&#39640;&#32500;&#25968;&#25454;&#38598;&#34920;&#31034;&#20026;&#20108;&#32500;&#22320;&#22270;&#65292;&#20351;&#29992;&#25968;&#25454;&#23545;&#35937;&#30340;&#23884;&#20837;&#36827;&#34892;&#25506;&#32034;&#65292;&#24182;&#29992;&#26377;&#21521;&#36793;&#34920;&#31034;&#23427;&#20204;&#30340;&#26102;&#38388;&#20851;&#31995;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38477;&#32500;&#25216;&#26415;&#65292;&#22914;t-SNE&#21644;UMAP&#65292;&#22312;&#26500;&#24314;&#23884;&#20837;&#26102;&#26410;&#32771;&#34385;&#25968;&#25454;&#30340;&#26102;&#38388;&#24615;&#25110;&#20851;&#31995;&#24615;&#65292;&#23548;&#33268;&#26102;&#38388;&#19978;&#26434;&#20081;&#30340;&#21487;&#35270;&#21270;&#65292;&#20351;&#28508;&#22312;&#26377;&#36259;&#30340;&#27169;&#24335;&#21464;&#24471;&#27169;&#31946;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;t-SNE&#30340;&#20248;&#21270;&#20989;&#25968;&#20013;&#25552;&#20986;&#20102;&#20004;&#20010;&#20114;&#34917;&#30340;&#26041;&#21521;&#24863;&#30693;&#25439;&#22833;&#39033;&#65292;&#24378;&#35843;&#25968;&#25454;&#30340;&#26102;&#38388;&#26041;&#38754;&#65292;&#24341;&#23548;&#20248;&#21270;&#21644;&#32467;&#26524;&#23884;&#20837;&#20197;&#23637;&#31034;&#21487;&#33021;&#34987;&#24573;&#30053;&#30340;&#26102;&#38388;&#27169;&#24335;&#12290;&#23450;&#21521;&#19968;&#33268;&#25439;&#22833;&#65288;DCL&#65289;&#40723;&#21169;&#38468;&#36817;&#30340;&#31661;&#22836;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19040v1 Announce Type: new  Abstract: Many real-world data sets contain a temporal component or involve transitions from state to state. For exploratory data analysis, we can represent these high-dimensional data sets in two-dimensional maps, using embeddings of the data objects under exploration and representing their temporal relationships with directed edges. Most existing dimensionality reduction techniques, such as t-SNE and UMAP, do not take into account the temporal or relational nature of the data when constructing the embeddings, resulting in temporally cluttered visualizations that obscure potentially interesting patterns. To address this problem, we propose two complementary, direction-aware loss terms in the optimization function of t-SNE that emphasize the temporal aspects of the data, guiding the optimization and the resulting embedding to reveal temporal patterns that might otherwise go unnoticed. The Directional Coherence Loss (DCL) encourages nearby arrows c
&lt;/p&gt;</description></item><item><title>&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.13804</link><description>&lt;p&gt;
&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#23398;&#20064;&#36827;&#34892;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Learning from Models and Data for Visual Grounding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13804
&lt;/p&gt;
&lt;p&gt;
&#32467;&#21512;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#27169;&#22411;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20248;&#21270;&#19968;&#33268;&#24615;&#30446;&#26631;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;SynGround&#65292;&#36825;&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#25968;&#25454;&#39537;&#21160;&#23398;&#20064;&#21644;&#20174;&#21508;&#31181;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#36827;&#34892;&#30693;&#35782;&#20256;&#36882;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#39044;&#35757;&#32451;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#23450;&#20301;&#33021;&#21147;&#12290;&#20174;&#27169;&#22411;&#20013;&#36827;&#34892;&#30340;&#30693;&#35782;&#20256;&#36882;&#24341;&#21457;&#20102;&#36890;&#36807;&#22270;&#20687;&#25551;&#36848;&#29983;&#25104;&#22120;&#29983;&#25104;&#22270;&#20687;&#25551;&#36848;&#12290;&#36825;&#20123;&#25551;&#36848;&#20855;&#26377;&#21452;&#37325;&#20316;&#29992;&#65306;&#23427;&#20204;&#20316;&#20026;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#22120;&#21512;&#25104;&#22270;&#20687;&#30340;&#25552;&#31034;&#65292;&#20197;&#21450;&#20316;&#20026;&#26597;&#35810;&#26469;&#21512;&#25104;&#25991;&#26412;&#65292;&#20174;&#20854;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30701;&#35821;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#24320;&#25918;&#35789;&#27719;&#30340;&#23545;&#35937;&#26816;&#27979;&#22120;&#20026;&#21512;&#25104;&#22270;&#20687;&#21644;&#25991;&#26412;&#29983;&#25104;&#21512;&#25104;&#36793;&#30028;&#26694;&#12290;&#36890;&#36807;&#20248;&#21270;&#19968;&#20010;&#36974;&#32617;-&#27880;&#24847;&#21147;&#19968;&#33268;&#24615;&#30446;&#26631;&#65292;&#22312;&#36825;&#20010;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#30446;&#26631;&#23558;&#21306;&#22495;&#27880;&#37322;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#27169;&#22411;&#35299;&#37322;&#36827;&#34892;&#23545;&#40784;&#12290;&#26368;&#32456;&#30340;&#27169;&#22411;&#25552;&#21319;&#20102;&#23450;&#20301;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13804v1 Announce Type: cross  Abstract: We introduce SynGround, a novel framework that combines data-driven learning and knowledge transfer from various large-scale pretrained models to enhance the visual grounding capabilities of a pretrained vision-and-language model. The knowledge transfer from the models initiates the generation of image descriptions through an image description generator. These descriptions serve dual purposes: they act as prompts for synthesizing images through a text-to-image generator, and as queries for synthesizing text, from which phrases are extracted using a large language model. Finally, we leverage an open-vocabulary object detector to generate synthetic bounding boxes for the synthetic images and texts. We finetune a pretrained vision-and-language model on this dataset by optimizing a mask-attention consistency objective that aligns region annotations with gradient-based model explanations. The resulting model improves the grounding capabilit
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.11353</link><description>&lt;p&gt;
&#28342;&#21058;&#24863;&#30693;&#30340;2D&#26680;&#30913;&#20849;&#25391;&#39044;&#27979;&#65306;&#21033;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#36845;&#20195;&#33258;&#35757;&#32451;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Solvent-Aware 2D NMR Prediction: Leveraging Multi-Tasking Training and Iterative Self-Training Strategies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11353
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#36845;&#20195;&#33258;&#25105;&#35757;&#32451;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20174;&#32780;&#35299;&#20915;&#20108;&#32500;&#26680;&#30913;&#20849;&#25391;&#65288;2D NMR&#65289;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#65292;&#24357;&#34917;&#20102;&#32570;&#20047;&#26631;&#27880;NMR&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26680;&#30913;&#20849;&#25391;&#65288;NMR&#65289;&#20809;&#35889;&#22312;&#21508;&#20010;&#31185;&#23398;&#39046;&#22495;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#20998;&#23376;&#30340;&#32467;&#26500;&#20449;&#24687;&#12289;&#30005;&#23376;&#24615;&#36136;&#21644;&#21160;&#24577;&#34892;&#20026;&#30340;&#35265;&#35299;&#12290;&#20934;&#30830;&#30340;NMR&#20809;&#35889;&#39044;&#27979;&#33021;&#22815;&#39640;&#25928;&#22320;&#29983;&#25104;&#20505;&#36873;&#20998;&#23376;&#65292;&#20351;&#21270;&#23398;&#23478;&#33021;&#22815;&#23558;&#23427;&#20204;&#19982;&#23454;&#38469;&#23454;&#39564;&#20809;&#35889;&#36827;&#34892;&#27604;&#36739;&#12290;&#35813;&#36807;&#31243;&#26377;&#21161;&#20110;&#30830;&#35748;&#20998;&#23376;&#32467;&#26500;&#25110;&#25351;&#20986;&#24046;&#24322;&#65292;&#24341;&#23548;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#12290;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#20998;&#23376;&#32467;&#26500;&#39044;&#27979;&#20998;&#23376;&#30340;&#21407;&#23376;NMR&#21270;&#23398;&#20301;&#31227;&#12290;&#34429;&#28982;&#22312;&#39044;&#27979;&#19968;&#32500;&#65288;1D&#65289;NMR&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#20108;&#32500;&#65288;2D&#65289;NMR&#39044;&#27979;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#65292;&#22240;&#20026;&#32570;&#20047;&#29992;&#20110;&#35757;&#32451;&#30340;&#26631;&#27880;&#30340;NMR&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#33258;&#35757;&#32451;&#65288;IST&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#39044;&#27979;&#21407;&#23376;2DNMR&#20301;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11353v1 Announce Type: cross  Abstract: Nuclear magnetic resonance (NMR) spectroscopy plays a pivotal role in various scientific fields, offering insights into structural information, electronic properties and dynamic behaviors of molecules. Accurate NMR spectrum prediction efficiently produces candidate molecules, enabling chemists to compare them with actual experimental spectra. This process aids in confirming molecular structures or pinpointing discrepancies, guiding further investigation. Machine Learning (ML) has then emerged as a promising alternative approach for predicting atomic NMR chemical shits of molecules given their structures. Although significant progresses have been made in predicting one-dimensional (1D) NMR, two-dimensional (2D) NMR prediction via ML remains a challenge due to the lack of annotated NMR training datasets. To address this gap, we propose an iterative self-training (IST) approach to train a deep learning model for predicting atomic 2DNMR sh
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;</title><link>https://arxiv.org/abs/2403.09919</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#29992;&#20110;&#24555;&#36895;&#25512;&#27979;&#35299;&#30721;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Recurrent Drafter for Fast Speculative Decoding in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24490;&#29615;&#33609;&#31295;&#26426;&#21046;&#65292;&#32467;&#21512;&#20102;&#32463;&#20856;&#21452;&#27169;&#22411;&#21644;&#26368;&#26032;&#21333;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#36816;&#29992;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#25512;&#27979;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#25913;&#36827;&#30340;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#20004;&#31181;&#25104;&#29087;&#25216;&#26415;&#30340;&#20248;&#21183;&#65306;&#32463;&#20856;&#30340;&#21452;&#27169;&#22411;&#25512;&#27979;&#35299;&#30721;&#26041;&#27861;&#21644;&#36739;&#26032;&#30340;&#21333;&#27169;&#22411;&#26041;&#27861;Medusa&#12290;&#20174;Medusa&#24471;&#21040;&#28789;&#24863;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#37319;&#29992;&#20102;&#21333;&#27169;&#22411;&#31574;&#30053;&#36827;&#34892;&#25512;&#27979;&#35299;&#30721;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#24490;&#29615;&#20381;&#36182;&#35774;&#35745;&#30340;&#21333;&#20010;&#36731;&#37327;&#32423;&#33609;&#31295;&#22836;&#26469;&#21306;&#20998;&#33258;&#24049;&#65292;&#26412;&#36136;&#19978;&#31867;&#20284;&#20110;&#32463;&#20856;&#25512;&#27979;&#35299;&#30721;&#20013;&#20351;&#29992;&#30340;&#23567;&#22411;&#33609;&#31295;&#27169;&#22411;&#65292;&#20294;&#36991;&#20813;&#20102;&#23436;&#25972;transformer&#26550;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#30001;&#20110;&#24490;&#29615;&#20381;&#36182;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#27874;&#26463;&#25628;&#32034;&#24555;&#36895;&#36807;&#28388;&#20986;&#33609;&#31295;&#22836;&#20013;&#19981;&#38656;&#35201;&#30340;&#20505;&#36873;&#39033;&#12290;&#20854;&#32467;&#26524;&#26159;&#19968;&#31181;&#32467;&#21512;&#20102;&#21333;&#27169;&#22411;&#35774;&#35745;&#31616;&#26131;&#24615;&#24182;&#36991;&#20813;&#20102;&#21019;&#24314;&#25968;&#25454;&#30456;&#20851;&#26641;&#20381;&#36182;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09919v1 Announce Type: new  Abstract: In this paper, we introduce an improved approach of speculative decoding aimed at enhancing the efficiency of serving large language models. Our method capitalizes on the strengths of two established techniques: the classic two-model speculative decoding approach, and the more recent single-model approach, Medusa. Drawing inspiration from Medusa, our approach adopts a single-model strategy for speculative decoding. However, our method distinguishes itself by employing a single, lightweight draft head with a recurrent dependency design, akin in essence to the small, draft model uses in classic speculative decoding, but without the complexities of the full transformer architecture. And because of the recurrent dependency, we can use beam search to swiftly filter out undesired candidates with the draft head. The outcome is a method that combines the simplicity of single-model design and avoids the need to create a data-dependent tree attent
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#20013;&#39030;&#28857;&#21040;&#36798;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Jordan&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20854;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.09755</link><description>&lt;p&gt;
&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#30340;&#21382;&#21490;
&lt;/p&gt;
&lt;p&gt;
Estimating the history of a random recursive tree
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#20013;&#39030;&#28857;&#21040;&#36798;&#39034;&#24207;&#30340;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;Jordan&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20854;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20272;&#35745;&#38543;&#26426;&#36882;&#24402;&#26641;&#20013;&#39030;&#28857;&#21040;&#36798;&#39034;&#24207;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#22522;&#26412;&#27169;&#22411;&#65306;&#22343;&#21248;&#36830;&#25509;&#27169;&#22411;&#21644;&#32447;&#24615;&#20248;&#20808;&#36830;&#25509;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Jordan&#20013;&#24515;&#24615;&#24230;&#37327;&#30340;&#39034;&#24207;&#20272;&#35745;&#22120;&#65292;&#24182;&#23450;&#20041;&#20102;&#19968;&#26063;&#39118;&#38505;&#24230;&#37327;&#26469;&#37327;&#21270;&#25490;&#24207;&#36807;&#31243;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#36825;&#20010;&#38382;&#39064;&#24314;&#31435;&#20102;&#26497;&#23567;-&#26368;&#22823;&#19979;&#30028;&#65292;&#24182;&#35777;&#26126;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#20960;&#20046;&#26159;&#26368;&#20248;&#30340;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#34920;&#26126;&#25152;&#25552;&#20986;&#30340;&#20272;&#35745;&#22120;&#20248;&#20110;&#22522;&#20110;&#24230;&#25968;&#21644;&#35889;&#25490;&#24207;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09755v1 Announce Type: cross  Abstract: This paper studies the problem of estimating the order of arrival of the vertices in a random recursive tree. Specifically, we study two fundamental models: the uniform attachment model and the linear preferential attachment model. We propose an order estimator based on the Jordan centrality measure and define a family of risk measures to quantify the quality of the ordering procedure. Moreover, we establish a minimax lower bound for this problem, and prove that the proposed estimator is nearly optimal. Finally, we numerically demonstrate that the proposed estimator outperforms degree-based and spectral ordering procedures.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25361;&#25112;&#30340;&#26368;&#20248;Top-Two&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09123</link><description>&lt;p&gt;
&#26368;&#20339;&#33218;&#35782;&#21035;&#21644;&#27969;&#20307;&#20998;&#26512;&#30340;&#26368;&#20248;Top-Two&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Optimal Top-Two Method for Best Arm Identification and Fluid Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09123
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#20013;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#25361;&#25112;&#30340;&#26368;&#20248;Top-Two&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Top-2&#26041;&#27861;&#22312;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#20013;&#21464;&#24471;&#27969;&#34892;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#19968;&#20010;&#31639;&#27861;&#35782;&#21035;&#26368;&#20339;&#33218;&#65292;&#21363;&#22312;&#26377;&#38480;&#25968;&#37327;&#33218;&#20013;&#20855;&#26377;&#26368;&#22823;&#22343;&#20540;&#30340;&#33218;&#65292;&#35813;&#31639;&#27861;&#22312;&#20219;&#20309;&#39034;&#24207;&#27493;&#39588;&#20013;&#29420;&#31435;&#22320;&#20197;&#22266;&#23450;&#27010;&#29575; &#946; &#25289;&#21160;&#32463;&#39564;&#26368;&#20339;&#33218;&#65292;&#24182;&#22312;&#20854;&#20182;&#24773;&#20917;&#19979;&#25289;&#21160;&#26368;&#20339;&#25361;&#25112;&#32773;&#33218;&#12290;&#36873;&#25321;&#38169;&#35823;&#30340;&#27010;&#29575;&#20445;&#35777;&#22312;&#25351;&#23450;&#30340;&#948; &gt;0&#20197;&#19979;&#12290;&#23545;&#20110;BAI&#38382;&#39064;&#65292;&#24050;&#30693;&#20449;&#24687;&#29702;&#35770;&#19979;&#30028;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#24182;&#22312;&#948; &#8594; 0&#26102;&#19982;&#35745;&#31639;&#35201;&#27714;&#39640;&#30340;&#25554;&#20214;&#26041;&#27861;&#28176;&#36817;&#21305;&#37197;&#12290; &#23545;&#20110;&#20219;&#20309; &#946; &#8712;&#65288;0,1&#65289;&#30340;&#19978;&#36848;Top 2&#31639;&#27861;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#22987;&#32456;&#20445;&#25345;&#22312;&#19979;&#30028;&#30340;&#24120;&#25968;&#33539;&#22260;&#20869;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#19982;&#19979;&#30028;&#21305;&#37197;&#30340;&#26368;&#20339; &#946; &#24050;&#34987;&#35777;&#26126;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#20248;&#30340;Top-2&#31867;&#22411;&#31639;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20998;&#37197;&#38170;&#28857;&#30340;&#19968;&#20010;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09123v1 Announce Type: new  Abstract: Top-$2$ methods have become popular in solving the best arm identification (BAI) problem. The best arm, or the arm with the largest mean amongst finitely many, is identified through an algorithm that at any sequential step independently pulls the empirical best arm, with a fixed probability $\beta$, and pulls the best challenger arm otherwise. The probability of incorrect selection is guaranteed to lie below a specified $\delta &gt;0$. Information theoretic lower bounds on sample complexity are well known for BAI problem and are matched asymptotically as $\delta \rightarrow 0$ by computationally demanding plug-in methods. The above top 2 algorithm for any $\beta \in (0,1)$ has sample complexity within a constant of the lower bound. However, determining the optimal $\beta$ that matches the lower bound has proven difficult. In this paper, we address this and propose an optimal top-2 type algorithm. We consider a function of allocations anchor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.07573</link><description>&lt;p&gt;
&#36808;&#21521;&#20855;&#26377;&#21487;&#36866;&#24212;&#24615;&#35745;&#31639;&#21644;&#32593;&#32476;&#34701;&#21512;&#30340;&#21160;&#24577;&#26410;&#26469;&#65288;ACNC&#65289;
&lt;/p&gt;
&lt;p&gt;
Towards a Dynamic Future with Adaptable Computing and Network Convergence (ACNC)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25512;&#36827;6G&#30340;&#32972;&#26223;&#19979;&#65292;&#39044;&#35745;&#20250;&#20986;&#29616;&#23454;&#36136;&#24615;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#31361;&#20986;&#20102;&#30001;&#22823;&#37327;&#36830;&#25509;&#21644;&#20005;&#26684;&#36981;&#23432;&#26381;&#21153;&#36136;&#37327;/&#20307;&#39564;&#65288;QoS/E&#65289;&#20808;&#20915;&#26465;&#20214;&#25152;&#29305;&#24449;&#21270;&#30340;&#20840;&#38754;&#30340;&#19968;&#20999;&#23545;&#19968;&#20999;&#20132;&#20114;&#12290;&#21363;&#23558;&#38754;&#20020;&#30340;&#25361;&#25112;&#28304;&#20110;&#36164;&#28304;&#31232;&#32570;&#65292;&#20419;&#20351;&#26377;&#24847;&#35782;&#22320;&#21521;&#35745;&#31639;-&#32593;&#32476;&#34701;&#21512;&#65288;CNC&#65289;&#36807;&#28193;&#65292;&#20316;&#20026;&#32852;&#21512;&#36164;&#28304;&#32534;&#25490;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#34429;&#28982;&#22522;&#20110;CNC&#30340;&#26426;&#21046;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#20204;&#22312;&#23454;&#29616;&#26410;&#26469;&#26381;&#21153;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#31867;&#20284;Metaverse&#30340;&#20351;&#29992;&#24773;&#26223;&#20013;&#65292;&#21487;&#33021;&#20250;&#30001;&#20110;&#29992;&#25143;&#12289;&#26381;&#21153;&#21644;&#36164;&#28304;&#19981;&#26029;&#21464;&#21270;&#30340;&#29305;&#24615;&#32780;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21487;&#36866;&#24212;&#24615;CNC&#65288;ACNC&#65289;&#30340;&#27010;&#24565;&#65292;&#20316;&#20026;&#19968;&#31181;&#33258;&#20027;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#36741;&#21161;&#26426;&#21046;&#65292;&#26088;&#22312;&#32852;&#21512;&#32534;&#25490;&#35745;&#31639;&#21644;&#32593;&#32476;&#36164;&#28304;&#65292;&#28385;&#36275;&#23545;&#21160;&#24577;&#21644;&#22823;&#37327;&#29992;&#25143;&#35831;&#27714;&#30340;&#20005;&#26684;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07573v1 Announce Type: cross  Abstract: In the context of advancing 6G, a substantial paradigm shift is anticipated, highlighting comprehensive everything-to-everything interactions characterized by numerous connections and stringent adherence to Quality of Service/Experience (QoS/E) prerequisites. The imminent challenge stems from resource scarcity, prompting a deliberate transition to Computing-Network Convergence (CNC) as an auspicious approach for joint resource orchestration. While CNC-based mechanisms have garnered attention, their effectiveness in realizing future services, particularly in use cases like the Metaverse, may encounter limitations due to the continually changing nature of users, services, and resources. Hence, this paper presents the concept of Adaptable CNC (ACNC) as an autonomous Machine Learning (ML)-aided mechanism crafted for the joint orchestration of computing and network resources, catering to dynamic and voluminous user requests with stringent r
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02780</link><description>&lt;p&gt;
&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data Collaboration Analysis Over Matrix Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#25913;&#36827;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#24535;&#30528;&#20248;&#36234;&#30340;&#36136;&#37327;&#65292;&#22686;&#24378;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#20197;&#20943;&#36731;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#21644;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24191;&#27867;&#30340;&#25968;&#25454;&#34701;&#21512;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#36947;&#24503;&#20851;&#20999;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#29992;&#25143;&#38544;&#31169;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#25259;&#38706;&#39118;&#38505;&#12290;&#24050;&#24314;&#31435;&#20102;&#21508;&#31181;&#20840;&#29699;&#31435;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#27861;&#35268;&#23545;&#20445;&#25252;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;ML&#25216;&#26415;&#30340;&#23454;&#38469;&#37096;&#32626;&#21464;&#24471;&#22797;&#26434;&#12290;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;(PPML)&#36890;&#36807;&#20445;&#25252;&#20174;&#20581;&#24247;&#35760;&#24405;&#21040;&#22320;&#29702;&#20301;&#32622;&#25968;&#25454;&#31561;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#23454;&#29616;&#23433;&#20840;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02780v1 Announce Type: new  Abstract: The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of th
&lt;/p&gt;</description></item><item><title>ConvTimeNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#20998;&#23618;&#20840;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#27573;&#21644;&#20840;&#21367;&#31215;&#22359;&#35774;&#35745;&#65292;&#26377;&#25928;&#25429;&#25417;&#20102;&#20840;&#23616;&#24207;&#21015;&#21644;&#36328;&#21464;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01493</link><description>&lt;p&gt;
ConvTimeNet: &#19968;&#31181;&#29992;&#20110;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#28145;&#24230;&#20998;&#23618;&#20840;&#21367;&#31215;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ConvTimeNet: A Deep Hierarchical Fully Convolutional Model for Multivariate Time Series Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01493
&lt;/p&gt;
&lt;p&gt;
ConvTimeNet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#20998;&#23618;&#20840;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#20998;&#27573;&#21644;&#20840;&#21367;&#31215;&#22359;&#35774;&#35745;&#65292;&#26377;&#25928;&#25429;&#25417;&#20102;&#20840;&#23616;&#24207;&#21015;&#21644;&#36328;&#21464;&#37327;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ConvTimeNet&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#20998;&#23618;&#20840;&#21367;&#31215;&#32593;&#32476;&#65292;&#26088;&#22312;&#20316;&#20026;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#36890;&#29992;&#27169;&#22411;&#12290;&#35813;&#32593;&#32476;&#30340;&#20851;&#38190;&#35774;&#35745;&#26159;&#20026;&#20102;&#20811;&#26381;&#20256;&#32479;&#21367;&#31215;&#32593;&#32476;&#30340;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#26102;&#38388;&#24207;&#21015;&#21010;&#20998;&#20026;&#23376;&#24207;&#21015;&#32423;&#34917;&#19969;&#30340;&#33258;&#36866;&#24212;&#20998;&#27573;&#65292;&#23558;&#20854;&#35270;&#20026;&#22522;&#26412;&#24314;&#27169;&#21333;&#20803;&#12290;&#36825;&#31181;&#35774;&#32622;&#36991;&#20813;&#20102;&#19982;&#21407;&#22987;&#28857;&#32423;&#26102;&#38388;&#27493;&#38271;&#30456;&#20851;&#32852;&#30340;&#31232;&#30095;&#35821;&#20041;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#21367;&#31215;&#22359;&#65292;&#36890;&#36807;&#24039;&#22937;&#22320;&#38598;&#25104;&#28145;&#24230;&#21644;&#28857;&#21367;&#31215;&#25805;&#20316;&#65292;&#36981;&#24490;Transformer&#32534;&#30721;&#22120;&#20013;&#37319;&#29992;&#30340;&#20808;&#36827;&#26500;&#24314;&#22359;&#39118;&#26684;&#12290;&#36825;&#20010;&#39592;&#24178;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#25429;&#25417;&#20840;&#23616;&#24207;&#21015;&#21644;&#36328;&#21464;&#37327;&#20381;&#36182;&#24615;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#34701;&#21512;&#20102;Transformer&#26550;&#26500;&#30340;&#20808;&#36827;&#24615;&#65292;&#36824;&#32487;&#25215;&#20102;&#21367;&#31215;&#30340;&#22266;&#26377;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01493v1 Announce Type: new  Abstract: This paper introduces ConvTimeNet, a novel deep hierarchical fully convolutional network designed to serve as a general-purpose model for time series analysis. The key design of this network is twofold, designed to overcome the limitations of traditional convolutional networks. Firstly, we propose an adaptive segmentation of time series into sub-series level patches, treating these as fundamental modeling units. This setting avoids the sparsity semantics associated with raw point-level time steps. Secondly, we design a fully convolutional block by skillfully integrating deepwise and pointwise convolution operations, following the advanced building block style employed in Transformer encoders. This backbone network allows for the effective capture of both global sequence and cross-variable dependence, as it not only incorporates the advancements of Transformer architecture but also inherits the inherent properties of convolution. Furtherm
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;</title><link>https://arxiv.org/abs/2403.00986</link><description>&lt;p&gt;
&#21512;&#24182;&#26469;&#33258;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#25991;&#26412;&#21464;&#25442;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Merging Text Transformer Models from Different Initializations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00986
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21512;&#24182;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;Transformer&#27169;&#22411;&#30340;&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#20197;&#30740;&#31350;&#36825;&#20123;&#27169;&#22411;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#21457;&#29616;&#19982;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#22987;&#32456;&#21487;&#20197;&#33719;&#24471;&#36739;&#20302;&#30340;&#25439;&#22833;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#19968;&#27425;&#24615;&#22522;&#20110;&#25490;&#21015;&#30340;&#27169;&#22411;&#21512;&#24182;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#19981;&#21516;&#21021;&#22987;&#21270;&#30340;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#20302;&#25110;&#38646;&#38556;&#30861;&#27169;&#36830;&#25509;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;Transformer&#26550;&#26500;&#22312;&#35821;&#35328;&#39046;&#22495;&#20013;&#21344;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#23578;&#26410;&#24310;&#20280;&#21040;Transformer&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#29420;&#31435;Transformer&#26497;&#23567;&#20540;&#23398;&#20064;&#31867;&#20284;&#29305;&#24449;&#30340;&#31243;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#21512;&#24182;&#25216;&#26415;&#65292;&#20197;&#30740;&#31350;&#25439;&#22833;&#26223;&#35266;&#20013;&#36825;&#20123;&#26497;&#23567;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#26550;&#26500;&#30340;&#20855;&#20307;&#32454;&#33410;&#65292;&#22914;&#20854;&#27531;&#24046;&#36830;&#25509;&#12289;&#22810;&#22836;&#27880;&#24847;&#21147;&#21644;&#31163;&#25955;&#30340;&#39034;&#24207;&#36755;&#20837;&#65292;&#38656;&#35201;&#29305;&#23450;&#30340;&#24178;&#39044;&#25514;&#26045;&#65292;&#20197;&#20415;&#35745;&#31639;&#30041;&#22312;&#30456;&#21516;&#21151;&#33021;&#31561;&#20215;&#31867;&#20013;&#30340;&#27169;&#22411;&#25490;&#21015;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#26041;&#27861;&#21512;&#24182;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#23545;&#20960;&#20010;&#22312;&#19968;&#20010;maske&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#36827;&#34892;&#27169;&#22411;&#24179;&#22343;&#30456;&#27604;&#65292;&#26368;&#23567;&#20540;&#20043;&#38388;&#30340;&#25439;&#22833;&#38556;&#30861;&#19968;&#30452;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00986v1 Announce Type: cross  Abstract: Recent work on one-shot permutation-based model merging has shown impressive low- or zero-barrier mode connectivity between models from completely different initializations. However, this line of work has not yet extended to the Transformer architecture, despite its dominant popularity in the language domain. Therefore, in this work, we investigate the extent to which separate Transformer minima learn similar features, and propose a model merging technique to investigate the relationship between these minima in the loss landscape. The specifics of the architecture, like its residual connections, multi-headed attention, and discrete, sequential input, require specific interventions in order to compute model permutations that remain within the same functional equivalence class. In merging these models with our method, we consistently find lower loss barriers between minima compared to model averaging for several models trained on a maske
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#24378;&#21644;&#22810;&#26679;&#21270;LOTL&#24694;&#24847;&#27963;&#21160;&#30340;&#23384;&#22312;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#24694;&#24847;&#27963;&#21160;&#26816;&#27979;&#24615;&#33021;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;</title><link>https://arxiv.org/abs/2402.18329</link><description>&lt;p&gt;
&#20381;&#38752;&#30693;&#24773;&#25968;&#25454;&#22686;&#24378;&#30340;&#25269;&#21046;&#29983;&#27963;-&#20381;&#36182;-&#22303;&#22320;&#21453;&#21521;&#22806;&#22771;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Living-off-The-Land Reverse-Shell Detection by Informed Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18329
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#24378;&#21644;&#22810;&#26679;&#21270;LOTL&#24694;&#24847;&#27963;&#21160;&#30340;&#23384;&#22312;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25913;&#21892;&#24694;&#24847;&#27963;&#21160;&#26816;&#27979;&#24615;&#33021;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#27963;-&#20381;&#36182;-&#22303;&#22320;(LOTL)&#36827;&#25915;&#26041;&#27861;&#20381;&#36182;&#20110;&#36890;&#36807;&#21512;&#27861;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#30340;&#21629;&#20196;&#38142;&#26469;&#29359;&#32618;&#34892;&#20026;&#65292;&#20165;&#21487;&#36890;&#36807;&#31995;&#32479;&#26085;&#24535;&#20998;&#26512;&#26469;&#35782;&#21035;&#12290;LOTL&#25216;&#26415;&#38544;&#34255;&#22312;&#26222;&#36890;&#21512;&#27861;&#27963;&#21160;&#20135;&#29983;&#30340;&#20107;&#20214;&#27969;&#20013;&#65292;&#23041;&#32961;&#34892;&#20026;&#32773;&#32463;&#24120;&#36890;&#36807;&#28151;&#28102;&#26469;&#20266;&#35013;&#27963;&#21160;&#65292;&#20351;&#20854;&#38590;&#20197;&#22312;&#19981;&#24341;&#36215;&#22823;&#37327;&#35823;&#35686;&#24773;&#20917;&#19979;&#26816;&#27979;&#65292;&#21363;&#20351;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#20063;&#26159;&#22914;&#27492;&#12290;&#20026;&#20102;&#22312;&#36825;&#26679;&#24694;&#21155;&#30340;&#29615;&#22659;&#20013;&#25552;&#39640;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#21644;&#20351;&#22810;&#26679;&#21270;LOTL&#24694;&#24847;&#27963;&#21160;&#22312;&#21512;&#27861;&#26085;&#24535;&#20013;&#30340;&#23384;&#22312;&#12290;&#22312;&#23041;&#32961;&#24773;&#25253;&#30340;&#25351;&#23548;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#27880;&#20837;&#24050;&#30693;&#22312;&#37326;&#22806;&#20351;&#29992;&#30340;&#25915;&#20987;&#27169;&#26495;&#29983;&#25104;&#25968;&#25454;&#38598;&#65292;&#36827;&#19968;&#27493;&#20016;&#23500;&#21512;&#27861;&#27963;&#21160;&#30340;&#21487;&#22609;&#27169;&#24335;&#65292;&#20197;&#22797;&#21046;&#22238;&#36991;&#23041;&#32961;&#34892;&#20026;&#32773;&#34892;&#20026;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18329v1 Announce Type: cross  Abstract: The living-off-the-land (LOTL) offensive methodologies rely on the perpetration of malicious actions through chains of commands executed by legitimate applications, identifiable exclusively by analysis of system logs. LOTL techniques are well hidden inside the stream of events generated by common legitimate activities, moreover threat actors often camouflage activity through obfuscation, making them particularly difficult to detect without incurring in plenty of false alarms, even using machine learning. To improve the performance of models in such an harsh environment, we propose an augmentation framework to enhance and diversify the presence of LOTL malicious activity inside legitimate logs. Guided by threat intelligence, we generate a dataset by injecting attack templates known to be employed in the wild, further enriched by malleable patterns of legitimate activities to replicate the behavior of evasive threat actors. We conduct an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20302;&#31209;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;PrivateLoRA&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#21644;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#30340;MI&#22686;&#30410;&#26469;&#25269;&#24481;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2402.11989</link><description>&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#20302;&#31209;&#36866;&#24212;Latent&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving Low-Rank Adaptation for Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11989
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38544;&#31169;&#20445;&#25252;&#30340;&#20302;&#31209;&#36866;&#24212;&#35299;&#20915;&#26041;&#26696;PrivateLoRA&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#21644;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#30340;MI&#22686;&#30410;&#26469;&#25269;&#24481;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#29992;&#20110;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#65292;&#33258;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#36866;&#24212;Latent&#25193;&#25955;&#27169;&#22411;&#65288;LDM&#65289;&#20197;&#29983;&#25104;&#29305;&#23450;&#23545;&#35937;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;LoRA&#36866;&#24212;&#30340;LDM&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#26029;&#65288;MI&#65289;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#31181;&#25915;&#20987;&#21487;&#20197;&#21028;&#26029;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#23646;&#20110;&#31169;&#20154;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#22240;&#27492;&#38754;&#20020;&#20005;&#37325;&#30340;&#38544;&#31169;&#27844;&#38706;&#39118;&#38505;&#12290;&#20026;&#20102;&#25269;&#24481;MI&#25915;&#20987;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#20010;&#30452;&#25509;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#38544;&#31169;&#20445;&#25252;&#30340;LoRA&#65288;PrivateLoRA&#65289;&#12290;PrivateLoRA&#34987;&#26500;&#24314;&#20026;&#19968;&#20010;&#26368;&#23567;&#26368;&#22823;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;MI&#22686;&#30410;&#26469;&#35757;&#32451;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#65292;&#32780;LDM&#21017;&#36890;&#36807;&#26368;&#23567;&#21270;&#36866;&#24212;&#25439;&#22833;&#21644;&#20195;&#29702;&#25915;&#20987;&#27169;&#22411;&#30340;MI&#22686;&#30410;&#20043;&#21644;&#26469;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#21457;&#29616;PrivateLoRA&#23384;&#22312;&#31283;&#23450;&#24615;&#20248;&#21270;&#38382;&#39064;&#65292;&#21363;&#30001;&#20110;&#26799;&#24230;&#35268;&#27169;&#30340;&#22823;&#24133;&#27874;&#21160;&#32780;&#22952;&#30861;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11989v1 Announce Type: new  Abstract: Low-rank adaptation (LoRA) is an efficient strategy for adapting latent diffusion models (LDMs) on a training dataset to generate specific objects by minimizing the adaptation loss. However, adapted LDMs via LoRA are vulnerable to membership inference (MI) attacks that can judge whether a particular data point belongs to private training datasets, thus facing severe risks of privacy leakage. To defend against MI attacks, we make the first effort to propose a straightforward solution: privacy-preserving LoRA (PrivateLoRA). PrivateLoRA is formulated as a min-max optimization problem where a proxy attack model is trained by maximizing its MI gain while the LDM is adapted by minimizing the sum of the adaptation loss and the proxy attack model's MI gain. However, we empirically disclose that PrivateLoRA has the issue of unstable optimization due to the large fluctuation of the gradient scale which impedes adaptation. To mitigate this issue, w
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11441</link><description>&lt;p&gt;
InfuserKI&#65306;&#36890;&#36807;Infuser&#24341;&#23548;&#30340;&#30693;&#35782;&#38598;&#25104;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11441
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#32531;&#35299;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24320;&#25918;&#24335;&#29983;&#25104;&#33021;&#21147;&#65292;&#20294;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#30693;&#35782;&#38598;&#25104;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#27169;&#22359;&#23558;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#22270;&#35889;&#19982;LLMs&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#25968;&#25454;&#25928;&#29575;&#20302;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#38656;&#35201;&#24050;&#30693;&#21644;&#26410;&#30693;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#24494;&#35843;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38382;&#39064;&#65292;&#21363;&#22914;&#20309;&#22312;&#19981;&#37325;&#22797;&#24050;&#30693;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#23558;&#26410;&#30693;&#30693;&#35782;&#38598;&#25104;&#21040;LLMs&#20013;&#12290;&#27880;&#20837;&#26032;&#30693;&#35782;&#20250;&#23548;&#33268;&#36951;&#24536;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#30340;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Infuser-Guided Knowledge Integration&#65288;InfuserKI&#65289;&#26694;&#26550;&#65292;&#21033;&#29992;transformer&#20869;&#37096;&#29366;&#24577;&#26469;&#30830;&#23450;&#26159;&#21542;&#24212;&#35813;&#22686;&#24378;&#21407;&#22987;LLM&#36755;&#20986;&#20449;&#24687;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20943;&#36731;&#30693;&#35782;&#36951;&#24536;&#38382;&#39064;&#12290;&#22312;UMLS-2.5k&#21644;MetaQ&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11441v1 Announce Type: cross  Abstract: Though Large Language Models (LLMs) have shown remarkable open-generation capabilities across diverse domains, they struggle with knowledge-intensive tasks. To alleviate this issue, knowledge integration methods have been proposed to enhance LLMs with domain-specific knowledge graphs using external modules. However, they suffer from data inefficiency as they require both known and unknown knowledge for fine-tuning. Thus, we study a novel problem of integrating unknown knowledge into LLMs efficiently without unnecessary overlap of known knowledge. Injecting new knowledge poses the risk of forgetting previously acquired knowledge. To tackle this, we propose a novel Infuser-Guided Knowledge Integration (InfuserKI) framework that utilizes transformer internal states to determine whether to enhance the original LLM output with additional information, thereby effectively mitigating knowledge forgetting. Evaluations on the UMLS-2.5k and MetaQ
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05668</link><description>&lt;p&gt;
&#23545;LLMs&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of Jailbreak Attacks Against LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05668
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#21462;&#20102;&#23433;&#20840;&#25514;&#26045;&#20197;&#30830;&#20445;LLMs&#31526;&#21512;&#31038;&#20250;&#20262;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#32469;&#36807;LLMs&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#65292;&#34987;&#31216;&#20026;&#36234;&#29425;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#25216;&#26415;&#65292;&#22914;&#35282;&#33394;&#25198;&#28436;&#22330;&#26223;&#12289;&#23545;&#25239;&#24615;&#26679;&#26412;&#25110;&#23545;&#23433;&#20840;&#30446;&#26631;&#30340;&#24494;&#22937;&#30772;&#22351;&#20316;&#20026;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20135;&#29983;&#19981;&#36866;&#24403;&#29978;&#33267;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#36234;&#29425;&#25915;&#20987;&#30340;&#31867;&#21035;&#65292;&#20294;&#20182;&#20204;&#37117;&#26159;&#23396;&#31435;&#22320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21508;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#26469;&#33258;&#22235;&#20010;&#31867;&#21035;&#30340;13&#31181;&#23574;&#31471;&#36234;&#29425;&#26041;&#27861;&#12289;16&#31181;&#36829;&#35268;&#31867;&#21035;&#30340;160&#20010;&#38382;&#39064;&#20197;&#21450;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#19978;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
&lt;/p&gt;</description></item><item><title>L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.04902</link><description>&lt;p&gt;
L4Q: &#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#37327;&#21270;&#35757;&#32451;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#25552;&#20379;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
L4Q: Parameter Efficient Quantization-Aware Training on Large Language Models via LoRA-wise LSQ
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04902
&lt;/p&gt;
&lt;p&gt;
L4Q&#26159;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#65292;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#37327;&#21270;&#35757;&#32451;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#21644;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;(QAT)&#26041;&#27861;&#27491;&#22312;&#27969;&#34892;&#36215;&#26469;&#65292;&#20197;&#32531;&#35299;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25152;&#24102;&#26469;&#30340;&#39640;&#20869;&#23384;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#23613;&#31649;&#21518;&#32773;&#20855;&#26377;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#28508;&#21147;&#65292;&#20294;&#30001;&#20110;&#20854;&#20943;&#23569;&#30340;&#35757;&#32451;&#24320;&#38144;&#65292;&#36890;&#24120;&#39318;&#36873;&#21518;&#35757;&#32451;&#37327;&#21270;&#12290;&#21516;&#26102;&#65292;&#20171;&#32461;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#22914;&#20302;&#31209;&#36866;&#24212;&#65288;LoRA&#65289;&#65292;&#24182;&#26368;&#36817;&#30340;&#24037;&#20316;&#24050;&#32463;&#25506;&#32034;&#20102;&#37327;&#21270;&#24863;&#30693;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#32570;&#20047;&#36890;&#29992;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#39044;&#37327;&#21270;&#27169;&#22411;&#30340;&#37197;&#32622;&#12290;&#30001;&#38750;&#32447;&#24615;&#37327;&#21270;&#25110;&#28151;&#21512;&#31934;&#24230;&#26435;&#37325;&#24341;&#36215;&#30340;&#25928;&#26524;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#65292;&#24182;&#19988;&#37325;&#26032;&#35757;&#32451;&#29305;&#23450;&#37327;&#21270;&#21442;&#25968;&#21487;&#33021;&#20250;&#24433;&#21709;&#26368;&#20248;&#24615;&#33021;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;L4Q&#65292;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#37327;&#21270;&#24863;&#30693;&#35757;&#32451;&#31639;&#27861;&#12290;L4Q&#21033;&#29992;&#20102;&#22522;&#20110;LoRA&#30340;&#23398;&#20064;&#30340;&#37327;&#21270;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training quantization (PTQ) and quantization-aware training (QAT) methods are gaining popularity in mitigating the high memory and computational costs associated with Large Language Models (LLMs). In resource-constrained scenarios, PTQ, with its reduced training overhead, is often preferred over QAT, despite the latter's potential for higher accuracy. Meanwhile, parameter-efficient fine-tuning (PEFT) methods like low-rank adaptation (LoRA) have been introduced, and recent efforts have explored quantization-aware PEFT techniques. However, these approaches may lack generality due to their reliance on the pre-quantized model's configuration. Their effectiveness may be compromised by non-linearly quantized or mixed-precision weights, and the retraining of specific quantization parameters might impede optimal performance. To address these challenges, we propose L4Q, an algorithm for parameter-efficient quantization-aware training. L4Q leverages LoRA-wise learned quantization step size 
&lt;/p&gt;</description></item><item><title>&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;RepairLLaMA&#21487;&#20026;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#20135;&#29983;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#12290;</title><link>https://arxiv.org/abs/2312.15698</link><description>&lt;p&gt;
RepairLLaMA&#65306;&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#29992;&#20110;&#31243;&#24207;&#20462;&#22797;
&lt;/p&gt;
&lt;p&gt;
RepairLLaMA: Efficient Representations and Fine-Tuned Adapters for Program Repair
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15698
&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#34920;&#31034;&#21644;&#24494;&#35843;&#36866;&#37197;&#22120;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;RepairLLaMA&#21487;&#20026;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#20135;&#29983;&#39640;&#25928;&#30340;&#36866;&#37197;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#31243;&#24207;&#20462;&#22797;&#65288;APR&#65289;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#24050;&#26377;&#20102;&#26174;&#33879;&#21457;&#23637;&#12290;&#23545;&#20110;&#31243;&#24207;&#20462;&#22797;&#36827;&#34892;LLMs&#30340;&#24494;&#35843;&#26159;&#26368;&#36817;&#30740;&#31350;&#30340;&#19968;&#20010;&#26032;&#39046;&#22495;&#65292;&#26377;&#35768;&#22810;&#26410;&#34987;&#25506;&#32034;&#30340;&#32500;&#24230;&#12290;&#29616;&#26377;&#24037;&#20316;&#22823;&#22810;&#20351;&#29992;&#31616;&#21333;&#30340;&#20195;&#30721;&#34920;&#31034;&#23545;LLMs&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#22312;&#33021;&#22815;&#24494;&#35843;&#26356;&#22823;&#22411;LLMs&#30340;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#23616;&#38480;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RepairLLaMA&#65292;&#19968;&#20010;&#32467;&#21512;&#20102;1&#65289;&#29992;&#20110;APR&#30340;&#20195;&#30721;&#34920;&#31034;&#21644;2&#65289;&#26368;&#20808;&#36827;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;LLM&#24494;&#35843;&#25216;&#26415;LoRA&#30340;&#26032;&#22411;&#31243;&#24207;&#20462;&#22797;&#26041;&#27861;&#12290;&#36825;&#20351;&#24471;RepairLLaMA&#20135;&#29983;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#8220;&#31243;&#24207;&#20462;&#22797;&#36866;&#37197;&#22120;&#8221;&#65292;&#29992;&#20110;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20462;&#22797;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#26377;&#25928;&#24615;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#20855;&#26377;&#31243;&#24207;&#20462;&#22797;&#29305;&#23450;&#20195;&#30721;&#34920;&#31034;&#30340;&#24494;&#35843;&#36866;&#37197;&#22120;&#20351;&#27169;&#22411;&#33021;&#22815;&#20351;&#29992;&#26377;&#24847;&#20041;&#30340;&#20462;&#22797;&#20449;&#21495;&#12290;&#20854;&#27425;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26377;&#21161;&#20110;&#24494;&#35843;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.15698v2 Announce Type: replace-cross  Abstract: Automated Program Repair (APR) has evolved significantly with the advent of Large Language Models (LLMs). Fine-tuning LLMs for program repair is a recent avenue of research, with many dimensions which have not been explored. Existing work mostly fine-tunes LLMs with naive code representations and is fundamentally limited in its ability to fine-tune larger LLMs. To address this problem, we propose RepairLLaMA, a novel program repair approach that combines 1) code representations for APR and 2) the state-of-the-art parameter-efficient LLM fine-tuning technique called LoRA. This results in RepairLLaMA producing a highly effective `program repair adapter' for fixing bugs with language models. Our experiments demonstrate the validity of both concepts. First, fine-tuning adapters with program repair specific code representations enables the model to use meaningful repair signals. Second, parameter-efficient fine-tuning helps fine-tun
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;1D&#24515;&#38899;&#22270;&#26679;&#26412;&#20013;&#24322;&#24120;&#26816;&#27979;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38899;&#39057;&#22686;&#24378;&#26041;&#27861;&#27604;&#36739;&#35780;&#20272;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2312.00502</link><description>&lt;p&gt;
&#23545;&#31283;&#20581;&#30340;OOD&#33258;&#30417;&#30563;&#23545;&#27604;&#24515;&#38899;&#22270;&#34920;&#31034;&#23398;&#20064;&#22686;&#24378;&#26041;&#27861;&#30340;&#20840;&#38754;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Evaluation of Augmentations for Robust OOD Self-Supervised Contrastive Phonocardiogram Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00502
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#24212;&#29992;&#20110;1D&#24515;&#38899;&#22270;&#26679;&#26412;&#20013;&#24322;&#24120;&#26816;&#27979;&#65292;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#38899;&#39057;&#22686;&#24378;&#26041;&#27861;&#27604;&#36739;&#35780;&#20272;&#21644;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#36817;&#24180;&#26469;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#30740;&#31350;&#27963;&#21160;&#26377;&#25152;&#22686;&#21152;&#65292;&#20294;&#22312;&#21307;&#23398;&#31561;&#22810;&#20010;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#65292;&#36825;&#20123;&#27169;&#22411;&#23578;&#26410;&#34987;&#24191;&#27867;&#25509;&#21463;&#12290;&#39640;&#36136;&#37327;&#26631;&#35760;&#25968;&#25454;&#30340;&#30701;&#32570;&#32463;&#24120;&#38459;&#30861;&#20102;&#24320;&#21457;&#31283;&#20581;&#19988;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#27169;&#22411;&#65292;&#24403;&#38754;&#20020;&#26032;&#25910;&#38598;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#20250;&#22240;&#25928;&#26524;&#19979;&#38477;&#32780;&#21463;&#25439;&#12290;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20026;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#25552;&#20379;&#20102;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#22240;&#20026;&#23427;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#22686;&#21152;&#27169;&#22411;&#30340;&#25928;&#33021;&#21644;&#31283;&#20581;&#24615;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23558;&#23545;&#27604;SSL&#24212;&#29992;&#20110;&#26816;&#27979;1D&#24515;&#38899;&#22270;&#65288;PCG&#65289;&#26679;&#26412;&#20013;&#30340;&#24322;&#24120;&#65292;&#36890;&#36807;&#23398;&#20064;&#20449;&#21495;&#30340;&#24191;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24191;&#27867;&#30340;&#27604;&#36739;&#35780;&#20272;&#65292;&#28041;&#21450;&#22810;&#31181;&#22522;&#20110;&#38899;&#39057;&#30340;&#22686;&#24378;&#26041;&#27861;&#65292;&#35780;&#20272;&#20102;&#22312;&#19981;&#21516;&#19979;&#28216;&#20219;&#21153;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#65292;&#26368;&#32456;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00502v2 Announce Type: replace  Abstract: Despite the recent increase in research activity, deep-learning models have not yet been widely accepted in several real-world settings, such as medicine. The shortage of high-quality annotated data often hinders the development of robust and generalizable models, which do not suffer from degraded effectiveness when presented with newly-collected, out-of-distribution (OOD) datasets. Contrastive Self-Supervised Learning (SSL) offers a potential solution to labeled data scarcity, as it takes advantage of unlabeled data to increase model effectiveness and robustness. In this research, we propose applying contrastive SSL for detecting abnormalities in 1D phonocardiogram (PCG) samples by learning a generalized representation of the signal. Specifically, we perform an extensive comparative evaluation of a wide range of audio-based augmentations, evaluate trained classifiers on multiple datasets across different downstream tasks, and finall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;SBBO&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20165;&#38656;&#22522;&#20110;&#37319;&#26679;&#30340;&#35775;&#38382;&#26469;&#20248;&#21270;&#33719;&#21462;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2401.10811</link><description>&lt;p&gt;
&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Simulation Based Bayesian Optimization. (arXiv:2401.10811v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;SBBO&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20165;&#38656;&#22522;&#20110;&#37319;&#26679;&#30340;&#35775;&#38382;&#26469;&#20248;&#21270;&#33719;&#21462;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#19982;&#25345;&#32493;&#20989;&#25968;&#35780;&#20272;&#30456;&#32467;&#21512;&#30340;&#24378;&#22823;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#36890;&#36807;&#26500;&#24314;&#19982;&#21327;&#21464;&#37327;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#27010;&#29575;&#20195;&#29702;&#27169;&#22411;&#26469;&#25351;&#23548;&#26410;&#26469;&#35780;&#20272;&#28857;&#30340;&#36873;&#25321;&#12290;&#23545;&#20110;&#24179;&#28369;&#36830;&#32493;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#39640;&#26031;&#36807;&#31243;&#32463;&#24120;&#34987;&#29992;&#20316;&#20195;&#29702;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#25552;&#20379;&#23545;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#30340;&#35299;&#26512;&#35775;&#38382;&#65292;&#20174;&#32780;&#20415;&#20110;&#35745;&#31639;&#21644;&#20248;&#21270;&#33719;&#21462;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#28041;&#21450;&#23545;&#20998;&#31867;&#25110;&#28151;&#21512;&#21327;&#21464;&#37327;&#31354;&#38388;&#36827;&#34892;&#20248;&#21270;&#30340;&#22797;&#26434;&#24773;&#20917;&#19979;&#65292;&#39640;&#26031;&#36807;&#31243;&#21487;&#33021;&#19981;&#26159;&#29702;&#24819;&#30340;&#36873;&#25321;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22522;&#20110;&#20223;&#30495;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#65288;SBBO&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#23545;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#36827;&#34892;&#22522;&#20110;&#37319;&#26679;&#30340;&#35775;&#38382;&#65292;&#20197;&#20248;&#21270;&#33719;&#21462;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimization (BO) is a powerful method for optimizing black-box functions by combining prior knowledge with ongoing function evaluations. BO constructs a probabilistic surrogate model of the objective function given the covariates, which is in turn used to inform the selection of future evaluation points through an acquisition function. For smooth continuous search spaces, Gaussian Processes (GPs) are commonly used as the surrogate model as they offer analytical access to posterior predictive distributions, thus facilitating the computation and optimization of acquisition functions. However, in complex scenarios involving optimizations over categorical or mixed covariate spaces, GPs may not be ideal.  This paper introduces Simulation Based Bayesian Optimization (SBBO) as a novel approach to optimizing acquisition functions that only requires \emph{sampling-based} access to posterior predictive distributions. SBBO allows the use of surrogate probabilistic models tailored for co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.02329</link><description>&lt;p&gt;
&#19981;&#26159;&#25152;&#26377;&#30340;&#23569;&#25968;&#32676;&#20307;&#37117;&#26159;&#24179;&#31561;&#30340;: &#31354;&#31867;&#21035;&#24863;&#30693;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Not all Minorities are Equal: Empty-Class-Aware Distillation for Heterogeneous Federated Learning. (arXiv:2401.02329v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;FedED&#65292;&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#65292;&#35299;&#20915;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23578;&#26410;&#20805;&#20998;&#35782;&#21035;&#31354;&#31867;&#21035;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#24322;&#36136;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#34920;&#29616;&#20026;&#23458;&#25143;&#31471;&#20043;&#38388;&#26412;&#22320;&#25968;&#25454;&#20998;&#24067;&#30340;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#24120;&#24120;&#22312;&#26412;&#22320;&#35757;&#32451;&#36807;&#31243;&#20013;&#37319;&#29992;&#31867;&#21035;&#24179;&#34913;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#26412;&#22320;&#31867;&#21035;&#20998;&#24067;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#25968;&#31867;&#21035;&#20013;&#30001;&#20110;&#36807;&#25311;&#21512;&#26412;&#22320;&#19981;&#24179;&#34913;&#25968;&#25454;&#32780;&#23548;&#33268;&#20934;&#30830;&#24615;&#36739;&#24046;&#30340;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedED&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24322;&#36136;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#21516;&#26102;&#25972;&#21512;&#20102;&#31354;&#31867;&#21035;&#33976;&#39311;&#21644;&#36923;&#36753;&#25233;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31354;&#31867;&#21035;&#33976;&#39311;&#21033;&#29992;&#30693;&#35782;&#33976;&#39311;&#30340;&#26041;&#27861;&#22312;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#35757;&#32451;&#20013;&#20445;&#30041;&#20102;&#19982;&#31354;&#31867;&#21035;&#30456;&#20851;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#36923;&#36753;&#25233;&#21046;&#30452;&#25509;&#38459;&#26029;&#20102;&#39044;&#27979;&#32467;&#26524;&#20013;&#23545;&#31354;&#31867;&#21035;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data heterogeneity, characterized by disparities in local data distribution across clients, poses a significant challenge in federated learning. Substantial efforts have been devoted to addressing the heterogeneity in local label distribution. As minority classes suffer from worse accuracy due to overfitting on local imbalanced data, prior methods often incorporate class-balanced learning techniques during local training. Despite the improved mean accuracy across all classes, we observe that empty classes-referring to categories absent from a client's data distribution-are still not well recognized. This paper introduces FedED, a novel approach in heterogeneous federated learning that integrates both empty-class distillation and logit suppression simultaneously. Specifically, empty-class distillation leverages knowledge distillation during local training on each client to retain essential information related to empty classes from the global model. Moreover, logit suppression directly p
&lt;/p&gt;</description></item><item><title>SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.00793</link><description>&lt;p&gt;
SecFormer&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for Large Language Models. (arXiv:2401.00793v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00793
&lt;/p&gt;
&lt;p&gt;
SecFormer&#26159;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#28040;&#38500;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;SecFormer&#33021;&#22815;&#26377;&#25928;&#35299;&#20915;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24212;&#29992;SMPC&#26102;&#30340;&#24615;&#33021;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22312;&#20113;&#24179;&#21488;&#19978;&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#25552;&#20379;&#25512;&#29702;&#26381;&#21153;&#30340;&#20351;&#29992;&#22686;&#21152;&#65292;&#38544;&#31169;&#38382;&#39064;&#26085;&#30410;&#21152;&#21095;&#65292;&#23588;&#20854;&#26159;&#28041;&#21450;&#25237;&#36164;&#35745;&#21010;&#21644;&#38134;&#34892;&#36134;&#25143;&#31561;&#25935;&#24863;&#25968;&#25454;&#12290;&#23433;&#20840;&#22810;&#26041;&#35745;&#31639;&#65288;SMPC&#65289;&#34987;&#35270;&#20026;&#20445;&#25252;&#25512;&#29702;&#25968;&#25454;&#21644;&#27169;&#22411;&#21442;&#25968;&#38544;&#31169;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;SMPC&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#27169;&#22411;&#65289;&#30340;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#24448;&#24448;&#20250;&#23548;&#33268;&#26174;&#33879;&#30340;&#20943;&#36895;&#25110;&#24615;&#33021;&#19979;&#38477;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;Transformer&#26550;&#26500;&#20013;&#30340;&#20247;&#22810;&#38750;&#32447;&#24615;&#25805;&#20316;&#19981;&#36866;&#21512;SMPC&#65292;&#24182;&#19988;&#38590;&#20197;&#26377;&#25928;&#35268;&#36991;&#25110;&#20248;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#36827;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;SecFormer&#65292;&#20197;&#23454;&#29616;Transformer&#27169;&#22411;&#30340;&#24555;&#36895;&#20934;&#30830;&#38544;&#31169;&#20445;&#25252;&#25512;&#29702;&#12290;&#36890;&#36807;&#23454;&#26045;&#27169;&#22411;&#35774;&#35745;&#20248;&#21270;&#65292;&#25105;&#20204;&#25104;&#21151;&#28040;&#38500;&#20102;&#39640;&#25104;&#26412;&#30340;&#25351;&#25968;&#21644;&#32447;&#24615;&#25805;&#20316;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the growing use of large language models hosted on cloud platforms to offer inference services, privacy concerns are escalating, especially concerning sensitive data like investment plans and bank account details. Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect the privacy of inference data and model parameters. However, the application of SMPC in Privacy-Preserving Inference (PPI) for large language models, particularly those based on the Transformer architecture, often leads to considerable slowdowns or declines in performance. This is largely due to the multitude of nonlinear operations in the Transformer architecture, which are not well-suited to SMPC and difficult to circumvent or optimize effectively. To address this concern, we introduce an advanced optimization framework called SecFormer, to achieve fast and accurate PPI for Transformer models. By implementing model design optimization, we successfully eliminate the high-cost exponential and 
&lt;/p&gt;</description></item><item><title>&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2310.07325</link><description>&lt;p&gt;
&#30452;&#25509;&#36923;&#36753;&#23646;&#24615;&#30340;&#23545;&#25239;&#24615;&#26679;&#26412;&#65306;gelu-4l&#20013;&#30340;&#20869;&#23384;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
An Adversarial Example for Direct Logit Attribution: Memory Management in gelu-4l. (arXiv:2310.07325v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07325
&lt;/p&gt;
&lt;p&gt;
&#22312;gelu-4l&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35777;&#25454;&#34920;&#26126;&#20869;&#23384;&#31649;&#29702;&#23545;&#20110;transformer&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#35828;&#26126;&#20102;Direct Logit Attribution&#25216;&#26415;&#30340;&#19981;&#20934;&#30830;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;4&#23618;transformer&#20013;&#20869;&#23384;&#31649;&#29702;&#30340;&#20855;&#20307;&#35777;&#25454;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#27169;&#22411;&#32452;&#20214;&#19968;&#33268;&#22320;&#31227;&#38500;&#21069;&#38754;&#32452;&#20214;&#30340;&#36755;&#20986;&#65292;&#36825;&#26159;&#19968;&#31181;&#28165;&#29702;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35299;&#37322;&#24615;&#25216;&#26415;Direct Logit Attribution&#25552;&#20379;&#20102;&#35823;&#23548;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26126;&#30830;&#30340;&#20363;&#23376;&#65292;&#35777;&#26126;&#36825;&#31181;&#25216;&#26415;&#26159;&#19981;&#20934;&#30830;&#30340;&#65292;&#22240;&#20026;&#23427;&#27809;&#26377;&#32771;&#34385;&#21040;&#28165;&#29702;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide concrete evidence for memory management in a 4-layer transformer. Specifically, we identify clean-up behavior, in which model components consistently remove the output of preceeding components during a forward pass. Our findings suggest that the interpretability technique Direct Logit Attribution provides misleading results. We show explicit examples where this technique is inaccurate, as it does not account for clean-up behavior.
&lt;/p&gt;</description></item><item><title>xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2310.02989</link><description>&lt;p&gt;
xVal: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
xVal: A Continuous Number Encoding for Large Language Models. (arXiv:2310.02989v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02989
&lt;/p&gt;
&lt;p&gt;
xVal&#26159;&#19968;&#31181;&#36830;&#32493;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;xVal&#26356;&#21152;&#39640;&#25928;&#65292;&#24182;&#19988;&#22312;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#23383;&#20196;&#29260;&#21270;&#30340;&#29420;&#29305;&#22256;&#38590;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23578;&#26410;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;xVal&#65292;&#19968;&#31181;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#65292;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#26631;&#35760;&#26469;&#34920;&#31034;&#20219;&#20309;&#23454;&#25968;&#12290;xVal&#36890;&#36807;&#23558;&#19987;&#29992;&#23884;&#20837;&#21521;&#37327;&#25353;&#25968;&#23383;&#20540;&#36827;&#34892;&#32553;&#25918;&#26469;&#34920;&#31034;&#32473;&#23450;&#30340;&#23454;&#25968;&#12290;&#32467;&#21512;&#20462;&#25913;&#21518;&#30340;&#25968;&#23383;&#25512;&#26029;&#26041;&#27861;&#65292;&#35813;&#31574;&#30053;&#20351;&#27169;&#22411;&#22312;&#32771;&#34385;&#20316;&#20026;&#20174;&#36755;&#20837;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#21040;&#36755;&#20986;&#23383;&#31526;&#20018;&#30340;&#25968;&#23383;&#30340;&#26144;&#23556;&#26102;&#25104;&#20026;&#31471;&#21040;&#31471;&#36830;&#32493;&#30340;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#26356;&#36866;&#29992;&#20110;&#31185;&#23398;&#39046;&#22495;&#24212;&#29992;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#21512;&#25104;&#21644;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;&#19982;&#29616;&#26377;&#30340;&#25968;&#23383;&#32534;&#30721;&#26041;&#26696;&#30456;&#27604;&#65292;&#25105;&#20204;&#21457;&#29616;xVal&#22312;&#20196;&#29260;&#25928;&#29575;&#21644;&#27867;&#21270;&#24615;&#33021;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models have not yet been broadly adapted for the analysis of scientific datasets due in part to the unique difficulties of tokenizing numbers. We propose xVal, a numerical encoding scheme that represents any real number using just a single token. xVal represents a given real number by scaling a dedicated embedding vector by the number value. Combined with a modified number-inference approach, this strategy renders the model end-to-end continuous when considered as a map from the numbers of the input string to those of the output string. This leads to an inductive bias that is generally more suitable for applications in scientific domains. We empirically evaluate our proposal on a number of synthetic and real-world datasets. Compared with existing number encoding schemes, we find that xVal is more token-efficient and demonstrates improved generalization.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Zen&#65292;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#24335;DNN&#35757;&#32451;&#20013;&#36817;&#20284;&#26368;&#20248;&#31232;&#30095;&#24352;&#37327;&#21516;&#27493;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#27969;&#34892;&#30340;DNN&#27169;&#22411;&#20013;&#31232;&#30095;&#24352;&#37327;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#25506;&#32034;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#36890;&#20449;&#26041;&#26696;&#12290;&#36890;&#36807;&#20943;&#23569;&#36890;&#20449;&#27969;&#37327;&#21644;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;Zen&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.13254</link><description>&lt;p&gt;
Zen&#65306;&#29992;&#20110;&#20998;&#24067;&#24335;DNN&#35757;&#32451;&#30340;&#36817;&#20284;&#26368;&#20248;&#31232;&#30095;&#24352;&#37327;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Zen: Near-Optimal Sparse Tensor Synchronization for Distributed DNN Training. (arXiv:2309.13254v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13254
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;Zen&#65292;&#19968;&#31181;&#29992;&#20110;&#20998;&#24067;&#24335;DNN&#35757;&#32451;&#20013;&#36817;&#20284;&#26368;&#20248;&#31232;&#30095;&#24352;&#37327;&#21516;&#27493;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#27969;&#34892;&#30340;DNN&#27169;&#22411;&#20013;&#31232;&#30095;&#24352;&#37327;&#30340;&#29305;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#25506;&#32034;&#35774;&#35745;&#31354;&#38388;&#65292;&#25214;&#21040;&#20102;&#26368;&#20339;&#30340;&#36890;&#20449;&#26041;&#26696;&#12290;&#36890;&#36807;&#20943;&#23569;&#36890;&#20449;&#27969;&#37327;&#21644;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#65292;Zen&#26377;&#25928;&#22320;&#25552;&#21319;&#20102;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#35757;&#32451;&#26159;&#20351;&#29992;&#22810;&#20010;GPU&#25193;&#23637;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#35757;&#32451;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#20998;&#24067;&#24335;&#35757;&#32451;&#30340;&#24615;&#33021;&#29942;&#39048;&#22312;&#20110;&#28176;&#21464;&#21516;&#27493;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#23454;&#36341;&#32773;&#35266;&#23519;&#21040;&#28176;&#21464;&#24352;&#37327;&#20013;&#23384;&#22312;&#31232;&#30095;&#24615;&#65292;&#34920;&#26126;&#21487;&#20197;&#20943;&#23569;&#36890;&#20449;&#30340;&#27969;&#37327;&#24182;&#25552;&#39640;&#31471;&#21040;&#31471;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#23436;&#20840;&#21457;&#25381;&#31232;&#30095;&#24615;&#30340;&#26368;&#20339;&#36890;&#20449;&#26041;&#26696;&#20173;&#28982;&#32570;&#22833;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#20998;&#26512;&#20102;&#27969;&#34892;DNN&#27169;&#22411;&#20013;&#31232;&#30095;&#24352;&#37327;&#30340;&#29305;&#24615;&#65292;&#20197;&#20102;&#35299;&#31232;&#30095;&#24615;&#30340;&#22522;&#26412;&#21407;&#29702;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#31232;&#30095;&#24352;&#37327;&#36890;&#20449;&#26041;&#26696;&#30340;&#35774;&#35745;&#31354;&#38388;&#24182;&#25214;&#21040;&#20102;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed training is the de facto standard to scale up the training of Deep Neural Networks (DNNs) with multiple GPUs. The performance bottleneck of distributed training lies in communications for gradient synchronization. Recently, practitioners have observed sparsity in gradient tensors, suggesting the potential to reduce the traffic volume in communication and improve end-to-end training efficiency. Yet, the optimal communication scheme to fully leverage sparsity is still missing. This paper aims to address this gap. We first analyze the characteristics of sparse tensors in popular DNN models to understand the fundamentals of sparsity. We then systematically explore the design space of communication schemes for sparse tensors and find the optimal one. % We then find the optimal scheme based on the characteristics by systematically exploring the design space. We also develop a gradient synchronization system called Zen that approximately realizes it for sparse tensors. We demonstr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#65288;QPF&#65289;&#22312;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#22312;MNIST&#12289;EMNIST&#21644;CIFAR-10&#19978;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;GTSRB&#19978;&#38477;&#20302;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.14930</link><description>&lt;p&gt;
&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#22312;&#23567;&#26679;&#26412;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Application of Quantum Pre-Processing Filter for Binary Image Classification with Small Samples. (arXiv:2308.14930v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14930
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#65288;QPF&#65289;&#22312;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#36890;&#36807;&#22312;MNIST&#12289;EMNIST&#21644;CIFAR-10&#19978;&#25552;&#39640;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#65292;&#24182;&#22312;GTSRB&#19978;&#38477;&#20302;&#20102;&#20998;&#31867;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#26469;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#22312;&#30740;&#31350;&#20154;&#21592;&#20013;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#26377;&#28508;&#21147;&#25913;&#21464;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#24050;&#24320;&#21457;&#20986;&#21033;&#29992;&#37327;&#23376;&#21147;&#23398;&#29305;&#24615;&#30340;&#20960;&#31181;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#25105;&#20204;&#20043;&#21069;&#25552;&#20986;&#30340;&#37327;&#23376;&#39044;&#22788;&#29702;&#28388;&#27874;&#22120;&#65288;QPF&#65289;&#22312;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;QPF&#30340;&#35780;&#20272;&#65306;MNIST&#65288;&#25163;&#20889;&#25968;&#23383;&#65289;&#12289;EMNIST&#65288;&#25163;&#20889;&#25968;&#23383;&#21644;&#23383;&#27597;&#65289;&#12289;CIFAR-10&#65288;&#29031;&#29255;&#22270;&#20687;&#65289;&#21644;GTSRB&#65288;&#30495;&#23454;&#20132;&#36890;&#26631;&#24535;&#22270;&#20687;&#65289;&#12290;&#19982;&#25105;&#20204;&#20043;&#21069;&#30340;&#22810;&#31867;&#21035;&#20998;&#31867;&#32467;&#26524;&#31867;&#20284;&#65292;&#24212;&#29992;QPF&#20351;&#24471;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23545;MNIST&#12289;EMNIST&#21644;CIFAR-10&#30340;&#20108;&#20540;&#22270;&#20687;&#20998;&#31867;&#20934;&#30830;&#29575;&#20998;&#21035;&#20174;98.9%&#25552;&#39640;&#21040;99.2%&#12289;&#20174;97.8%&#25552;&#39640;&#21040;98.3%&#21644;&#20174;71.2%&#25552;&#39640;&#21040;76.1%&#65292;&#20294;&#22312;GTSRB&#19978;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#20102;&#20174;93.5%&#21040;92.0%&#12290;&#28982;&#21518;&#25105;&#20204;&#23558;QPF&#24212;&#29992;&#20110;&#35757;&#32451;&#26679;&#26412;&#25968;&#37327;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, there has been significant interest in Quantum Machine Learning (QML) among researchers, as it has the potential to transform the field of machine learning. Several models that exploit the properties of quantum mechanics have been developed for practical applications. In this study, we investigated the application of our previously proposed quantum pre-processing filter (QPF) to binary image classification. We evaluated the QPF on four datasets: MNIST (handwritten digits), EMNIST (handwritten digits and alphabets), CIFAR-10 (photographic images) and GTSRB (real-life traffic sign images). Similar to our previous multi-class classification results, the application of QPF improved the binary image classification accuracy using neural network against MNIST, EMNIST, and CIFAR-10 from 98.9% to 99.2%, 97.8% to 98.3%, and 71.2% to 76.1%, respectively, but degraded it against GTSRB from 93.5% to 92.0%. We then applied QPF in cases using a smaller number of training and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;HAGNN&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#22270;&#30340;&#28151;&#21512;&#32858;&#21512;&#26041;&#27861;&#12290;HAGNN&#21516;&#26102;&#21033;&#29992;&#20803;&#36335;&#24452;&#37051;&#23621;&#21644;&#30452;&#25509;&#36830;&#25509;&#37051;&#23621;&#36827;&#34892;&#33410;&#28857;&#32858;&#21512;&#65292;&#23558;&#25972;&#20010;&#32858;&#21512;&#36807;&#31243;&#20998;&#20026;&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#20869;&#31867;&#22411;&#32858;&#21512;&#21644;&#26080;&#20803;&#36335;&#24452;&#30340;&#36328;&#31867;&#22411;&#32858;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20840;&#38754;&#21033;&#29992;&#24322;&#26500;&#22270;&#20013;&#30340;&#31867;&#22411;&#35821;&#20041;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2307.01636</link><description>&lt;p&gt;
HAGNN: &#28151;&#21512;&#32858;&#21512;&#29992;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
HAGNN: Hybrid Aggregation for Heterogeneous Graph Neural Networks. (arXiv:2307.01636v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;HAGNN&#26694;&#26550;&#65292;&#23427;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#24322;&#26500;&#22270;&#30340;&#28151;&#21512;&#32858;&#21512;&#26041;&#27861;&#12290;HAGNN&#21516;&#26102;&#21033;&#29992;&#20803;&#36335;&#24452;&#37051;&#23621;&#21644;&#30452;&#25509;&#36830;&#25509;&#37051;&#23621;&#36827;&#34892;&#33410;&#28857;&#32858;&#21512;&#65292;&#23558;&#25972;&#20010;&#32858;&#21512;&#36807;&#31243;&#20998;&#20026;&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#20869;&#31867;&#22411;&#32858;&#21512;&#21644;&#26080;&#20803;&#36335;&#24452;&#30340;&#36328;&#31867;&#22411;&#32858;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#20840;&#38754;&#21033;&#29992;&#24322;&#26500;&#22270;&#20013;&#30340;&#31867;&#22411;&#35821;&#20041;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#22788;&#29702;&#24322;&#26500;&#22270;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#29616;&#26377;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#20803;&#36335;&#24452;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25351;&#20986;&#65292;&#31616;&#21333;&#30340;&#26080;&#20803;&#36335;&#24452;&#30340;&#21516;&#36136;&#22270;&#27169;&#22411;&#20063;&#21487;&#20197;&#21462;&#24471;&#30456;&#36817;&#30340;&#32467;&#26524;&#65292;&#36825;&#23545;&#20803;&#36335;&#24452;&#30340;&#24517;&#35201;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#22522;&#20110;&#20803;&#36335;&#24452;&#21644;&#26080;&#20803;&#36335;&#24452;&#27169;&#22411;&#20043;&#38388;&#30340;&#20869;&#22312;&#24046;&#24322;&#65292;&#21363;&#22914;&#20309;&#36873;&#25321;&#33410;&#28857;&#32858;&#21512;&#30340;&#37051;&#23621;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#26469;&#20840;&#38754;&#21033;&#29992;&#24322;&#26500;&#22270;&#20013;&#20016;&#23500;&#30340;&#31867;&#22411;&#35821;&#20041;&#20449;&#24687;&#65292;&#21363;HAGNN&#65288;&#28151;&#21512;&#32858;&#21512;&#29992;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#12290;HAGNN&#30340;&#26680;&#24515;&#26159;&#21516;&#26102;&#21033;&#29992;&#20803;&#36335;&#24452;&#37051;&#23621;&#21644;&#30452;&#25509;&#36830;&#25509;&#37051;&#23621;&#36827;&#34892;&#33410;&#28857;&#32858;&#21512;&#12290;HAGNN&#23558;&#25972;&#20010;&#32858;&#21512;&#36807;&#31243;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#22522;&#20110;&#20803;&#36335;&#24452;&#30340;&#20869;&#31867;&#22411;&#32858;&#21512;&#21644;&#26080;&#20803;&#36335;&#24452;&#30340;&#36328;&#31867;&#22411;&#32858;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks (GNNs) have been successful in handling heterogeneous graphs. In existing heterogeneous GNNs, meta-path plays an essential role. However, recent work pointed out that simple homogeneous graph model without meta-path can also achieve comparable results, which calls into question the necessity of meta-path. In this paper, we first present the intrinsic difference about meta-path-based and meta-path-free models, i.e., how to select neighbors for node aggregation. Then, we propose a novel framework to utilize the rich type semantic information in heterogeneous graphs comprehensively, namely HAGNN (Hybrid Aggregation for Heterogeneous GNNs). The core of HAGNN is to leverage the meta-path neighbors and the directly connected neighbors simultaneously for node aggregations. HAGNN divides the overall aggregation process into two phases: meta-path-based intra-type aggregation and meta-path-free inter-type aggregation. During the intra-type aggregation phase, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;</title><link>http://arxiv.org/abs/2306.17184</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#35299;&#20915;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#38382;&#39064;&#65311;&#25968;&#23398;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Why can neural language models solve next-word prediction? A mathematical perspective. (arXiv:2306.17184v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17184
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#25104;&#21151;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#19968;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#33521;&#35821;&#21477;&#23376;&#31034;&#20363;&#20013;&#25552;&#20379;&#20102;&#38646;&#38169;&#35823;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#26041;&#38754;&#35777;&#26126;&#20102;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#22312;&#24418;&#24335;&#35821;&#35328;&#29702;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#20851;&#20110;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#21487;&#20197;&#23398;&#20064;&#21040;&#32452;&#21512;&#35268;&#21017;&#30340;&#25104;&#21151;&#30340;&#20005;&#26684;&#29702;&#35770;&#35299;&#37322;&#23578;&#26410;&#34987;&#25552;&#20986;&#65292;&#22240;&#20026;&#23578;&#19981;&#28165;&#26970;&#20026;&#20160;&#20040;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#21040;&#25511;&#21046;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#30340;&#32452;&#21512;&#35268;&#21017;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31867;&#21487;&#20197;&#29992;&#26469;&#27169;&#25311;&#33521;&#35821;&#21477;&#23376;&#30340;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#24418;&#24335;&#35821;&#35328;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#20219;&#21153;&#65292;&#19988;&#38169;&#35823;&#29575;&#20026;&#38646;&#12290;&#25105;&#20204;&#30340;&#35777;&#26126;&#31361;&#20986;&#20102;&#23884;&#20837;&#23618;&#21644;&#20840;&#36830;&#25509;&#32452;&#20214;&#22312;&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, deep learning has revolutionized the field of natural language processing, with neural language models proving to be very effective for next-word prediction. However, a rigorous theoretical explanation for their success in the context of formal language theory has not yet been developed, as it is unclear why neural language models can learn the combinatorial rules that govern the next-word prediction task. In this paper, we study a class of formal languages that can be used to model real-world examples of English sentences. We construct neural language models can solve the next-word prediction task in this context with zero error. Our proof highlights the different roles of the embedding layer and the fully connected component within the neural language model.
&lt;/p&gt;</description></item><item><title>RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.15909</link><description>&lt;p&gt;
RL$^3$:&#36890;&#36807;RL&#20869;&#37096;&#30340;RL$^2$&#25552;&#21319;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
RL$^3$: Boosting Meta Reinforcement Learning via RL inside RL$^2$. (arXiv:2306.15909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15909
&lt;/p&gt;
&lt;p&gt;
RL$^3$&#26159;&#19968;&#31181;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#21040;&#30340;&#20219;&#21153;&#29305;&#23450;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#65292;&#25552;&#39640;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;meta-RL&#65289;&#26041;&#27861;&#65292;&#22914;RL$^2$&#65292;&#24050;&#32463;&#25104;&#20026;&#23398;&#20064;&#38024;&#23545;&#32473;&#23450;&#20219;&#21153;&#20998;&#24067;&#30340;&#25968;&#25454;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#38271;&#26399;&#20219;&#21153;&#21644;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26469;&#22788;&#29702;&#32463;&#39564;&#24207;&#21015;&#65292;&#32780;&#19981;&#26159;&#23558;&#23427;&#20204;&#24635;&#32467;&#20026;&#19968;&#33324;&#30340;&#24378;&#21270;&#23398;&#20064;&#32452;&#20214;&#65292;&#20363;&#22914;&#20215;&#20540;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;transformers&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#25104;&#26412;&#21464;&#24471;&#31105;&#27490;&#20043;&#21069;&#20063;&#23545;&#23427;&#20204;&#21487;&#20197;&#26377;&#25928;&#25512;&#29702;&#30340;&#21382;&#21490;&#38271;&#24230;&#26377;&#23454;&#38469;&#38480;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#19981;&#36275;&#65292;&#22240;&#20026;&#23427;&#20204;&#27809;&#26377;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#65292;&#20294;&#38543;&#30528;&#26356;&#22810;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#23427;&#20204;&#20250;&#25910;&#25947;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL$^3$&#65292;&#19968;&#31181;&#32452;&#21512;&#20102;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#28151;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36890;&#36807;&#20256;&#32479;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#21160;&#20316;&#20540;&#20316;&#20026;&#20803;&#24378;&#21270;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#20010;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta reinforcement learning (meta-RL) methods such as RL$^2$ have emerged as promising approaches for learning data-efficient RL algorithms tailored to a given task distribution. However, these RL algorithms struggle with long-horizon tasks and out-of-distribution tasks since they rely on recurrent neural networks to process the sequence of experiences instead of summarizing them into general RL components such as value functions. Moreover, even transformers have a practical limit to the length of histories they can efficiently reason about before training and inference costs become prohibitive. In contrast, traditional RL algorithms are data-inefficient since they do not leverage domain knowledge, but they do converge to an optimal policy as more data becomes available. In this paper, we propose RL$^3$, a principled hybrid approach that combines traditional RL and meta-RL by incorporating task-specific action-values learned through traditional RL as an input to the meta-RL neural netw
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;PROVEXPLAINER&#26694;&#26550;&#65292;&#36890;&#36807;&#22797;&#21046;GNN-based security models&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21033;&#29992;&#20915;&#31574;&#26641;&#21644;&#22270;&#32467;&#26500;&#29305;&#24449;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;GNN&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#35810;&#38382;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00934</link><description>&lt;p&gt;
&#22522;&#20110;&#26435;&#23041;&#22270;&#32467;&#26500;&#29305;&#24449;&#23545;&#22522;&#20110;GNN&#30340;IDS&#26816;&#27979;&#36827;&#34892;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Interpreting GNN-based IDS Detections Using Provenance Graph Structural Features. (arXiv:2306.00934v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00934
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;PROVEXPLAINER&#26694;&#26550;&#65292;&#36890;&#36807;&#22797;&#21046;GNN-based security models&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#21033;&#29992;&#20915;&#31574;&#26641;&#21644;&#22270;&#32467;&#26500;&#29305;&#24449;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20197;&#22686;&#24378;GNN&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#35810;&#38382;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#40657;&#21283;&#23376;&#26412;&#36136;&#22952;&#30861;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#22240;&#20026;&#23427;&#20204;&#32570;&#20047;&#36923;&#36753;&#35299;&#37322;&#21644;&#21487;&#25191;&#34892;&#21518;&#32493;&#34892;&#21160;&#30340;&#39044;&#27979;&#12290;&#20026;&#20102;&#22686;&#24378;&#22312;&#31995;&#32479;&#26469;&#28304;&#20998;&#26512;&#20013;&#20351;&#29992;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23433;&#20840;&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#38382;&#36131;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PROVEXPLAINER&#65292;&#19968;&#31181;&#23558;&#25277;&#35937;GNN&#20915;&#31574;&#36793;&#30028;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#29305;&#24449;&#31354;&#38388;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#31616;&#21333;&#19988;&#21487;&#35299;&#37322;&#30340;&#27169;&#22411;&#65292;&#22914;&#20915;&#31574;&#26641;&#65288;DT&#65289;&#65292;&#22797;&#21046;&#22522;&#20110;GNN&#30340;&#23433;&#20840;&#27169;&#22411;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#20026;&#20102;&#26368;&#22823;&#21270;&#26367;&#20195;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#22270;&#35770;&#30340;&#22270;&#32467;&#26500;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#23433;&#20840;&#39046;&#22495;&#30693;&#35782;&#30340;&#24191;&#27867;&#25968;&#25454;&#30740;&#31350;&#23545;&#20854;&#36827;&#34892;&#20102;&#22686;&#24378;&#12290;&#25105;&#20204;&#30340;&#22270;&#32467;&#26500;&#29305;&#24449;&#19982;&#31995;&#32479;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#38382;&#39064;&#31354;&#38388;&#34892;&#21160;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#20351;&#26816;&#27979;&#32467;&#26524;&#21487;&#29992;&#20154;&#31867;&#35821;&#35328;&#25551;&#36848;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
The black-box nature of complex Neural Network (NN)-based models has hindered their widespread adoption in security domains due to the lack of logical explanations and actionable follow-ups for their predictions. To enhance the transparency and accountability of Graph Neural Network (GNN) security models used in system provenance analysis, we propose PROVEXPLAINER, a framework for projecting abstract GNN decision boundaries onto interpretable feature spaces.  We first replicate the decision-making process of GNNbased security models using simpler and explainable models such as Decision Trees (DTs). To maximize the accuracy and fidelity of the surrogate models, we propose novel graph structural features founded on classical graph theory and enhanced by extensive data study with security domain knowledge. Our graph structural features are closely tied to problem-space actions in the system provenance domain, which allows the detection results to be explained in descriptive, human languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#20026;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02381</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Inspired Interpretability Of Machine Learning Models. (arXiv:2304.02381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#20026;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#35299;&#37322;&#33021;&#21147;&#19968;&#30452;&#26159;&#38480;&#21046;&#20854;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#22914;&#21307;&#23398;&#12289;&#32593;&#32476;&#23433;&#20840;&#25110;&#33258;&#21160;&#39550;&#39542;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21463;&#29289;&#29702;&#23398;&#39046;&#22495;&#30340;&#33021;&#37327;&#26223;&#35266;&#30740;&#31350;&#26041;&#27861;&#21551;&#21457;&#65292;&#20197;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#30456;&#20851;&#29305;&#24449;&#20197;&#20419;&#36827;&#27169;&#22411;&#20915;&#31574;&#12290;&#36890;&#36807;&#35782;&#21035;&#25439;&#22833;&#26223;&#35266;&#23616;&#37096;&#26497;&#23567;&#20540;&#32452;&#20013;&#30340;&#23432;&#24658;&#26435;&#37325;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#23384;&#22312;&#31867;&#20284;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22352;&#26631;&#19981;&#21464;&#37327;&#25110;&#26377;&#24207;&#21442;&#25968;&#26469;&#30830;&#23450;&#20998;&#23376;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#26223;&#35266;&#20013;&#27809;&#26377;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#23637;&#31034;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to explain decisions made by machine learning models remains one of the most significant hurdles towards widespread adoption of AI in highly sensitive areas such as medicine, cybersecurity or autonomous driving. Great interest exists in understanding which features of the input data prompt model decision making. In this contribution, we propose a novel approach to identify relevant features of the input data, inspired by methods from the energy landscapes field, developed in the physical sciences. By identifying conserved weights within groups of minima of the loss landscapes, we can identify the drivers of model decision making. Analogues to this idea exist in the molecular sciences, where coordinate invariants or order parameters are employed to identify critical features of a molecule. However, no such approach exists for machine learning loss landscapes. We will demonstrate the applicability of energy landscape methods to machine learning models and give examples, both 
&lt;/p&gt;</description></item></channel></rss>