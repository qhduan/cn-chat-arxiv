<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01401</link><description>&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#22312;&#35268;&#27169;&#19978;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Machine Unlearning at Scale via Lipschitz Regularization
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01401
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Lipschitz&#27491;&#21017;&#21270;&#23454;&#29616;&#38646;&#26679;&#26412;&#26426;&#22120;&#36951;&#24536;&#65292;&#21487;&#20197;&#21450;&#26102;&#24536;&#35760;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#36981;&#23432;&#20154;&#24037;&#26234;&#33021;&#21644;&#25968;&#25454;&#35268;&#23450;&#65292;&#20174;&#35757;&#32451;&#24471;&#21040;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#36951;&#24536;&#31169;&#20154;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36951;&#24536;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#21450;&#26102;&#24536;&#35760;&#24517;&#35201;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38646;&#26679;&#26412;&#36951;&#24536;&#30340;&#22330;&#26223;&#65292;&#21363;&#21482;&#26377;&#19968;&#20010;&#32463;&#36807;&#35757;&#32451;&#30340;&#27169;&#22411;&#21644;&#35201;&#36951;&#24536;&#30340;&#25968;&#25454;&#65292;&#36951;&#24536;&#31639;&#27861;&#24517;&#39035;&#33021;&#22815;&#31227;&#38500;&#25968;&#25454;&#12290;&#26681;&#25454;&#36825;&#26679;&#23450;&#20041;&#65292;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26159;&#19981;&#22815;&#30340;&#12290;&#22522;&#20110;Lipschitz&#36830;&#32493;&#24615;&#30340;&#27010;&#24565;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#26679;&#26412;&#25200;&#21160;&#30340;&#36755;&#20986;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#26469;&#35825;&#23548;&#36951;&#24536;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#24179;&#28369;&#24615;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#36951;&#24536;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#24635;&#20307;&#27169;&#22411;&#24615;&#33021;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#65292;&#21253;&#25324;&#19968;&#31995;&#21015;&#24403;&#20195;&#22522;&#20934;&#27979;&#35797;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20005;&#26684;&#30340;&#38646;&#26679;&#26412;&#32422;&#26463;&#19979;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To comply with AI and data regulations, the need to forget private or copyrighted information from trained machine learning models is increasingly important. The key challenge in unlearning is forgetting the necessary data in a timely manner, while preserving model performance. In this work, we address the zero-shot unlearning scenario, whereby an unlearning algorithm must be able to remove data given only a trained model and the data to be forgotten. Under such a definition, existing state-of-the-art methods are insufficient. Building on the concepts of Lipschitz continuity, we present a method that induces smoothing of the forget sample's output, with respect to perturbations of that sample. We show this smoothing successfully results in forgetting while preserving general model performance. We perform extensive empirical evaluation of our method over a range of contemporary benchmarks, verifying that our method achieves state-of-the-art performance under the strict constraints of ze
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02595</link><description>&lt;p&gt;
QFNN-FFD&#65306;&#29992;&#20110;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#30340;&#37327;&#23376;&#32852;&#37030;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
QFNN-FFD: Quantum Federated Neural Network for Financial Fraud Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02595
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#23558;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#26694;&#26550;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#27450;&#35784;&#20132;&#26131;&#35782;&#21035;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD)&#65292;&#36825;&#26159;&#19968;&#20010;&#34701;&#21512;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21644;&#37327;&#23376;&#35745;&#31639;&#25216;&#26415;&#19982;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#21069;&#27839;&#26694;&#26550;&#65292;&#29992;&#20110;&#21019;&#26032;&#37329;&#34701;&#27450;&#35784;&#26816;&#27979;&#12290;&#21033;&#29992;&#37327;&#23376;&#25216;&#26415;&#30340;&#35745;&#31639;&#33021;&#21147;&#21644;FL&#30340;&#25968;&#25454;&#38544;&#31169;&#65292;QFNN-FFD&#25552;&#20986;&#20102;&#19968;&#31181;&#23433;&#20840;&#12289;&#39640;&#25928;&#30340;&#35782;&#21035;&#27450;&#35784;&#20132;&#26131;&#30340;&#26041;&#27861;&#12290;&#22312;&#20998;&#24067;&#24335;&#23458;&#25143;&#31471;&#23454;&#26045;&#21452;&#38454;&#27573;&#35757;&#32451;&#27169;&#22411;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#24615;&#33021;&#26041;&#27861;&#12290;QFNN-FFD&#26174;&#33879;&#25913;&#36827;&#20102;&#27450;&#35784;&#26816;&#27979;&#24182;&#30830;&#20445;&#20102;&#25968;&#25454;&#26426;&#23494;&#24615;&#65292;&#26631;&#24535;&#30528;&#37329;&#34701;&#31185;&#25216;&#35299;&#20915;&#26041;&#26696;&#30340;&#37325;&#22823;&#36827;&#27493;&#65292;&#24182;&#20026;&#20197;&#38544;&#31169;&#20026;&#37325;&#28857;&#30340;&#27450;&#35784;&#26816;&#27979;&#24314;&#31435;&#20102;&#26032;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02595v1 Announce Type: cross  Abstract: This study introduces the Quantum Federated Neural Network for Financial Fraud Detection (QFNN-FFD), a cutting-edge framework merging Quantum Machine Learning (QML) and quantum computing with Federated Learning (FL) to innovate financial fraud detection. Using quantum technologies' computational power and FL's data privacy, QFNN-FFD presents a secure, efficient method for identifying fraudulent transactions. Implementing a dual-phase training model across distributed clients surpasses existing methods in performance. QFNN-FFD significantly improves fraud detection and ensures data confidentiality, marking a significant advancement in fintech solutions and establishing a new standard for privacy-focused fraud detection.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01245</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#26694;&#26550;: &#26530;&#36724;&#12289;&#26816;&#27979;&#25928;&#29575;&#21644;&#26368;&#20248;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework of Watermarks for Large Language Models: Pivot, Detection Efficiency and Optimal Rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01245
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#21644;&#26816;&#27979;&#35268;&#21017;&#65292;&#36890;&#36807;&#20851;&#38190;&#32479;&#35745;&#37327;&#21644;&#31192;&#23494;&#23494;&#38053;&#25511;&#21046;&#35823;&#25253;&#29575;&#65292;&#21516;&#26102;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;ChatGPT&#20110;2022&#24180;11&#26376;&#25512;&#20986;&#20197;&#26469;&#65292;&#23558;&#20960;&#20046;&#19981;&#21487;&#23519;&#35273;&#30340;&#32479;&#35745;&#20449;&#21495;&#23884;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#65292;&#20063;&#34987;&#31216;&#20026;&#27700;&#21360;&#65292;&#24050;&#34987;&#29992;&#20316;&#20174;&#20854;&#20154;&#31867;&#25776;&#20889;&#23545;&#24212;&#29289;&#19978;&#21487;&#35777;&#26816;&#27979;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#12290; &#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25512;&#29702;&#27700;&#21360;&#30340;&#32479;&#35745;&#25928;&#29575;&#24182;&#35774;&#35745;&#24378;&#22823;&#30340;&#26816;&#27979;&#35268;&#21017;&#12290;&#21463;&#27700;&#21360;&#26816;&#27979;&#30340;&#20551;&#35774;&#26816;&#39564;&#20844;&#24335;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#39318;&#20808;&#36873;&#25321;&#25991;&#26412;&#30340;&#26530;&#36724;&#32479;&#35745;&#37327;&#21644;&#30001;LLM&#25552;&#20379;&#32473;&#39564;&#35777;&#22120;&#30340;&#31192;&#23494;&#23494;&#38053;&#65292;&#20197;&#23454;&#29616;&#25511;&#21046;&#35823;&#25253;&#29575;&#65288;&#23558;&#20154;&#31867;&#25776;&#20889;&#30340;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;LLM&#29983;&#25104;&#30340;&#38169;&#35823;&#65289;&#12290; &#25509;&#19979;&#26469;&#65292;&#35813;&#26694;&#26550;&#20801;&#35768;&#36890;&#36807;&#33719;&#21462;&#28176;&#36817;&#38169;&#35823;&#36127;&#29575;&#65288;&#23558;LLM&#29983;&#25104;&#25991;&#26412;&#38169;&#35823;&#22320;&#26816;&#27979;&#20026;&#20154;&#31867;&#25776;&#20889;&#30340;&#38169;&#35823;&#65289;&#30340;&#23553;&#38381;&#24418;&#24335;&#34920;&#36798;&#24335;&#26469;&#35780;&#20272;&#27700;&#21360;&#26816;&#27979;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01245v1 Announce Type: cross  Abstract: Since ChatGPT was introduced in November 2022, embedding (nearly) unnoticeable statistical signals into text generated by large language models (LLMs), also known as watermarking, has been used as a principled approach to provable detection of LLM-generated text from its human-written counterpart. In this paper, we introduce a general and flexible framework for reasoning about the statistical efficiency of watermarks and designing powerful detection rules. Inspired by the hypothesis testing formulation of watermark detection, our framework starts by selecting a pivotal statistic of the text and a secret key -- provided by the LLM to the verifier -- to enable controlling the false positive rate (the error of mistakenly detecting human-written text as LLM-generated). Next, this framework allows one to evaluate the power of watermark detection rules by obtaining a closed-form expression of the asymptotic false negative rate (the error of 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#22312;&#26102;&#38388;&#31232;&#30095;&#21327;&#35843;&#22270;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#20851;&#31995;&#24182;&#21033;&#29992;&#21382;&#21490;&#35266;&#27979;&#26469;&#36827;&#34892;&#30693;&#35782;&#20132;&#25442;</title><link>https://arxiv.org/abs/2403.19253</link><description>&lt;p&gt;
&#25512;&#26029;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#22312;&#26102;&#38388;&#31232;&#30095;&#21327;&#35843;&#22270;
&lt;/p&gt;
&lt;p&gt;
Inferring Latent Temporal Sparse Coordination Graph for Multi-Agent Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19253
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#28508;&#22312;&#26102;&#38388;&#31232;&#30095;&#21327;&#35843;&#22270;&#65292;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#20851;&#31995;&#24182;&#21033;&#29992;&#21382;&#21490;&#35266;&#27979;&#26469;&#36827;&#34892;&#30693;&#35782;&#20132;&#25442;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#26234;&#33021;&#20307;&#21327;&#35843;&#23545;&#20110;&#21512;&#20316;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#33267;&#20851;&#37325;&#35201;&#12290;&#24403;&#21069;MARL&#20013;&#30340;&#22270;&#23398;&#20064;&#26041;&#27861;&#23616;&#38480;&#24615;&#36739;&#22823;&#65292;&#20165;&#20165;&#20381;&#36182;&#19968;&#27493;&#35266;&#23519;&#65292;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#21382;&#21490;&#32463;&#39564;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#22270;&#23384;&#22312;&#32570;&#38519;&#65292;&#20419;&#36827;&#20102;&#20887;&#20313;&#25110;&#26377;&#23475;&#20449;&#24687;&#20132;&#25442;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25512;&#26029;&#29992;&#20110;MARL&#30340;&#28508;&#22312;&#26102;&#38388;&#31232;&#30095;&#21327;&#35843;&#22270;&#65288;LTS-CG&#65289;&#12290;LTS-CG&#21033;&#29992;&#26234;&#33021;&#20307;&#30340;&#21382;&#21490;&#35266;&#27979;&#26469;&#35745;&#31639;&#26234;&#33021;&#20307;&#23545;&#27010;&#29575;&#30697;&#38453;&#65292;&#20174;&#20013;&#25277;&#21462;&#31232;&#30095;&#22270;&#24182;&#29992;&#20110;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30693;&#35782;&#20132;&#25442;&#65292;&#20174;&#32780;&#21516;&#26102;&#25429;&#25417;&#26234;&#33021;&#20307;&#30340;&#20381;&#36182;&#20851;&#31995;&#21644;&#20851;&#31995;&#19981;&#30830;&#23450;&#24615;&#12290;&#35813;&#36807;&#31243;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#20165;&#19982;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19253v1 Announce Type: new  Abstract: Effective agent coordination is crucial in cooperative Multi-Agent Reinforcement Learning (MARL). While agent cooperation can be represented by graph structures, prevailing graph learning methods in MARL are limited. They rely solely on one-step observations, neglecting crucial historical experiences, leading to deficient graphs that foster redundant or detrimental information exchanges. Additionally, high computational demands for action-pair calculations in dense graphs impede scalability. To address these challenges, we propose inferring a Latent Temporal Sparse Coordination Graph (LTS-CG) for MARL. The LTS-CG leverages agents' historical observations to calculate an agent-pair probability matrix, where a sparse graph is sampled from and used for knowledge exchange between agents, thereby simultaneously capturing agent dependencies and relation uncertainty. The computational complexity of this procedure is only related to the number o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;</title><link>https://arxiv.org/abs/2403.19211</link><description>&lt;p&gt;
&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#25552;&#20379;&#21452;&#37325;&#20010;&#24615;&#21270;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Dual-Personalizing Adapter for Federated Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19211
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#19981;&#20165;&#20851;&#27880;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#36890;&#36807;&#24494;&#35843;&#22823;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#65292;&#23637;&#29616;&#20986;&#20102;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#22312;&#20998;&#24067;&#24335;&#23398;&#20064;&#65288;FL&#65289;&#29615;&#22659;&#19979;&#36890;&#36807;&#21033;&#29992;&#35768;&#22810;&#20998;&#24067;&#24335;&#25968;&#25454;&#38598;&#36827;&#34892;&#21327;&#20316;&#24494;&#35843;&#27169;&#22411;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#36890;&#20449;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#24341;&#20837;&#20102;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#24182;&#19988;&#19968;&#20123;&#30740;&#31350;&#23558;&#20010;&#24615;&#21270;&#26041;&#27861;&#35843;&#25972;&#20026;&#32852;&#37030;&#22522;&#37329;&#20250;&#27169;&#22411;&#65292;&#20197;&#33719;&#24471;&#26356;&#22909;&#30340;&#29992;&#25143;&#20559;&#22909;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#21475;&#26159;&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#24573;&#30053;&#20102;&#27979;&#35797;&#26102;&#38388;&#20998;&#24067;&#36716;&#31227;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35774;&#32622;&#65292;&#31216;&#20026;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#65292;&#23427;&#19981;&#20165;&#19987;&#27880;&#20110;&#30446;&#26631;&#26412;&#22320;&#20219;&#21153;&#65292;&#36824;&#24310;&#20280;&#21040;&#20854;&#20182;&#23637;&#31034;&#27979;&#35797;&#26102;&#38388;&#20010;&#24615;&#21270;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19211v1 Announce Type: cross  Abstract: Recently, foundation models, particularly large language models (LLMs), have demonstrated an impressive ability to adapt to various tasks by fine-tuning large amounts of instruction data. Notably, federated foundation models emerge as a privacy preservation method to fine-tune models collaboratively under federated learning (FL) settings by leveraging many distributed datasets with non-IID data. To alleviate communication and computation overhead, parameter-efficient methods are introduced for efficiency, and some research adapted personalization methods to federated foundation models for better user preferences alignment. However, a critical gap in existing research is the neglect of test-time distribution shifts in real-world applications. Therefore, to bridge this gap, we propose a new setting, termed test-time personalization, which not only concentrates on the targeted local task but also extends to other tasks that exhibit test-t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Decoupled VFL&#65288;DVFL&#65289;&#65292;&#19968;&#31181;&#38754;&#21521;VFL&#30340;&#20998;&#27573;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#32858;&#21512;&#21644;&#38548;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23481;&#38169;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03871</link><description>&lt;p&gt;
&#38754;&#21521;&#22402;&#30452;&#20998;&#21306;&#25968;&#25454;&#30340;&#35299;&#32806;&#24335;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65292;&#29992;&#20110;&#23454;&#38469;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Decoupled Vertical Federated Learning for Practical Training on Vertically Partitioned Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03871
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Decoupled VFL&#65288;DVFL&#65289;&#65292;&#19968;&#31181;&#38754;&#21521;VFL&#30340;&#20998;&#27573;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#20998;&#25955;&#32858;&#21512;&#21644;&#38548;&#31163;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#23481;&#38169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20849;&#21516;&#23454;&#20307;&#30340;&#19981;&#21516;&#29305;&#24449;&#25152;&#26377;&#32773;&#21512;&#20316;&#23398;&#20064;&#20840;&#23616;&#27169;&#22411;&#32780;&#26080;&#38656;&#20849;&#20139;&#25968;&#25454;&#12290;&#22312;VFL&#20013;&#65292;&#20027;&#26426;&#23458;&#25143;&#31471;&#25317;&#26377;&#27599;&#20010;&#23454;&#20307;&#30340;&#25968;&#25454;&#26631;&#31614;&#65292;&#24182;&#22522;&#20110;&#25152;&#26377;&#23458;&#25143;&#31471;&#30340;&#20013;&#38388;&#26412;&#22320;&#34920;&#31034;&#23398;&#20064;&#26368;&#32456;&#34920;&#31034;&#12290;&#22240;&#27492;&#65292;&#20027;&#26426;&#26159;&#19968;&#20010;&#21333;&#28857;&#25925;&#38556;&#65292;&#26631;&#31614;&#21453;&#39304;&#21487;&#20197;&#34987;&#24694;&#24847;&#23458;&#25143;&#31471;&#29992;&#26469;&#25512;&#26029;&#31169;&#26377;&#29305;&#24449;&#12290;&#35201;&#27714;&#25152;&#26377;&#21442;&#19982;&#32773;&#22312;&#25972;&#20010;&#35757;&#32451;&#36807;&#31243;&#20013;&#20445;&#25345;&#27963;&#36291;&#21644;&#20540;&#24471;&#20449;&#36182;&#36890;&#24120;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#22312;&#21463;&#25511;&#29615;&#22659;&#20043;&#22806;&#23436;&#20840;&#19981;&#21487;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;VFL&#30340;&#20998;&#27573;&#23398;&#20064;&#26041;&#27861;Decoupled VFL&#65288;DVFL&#65289;&#12290;&#36890;&#36807;&#22312;&#21508;&#33258;&#30340;&#30446;&#26631;&#19978;&#35757;&#32451;&#27599;&#20010;&#27169;&#22411;&#65292;DVFL&#20801;&#35768;&#29305;&#24449;&#23398;&#20064;&#21644;&#26631;&#31614;&#30417;&#30563;&#20043;&#38388;&#30340;&#20998;&#25955;&#32858;&#21512;&#21644;&#38548;&#31163;&#12290;&#20855;&#26377;&#36825;&#20123;&#23646;&#24615;&#65292;DVFL&#20855;&#26377;&#23481;&#38169;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03871v1 Announce Type: new  Abstract: Vertical Federated Learning (VFL) is an emergent distributed machine learning paradigm wherein owners of disjoint features of a common set of entities collaborate to learn a global model without sharing data. In VFL, a host client owns data labels for each entity and learns a final representation based on intermediate local representations from all guest clients. Therefore, the host is a single point of failure and label feedback can be used by malicious guest clients to infer private features. Requiring all participants to remain active and trustworthy throughout the entire training process is generally impractical and altogether infeasible outside of controlled environments. We propose Decoupled VFL (DVFL), a blockwise learning approach to VFL. By training each model on its own objective, DVFL allows for decentralized aggregation and isolation between feature learning and label supervision. With these properties, DVFL is fault tolerant
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#28151;&#21512;&#24577;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QMSAN&#65289;&#65292;&#32467;&#21512;&#20102;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#22788;&#29702;NLP&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.02871</link><description>&lt;p&gt;
&#37327;&#23376;&#28151;&#21512;&#24577;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Quantum Mixed-State Self-Attention Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02871
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#28151;&#21512;&#24577;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QMSAN&#65289;&#65292;&#32467;&#21512;&#20102;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#21644;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#20197;&#22686;&#24378;&#22788;&#29702;NLP&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#35745;&#31639;&#30340;&#24555;&#36895;&#21457;&#23637;&#36234;&#26469;&#36234;&#31361;&#20986;&#20102;&#20854;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65288;QML&#65289;&#21033;&#29992;&#37327;&#23376;&#35745;&#31639;&#30340;&#29420;&#29305;&#33021;&#21147;&#20026;&#22797;&#26434;&#25968;&#25454;&#22788;&#29702;&#21644;&#27169;&#24335;&#35782;&#21035;&#25361;&#25112;&#25552;&#20379;&#26032;&#39062;&#30340;&#35270;&#35282;&#21644;&#26041;&#27861;&#35770;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#37327;&#23376;&#28151;&#21512;&#24577;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;QMSAN&#65289;&#65292;&#23427;&#23558;&#37327;&#23376;&#35745;&#31639;&#21407;&#29702;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#65292;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22788;&#29702;NLP&#20219;&#21153;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;QMSAN&#27169;&#22411;&#37319;&#29992;&#22522;&#20110;&#28151;&#21512;&#24577;&#30340;&#37327;&#23376;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#39046;&#22495;&#20869;&#26597;&#35810;&#21644;&#38190;&#20043;&#38388;&#30456;&#20284;&#24615;&#30340;&#39640;&#25928;&#30452;&#25509;&#20272;&#35745;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#33719;&#21462;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#37327;&#23376; posit
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02871v1 Announce Type: cross  Abstract: The rapid advancement of quantum computing has increasingly highlighted its potential in the realm of machine learning, particularly in the context of natural language processing (NLP) tasks. Quantum machine learning (QML) leverages the unique capabilities of quantum computing to offer novel perspectives and methodologies for complex data processing and pattern recognition challenges. This paper introduces a novel Quantum Mixed-State Attention Network (QMSAN), which integrates the principles of quantum computing with classical machine learning algorithms, especially self-attention networks, to enhance the efficiency and effectiveness in handling NLP tasks. QMSAN model employs a quantum attention mechanism based on mixed states, enabling efficient direct estimation of similarity between queries and keys within the quantum domain, leading to more effective attention weight acquisition. Additionally, we propose an innovative quantum posit
&lt;/p&gt;</description></item><item><title>Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.14776</link><description>&lt;p&gt;
2D Matryoshka&#21477;&#23376;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
2D Matryoshka Sentence Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14776
&lt;/p&gt;
&lt;p&gt;
Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#20197;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#26356;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#65292;&#20174;&#32780;&#21152;&#24555;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032;  &#25688;&#35201;&#65306;&#24120;&#35265;&#26041;&#27861;&#20381;&#36182;&#20110;&#20174;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#22266;&#23450;&#38271;&#24230;&#30340;&#23884;&#20837;&#21521;&#37327;&#20316;&#20026;&#21477;&#23376;&#23884;&#20837;&#65292;&#29992;&#20110;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24615;&#65288;STS&#65289;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#30001;&#20110;&#22312;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#23384;&#22312;&#26410;&#30693;&#30340;&#35745;&#31639;&#32422;&#26463;&#21644;&#39044;&#31639;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#28789;&#27963;&#24615;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;Matryoshka&#34920;&#31034;&#23398;&#20064;(MRL)(Kusupati&#31561;&#20154;&#65292;2022)&#20197;&#26356;&#32454;&#31890;&#24230;&#22320;&#32534;&#30721;&#20449;&#24687;&#65292;&#21363;&#20351;&#29992;&#36739;&#20302;&#30340;&#23884;&#20837;&#32500;&#24230;&#65292;&#20197;&#33258;&#36866;&#24212;&#22320;&#36866;&#24212;&#20020;&#26102;&#20219;&#21153;&#12290;&#21487;&#20197;&#36890;&#36807;&#36739;&#23567;&#30340;&#23884;&#20837;&#22823;&#23567;&#36798;&#21040;&#31867;&#20284;&#30340;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#21152;&#24555;&#19979;&#28216;&#20219;&#21153;&#12290;&#23613;&#31649;&#20854;&#25913;&#36827;&#20102;&#25928;&#29575;&#65292;MRL&#20173;&#35201;&#22312;&#33719;&#24471;&#23884;&#20837;&#20043;&#21069;&#36941;&#21382;&#25152;&#26377;Transformer&#23618;&#65292;&#36825;&#20173;&#28982;&#26159;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#36825;&#24341;&#21457;&#20102;&#26159;&#21542;&#22266;&#23450;&#25968;&#37327;&#30340;Transformer&#23618;&#20250;&#24433;&#21709;&#34920;&#31034;&#36136;&#37327;&#20197;&#21450;&#20351;&#29992;&#20013;&#38388;&#23618;&#36827;&#34892;&#21477;&#23376;&#34920;&#31034;&#26159;&#21542;&#21487;&#34892;&#30340;&#32771;&#34385;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14776v1 Announce Type: new  Abstract: Common approaches rely on fixed-length embedding vectors from language models as sentence embeddings for downstream tasks such as semantic textual similarity (STS). Such methods are limited in their flexibility due to unknown computational constraints and budgets across various applications. Matryoshka Representation Learning (MRL) (Kusupati et al., 2022) encodes information at finer granularities, i.e., with lower embedding dimensions, to adaptively accommodate ad hoc tasks. Similar accuracy can be achieved with a smaller embedding size, leading to speedups in downstream tasks. Despite its improved efficiency, MRL still requires traversing all Transformer layers before obtaining the embedding, which remains the dominant factor in time and memory consumption. This prompts consideration of whether the fixed number of Transformer layers affects representation quality and whether using intermediate layers for sentence representation is feas
&lt;/p&gt;</description></item><item><title>&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#20250;&#23436;&#20840;&#22833;&#36133;&#65292;&#25581;&#31034;&#20986;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.12527</link><description>&lt;p&gt;
&#31163;&#32447;&#27169;&#22411;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36793;&#32536;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Edge-of-Reach Problem in Offline Model-Based Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12527
&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#26102;&#65292;&#29616;&#26377;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#20250;&#23436;&#20840;&#22833;&#36133;&#65292;&#25581;&#31034;&#20986;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20174;&#39044;&#20808;&#25910;&#38598;&#30340;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#32780;&#65292;&#30001;&#27492;&#24102;&#26469;&#20102;&#19968;&#20010;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#21363;&#20272;&#35745;&#25968;&#25454;&#38598;&#20013;&#26410;&#28085;&#30422;&#30340;&#34892;&#20026;&#30340;&#20215;&#20540;&#12290;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#36890;&#36807;&#20801;&#35768;&#26234;&#33021;&#20307;&#36890;&#36807;&#22312;&#23398;&#20064;&#21160;&#21147;&#23398;&#27169;&#22411;&#20013;&#36827;&#34892;&#23637;&#24320;&#36827;&#34892;&#25910;&#38598;&#39069;&#22806;&#30340;&#21512;&#25104;&#25968;&#25454;&#26469;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34987;&#30495;&#23454;&#19988;&#26080;&#35823;&#24046;&#30340;&#21160;&#21147;&#23398;&#26367;&#20195;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#39537;&#21160;&#26041;&#27861;&#23558;&#23436;&#20840;&#22833;&#36133;&#12290;&#36825;&#25581;&#31034;&#20102;&#19968;&#20010;&#37325;&#22823;&#35823;&#35299;&#12290;&#25105;&#20204;&#30340;&#21518;&#32493;&#35843;&#26597;&#21457;&#29616;&#65292;&#27169;&#22411;&#39537;&#21160;&#31639;&#27861;&#20013;&#20351;&#29992;&#30340;&#19968;&#33324;&#36807;&#31243;&#23548;&#33268;&#23384;&#22312;&#19968;&#32452;&#35302;&#21457;&#30149;&#24577;&#20540;&#36807;&#39640;&#30340;&#36793;&#32536;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12527v1 Announce Type: cross  Abstract: Offline reinforcement learning aims to enable agents to be trained from pre-collected datasets, however, this comes with the added challenge of estimating the value of behavior not covered in the dataset. Model-based methods offer a solution by allowing agents to collect additional synthetic data via rollouts in a learned dynamics model. The prevailing theoretical understanding is that this can then be viewed as online reinforcement learning in an approximate dynamics model, and any remaining gap is therefore assumed to be due to the imperfect dynamics model. Surprisingly, however, we find that if the learned dynamics model is replaced by the true error-free dynamics, existing model-based methods completely fail. This reveals a major misconception. Our subsequent investigation finds that the general procedure used in model-based algorithms results in the existence of a set of edge-of-reach states which trigger pathological value overes
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11858</link><description>&lt;p&gt;
&#22312;&#26446;&#32676;&#19978;&#30340;&#38543;&#26426;Hessian&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Hessian Fitting on Lie Group
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#12290;&#20351;&#29992;&#20102;&#19968;&#20010;Hessian&#25311;&#21512;&#20934;&#21017;&#65292;&#21487;&#29992;&#20110;&#25512;&#23548;&#22823;&#37096;&#20998;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;BFGS&#12289;&#39640;&#26031;&#29275;&#39039;&#12289;AdaGrad&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#19981;&#21516;&#25910;&#25947;&#36895;&#29575;&#65292;&#20363;&#22914;&#65292;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#21644;&#23545;&#31216;&#27491;&#23450;&#65288;SPL&#65289;&#30697;&#38453;&#21644;&#26576;&#20123;&#26446;&#32676;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#32447;&#24615;&#36895;&#29575;&#12290;&#22312;&#29305;&#23450;&#19988;&#36275;&#22815;&#19968;&#33324;&#30340;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#34987;&#35777;&#26126;&#26159;&#24378;&#20984;&#30340;&#12290;&#20026;&#20102;&#30830;&#35748;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22914;&#26377;&#22122;&#22768;&#30340;Hessian-&#21521;&#37327;&#20056;&#31215;&#12289;&#26102;&#21464;&#30340;Hessians&#21644;&#20302;&#31934;&#24230;&#31639;&#26415;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20381;&#36182;&#20110;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#30340;&#26041;&#27861;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;5G&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#20013;&#20849;&#20139;&#25968;&#25454;&#24211;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27927;&#29260;&#30340;&#21487;&#23398;&#20064;&#21152;&#23494;&#25216;&#26415;&#26469;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;</title><link>https://arxiv.org/abs/2402.09710</link><description>&lt;p&gt;
&#22312;&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#20013;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#24212;&#29992;&#30340;&#25968;&#25454;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Preserving Data Privacy for ML-driven Applications in Open Radio Access Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09710
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;5G&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#20013;&#20849;&#20139;&#25968;&#25454;&#24211;&#22330;&#26223;&#19979;&#30340;&#25968;&#25454;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27927;&#29260;&#30340;&#21487;&#23398;&#20064;&#21152;&#23494;&#25216;&#26415;&#26469;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25968;&#25454;&#38544;&#31169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#25913;&#36827;&#39057;&#35889;&#35775;&#38382;&#25216;&#26415;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#26469;&#31649;&#29702;&#21644;&#20849;&#20139;&#26377;&#38480;&#30340;&#39057;&#35889;&#36164;&#28304;&#65292;&#29992;&#20110;&#26032;&#20852;&#24212;&#29992;&#12290;&#23545;&#20110;&#20854;&#20013;&#20960;&#31181;&#24212;&#29992;&#65292;&#25935;&#24863;&#30340;&#26080;&#32447;&#25968;&#25454;&#65288;&#22914;&#39057;&#35889;&#22270;&#65289;&#23384;&#20648;&#22312;&#20849;&#20139;&#25968;&#25454;&#24211;&#25110;&#22810;&#26041;&#21033;&#30410;&#30456;&#20851;&#32773;&#20113;&#29615;&#22659;&#20013;&#65292;&#22240;&#27492;&#23481;&#26131;&#36896;&#25104;&#38544;&#31169;&#27844;&#28431;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#30740;&#31350;5G&#24320;&#25918;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#20013;&#20849;&#20139;&#25968;&#25454;&#24211;&#22330;&#26223;&#30340;&#20856;&#22411;&#26696;&#20363;&#26469;&#35299;&#20915;&#27492;&#31867;&#38544;&#31169;&#38382;&#39064;&#65292;&#22312;&#36825;&#20123;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#22312;&#36817;&#23454;&#26102;&#65288;near-RT&#65289;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#26234;&#33021;&#25511;&#21046;&#22120;&#20013;&#26377;&#19968;&#20010;&#20849;&#20139;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#30528;&#37325;&#35752;&#35770;&#20102;&#22914;&#20309;&#20445;&#25252;&#29992;&#20110;&#39057;&#35889;&#20849;&#20139;&#21644;&#24178;&#25200;&#32531;&#35299;&#24212;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#25968;&#25454;&#65292;&#21516;&#26102;&#19981;&#24433;&#21709;&#27169;&#22411;&#21644;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#20854;&#20013;&#30340;&#22522;&#26412;&#24819;&#27861;&#26159;&#21033;&#29992;&#22522;&#20110;&#27927;&#29260;&#30340;&#21487;&#23398;&#20064;&#21152;&#23494;&#25216;&#26415;&#26469;&#21152;&#23494;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09710v1 Announce Type: cross  Abstract: Deep learning offers a promising solution to improve spectrum access techniques by utilizing data-driven approaches to manage and share limited spectrum resources for emerging applications. For several of these applications, the sensitive wireless data (such as spectrograms) are stored in a shared database or multistakeholder cloud environment and are therefore prone to privacy leaks. This paper aims to address such privacy concerns by examining the representative case study of shared database scenarios in 5G Open Radio Access Network (O-RAN) networks where we have a shared database within the near-real-time (near-RT) RAN intelligent controller. We focus on securing the data that can be used by machine learning (ML) models for spectrum sharing and interference mitigation applications without compromising the model and network performances. The underlying idea is to leverage a (i) Shuffling-based learnable encryption technique to encryp
&lt;/p&gt;</description></item><item><title>DoorINet&#26159;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.09427</link><description>&lt;p&gt;
DoorINet: &#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DoorINet: A Deep-Learning Inertial Framework for Door-Mounted IoT Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09427
&lt;/p&gt;
&lt;p&gt;
DoorINet&#26159;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#29289;&#32852;&#32593;&#24212;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#24815;&#24615;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#29289;&#32852;&#32593;&#24212;&#29992;&#20351;&#29992;&#20302;&#25104;&#26412;&#30340;&#24494;&#22411;&#30005;&#21160;&#26426;&#26800;&#24815;&#24615;&#20256;&#24863;&#22120;&#65292;&#20854;&#20013;&#19968;&#20010;&#24120;&#35265;&#30340;&#20219;&#21153;&#26159;&#26041;&#21521;&#20272;&#35745;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#31181;&#20219;&#21153;&#65292;&#24212;&#29992;&#23039;&#24577;&#21644;&#33322;&#21521;&#21442;&#32771;&#31995;&#32479;&#31639;&#27861;&#12290;&#21033;&#29992;&#38464;&#34746;&#20202;&#35835;&#25968;&#65292;&#36890;&#36807;&#21152;&#36895;&#24230;&#35745;&#35835;&#25968;&#26356;&#26032;&#23039;&#24577;&#35282;&#24230;&#65292;&#21033;&#29992;&#30913;&#21147;&#35745;&#27979;&#37327;&#26356;&#26032;&#33322;&#21521;&#35282;&#24230;&#12290;&#22312;&#23460;&#20869;&#29615;&#22659;&#20013;&#65292;&#30913;&#21147;&#35745;&#21463;&#21040;&#24178;&#25200;&#65292;&#20250;&#38477;&#20302;&#20854;&#24615;&#33021;&#12290;&#36825;&#20027;&#35201;&#24433;&#21709;&#21040;&#20272;&#35745;&#33322;&#21521;&#35282;&#24230;&#30340;&#24212;&#29992;&#65292;&#27604;&#22914;&#25214;&#21040;&#34915;&#26588;&#25110;&#20912;&#31665;&#38376;&#30340;&#33322;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DoorINet&#65292;&#19968;&#31181;&#29992;&#20110;&#38376;&#36148;&#24335;&#20302;&#25104;&#26412;&#24815;&#24615;&#20256;&#24863;&#22120;&#30340;&#31471;&#21040;&#31471;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#26080;&#38656;&#20351;&#29992;&#30913;&#21147;&#35745;&#21363;&#21487;&#35745;&#31639;&#33322;&#21521;&#35282;&#24230;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#35760;&#24405;&#20102;&#19968;&#20010;&#21253;&#21547;391&#20998;&#38047;&#21152;&#36895;&#24230;&#35745;&#21644;&#38464;&#34746;&#20202;&#27979;&#37327;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09427v1 Announce Type: cross  Abstract: Many Internet of Things applications utilize low-cost, micro, electro-mechanical inertial sensors. A common task is orientation estimation. To tackle such a task, attitude and heading reference system algorithms are applied. Relying on the gyroscope readings, the accelerometer readings are used to update the attitude angles, and magnetometer measurements are utilized to update the heading angle. In indoor environments, magnetometers suffer from interference that degrades their performance. This mainly influences applications focused on estimating the heading angle like finding the heading angle of a closet or fridge door. To circumvent such situations, we propose DoorINet, an end-to-end deep-learning framework to calculate the heading angle from door-mounted, low-cost inertial sensors without using magnetometers. To evaluate our approach, we record a unique dataset containing 391 minutes of accelerometer and gyroscope measurements and 
&lt;/p&gt;</description></item><item><title>&#20462;&#27491;&#20102;&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20013;&#30340;&#38169;&#35823;&#23616;&#37096;&#35823;&#24046;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#26512;&#25968;&#20540;&#31163;&#25955;&#36941;&#21382;SDE&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#65292;&#24182;&#35299;&#20915;&#20102;&#23454;&#36341;&#20013;&#32500;&#24230;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08711</link><description>&lt;p&gt;
&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Correction to "Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations"
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08711
&lt;/p&gt;
&lt;p&gt;
&#20462;&#27491;&#20102;&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20013;&#30340;&#38169;&#35823;&#23616;&#37096;&#35823;&#24046;&#20272;&#35745;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#20998;&#26512;&#25968;&#20540;&#31163;&#25955;&#36941;&#21382;SDE&#30340;Wasserstein-2&#36317;&#31163;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#65292;&#24182;&#35299;&#20915;&#20102;&#23454;&#36341;&#20013;&#32500;&#24230;&#20381;&#36182;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;San-Serna&#21644;Zygalakis&#30340;&#12298;&#23545;&#20110;&#25968;&#20540;&#36924;&#36817;&#36941;&#21382;SDE&#30340;&#20998;&#24067;&#30340;Wasserstein&#36317;&#31163;&#20272;&#35745;&#12299;&#20013;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#25968;&#20540;&#31163;&#25955;&#20998;&#26512;&#26041;&#27861;&#36827;&#34892;&#20102;&#20462;&#27491;&#12290;&#20182;&#20204;&#20998;&#26512;&#20102;UBU&#31215;&#20998;&#22120;&#65292;&#35813;&#31215;&#20998;&#22120;&#26159;&#20108;&#38454;&#24378;&#22411;&#30340;&#65292;&#24182;&#19988;&#27599;&#20010;&#27493;&#39588;&#21482;&#38656;&#35201;&#19968;&#27425;&#26799;&#24230;&#35780;&#20272;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#29702;&#24819;&#30340;&#38750;&#28176;&#36817;&#20445;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;Wasserstein-2&#36317;&#31163;&#20013;&#21040;&#36798;&#31163;&#30446;&#26631;&#20998;&#24067; $\epsilon &gt; 0$ &#30340;&#36317;&#31163;&#20165;&#38656; $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ &#27493;&#12290;&#28982;&#32780;&#65292;Sanz-Serna&#21644;Zygalakis (2021)&#20013;&#30340;&#23616;&#37096;&#35823;&#24046;&#20272;&#35745;&#23384;&#22312;&#38169;&#35823;&#65292;&#22312;&#23454;&#36341;&#20013;&#38656;&#35201;&#26356;&#24378;&#30340;&#20551;&#35774;&#25165;&#33021;&#23454;&#29616;&#36825;&#20123;&#22797;&#26434;&#24230;&#20272;&#35745;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#29702;&#35770;&#19982;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#30340;&#35768;&#22810;&#24212;&#29992;&#22330;&#26223;&#20013;&#30340;&#32500;&#24230;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08711v1 Announce Type: cross Abstract: A method for analyzing non-asymptotic guarantees of numerical discretizations of ergodic SDEs in Wasserstein-2 distance is presented by Sanz-Serna and Zygalakis in ``Wasserstein distance estimates for the distributions of numerical approximations to ergodic stochastic differential equations". They analyze the UBU integrator which is strong order two and only requires one gradient evaluation per step, resulting in desirable non-asymptotic guarantees, in particular $\mathcal{O}(d^{1/4}\epsilon^{-1/2})$ steps to reach a distance of $\epsilon &gt; 0$ in Wasserstein-2 distance away from the target distribution. However, there is a mistake in the local error estimates in Sanz-Serna and Zygalakis (2021), in particular, a stronger assumption is needed to achieve these complexity estimates. This note reconciles the theory with the dimension dependence observed in practice in many applications of interest.
&lt;/p&gt;</description></item><item><title>&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;&#25506;&#32034;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#27493;&#21644;&#26080;&#38480;&#23567;&#25200;&#21160;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.08573</link><description>&lt;p&gt;
&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;
&lt;/p&gt;
&lt;p&gt;
Two Tales of Single-Phase Contrastive Hebbian Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08573
&lt;/p&gt;
&lt;p&gt;
&#20004;&#31181;&#21333;&#30456;&#23545;&#27604;&#28023;&#27604;&#23433;&#23398;&#20064;&#30340;&#25925;&#20107;&#25506;&#32034;&#20102;&#23398;&#20064;&#31639;&#27861;&#30340;&#29983;&#29289;&#21512;&#29702;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#28040;&#38500;&#19982;&#21453;&#21521;&#20256;&#25773;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#35299;&#20915;&#20102;&#21516;&#27493;&#21644;&#26080;&#38480;&#23567;&#25200;&#21160;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#8220;&#29983;&#29289;&#23398;&#19978;&#21512;&#29702;&#8221;&#30340;&#23398;&#20064;&#31639;&#27861;&#30340;&#25506;&#32034;&#24050;&#32463;&#25910;&#25947;&#20110;&#23558;&#26799;&#24230;&#34920;&#31034;&#20026;&#27963;&#21160;&#24046;&#24322;&#30340;&#24819;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#36739;&#39640;&#31243;&#24230;&#30340;&#21516;&#27493;&#65288;&#23398;&#20064;&#26399;&#38388;&#30340;&#19981;&#21516;&#38454;&#27573;&#65289;&#24182;&#24341;&#20837;&#22823;&#37327;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#36825;&#23545;&#20110;&#23427;&#20204;&#30340;&#29983;&#29289;&#23398;&#21512;&#29702;&#24615;&#20197;&#21450;&#20854;&#22312;&#31070;&#32463;&#24418;&#24577;&#35745;&#31639;&#20013;&#30340;&#28508;&#22312;&#25928;&#29992;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#36755;&#20986;&#21333;&#20803;&#26045;&#21152;&#26080;&#38480;&#23567;&#25200;&#21160;&#65288;nudges&#65289;&#65292;&#36825;&#22312;&#22024;&#26434;&#29615;&#22659;&#20013;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#26368;&#36817;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#23558;&#20154;&#24037;&#31070;&#32463;&#20803;&#24314;&#27169;&#20026;&#20004;&#20010;&#30456;&#21453;&#25200;&#21160;&#30340;&#32452;&#20214;&#65292;&#21517;&#20026;&#8220;&#21452;&#21521;&#20256;&#25773;&#8221;&#30340;&#20840;&#23616;&#23398;&#20064;&#31639;&#27861;&#33021;&#22815;&#24357;&#21512;&#21040;&#21453;&#21521;&#20256;&#25773;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#32780;&#19981;&#38656;&#35201;&#20998;&#21035;&#30340;&#23398;&#20064;&#38454;&#27573;&#25110;&#26080;&#38480;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;&#35813;&#31639;&#27861;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#20381;&#36182;&#20110;&#23545;&#31216;&#25200;&#21160;&#65292;&#36825;&#21487;&#33021;&#22312;&#29983;&#29289;&#23398;&#19978;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The search for "biologically plausible" learning algorithms has converged on the idea of representing gradients as activity differences. However, most approaches require a high degree of synchronization (distinct phases during learning) and introduce substantial computational overhead, which raises doubts regarding their biological plausibility as well as their potential utility for neuromorphic computing. Furthermore, they commonly rely on applying infinitesimal perturbations (nudges) to output units, which is impractical in noisy environments. Recently it has been shown that by modelling artificial neurons as dyads with two oppositely nudged compartments, it is possible for a fully local learning algorithm named ``dual propagation'' to bridge the performance gap to backpropagation, without requiring separate learning phases or infinitesimal nudging. However, the algorithm has the drawback that its numerical stability relies on symmetric nudging, which may be restrictive in biological
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>PoCo&#26159;&#19968;&#31181;&#31574;&#30053;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#27169;&#24577;&#21644;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#35757;&#32451;&#36890;&#29992;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22330;&#26223;&#32423;&#21644;&#20219;&#21153;&#32423;&#30340;&#24191;&#20041;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#20219;&#21153;&#32423;&#32452;&#21512;&#21644;&#20998;&#26512;&#25104;&#26412;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2402.02511</link><description>&lt;p&gt;
PoCo: &#26469;&#33258;&#21644;&#20026;&#24322;&#26500;&#26426;&#22120;&#20154;&#23398;&#20064;&#30340;&#31574;&#30053;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
PoCo: Policy Composition from and for Heterogeneous Robot Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02511
&lt;/p&gt;
&lt;p&gt;
PoCo&#26159;&#19968;&#31181;&#31574;&#30053;&#32452;&#21512;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#21512;&#19981;&#21516;&#27169;&#24577;&#21644;&#39046;&#22495;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#23454;&#29616;&#20102;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#35757;&#32451;&#36890;&#29992;&#26426;&#22120;&#20154;&#31574;&#30053;&#30340;&#30446;&#26631;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#22330;&#26223;&#32423;&#21644;&#20219;&#21153;&#32423;&#30340;&#24191;&#20041;&#25805;&#20316;&#25216;&#33021;&#23398;&#20064;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#36890;&#36807;&#20219;&#21153;&#32423;&#32452;&#21512;&#21644;&#20998;&#26512;&#25104;&#26412;&#20989;&#25968;&#36827;&#34892;&#31574;&#30053;&#34892;&#20026;&#30340;&#33258;&#36866;&#24212;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#35757;&#32451;&#36890;&#29992;&#30340;&#26426;&#22120;&#20154;&#31574;&#30053;&#20197;&#22788;&#29702;&#19981;&#21516;&#20219;&#21153;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26426;&#22120;&#20154;&#25968;&#25454;&#38598;&#22312;&#39068;&#33394;&#12289;&#28145;&#24230;&#12289;&#35302;&#35273;&#21644;&#23039;&#24577;&#24863;&#31561;&#19981;&#21516;&#27169;&#24577;&#19978;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#22312;&#27169;&#25311;&#12289;&#30495;&#23454;&#26426;&#22120;&#20154;&#21644;&#20154;&#31867;&#35270;&#39057;&#31561;&#19981;&#21516;&#39046;&#22495;&#25910;&#38598;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#20250;&#25910;&#38598;&#24182;&#27719;&#38598;&#19968;&#20010;&#39046;&#22495;&#30340;&#25152;&#26377;&#25968;&#25454;&#65292;&#20197;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#31574;&#30053;&#26469;&#22788;&#29702;&#20219;&#21153;&#21644;&#39046;&#22495;&#30340;&#24322;&#26500;&#24615;&#65292;&#36825;&#26159;&#38750;&#24120;&#26114;&#36149;&#21644;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#31574;&#30053;&#32452;&#21512;&#65292;&#36890;&#36807;&#32452;&#21512;&#29992;&#25193;&#25955;&#27169;&#22411;&#34920;&#31034;&#30340;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#65292;&#23558;&#36328;&#19981;&#21516;&#27169;&#24577;&#21644;&#39046;&#22495;&#30340;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#23398;&#20064;&#22330;&#26223;&#32423;&#21644;&#20219;&#21153;&#32423;&#30340;&#24191;&#20041;&#25805;&#20316;&#25216;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#22810;&#20219;&#21153;&#25805;&#20316;&#20013;&#20351;&#29992;&#20219;&#21153;&#32423;&#32452;&#21512;&#65292;&#24182;&#19982;&#20998;&#26512;&#25104;&#26412;&#20989;&#25968;&#32452;&#21512;&#65292;&#20197;&#22312;&#25512;&#29702;&#26102;&#35843;&#25972;&#31574;&#30053;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#12289;&#20154;&#31867;&#21644;&#23454;&#38469;&#26426;&#22120;&#20154;&#25968;&#25454;&#19978;&#35757;&#32451;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training general robotic policies from heterogeneous data for different tasks is a significant challenge. Existing robotic datasets vary in different modalities such as color, depth, tactile, and proprioceptive information, and collected in different domains such as simulation, real robots, and human videos. Current methods usually collect and pool all data from one domain to train a single policy to handle such heterogeneity in tasks and domains, which is prohibitively expensive and difficult. In this work, we present a flexible approach, dubbed Policy Composition, to combine information across such diverse modalities and domains for learning scene-level and task-level generalized manipulation skills, by composing different data distributions represented with diffusion models. Our method can use task-level composition for multi-task manipulation and be composed with analytic cost functions to adapt policy behaviors at inference time. We train our method on simulation, human, and real 
&lt;/p&gt;</description></item><item><title>GLaPE&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#30340;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>https://arxiv.org/abs/2402.02408</link><description>&lt;p&gt;
GLaPE&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#19982;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
GLaPE: Gold Label-agnostic Prompt Evaluation and Optimization for Large Language Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02408
&lt;/p&gt;
&lt;p&gt;
GLaPE&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#30340;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#65292;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#20219;&#21153;&#24615;&#33021;&#20173;&#28982;&#23545;&#25552;&#31034;&#35774;&#35745;&#25935;&#24863;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#32034;&#20102;&#21033;&#29992;LLM&#33258;&#36523;&#20316;&#20026;&#20248;&#21270;&#22120;&#26469;&#35782;&#21035;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26368;&#20248;&#25552;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#25552;&#31034;&#26102;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#38590;&#20197;&#33719;&#21462;&#30340;&#25163;&#21160;&#26631;&#27880;&#30340;&#37329;&#26631;&#31614;&#65292;&#20197;&#35745;&#31639;&#27599;&#20010;&#20505;&#36873;&#25552;&#31034;&#30340;&#20219;&#21153;&#20934;&#30830;&#24615;&#65292;&#36825;&#38459;&#30861;&#20102;&#24191;&#27867;&#30340;&#23454;&#26045;&#21644;&#36890;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#20381;&#36182;&#20110;&#37329;&#26631;&#31614;&#30340;&#25552;&#31034;&#35780;&#20272;&#26041;&#27861;&#65288;GLaPE&#65289;&#65292;&#20197;&#20943;&#23569;&#23545;&#37329;&#26631;&#31614;&#30340;&#20381;&#36182;&#12290;&#21463;&#21040;&#33258;&#19968;&#33268;&#24615;&#21644;&#31572;&#26696;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#33258;&#19968;&#33268;&#24615;&#20316;&#20026;&#21021;&#22987;&#35780;&#20272;&#20998;&#25968;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#23545;&#20135;&#29983;&#30456;&#21516;&#31572;&#26696;&#30340;&#25552;&#31034;&#36827;&#34892;&#24471;&#20998;&#30340;&#20114;&#30456;&#19968;&#33268;&#24615;&#30340;&#25913;&#36827;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GLaPE&#22312;&#27809;&#26377;&#37329;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#20102;&#19982;&#20934;&#30830;&#24615;&#30456;&#19968;&#33268;&#30340;&#21487;&#38752;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#20845;&#20010;&#20219;&#21153;&#65292;GLaPE&#22312;&#32477;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#24471;&#21040;&#30340;&#35780;&#20272;&#32467;&#26524;&#19982;&#20351;&#29992;&#30495;&#23454;&#37329;&#26631;&#31614;&#35780;&#20272;&#30340;&#32467;&#26524;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid progress of large language models (LLMs), their task performance remains sensitive to prompt design. Recent studies have explored leveraging the LLM itself as an optimizer to identify optimal prompts that maximize task accuracy. However, when evaluating prompts, such approaches heavily rely on elusive manually annotated gold labels to calculate task accuracy for each candidate prompt, which hinders the widespread implementation and generality. To overcome the limitation, this work proposes a gold label-agnostic prompt evaluation (GLaPE) to alleviate dependence on gold labels. Motivated by the observed correlation between self-consistency and the accuracy of the answer, we adopt self-consistency as the initial evaluation score. Subsequently, we refine the scores of prompts producing identical answers to be mutually consistent. Experimental results show that GLaPE provides reliable evaluations uniform with accuracy, even in the absence of gold labels. Moreover, on six p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2402.02054</link><description>&lt;p&gt;
&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Neural Scaling Laws on Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02054
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#22270;&#19978;&#28145;&#20837;&#30740;&#31350;&#20102;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65292;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#21457;&#29616;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#21644;&#36807;&#25311;&#21512;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#23545;&#32553;&#25918;&#34892;&#20026;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25552;&#20986;&#20102;&#22270;&#25968;&#37327;&#19981;&#36866;&#21512;&#20316;&#20026;&#34913;&#37327;&#32553;&#25918;&#23450;&#24459;&#20013;&#22270;&#25968;&#25454;&#37327;&#30340;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22270;&#27169;&#22411;&#65288;&#20363;&#22914;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#21464;&#25442;&#22120;&#65289;&#24050;&#25104;&#20026;&#21033;&#29992;&#21508;&#31181;&#31867;&#22411;&#22270;&#30340;&#30693;&#35782;&#30340;&#37325;&#35201;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#32553;&#25918;&#29305;&#24615;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30740;&#31350;&#65292;&#23545;&#36890;&#36807;&#25193;&#22823;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26469;&#23454;&#29616;&#22823;&#22411;&#22270;&#27169;&#22411;&#30340;&#21487;&#34892;&#24615;&#20135;&#29983;&#20102;&#30097;&#38382;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35282;&#24230;&#28145;&#20837;&#25506;&#32034;&#20102;&#22270;&#19978;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#12290;&#25105;&#20204;&#39318;&#20808;&#39564;&#35777;&#20102;&#36825;&#20123;&#23450;&#24459;&#22312;&#22270;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#24314;&#31435;&#20102;&#25551;&#36848;&#32553;&#25918;&#34892;&#20026;&#30340;&#20844;&#24335;&#12290;&#23545;&#20110;&#27169;&#22411;&#32553;&#25918;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#32553;&#25918;&#23450;&#24459;&#23849;&#28291;&#29616;&#35937;&#65292;&#24182;&#30830;&#23450;&#20102;&#36807;&#25311;&#21512;&#21487;&#33021;&#26159;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#28145;&#24230;&#22270;&#27169;&#22411;&#30340;&#27169;&#22411;&#28145;&#24230;&#21487;&#20197;&#24433;&#21709;&#27169;&#22411;&#32553;&#25918;&#34892;&#20026;&#65292;&#36825;&#19982;&#20854;&#20182;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65289;&#30340;&#35266;&#23519;&#32467;&#26524;&#19981;&#21516;&#12290;&#23545;&#20110;&#25968;&#25454;&#32553;&#25918;&#65292;&#25105;&#20204;&#24314;&#35758;&#22270;&#25968;&#37327;&#26080;&#27861;&#26377;&#25928;&#34913;&#37327;&#22270;&#25968;&#25454;&#37327;&#30340;&#32553;&#25918;&#23450;&#24459;&#65292;&#22240;&#20026;...
&lt;/p&gt;
&lt;p&gt;
Deep graph models (e.g., graph neural networks and graph transformers) have become important techniques for leveraging knowledge across various types of graphs. Yet, the scaling properties of deep graph models have not been systematically investigated, casting doubt on the feasibility of achieving large graph models through enlarging the model and dataset sizes. In this work, we delve into neural scaling laws on graphs from both model and data perspectives. We first verify the validity of such laws on graphs, establishing formulations to describe the scaling behaviors. For model scaling, we investigate the phenomenon of scaling law collapse and identify overfitting as the potential reason. Moreover, we reveal that the model depth of deep graph models can impact the model scaling behaviors, which differ from observations in other domains such as CV and NLP. For data scaling, we suggest that the number of graphs can not effectively metric the graph data volume in scaling law since the si
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#65292;&#34987;&#21160;&#30340;&#22909;&#22855;&#25932;&#25163;&#21487;&#20197;&#22312;&#20960;&#27425;&#20445;&#25252;&#38544;&#31169;&#30340;&#27714;&#21644;&#25805;&#20316;&#21518;&#25512;&#26029;&#20986;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#20154;&#25968;&#25454;&#12290;</title><link>https://arxiv.org/abs/2312.05248</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#30340;&#21435;&#37325;&#24314;&#38450;&#25252;&#22312;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Topology-Based Reconstruction Prevention for Decentralised Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05248
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20013;&#65292;&#34987;&#21160;&#30340;&#22909;&#22855;&#25932;&#25163;&#21487;&#20197;&#22312;&#20960;&#27425;&#20445;&#25252;&#38544;&#31169;&#30340;&#27714;&#21644;&#25805;&#20316;&#21518;&#25512;&#26029;&#20986;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#20154;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#32852;&#37030;&#23398;&#20064;&#30340;&#26041;&#24335;&#65292;&#33719;&#24471;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20854;&#20013;&#25968;&#25454;&#21644;&#21327;&#35843;&#37117;&#20998;&#24067;&#22312;&#29992;&#25143;&#20043;&#38388;&#12290;&#20026;&#20102;&#20445;&#25252;&#25968;&#25454;&#30340;&#26426;&#23494;&#24615;&#65292;&#21435;&#20013;&#24515;&#21270;&#23398;&#20064;&#20381;&#36182;&#20110;&#24046;&#20998;&#38544;&#31169;&#12289;&#22810;&#26041;&#35745;&#31639;&#65292;&#25110;&#32773;&#20108;&#32773;&#30340;&#32467;&#21512;&#12290;&#28982;&#32780;&#65292;&#36830;&#32493;&#36816;&#34892;&#22810;&#20010;&#20445;&#25252;&#38544;&#31169;&#30340;&#27714;&#21644;&#25805;&#20316;&#21487;&#33021;&#20250;&#20351;&#23545;&#25163;&#36827;&#34892;&#37325;&#24314;&#25915;&#20987;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#24403;&#21069;&#30340;&#37325;&#24314;&#23545;&#31574;&#35201;&#20040;&#26080;&#27861;&#31616;&#21333;&#22320;&#36866;&#24212;&#20998;&#24067;&#24335;&#29615;&#22659;&#65292;&#35201;&#20040;&#20250;&#28155;&#21152;&#36807;&#22810;&#30340;&#22122;&#38899;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#34920;&#26126;&#65292;&#34987;&#21160;&#30340;&#22909;&#22855;&#25932;&#25163;&#21487;&#20197;&#22312;&#20960;&#27425;&#20445;&#25252;&#38544;&#31169;&#30340;&#27714;&#21644;&#20043;&#21518;&#25512;&#26029;&#20986;&#20854;&#20182;&#29992;&#25143;&#30340;&#31169;&#20154;&#25968;&#25454;&#12290;&#20363;&#22914;&#65292;&#22312;&#25299;&#25169;&#20013;&#26377;18&#20010;&#29992;&#25143;&#30340;&#23376;&#22270;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#21482;&#26377;&#19977;&#20010;&#34987;&#21160;&#30340;&#22909;&#22855;&#25932;&#25163;&#25104;&#21151;&#37325;&#24314;&#31169;&#20154;&#25968;&#25454;&#30340;&#27010;&#29575;&#20026;11.0%&#65292;&#24179;&#22343;&#27599;&#20010;&#23545;&#25163;&#38656;&#35201;8.8&#27425;&#27714;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05248v2 Announce Type: replace-cross  Abstract: Decentralised learning has recently gained traction as an alternative to federated learning in which both data and coordination are distributed over its users. To preserve data confidentiality, decentralised learning relies on differential privacy, multi-party computation, or a combination thereof. However, running multiple privacy-preserving summations in sequence may allow adversaries to perform reconstruction attacks. Unfortunately, current reconstruction countermeasures either cannot trivially be adapted to the distributed setting, or add excessive amounts of noise.   In this work, we first show that passive honest-but-curious adversaries can infer other users' private data after several privacy-preserving summations. For example, in subgraphs with 18 users, we show that only three passive honest-but-curious adversaries succeed at reconstructing private data 11.0% of the time, requiring an average of 8.8 summations per adve
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;</title><link>http://arxiv.org/abs/2401.17133</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#30340;&#20027;&#21160;&#24615;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65306;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion. (arXiv:2401.17133v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;(SVC)&#36890;&#36807;&#23558;&#19968;&#20010;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#25104;&#21478;&#19968;&#20010;&#30446;&#26631;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;&#27468;&#35789;&#21644;&#26059;&#24459;&#65292;&#33258;&#21160;&#21270;&#20102;&#27468;&#26354;&#32763;&#21809;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#29256;&#26435;&#21644;&#20844;&#27665;&#26435;&#21033;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; SongBsAb&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20027;&#21160;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110; SVC &#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;SongBsAb &#22312;&#21457;&#24067;&#27468;&#21809;&#22768;&#38899;&#20043;&#21069;&#24341;&#20837;&#20102;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#36825;&#26679;&#24403;&#23427;&#20204;&#34987;&#20351;&#29992;&#26102;&#65292;SVC &#30340;&#29983;&#25104;&#36807;&#31243;&#23558;&#34987;&#24178;&#25200;&#65292;&#23548;&#33268;&#24847;&#22806;&#30340;&#27468;&#21809;&#22768;&#38899;&#12290; SongBsAb &#20855;&#26377;&#21452;&#37325;&#39044;&#38450;&#25928;&#26524;&#65292;&#24341;&#36215;&#27468;&#25163;&#36523;&#20221;&#21644;&#27468;&#35789;&#30340;&#28151;&#20081;&#65292;&#21363; SVC &#35206;&#30422;&#30340;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;&#20026;&#20102;&#25552;&#39640;&#25200;&#21160;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20197;&#20276;&#22863;&#26354;&#20316;&#20026;&#39069;&#22806;&#25513;&#34109;&#32773;&#30340;&#22522;&#20110;&#24515;&#29702;&#22768;&#23398;&#27169;&#22411;&#30340;&#25439;&#22833;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Singing voice conversion (SVC) automates song covers by converting one singer's singing voice into another target singer's singing voice with the original lyrics and melody. However, it raises serious concerns about copyright and civil right infringements to multiple entities. This work proposes SongBsAb, the first proactive approach to mitigate unauthorized SVC-based illegal song covers. SongBsAb introduces human-imperceptible perturbations to singing voices before releasing them, so that when they are used, the generation process of SVC will be interfered, resulting in unexpected singing voices. SongBsAb features a dual prevention effect by causing both (singer) identity disruption and lyric disruption, namely, the SVC-covered singing voice neither imitates the target singer nor preserves the original lyrics. To improve the imperceptibility of perturbations, we refine a psychoacoustic model-based loss with the backing track as an additional masker, a unique accompanying element for s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#23618;&#30340;&#24490;&#29615;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#21516;&#26102;&#35299;&#37322;&#31995;&#32479;&#32423;&#21644;&#20010;&#20307;&#32423;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32676;&#20307;&#21160;&#24577;&#12290;</title><link>http://arxiv.org/abs/2401.14973</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#24490;&#29615;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#21457;&#29616;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32676;&#20307;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Discovering group dynamics in synchronous time series via hierarchical recurrent switching-state models. (arXiv:2401.14973v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14973
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#30340;&#24490;&#29615;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#26080;&#30417;&#30563;&#22320;&#21516;&#26102;&#35299;&#37322;&#31995;&#32479;&#32423;&#21644;&#20010;&#20307;&#32423;&#30340;&#21160;&#24577;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#24314;&#27169;&#21516;&#27493;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#32676;&#20307;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#23545;&#21516;&#19968;&#26102;&#38388;&#27573;&#20869;&#22810;&#20010;&#23454;&#20307;&#30456;&#20114;&#20316;&#29992;&#32780;&#20135;&#29983;&#30340;&#26102;&#38388;&#24207;&#21015;&#38598;&#21512;&#36827;&#34892;&#24314;&#27169;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24314;&#27169;&#20010;&#20307;&#26102;&#38388;&#24207;&#21015;&#26041;&#38754;&#23545;&#25105;&#20204;&#30340;&#39044;&#26399;&#24212;&#29992;&#26159;&#19981;&#36275;&#22815;&#30340;&#65292;&#20854;&#20013;&#38598;&#20307;&#31995;&#32479;&#32423;&#34892;&#20026;&#24433;&#21709;&#30528;&#20010;&#20307;&#23454;&#20307;&#30340;&#36712;&#36857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31867;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#20999;&#25442;&#29366;&#24577;&#27169;&#22411;&#65292;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#35757;&#32451;&#65292;&#21516;&#26102;&#35299;&#37322;&#31995;&#32479;&#32423;&#21644;&#20010;&#20307;&#32423;&#30340;&#21160;&#24577;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#38544;&#21547;&#30340;&#31995;&#32479;&#32423;&#31163;&#25955;&#29366;&#24577;&#39532;&#23572;&#21487;&#22827;&#38142;&#65292;&#39537;&#21160;&#30528;&#38544;&#21547;&#30340;&#23454;&#20307;&#32423;&#38142;&#65292;&#36827;&#32780;&#25511;&#21046;&#27599;&#20010;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#30340;&#21160;&#24577;&#12290;&#35266;&#27979;&#32467;&#26524;&#22312;&#23454;&#20307;&#21644;&#31995;&#32479;&#32423;&#30340;&#38142;&#20043;&#38388;&#36827;&#34892;&#21453;&#39304;&#65292;&#36890;&#36807;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#29366;&#24577;&#36716;&#25442;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#23618;&#20999;&#25442;&#24490;&#29615;&#21160;&#21147;&#23398;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23553;&#38381;&#24418;&#24335;&#30340;&#21464;&#20998;&#22352;&#26631;&#19978;&#21319;&#26356;&#26032;&#26469;&#23398;&#20064;&#65292;&#20854;&#22312;&#20010;&#20307;&#25968;&#37327;&#19978;&#21576;&#32447;&#24615;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We seek to model a collection of time series arising from multiple entities interacting over the same time period. Recent work focused on modeling individual time series is inadequate for our intended applications, where collective system-level behavior influences the trajectories of individual entities. To address such problems, we present a new hierarchical switching-state model that can be trained in an unsupervised fashion to simultaneously explain both system-level and individual-level dynamics. We employ a latent system-level discrete state Markov chain that drives latent entity-level chains which in turn govern the dynamics of each observed time series. Feedback from the observations to the chains at both the entity and system levels improves flexibility via context-dependent state transitions. Our hierarchical switching recurrent dynamical models can be learned via closed-form variational coordinate ascent updates to all latent chains that scale linearly in the number of indivi
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#23631;&#34109;&#26041;&#27861;&#26469;&#28040;&#38500;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#31227;&#38500;&#23427;&#20204;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#32508;&#21512;&#32771;&#34385;&#29305;&#24449;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2401.12644</link><description>&lt;p&gt;
&#20108;&#36827;&#21046;&#29305;&#24449;&#23631;&#34109;&#20248;&#21270;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Binary Feature Mask Optimization for Feature Selection. (arXiv:2401.12644v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12644
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36873;&#25321;&#26694;&#26550;&#65292;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#23631;&#34109;&#26041;&#27861;&#26469;&#28040;&#38500;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#31227;&#38500;&#23427;&#20204;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#32508;&#21512;&#32771;&#34385;&#29305;&#24449;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#36873;&#25321;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#32771;&#34385;&#20102;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#36873;&#25321;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#23631;&#34109;&#26041;&#27861;&#65292;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#28040;&#38500;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#20174;&#25968;&#25454;&#38598;&#20013;&#23436;&#20840;&#31227;&#38500;&#23427;&#20204;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#36873;&#25321;&#36807;&#31243;&#20013;&#20351;&#29992;&#30456;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#20687;&#20854;&#20182;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#37027;&#26679;&#38656;&#35201;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22240;&#20026;&#25968;&#25454;&#38598;&#30340;&#32500;&#24230;&#19981;&#21516;&#12290;&#25105;&#20204;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#26469;&#33719;&#21462;&#23631;&#34109;&#25805;&#20316;&#31526;&#65292;&#36825;&#20026;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#25552;&#20379;&#20102;&#23545;&#29305;&#24449;&#23376;&#38598;&#30340;&#20840;&#38754;&#35266;&#23519;&#12290;&#29305;&#24449;&#36873;&#25321;&#25991;&#29486;&#20013;&#23384;&#22312;&#21508;&#31181;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#30740;&#31350;&#24341;&#20837;&#19968;&#20010;&#38024;&#23545;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#20197;&#25972;&#20307;&#32771;&#34385;&#29305;&#24449;&#23376;&#38598;&#30340;&#37325;&#35201;&#24615;&#65292;&#32780;&#19981;&#26159;&#21482;&#20851;&#27880;&#21333;&#20010;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate feature selection problem for generic machine learning (ML) models. We introduce a novel framework that selects features considering the predictions of the model. Our framework innovates by using a novel feature masking approach to eliminate the features during the selection process, instead of completely removing them from the dataset. This allows us to use the same ML model during feature selection, unlike other feature selection methods where we need to train the ML model again as the dataset has different dimensions on each iteration. We obtain the mask operator using the predictions of the ML model, which offers a comprehensive view on the subsets of the features essential for the predictive performance of the model. A variety of approaches exist in the feature selection literature. However, no study has introduced a training-free framework for a generic ML model to select features while considering the importance of the feature subsets as a whole, instead of focusi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#21644;&#28385;&#31209;&#21464;&#20998;&#26063;&#20043;&#38388;&#30340;&#29702;&#35770;&#20013;&#38388;&#22320;&#24102;&#65306;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#21487;&#20197;&#22312;&#36845;&#20195;&#22797;&#26434;&#24615;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32553;&#25918;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.10989</link><description>&lt;p&gt;
&#20855;&#26377;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#30340;&#21487;&#35777;&#20280;&#32553;&#24615;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Provably Scalable Black-Box Variational Inference with Structured Variational Families. (arXiv:2401.10989v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#21644;&#28385;&#31209;&#21464;&#20998;&#26063;&#20043;&#38388;&#30340;&#29702;&#35770;&#20013;&#38388;&#22320;&#24102;&#65306;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#21487;&#20197;&#22312;&#36845;&#20195;&#22797;&#26434;&#24615;&#19978;&#34920;&#29616;&#26356;&#22909;&#65292;&#32553;&#25918;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#30693;&#20855;&#26377;&#28385;&#31209;&#21327;&#26041;&#24046;&#36924;&#36817;&#30340;&#21464;&#20998;&#26063;&#22312;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#26080;&#35770;&#26159;&#20174;&#23454;&#35777;&#19978;&#36824;&#26159;&#29702;&#35770;&#19978;&#12290;&#20107;&#23454;&#19978;&#65292;&#26368;&#36817;&#23545;&#40657;&#30418;&#21464;&#20998;&#25512;&#26029;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#30456;&#27604;&#65292;&#28385;&#31209;&#21464;&#20998;&#26063;&#22312;&#38382;&#39064;&#30340;&#32500;&#24230;&#19978;&#25193;&#23637;&#24471;&#24456;&#24046;&#12290;&#36825;&#23545;&#20855;&#26377;&#26412;&#22320;&#21464;&#37327;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#27169;&#22411;&#23588;&#20026;&#20851;&#38190;&#65292;&#23427;&#20204;&#30340;&#32500;&#24230;&#38543;&#30528;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#32780;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#36845;&#20195;&#22797;&#26434;&#24615;&#23545;&#25968;&#25454;&#38598;&#22823;&#23567;N&#23384;&#22312;&#26126;&#30830;&#30340;O(N^2)&#20381;&#36182;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22343;&#20540;&#22330;&#21464;&#20998;&#26063;&#21644;&#28385;&#31209;&#21464;&#20998;&#26063;&#20043;&#38388;&#30340;&#29702;&#35770;&#20013;&#38388;&#22320;&#24102;&#65306;&#32467;&#26500;&#21270;&#21464;&#20998;&#26063;&#12290;&#25105;&#20204;&#20005;&#26684;&#35777;&#26126;&#20102;&#26576;&#20123;&#23610;&#24230;&#30697;&#38453;&#32467;&#26500;&#21487;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24615;O(N)&#65292;&#20174;&#32780;&#19982;N&#30340;&#32553;&#25918;&#26356;&#22909;&#22320;&#21305;&#37197;&#12290;&#25105;&#20204;&#22312;&#29616;&#23454;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
Variational families with full-rank covariance approximations are known not to work well in black-box variational inference (BBVI), both empirically and theoretically. In fact, recent computational complexity results for BBVI have established that full-rank variational families scale poorly with the dimensionality of the problem compared to e.g. mean field families. This is particularly critical to hierarchical Bayesian models with local variables; their dimensionality increases with the size of the datasets. Consequently, one gets an iteration complexity with an explicit $\mathcal{O}(N^2)$ dependence on the dataset size $N$. In this paper, we explore a theoretical middle ground between mean-field variational families and full-rank families: structured variational families. We rigorously prove that certain scale matrix structures can achieve a better iteration complexity of $\mathcal{O}(N)$, implying better scaling with respect to $N$. We empirically verify our theoretical results on l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#19988;&#26102;&#31354;&#21464;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20004;&#31181;&#21453;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#25581;&#31034;&#21463;&#20559;&#24046;&#24433;&#21709;&#30340;&#30495;&#23454;&#29366;&#24577;&#65292;&#24182;&#22312;&#32473;&#23450;&#31232;&#30095;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.10306</link><description>&lt;p&gt;
&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#21453;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Physics-constrained convolutional neural networks for inverse problems in spatiotemporal partial differential equations. (arXiv:2401.10306v1 [physics.flu-dyn])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#19988;&#26102;&#31354;&#21464;&#21270;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#20004;&#31181;&#21453;&#38382;&#39064;&#12290;&#35813;&#32593;&#32476;&#21487;&#20197;&#25581;&#31034;&#21463;&#20559;&#24046;&#24433;&#21709;&#30340;&#30495;&#23454;&#29366;&#24577;&#65292;&#24182;&#22312;&#32473;&#23450;&#31232;&#30095;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29289;&#29702;&#32422;&#26463;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;PC-CNN&#65289;&#26469;&#35299;&#20915;&#20559;&#24494;&#20998;&#26041;&#31243;&#20013;&#20004;&#31181;&#31867;&#22411;&#30340;&#21453;&#38382;&#39064;&#65292;&#36825;&#20123;&#26041;&#31243;&#22312;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#37117;&#26159;&#38750;&#32447;&#24615;&#19988;&#21464;&#21270;&#30340;&#12290;&#22312;&#31532;&#19968;&#20010;&#21453;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#21463;&#31354;&#38388;&#21464;&#21270;&#30340;&#31995;&#32479;&#35823;&#24046;&#65288;&#21363;&#20559;&#24046;&#65292;&#20063;&#31216;&#20026;&#35748;&#35782;&#19981;&#30830;&#23450;&#24615;&#65289;&#20559;&#31227;&#30340;&#25968;&#25454;&#12290;&#20219;&#21153;&#26159;&#20174;&#20559;&#24046;&#25968;&#25454;&#20013;&#25581;&#31034;&#30495;&#23454;&#29366;&#24577;&#65292;&#21363;PDE&#30340;&#35299;&#12290;&#22312;&#31532;&#20108;&#20010;&#21453;&#38382;&#39064;&#20013;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;PDE&#35299;&#30340;&#31232;&#30095;&#20449;&#24687;&#12290;&#20219;&#21153;&#26159;&#20197;&#39640;&#20998;&#36776;&#29575;&#37325;&#24314;&#31354;&#38388;&#20013;&#30340;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;PC-CNN&#65292;&#23427;&#36890;&#36807;&#31616;&#21333;&#30340;&#26102;&#38388;&#31383;&#21475;&#26041;&#26696;&#32422;&#26463;PDE&#26469;&#22788;&#29702;&#26102;&#24207;&#25968;&#25454;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;PC-CNN&#22312;&#20174;&#20559;&#24046;&#25968;&#25454;&#20013;&#25581;&#31034;&#35299;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#23545;&#27969;&#25193;&#25955;&#26041;&#31243;&#20197;&#21450;&#32435;&#32500;-&#26031;&#25176;&#20811;&#26031;&#26041;&#31243;&#65292;&#21518;&#32773;&#25551;&#36848;&#20102;&#28237;&#27969;&#27969;&#21160;&#30340;&#26102;&#31354;&#28151;&#27788;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a physics-constrained convolutional neural network (PC-CNN) to solve two types of inverse problems in partial differential equations (PDEs), which are nonlinear and vary both in space and time. In the first inverse problem, we are given data that is offset by spatially varying systematic error (i.e., the bias, also known the epistemic uncertainty). The task is to uncover from the biased data the true state, which is the solution of the PDE. In the second inverse problem, we are given sparse information on the solution of a PDE. The task is to reconstruct the solution in space with high-resolution. First, we present the PC-CNN, which constrains the PDE with a simple time-windowing scheme to handle sequential data. Second, we analyse the performance of the PC-CNN for uncovering solutions from biased data. We analyse both linear and nonlinear convection-diffusion equations, and the Navier-Stokes equations, which govern the spatiotemporally chaotic dynamics of turbulent flows. W
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#24494;&#22937;&#24046;&#24322;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.03482</link><description>&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification on Clinical Trial Outcome Prediction. (arXiv:2401.03482v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#24212;&#29992;&#20110;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#65292;&#25552;&#39640;&#27169;&#22411;&#23545;&#24494;&#22937;&#24046;&#24322;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#20854;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#22312;&#26426;&#22120;&#23398;&#20064;&#30340;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#37325;&#35201;&#24615;&#26085;&#30410;&#34987;&#35748;&#35782;&#21040;&#12290;&#20934;&#30830;&#35780;&#20272;&#27169;&#22411;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#21644;&#22686;&#21152;&#20449;&#24515;&#12290;&#36825;&#22312;&#21307;&#23398;&#35786;&#26029;&#21644;&#33647;&#29289;&#21457;&#29616;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#21487;&#38752;&#30340;&#39044;&#27979;&#30452;&#25509;&#24433;&#21709;&#30740;&#31350;&#36136;&#37327;&#21644;&#24739;&#32773;&#20581;&#24247;&#12290;&#26412;&#25991;&#25552;&#20986;&#23558;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#32435;&#20837;&#20020;&#24202;&#35797;&#39564;&#32467;&#26524;&#39044;&#27979;&#20013;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#25552;&#39640;&#27169;&#22411;&#36776;&#21035;&#24494;&#22937;&#24046;&#24322;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#26174;&#33879;&#25913;&#21892;&#20854;&#25972;&#20307;&#24615;&#33021;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#20998;&#31867;&#26041;&#27861;&#26469;&#23454;&#29616;&#25105;&#20204;&#30340;&#30446;&#26631;&#65292;&#24182;&#23558;&#20854;&#19982;&#23618;&#27425;&#20132;&#20114;&#32593;&#32476;(HINT)&#26080;&#32541;&#38598;&#25104;&#65292;HINT&#26159;&#20020;&#24202;&#35797;&#39564;&#39044;&#27979;&#24314;&#27169;&#30340;&#26368;&#21069;&#27839;&#12290;&#36873;&#25321;&#24615;&#20998;&#31867;&#28085;&#30422;&#20102;&#19968;&#31995;&#21015;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20445;&#30041;&#20449;&#24687;&#20197;&#20379;&#36827;&#19968;&#27493;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The importance of uncertainty quantification is increasingly recognized in the diverse field of machine learning. Accurately assessing model prediction uncertainty can help provide deeper understanding and confidence for researchers and practitioners. This is especially critical in medical diagnosis and drug discovery areas, where reliable predictions directly impact research quality and patient health.  In this paper, we proposed incorporating uncertainty quantification into clinical trial outcome predictions. Our main goal is to enhance the model's ability to discern nuanced differences, thereby significantly improving its overall performance.  We have adopted a selective classification approach to fulfill our objective, integrating it seamlessly with the Hierarchical Interaction Network (HINT), which is at the forefront of clinical trial prediction modeling. Selective classification, encompassing a spectrum of methods for uncertainty quantification, empowers the model to withhold de
&lt;/p&gt;</description></item><item><title>&#26657;&#20934;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#32452;&#32455;&#25915;&#20987;&#26469;&#20351;&#21463;&#23475;&#27169;&#22411;&#22833;&#21435;&#20934;&#30830;&#26657;&#20934;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#22522;&#20110;&#32622;&#20449;&#20998;&#25968;&#30340;&#20915;&#31574;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#26657;&#20934;&#25915;&#20987;&#24418;&#24335;&#65292;&#24182;&#23545;&#24120;&#29992;&#30340;&#23545;&#25239;&#38450;&#24481;&#21644;&#26657;&#20934;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2401.02718</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;&#26657;&#20934;&#25915;&#20987;&#65306;&#38024;&#23545;&#26657;&#20934;&#24615;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Calibration Attack: A Framework For Adversarial Attacks Targeting Calibration. (arXiv:2401.02718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02718
&lt;/p&gt;
&lt;p&gt;
&#26657;&#20934;&#25915;&#20987;&#26159;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#21644;&#32452;&#32455;&#25915;&#20987;&#26469;&#20351;&#21463;&#23475;&#27169;&#22411;&#22833;&#21435;&#20934;&#30830;&#26657;&#20934;&#65292;&#32780;&#19981;&#24433;&#21709;&#20854;&#21407;&#22987;&#20934;&#30830;&#24615;&#12290;&#36825;&#23545;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#22522;&#20110;&#32622;&#20449;&#20998;&#25968;&#30340;&#20915;&#31574;&#26500;&#25104;&#20005;&#37325;&#23041;&#32961;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#26657;&#20934;&#25915;&#20987;&#24418;&#24335;&#65292;&#24182;&#23545;&#24120;&#29992;&#30340;&#23545;&#25239;&#38450;&#24481;&#21644;&#26657;&#20934;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#26657;&#20934;&#25915;&#20987;&#30340;&#26032;&#23545;&#25239;&#25915;&#20987;&#26694;&#26550;&#65292;&#20854;&#20013;&#25915;&#20987;&#34987;&#29983;&#25104;&#21644;&#32452;&#32455;&#20197;&#20351;&#21463;&#23475;&#27169;&#22411;&#22833;&#21435;&#20934;&#30830;&#26657;&#20934;&#65292;&#21516;&#26102;&#19981;&#25913;&#21464;&#20854;&#21407;&#22987;&#20934;&#30830;&#24615;&#65292;&#20174;&#32780;&#20005;&#37325;&#21361;&#21450;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#21644;&#22522;&#20110;&#20854;&#32622;&#20449;&#20998;&#25968;&#30340;&#20219;&#20309;&#20915;&#31574;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#31181;&#26032;&#22411;&#26657;&#20934;&#25915;&#20987;&#24418;&#24335;&#65306;&#20302;&#32622;&#20449;&#25915;&#20987;&#12289;&#39640;&#32622;&#20449;&#25915;&#20987;&#12289;&#26368;&#22823;&#22833;&#30495;&#25915;&#20987;&#21644;&#38543;&#26426;&#32622;&#20449;&#25915;&#20987;&#65292;&#36866;&#29992;&#20110;&#30333;&#30418;&#21644;&#40657;&#30418;&#35774;&#32622;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20840;&#38754;&#30340;&#25968;&#25454;&#38598;&#23545;&#20856;&#22411;&#30340;&#21463;&#23475;&#27169;&#22411;&#36827;&#34892;&#20102;&#36825;&#20123;&#26032;&#22411;&#25915;&#20987;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#21363;&#20351;&#21482;&#36827;&#34892;&#30456;&#23545;&#36739;&#23569;&#30340;&#26597;&#35810;&#65292;&#25915;&#20987;&#20063;&#33021;&#36896;&#25104;&#37325;&#22823;&#30340;&#26657;&#20934;&#38169;&#35823;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#20197;&#20102;&#35299;&#26657;&#20934;&#25915;&#20987;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#23545;&#25239;&#38450;&#24481;&#21644;&#26657;&#20934;&#26041;&#27861;&#23545;&#36825;&#20123;&#25915;&#20987;&#31867;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new framework of adversarial attacks, named calibration attacks, in which the attacks are generated and organized to trap victim models to be miscalibrated without altering their original accuracy, hence seriously endangering the trustworthiness of the models and any decision-making based on their confidence scores. Specifically, we identify four novel forms of calibration attacks: underconfidence attacks, overconfidence attacks, maximum miscalibration attacks, and random confidence attacks, in both the black-box and white-box setups. We then test these new attacks on typical victim models with comprehensive datasets, demonstrating that even with a relatively low number of queries, the attacks can create significant calibration mistakes. We further provide detailed analyses to understand different aspects of calibration attacks. Building on that, we investigate the effectiveness of widely used adversarial defences and calibration methods against these types of attacks, w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2311.03976</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Foundation Graph Model. (arXiv:2311.03976v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.03976
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#22270;&#27169;&#22411;FoToM&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#36827;&#34892;&#22270;&#39044;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#39046;&#22495;&#19978;&#23454;&#29616;&#20102;&#27491;&#21521;&#36801;&#31227;&#65292;&#24182;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#20027;&#35201;&#20248;&#21183;&#26159;&#22312;&#25968;&#25454;&#25110;&#26631;&#31614;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#26159;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#65292;&#20445;&#25345;&#39044;&#35757;&#32451;&#21644;&#30446;&#26631;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#33410;&#28857;&#21644;&#36793;&#23646;&#24615;&#19968;&#33268;&#12290;&#36825;&#20351;&#24471;&#26080;&#27861;&#22312;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#36801;&#31227;&#12290;&#33021;&#22815;&#22312;&#20219;&#24847;&#20219;&#21153;&#21644;&#39046;&#22495;&#19978;&#23454;&#29616;&#27491;&#21521;&#36801;&#31227;&#30340;&#27169;&#22411;&#23558;&#25104;&#20026;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23545;&#25239;&#24615;&#23545;&#27604;&#23398;&#20064;&#25552;&#20986;&#20102;FoToM&#65292;&#19968;&#31181;&#22522;&#20110;&#33410;&#28857;&#21644;&#36793;&#29305;&#24449;&#25490;&#38500;&#30340;&#22270;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;FoToM&#22312;&#22810;&#20010;&#22270;&#39046;&#22495;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24471;&#21040;&#20102;&#31532;&#19968;&#20010;&#22522;&#30784;&#22270;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#35780;&#20272;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27491;&#21521;&#36801;&#31227;&#12290;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#65292;&#24615;&#33021;&#26368;&#24046;&#26102;&#19982;&#26377;&#30417;&#30563;&#22522;&#32447;&#30456;&#24403;&#65292;76%&#30340;&#25968;&#25454;&#38598;&#22312;95%&#32622;&#20449;&#24230;&#19979;&#37117;&#26174;&#33879;&#20248;&#20110;&#26377;&#30417;&#30563;&#22522;&#32447;&#65288;P&#8804;0.01&#65289;&#65292;&#35823;&#24046;&#20943;&#23569;&#20102;8%&#33267;40%&#12290;
&lt;/p&gt;
&lt;p&gt;
The principal benefit of unsupervised graph representation learning is that a pre-trained model can be fine-tuned where data or labels are scarce. Existing approaches are domain specific, maintaining consistent node and edge attributes across the pre-training and target datasets. This precludes transfer to other domains. A model capable of positive transfer on arbitrary tasks and domains would represent the first foundation graph model.  In this work we use adversarial contrastive learning to present FoToM, a graph pre-training method based on node and edge feature exclusion. We use FoToM to pre-train models over multiple graph domains, producing the first foundation graph models. We demonstrate positive transfer on evaluation datasets from multiple domains, including domains not present in pre-training data. On all datasets performance is at worst on-par and on 76% significantly better than a supervised baseline ($P \leq 0.01$), with an 8 to 40% reduction in error at 95% confidence. C
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17249</link><description>&lt;p&gt;
&#25209;&#37327;&#26657;&#20934;&#65306;&#37325;&#26032;&#24605;&#32771;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#25552;&#31034;&#24037;&#31243;&#30340;&#26657;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Batch Calibration: Rethinking Calibration for In-Context Learning and Prompt Engineering. (arXiv:2309.17249v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#20559;&#35265;&#22240;&#32032;&#23548;&#33268;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;BC&#36890;&#36807;&#25511;&#21046;&#25209;&#37327;&#36755;&#20837;&#30340;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#38646;-shot&#21644;&#20165;&#25512;&#29702;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#24050;&#25104;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#39640;&#25928;&#23398;&#20064;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;LLM&#23384;&#22312;&#25552;&#31034;&#33030;&#24369;&#24615;&#21644;&#21508;&#31181;&#20559;&#35265;&#22240;&#32032;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#26684;&#24335;&#12289;&#36873;&#25321;&#24615;&#30340;&#34920;&#36798;&#26041;&#24335;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#31034;&#20363;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26657;&#20934;&#26041;&#27861;&#26469;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#24433;&#21709;&#24182;&#24674;&#22797;LLM&#30340;&#24615;&#33021;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29616;&#26377;&#30340;&#26657;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#35266;&#28857;&#24182;&#25581;&#31034;&#20102;&#22833;&#36133;&#26696;&#20363;&#12290;&#21463;&#36825;&#20123;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25209;&#37327;&#26657;&#20934;&#65288;BC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#30452;&#35266;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#25209;&#37327;&#36755;&#20837;&#20013;&#25511;&#21046;&#19978;&#19979;&#25991;&#20559;&#35265;&#65292;&#32479;&#19968;&#20102;&#21508;&#31181;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#19978;&#36848;&#38382;&#39064;&#12290;BC&#26159;&#38646;-shot&#12289;&#20165;&#25512;&#29702;&#21644;&#39069;&#22806;&#25104;&#26412;&#21487;&#24573;&#30053;&#12290;&#22312;&#23569;-shot&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;BC&#20197;&#23454;&#29616;&#20840;&#37096;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Prompting and in-context learning (ICL) have become efficient learning paradigms for large language models (LLMs). However, LLMs suffer from prompt brittleness and various bias factors in the prompt, including but not limited to the formatting, the choice verbalizers, and the ICL examples. To address this problem that results in unexpected performance degradation, calibration methods have been developed to mitigate the effects of these biases while recovering LLM performance. In this work, we first conduct a systematic analysis of the existing calibration methods, where we both provide a unified view and reveal the failure cases. Inspired by these analyses, we propose Batch Calibration (BC), a simple yet intuitive method that controls the contextual bias from the batched input, unifies various prior approaches, and effectively addresses the aforementioned issues. BC is zero-shot, inference-only, and incurs negligible additional costs. In the few-shot setup, we further extend BC to allo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#20013;&#25968;&#25454;&#20351;&#29992;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.12380</link><description>&lt;p&gt;
&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#65306;&#19968;&#39033;&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Methods for generating and evaluating synthetic longitudinal patient data: a systematic review. (arXiv:2309.12380v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#20197;&#35299;&#20915;&#21307;&#23398;&#39046;&#22495;&#20013;&#25968;&#25454;&#20351;&#29992;&#21644;&#38544;&#31169;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#25968;&#25454;&#30340;&#36805;&#29467;&#22686;&#38271;&#20419;&#36827;&#20102;&#21508;&#31181;&#32479;&#35745;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#65292;&#21152;&#24555;&#20102;&#30740;&#31350;&#21644;&#24320;&#21457;&#27963;&#21160;&#12290;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#34892;&#19994;&#37117;&#33021;&#20174;&#25968;&#25454;&#30340;&#22686;&#21152;&#20013;&#21516;&#31561;&#21463;&#30410;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#25968;&#25454;&#20351;&#29992;&#21644;&#38544;&#31169;&#35268;&#23450;&#30340;&#27861;&#24459;&#38480;&#21046;&#65292;&#20363;&#22914;&#21307;&#23398;&#39046;&#22495;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#32479;&#35745;&#25259;&#38706;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#12290;&#21512;&#25104;&#25968;&#25454;&#26159;&#22522;&#20110;&#19968;&#20123;&#29616;&#26377;&#25968;&#25454;&#29983;&#25104;&#30340;&#65292;&#30446;&#30340;&#26159;&#23613;&#21487;&#33021;&#22320;&#22797;&#21046;&#23427;&#20204;&#65292;&#24182;&#20805;&#24403;&#30495;&#23454;&#25935;&#24863;&#25968;&#25454;&#30340;&#20195;&#29702;&#12290;&#26412;&#25991;&#23545;&#29983;&#25104;&#21644;&#35780;&#20272;&#21512;&#25104;&#32437;&#21521;&#24739;&#32773;&#25968;&#25454;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#32508;&#36848;&#65292;&#36825;&#26159;&#21307;&#23398;&#39046;&#22495;&#20013;&#19968;&#31181;&#24120;&#35265;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#35813;&#32508;&#36848;&#36981;&#24490;PRISMA&#25351;&#21335;&#65292;&#24182;&#28085;&#30422;&#20102;&#33258;2022&#24180;&#24213;&#20197;&#26469;&#30340;&#20116;&#20010;&#25968;&#25454;&#24211;&#30340;&#25991;&#29486;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;17&#31181;&#26041;&#27861;&#65292;&#20174;&#20256;&#32479;&#26041;&#27861;&#21040;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of data in recent years has led to the advancement and utilization of various statistical and deep learning techniques, thus expediting research and development activities. However, not all industries have benefited equally from the surge in data availability, partly due to legal restrictions on data usage and privacy regulations, such as in medicine. To address this issue, various statistical disclosure and privacy-preserving methods have been proposed, including the use of synthetic data generation. Synthetic data are generated based on some existing data, with the aim of replicating them as closely as possible and acting as a proxy for real sensitive data. This paper presents a systematic review of methods for generating and evaluating synthetic longitudinal patient data, a prevalent data type in medicine. The review adheres to the PRISMA guidelines and covers literature from five databases until the end of 2022. The paper describes 17 methods, ranging from traditi
&lt;/p&gt;</description></item><item><title>Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2309.03468</link><description>&lt;p&gt;
&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#23545;&#20110;Bongard&#38382;&#39064;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03468
&lt;/p&gt;
&lt;p&gt;
Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#27010;&#24565;&#24182;&#36827;&#34892;&#20998;&#31867;&#30340;&#26234;&#21147;&#27979;&#35797;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;Bongard&#38382;&#39064;&#20013;&#20934;&#30830;&#29575;&#36739;&#20302;&#12290;&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#36825;&#26159;&#22240;&#20026;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#25972;&#21512;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#65292;&#32780;&#26159;&#20165;&#20381;&#36182;&#20110;&#21333;&#20010;&#25903;&#25345;&#22270;&#20687;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;Bongard&#38382;&#39064;&#26102;&#23384;&#22312;&#22256;&#38590;&#12290;Bongard&#38382;&#39064;&#26159;&#19968;&#31181;&#38656;&#35201;&#20174;&#19968;&#32452;&#27491;&#36127;&#8220;&#25903;&#25345;&#8221;&#22270;&#20687;&#20013;&#25512;&#23548;&#20986;&#25277;&#35937;&#8220;&#27010;&#24565;&#8221;&#65292;&#28982;&#21518;&#23545;&#20110;&#26032;&#30340;&#26597;&#35810;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#21028;&#26029;&#23427;&#26159;&#21542;&#25551;&#36848;&#20102;&#20851;&#38190;&#27010;&#24565;&#30340;&#26234;&#21147;&#27979;&#35797;&#12290;&#22312;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;Bongard&#38382;&#39064;&#30340;&#22522;&#20934;&#27979;&#35797;Bongard-HOI&#20013;&#65292;&#29616;&#26377;&#26041;&#27861;&#30340;&#20934;&#30830;&#29575;&#20165;&#36798;&#21040;&#20102;66%&#65288;&#20598;&#28982;&#20934;&#30830;&#29575;&#20026;50%&#65289;&#12290;&#20302;&#20934;&#30830;&#29575;&#36890;&#24120;&#24402;&#22240;&#20110;&#31070;&#32463;&#32593;&#32476;&#32570;&#20047;&#21457;&#29616;&#31867;&#20284;&#20154;&#31867;&#31526;&#21495;&#35268;&#21017;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25351;&#20986;&#65292;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#30001;&#20110;&#19968;&#20010;&#26356;&#31616;&#21333;&#30340;&#38382;&#39064;&#32780;&#22833;&#21435;&#20102;&#20934;&#30830;&#24615;&#65306;&#23427;&#20204;&#27809;&#26377;&#23558;&#25903;&#25345;&#38598;&#21512;&#20013;&#30340;&#20449;&#24687;&#20316;&#20026;&#19968;&#20010;&#25972;&#20307;&#21152;&#20837;&#65292;&#32780;&#26159;&#20381;&#36182;&#20110;&#20174;&#21333;&#20010;&#25903;&#25345;&#20013;&#25552;&#21462;&#30340;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#22240;&#20026;&#19982;&#28041;&#21450;&#23545;&#35937;&#20998;&#31867;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#19981;&#21516;&#65292;&#19968;&#20010;&#20856;&#22411;&#30340;Bongard&#38382;&#39064;&#20013;&#30340;&#8220;&#20851;&#38190;&#27010;&#24565;&#8221;&#21482;&#33021;&#20351;&#29992;&#22810;&#20010;&#27491;&#20363;&#21644;&#22810;&#20010;&#21453;&#20363;&#26469;&#21306;&#20998;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#36328;&#22270;&#20687;&#19978;&#19979;&#25991;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current machine learning methods struggle to solve Bongard problems, which are a type of IQ test that requires deriving an abstract "concept" from a set of positive and negative "support" images, and then classifying whether or not a new query image depicts the key concept. On Bongard-HOI, a benchmark for natural-image Bongard problems, existing methods have only reached 66% accuracy (where chance is 50%). Low accuracy is often attributed to neural nets' lack of ability to find human-like symbolic rules. In this work, we point out that many existing methods are forfeiting accuracy due to a much simpler problem: they do not incorporate information contained in the support set as a whole, and rely instead on information extracted from individual supports. This is a critical issue, because unlike in few-shot learning tasks concerning object classification, the "key concept" in a typical Bongard problem can only be distinguished using multiple positives and multiple negatives. We explore a
&lt;/p&gt;</description></item><item><title>MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.02490</link><description>&lt;p&gt;
MM-Vet: &#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32508;&#21512;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02490
&lt;/p&gt;
&lt;p&gt;
MM-Vet&#26159;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#22797;&#26434;&#20219;&#21153;&#19978;&#30340;&#32508;&#21512;&#33021;&#21147;&#12290;&#35813;&#26631;&#20934;&#35299;&#20915;&#20102;&#22914;&#20309;&#32467;&#26500;&#21270;&#21644;&#35780;&#20272;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#12289;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#38382;&#39064;&#21644;&#22238;&#31572;&#31867;&#22411;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#21450;&#22914;&#20309;&#25552;&#20379;&#27169;&#22411;&#27934;&#23519;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#33021;&#21147;&#65292;MM-Vet&#23637;&#31034;&#20102;&#26377;&#36259;&#30340;&#33021;&#21147;&#21644;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#19968;&#20010;&#35780;&#20272;&#26631;&#20934;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;&#22797;&#26434;&#22810;&#27169;&#24577;&#20219;&#21153;&#19978;&#30340;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#34920;&#29616;&#12290;&#26368;&#36817;&#30340;LMM&#23637;&#31034;&#20102;&#21508;&#31181;&#26377;&#36259;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35299;&#20915;&#20070;&#20889;&#22312;&#40657;&#26495;&#19978;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#25512;&#29702;&#26032;&#38395;&#22270;&#29255;&#20013;&#30340;&#20107;&#20214;&#21644;&#21517;&#20154;&#65292;&#20197;&#21450;&#35299;&#37322;&#35270;&#35273;&#31505;&#35805;&#12290;&#24555;&#36895;&#30340;&#27169;&#22411;&#36827;&#27493;&#32473;&#35780;&#20272;&#26631;&#20934;&#30340;&#24320;&#21457;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#38382;&#39064;&#21253;&#25324;&#65306;&#65288;1&#65289;&#22914;&#20309;&#31995;&#32479;&#22320;&#26500;&#24314;&#21644;&#35780;&#20272;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#65307;&#65288;2&#65289;&#22914;&#20309;&#35774;&#35745;&#36866;&#29992;&#20110;&#19981;&#21516;&#31867;&#22411;&#38382;&#39064;&#21644;&#22238;&#31572;&#30340;&#35780;&#20272;&#25351;&#26631;&#65307;&#65288;3&#65289;&#22914;&#20309;&#32473;&#20986;&#36229;&#20986;&#31616;&#21333;&#24615;&#33021;&#25490;&#21517;&#30340;&#27169;&#22411;&#27934;&#23519;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MM-Vet&#65292;&#22522;&#20110;&#36825;&#26679;&#19968;&#20010;&#27934;&#23519;&#65306;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26377;&#36259;&#33021;&#21147;&#36890;&#24120;&#36890;&#36807;&#19968;&#31181;&#36890;&#25165;&#27169;&#22411;&#33021;&#22815;&#25972;&#21512;&#19981;&#21516;&#30340;&#26680;&#24515;&#35270;&#35273;-&#35821;&#35328;&#65288;VL&#65289;&#33021;&#21147;&#26469;&#23454;&#29616;&#12290;MM-Vet&#23450;&#20041;&#20102;6&#20010;&#26680;&#24515;VL&#33021;&#21147;&#65292;&#24182;&#26816;&#26597;&#20102;&#20174;&#36825;&#20123;&#33021;&#21147;&#32452;&#21512;&#20013;&#24471;&#20986;&#30340;16&#31181;&#26377;&#36259;&#30340;&#25972;&#21512;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose MM-Vet, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various intriguing abilities, such as solving math problems written on the blackboard, reasoning about events and celebrities in news images, and explaining visual jokes. Rapid model advancements pose challenges to evaluation benchmark development. Problems include: (1) How to systematically structure and evaluate the complicated multimodal tasks; (2) How to design evaluation metrics that work well across question and answer types; and (3) How to give model insights beyond a simple performance ranking. To this end, we present MM-Vet, designed based on the insight that the intriguing ability to solve complicated tasks is often achieved by a generalist model being able to integrate different core vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and examines the 16 integrations of interest derived from the capability combin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#21253;&#21547;&#26080;&#25928;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#31934;&#24230;&#21644;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21482;&#20351;&#29992;&#26377;&#25928;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26631;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#26080;&#25928;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.15166</link><description>&lt;p&gt;
&#20174;&#26080;&#25928;&#25968;&#25454;&#20013;&#23398;&#20064;&#65306;&#20851;&#20110;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Learning from Invalid Data: On Constraint Satisfaction in Generative Models. (arXiv:2306.15166v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15166
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#21253;&#21547;&#26080;&#25928;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#29983;&#25104;&#27169;&#22411;&#30340;&#35757;&#32451;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#32467;&#26524;&#30340;&#31934;&#24230;&#21644;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#21482;&#20351;&#29992;&#26377;&#25928;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26631;&#20934;&#27169;&#22411;&#30456;&#27604;&#65292;&#22522;&#20110;&#26080;&#25928;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#26126;&#26174;&#20248;&#20110;&#26631;&#20934;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35821;&#38899;&#31561;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#26377;&#22823;&#37327;&#30340;&#25968;&#25454;&#38598;&#65292;&#23427;&#20204;&#20173;&#28982;&#22312;&#31934;&#24230;&#19978;&#23384;&#22312;&#22256;&#38590;&#65292;&#29983;&#25104;&#20986;&#29289;&#29702;&#19978;&#26080;&#25928;&#25110;&#20107;&#23454;&#19978;&#19981;&#27491;&#30830;&#30340;&#25968;&#25454;&#12290;&#24403;&#29983;&#25104;&#30340;&#25968;&#25454;&#24517;&#39035;&#28385;&#36275;&#32422;&#26463;&#26465;&#20214;&#26102;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#20005;&#37325;&#65292;&#20363;&#22914;&#65292;&#22312;&#24037;&#31243;&#35774;&#35745;&#20013;&#28385;&#36275;&#20135;&#21697;&#35268;&#26684;&#25110;&#32773;&#22312;&#33258;&#28982;&#22330;&#26223;&#20013;&#36981;&#23432;&#29289;&#29702;&#23450;&#24459;&#12290;&#20026;&#20102;&#25552;&#39640;&#31934;&#24230;&#24182;&#20445;&#25345;&#22810;&#26679;&#24615;&#21644;&#20445;&#30495;&#24230;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26426;&#21046;&#65292;&#21033;&#29992;&#21253;&#21547;&#26080;&#25928;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26368;&#23567;&#21270;&#20102;&#29983;&#25104;&#20998;&#24067;&#19982;&#26377;&#25928;&#20808;&#39564;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21516;&#26102;&#26368;&#22823;&#21270;&#20102;&#19982;&#26080;&#25928;&#20998;&#24067;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#23558;GAN&#21644;DDPM&#31561;&#29983;&#25104;&#27169;&#22411;&#19982;&#26080;&#25928;&#25968;&#25454;&#19968;&#36215;&#35757;&#32451;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#20165;&#20351;&#29992;&#26377;&#25928;&#25968;&#25454;&#28857;&#36827;&#34892;&#35757;&#32451;&#30340;&#26631;&#20934;&#27169;&#22411;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#30340;&#35757;&#32451;&#36807;&#31243;&#29983;&#25104;&#20102;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
Generative models have demonstrated impressive results in vision, language, and speech. However, even with massive datasets, they struggle with precision, generating physically invalid or factually incorrect data. This is particularly problematic when the generated data must satisfy constraints, for example, to meet product specifications in engineering design or to adhere to the laws of physics in a natural scene. To improve precision while preserving diversity and fidelity, we propose a novel training mechanism that leverages datasets of constraint-violating data points, which we consider invalid. Our approach minimizes the divergence between the generative distribution and the valid prior while maximizing the divergence with the invalid distribution. We demonstrate how generative models like GANs and DDPMs that we augment to train with invalid data vastly outperform their standard counterparts which solely train on valid data points. For example, our training procedure generates up 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26550;&#26500;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;MAC&#21644;&#24310;&#36831;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19982;&#20351;&#29992;&#26356;&#22810;&#36164;&#28304;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.15798</link><description>&lt;p&gt;
&#20851;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#26550;&#26500;&#21387;&#32553;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Architectural Compression of Text-to-Image Diffusion Models. (arXiv:2305.15798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#26550;&#26500;&#21387;&#32553;&#26041;&#27861;&#23454;&#29616;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#39640;&#25928;&#21270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#26041;&#27861;&#65292;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#30340;&#21516;&#26102;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;MAC&#21644;&#24310;&#36831;&#65292;&#26368;&#32456;&#23454;&#29616;&#20102;&#19982;&#20351;&#29992;&#26356;&#22810;&#36164;&#28304;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#31454;&#20105;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#65288;SDMs&#65289;&#20013;&#20986;&#33394;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#32467;&#26524;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36817;&#26399;&#20851;&#20110;&#39640;&#25928;SDMs&#30340;&#30740;&#31350;&#23558;&#37325;&#28857;&#25918;&#22312;&#20943;&#23569;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#21644;&#21033;&#29992;&#32593;&#32476;&#37327;&#21270;&#19978;&#12290;&#19982;&#36825;&#20123;&#26041;&#21521;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;&#22359;&#21024;&#38500;&#30693;&#35782;&#25552;&#21462;SDMs&#65288;BK-SDMs&#65289;&#65292;&#24378;&#35843;&#20102;&#32463;&#20856;&#26550;&#26500;&#21387;&#32553;&#22312;&#36890;&#29992;T2I&#21512;&#25104;&#20013;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#20174;SDMs&#30340;U-Net&#20013;&#21024;&#38500;&#20102;&#20960;&#20010;&#27531;&#24046;&#21644;&#27880;&#24847;&#21147;&#22359;&#65292;&#20351;&#21442;&#25968;&#25968;&#37327;&#12289;&#27599;&#20010;&#37319;&#26679;&#27493;&#39588;&#30340;MAC&#21644;&#24310;&#36831;&#20943;&#23569;&#20102;&#36229;&#36807;30&#65285;&#12290;&#25105;&#20204;&#22312;&#21333;&#20010;A100 GPU&#19978;&#20165;&#20351;&#29992;0.22M LAION&#23545;&#36827;&#34892;&#33976;&#39311;&#39044;&#35757;&#32451;&#65288;&#23569;&#20110;&#20840;&#20307;&#35757;&#32451;&#23545;&#30340;0.1&#65285;&#65289;&#12290;&#23613;&#31649;&#20351;&#29992;&#26377;&#38480;&#30340;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30340;&#32039;&#20945;&#22411;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#20256;&#36882;&#30340;&#30693;&#35782;&#27169;&#20223;&#21407;&#22987;SDM&#65292;&#24182;&#22312;&#23545;&#25239;&#36739;&#22823;&#30340;&#22810;&#21313;&#20159;&#21442;&#25968;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exceptional text-to-image (T2I) generation results of Stable Diffusion models (SDMs) come with substantial computational demands. To resolve this issue, recent research on efficient SDMs has prioritized reducing the number of sampling steps and utilizing network quantization. Orthogonal to these directions, this study highlights the power of classical architectural compression for general-purpose T2I synthesis by introducing block-removed knowledge-distilled SDMs (BK-SDMs). We eliminate several residual and attention blocks from the U-Net of SDMs, obtaining over a 30% reduction in the number of parameters, MACs per sampling step, and latency. We conduct distillation-based pretraining with only 0.22M LAION pairs (fewer than 0.1% of the full training pairs) on a single A100 GPU. Despite being trained with limited resources, our compact models can imitate the original SDM by benefiting from transferred knowledge and achieve competitive results against larger multi-billion parameter models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#21644;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#30456;&#20851;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.05655</link><description>&lt;p&gt;
&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#30340;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
Localisation of Regularised and Multiview Support Vector Machine Learning. (arXiv:2304.05655v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.05655
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#65292;&#35777;&#26126;&#20102;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#65292;&#30740;&#31350;&#20102;&#19982;&#25439;&#22833;&#20989;&#25968;&#21644;&#36755;&#20837;&#31354;&#38388;&#32500;&#24230;&#30456;&#20851;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102; H.Q. Minh&#12289;L. Bazzani &#21644; V. Murino &#22312;&#12298;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12299;&#65288;Journal of Machine Learning Research&#65289;&#20013;&#20171;&#32461;&#30340;&#19968;&#31181;&#28041;&#21450;&#31639;&#23376;&#20540;&#27491;&#23450;&#26680;&#21450;&#20854;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#30340;&#27491;&#21017;&#21270;&#21644;&#22810;&#35270;&#35282;&#25903;&#25345;&#21521;&#37327;&#26426;&#23398;&#20064;&#38382;&#39064;&#30340;&#26412;&#22320;&#21270;&#29256;&#26412;&#30340;&#19968;&#20123;&#34920;&#31034;&#23450;&#29702;&#12290;&#32467;&#26524;&#28041;&#21450;&#21040;&#32771;&#34385;&#20984;&#25110;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#20197;&#21450;&#26377;&#38480;&#25110;&#26080;&#38480;&#32500;&#36755;&#20837;&#31354;&#38388;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#19968;&#33324;&#26694;&#26550;&#20801;&#35768;&#19968;&#20123;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#26080;&#38480;&#32500;&#36755;&#20837;&#31354;&#38388;&#21644;&#38750;&#20984;&#25439;&#22833;&#20989;&#25968;&#65292;&#29305;&#21035;&#26159;&#24403;&#25439;&#22833;&#20989;&#25968;&#20026; G&#226;teaux &#21487;&#24494;&#20989;&#25968;&#26102;&#12290;&#23545;&#23548;&#33268;&#37096;&#20998;&#38750;&#32447;&#24615;&#38382;&#39064;&#30340;&#25351;&#25968;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20102;&#35814;&#32454;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove a few representer theorems for a localised version of the regularised and multiview support vector machine learning problem introduced by H.Q.~Minh, L.~Bazzani, and V.~Murino, \textit{Journal of Machine Learning Research}, \textbf{17}(2016) 1--72, that involves operator valued positive semidefinite kernels and their reproducing kernel Hilbert spaces. The results concern general cases when convex or nonconvex loss functions and finite or infinite dimensional input spaces are considered. We show that the general framework allows infinite dimensional input spaces and nonconvex loss functions for some special cases, in particular in case the loss functions are G\^ateaux differentiable. Detailed calculations are provided for the exponential least squares loss functions that leads to partially nonlinear problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;OBCD&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#19968;&#33324;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#12290; OBCD&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#33719;&#24471;&#20005;&#26684;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.03641</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#27491;&#20132;&#32422;&#26463;&#19979;&#30340;&#38750;&#20809;&#28369;&#32452;&#21512;&#20248;&#21270;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Block Coordinate Descent Method for Nonsmooth Composite Optimization under Orthogonality Constraints. (arXiv:2304.03641v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03641
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;OBCD&#65292;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#19968;&#33324;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#12290; OBCD&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#24182;&#19988;&#33719;&#24471;&#20005;&#26684;&#30340;&#25910;&#25947;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#27491;&#20132;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#32452;&#21512;&#20248;&#21270;&#22312;&#32479;&#35745;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;&#30001;&#20110;&#20854;&#38750;&#20984;&#24615;&#21644;&#38750;&#20809;&#28369;&#24615;&#36136;&#65292;&#35813;&#38382;&#39064;&#36890;&#24120;&#24456;&#38590;&#27714;&#35299;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#21463;&#21040;&#20197;&#19979;&#19968;&#20010;&#25110;&#22810;&#20010;&#38480;&#21046;&#30340;&#38480;&#21046;&#65306;&#65288;i&#65289;&#23427;&#20204;&#26159;&#38656;&#35201;&#27599;&#27425;&#36845;&#20195;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#20840;&#26799;&#24230;&#26041;&#27861;&#65307;&#65288;ii&#65289;&#23427;&#20204;&#26080;&#27861;&#35299;&#20915;&#19968;&#33324;&#30340;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#65307;&#65288;iii&#65289;&#23427;&#20204;&#26159;&#19981;&#21487;&#34892;&#26041;&#27861;&#65292;&#24182;&#19988;&#21482;&#33021;&#22312;&#26497;&#38480;&#28857;&#22788;&#23454;&#29616;&#35299;&#30340;&#21487;&#34892;&#24615;&#65307;&#65288;iv&#65289;&#23427;&#20204;&#32570;&#20047;&#20005;&#26684;&#30340;&#25910;&#25947;&#20445;&#35777;&#65307;&#65288;v&#65289;&#23427;&#20204;&#21482;&#33021;&#33719;&#24471;&#20851;&#38190;&#28857;&#30340;&#24369;&#26368;&#20248;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22359;&#22352;&#26631;&#19979;&#38477;&#26041;&#27861;OBCD&#65292;&#29992;&#20110;&#35299;&#20915;&#27491;&#20132;&#32422;&#26463;&#19979;&#30340;&#19968;&#33324;&#38750;&#20809;&#28369;&#32452;&#21512;&#38382;&#39064;&#12290;OBCD&#26159;&#19968;&#31181;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#20302;&#30340;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20250;&#26356;&#26032;...
&lt;/p&gt;
&lt;p&gt;
Nonsmooth composite optimization with orthogonality constraints has a broad spectrum of applications in statistical learning and data science. However, this problem is generally challenging to solve due to its non-convex and non-smooth nature. Existing solutions are limited by one or more of the following restrictions: (i) they are full gradient methods that require high computational costs in each iteration; (ii) they are not capable of solving general nonsmooth composite problems; (iii) they are infeasible methods and can only achieve the feasibility of the solution at the limit point; (iv) they lack rigorous convergence guarantees; (v) they only obtain weak optimality of critical points. In this paper, we propose \textit{\textbf{OBCD}}, a new Block Coordinate Descent method for solving general nonsmooth composite problems under Orthogonality constraints. \textit{\textbf{OBCD}} is a feasible method with low computation complexity footprints. In each iteration, our algorithm updates $
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;</title><link>http://arxiv.org/abs/2303.16668</link><description>&lt;p&gt;
&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#25308;&#21344;&#24237;&#23481;&#38169;&#32858;&#21512;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Byzantine-Resilient Aggregation Scheme for Federated Learning via Matrix Autoregression on Client Updates. (arXiv:2303.16668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#33258;&#22238;&#24402;&#30340;&#32852;&#37030;&#23398;&#20064;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25308;&#21344;&#24237;&#25915;&#20987;&#38450;&#24481;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;FLANDERS&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#32858;&#21512;&#26041;&#26696;&#65292;&#21487;&#20197;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#12290;FLANDERS&#23558;&#27599;&#20010;FL&#36718;&#27425;&#20013;&#30001;&#23458;&#25143;&#31471;&#21457;&#36865;&#30340;&#26412;&#22320;&#27169;&#22411;&#26356;&#26032;&#35270;&#20026;&#30697;&#38453;&#20540;&#26102;&#38388;&#24207;&#21015;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#35266;&#27979;&#19982;&#30001;&#30697;&#38453;&#33258;&#22238;&#24402;&#39044;&#27979;&#27169;&#22411;&#20272;&#35745;&#30340;&#35266;&#27979;&#36827;&#34892;&#27604;&#36739;&#65292;&#35782;&#21035;&#24694;&#24847;&#23458;&#25143;&#31471;&#20316;&#20026;&#36825;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#24322;&#24120;&#20540;&#12290;&#22312;&#19981;&#21516;FL&#35774;&#32622;&#19979;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FLANDERS&#22312;&#25269;&#24481;&#25308;&#21344;&#24237;&#25915;&#20987;&#26041;&#38754;&#19982;&#26368;&#24378;&#22823;&#30340;&#22522;&#32447;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#19982;&#29616;&#26377;&#30340;&#38450;&#24481;&#31574;&#30053;&#30456;&#27604;&#65292; FLANDERS&#21363;&#20351;&#22312;&#26497;&#20854;&#20005;&#37325;&#30340;&#25915;&#20987;&#22330;&#26223;&#19979;&#20173;&#28982;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose FLANDERS, a novel federated learning (FL) aggregation scheme robust to Byzantine attacks. FLANDERS considers the local model updates sent by clients at each FL round as a matrix-valued time series. Then, it identifies malicious clients as outliers of this time series by comparing actual observations with those estimated by a matrix autoregressive forecasting model. Experiments conducted on several datasets under different FL settings demonstrate that FLANDERS matches the robustness of the most powerful baselines against Byzantine clients. Furthermore, FLANDERS remains highly effective even under extremely severe attack scenarios, as opposed to existing defense strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2303.09874</link><description>&lt;p&gt;
&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#24863;&#30693;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Disentangling the Link Between Image Statistics and Human Perception. (arXiv:2303.09874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09874
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#24433;&#21709;&#20154;&#31867;&#24863;&#30693;&#12290;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#65292;&#35770;&#25991;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;20&#19990;&#32426;50&#24180;&#20195;&#65292;&#38669;&#21202;&#26031;&#24052;&#27931;&#21644;&#24343;&#38647;&#24503;&#38463;&#29305;&#32435;&#22827;&#25552;&#20986;&#20102;&#24863;&#23448;&#31995;&#32479;&#21644;&#23427;&#20204;&#22914;&#20309;&#36866;&#24212;&#29615;&#22659;&#20043;&#38388;&#30340;&#20851;&#31995;&#65306;&#26089;&#26399;&#35270;&#35273;&#30340;&#36827;&#21270;&#26159;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#20256;&#36882;&#20851;&#20110;&#36755;&#20837;&#20449;&#21495;&#30340;&#20449;&#24687;&#12290;&#25353;&#29031;&#39321;&#20892;&#30340;&#23450;&#20041;&#65292;&#36825;&#20123;&#20449;&#24687;&#26159;&#36890;&#36807;&#33258;&#28982;&#22330;&#26223;&#20013;&#25293;&#25668;&#30340;&#22270;&#20687;&#30340;&#27010;&#29575;&#26469;&#25551;&#36848;&#30340;&#12290;&#30001;&#20110;&#35745;&#31639;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20197;&#21069;&#26080;&#27861;&#30452;&#25509;&#20934;&#30830;&#22320;&#39044;&#27979;&#22270;&#20687;&#30340;&#27010;&#29575;&#12290;&#23613;&#31649;&#36825;&#31181;&#24819;&#27861;&#30340;&#25506;&#32034;&#26159;&#38388;&#25509;&#30340;&#65292;&#20027;&#35201;&#22522;&#20110;&#22270;&#20687;&#23494;&#24230;&#30340;&#36807;&#24230;&#31616;&#21270;&#27169;&#22411;&#25110;&#31995;&#32479;&#35774;&#35745;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#37325;&#29616;&#21508;&#31181;&#29983;&#29702;&#21644;&#24515;&#29702;&#29289;&#29702;&#29616;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30452;&#25509;&#35780;&#20272;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#65292;&#24182;&#20998;&#26512;&#23427;&#22914;&#20309;&#30830;&#23450;&#30693;&#35273;&#28789;&#25935;&#24230;&#12290;&#25105;&#20204;&#20351;&#29992;&#19982;&#20154;&#31867;&#24847;&#35265;&#30456;&#20851;&#24615;&#24456;&#39640;&#30340;&#22270;&#20687;&#36136;&#37327;&#25351;&#26631;&#20316;&#20026;&#20154;&#31867;&#35270;&#35273;&#30340;&#20195;&#29702;&#65292;&#20197;&#21450;&#19968;&#20010;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#26469;&#30452;&#25509;&#20272;&#35745;&#33258;&#28982;&#22270;&#20687;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26681;&#25454;Barlow&#21644;Attneave&#29702;&#35770;&#39044;&#27979;&#30340;&#22270;&#20687;&#32479;&#35745;&#19982;&#20154;&#31867;&#30693;&#35273;&#20043;&#38388;&#23384;&#22312;&#31995;&#32479;&#24615;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20855;&#26377;&#26356;&#20016;&#23500;&#32479;&#35745;&#29305;&#24449;&#30340;&#33258;&#28982;&#22270;&#20687;&#34987;&#24863;&#30693;&#20026;&#20855;&#26377;&#26356;&#22823;&#30340;&#26174;&#30528;&#24615;&#26469;&#35828;&#26126;&#36825;&#19968;&#21457;&#29616;&#65292;&#36825;&#26159;&#36890;&#36807;&#35270;&#35273;&#25628;&#32034;&#23454;&#39564;&#27979;&#37327;&#30340;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#30452;&#25509;&#25903;&#25345;Barlow&#21644;Attneave&#29702;&#35770;&#30340;&#35777;&#25454;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#32479;&#35745;&#19982;&#30693;&#35273;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the 1950s Horace Barlow and Fred Attneave suggested a connection between sensory systems and how they are adapted to the environment: early vision evolved to maximise the information it conveys about incoming signals. Following Shannon's definition, this information was described using the probability of the images taken from natural scenes. Previously, direct accurate predictions of image probabilities were not possible due to computational limitations. Despite the exploration of this idea being indirect, mainly based on oversimplified models of the image density or on system design methods, these methods had success in reproducing a wide range of physiological and psychophysical phenomena. In this paper, we directly evaluate the probability of natural images and analyse how it may determine perceptual sensitivity. We employ image quality metrics that correlate well with human opinion as a surrogate of human vision, and an advanced generative model to directly estimate the probabil
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.03106</link><description>&lt;p&gt;
&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#29992;&#20110;&#27169;&#22411;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Rotation Invariant Quantization for Model Compression. (arXiv:2303.03106v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.03106
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#21516;&#23618;&#27425;&#19978;&#23454;&#29616;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#65292;&#29992;&#20110;&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#21387;&#32553;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#22312;&#22810;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#21387;&#32553;&#26159;&#19968;&#31181;&#23558;&#22823;&#22411;&#12289;&#28040;&#32791;&#20869;&#23384;&#30340;&#27169;&#22411;&#37096;&#32626;&#21040;&#20869;&#23384;&#36164;&#28304;&#26377;&#38480;&#35774;&#22791;&#19978;&#30340;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;NN&#27169;&#22411;&#21387;&#32553;&#30340;&#36895;&#29575;-&#22833;&#30495;&#26435;&#34913;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26059;&#36716;&#19981;&#21464;&#37327;&#37327;&#21270;&#65288;RIQ&#65289;&#25216;&#26415;&#65292;&#23427;&#21033;&#29992;&#19968;&#20010;&#21333;&#19968;&#21442;&#25968;&#37327;&#21270;&#25972;&#20010;NN&#27169;&#22411;&#65292;&#22312;&#27599;&#20010;&#23618;&#27425;&#19978;&#24471;&#21040;&#19981;&#21516;&#30340;&#36895;&#29575;&#65292;&#21363;&#28151;&#21512;&#31934;&#24230;&#37327;&#21270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26059;&#36716;&#19981;&#21464;&#37327;&#26041;&#27861;&#22312;&#21387;&#32553;&#26041;&#38754;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;RIQ&#36827;&#34892;&#20102;&#20005;&#26684;&#35780;&#20272;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#21508;&#31181;&#27169;&#22411;&#21644;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#20363;&#22914;&#65292;RIQ&#22312;&#39044;&#35757;&#32451;&#30340;VGG&#31264;&#23494;&#21644;&#20462;&#21098;&#27169;&#22411;&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;19.4&#20493;&#21644;52.9&#20493;&#30340;&#21387;&#32553;&#27604;&#65292;&#31934;&#24230;&#38477;&#20302;&#23567;&#20110;0.4%&#12290;&#20195;&#30721;&#21487;&#20197;&#22312;\url{https://github.com/ehaleva/RIQ}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Post-training Neural Network (NN) model compression is an attractive approach for deploying large, memory-consuming models on devices with limited memory resources. In this study, we investigate the rate-distortion tradeoff for NN model compression. First, we suggest a Rotation-Invariant Quantization (RIQ) technique that utilizes a single parameter to quantize the entire NN model, yielding a different rate at each layer, i.e., mixed-precision quantization. Then, we prove that our rotation-invariant approach is optimal in terms of compression. We rigorously evaluate RIQ and demonstrate its capabilities on various models and tasks. For example, RIQ facilitates $\times 19.4$ and $\times 52.9$ compression ratios on pre-trained VGG dense and pruned models, respectively, with $&lt;0.4\%$ accuracy degradation. Code is available in \url{https://github.com/ehaleva/RIQ}.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#30340;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968; $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>http://arxiv.org/abs/2302.08766</link><description>&lt;p&gt;
&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#30340;&#19979;&#30028;&#21644;&#36817;&#20284;&#26368;&#20248;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Lower Bound and a Near-Optimal Algorithm for Bilevel Empirical Risk Minimization. (arXiv:2302.08766v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#23618;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#20351;&#29992;&#30340;&#26799;&#24230;&#35745;&#31639;&#27425;&#25968; $O((n+m)^{\frac{1}{2}}\varepsilon^{-1})$&#65292;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#23618;&#26368;&#20248;&#21270;&#38382;&#39064;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#19978;&#23618;&#21644;&#19979;&#23618;&#30446;&#26631;&#23545;&#24212;&#20110;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#24182;&#22240;&#27492;&#20855;&#26377;&#24635;&#21644;&#32467;&#26500;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33879;&#21517;&#30340;SARAH&#31639;&#27861;&#30340;&#21452;&#23618;&#25193;&#23637;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#38656;&#35201;$\mathcal {O}((n+m)^{\frac{1}{2}}\varepsilon ^{-1})$&#27425;&#26799;&#24230;&#35745;&#31639;&#25165;&#33021;&#23454;&#29616;$\varepsilon$&#31283;&#23450;&#24615;&#65292;&#20854;&#20013;$n+m$&#26159;&#26679;&#26412;&#24635;&#25968;&#65292;&#36825;&#27604;&#20808;&#21069;&#25152;&#26377;&#30340;&#21452;&#23618;&#31639;&#27861;&#37117;&#35201;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#19979;&#30028;&#65292;&#29992;&#20110;&#24471;&#21040;&#21452;&#23618;&#38382;&#39064;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#36817;&#20284;&#31283;&#23450;&#28857;&#25152;&#38656;&#30340;oracle&#35843;&#29992;&#27425;&#25968;&#12290;&#36825;&#20010;&#19979;&#30028;&#27491;&#26159;&#25105;&#20204;&#30340;&#31639;&#27861;&#25152;&#36798;&#21040;&#30340;&#65292;&#22240;&#27492;&#22312;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#38754;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bilevel optimization problems, which are problems where two optimization problems are nested, have more and more applications in machine learning. In many practical cases, the upper and the lower objectives correspond to empirical risk minimization problems and therefore have a sum structure. In this context, we propose a bilevel extension of the celebrated SARAH algorithm. We demonstrate that the algorithm requires $\mathcal{O}((n+m)^{\frac12}\varepsilon^{-1})$ gradient computations to achieve $\varepsilon$-stationarity with $n+m$ the total number of samples, which improves over all previous bilevel algorithms. Moreover, we provide a lower bound on the number of oracle calls required to get an approximate stationary point of the objective function of the bilevel problem. This lower bound is attained by our algorithm, which is therefore optimal in terms of sample complexity.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#20013;&#30340;&#22797;&#29616;&#36880;&#28176;&#37325;&#26500;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#39537;&#21160;&#20449;&#21495;&#65292;&#20174;&#32780;&#21487;&#38752;&#22320;&#25512;&#26029;&#20849;&#20139;&#22240;&#26524;&#39537;&#21160;&#32773;&#65292;&#20854;&#24050;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#36827;&#34892;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2301.13516</link><description>&lt;p&gt;
&#24490;&#29615;&#25581;&#31034;&#20102;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#20849;&#20139;&#22240;&#26524;&#39537;&#21160;&#32773;
&lt;/p&gt;
&lt;p&gt;
Recurrences reveal shared causal drivers of complex time series. (arXiv:2301.13516v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13516
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#33021;&#22815;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#20013;&#30340;&#22797;&#29616;&#36880;&#28176;&#37325;&#26500;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#39537;&#21160;&#20449;&#21495;&#65292;&#20174;&#32780;&#21487;&#38752;&#22320;&#25512;&#26029;&#20849;&#20139;&#22240;&#26524;&#39537;&#21160;&#32773;&#65292;&#20854;&#24050;&#22312;&#22810;&#20010;&#31034;&#20363;&#20013;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#23454;&#39564;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#20849;&#20139;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#22240;&#26524;&#39537;&#21160;&#22120;&#12290;&#20363;&#22914;&#65292;&#21463;&#36716;&#24405;&#22240;&#23376;&#38774;&#21521;&#30340;&#22522;&#22240;&#12289;&#21463;&#22823;&#23610;&#24230;&#22823;&#27668;&#29615;&#27969;&#24433;&#21709;&#30340;&#28023;&#27915;&#27969;&#21160;&#65292;&#20197;&#21450;&#34987;&#19979;&#38477;&#31070;&#32463;&#20803;&#25511;&#21046;&#30340;&#30005;&#26426;&#30005;&#36335;&#12290;&#21487;&#38752;&#22320;&#25512;&#26029;&#36825;&#31181;&#30475;&#19981;&#35265;&#30340;&#39537;&#21160;&#21147;&#26159;&#24517;&#35201;&#30340;&#65292;&#20197;&#20102;&#35299;&#19981;&#21516;&#29983;&#29289;&#21644;&#24037;&#31243;&#31995;&#32479;&#20013;&#33258;&#19978;&#32780;&#19979;&#25511;&#21046;&#26041;&#26696;&#30340;&#38388;&#27463;&#24615;&#26412;&#36136;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#26102;&#38388;&#24207;&#21015;&#27979;&#37327;&#20013;&#30340;&#22797;&#29616;&#36880;&#28176;&#37325;&#26500;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#39537;&#21160;&#20449;&#21495;&#12290;&#20511;&#21161;&#20110;&#26012;&#31215;&#21160;&#21147;&#31995;&#32479;&#30340;&#25968;&#23398;&#29702;&#35770;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21709;&#24212;&#26102;&#38388;&#24207;&#21015;&#38388;&#20849;&#20139;&#30340;&#22797;&#29616;&#20107;&#20214;&#65292;&#36825;&#20123;&#20107;&#20214;&#38544;&#21547;&#22320;&#23450;&#20041;&#20102;&#19968;&#20010;&#20855;&#26377;&#29627;&#29827;&#29366;&#32467;&#26500;&#30340;&#22797;&#29616;&#22270;&#12290;&#38543;&#30528;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#37327;&#25110;&#36136;&#37327;&#30340;&#25913;&#21892;&#65292;&#35813;&#22797;&#29616;&#22270;&#32463;&#21382;&#20102;&#19968;&#20010;&#28183;&#27969;&#36716;&#21464;&#65292;&#34920;&#29616;&#20026;&#38543;&#26426;&#34892;&#36208;&#22312;&#35825;&#23548;&#30340;&#26223;&#35266;&#19978;&#20986;&#29616;&#24494;&#24369;&#30340;&#36941;&#21382;&#24615;&#30772;&#35010; - &#26292;&#38706;&#20986;&#20849;&#20139;&#39537;&#21160;&#20449;&#21495;&#20316;&#20026;&#23569;&#25968;&#21442;&#25968;&#30340;&#20989;&#25968;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21512;&#25104;&#31034;&#20363;&#20013;&#23637;&#31034;&#20102;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#29616;&#23454;&#29983;&#24577;&#25968;&#25454;&#38598;&#65292;&#24182;&#30830;&#23450;&#20102;&#21380;&#23572;&#23612;&#35834;&#12289;&#23395;&#33410;&#24615;&#40060;&#31867;&#36801;&#31227;&#21644;&#26524;&#34631;&#31181;&#32676;&#21160;&#24577;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many experimental time series measurements share unobserved causal drivers. Examples include genes targeted by transcription factors, ocean flows influenced by large-scale atmospheric currents, and motor circuits steered by descending neurons. Reliably inferring this unseen driving force is necessary to understand the intermittent nature of top-down control schemes in diverse biological and engineered systems. Here, we introduce a new unsupervised learning algorithm that uses recurrences in time series measurements to gradually reconstruct an unobserved driving signal. Drawing on the mathematical theory of skew-product dynamical systems, we identify recurrence events shared across response time series, which implicitly define a recurrence graph with glass-like structure. As the amount or quality of observed data improves, this recurrence graph undergoes a percolation transition manifesting as weak ergodicity breaking for random walks on the induced landscape -- revealing the shared dri
&lt;/p&gt;</description></item></channel></rss>