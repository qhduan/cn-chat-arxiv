<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;VAE&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#25311;&#22120;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;55%&#12290;</title><link>https://arxiv.org/abs/2404.01932</link><description>&lt;p&gt;
&#36328;&#36234;&#35821;&#35328;&#12289;&#35270;&#35273;&#21644;&#34892;&#21160;&#65306;&#22810;&#27169;&#24577;VAE&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Bridging Language, Vision and Action: Multimodal VAEs in Robotic Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22810;&#27169;&#24577;VAE&#22914;&#20309;&#22312;&#26080;&#30417;&#30563;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#25311;&#22120;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;55%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#20013;&#26080;&#30417;&#30563;&#30340;&#35270;&#35273;-&#35821;&#35328;-&#21160;&#20316;&#26144;&#23556;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22810;&#27169;&#24577;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#22914;&#20309;&#34987;&#24212;&#29992;&#20110;&#26080;&#30417;&#30563;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#25913;&#36827;&#27169;&#22411;&#24615;&#33021;&#30340;&#27169;&#22411;&#19981;&#21464;&#24335;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#27169;&#25311;&#22120;&#20013;&#30340;&#27169;&#22411;&#24615;&#33021;&#25552;&#39640;&#39640;&#36798;55%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01932v1 Announce Type: cross  Abstract: In this work, we focus on unsupervised vision-language-action mapping in the area of robotic manipulation. Recently, multiple approaches employing pre-trained large language and vision models have been proposed for this task. However, they are computationally demanding and require careful fine-tuning of the produced outputs. A more lightweight alternative would be the implementation of multimodal Variational Autoencoders (VAEs) which can extract the latent features of the data and integrate them into a joint representation, as has been demonstrated mostly on image-image or image-text data for the state-of-the-art models. Here we explore whether and how can multimodal VAEs be employed in unsupervised robotic manipulation tasks in a simulated environment. Based on the obtained results, we propose a model-invariant training alternative that improves the models' performance in a simulator by up to 55%. Moreover, we systematically evaluate 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#26368;&#20339;&#21452;&#36194;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20248;&#24615;&#24046;&#36317;&#21463;&#21040;&#19979;&#30028;&#38480;&#21046;&#26102;&#36951;&#25022;&#20026;$O(\log(T))$&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#36793;&#32536;&#26465;&#20214;&#26469;&#25551;&#36848;&#27425;&#20248;&#24615;&#24046;&#36317;&#23545;&#38382;&#39064;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.03219</link><description>&lt;p&gt;
LC-Tsalis-INF: &#24191;&#20041;&#26368;&#20339;&#21452;&#36194;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
LC-Tsalis-INF: Generalized Best-of-Both-Worlds Linear Contextual Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24191;&#20041;&#26368;&#20339;&#21452;&#36194;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27425;&#20248;&#24615;&#24046;&#36317;&#21463;&#21040;&#19979;&#30028;&#38480;&#21046;&#26102;&#36951;&#25022;&#20026;$O(\log(T))$&#12290;&#21516;&#26102;&#24341;&#20837;&#20102;&#36793;&#32536;&#26465;&#20214;&#26469;&#25551;&#36848;&#27425;&#20248;&#24615;&#24046;&#36317;&#23545;&#38382;&#39064;&#38590;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20855;&#26377;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#32972;&#26223;&#30340;&#32447;&#24615;&#32972;&#26223;&#24378;&#21270;&#22411;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#38382;&#39064;&#20013;&#65292;&#29616;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#26368;&#20339;&#21452;&#36194;&#65288;BoBW&#65289;&#31639;&#27861;&#65292;&#20854;&#36951;&#25022;&#22312;&#38543;&#26426;&#21306;&#22495;&#20013;&#28385;&#36275;$O(\log^2(T))$&#65292;&#20854;&#20013;$T$&#20026;&#22238;&#21512;&#25968;&#65292;&#20854;&#27425;&#20248;&#24615;&#24046;&#36317;&#30001;&#27491;&#24120;&#25968;&#19979;&#30028;&#65292;&#21516;&#26102;&#22312;&#23545;&#25239;&#24615;&#21306;&#22495;&#20013;&#28385;&#36275;$O(\sqrt{T})$&#12290;&#28982;&#32780;&#65292;&#23545;$T$&#30340;&#20381;&#36182;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#65292;&#24182;&#19988;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#20551;&#35774;&#21487;&#20197;&#25918;&#23485;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#24403;&#27425;&#20248;&#24615;&#24046;&#36317;&#21463;&#21040;&#19979;&#30028;&#38480;&#21046;&#26102;&#65292;&#20854;&#36951;&#25022;&#28385;&#36275;$O(\log(T))$&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36793;&#32536;&#26465;&#20214;&#65292;&#21363;&#23545;&#27425;&#20248;&#24615;&#24046;&#36317;&#30340;&#19968;&#20010;&#26356;&#28201;&#21644;&#30340;&#20551;&#35774;&#12290;&#35813;&#26465;&#20214;&#20351;&#29992;&#21442;&#25968;$\beta \in (0, \infty]$&#34920;&#24449;&#19982;&#27425;&#20248;&#24615;&#24046;&#36317;&#30456;&#20851;&#30340;&#38382;&#39064;&#38590;&#24230;&#12290;&#28982;&#21518;&#25105;&#20204;&#35777;&#26126;&#35813;&#31639;&#27861;&#30340;&#36951;&#25022;&#28385;&#36275;$O\left(\
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03219v1 Announce Type: new  Abstract: This study considers the linear contextual bandit problem with independent and identically distributed (i.i.d.) contexts. In this problem, existing studies have proposed Best-of-Both-Worlds (BoBW) algorithms whose regrets satisfy $O(\log^2(T))$ for the number of rounds $T$ in a stochastic regime with a suboptimality gap lower-bounded by a positive constant, while satisfying $O(\sqrt{T})$ in an adversarial regime. However, the dependency on $T$ has room for improvement, and the suboptimality-gap assumption can be relaxed. For this issue, this study proposes an algorithm whose regret satisfies $O(\log(T))$ in the setting when the suboptimality gap is lower-bounded. Furthermore, we introduce a margin condition, a milder assumption on the suboptimality gap. That condition characterizes the problem difficulty linked to the suboptimality gap using a parameter $\beta \in (0, \infty]$. We then show that the algorithm's regret satisfies $O\left(\
&lt;/p&gt;</description></item><item><title>FedStruct&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#23618;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#22312;&#20114;&#32852;&#22270;&#19978;&#36827;&#34892;&#32852;&#21512;&#35299;&#32806;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25252;&#38544;&#31169;&#24182;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2402.19163</link><description>&lt;p&gt;
FedStruct&#65306;&#32852;&#21512;&#35299;&#32806;&#23398;&#20064;&#22312;&#20114;&#32852;&#22270;&#19978;
&lt;/p&gt;
&lt;p&gt;
FedStruct: Federated Decoupled Learning over Interconnected Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19163
&lt;/p&gt;
&lt;p&gt;
FedStruct&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#28145;&#23618;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#22312;&#20114;&#32852;&#22270;&#19978;&#36827;&#34892;&#32852;&#21512;&#35299;&#32806;&#23398;&#20064;&#65292;&#26377;&#25928;&#22320;&#32500;&#25252;&#38544;&#31169;&#24182;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#35299;&#20915;&#20102;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30340;&#32852;&#21512;&#23398;&#20064;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20851;&#27880;&#20114;&#32852;&#23376;&#22270;&#30340;&#26222;&#36941;&#24773;&#20917;&#65292;&#20854;&#20013;&#19981;&#21516;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#30456;&#20114;&#36830;&#25509;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#30340;&#19968;&#31181;&#26032;&#39062;&#26694;&#26550;&#65292;&#21517;&#20026;FedStruct&#65292;&#23427;&#21033;&#29992;&#28145;&#23618;&#32467;&#26500;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#32500;&#25252;&#38544;&#31169;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;FedStruct&#28040;&#38500;&#20102;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#20849;&#20139;&#25110;&#29983;&#25104;&#25935;&#24863;&#33410;&#28857;&#29305;&#24449;&#25110;&#23884;&#20837;&#30340;&#24517;&#35201;&#24615;&#12290;&#30456;&#21453;&#65292;&#23427;&#21033;&#29992;&#26174;&#24335;&#20840;&#23616;&#22270;&#32467;&#26500;&#20449;&#24687;&#26469;&#25429;&#25417;&#33410;&#28857;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#20845;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;FedStruct&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#22312;&#21508;&#31181;&#24773;&#20917;&#19979;&#65288;&#21253;&#25324;&#19981;&#21516;&#25968;&#25454;&#20998;&#21306;&#26041;&#27861;&#12289;&#19981;&#21516;&#26631;&#31614;&#21487;&#29992;&#24615;&#20197;&#21450;&#23458;&#25143;&#20010;&#25968;&#30340;&#65289;&#25509;&#36817;&#20110;&#38598;&#20013;&#24335;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19163v1 Announce Type: new  Abstract: We address the challenge of federated learning on graph-structured data distributed across multiple clients. Specifically, we focus on the prevalent scenario of interconnected subgraphs, where inter-connections between different clients play a critical role. We present a novel framework for this scenario, named FedStruct, that harnesses deep structural dependencies. To uphold privacy, unlike existing methods, FedStruct eliminates the necessity of sharing or generating sensitive node features or embeddings among clients. Instead, it leverages explicit global graph structure information to capture inter-node dependencies. We validate the effectiveness of FedStruct through experimental results conducted on six datasets for semi-supervised node classification, showcasing performance close to the centralized approach across various scenarios, including different data partitioning methods, varying levels of label availability, and number of cl
&lt;/p&gt;</description></item><item><title>InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2402.02933</link><description>&lt;p&gt;
InterpretCC: &#36866;&#20110;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26465;&#20214;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
InterpretCC: Conditional Computation for Inherently Interpretable Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02933
&lt;/p&gt;
&lt;p&gt;
InterpretCC&#26159;&#19968;&#31181;&#26032;&#30340;&#35299;&#37322;&#24615;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#26465;&#20214;&#35745;&#31639;&#21644;&#31232;&#30095;&#28608;&#27963;&#29305;&#24449;&#65292;&#22312;&#20445;&#25345;&#24615;&#33021;&#30340;&#21516;&#26102;&#23454;&#29616;&#20102;&#20154;&#31867;&#20013;&#24515;&#30340;&#35299;&#37322;&#33021;&#21147;&#12290;&#35813;&#27169;&#22411;&#36866;&#29992;&#20110;&#38656;&#35201;&#21487;&#20449;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#35299;&#37322;&#21644;&#20934;&#30830;&#39044;&#27979;&#30340;&#20154;&#31867;&#38754;&#21521;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#30495;&#23454;&#19990;&#30028;&#35299;&#37322;&#24615;&#22312;&#19977;&#20010;&#26041;&#38754;&#20043;&#38388;&#23384;&#22312;&#26435;&#34913;&#65306;1&#65289;&#38656;&#35201;&#20154;&#31867;&#20449;&#20219;&#35299;&#37322;&#30340;&#36817;&#20284;&#65288;&#20363;&#22914;&#20107;&#21518;&#26041;&#27861;&#65289;&#65307;2&#65289;&#21066;&#24369;&#20102;&#35299;&#37322;&#30340;&#21487;&#29702;&#35299;&#24615;&#65288;&#20363;&#22914;&#33258;&#21160;&#35782;&#21035;&#30340;&#29305;&#24449;&#25513;&#30721;&#65289;&#65307;3&#65289;&#21066;&#24369;&#20102;&#27169;&#22411;&#24615;&#33021;&#65288;&#20363;&#22914;&#20915;&#31574;&#26641;&#65289;&#12290;&#36825;&#20123;&#32570;&#28857;&#23545;&#20110;&#38754;&#21521;&#20154;&#31867;&#30340;&#39046;&#22495;&#65288;&#22914;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#25110;&#33258;&#28982;&#35821;&#35328;&#65289;&#26159;&#19981;&#21487;&#25509;&#21463;&#30340;&#65292;&#36825;&#20123;&#39046;&#22495;&#38656;&#35201;&#21487;&#20449;&#30340;&#35299;&#37322;&#12289;&#21487;&#25805;&#20316;&#30340;&#35299;&#37322;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InterpretCC&#65288;&#21487;&#35299;&#37322;&#30340;&#26465;&#20214;&#35745;&#31639;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#24615;&#30340;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#31995;&#21015;&#65292;&#36890;&#36807;&#22312;&#39044;&#27979;&#20043;&#21069;&#33258;&#36866;&#24212;&#21644;&#31232;&#30095;&#22320;&#28608;&#27963;&#29305;&#24449;&#65292;&#30830;&#20445;&#20154;&#31867;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#26368;&#20808;&#36827;&#27169;&#22411;&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#24605;&#24819;&#25193;&#23637;&#20026;&#21487;&#35299;&#37322;&#30340;&#19987;&#23478;&#28151;&#21512;&#27169;&#22411;&#65292;&#20801;&#35768;&#20154;&#20204;&#31163;&#25955;&#22320;&#25351;&#23450;&#20852;&#36259;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world interpretability for neural networks is a tradeoff between three concerns: 1) it requires humans to trust the explanation approximation (e.g. post-hoc approaches), 2) it compromises the understandability of the explanation (e.g. automatically identified feature masks), and 3) it compromises the model performance (e.g. decision trees). These shortcomings are unacceptable for human-facing domains, like education, healthcare, or natural language, which require trustworthy explanations, actionable interpretations, and accurate predictions. In this work, we present InterpretCC (interpretable conditional computation), a family of interpretable-by-design neural networks that guarantee human-centric interpretability while maintaining comparable performance to state-of-the-art models by adaptively and sparsely activating features before prediction. We extend this idea into an interpretable mixture-of-experts model, that allows humans to specify topics of interest, discretely separate
&lt;/p&gt;</description></item><item><title>&#20803;&#20849;&#35757;&#32451;&#36890;&#36807;&#22312;&#25968;&#25454;&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18083</link><description>&lt;p&gt;
&#20803;&#20849;&#35757;&#32451;&#65306;&#20004;&#31181;&#35270;&#35282;&#20248;&#20110;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
Meta Co-Training: Two Views are Better than One
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18083
&lt;/p&gt;
&lt;p&gt;
&#20803;&#20849;&#35757;&#32451;&#36890;&#36807;&#22312;&#25968;&#25454;&#19978;&#26500;&#24314;&#19981;&#21516;&#30340;&#35270;&#35282;&#65292;&#24182;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#20849;&#21516;&#35757;&#32451;&#65292;&#25552;&#39640;&#20102;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#22330;&#26223;&#20013;&#65292;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#24456;&#22810;&#65292;&#20294;&#26631;&#31614;&#21364;&#31232;&#32570;&#19988;&#38590;&#20197;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#21322;&#30417;&#30563;&#23398;&#20064;&#21033;&#29992;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#25552;&#21319;&#30417;&#30563;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#24050;&#32463;&#22312;&#26368;&#36817;&#30340;&#25991;&#29486;&#20013;&#24341;&#36215;&#20102;&#37325;&#35201;&#30340;&#20851;&#27880;&#12290;&#20854;&#20013;&#19968;&#31181;&#20027;&#35201;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#26159;&#20849;&#35757;&#32451;&#12290;&#22312;&#20849;&#35757;&#32451;&#20013;&#65292;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#22411;&#21033;&#29992;&#25968;&#25454;&#30340;&#19981;&#21516;&#29420;&#31435;&#21644;&#36275;&#22815;&#30340;&#8220;&#35270;&#35282;&#8221;&#26469;&#20849;&#21516;&#36827;&#34892;&#26356;&#22909;&#30340;&#39044;&#27979;&#12290;&#22312;&#20849;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#27169;&#22411;&#22312;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#28857;&#19978;&#21019;&#24314;&#20266;&#26631;&#31614;&#65292;&#29992;&#20110;&#25913;&#36827;&#21478;&#19968;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#24120;&#35265;&#24773;&#20917;&#19979;&#65292;&#24403;&#29420;&#31435;&#35270;&#35282;&#19981;&#21487;&#29992;&#26102;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#24265;&#20215;&#22320;&#26500;&#24314;&#36825;&#20123;&#35270;&#35282;&#12290;&#22312;&#26500;&#24314;&#30340;&#35270;&#35282;&#19978;&#36827;&#34892;&#20849;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#20248;&#20110;&#25105;&#20204;&#26500;&#24314;&#30340;&#20219;&#20309;&#21333;&#20010;&#35270;&#35282;&#65292;&#24182;&#19988;&#19982;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#26368;&#26032;&#26041;&#27861;&#24615;&#33021;&#30456;&#24403;&#65292;&#20294;&#20855;&#26377;&#19968;&#20123;&#19981;&#21487;&#21462;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many practical computer vision scenarios unlabeled data is plentiful, but labels are scarce and difficult to obtain. As a result, semi-supervised learning which leverages unlabeled data to boost the performance of supervised classifiers have received significant attention in recent literature. One major class of semi-supervised algorithms is co-training. In co-training two different models leverage different independent and sufficient "views" of the data to jointly make better predictions. During co-training each model creates pseudo labels on unlabeled points which are used to improve the other model. We show that in the common case when independent views are not available we can construct such views inexpensively using pre-trained models. Co-training on the constructed views yields a performance improvement over any of the individual views we construct and performance comparable with recent approaches in semi-supervised learning, but has some undesirable properties. To alleviate t
&lt;/p&gt;</description></item><item><title>RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2311.15876</link><description>&lt;p&gt;
LMM&#36741;&#21161;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#19979;&#20083;&#33146;&#30284;&#27835;&#30103;&#30446;&#26631;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LMM-Assisted Breast Cancer Treatment Target Segmentation with Consistency Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15876
&lt;/p&gt;
&lt;p&gt;
RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#28145;&#21051;&#24433;&#21709;&#20102;&#21307;&#23398;&#39046;&#22495;&#65292;&#20026;&#38477;&#20302;&#20020;&#24202;&#24037;&#20316;&#37327;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21463;&#38480;&#20110;&#25191;&#34892;&#21333;&#27169;&#24335;&#20219;&#21153;&#65292;&#19982;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#25152;&#20351;&#29992;&#30340;&#32508;&#21512;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;RO-LMM&#65292;&#19968;&#20010;&#19987;&#20026;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#12290;&#35813;&#27169;&#22411;&#28085;&#30422;&#20102;&#20020;&#24202;&#24037;&#20316;&#27969;&#20013;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#25797;&#38271;&#20020;&#24202;&#25253;&#21578;&#25688;&#35201;&#12289;&#25918;&#30103;&#27835;&#30103;&#35745;&#21010;&#24314;&#35758;&#21644;&#35745;&#21010;&#24341;&#23548;&#30340;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;&#20026;&#20102;&#25191;&#34892;&#36830;&#32493;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#24494;&#35843;&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;LMM&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27010;&#24565;&#36716;&#21270;&#20026;LMM&#39537;&#21160;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#21363;&#19968;&#33268;&#24615;&#23884;&#20837;S&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15876v2 Announce Type: replace-cross  Abstract: Recent advancements in Artificial Intelligence (AI) have profoundly influenced medical fields, by providing tools to reduce clinical workloads. However, most AI models are constrained to execute unimodal tasks, in stark contrast to the comprehensive approaches utilized by medical professionals. To address this, here we present RO-LMM, a multi-purpose large multimodal model (LMM) tailored for the field of radiation oncology. This model covers series of tasks within clinical workflow, adept at clinical report summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation. In particular, to perform consecutive clinical tasks, we further present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the capability of handling clean inputs, and transform this concept into LMM-driven segmentation framework as Consistency Embedding S
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.10690</link><description>&lt;p&gt;
&#36229;&#36234;RMSE&#21644;MAE&#65306;&#24341;&#20837;EAUC&#26469;&#25581;&#31034;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#30340;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Beyond RMSE and MAE: Introducing EAUC to unmask hidden bias and unfairness in dyadic regression models. (arXiv:2401.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;EAUC&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#29992;&#20197;&#25581;&#31034;&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#21644;&#19981;&#20844;&#24179;&#38382;&#39064;&#12290;&#20256;&#32479;&#30340;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;RMSE&#21644;MAE&#26080;&#27861;&#25429;&#25417;&#21040;&#36825;&#31181;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36842;&#20122;&#24503;&#22238;&#24402;&#27169;&#22411;&#29992;&#20110;&#39044;&#27979;&#19968;&#23545;&#23454;&#20307;&#30340;&#23454;&#20540;&#32467;&#26524;&#65292;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#37117;&#26159;&#22522;&#30784;&#30340;&#65288;&#20363;&#22914;&#65292;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#39044;&#27979;&#29992;&#25143;&#23545;&#20135;&#21697;&#30340;&#35780;&#20998;&#65289;&#65292;&#22312;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#20013;&#20063;&#26377;&#35768;&#22810;&#28508;&#21147;&#20294;&#23578;&#26410;&#28145;&#20837;&#25506;&#32034;&#65288;&#20363;&#22914;&#65292;&#22312;&#20010;&#24615;&#21270;&#33647;&#29702;&#23398;&#20013;&#36817;&#20284;&#30830;&#23450;&#24739;&#32773;&#30340;&#36866;&#24403;&#21058;&#37327;&#65289;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20010;&#20307;&#23454;&#20307;&#35266;&#23519;&#20540;&#20998;&#24067;&#30340;&#38750;&#22343;&#21248;&#24615;&#23548;&#33268;&#20102;&#26368;&#20808;&#36827;&#27169;&#22411;&#20013;&#30340;&#20005;&#37325;&#20559;&#35265;&#39044;&#27979;&#65292;&#20559;&#21521;&#20110;&#23454;&#20307;&#30340;&#35266;&#23519;&#36807;&#21435;&#20540;&#30340;&#24179;&#22343;&#20540;&#65292;&#24182;&#22312;&#21478;&#31867;&#20294;&#21516;&#26679;&#37325;&#35201;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#27604;&#38543;&#26426;&#39044;&#27979;&#26356;&#24046;&#30340;&#39044;&#27979;&#33021;&#21147;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20840;&#23616;&#38169;&#35823;&#24230;&#37327;&#26631;&#20934;&#22914;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#21644;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65288;MAE&#65289;&#19981;&#36275;&#20197;&#25429;&#25417;&#21040;&#36825;&#31181;&#29616;&#35937;&#65292;&#25105;&#20204;&#23558;&#20854;&#21629;&#21517;&#20026;&#21478;&#31867;&#20559;&#35265;&#65292;&#24182;&#24341;&#20837;&#21478;&#31867;-&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;EAUC&#65289;&#20316;&#20026;&#19968;&#20010;&#26032;&#30340;&#34917;&#20805;&#24230;&#37327;&#65292;&#21487;&#20197;&#22312;&#25152;&#26377;&#30740;&#31350;&#30340;&#27169;&#22411;&#20013;&#37327;&#21270;&#23427;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dyadic regression models, which predict real-valued outcomes for pairs of entities, are fundamental in many domains (e.g. predicting the rating of a user to a product in Recommender Systems) and promising and under exploration in many others (e.g. approximating the adequate dosage of a drug for a patient in personalized pharmacology). In this work, we demonstrate that non-uniformity in the observed value distributions of individual entities leads to severely biased predictions in state-of-the-art models, skewing predictions towards the average of observed past values for the entity and providing worse-than-random predictive power in eccentric yet equally important cases. We show that the usage of global error metrics like Root Mean Squared Error (RMSE) and Mean Absolute Error (MAE) is insufficient to capture this phenomenon, which we name eccentricity bias, and we introduce Eccentricity-Area Under the Curve (EAUC) as a new complementary metric that can quantify it in all studied models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2310.08731</link><description>&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23454;&#29616;&#26032;&#39062;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Simple Way to Incorporate Novelty Detection in World Models. (arXiv:2310.08731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#65292;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#12290;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20855;&#26377;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#19990;&#30028;&#26426;&#21046;&#25110;&#23646;&#24615;&#21457;&#29983;&#31361;&#28982;&#21464;&#21270;&#26102;&#65292;&#20195;&#29702;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#21487;&#33021;&#20250;&#26174;&#33879;&#19979;&#38477;&#12290;&#25105;&#20204;&#23558;&#35270;&#35273;&#23646;&#24615;&#25110;&#29366;&#24577;&#36716;&#25442;&#30340;&#31361;&#21464;&#31216;&#20026;&#8220;&#26032;&#39062;&#24615;&#8221;&#12290;&#22312;&#29983;&#25104;&#30340;&#19990;&#30028;&#27169;&#22411;&#26694;&#26550;&#20013;&#23454;&#26045;&#26032;&#39062;&#24615;&#26816;&#27979;&#26159;&#20445;&#25252;&#37096;&#32626;&#26102;&#20195;&#29702;&#30340;&#20851;&#38190;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#36793;&#30028;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#26032;&#39062;&#24615;&#26816;&#27979;&#32435;&#20837;&#19990;&#30028;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#24187;&#35273;&#29366;&#24577;&#21644;&#30495;&#23454;&#35266;&#23519;&#29366;&#24577;&#30340;&#19981;&#21305;&#37197;&#24615;&#20316;&#20026;&#24322;&#24120;&#20998;&#25968;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19982;&#24207;&#21015;&#20915;&#31574;&#30456;&#20851;&#30340;&#26032;&#39062;&#24615;&#26816;&#27979;&#26412;&#20307;&#35770;&#65292;&#28982;&#21518;&#25105;&#20204;&#25552;&#20379;&#20102;&#22312;&#20195;&#29702;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#23398;&#20064;&#30340;&#36716;&#25442;&#20998;&#24067;&#20013;&#26816;&#27979;&#26032;&#39062;&#24615;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20316;&#22312;&#26032;&#29615;&#22659;&#20013;&#19982;&#20256;&#32479;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) using world models has found significant recent successes. However, when a sudden change to world mechanics or properties occurs then agent performance and reliability can dramatically decline. We refer to the sudden change in visual properties or state transitions as {\em novelties}. Implementing novelty detection within generated world model frameworks is a crucial task for protecting the agent when deployed. In this paper, we propose straightforward bounding approaches to incorporate novelty detection into world model RL agents, by utilizing the misalignment of the world model's hallucinated states and the true observed states as an anomaly score. We first provide an ontology of novelty detection relevant to sequential decision making, then we provide effective approaches to detecting novelties in a distribution of transitions learned by an agent in a world model. Finally, we show the advantage of our work in a novel environment compared to traditional ma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;"Cluster-DP"&#65292;&#23427;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#36739;&#20302;&#30340;&#26041;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.00957</link><description>&lt;p&gt;
&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;(&#20998;&#32452;)&#32467;&#26524;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Causal Inference with Differentially Private (Clustered) Outcomes. (arXiv:2308.00957v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;"Cluster-DP"&#65292;&#23427;&#22312;&#20445;&#35777;&#38544;&#31169;&#30340;&#21516;&#26102;&#21033;&#29992;&#25968;&#25454;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#24378;&#30340;&#38544;&#31169;&#20445;&#35777;&#21644;&#36739;&#20302;&#30340;&#26041;&#24046;&#65292;&#21487;&#20197;&#29992;&#20110;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#23454;&#39564;&#20013;&#20272;&#35745;&#22240;&#26524;&#25928;&#24212;&#21482;&#26377;&#22312;&#21442;&#19982;&#32773;&#21516;&#24847;&#36879;&#38706;&#20182;&#20204;&#21487;&#33021;&#25935;&#24863;&#30340;&#21709;&#24212;&#26102;&#25165;&#21487;&#34892;&#12290;&#22312;&#30830;&#20445;&#38544;&#31169;&#30340;&#35768;&#22810;&#26041;&#27861;&#20013;&#65292;&#26631;&#31614;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#31639;&#27861;&#38544;&#31169;&#20445;&#35777;&#24230;&#37327;&#65292;&#21487;&#20197;&#40723;&#21169;&#21442;&#19982;&#32773;&#20998;&#20139;&#21709;&#24212;&#32780;&#19981;&#20250;&#38754;&#20020;&#21435;&#21311;&#21517;&#21270;&#30340;&#39118;&#38505;&#12290;&#35768;&#22810;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;&#20250;&#21521;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#22122;&#38899;&#26469;&#23454;&#29616;&#36825;&#31181;&#38544;&#31169;&#20445;&#35777;&#65292;&#36825;&#20250;&#22686;&#21152;&#22823;&#22810;&#25968;&#32479;&#35745;&#20272;&#35745;&#37327;&#30340;&#26041;&#24046;&#65292;&#20351;&#24471;&#31934;&#30830;&#27979;&#37327;&#22240;&#26524;&#25928;&#24212;&#21464;&#24471;&#22256;&#38590;&#65306;&#20174;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;&#23384;&#22312;&#30528;&#22266;&#26377;&#30340;&#38544;&#31169;-&#26041;&#24046;&#26435;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#24378;&#38544;&#31169;&#20445;&#35777;&#30340;&#36739;&#20302;&#26041;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24046;&#20998;&#38544;&#31169;&#26426;&#21046;"Cluster-DP"&#65292;&#23427;&#21033;&#29992;&#25968;&#25454;&#30340;&#20219;&#20309;&#32473;&#23450;&#30340;&#32858;&#31867;&#32467;&#26500;&#65292;&#21516;&#26102;&#20173;&#28982;&#20801;&#35768;&#23545;&#22240;&#26524;&#25928;&#24212;&#36827;&#34892;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating causal effects from randomized experiments is only feasible if participants agree to reveal their potentially sensitive responses. Of the many ways of ensuring privacy, label differential privacy is a widely used measure of an algorithm's privacy guarantee, which might encourage participants to share responses without running the risk of de-anonymization. Many differentially private mechanisms inject noise into the original data-set to achieve this privacy guarantee, which increases the variance of most statistical estimators and makes the precise measurement of causal effects difficult: there exists a fundamental privacy-variance trade-off to performing causal analyses from differentially private data. With the aim of achieving lower variance for stronger privacy guarantees, we suggest a new differential privacy mechanism, "Cluster-DP", which leverages any given cluster structure of the data while still allowing for the estimation of causal effects. We show that, depending 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#27491;&#28608;&#21169;&#22122;&#22768;&#26694;&#26550;&#19979;&#30340;&#38543;&#26426;&#22122;&#22768;&#20351;&#32463;&#20856;&#27169;&#22411;&#21463;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#21464;&#20998;Pi-Noise&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#27169;&#22411;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#21644;&#31616;&#21270;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.07651</link><description>&lt;p&gt;
&#21464;&#20998;&#28608;&#21169;&#22122;&#22768;&#65306;&#22122;&#22768;&#22914;&#20309;&#25913;&#36827;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Variational Positive-incentive Noise: How Noise Benefits Models. (arXiv:2306.07651v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#27491;&#28608;&#21169;&#22122;&#22768;&#26694;&#26550;&#19979;&#30340;&#38543;&#26426;&#22122;&#22768;&#20351;&#32463;&#20856;&#27169;&#22411;&#21463;&#30410;&#65292;&#24182;&#25552;&#20986;&#20102;&#21464;&#20998;Pi-Noise&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#25913;&#21464;&#21407;&#22987;&#27169;&#22411;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;&#21644;&#31616;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#30740;&#31350;&#26088;&#22312;&#20943;&#36731;&#30001;&#20110;&#36127;&#38754;&#22122;&#22768;&#30340;&#22522;&#26412;&#20551;&#35774;&#32780;&#23548;&#33268;&#30340;&#22122;&#22768;&#24433;&#21709;&#12290;&#20294;&#26159;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#20551;&#35774;&#24182;&#19981;&#24635;&#26159;&#25104;&#31435;&#30340;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#27491;&#28608;&#21169;&#22122;&#22768;&#65288;Pi-Noise&#65289;&#26694;&#26550;&#19979;&#36890;&#36807;&#38543;&#26426;&#22122;&#22768;&#20351;&#32463;&#20856;&#27169;&#22411;&#21463;&#30410;&#12290;&#30001;&#20110;Pi-Noise&#30340;&#29702;&#24819;&#30446;&#26631;&#26159;&#38590;&#20197;&#23454;&#29616;&#30340;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#20854;&#21464;&#20998;&#19979;&#30028;&#36827;&#34892;&#20248;&#21270;&#30340;&#21464;&#20998;Pi-Noise&#65288;VPN&#65289;&#65292;&#36890;&#36807;&#21464;&#20998;&#25512;&#26029;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;VPN&#29983;&#25104;&#22120;&#26469;&#22686;&#24378;&#22522;&#30784;&#27169;&#22411;&#24182;&#31616;&#21270;&#22522;&#30784;&#27169;&#22411;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#25913;&#21464;&#22522;&#30784;&#27169;&#22411;&#30340;&#26550;&#26500;&#12290;&#30001;&#20110;&#22522;&#30784;&#27169;&#22411;&#21644;VPN&#29983;&#25104;&#22120;&#30340;&#29420;&#31435;&#35774;&#35745;&#65292; VPN&#29983;&#25104;&#22120;&#21487;&#20197;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#12290;&#20174;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;&#25152;&#25552;&#20986;&#30340;VPN&#29983;&#25104;&#22120;&#21487;&#20197;&#25913;&#36827;&#22522;&#26412;&#27169;&#22411;&#12290;&#20540;&#24471;&#31216;&#36190;&#30340;&#26159;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#21464;&#20998;VPN&#29983;&#25104;&#22120;&#26356;&#21916;&#27426;&#29420;&#31435;&#23494;&#38598;&#22411;&#22122;&#22768;&#12290;&#65288;&#32763;&#35793;&#26377;&#21024;&#20943;&#65289;
&lt;/p&gt;
&lt;p&gt;
A large number of works aim to alleviate the impact of noise due to an underlying conventional assumption of the negative role of noise. However, some existing works show that the assumption does not always hold. In this paper, we investigate how to benefit the classical models by random noise under the framework of Positive-incentive Noise (Pi-Noise). Since the ideal objective of Pi-Noise is intractable, we propose to optimize its variational bound instead, namely variational Pi-Noise (VPN). With the variational inference, a VPN generator implemented by neural networks is designed for enhancing base models and simplifying the inference of base models, without changing the architecture of base models. Benefiting from the independent design of base models and VPN generators, the VPN generator can work with most existing models. From the experiments, it is shown that the proposed VPN generator can improve the base models. It is appealing that the trained variational VPN generator prefers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25913;&#36830;&#26041;&#27861;&#26159;&#21542;&#26377;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2305.19717</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25913;&#36830;&#26159;&#21542;&#30495;&#27491;&#26377;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Rewiring Actually Helpful in Graph Neural Networks?. (arXiv:2305.19717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25913;&#36830;&#26041;&#27861;&#26159;&#21542;&#26377;&#29992;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25191;&#34892;&#22810;&#20010;&#28040;&#24687;&#20256;&#36882;&#27493;&#39588;&#26469;&#35745;&#31639;&#33410;&#28857;&#34920;&#31034;&#65292;&#36825;&#20123;&#27493;&#39588;&#21253;&#25324;&#33410;&#28857;&#29305;&#24449;&#30340;&#26412;&#22320;&#32858;&#21512;&#12290;&#20294;&#26159;&#65292;&#28145;&#23618;&#27169;&#22411;&#33021;&#22815;&#21033;&#29992;&#33410;&#28857;&#20043;&#38388;&#26356;&#38271;&#36317;&#31163;&#30340;&#20132;&#20114;&#30340;&#38382;&#39064;&#21463;&#21040;&#20102;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#30340;&#24433;&#21709;&#12290;&#32780;&#21518;&#32773;&#24402;&#22240;&#20110;&#25351;&#23548;&#28040;&#24687;&#20256;&#36882;&#30340;&#22270;&#25299;&#25169;&#65292;&#23548;&#33268;&#33410;&#28857;&#34920;&#31034;&#23545;&#21253;&#21547;&#22312;&#36828;&#31243;&#33410;&#28857;&#19978;&#30340;&#20449;&#24687;&#19981;&#25935;&#24863;&#12290;&#35768;&#22810;&#25913;&#36830;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#35299;&#20915;&#25110;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36807;&#24230;&#21387;&#32553;&#19982;&#20854;&#20182;&#19982;&#27169;&#22411;&#35757;&#32451;&#23494;&#20999;&#30456;&#20851;&#30340;&#38382;&#39064;&#65288;&#22914;&#28040;&#22833;&#30340;&#26799;&#24230;&#65289;&#30456;&#32806;&#21512;&#65292;&#25152;&#20197;&#27491;&#30830;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#22909;&#22788;&#26159;&#22256;&#38590;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28040;&#24687;&#20256;&#36882;&#27169;&#22411;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#36825;&#20123;&#27169;&#22411;&#19981;&#38656;&#35201;&#35757;&#32451;&#21363;&#21487;&#35745;&#31639;&#33410;&#28857;&#21644;&#22270;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#33410;&#28857;&#21644;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks compute node representations by performing multiple message-passing steps that consist in local aggregations of node features. Having deep models that can leverage longer-range interactions between nodes is hindered by the issues of over-smoothing and over-squashing. In particular, the latter is attributed to the graph topology which guides the message-passing, causing a node representation to become insensitive to information contained at distant nodes. Many graph rewiring methods have been proposed to remedy or mitigate this problem. However, properly evaluating the benefits of these methods is made difficult by the coupling of over-squashing with other issues strictly related to model training, such as vanishing gradients. Therefore, we propose an evaluation setting based on message-passing models that do not require training to compute node and graph representations. We perform a systematic experimental comparison on real-world node and graph classification ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25910;&#38598;&#39134;&#34892;&#36712;&#36857;&#21644;&#31243;&#24207;&#25968;&#25454;&#23398;&#20064;&#39134;&#34892;&#22120;&#34892;&#20026;&#21464;&#24322;&#24615;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#28041;&#21450;&#20219;&#24847;&#25968;&#37327;&#39134;&#34892;&#22120;&#30340;&#20132;&#36890;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.09981</link><description>&lt;p&gt;
&#20174;&#39134;&#34892;&#36712;&#36857;&#21644;&#31243;&#24207;&#20013;&#25512;&#26029;&#32456;&#31471;&#31354;&#22495;&#20132;&#36890;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Inferring Traffic Models in Terminal Airspace from Flight Tracks and Procedures. (arXiv:2303.09981v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25910;&#38598;&#39134;&#34892;&#36712;&#36857;&#21644;&#31243;&#24207;&#25968;&#25454;&#23398;&#20064;&#39134;&#34892;&#22120;&#34892;&#20026;&#21464;&#24322;&#24615;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#24182;&#19988;&#21487;&#20197;&#29983;&#25104;&#28041;&#21450;&#20219;&#24847;&#25968;&#37327;&#39134;&#34892;&#22120;&#30340;&#20132;&#36890;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#30340;&#33322;&#31354;&#22120;&#36712;&#36857;&#27169;&#22411;&#23545;&#20110;&#31354;&#20013;&#20132;&#36890;&#31649;&#29702;&#65288;ATM&#65289;&#31995;&#32479;&#35774;&#35745;&#21644;&#39564;&#35777;&#24456;&#26377;&#29992;&#12290;&#20202;&#34920;&#39134;&#34892;&#35268;&#21017;&#65288;IFR&#65289;&#19979;&#25805;&#20316;&#30340;&#39134;&#34892;&#22120;&#27169;&#22411;&#38656;&#35201;&#25429;&#25417;&#39134;&#34892;&#22120;&#25353;&#29031;&#26631;&#20934;&#39134;&#34892;&#31243;&#24207;&#30340;&#22266;&#26377;&#21464;&#24322;&#24615;&#12290;&#39134;&#34892;&#22120;&#34892;&#20026;&#30340;&#21464;&#24322;&#24615;&#22312;&#19981;&#21516;&#30340;&#39134;&#34892;&#38454;&#27573;&#20043;&#38388;&#21508;&#19981;&#30456;&#21516;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27010;&#29575;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#31243;&#24207;&#25968;&#25454;&#21644;&#20174;&#38647;&#36798;&#30417;&#35270;&#25968;&#25454;&#25910;&#38598;&#30340;&#39134;&#34892;&#36712;&#36857;&#20013;&#23398;&#20064;&#21464;&#24322;&#24615;&#12290; &#23545;&#20110;&#27599;&#20010;&#27573;&#33853;&#65292;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23398;&#20064;&#39134;&#34892;&#22120;&#36712;&#36857;&#19982;&#20854;&#31243;&#24207;&#20043;&#38388;&#30340;&#20559;&#24046;&#12290;&#32473;&#23450;&#26032;&#30340;&#31243;&#24207;&#65292;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#20174;&#32463;&#36807;&#35757;&#32451;&#30340;&#39640;&#26031;&#20998;&#24067;&#20013;&#25277;&#26679;&#19968;&#31995;&#21015;&#20559;&#24046;&#65292;&#24182;&#20351;&#29992;&#20559;&#24046;&#21644;&#31243;&#24207;&#37325;&#26500;&#39134;&#34892;&#22120;&#36712;&#36857;&#26469;&#29983;&#25104;&#21512;&#25104;&#36712;&#36857;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#25193;&#23637;&#21040;&#25429;&#25417;&#39134;&#34892;&#22120;&#20043;&#38388;&#30340;&#25104;&#23545;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#22914;&#20309;&#20351;&#29992;&#25104;&#23545;&#27169;&#22411;&#26469;&#29983;&#25104;&#28041;&#21450;&#20219;&#24847;&#25968;&#37327;&#39134;&#34892;&#22120;&#30340;&#20132;&#36890;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Realistic aircraft trajectory models are useful in the design and validation of air traffic management (ATM) systems. Models of aircraft operated under instrument flight rules (IFR) require capturing the variability inherent in how aircraft follow standard flight procedures. The variability in aircraft behavior varies among flight stages. In this paper, we propose a probabilistic model that can learn the variability from the procedural data and flight tracks collected from radar surveillance data. For each segment, a Gaussian mixture model is used to learn the deviations of aircraft trajectories from their procedures. Given new procedures, we can generate synthetic trajectories by sampling a series of deviations from the trained Gaussian distributions and reconstructing the aircraft trajectory using the deviations and the procedures. We extend this method to capture pairwise correlations between aircraft and show how a pairwise model can be used to generate traffic involving an arbitra
&lt;/p&gt;</description></item></channel></rss>