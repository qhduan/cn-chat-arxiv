<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;</title><link>https://arxiv.org/abs/2403.17983</link><description>&lt;p&gt;
LLM&#29983;&#25104;&#20195;&#30721;&#30340;&#27700;&#21360;&#25216;&#26415;&#26159;&#21542;&#20855;&#26377;&#40065;&#26834;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Watermarking LLM-Generated Code Robust?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17983
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#65292;&#21457;&#29616;&#23481;&#26131;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#36716;&#25442;&#26469;&#31227;&#38500;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#20102;&#29616;&#26377;&#27700;&#21360;&#25216;&#26415;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;Python&#20195;&#30721;&#19978;&#30340;&#40065;&#26834;&#24615;&#12290;&#23613;&#31649;&#29616;&#26377;&#20316;&#21697;&#34920;&#26126;&#27700;&#21360;&#25216;&#26415;&#23545;&#33258;&#28982;&#35821;&#35328;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#20445;&#30041;&#35821;&#20041;&#30340;&#36716;&#25442;&#24456;&#23481;&#26131;&#31227;&#38500;&#20195;&#30721;&#19978;&#30340;&#36825;&#20123;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17983v1 Announce Type: cross  Abstract: We present the first study of the robustness of existing watermarking techniques on Python code generated by large language models. Although existing works showed that watermarking can be robust for natural language, we show that it is easy to remove these watermarks on code by semantic-preserving transformations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;</title><link>https://arxiv.org/abs/2402.14878</link><description>&lt;p&gt;
&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#30340;&#33021;&#25928;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Energy-efficiency Limits on Training AI Systems using Learning-in-Memory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14878
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#20869;&#23384;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#25928;&#38480;&#21046;&#65292;&#24182;&#25512;&#23548;&#20102;&#26032;&#30340;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: &#20869;&#23384;&#20013;&#23398;&#20064;&#65288;LIM&#65289;&#26159;&#19968;&#31181;&#26368;&#36817;&#25552;&#20986;&#30340;&#33539;Paradigm&#65292;&#26088;&#22312;&#20811;&#26381;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#22522;&#26412;&#20869;&#23384;&#29942;&#39048;&#12290;&#34429;&#28982;&#35745;&#31639;&#20110;&#20869;&#23384;&#65288;CIM&#65289;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#25152;&#35859;&#30340;&#20869;&#23384;&#22681;&#38382;&#39064;&#65288;&#21363;&#30001;&#20110;&#37325;&#22797;&#20869;&#23384;&#35835;&#21462;&#35775;&#38382;&#32780;&#28040;&#32791;&#30340;&#33021;&#37327;&#65289;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#20197;&#35757;&#32451;&#25152;&#38656;&#30340;&#31934;&#24230;&#37325;&#22797;&#20869;&#23384;&#20889;&#20837;&#26102;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#26356;&#26032;&#22681;&#65289;&#26159;&#19981;&#21487;&#30693;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#32771;&#34385;&#22312;&#30701;&#26399;&#21644;&#38271;&#26399;&#35760;&#24518;&#20043;&#38388;&#20256;&#36755;&#20449;&#24687;&#26102;&#25152;&#28040;&#32791;&#30340;&#33021;&#37327;&#65288;&#25972;&#21512;&#22681;&#65289;&#12290;LIM&#33539;&#24335;&#25552;&#20986;&#65292;&#22914;&#26524;&#29289;&#29702;&#20869;&#23384;&#30340;&#33021;&#37327;&#23631;&#38556;&#34987;&#33258;&#36866;&#24212;&#35843;&#21046;&#65292;&#20351;&#24471;&#23384;&#20648;&#22120;&#26356;&#26032;&#21644;&#25972;&#21512;&#30340;&#21160;&#24577;&#19982;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;AI&#27169;&#22411;&#30340;Lyapunov&#21160;&#24577;&#30456;&#21305;&#37197;&#65292;&#37027;&#20040;&#36825;&#20123;&#29942;&#39048;&#20063;&#21487;&#20197;&#34987;&#20811;&#26381;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25512;&#23548;&#20102;&#20351;&#29992;&#19981;&#21516;LIM&#24212;&#29992;&#31243;&#24207;&#35757;&#32451;AI&#31995;&#32479;&#26102;&#30340;&#33021;&#32791;&#30340;&#26032;&#29702;&#35770;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14878v1 Announce Type: cross  Abstract: Learning-in-memory (LIM) is a recently proposed paradigm to overcome fundamental memory bottlenecks in training machine learning systems. While compute-in-memory (CIM) approaches can address the so-called memory-wall (i.e. energy dissipated due to repeated memory read access) they are agnostic to the energy dissipated due to repeated memory writes at the precision required for training (the update-wall), and they don't account for the energy dissipated when transferring information between short-term and long-term memories (the consolidation-wall). The LIM paradigm proposes that these bottlenecks, too, can be overcome if the energy barrier of physical memories is adaptively modulated such that the dynamics of memory updates and consolidation match the Lyapunov dynamics of gradient-descent training of an AI model. In this paper, we derive new theoretical lower bounds on energy dissipation when training AI systems using different LIM app
&lt;/p&gt;</description></item><item><title>SMC&#24182;&#34892;&#25193;&#23637;&#26041;&#27861;pSMC&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26377;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06173</link><description>&lt;p&gt;
SMC&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#65306;&#24182;&#34892;&#24378;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
SMC Is All You Need: Parallel Strong Scaling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06173
&lt;/p&gt;
&lt;p&gt;
SMC&#24182;&#34892;&#25193;&#23637;&#26041;&#27861;pSMC&#20855;&#26377;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20855;&#26377;&#26377;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#36866;&#29992;&#20110;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#19968;&#33324;&#26694;&#26550;&#20013;&#65292;&#30446;&#26631;&#20998;&#24067;&#21482;&#33021;&#25353;&#27604;&#20363;&#24120;&#25968;&#36827;&#34892;&#35780;&#20272;&#12290;&#20256;&#32479;&#30340;&#19968;&#33268;Bayesian&#26041;&#27861;&#65292;&#22914;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;(SMC)&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;(MCMC)&#65292;&#20855;&#26377;&#26080;&#30028;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23436;&#20840;&#24182;&#34892;&#30340;&#24207;&#36143;&#33945;&#29305;&#21345;&#27931;(pSMC)&#26041;&#27861;&#65292;&#21487;&#20197;&#35777;&#26126;&#23427;&#20855;&#26377;&#24182;&#34892;&#24378;&#25193;&#23637;&#24615;&#65292;&#21363;&#22914;&#26524;&#20801;&#35768;&#24322;&#27493;&#36827;&#31243;&#25968;&#37327;&#22686;&#38271;&#65292;&#26102;&#38388;&#22797;&#26434;&#24615;(&#21644;&#27599;&#20010;&#33410;&#28857;&#30340;&#20869;&#23384;)&#20173;&#28982;&#20445;&#25345;&#26377;&#30028;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;pSMC&#20855;&#26377;MSE$=O(1/NR)$&#30340;&#29702;&#35770;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$N$&#34920;&#31034;&#27599;&#20010;&#22788;&#29702;&#22120;&#20013;&#30340;&#36890;&#20449;&#26679;&#26412;&#25968;&#37327;&#65292;$R$&#34920;&#31034;&#22788;&#29702;&#22120;&#25968;&#37327;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#36866;&#24403;&#22823;&#30340;&#38382;&#39064;&#30456;&#20851;$N$&#65292;&#24403;$R\rightarrow \infty$&#26102;&#65292;&#35813;&#26041;&#27861;&#20197;&#22266;&#23450;&#26377;&#38480;&#30340;&#26102;&#38388;&#22797;&#26434;&#24615;Cost$=O(1)$&#25910;&#25947;&#21040;&#26080;&#31351;&#23567;&#31934;&#24230;MSE$=O(\varepsilon^2)$&#65292;&#27809;&#26377;&#25928;&#29575;&#27844;&#28431;&#65292;&#21363;&#35745;&#31639;&#22797;&#26434;&#24615;Cost$=O(\varepsilon)$&#12290;
&lt;/p&gt;
&lt;p&gt;
In the general framework of Bayesian inference, the target distribution can only be evaluated up-to a constant of proportionality. Classical consistent Bayesian methods such as sequential Monte Carlo (SMC) and Markov chain Monte Carlo (MCMC) have unbounded time complexity requirements. We develop a fully parallel sequential Monte Carlo (pSMC) method which provably delivers parallel strong scaling, i.e. the time complexity (and per-node memory) remains bounded if the number of asynchronous processes is allowed to grow. More precisely, the pSMC has a theoretical convergence rate of MSE$ = O(1/NR)$, where $N$ denotes the number of communicating samples in each processor and $R$ denotes the number of processors. In particular, for suitably-large problem-dependent $N$, as $R \rightarrow \infty$ the method converges to infinitesimal accuracy MSE$=O(\varepsilon^2)$ with a fixed finite time-complexity Cost$=O(1)$ and with no efficiency leakage, i.e. computational complexity Cost$=O(\varepsilon
&lt;/p&gt;</description></item><item><title>&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03055</link><description>&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65306;&#23398;&#20064;&#20197;PAC-Bayes&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Actor-Critic: Learning to Explore with PAC-Bayes Uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03055
&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65288;PAC&#65289;&#36890;&#36807;&#22312;&#35780;&#35770;&#23478;&#20013;&#24314;&#27169;&#21644;&#25512;&#26029;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#25913;&#36827;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#33258;&#36866;&#24212;&#30340;&#25506;&#32034;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#29575;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;PAC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#32531;&#35299;&#25506;&#32034;&#19982;&#21033;&#29992;&#30340;&#24179;&#34913;&#38382;&#39064;&#65292;&#25913;&#36827;&#20102;&#36830;&#32493;&#25511;&#21046;&#24615;&#33021;&#12290;PAC&#36890;&#36807;&#23558;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#26080;&#32541;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28436;&#21592;&#35757;&#32451;&#20043;&#38388;&#30340;&#21160;&#24577;&#21327;&#21516;&#20316;&#29992;&#12290;&#25105;&#20204;&#30340;PAC&#31639;&#27861;&#30340;&#20851;&#38190;&#36129;&#29486;&#22312;&#20110;&#36890;&#36807;Probably Approximately Correct-Bayesian&#65288;PAC-Bayes&#65289;&#20998;&#26512;&#65292;&#26126;&#30830;&#24314;&#27169;&#21644;&#25512;&#26029;&#35780;&#35770;&#23478;&#30340;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#12290;&#36825;&#31181;&#23545;&#35780;&#35770;&#23478;&#19981;&#30830;&#23450;&#24615;&#30340;&#34701;&#20837;&#20351;PAC&#33021;&#22815;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#35843;&#25972;&#20854;&#25506;&#32034;&#31574;&#30053;&#65292;&#25351;&#23548;&#28436;&#21592;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#20013;&#30340;&#22266;&#23450;&#25110;&#39044;&#23450;&#30340;&#25506;&#32034;&#26041;&#26696;&#30456;&#27604;&#65292;PAC&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;&#36890;&#36807;PAC-Bayes&#20998;&#26512;&#24341;&#23548;&#30340;&#38543;&#26426;&#31574;&#30053;&#21644;&#35780;&#35770;&#23478;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#26159;&#21521;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#26356;&#20855;&#33258;&#36866;&#24212;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#25506;&#32034;&#31574;&#30053;&#36808;&#20986;&#30340;&#20851;&#38190;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Probabilistic Actor-Critic (PAC), a novel reinforcement learning algorithm with improved continuous control performance thanks to its ability to mitigate the exploration-exploitation trade-off. PAC achieves this by seamlessly integrating stochastic policies and critics, creating a dynamic synergy between the estimation of critic uncertainty and actor training. The key contribution of our PAC algorithm is that it explicitly models and infers epistemic uncertainty in the critic through Probably Approximately Correct-Bayesian (PAC-Bayes) analysis. This incorporation of critic uncertainty enables PAC to adapt its exploration strategy as it learns, guiding the actor's decision-making process. PAC compares favorably against fixed or pre-scheduled exploration schemes of the prior art. The synergy between stochastic policies and critics, guided by PAC-Bayes analysis, represents a fundamental step towards a more adaptive and effective exploration strategy in deep reinforcement lear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.00162</link><description>&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#25506;&#32034;&#32972;&#21518;&#30340;&#31070;&#35805;
&lt;/p&gt;
&lt;p&gt;
Behind the Myth of Exploration in Policy Gradients
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#23545;&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#20013;&#25506;&#32034;&#39033;&#30340;&#26032;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#20854;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#21644;&#22686;&#21152;&#26799;&#24230;&#20272;&#35745;&#30340;&#20004;&#31181;&#19981;&#21516;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#35814;&#32454;&#35752;&#35770;&#21644;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24320;&#36767;&#20102;&#26410;&#26469;&#23545;&#36825;&#20123;&#31574;&#30053;&#35774;&#35745;&#21644;&#20998;&#26512;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25919;&#31574;&#26799;&#24230;&#31639;&#27861;&#26159;&#35299;&#20915;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#30340;&#25511;&#21046;&#38382;&#39064;&#30340;&#26377;&#25928;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;&#20026;&#20102;&#35745;&#31639;&#25509;&#36817;&#26368;&#20248;&#30340;&#31574;&#30053;&#65292;&#22312;&#23454;&#36341;&#20013;&#24517;&#39035;&#22312;&#23398;&#20064;&#30446;&#26631;&#20013;&#21253;&#21547;&#25506;&#32034;&#39033;&#12290;&#23613;&#31649;&#36825;&#20123;&#39033;&#30340;&#26377;&#25928;&#24615;&#36890;&#24120;&#36890;&#36807;&#23545;&#25506;&#32034;&#29615;&#22659;&#30340;&#20869;&#22312;&#38656;&#27714;&#36827;&#34892;&#35777;&#26126;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#65292;&#21306;&#20998;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#20004;&#31181;&#19981;&#21516;&#21547;&#20041;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#20351;&#24471;&#24179;&#28369;&#23398;&#20064;&#30446;&#26631;&#25104;&#20026;&#21487;&#33021;&#65292;&#24182;&#22312;&#20445;&#25345;&#20840;&#23616;&#26368;&#22823;&#20540;&#30340;&#21516;&#26102;&#28040;&#38500;&#20102;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#20462;&#25913;&#20102;&#26799;&#24230;&#20272;&#35745;&#65292;&#22686;&#21152;&#20102;&#38543;&#26426;&#21442;&#25968;&#26356;&#26032;&#26368;&#32456;&#25552;&#20379;&#26368;&#20248;&#31574;&#30053;&#30340;&#27010;&#29575;&#12290;&#22522;&#20110;&#36825;&#20123;&#25928;&#24212;&#65292;&#25105;&#20204;&#35752;&#35770;&#24182;&#23454;&#35777;&#20102;&#22522;&#20110;&#29109;&#22870;&#21169;&#30340;&#25506;&#32034;&#31574;&#30053;&#65292;&#31361;&#20986;&#20102;&#20854;&#23616;&#38480;&#24615;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#20998;&#26512;&#36825;&#20123;&#31574;&#30053;&#30340;&#26410;&#26469;&#30740;&#31350;&#24320;&#36767;&#20102;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they make it possible to smooth the learning objective and to eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00019</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion MRI with Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26426;&#22120;&#23398;&#20064;&#22312;&#24357;&#25955;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23545;&#29616;&#26377;&#26041;&#27861;&#30340;&#24635;&#32467;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24357;&#25955;&#21152;&#26435;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;dMRI&#65289;&#20855;&#26377;&#38750;&#20405;&#20837;&#24615;&#35780;&#20272;&#22823;&#33041;&#24494;&#32467;&#26500;&#21644;&#32467;&#26500;&#36830;&#25509;&#30340;&#29420;&#29305;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20998;&#26512;dMRI&#25968;&#25454;&#20197;&#25552;&#21462;&#20020;&#24202;&#21644;&#31185;&#23398;&#30446;&#30340;&#30340;&#26377;&#29992;&#20449;&#24687;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290; dMRI&#27979;&#37327;&#36890;&#24120;&#21463;&#21040;&#24378;&#22122;&#22768;&#21644;&#20266;&#24433;&#30340;&#24178;&#25200;&#65292;&#25968;&#25454;&#20013;&#36890;&#24120;&#23384;&#22312;&#39640;&#30340;&#20250;&#35805;&#38388;&#21644;&#25195;&#25551;&#32773;&#38388;&#24322;&#36136;&#24615;&#65292;&#20197;&#21450;&#22823;&#33041;&#32467;&#26500;&#30340;&#30456;&#24403;&#22823;&#30340;&#20010;&#20307;&#38388;&#21464;&#24322;&#65292;&#24182;&#19988;&#27979;&#37327;&#21644;&#24863;&#20852;&#36259;&#29616;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21487;&#33021;&#38750;&#24120;&#22797;&#26434;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;dMRI&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#36825;&#20123;&#23581;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#24050;&#32463;&#35299;&#20915;&#20102;&#24494;&#32467;&#26500;&#26144;&#23556;&#12289;&#32420;&#32500;&#26463;&#25551;&#35760;&#12289;&#30333;&#36136;&#32420;&#32500;&#26463;&#20998;&#26512;&#20197;&#21450;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#21327;&#35843;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#20027;&#35201;&#21457;&#29616;&#12289;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-weighted magnetic resonance imaging (dMRI) offers unique capabilities such as noninvasive assessment of brain's micro-structure and structural connectivity. However, analyzing the dMRI data to extract useful information for clinical and scientific purposes is challenging. The dMRI measurements often suffer from strong noise and artifacts, there is usually high inter-session and inter-scanner heterogeneity in the data and considerable inter-subject variability in brain structure, and the relationship between measurements and the phenomena of interest can be highly complex. Recent years have witnessed increasing use of machine learning methods for dMRI analysis. This manuscript aims to assess these efforts, with a focus on methods that have addressed micro-structure mapping, tractography, white matter tract analysis, as well as data preprocessing and harmonization. We summarize the main findings, strengths, and weaknesses of the existing methods and suggest topics for future re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;</title><link>http://arxiv.org/abs/2401.13796</link><description>&lt;p&gt;
&#19981;&#35201;&#25353;&#25353;&#38062;&#65281;&#25506;&#32034;&#26426;&#22120;&#23398;&#20064;&#21644;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Don't Push the Button! Exploring Data Leakage Risks in Machine Learning and Transfer Learning. (arXiv:2401.13796v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#38382;&#39064;&#65292;&#21363;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#65292;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#32780;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#23548;&#33268;&#20048;&#35266;&#30340;&#24615;&#33021;&#20272;&#35745;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#30340;&#36827;&#23637;&#65292;&#20026;&#22810;&#20010;&#39046;&#22495;&#25552;&#20379;&#20102;&#39044;&#27979;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;ML&#24037;&#20855;&#30340;&#26085;&#30410;&#21487;&#33719;&#24471;&#24615;&#65292;&#35768;&#22810;&#20174;&#19994;&#32773;&#32570;&#20047;&#28145;&#20837;&#30340;ML&#19987;&#19994;&#30693;&#35782;&#65292;&#37319;&#29992;&#20102;&#8220;&#25353;&#25353;&#38062;&#8221;&#26041;&#27861;&#65292;&#21033;&#29992;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#32780;&#24573;&#35270;&#20102;&#24213;&#23618;&#31639;&#27861;&#30340;&#28145;&#20837;&#29702;&#35299;&#12290;&#34429;&#28982;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#20294;&#23427;&#24341;&#21457;&#20102;&#23545;&#32467;&#26524;&#21487;&#38752;&#24615;&#30340;&#25285;&#24551;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#24615;&#33021;&#35780;&#20272;&#31561;&#25361;&#25112;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;ML&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#21363;&#25968;&#25454;&#27844;&#38706;&#65292;&#20854;&#20013;&#26410;&#39044;&#26399;&#30340;&#20449;&#24687;&#27745;&#26579;&#20102;&#35757;&#32451;&#25968;&#25454;&#65292;&#24433;&#21709;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#35780;&#20272;&#12290;&#30001;&#20110;&#32570;&#20047;&#29702;&#35299;&#65292;&#29992;&#25143;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#24573;&#35270;&#20851;&#38190;&#27493;&#39588;&#65292;&#20174;&#32780;&#23548;&#33268;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#30340;&#20048;&#35266;&#24615;&#33021;&#20272;&#35745;&#12290;&#35780;&#20272;&#24615;&#33021;&#19982;&#23454;&#38469;&#22312;&#26032;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30340;&#24046;&#24322;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#20851;&#27880;&#28857;&#12290;&#26412;&#25991;&#29305;&#21035;&#23558;ML&#20013;&#30340;&#25968;&#25454;&#27844;&#38706;&#20998;&#20026;&#19981;&#21516;&#31867;&#21035;&#65292;&#24182;&#35752;&#35770;&#20102;&#30456;&#20851;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Learning (ML) has revolutionized various domains, offering predictive capabilities in several areas. However, with the increasing accessibility of ML tools, many practitioners, lacking deep ML expertise, adopt a "push the button" approach, utilizing user-friendly interfaces without a thorough understanding of underlying algorithms. While this approach provides convenience, it raises concerns about the reliability of outcomes, leading to challenges such as incorrect performance evaluation. This paper addresses a critical issue in ML, known as data leakage, where unintended information contaminates the training data, impacting model performance evaluation. Users, due to a lack of understanding, may inadvertently overlook crucial steps, leading to optimistic performance estimates that may not hold in real-world scenarios. The discrepancy between evaluated and actual performance on new data is a significant concern. In particular, this paper categorizes data leakage in ML, discussi
&lt;/p&gt;</description></item></channel></rss>