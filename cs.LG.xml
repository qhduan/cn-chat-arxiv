<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02353</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#35821;&#20041;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Semantic Augmentation in Images using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02353
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#32570;&#20047;&#36825;&#20123;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38480;&#21046;&#20854;&#27867;&#21270;&#21040;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21033;&#29992;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02353v1 Announce Type: cross  Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;LNPT&#65292;&#19968;&#31181;&#26080;&#26631;&#31614;&#32593;&#32476;&#20462;&#21098;&#21644;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#24046;&#36317;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#20854;&#20934;&#30830;&#30456;&#20851;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#30830;&#23450;&#20462;&#21098;&#32467;&#26500;&#30340;&#38590;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.12690</link><description>&lt;p&gt;
LNPT&#65306;&#26080;&#26631;&#31614;&#32593;&#32476;&#20462;&#21098;&#19982;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
LNPT: Label-free Network Pruning and Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12690
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LNPT&#65292;&#19968;&#31181;&#26080;&#26631;&#31614;&#32593;&#32476;&#20462;&#21098;&#21644;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#23398;&#20064;&#24046;&#36317;&#30340;&#27010;&#24565;&#65292;&#24378;&#35843;&#20854;&#20934;&#30830;&#30456;&#20851;&#24615;&#65292;&#20197;&#35299;&#20915;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#30830;&#23450;&#20462;&#21098;&#32467;&#26500;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#20043;&#21069;&#20462;&#21098;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#20854;&#33021;&#22815;&#37096;&#32626;&#22312;&#26234;&#33021;&#35774;&#22791;&#19978;&#12290;&#36890;&#36807;&#20445;&#30041;&#26377;&#21161;&#20110;&#27867;&#21270;&#30340;&#26435;&#37325;&#65292;&#20462;&#21098;&#21518;&#30340;&#32593;&#32476;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#26234;&#33021;&#35774;&#22791;&#19978;&#36816;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#24046;&#36317;&#30340;&#27010;&#24565;&#65292;&#24182;&#24378;&#35843;&#23427;&#19982;&#27867;&#21270;&#30340;&#20934;&#30830;&#30456;&#20851;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#24046;&#36317;&#36890;&#36807;&#32593;&#32476;&#20498;&#25968;&#31532;&#20108;&#23618;&#30340;&#29305;&#24449;&#22270;&#24418;&#24335;&#19982;&#27867;&#21270;&#24615;&#33021;&#30340;&#21464;&#21270;&#30456;&#19968;&#33268;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#26694;&#26550; LNPT&#65292;&#20351;&#24471;&#20113;&#31471;&#25104;&#29087;&#32593;&#32476;&#33021;&#22815;&#25552;&#20379;&#22312;&#32447;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12690v1 Announce Type: new  Abstract: Pruning before training enables the deployment of neural networks on smart devices. By retaining weights conducive to generalization, pruned networks can be accommodated on resource-constrained smart devices. It is commonly held that the distance on weight norms between the initialized and the fully-trained networks correlates with generalization performance. However, as we have uncovered, inconsistency between this metric and generalization during training processes, which poses an obstacle to determine the pruned structures on smart devices in advance. In this paper, we introduce the concept of the learning gap, emphasizing its accurate correlation with generalization. Experiments show that the learning gap, in the form of feature maps from the penultimate layer of networks, aligns with variations of generalization performance. We propose a novel learning framework, LNPT, which enables mature networks on the cloud to provide online gui
&lt;/p&gt;</description></item><item><title>SEVEN&#36890;&#36807;&#20445;&#30041;&#26799;&#24230;&#22122;&#22768;&#36739;&#23567;&#30340;&#26435;&#37325;&#65292;&#22312;&#21098;&#26525;Transformer&#27169;&#22411;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.12688</link><description>&lt;p&gt;
SEVEN: &#36890;&#36807;&#20445;&#30041;&#21736;&#20853;&#26469;&#21098;&#26525;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SEVEN: Pruning Transformer Model by Reserving Sentinels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12688
&lt;/p&gt;
&lt;p&gt;
SEVEN&#36890;&#36807;&#20445;&#30041;&#26799;&#24230;&#22122;&#22768;&#36739;&#23567;&#30340;&#26435;&#37325;&#65292;&#22312;&#21098;&#26525;Transformer&#27169;&#22411;&#26102;&#21462;&#24471;&#20102;&#20248;&#24322;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;Transformer&#27169;&#22411;&#24050;&#32463;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#21487;&#35266;&#30340;&#21442;&#25968;&#35268;&#27169;&#65292;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#38480;&#21046;&#65292;&#23588;&#20854;&#26159;&#22312;&#31227;&#21160;&#35774;&#22791;&#19978;&#12290;&#37492;&#20110;Transformer&#27169;&#22411;&#30456;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#26799;&#24230;&#26159;&#21160;&#24577;&#19988;&#38169;&#32508;&#22797;&#26434;&#30340;&#65292;&#24120;&#29992;&#30340;&#21098;&#26525;&#26041;&#27861;&#24448;&#24448;&#20250;&#20445;&#30041;&#20855;&#26377;&#36739;&#22823;&#26799;&#24230;&#22122;&#22768;&#30340;&#26435;&#37325;&#12290;&#36825;&#23548;&#33268;&#34987;&#21098;&#26525;&#30340;&#27169;&#22411;&#23545;&#31232;&#30095;&#24615;&#21644;&#25968;&#25454;&#38598;&#25935;&#24863;&#65292;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#31526;&#21495;&#19979;&#38477;&#65288;SD&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21644;&#24494;&#35843;Transformer&#27169;&#22411;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;SD&#30340;&#32047;&#31215;&#36807;&#31243;&#25551;&#36848;Transformer&#27169;&#22411;&#19978;&#30340;&#22122;&#22768;&#25209;&#26799;&#24230;&#24207;&#21015;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35774;&#35745;&#21160;&#24577;&#35780;&#20272;&#26435;&#37325;&#30340;&#37325;&#35201;&#24615;&#20998;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SEVEN&#65292;&#29305;&#21035;&#20559;&#21521;&#20110;&#20855;&#26377;&#25345;&#32493;&#39640;&#25935;&#24863;&#24230;&#30340;&#26435;&#37325;&#65292;&#21363;&#26799;&#24230;&#22122;&#22768;&#36739;&#23567;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12688v1 Announce Type: new  Abstract: Large-scale Transformer models (TM) have demonstrated outstanding performance across various tasks. However, their considerable parameter size restricts their applicability, particularly on mobile devices. Due to the dynamic and intricate nature of gradients on TM compared to Convolutional Neural Networks, commonly used pruning methods tend to retain weights with larger gradient noise. This results in pruned models that are sensitive to sparsity and datasets, exhibiting suboptimal performance. Symbolic Descent (SD) is a general approach for training and fine-tuning TM. In this paper, we attempt to describe the noisy batch gradient sequences on TM through the cumulative process of SD. We utilize this design to dynamically assess the importance scores of weights.SEVEN is introduced by us, which particularly favors weights with consistently high sensitivity, i.e., weights with small gradient noise. These weights are tended to be preserved b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#20250;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#65292;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#34920;&#31034;&#20197;&#21450;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#20063;&#20351;&#24471;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#21464;&#24471;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.10791</link><description>&lt;p&gt;
&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#26159;&#19968;&#25226;&#21452;&#20995;&#21073;
&lt;/p&gt;
&lt;p&gt;
Early alignment in two-layer networks training is a two-edged sword. (arXiv:2401.10791v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10791
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#23618;&#32593;&#32476;&#35757;&#32451;&#20013;&#30340;&#26089;&#26399;&#23545;&#40784;&#29616;&#35937;&#65292;&#21457;&#29616;&#22312;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#20013;&#65292;&#31070;&#32463;&#20803;&#20250;&#22312;&#35757;&#32451;&#30340;&#26089;&#26399;&#38454;&#27573;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#65292;&#23548;&#33268;&#32593;&#32476;&#31232;&#30095;&#34920;&#31034;&#20197;&#21450;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#20063;&#20351;&#24471;&#35757;&#32451;&#30446;&#26631;&#30340;&#26368;&#23567;&#21270;&#21464;&#24471;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#19968;&#38454;&#20248;&#21270;&#26041;&#27861;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#26159;&#28145;&#24230;&#23398;&#20064;&#25104;&#21151;&#30340;&#26680;&#24515;&#12290;&#21021;&#22987;&#21270;&#30340;&#35268;&#27169;&#26159;&#19968;&#20010;&#20851;&#38190;&#22240;&#32032;&#65292;&#22240;&#20026;&#23567;&#30340;&#21021;&#22987;&#21270;&#36890;&#24120;&#19982;&#29305;&#24449;&#23398;&#20064;&#27169;&#24335;&#30456;&#20851;&#65292;&#22312;&#36825;&#31181;&#27169;&#24335;&#19979;&#65292;&#26799;&#24230;&#19979;&#38477;&#23545;&#31616;&#21333;&#35299;&#38544;&#21547;&#20559;&#22909;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26089;&#26399;&#23545;&#40784;&#38454;&#27573;&#30340;&#26222;&#36941;&#21644;&#37327;&#21270;&#25551;&#36848;&#65292;&#26368;&#21021;&#30001;Maennel&#31561;&#20154;&#25552;&#20986;&#12290;&#23545;&#20110;&#23567;&#21021;&#22987;&#21270;&#21644;&#19968;&#20010;&#38544;&#34255;&#30340;ReLU&#23618;&#32593;&#32476;&#65292;&#35757;&#32451;&#21160;&#24577;&#30340;&#26089;&#26399;&#38454;&#27573;&#23548;&#33268;&#31070;&#32463;&#20803;&#21521;&#20851;&#38190;&#26041;&#21521;&#36827;&#34892;&#23545;&#40784;&#12290;&#36825;&#31181;&#23545;&#40784;&#24341;&#21457;&#20102;&#32593;&#32476;&#30340;&#31232;&#30095;&#34920;&#31034;&#65292;&#36825;&#19982;&#26799;&#24230;&#27969;&#22312;&#25910;&#25947;&#26102;&#30340;&#38544;&#21547;&#20559;&#22909;&#30452;&#25509;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31232;&#30095;&#35825;&#23548;&#30340;&#23545;&#40784;&#26159;&#20197;&#22312;&#26368;&#23567;&#21270;&#35757;&#32451;&#30446;&#26631;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#20026;&#20195;&#20215;&#30340;&#65306;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#31034;&#20363;&#65292;&#20854;&#20013;&#36229;&#21442;&#25968;&#32593;&#32476;&#26080;&#27861;&#25910;&#25947;&#21040;&#20840;&#23616;&#26368;&#23567;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training neural networks with first order optimisation methods is at the core of the empirical success of deep learning. The scale of initialisation is a crucial factor, as small initialisations are generally associated to a feature learning regime, for which gradient descent is implicitly biased towards simple solutions. This work provides a general and quantitative description of the early alignment phase, originally introduced by Maennel et al. (2018) . For small initialisation and one hidden ReLU layer networks, the early stage of the training dynamics leads to an alignment of the neurons towards key directions. This alignment induces a sparse representation of the network, which is directly related to the implicit bias of gradient flow at convergence. This sparsity inducing alignment however comes at the expense of difficulties in minimising the training objective: we also provide a simple data example for which overparameterised networks fail to converge towards global minima and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#21644;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01100</link><description>&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#23454;&#29616;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding. (arXiv:2401.01100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01100
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#21644;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#26088;&#22312;&#25581;&#31034;&#39640;&#32500;&#31354;&#38388;&#20013;&#22797;&#26434;&#38750;&#32447;&#24615;&#27969;&#24418;&#20869;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#27969;&#24418;&#20551;&#35774;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#26469;&#20419;&#36827;&#21487;&#35270;&#21270;&#12289;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#33719;&#24471;&#20851;&#38190;&#27934;&#23519;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20840;&#23616;&#32467;&#26500;&#20013;&#30340;&#22823;&#37327;&#22833;&#30495;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#24213;&#23618;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;(scML)&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;&#23547;&#25214;&#19968;&#32452;&#22320;&#26631;&#26469;&#26500;&#24314;&#25972;&#20010;&#25968;&#25454;&#30340;&#20302;&#32500;&#39592;&#26550;&#65292;&#28982;&#21518;&#23558;&#38750;&#22320;&#26631;&#24341;&#20837;&#22320;&#26631;&#31354;&#38388;&#20013;
&lt;/p&gt;
&lt;p&gt;
As a pivotal approach in machine learning and data science, manifold learning aims to uncover the intrinsic low-dimensional structure within complex nonlinear manifolds in high-dimensional space. By exploiting the manifold hypothesis, various techniques for nonlinear dimension reduction have been developed to facilitate visualization, classification, clustering, and gaining key insights. Although existing manifold learning methods have achieved remarkable successes, they still suffer from extensive distortions incurred in the global structure, which hinders the understanding of underlying patterns. Scalability issues also limit their applicability for handling large-scale data. Here, we propose a scalable manifold learning (scML) method that can manipulate large-scale and high-dimensional data in an efficient manner. It starts by seeking a set of landmarks to construct the low-dimensional skeleton of the entire data and then incorporates the non-landmarks into the landmark space based 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20048;&#39640;&#31215;&#26408;&#65292;&#36890;&#36807;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#20016;&#23500;&#21644;&#20840;&#23616;&#20869;&#23481;&#21327;&#35843;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#21487;&#33258;&#36866;&#24212;&#30340;&#36845;&#20195;&#32454;&#21270;&#25193;&#25955;&#24314;&#27169;&#12290;&#36825;&#20123;&#31215;&#26408;&#21487;&#20197;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#37325;&#26500;&#65292;&#20174;&#32780;&#20943;&#23569;&#37319;&#26679;&#25104;&#26412;&#24182;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2310.06389</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#22534;&#21472;&#21644;&#21487;&#36339;&#36807;&#30340;&#20048;&#39640;&#31215;&#26408;&#20197;&#23454;&#29616;&#39640;&#25928;&#12289;&#21487;&#37325;&#26500;&#21644;&#21487;&#21464;&#20998;&#36776;&#29575;&#30340;&#25193;&#25955;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Learning Stackable and Skippable LEGO Bricks for Efficient, Reconfigurable, and Variable-Resolution Diffusion Modeling. (arXiv:2310.06389v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20048;&#39640;&#31215;&#26408;&#65292;&#36890;&#36807;&#38598;&#25104;&#23616;&#37096;&#29305;&#24449;&#20016;&#23500;&#21644;&#20840;&#23616;&#20869;&#23481;&#21327;&#35843;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#19988;&#21487;&#33258;&#36866;&#24212;&#30340;&#36845;&#20195;&#32454;&#21270;&#25193;&#25955;&#24314;&#27169;&#12290;&#36825;&#20123;&#31215;&#26408;&#21487;&#20197;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#29992;&#20110;&#22312;&#27979;&#35797;&#26102;&#26681;&#25454;&#38656;&#35201;&#36827;&#34892;&#37325;&#26500;&#65292;&#20174;&#32780;&#20943;&#23569;&#37319;&#26679;&#25104;&#26412;&#24182;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22312;&#29983;&#25104;&#30495;&#23454;&#24863;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#35757;&#32451;&#21644;&#37319;&#26679;&#26041;&#38754;&#20855;&#26377;&#26174;&#33879;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#23613;&#31649;&#26377;&#21508;&#31181;&#25216;&#26415;&#26469;&#35299;&#20915;&#36825;&#20123;&#35745;&#31639;&#25361;&#25112;&#65292;&#20294;&#19968;&#20010;&#36739;&#23569;&#25506;&#32034;&#30340;&#38382;&#39064;&#26159;&#35774;&#35745;&#19968;&#20010;&#39640;&#25928;&#19988;&#36866;&#24212;&#24615;&#24378;&#30340;&#32593;&#32476;&#39592;&#24178;&#65292;&#29992;&#20110;&#36845;&#20195;&#32454;&#21270;&#12290;&#24403;&#21069;&#30340;&#36873;&#39033;&#22914;U-Net&#21644;Vision Transformer&#36890;&#24120;&#20381;&#36182;&#20110;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#28145;&#24230;&#32593;&#32476;&#65292;&#32570;&#20047;&#22312;&#21464;&#37327;&#20998;&#36776;&#29575;&#19979;&#29983;&#25104;&#22270;&#20687;&#25110;&#20351;&#29992;&#27604;&#35757;&#32451;&#20013;&#26356;&#23567;&#30340;&#32593;&#32476;&#25152;&#38656;&#30340;&#28789;&#27963;&#24615;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#20048;&#39640;&#31215;&#26408;&#65292;&#23427;&#20204;&#26080;&#32541;&#38598;&#25104;&#20102;&#23616;&#37096;&#29305;&#24449;&#20016;&#23500;&#21644;&#20840;&#23616;&#20869;&#23481;&#21327;&#35843;&#12290;&#36825;&#20123;&#31215;&#26408;&#21487;&#20197;&#22534;&#21472;&#22312;&#19968;&#36215;&#65292;&#21019;&#24314;&#19968;&#20010;&#27979;&#35797;&#26102;&#21487;&#37325;&#26500;&#30340;&#25193;&#25955;&#39592;&#24178;&#65292;&#20801;&#35768;&#36873;&#25321;&#24615;&#36339;&#36807;&#31215;&#26408;&#20197;&#20943;&#23569;&#37319;&#26679;&#25104;&#26412;&#65292;&#24182;&#29983;&#25104;&#27604;&#35757;&#32451;&#25968;&#25454;&#26356;&#39640;&#20998;&#36776;&#29575;&#30340;&#22270;&#20687;&#12290;&#20048;&#39640;&#31215;&#26408;&#36890;&#36807;MLP&#23545;&#23616;&#37096;&#21306;&#22495;&#36827;&#34892;&#20016;&#23500;&#65292;&#24182;&#20351;&#29992;Transformer&#22359;&#36827;&#34892;&#21464;&#25442;&#65292;&#21516;&#26102;&#20445;&#25345;&#19968;&#33268;&#30340;&#20840;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Diffusion models excel at generating photo-realistic images but come with significant computational costs in both training and sampling. While various techniques address these computational challenges, a less-explored issue is designing an efficient and adaptable network backbone for iterative refinement. Current options like U-Net and Vision Transformer often rely on resource-intensive deep networks and lack the flexibility needed for generating images at variable resolutions or with a smaller network than used in training. This study introduces LEGO bricks, which seamlessly integrate Local-feature Enrichment and Global-content Orchestration. These bricks can be stacked to create a test-time reconfigurable diffusion backbone, allowing selective skipping of bricks to reduce sampling costs and generate higher-resolution images than the training data. LEGO bricks enrich local regions with an MLP and transform them using a Transformer block while maintaining a consistent full-resolution i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2309.01219</link><description>&lt;p&gt;
AI&#28023;&#27915;&#20013;&#30340;&#22934;&#24618;&#20043;&#27468;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24187;&#35273;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models. (arXiv:2309.01219v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24187;&#35273;&#30340;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#20294;&#20154;&#20204;&#23545;&#20854;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#34920;&#31034;&#25285;&#24551;&#65306;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#19982;&#29992;&#25143;&#36755;&#20837;&#19981;&#31526;&#12289;&#19982;&#20808;&#21069;&#29983;&#25104;&#30340;&#20869;&#23481;&#30456;&#30683;&#30462;&#25110;&#19982;&#24050;&#24314;&#31435;&#30340;&#19990;&#30028;&#30693;&#35782;&#19981;&#31526;&#30340;&#20869;&#23481;&#12290;&#36825;&#31181;&#29616;&#35937;&#23545;LLMs&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#21487;&#38752;&#24615;&#26500;&#25104;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#23545;&#20851;&#20110;&#24187;&#35273;&#26816;&#27979;&#12289;&#35299;&#37322;&#21644;&#32531;&#35299;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#37325;&#28857;&#25506;&#35752;&#20102;LLMs&#25152;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LLM&#24187;&#35273;&#29616;&#35937;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#20998;&#31867;&#65292;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#26088;&#22312;&#32531;&#35299;LLM&#24187;&#35273;&#30340;&#26041;&#27861;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated remarkable capabilities across a range of downstream tasks, a significant concern revolves around their propensity to exhibit hallucinations: LLMs occasionally generate content that diverges from the user input, contradicts previously generated context, or misaligns with established world knowledge. This phenomenon poses a substantial challenge to the reliability of LLMs in real-world scenarios. In this paper, we survey recent efforts on the detection, explanation, and mitigation of hallucination, with an emphasis on the unique challenges posed by LLMs. We present taxonomies of the LLM hallucination phenomena and evaluation benchmarks, analyze existing approaches aiming at mitigating LLM hallucination, and discuss potential directions for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#27010;&#29575;&#30005;&#36335;&#30340;&#21464;&#20998;&#30005;&#36335;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;Max-Cut&#38382;&#39064;&#30340;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21464;&#20998;&#30005;&#36335;&#22312;&#22810;&#31181;&#22270;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#35780;&#20272;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#24615;&#33021;&#26102;&#65292;&#21487;&#20197;&#23558;&#20854;&#19982;&#20855;&#26377;&#23376;&#36890;&#29992;&#38376;&#38598;&#30340;&#21464;&#20998;&#30005;&#36335;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35782;&#21035;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#20248;&#21183;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2308.14981</link><description>&lt;p&gt;
&#23376;&#36890;&#29992;&#21464;&#20998;&#30005;&#36335;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#32763;&#35793;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Sub-universal variational circuits for combinatorial optimization problems. (arXiv:2308.14981v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14981
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32463;&#20856;&#27010;&#29575;&#30005;&#36335;&#30340;&#21464;&#20998;&#30005;&#36335;&#65292;&#29992;&#20110;&#35299;&#20915;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#12290;&#36890;&#36807;&#23545;Max-Cut&#38382;&#39064;&#30340;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#31181;&#21464;&#20998;&#30005;&#36335;&#22312;&#22810;&#31181;&#22270;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#12290;&#22312;&#35780;&#20272;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#24615;&#33021;&#26102;&#65292;&#21487;&#20197;&#23558;&#20854;&#19982;&#20855;&#26377;&#23376;&#36890;&#29992;&#38376;&#38598;&#30340;&#21464;&#20998;&#30005;&#36335;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35782;&#21035;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#20248;&#21183;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#21644;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#65292;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32463;&#20856;&#27010;&#29575;&#30005;&#36335;&#65292;&#29992;&#20110;&#29983;&#25104;&#23545;&#30001;&#20108;&#20301;&#38543;&#26426;&#30697;&#38453;&#26500;&#24314;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#30340;&#36817;&#20284;&#35299;&#12290;&#36890;&#36807;&#25968;&#20540;&#30740;&#31350;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#21464;&#20998;&#30005;&#36335;&#22312;&#35299;&#20915;&#19981;&#26029;&#22686;&#21152;&#35268;&#27169;&#30340;&#21508;&#31181;&#22270;&#30340;Max-Cut&#38382;&#39064;&#20013;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32463;&#20856;&#31639;&#27861;&#22312;&#22810;&#31181;&#31867;&#22411;&#30340;&#22270;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#30456;&#27604;&#20110;&#37327;&#23376;&#36817;&#20284;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#23558;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#30340;&#24615;&#33021;&#19982;&#20855;&#26377;&#23376;&#36890;&#29992;&#38376;&#38598;&#30340;&#21464;&#20998;&#30005;&#36335;&#36827;&#34892;&#35780;&#20272;&#65292;&#26159;&#35782;&#21035;&#37327;&#23376;&#21464;&#20998;&#30005;&#36335;&#21487;&#31361;&#20986;&#20248;&#21183;&#39046;&#22495;&#30340;&#26377;&#20215;&#20540;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum variational circuits have gained significant attention due to their applications in the quantum approximate optimization algorithm and quantum machine learning research. This work introduces a novel class of classical probabilistic circuits designed for generating approximate solutions to combinatorial optimization problems constructed using two-bit stochastic matrices. Through a numerical study, we investigate the performance of our proposed variational circuits in solving the Max-Cut problem on various graphs of increasing sizes. Our classical algorithm demonstrates improved performance for several graph types to the quantum approximate optimization algorithm. Our findings suggest that evaluating the performance of quantum variational circuits against variational circuits with sub-universal gate sets is a valuable benchmark for identifying areas where quantum variational circuits can excel.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.01222</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#65306;&#26368;&#26032;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Calibration in Deep Learning: A Survey of the State-of-the-Art. (arXiv:2308.01222v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#26657;&#20934;&#26041;&#27861;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20854;&#21407;&#29702;&#30340;&#29702;&#35299;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33021;&#21147;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#23548;&#33268;&#27169;&#22411;&#39044;&#27979;&#19981;&#21487;&#38752;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#20123;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#21487;&#38752;&#12289;&#40065;&#26834;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#27169;&#22411;&#30340;&#26657;&#20934;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20855;&#26377;&#39640;&#39044;&#27979;&#33021;&#21147;&#30340;&#29616;&#20195;&#31070;&#32463;&#32593;&#32476;&#30340;&#26657;&#20934;&#24615;&#36739;&#24046;&#65292;&#20135;&#29983;&#19981;&#21487;&#38752;&#30340;&#27169;&#22411;&#39044;&#27979;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#23545;&#27169;&#22411;&#30340;&#26657;&#20934;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#29702;&#24819;&#30340;&#28145;&#24230;&#27169;&#22411;&#19981;&#20165;&#24212;&#20855;&#26377;&#39640;&#39044;&#27979;&#24615;&#33021;&#65292;&#36824;&#24212;&#20855;&#26377;&#33391;&#22909;&#30340;&#26657;&#20934;&#24615;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#20123;&#20351;&#29992;&#19981;&#21516;&#26426;&#21046;&#36827;&#34892;&#28145;&#24230;&#27169;&#22411;&#26657;&#20934;&#30340;&#26041;&#27861;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#26368;&#26032;&#30340;&#26657;&#20934;&#26041;&#27861;&#65292;&#24182;&#35299;&#37322;&#20102;&#23427;&#20204;&#25191;&#34892;&#27169;&#22411;&#26657;&#20934;&#30340;&#21407;&#29702;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20174;&#27169;&#22411;&#26657;&#20934;&#30340;&#23450;&#20041;&#24320;&#22987;&#65292;&#35299;&#37322;&#20102;&#27169;&#22411;&#26657;&#20934;&#19981;&#20934;&#30830;&#30340;&#26681;&#26412;&#21407;&#22240;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#20197;&#34913;&#37327;&#27169;&#22411;&#26657;&#20934;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#19968;&#20123;&#26657;&#20934;&#26041;&#27861;&#30340;&#26041;&#27861;&#21644;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep neural models plays an important role in building reliable, robust AI systems in safety-critical applications. Recent work has shown that modern neural networks that possess high predictive capability are poorly calibrated and produce unreliable model predictions. Though deep learning models achieve remarkable performance on various benchmarks, the study of model calibration and reliability is relatively underexplored. Ideal deep models should have not only high predictive performance but also be well calibrated. There have been some recent methods proposed to calibrate deep models by using different mechanisms. In this survey, we review the state-of-the-art calibration methods and provide an understanding of their principles for performing model calibration. First, we start with the definition of model calibration and explain the root causes of model miscalibration. Then we introduce the key metrics that can measure this aspect. It is followed by a summary of calibrat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#28909;&#21147;&#23398;&#35266;&#28857;&#65292;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;Ising&#27169;&#22411;&#26469;&#35745;&#31639;&#30001;&#35757;&#32451;&#25968;&#25454;&#24341;&#21457;&#30340;&#31232;&#30095;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#31354;&#38388;&#37197;&#32622;&#21644;&#37325;&#26500;&#22797;&#26434;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2307.11838</link><description>&lt;p&gt;
&#31232;&#30095;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#24341;&#21457;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Data-Induced Interactions of Sparse Sensors. (arXiv:2307.11838v1 [cond-mat.stat-mech])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#28909;&#21147;&#23398;&#35266;&#28857;&#65292;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;Ising&#27169;&#22411;&#26469;&#35745;&#31639;&#30001;&#35757;&#32451;&#25968;&#25454;&#24341;&#21457;&#30340;&#31232;&#30095;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20174;&#32780;&#20248;&#21270;&#20256;&#24863;&#22120;&#30340;&#31354;&#38388;&#37197;&#32622;&#21644;&#37325;&#26500;&#22797;&#26434;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#21644;&#24037;&#31243;&#20013;&#65292;&#22823;&#32500;&#24230;&#30340;&#32463;&#39564;&#25968;&#25454;&#32463;&#24120;&#20855;&#26377;&#20302;&#31209;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#34920;&#31034;&#20026;&#20165;&#30001;&#20960;&#20010;&#29305;&#24449;&#27169;&#24335;&#30340;&#32452;&#21512;&#12290;&#30001;&#20110;&#36825;&#31181;&#32467;&#26500;&#65292;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#20165;&#26377;&#23569;&#25968;&#23616;&#37096;&#21270;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#26469;&#37325;&#26032;&#26500;&#24314;&#22797;&#26434;&#31995;&#32479;&#30340;&#23436;&#25972;&#29366;&#24577;&#12290;&#36825;&#31181;&#37325;&#26500;&#30340;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#20256;&#24863;&#22120;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#26174;&#33879;&#21462;&#20915;&#20110;&#20256;&#24863;&#22120;&#30340;&#31354;&#38388;&#37197;&#32622;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#22810;&#31181;&#22522;&#20110;&#32570;&#22833;&#25554;&#20540;&#21644;QR&#20998;&#35299;&#30340;&#31639;&#27861;&#26469;&#20248;&#21270;&#20256;&#24863;&#22120;&#20301;&#32622;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#37319;&#29992;&#28909;&#21147;&#23398;&#35266;&#28857;&#35745;&#31639;&#30001;&#35757;&#32451;&#25968;&#25454;&#24341;&#21457;&#30340;&#20256;&#24863;&#22120;&#30456;&#20114;&#20316;&#29992;&#30340;&#23436;&#25972;&#22320;&#24418;&#12290;&#35813;&#22320;&#24418;&#37319;&#29992;&#32479;&#35745;&#29289;&#29702;&#23398;&#20013;&#30340;Ising&#27169;&#22411;&#30340;&#24418;&#24335;&#65292;&#32771;&#34385;&#21040;&#27599;&#20010;&#20256;&#24863;&#22120;&#20301;&#32622;&#25429;&#33719;&#30340;&#25968;&#25454;&#26041;&#24046;&#20197;&#21450;&#20256;&#24863;&#22120;&#20043;&#38388;&#30340;&#20018;&#25200;&#12290;&#32472;&#21046;&#20986;&#36825;&#20123;&#25968;&#25454;&#24341;&#21457;&#30340;&#20256;&#24863;&#22120;&#30456;&#20114;&#20316;&#29992;&#30340;&#22270;&#26223;&#20801;&#35768;
&lt;/p&gt;
&lt;p&gt;
Large-dimensional empirical data in science and engineering frequently has low-rank structure and can be represented as a combination of just a few eigenmodes. Because of this structure, we can use just a few spatially localized sensor measurements to reconstruct the full state of a complex system. The quality of this reconstruction, especially in the presence of sensor noise, depends significantly on the spatial configuration of the sensors. Multiple algorithms based on gappy interpolation and QR factorization have been proposed to optimize sensor placement. Here, instead of an algorithm that outputs a singular "optimal" sensor configuration, we take a thermodynamic view to compute the full landscape of sensor interactions induced by the training data. The landscape takes the form of the Ising model in statistical physics, and accounts for both the data variance captured at each sensor location and the crosstalk between sensors. Mapping out these data-induced sensor interactions allow
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;</title><link>http://arxiv.org/abs/2305.12766</link><description>&lt;p&gt;
&#23558; Emergent In-Context Learning &#35299;&#37322;&#20026;&#26680;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Explaining Emergent In-Context Learning as Kernel Regression. (arXiv:2305.12766v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20026;&#20160;&#20040;&#22312;&#39044;&#35757;&#32451;&#20043;&#21518;&#65292;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20551;&#35774;&#65292;&#35748;&#20026;LLMs&#22312;&#38754;&#23545;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#19968;&#22330;&#33539;&#24335;&#36716;&#21464;&#12290;&#19982;&#32463;&#20856;&#30340;&#39044;&#35757;&#32451;-&#24494;&#35843;&#36807;&#31243;&#30456;&#27604;&#65292;&#20026;&#20102;&#23558;LLMs&#29992;&#20110;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#65292;&#21482;&#38656;&#35201;&#25552;&#20379;&#19968;&#20123;&#31034;&#20363;&#65292;&#21363;&#19978;&#19979;&#25991;&#31034;&#20363;&#65292;&#32780;&#26080;&#38656;&#28155;&#21152;&#25110;&#26356;&#26032;&#29616;&#26377;&#30340;&#27169;&#22411;&#21442;&#25968;&#12290;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#38750;&#24120;&#26377;&#24847;&#24605;&#65292;&#20294;&#30446;&#21069;&#23578;&#19981;&#23436;&#20840;&#20102;&#35299;&#39044;&#35757;&#32451;LLMs&#22914;&#20309;&#33719;&#24471;&#36825;&#31181;&#33021;&#21147;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20551;&#35774;&#65292;&#21363;&#24403;&#38754;&#20020;&#19978;&#19979;&#25991;&#31034;&#20363;&#26102;&#65292;LLMs&#33021;&#22815;&#36890;&#36807;&#20869;&#37096;&#34920;&#31034;&#27169;&#25311;&#26680;&#22238;&#24402;&#65292;&#26469;&#30740;&#31350;&#20026;&#20309;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#22312;&#39044;&#35757;&#32451;&#36890;&#29992;&#35821;&#26009;&#24211;&#20043;&#21518;&#23454;&#29616;&#19978;&#19979;&#25991;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#19978;&#19979;&#25991;&#25552;&#31034;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#26680;&#22238;&#24402; $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$&#65292;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have initiated a paradigm shift in transfer learning. In contrast to the classic pretraining-then-finetuning procedure, in order to use LLMs for downstream prediction tasks, one only needs to provide a few demonstrations, known as in-context examples, without adding more or updating existing model parameters. This in-context learning (ICL) capability of LLMs is intriguing, and it is not yet fully understood how pretrained LLMs acquire such capabilities. In this paper, we investigate the reason why a transformer-based language model can accomplish in-context learning after pre-training on a general language corpus by proposing one hypothesis that LLMs can simulate kernel regression with internal representations when faced with in-context examples. More concretely, we first prove that Bayesian inference on in-context prompts can be asymptotically understood as kernel regression $\hat y = \sum_i y_i K(x, x_i)/\sum_i K(x, x_i)$ as the number of in-context demon
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#38754;&#20307;&#29702;&#35770;&#30340;&#20132;&#21449;&#39046;&#22495;&#12290;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#31561;&#20989;&#25968;&#20351;&#24471;&#19968;&#20123;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#33021;&#22815;&#36890;&#36807;&#22810;&#38754;&#20307;&#29702;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#24212;&#29992;&#32447;&#24615;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#23454;&#29616;&#32593;&#32476;&#20462;&#21098;&#12289;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.00241</link><description>&lt;p&gt;
&#24403;&#28145;&#24230;&#23398;&#20064;&#36935;&#35265;&#22810;&#38754;&#20307;&#29702;&#35770;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Deep Learning Meets Polyhedral Theory: A Survey. (arXiv:2305.00241v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#19982;&#22810;&#38754;&#20307;&#29702;&#35770;&#30340;&#20132;&#21449;&#39046;&#22495;&#12290;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#31561;&#20989;&#25968;&#20351;&#24471;&#19968;&#20123;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#33021;&#22815;&#36890;&#36807;&#22810;&#38754;&#20307;&#29702;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#24212;&#29992;&#32447;&#24615;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#26469;&#23454;&#29616;&#32593;&#32476;&#20462;&#21098;&#12289;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#20102;&#39044;&#27979;&#24314;&#27169;&#30340;&#20027;&#35201;&#26041;&#27861;&#65292;&#24471;&#30410;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#20219;&#21153;&#20013;&#30340;&#26174;&#33879;&#20934;&#30830;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#32467;&#26500;&#22238;&#24402;&#21040;&#20102;&#22522;&#20110;&#20998;&#27573;&#24120;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#20989;&#25968;&#30340;&#31616;&#21333;&#34920;&#31034;&#65292;&#20363;&#22914;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#65292;&#36825;&#31181;&#28608;&#27963;&#20989;&#25968;&#25104;&#20026;&#31070;&#32463;&#32593;&#32476;&#20013;&#26368;&#24120;&#29992;&#30340;&#31867;&#22411;&#12290;&#36825;&#20351;&#24471;&#26576;&#20123;&#31867;&#22411;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;&#22914;&#20856;&#22411;&#30340;&#20840;&#36830;&#25509;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#33021;&#22815;&#36890;&#36807;&#22810;&#38754;&#20307;&#29702;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#24182;&#24212;&#29992;&#32447;&#24615;&#35268;&#21010;&#65288;LP&#65289;&#21644;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;MILP&#65289;&#31561;&#26041;&#27861;&#29992;&#20110;&#21508;&#31181;&#30446;&#30340;&#12290;&#26412;&#25991;&#32508;&#36848;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#28044;&#29616;&#30340;&#20027;&#35201;&#20027;&#39064;&#65292;&#20026;&#26356;&#35814;&#32454;&#22320;&#20102;&#35299;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#24212;&#29992;&#25968;&#23398;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#22810;&#38754;&#20307;&#29702;&#35770;&#30340;&#22522;&#30784;&#30693;&#35782;&#20197;&#21450;&#23427;&#19982;&#28145;&#24230;&#23398;&#20064;&#30340;&#20851;&#31995;&#65292;&#24182;&#22238;&#39038;&#20102;&#35813;&#20027;&#39064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22312;&#32593;&#32476;&#20462;&#21098;&#12289;&#40065;&#26834;&#24615;&#20998;&#26512;&#21644;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31561;&#20219;&#21153;&#20013;&#20351;&#29992;LP&#21644;MILP&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit (ReLU), which became the most commonly used type of activation function in neural networks. That made certain types of network structure $\unicode{x2014}$such as the typical fully-connected feedforward neural network$\unicode{x2014}$ amenable to analysis through polyhedral theory and to the application of methodologies such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this paper, we survey the main topics emerging from this fast-paced area of work, which bring a fresh perspective to understanding neural networks in more detail as well as to app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#33258;&#36866;&#24212;&#31232;&#30095;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#22256;&#38590;&#37319;&#26679;&#38382;&#39064;&#30340;&#21152;&#36895;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#24182;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;MCMC&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.08724</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Piecewise Deterministic Markov Processes for Bayesian Neural Networks. (arXiv:2302.08724v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08724
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#30340;&#33258;&#36866;&#24212;&#31232;&#30095;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#22256;&#38590;&#37319;&#26679;&#38382;&#39064;&#30340;&#21152;&#36895;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#21487;&#34892;&#65292;&#24182;&#33021;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;MCMC&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#25512;&#29702;&#36890;&#24120;&#20381;&#36182;&#20110;&#21464;&#20998;&#25512;&#26029;&#22788;&#29702;&#65292;&#36825;&#35201;&#27714;&#36829;&#21453;&#20102;&#29420;&#31435;&#24615;&#21644;&#21518;&#39564;&#24418;&#24335;&#30340;&#20551;&#35774;&#12290;&#20256;&#32479;&#30340;MCMC&#26041;&#27861;&#36991;&#20813;&#20102;&#36825;&#20123;&#20551;&#35774;&#65292;&#20294;&#30001;&#20110;&#26080;&#27861;&#36866;&#24212;&#20284;&#28982;&#30340;&#23376;&#37319;&#26679;&#65292;&#23548;&#33268;&#35745;&#31639;&#37327;&#22686;&#21152;&#12290;&#26032;&#30340;&#20998;&#27573;&#30830;&#23450;&#24615;&#39532;&#23572;&#21487;&#22827;&#36807;&#31243;&#65288;PDMP&#65289;&#37319;&#26679;&#22120;&#20801;&#35768;&#23376;&#37319;&#26679;&#65292;&#20294;&#24341;&#20837;&#20102;&#27169;&#22411;&#29305;&#23450;&#30340;&#19981;&#22343;&#21248;&#27850;&#26494;&#36807;&#31243;&#65288;IPPs&#65289;&#65292;&#20174;&#20013;&#37319;&#26679;&#22256;&#38590;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#33258;&#36866;&#24212;&#31232;&#30095;&#26041;&#26696;&#65292;&#29992;&#20110;&#20174;&#36825;&#20123;IPPs&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#21152;&#36895;&#23558;PDMPs&#24212;&#29992;&#20110;BNNs&#25512;&#29702;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#25512;&#29702;&#22312;&#35745;&#31639;&#19978;&#26159;&#21487;&#34892;&#30340;&#65292;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12289;MCMC&#28151;&#21512;&#24615;&#33021;&#65292;&#24182;&#19982;&#20854;&#20182;&#36817;&#20284;&#25512;&#29702;&#26041;&#26696;&#30456;&#27604;&#65292;&#25552;&#20379;&#26356;&#26377;&#20449;&#24687;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#27979;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inference on modern Bayesian Neural Networks (BNNs) often relies on a variational inference treatment, imposing violated assumptions of independence and the form of the posterior. Traditional MCMC approaches avoid these assumptions at the cost of increased computation due to its incompatibility to subsampling of the likelihood. New Piecewise Deterministic Markov Process (PDMP) samplers permit subsampling, though introduce a model specific inhomogenous Poisson Process (IPPs) which is difficult to sample from. This work introduces a new generic and adaptive thinning scheme for sampling from these IPPs, and demonstrates how this approach can accelerate the application of PDMPs for inference in BNNs. Experimentation illustrates how inference with these methods is computationally feasible, can improve predictive accuracy, MCMC mixing performance, and provide informative uncertainty measurements when compared against other approximate inference schemes.
&lt;/p&gt;</description></item></channel></rss>