<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07563</link><description>&lt;p&gt;
&#23398;&#20064;&#31227;&#21160;&#25805;&#20316;&#30340;&#36890;&#29992;&#29305;&#24449;&#22330;
&lt;/p&gt;
&lt;p&gt;
Learning Generalizable Feature Fields for Mobile Manipulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07563
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#26469;&#25552;&#39640;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#25805;&#20316;&#20013;&#30340;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#26159;&#22914;&#20309;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#34920;&#31034;&#29289;&#20307;&#21644;&#22330;&#26223;&#65292;&#20351;&#24471;&#26426;&#22120;&#20154;&#21487;&#20197;&#21516;&#26102;&#29992;&#20110;&#22312;&#29615;&#22659;&#20013;&#23548;&#33322;&#21644;&#25805;&#20316;&#29289;&#20307;&#12290;&#26412;&#24037;&#20316;&#25552;&#20986;&#20102;GeFF&#65288;&#36890;&#29992;&#29305;&#24449;&#22330;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22330;&#26223;&#32423;&#30340;&#36890;&#29992;&#31070;&#32463;&#29305;&#24449;&#22330;&#65292;&#20316;&#20026;&#23548;&#33322;&#21644;&#25805;&#20316;&#30340;&#32479;&#19968;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#26102;&#25191;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#29983;&#25104;&#26032;&#35270;&#22270;&#21512;&#25104;&#35270;&#20026;&#19968;&#20010;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#28982;&#21518;&#36890;&#36807;CLIP&#29305;&#24449;&#25552;&#28860;&#23558;&#29983;&#25104;&#30340;&#20016;&#23500;&#22330;&#26223;&#20808;&#39564;&#19982;&#33258;&#28982;&#35821;&#35328;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07563v1 Announce Type: cross  Abstract: An open problem in mobile manipulation is how to represent objects and scenes in a unified manner, so that robots can use it both for navigating in the environment and manipulating objects. The latter requires capturing intricate geometry while understanding fine-grained semantics, whereas the former involves capturing the complexity inherit to an expansive physical scale. In this work, we present GeFF (Generalizable Feature Fields), a scene-level generalizable neural feature field that acts as a unified representation for both navigation and manipulation that performs in real-time. To do so, we treat generative novel view synthesis as a pre-training task, and then align the resulting rich scene priors with natural language via CLIP feature distillation. We demonstrate the effectiveness of this approach by deploying GeFF on a quadrupedal robot equipped with a manipulator. We evaluate GeFF's ability to generalize to open-set objects as 
&lt;/p&gt;</description></item><item><title>UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.00131</link><description>&lt;p&gt;
UniTS: &#26500;&#24314;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UniTS: Building a Unified Time Series Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00131
&lt;/p&gt;
&lt;p&gt;
UNITS&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#20102;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#25104;&#21151;&#25903;&#25345;&#22810;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;LLMs&#65292;&#27491;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#25105;&#20204;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#25552;&#31034;&#25110;&#24494;&#35843;&#23558;&#21333;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#32780;&#19981;&#26159;&#35757;&#32451;&#35768;&#22810;&#29305;&#23450;&#20219;&#21153;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#30784;&#27169;&#22411;&#36866;&#29992;&#20110;&#24207;&#21015;&#25968;&#25454;&#65292;&#20294;&#19981;&#36866;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#22266;&#26377;&#22810;&#26679;&#24615;&#21644;&#22810;&#39046;&#22495;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#39044;&#27979;&#12289;&#20998;&#31867;&#21644;&#20854;&#20182;&#31867;&#22411;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#35268;&#33539;&#20998;&#27495;&#65292;&#20197;&#21450;&#23545;&#20219;&#21153;&#19987;&#29992;&#27169;&#22411;&#30340;&#26126;&#26174;&#38656;&#27714;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;UNITS&#65292;&#19968;&#31181;&#25903;&#25345;&#36890;&#29992;&#20219;&#21153;&#35268;&#33539;&#30340;&#32479;&#19968;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#65292;&#21487;&#23481;&#32435;&#20998;&#31867;&#12289;&#39044;&#27979;&#12289;&#25554;&#34917;&#21644;&#24322;&#24120;&#26816;&#27979;&#20219;&#21153;&#12290;&#36825;&#26159;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#32593;&#32476;&#39592;&#24178;&#23454;&#29616;&#30340;&#65292;&#35813;&#39592;&#24178;&#32467;&#21512;&#20102;&#24207;&#21015;&#21644;&#21464;&#37327;&#27880;&#24847;&#21147;&#20197;&#21450;&#21160;&#24577;&#32447;&#24615;&#31639;&#23376;&#65292;&#24182;&#20316;&#20026;&#32479;&#19968;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;38&#20010;&#22810;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#65292;UNITS&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00131v1 Announce Type: cross  Abstract: Foundation models, especially LLMs, are profoundly transforming deep learning. Instead of training many task-specific models, we can adapt a single pretrained model to many tasks via fewshot prompting or fine-tuning. However, current foundation models apply to sequence data but not to time series, which present unique challenges due to the inherent diverse and multidomain time series datasets, diverging task specifications across forecasting, classification and other types of tasks, and the apparent need for task-specialized models. We developed UNITS, a unified time series model that supports a universal task specification, accommodating classification, forecasting, imputation, and anomaly detection tasks. This is achieved through a novel unified network backbone, which incorporates sequence and variable attention along with a dynamic linear operator and is trained as a unified model. Across 38 multi-domain datasets, UNITS demonstrate
&lt;/p&gt;</description></item><item><title>SGCL&#27169;&#22411;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#24418;&#25104;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;</title><link>https://arxiv.org/abs/2402.15270</link><description>&lt;p&gt;
&#36890;&#36807;&#26080;&#32541;&#25509;&#36817;&#24230;&#25972;&#21512;&#30340;&#24179;&#28369;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Smoothed Graph Contrastive Learning via Seamless Proximity Integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.15270
&lt;/p&gt;
&lt;p&gt;
SGCL&#27169;&#22411;&#36890;&#36807;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#20174;&#32780;&#24418;&#25104;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#36890;&#36807;&#23558;&#33410;&#28857;&#23545;&#24402;&#31867;&#20026;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#26469;&#23545;&#40784;&#33410;&#28857;&#34920;&#31034;&#65292;&#20854;&#36873;&#25321;&#36807;&#31243;&#36890;&#24120;&#20381;&#36182;&#20110;&#22312;&#20004;&#20010;&#22686;&#24378;&#22270;&#20013;&#24314;&#31435;&#23545;&#24212;&#20851;&#31995;&#12290;&#20256;&#32479;&#30340;GCL&#26041;&#27861;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#32479;&#19968;&#22320;&#34701;&#20837;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#36127;&#33410;&#28857;&#34987;&#24179;&#31561;&#23545;&#24453;&#65292;&#32780;&#19981;&#32771;&#34385;&#23427;&#20204;&#19982;&#30495;&#27491;&#27491;&#26679;&#26412;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24179;&#28369;&#22270;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65288;SGCL&#65289;&#65292;&#21033;&#29992;&#22686;&#24378;&#22270;&#30340;&#20960;&#20309;&#32467;&#26500;&#26469;&#22312;&#23545;&#27604;&#25439;&#22833;&#20013;&#27880;&#20837;&#19982;&#27491;&#36127;&#26679;&#26412;&#30456;&#20851;&#30340;&#25509;&#36817;&#24230;&#20449;&#24687;&#65292;&#20174;&#32780;&#26174;&#33879;&#35268;&#33539;&#21270;&#23398;&#20064;&#36807;&#31243;&#12290;&#25152;&#25552;&#20986;&#30340;SGCL&#36890;&#36807;&#25972;&#21512;&#19977;&#31181;&#19981;&#21516;&#30340;&#24179;&#28369;&#25216;&#26415;&#35843;&#25972;&#23545;&#27604;&#25439;&#22833;&#20013;&#33410;&#28857;&#23545;&#30340;&#24809;&#32602;&#65292;&#24418;&#25104;&#20102;&#20855;&#26377;&#25509;&#36817;&#24230;&#24863;&#30693;&#30340;&#27491;&#26679;&#26412;&#21644;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.15270v1 Announce Type: cross  Abstract: Graph contrastive learning (GCL) aligns node representations by classifying node pairs into positives and negatives using a selection process that typically relies on establishing correspondences within two augmented graphs. The conventional GCL approaches incorporate negative samples uniformly in the contrastive loss, resulting in the equal treatment negative nodes, regardless of their proximity to the true positive. In this paper, we present a Smoothed Graph Contrastive Learning model (SGCL), which leverages the geometric structure of augmented graphs to inject proximity information associated with positive/negative pairs in the contrastive loss, thus significantly regularizing the learning process. The proposed SGCL adjusts the penalties associated with node pairs in the contrastive loss by incorporating three distinct smoothing techniques that result in proximity aware positives and negatives. To enhance scalability for large-scale
&lt;/p&gt;</description></item><item><title>BioNeRF&#26159;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23384;&#20648;&#33021;&#21147;&#21644;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.07310</link><description>&lt;p&gt;
BioNeRF: &#29992;&#20110;&#35270;&#22270;&#21512;&#25104;&#30340;&#29983;&#29289;&#21512;&#29702;&#31070;&#32463;&#36752;&#23556;&#22330;
&lt;/p&gt;
&lt;p&gt;
BioNeRF: Biologically Plausible Neural Radiance Fields for View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07310
&lt;/p&gt;
&lt;p&gt;
BioNeRF&#26159;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#23427;&#23454;&#29616;&#20102;&#19968;&#31181;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#25552;&#39640;&#20102;&#23384;&#20648;&#33021;&#21147;&#21644;&#25552;&#21462;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BioNeRF&#65292;&#19968;&#31181;&#29983;&#29289;&#21512;&#29702;&#30340;&#26550;&#26500;&#65292;&#23427;&#36890;&#36807;&#36752;&#23556;&#22330;&#23545;&#22330;&#26223;&#36827;&#34892;3D&#34920;&#31034;&#24182;&#21512;&#25104;&#26032;&#35270;&#22270;&#12290;&#30001;&#20110;NeRF&#20381;&#36182;&#20110;&#32593;&#32476;&#26435;&#37325;&#26469;&#23384;&#20648;&#22330;&#26223;&#30340;&#19977;&#32500;&#34920;&#31034;&#65292;BioNeRF&#23454;&#29616;&#20102;&#19968;&#31181;&#21463;&#35748;&#30693;&#21551;&#21457;&#30340;&#26426;&#21046;&#65292;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#36755;&#20837;&#34701;&#21512;&#25104;&#20869;&#23384;&#31867;&#20284;&#30340;&#32467;&#26500;&#65292;&#25552;&#39640;&#23384;&#20648;&#33021;&#21147;&#24182;&#25552;&#21462;&#26356;&#22810;&#20869;&#22312;&#21644;&#30456;&#20851;&#20449;&#24687;&#12290;BioNeRF&#36824;&#27169;&#20223;&#20102;&#37329;&#23383;&#22612;&#32454;&#32990;&#20013;&#20851;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#19968;&#31181;&#34892;&#20026;&#65292;&#20854;&#20013;&#20869;&#23384;&#20316;&#20026;&#19978;&#19979;&#25991;&#25552;&#20379;&#65292;&#24182;&#19982;&#20004;&#20010;&#21518;&#32493;&#31070;&#32463;&#27169;&#22411;&#30340;&#36755;&#20837;&#30456;&#32467;&#21512;&#65292;&#19968;&#20010;&#36127;&#36131;&#29983;&#25104;&#23481;&#31215;&#23494;&#24230;&#65292;&#21478;&#19968;&#20010;&#36127;&#36131;&#28210;&#26579;&#22330;&#26223;&#30340;&#39068;&#33394;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BioNeRF&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#65288;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#21644;&#21512;&#25104;&#25968;&#25454;&#65289;&#19978;&#36229;&#36807;&#20102;&#20197;&#20154;&#30340;&#24863;&#30693;&#20026;&#22522;&#30784;&#30340;&#36136;&#37327;&#24230;&#37327;&#30340;&#26368;&#26032;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents BioNeRF, a biologically plausible architecture that models scenes in a 3D representation and synthesizes new views through radiance fields. Since NeRF relies on the network weights to store the scene's 3-dimensional representation, BioNeRF implements a cognitive-inspired mechanism that fuses inputs from multiple sources into a memory-like structure, improving the storing capacity and extracting more intrinsic and correlated information. BioNeRF also mimics a behavior observed in pyramidal cells concerning contextual information, in which the memory is provided as the context and combined with the inputs of two subsequent neural models, one responsible for producing the volumetric densities and the other the colors used to render the scene. Experimental results show that BioNeRF outperforms state-of-the-art results concerning a quality measure that encodes human perception in two datasets: real-world images and synthetic data.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.06388</link><description>&lt;p&gt;
&#20851;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#21450;&#20854;&#22312;&#20462;&#25913;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#19978;&#30340;&#31574;&#30053;&#26799;&#24230;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Convergence Rate of the Stochastic Gradient Descent (SGD) and application to a modified policy gradient for the Multi Armed Bandit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06388
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#25353;&#29031;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#24212;&#29992;&#20110;&#20462;&#25913;&#30340;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21253;&#21547;&#30340;&#35777;&#26126;&#65292;&#35777;&#26126;&#20102;&#24403;&#23398;&#20064;&#36895;&#29575;&#36981;&#24490;&#36870;&#26102;&#38388;&#34928;&#20943;&#35268;&#21017;&#26102;&#65292;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#25910;&#25947;&#36895;&#24230;&#65307;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#24212;&#29992;&#20110;&#24102;&#26377;L2&#27491;&#21017;&#21270;&#30340;&#20462;&#25913;&#30340;&#31574;&#30053;&#26799;&#24230;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#30340;&#25910;&#25947;&#24615;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a self-contained proof of the convergence rate of the Stochastic Gradient Descent (SGD) when the learning rate follows an inverse time decays schedule; we next apply the results to the convergence of a modified form of policy gradient Multi-Armed Bandit (MAB) with $L2$ regularization.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.15122</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#32423;&#23545;&#31216;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
A Multi-Grained Symmetric Differential Equation Model for Learning Protein-Ligand Binding Dynamics. (arXiv:2401.15122v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15122
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#27169;&#25311;&#24182;&#26377;&#25928;&#27169;&#25311;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#30340;NeuralMD&#26041;&#27861;&#65292;&#37319;&#29992;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#24314;&#27169;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33647;&#29289;&#21457;&#29616;&#20013;&#65292;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#30340;&#20998;&#23376;&#21160;&#21147;&#23398;&#65288;MD&#65289;&#27169;&#25311;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#39044;&#27979;&#32467;&#21512;&#20146;&#21644;&#21147;&#65292;&#20272;&#35745;&#36816;&#36755;&#24615;&#33021;&#21644;&#25506;&#32034;&#21475;&#34955;&#20301;&#28857;&#12290;&#36890;&#36807;&#25913;&#36827;&#25968;&#20540;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#22686;&#24378;MD&#27169;&#25311;&#30340;&#25928;&#29575;&#24050;&#32463;&#26377;&#20102;&#24456;&#38271;&#30340;&#21382;&#21490;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#20363;&#22914;&#20934;&#30830;&#24314;&#27169;&#25193;&#23637;&#26102;&#38388;&#23610;&#24230;&#30340;&#27169;&#25311;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;NeuralMD&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#20419;&#36827;&#25968;&#20540;MD&#24182;&#25552;&#20379;&#20934;&#30830;&#30340;&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#21160;&#21147;&#23398;&#27169;&#25311;&#30340;ML&#36741;&#21161;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21512;&#29702;&#30340;&#26041;&#27861;&#65292;&#23558;&#19968;&#31181;&#26032;&#30340;&#29289;&#29702;&#20449;&#24687;&#22810;&#32423;&#23545;&#31216;&#26694;&#26550;&#32435;&#20837;&#27169;&#22411;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#65288;1&#65289;&#19968;&#20010;&#20351;&#29992;&#21521;&#37327;&#26694;&#26550;&#28385;&#36275;&#32676;&#23545;&#31216;&#24615;&#24182;&#25429;&#33719;&#22810;&#32423;&#34507;&#30333;&#36136;-&#37197;&#20307;&#30456;&#20114;&#20316;&#29992;&#30340;BindingNet&#27169;&#22411;&#65292;&#20197;&#21450;&#65288;2&#65289;&#19968;&#20010;&#22686;&#24378;&#30340;&#31070;&#32463;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#65292;&#23398;&#20064;&#36712;&#36857;&#30340;&#28436;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In drug discovery, molecular dynamics (MD) simulation for protein-ligand binding provides a powerful tool for predicting binding affinities, estimating transport properties, and exploring pocket sites. There has been a long history of improving the efficiency of MD simulations through better numerical methods and, more recently, by augmenting them with machine learning (ML) methods. Yet, challenges remain, such as accurate modeling of extended-timescale simulations. To address this issue, we propose NeuralMD, the first ML surrogate that can facilitate numerical MD and provide accurate simulations of protein-ligand binding dynamics. We propose a principled approach that incorporates a novel physics-informed multi-grained group symmetric framework. Specifically, we propose (1) a BindingNet model that satisfies group symmetry using vector frames and captures the multi-level protein-ligand interactions, and (2) an augmented neural differential equation solver that learns the trajectory und
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#28789;&#24039;&#24615;&#23547;&#25214;&#21644;&#25805;&#20316;&#29289;&#20307;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#29289;&#20307;&#25628;&#32034;&#21644;&#25805;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#20063;&#33021;&#22815;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.12496</link><description>&lt;p&gt;
DexTouch&#65306;&#23398;&#20064;&#20351;&#29992;&#35302;&#35273;&#28789;&#24039;&#24615;&#23547;&#25214;&#21644;&#25805;&#20316;&#29289;&#20307;
&lt;/p&gt;
&lt;p&gt;
DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity. (arXiv:2401.12496v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#28789;&#24039;&#24615;&#23547;&#25214;&#21644;&#25805;&#20316;&#29289;&#20307;&#30340;&#22810;&#25351;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#36890;&#36807;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#29289;&#20307;&#25628;&#32034;&#21644;&#25805;&#20316;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21363;&#20351;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#20154;&#20063;&#33021;&#22815;&#20855;&#22791;&#31867;&#20284;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35302;&#35273;&#33021;&#21147;&#23545;&#20110;&#29087;&#32451;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#23427;&#33021;&#22815;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#21644;&#25805;&#20316;&#29289;&#20307;&#12290;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#23558;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#25351;&#26426;&#22120;&#20154;&#31995;&#32479;&#65292;&#26088;&#22312;&#21033;&#29992;&#35302;&#35273;&#24863;&#21463;&#22120;&#25628;&#32034;&#21644;&#25805;&#20316;&#29289;&#20307;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#12290;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#26469;&#25628;&#32034;&#38543;&#26426;&#25918;&#32622;&#30340;&#30446;&#26631;&#29289;&#20307;&#65292;&#24182;&#36827;&#34892;&#27169;&#25311;&#26085;&#24120;&#20219;&#21153;&#30340;&#29289;&#20307;&#25805;&#20316;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#36171;&#20104;&#26426;&#22120;&#20154;&#31867;&#20284;&#20154;&#31867;&#30340;&#35302;&#35273;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#22312;&#26426;&#22120;&#20154;&#25163;&#30340;&#19968;&#20391;&#23454;&#29616;&#20102;&#20108;&#20540;&#35302;&#35273;&#20256;&#24863;&#22120;&#65292;&#20197;&#23613;&#37327;&#20943;&#23569;&#27169;&#25311;&#19982;&#30495;&#23454;&#29615;&#22659;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#20013;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#31574;&#30053;&#65292;&#24182;&#23558;&#35757;&#32451;&#22909;&#30340;&#31574;&#30053;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20351;&#29992;&#35302;&#35273;&#20256;&#24863;&#22120;&#36827;&#34892;&#29289;&#20307;&#25628;&#32034;&#21644;&#25805;&#20316;&#26159;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20381;&#36182;&#20110;&#35270;&#35273;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
The sense of touch is an essential ability for skillfully performing a variety of tasks, providing the capacity to search and manipulate objects without relying on visual information. Extensive research has been conducted over time to apply these human tactile abilities to robots. In this paper, we introduce a multi-finger robot system designed to search for and manipulate objects using the sense of touch without relying on visual information. Randomly located target objects are searched using tactile sensors, and the objects are manipulated for tasks that mimic daily-life. The objective of the study is to endow robots with human-like tactile capabilities. To achieve this, binary tactile sensors are implemented on one side of the robot hand to minimize the Sim2Real gap. Training the policy through reinforcement learning in simulation and transferring the trained policy to the real environment, we demonstrate that object search and manipulation using tactile sensors is possible even in 
&lt;/p&gt;</description></item><item><title>Powerformer&#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#12290;&#23427;&#36890;&#36807;&#24320;&#21457;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#20379;&#26356;&#21152;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2401.02771</link><description>&lt;p&gt;
Powerformer&#65306;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#29992;&#20110;&#30005;&#21147;&#27969;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Powerformer: A Section-adaptive Transformer for Power Flow Adjustment. (arXiv:2401.02771v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02771
&lt;/p&gt;
&lt;p&gt;
Powerformer&#26159;&#19968;&#31181;&#36866;&#24212;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#29992;&#20110;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#12290;&#23427;&#36890;&#36807;&#24320;&#21457;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#24341;&#20837;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#20379;&#26356;&#21152;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#22312;&#19977;&#20010;&#19981;&#21516;&#30340;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#20026;&#23398;&#20064;&#31283;&#20581;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#34920;&#31034;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#26088;&#22312;&#20248;&#21270;&#36328;&#19981;&#21516;&#20256;&#36755;&#21306;&#27573;&#30340;&#30005;&#21147;&#35843;&#24230;&#20197;&#36827;&#34892;&#30005;&#21147;&#27969;&#35843;&#25972;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;Powerformer&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#19987;&#29992;&#30340;&#21306;&#27573;&#33258;&#36866;&#24212;&#27880;&#24847;&#26426;&#21046;&#65292;&#19982;&#20256;&#32479;&#21464;&#21387;&#22120;&#20013;&#20351;&#29992;&#30340;&#33258;&#27880;&#24847;&#20998;&#31163;&#24320;&#26469;&#12290;&#35813;&#26426;&#21046;&#26377;&#25928;&#22320;&#23558;&#30005;&#21147;&#31995;&#32479;&#29366;&#24577;&#19982;&#20256;&#36755;&#21306;&#27573;&#20449;&#24687;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#26377;&#21161;&#20110;&#24320;&#21457;&#31283;&#20581;&#30340;&#29366;&#24577;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32771;&#34385;&#30005;&#21147;&#31995;&#32479;&#30340;&#22270;&#25299;&#25169;&#21644;&#27597;&#32447;&#33410;&#28857;&#30340;&#30005;&#27668;&#23646;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#23450;&#21046;&#31574;&#30053;&#26469;&#36827;&#19968;&#27493;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#20256;&#25773;&#21644;&#22810;&#22240;&#32032;&#27880;&#24847;&#26426;&#21046;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#30005;&#21147;&#31995;&#32479;&#22330;&#26223;&#65288;&#21253;&#25324;IEEE 118&#33410;&#28857;&#31995;&#32479;&#12289;&#20013;&#22269;&#23454;&#38469;300&#33410;&#28857;&#31995;&#32479;&#21644;&#19968;&#20010;&#22823;&#22411;&#31995;&#32479;&#65289;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel transformer architecture tailored for learning robust power system state representations, which strives to optimize power dispatch for the power flow adjustment across different transmission sections. Specifically, our proposed approach, named Powerformer, develops a dedicated section-adaptive attention mechanism, separating itself from the self-attention used in conventional transformers. This mechanism effectively integrates power system states with transmission section information, which facilitates the development of robust state representations. Furthermore, by considering the graph topology of power system and the electrical attributes of bus nodes, we introduce two customized strategies to further enhance the expressiveness: graph neural network propagation and multi-factor attention mechanism. Extensive evaluations are conducted on three power system scenarios, including the IEEE 118-bus system, a realistic 300-bus system in China, and a large-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;</title><link>http://arxiv.org/abs/2310.13018</link><description>&lt;p&gt;
&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#36798;&#25104;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Getting aligned on representational alignment. (arXiv:2310.13018v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#30340;&#34920;&#31034;&#19968;&#33268;&#24615;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31995;&#32479;&#20043;&#38388;&#30340;&#34920;&#31034;&#26159;&#21542;&#19968;&#33268;&#20197;&#21450;&#22914;&#20309;&#35843;&#25972;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#20854;&#20182;&#31995;&#32479;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20316;&#20026;&#20849;&#21516;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#21644;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#31995;&#32479;&#26500;&#24314;&#21487;&#20197;&#29992;&#26469;&#36827;&#34892;&#20998;&#31867;&#12289;&#25512;&#29702;&#12289;&#35268;&#21010;&#12289;&#23548;&#33322;&#21644;&#20915;&#31574;&#30340;&#19990;&#30028;&#34920;&#31034;&#12290;&#36825;&#20123;&#22810;&#26679;&#21270;&#31995;&#32479;&#25152;&#26500;&#24314;&#30340;&#34920;&#31034;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#26159;&#19968;&#33268;&#30340;&#65311;&#21363;&#20351;&#34920;&#31034;&#19981;&#21516;&#65292;&#26159;&#21542;&#20173;&#28982;&#33021;&#22815;&#23548;&#33268;&#30456;&#21516;&#30340;&#34892;&#20026;&#65311;&#31995;&#32479;&#22914;&#20309;&#20462;&#25913;&#23427;&#20204;&#30340;&#34920;&#31034;&#20197;&#26356;&#22909;&#22320;&#21305;&#37197;&#21478;&#19968;&#20010;&#31995;&#32479;&#30340;&#34920;&#31034;&#65311;&#36825;&#20123;&#20851;&#20110;&#34920;&#31034;&#19968;&#33268;&#24615;&#30740;&#31350;&#30340;&#38382;&#39064;&#26159;&#24403;&#20195;&#35748;&#30693;&#31185;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20123;&#26368;&#27963;&#36291;&#30340;&#30740;&#31350;&#39046;&#22495;&#30340;&#26680;&#24515;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23545;&#20110;&#23545;&#34920;&#31034;&#19968;&#33268;&#24615;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#31038;&#21306;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#26377;&#38480;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#22312;&#19968;&#20010;&#39046;&#22495;&#30340;&#36827;&#23637;&#26368;&#32456;&#20250;&#22312;&#21478;&#19968;&#20010;&#39046;&#22495;&#29420;&#31435;&#22320;&#37325;&#26032;&#21457;&#29616;&#65292;&#32780;&#26356;&#24191;&#27867;&#30340;&#39046;&#22495;&#38388;&#20132;&#27969;&#23558;&#26159;&#26377;&#21033;&#30340;&#12290;&#20026;&#20102;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#30340;&#20132;&#27969;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#20316;&#20026;&#19968;&#31181;&#20849;&#21516;&#30340;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. To what extent do the representations formed by these diverse systems agree? Can diverging representations still lead to the same behaviors? And how can systems modify their representations to better match those of another system? These questions pertaining to the study of \textbf{\emph{representational alignment}} are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. Unfortunately, there is limited knowledge-transfer between research communities interested in representational alignment, and much of the progress in one field ends up being rediscovered independently in another, when greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#27969;&#20013;&#30340;&#20004;&#20010;&#26679;&#26412;&#26469;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#65292;&#21435;&#38500;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39069;&#22806;&#26435;&#37325;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.01170</link><description>&lt;p&gt;
&#30452;&#25509;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Direct Gradient Temporal Difference Learning. (arXiv:2308.01170v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#27969;&#20013;&#30340;&#20004;&#20010;&#26679;&#26412;&#26469;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#65292;&#21435;&#38500;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39069;&#22806;&#26435;&#37325;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#23398;&#20064;&#20351;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#33021;&#22815;&#21453;&#20107;&#23454;&#22320;&#25512;&#29702;&#26410;&#25191;&#34892;&#30340;&#31574;&#30053;&#65292;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#24605;&#24819;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#36825;&#20004;&#20010;&#22312;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#22240;&#32032;&#32467;&#21512;&#26102;&#65292;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;&#36825;&#23601;&#26159;&#33261;&#21517;&#26157;&#33879;&#30340;&#33268;&#21629;&#19977;&#20803;&#32452;&#12290;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#65288;GTD&#65289;&#26159;&#35299;&#20915;&#36825;&#20010;&#33268;&#21629;&#19977;&#20803;&#32452;&#30340;&#19968;&#31181;&#24378;&#22823;&#24037;&#20855;&#12290;&#23427;&#30340;&#25104;&#21151;&#26159;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#22797;&#21046;&#25110;Fenchel&#23545;&#20598;&#38388;&#25509;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#32780;&#23454;&#29616;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#65292;&#21482;&#38656;&#22312;&#36880;&#28176;&#22686;&#21152;&#30340;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#27969;&#20013;&#20351;&#29992;&#20004;&#20010;&#26679;&#26412;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#19982;GTD&#19968;&#26679;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#20294;&#25682;&#24323;&#20102;GTD&#30340;&#39069;&#22806;&#26435;&#37325;&#12290;&#25105;&#20204;&#25152;&#20184;&#20986;&#30340;&#21807;&#19968;&#20195;&#20215;&#26159;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#20869;&#23384;&#21576;&#23545;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28176;&#36817;&#21644;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#65292;&#20854;&#20013;&#25910;&#25947;&#24615;&#21487;&#20197;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve the deadly triad. Its success results from solving a doubling sampling issue indirectly with weight duplication or Fenchel duality. In this paper, we instead propose a direct method to solve the double sampling issue by simply using two samples in a Markovian data stream with an increasing gap. The resulting algorithm is as computationally efficient as GTD but gets rid of GTD's extra weights. The only price we pay is a logarithmically increasing memory as time progresses. We provide both asymptotic and finite sample analysis, where the conver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.13549</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Multimodal Large Language Models. (arXiv:2306.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36861;&#36394;&#21644;&#24635;&#32467;&#20102;&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#31561;&#24212;&#29992;&#65292;&#25351;&#20986;&#20102;&#29616;&#26377;&#25361;&#25112;&#21644;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#30740;&#31350;&#28909;&#28857;&#65292;&#20351;&#29992;&#24378;&#22823;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#22823;&#33041;&#25191;&#34892;&#22810;&#27169;&#24577;&#20219;&#21153;&#12290;MLLM &#30340;&#24778;&#20154;&#33021;&#21147;&#65292;&#22914;&#22522;&#20110;&#22270;&#20687;&#32534;&#20889;&#25925;&#20107;&#21644;&#26080;OCR&#25968;&#23398;&#25512;&#29702;&#31561;&#65292;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#24456;&#23569;&#35265;&#65292;&#34920;&#26126;&#20102;&#36890;&#21521;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#36335;&#24452;&#12290;&#26412;&#25991;&#26088;&#22312;&#36861;&#36394;&#21644;&#24635;&#32467; MLLM &#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; MLLM &#30340;&#26500;&#25104;&#65292;&#27010;&#36848;&#20102;&#30456;&#20851;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;&#20851;&#38190;&#25216;&#26415;&#21644;&#24212;&#29992;&#65292;&#21253;&#25324;&#22810;&#27169;&#24577;&#25351;&#20196;&#35843;&#25972;&#65288;M-IT&#65289;&#12289;&#22810;&#27169;&#24577;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;M-ICL&#65289;&#12289;&#22810;&#27169;&#24577;&#24605;&#32500;&#38142;&#65288;M-CoT&#65289;&#21644;LLM&#36741;&#21161;&#35270;&#35273;&#25512;&#29702;&#65288;LAVR&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29616;&#26377;&#30340;&#25361;&#25112;&#65292;&#24182;&#25351;&#20986;&#20102;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#37492;&#20110; MLLM &#26102;&#20195;&#25165;&#21018;&#21018;&#24320;&#22987;&#65292;&#25105;&#20204;&#20250;&#19981;&#26029;&#26356;&#26032;&#36825;&#20010;&#32508;&#36848;&#65292;&#24182;&#24076;&#26395;&#33021;&#28608;&#21457;&#26356;&#22810;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal Large Language Model (MLLM) recently has been a new rising research hotspot, which uses powerful Large Language Models (LLMs) as a brain to perform multimodal tasks. The surprising emergent capabilities of MLLM, such as writing stories based on images and OCR-free math reasoning, are rare in traditional methods, suggesting a potential path to artificial general intelligence. In this paper, we aim to trace and summarize the recent progress of MLLM. First of all, we present the formulation of MLLM and delineate its related concepts. Then, we discuss the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimodal In-Context Learning (M-ICL), Multimodal Chain of Thought (M-CoT), and LLM-Aided Visual Reasoning (LAVR). Finally, we discuss existing challenges and point out promising research directions. In light of the fact that the era of MLLM has only just begun, we will keep updating this survey and hope it can inspire more research. An associated
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#23485;&#24230;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#65292;&#21457;&#29616;&#24403;&#19988;&#20165;&#24403;&#28608;&#27963;&#20989;&#25968;&#26082;&#19981;&#26159;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159;&#21453;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159; $\mathbb{R}$-&#20223;&#23556;&#30340;&#26102;&#65292;&#28145;&#31364;&#30340;&#22797;&#20540;&#32593;&#32476;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36275;&#22815;&#30340;&#23485;&#24230;&#20381;&#36182;&#20110;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#19968;&#31867;&#21487;&#20801;&#35768;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23485;&#24230;&#20026; $n+m+4$ &#26159;&#36275;&#22815;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.16910</link><description>&lt;p&gt;
&#24102;&#26377;&#22797;&#20540;&#30340;&#28145;&#31364;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
Universal approximation with complex-valued deep narrow neural networks. (arXiv:2305.16910v1 [math.FA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16910
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#23485;&#24230;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#65292;&#21457;&#29616;&#24403;&#19988;&#20165;&#24403;&#28608;&#27963;&#20989;&#25968;&#26082;&#19981;&#26159;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159;&#21453;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159; $\mathbb{R}$-&#20223;&#23556;&#30340;&#26102;&#65292;&#28145;&#31364;&#30340;&#22797;&#20540;&#32593;&#32476;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36275;&#22815;&#30340;&#23485;&#24230;&#20381;&#36182;&#20110;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23545;&#20110;&#19968;&#31867;&#21487;&#20801;&#35768;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23485;&#24230;&#20026; $n+m+4$ &#26159;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#26377;&#30028;&#23485;&#24230;&#21644;&#20219;&#24847;&#28145;&#24230;&#30340;&#22797;&#20540;&#31070;&#32463;&#32593;&#32476;&#30340;&#26222;&#36866;&#24615;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#37027;&#20123;&#28608;&#27963;&#20989;&#25968; $\varrho:\mathbb{CC}\to \mathbb{C}$ &#30340;&#23436;&#25972;&#25551;&#36848;&#65292;&#36825;&#20123;&#20989;&#25968;&#20855;&#26377;&#36825;&#26679;&#19968;&#20010;&#23646;&#24615;&#65306;&#23427;&#20204;&#20851;&#32852;&#30340;&#32593;&#32476;&#26159;&#26222;&#36866;&#30340;&#65292;&#21363;&#33021;&#22815;&#22312;&#32039;&#33268;&#22495;&#19978;&#36924;&#36817;&#36830;&#32493;&#20989;&#25968;&#33267;&#20219;&#24847;&#31934;&#24230;&#12290;&#20934;&#30830;&#22320;&#35828;&#65292;&#25105;&#20204;&#34920;&#26126;&#20102;&#24403;&#19988;&#20165;&#24403;&#23427;&#20204;&#30340;&#28608;&#27963;&#20989;&#25968;&#26082;&#19981;&#26159;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159;&#21453;&#20840;&#32431;&#30340;&#65292;&#20063;&#19981;&#26159; $\mathbb{R}$-&#20223;&#23556;&#30340;&#65292;&#28145;&#31364;&#30340;&#22797;&#20540;&#32593;&#32476;&#26159;&#26222;&#36866;&#30340;&#12290;&#36825;&#26159;&#19968;&#20010;&#27604;&#23485;&#24230;&#20219;&#24847;&#12289;&#28145;&#24230;&#22266;&#23450;&#30340;&#23545;&#20598;&#35774;&#32622;&#20013;&#26356;&#22823;&#30340;&#20989;&#25968;&#31867;&#12290;&#19982;&#23454;&#20540;&#24773;&#20917;&#19981;&#21516;&#30340;&#26159;&#65292;&#36275;&#22815;&#30340;&#23485;&#24230;&#20381;&#36182;&#20110;&#32771;&#34385;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#23485;&#24230;&#20026; $2n+2m+5$ &#24635;&#26159;&#36275;&#22815;&#30340;&#65292;&#24182;&#19988;&#36890;&#24120; $\max\{2n,2m\}$ &#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#19968;&#31867;&#21487;&#20801;&#35768;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#23485;&#24230;&#20026; $n+m+4$ &#26159;&#36275;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the universality of complex-valued neural networks with bounded widths and arbitrary depths. Under mild assumptions, we give a full description of those activation functions $\varrho:\mathbb{CC}\to \mathbb{C}$ that have the property that their associated networks are universal, i.e., are capable of approximating continuous functions to arbitrary accuracy on compact domains. Precisely, we show that deep narrow complex-valued networks are universal if and only if their activation function is neither holomorphic, nor antiholomorphic, nor $\mathbb{R}$-affine. This is a much larger class of functions than in the dual setting of arbitrary width and fixed depth. Unlike in the real case, the sufficient width differs significantly depending on the considered activation function. We show that a width of $2n+2m+5$ is always sufficient and that in general a width of $\max\{2n,2m\}$ is necessary. We prove, however, that a width of $n+m+4$ suffices for a rich subclass of the admissible acti
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24739;&#32773;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25552;&#21462;&#30340;&#24515;&#30005;&#22270;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#23545;&#29992;&#25143;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.06529</link><description>&lt;p&gt;
&#20805;&#20998;&#21457;&#25381;&#24515;&#30005;&#22270;&#30340;&#33021;&#21147;&#65306;&#19968;&#31181;&#22312;&#20855;&#26377;&#24515;&#30005;&#22270;&#20449;&#21495;&#30340;&#21307;&#30103;&#31995;&#32479;&#20013;&#36827;&#34892;&#24739;&#32773;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Power of Electrocardiograms: A novel approach for Patient Identification in Healthcare Systems with ECG Signals. (arXiv:2302.06529v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24739;&#32773;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23545;&#25552;&#21462;&#30340;&#24515;&#30005;&#22270;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#32508;&#21512;&#32771;&#34385;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#23545;&#29992;&#25143;&#35782;&#21035;&#30340;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#20854;&#20934;&#30830;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20108;&#21313;&#24180;&#20013;&#65292;&#22823;&#37327;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#23454;&#20102;&#21033;&#29992;&#24515;&#33039;&#20449;&#21495;&#20316;&#20026;&#29983;&#29289;&#35782;&#21035;&#27169;&#24335;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#24515;&#30005;&#22270;&#20449;&#21495;&#36827;&#34892;&#24739;&#32773;&#35782;&#21035;&#30340;&#26032;&#26041;&#27861;&#12290;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22522;&#20110;&#20174;&#24515;&#30005;&#22270;&#20449;&#21495;&#20013;&#25552;&#21462;&#30340;&#22270;&#20687;&#23545;&#29992;&#25143;&#36827;&#34892;&#20998;&#31867;&#12290;&#35813;&#35782;&#21035;&#31995;&#32479;&#22312;&#22810;&#20010;&#25968;&#25454;&#24211;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20840;&#38754;&#20102;&#35299;&#20854;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#28508;&#21147;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#26222;&#36890;&#29992;&#25143;&#35782;&#21035;&#20013;&#24448;&#24448;&#24573;&#30053;&#20102;&#24515;&#34880;&#31649;&#30142;&#30149;&#30340;&#24433;&#21709;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#24739;&#32773;&#30340;&#24515;&#34880;&#31649;&#29366;&#20917;&#65292;&#30830;&#20445;&#25152;&#24471;&#32467;&#26524;&#19981;&#20855;&#26377;&#20559;&#35265;&#25110;&#38480;&#21046;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25152;&#24471;&#32467;&#26524;&#20855;&#26377;&#19968;&#33268;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20855;&#26377;&#36739;&#20302;&#30340;&#38169;&#35823;&#29575;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24230;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the course of the past two decades, a substantial body of research has substantiated the viability of utilising cardiac signals as a biometric modality. This paper presents a novel approach for patient identification in healthcare systems using electrocardiogram signals. A convolutional neural network is used to classify users based on images extracted from ECG signals. The proposed identification system is evaluated in multiple databases, providing a comprehensive understanding of its potential in real-world scenarios. The impact of Cardiovascular Diseases on generic user identification has been largely overlooked in previous studies. The presented method takes into account the cardiovascular condition of the patients, ensuring that the results obtained are not biased or limited. Furthermore, the results obtained are consistent and reliable, with lower error rates and higher accuracy metrics, as demonstrated through extensive experimentation. All these features make the proposed 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#37325;&#35201;&#24212;&#29992;&#30340;&#40654;&#26364;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#38750;&#20984;&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20998;&#26512;&#20102;&#31639;&#27861;&#24471;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2211.02163</link><description>&lt;p&gt;
&#19968;&#31181;&#40654;&#26364;ADMM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Riemannian ADMM. (arXiv:2211.02163v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#37325;&#35201;&#24212;&#29992;&#30340;&#40654;&#26364;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#38750;&#20984;&#32422;&#26463;&#21644;&#38750;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#24182;&#20998;&#26512;&#20102;&#31639;&#27861;&#24471;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31867;&#40654;&#26364;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#20989;&#25968;&#26159;&#22312;&#29615;&#31354;&#38388;&#20013;&#30340;&#20809;&#28369;&#20989;&#25968;&#21644;&#38750;&#20809;&#28369;&#20989;&#25968;&#20043;&#21644;&#12290;&#36825;&#31867;&#38382;&#39064;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#20013;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#65292;&#20363;&#22914;&#31232;&#30095;&#20027;&#25104;&#20998;&#20998;&#26512;&#12289;&#31232;&#30095;&#35889;&#32858;&#31867;&#21644;&#27491;&#20132;&#23383;&#20856;&#23398;&#20064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#40654;&#26364;&#20132;&#26367;&#26041;&#21521;&#20056;&#23376;&#27861;&#65288;ADMM&#65289;&#26469;&#35299;&#20915;&#36825;&#19968;&#31867;&#38382;&#39064;&#12290;&#27599;&#27425;&#36845;&#20195;&#25105;&#20204;&#37319;&#29992;&#26131;&#20110;&#35745;&#31639;&#30340;&#27493;&#39588;&#12290;&#22312;&#28201;&#21644;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#31639;&#27861;&#24471;&#21040;$\epsilon$-&#31283;&#23450;&#28857;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35299;&#20915;&#38750;&#20984;&#38382;&#39064;&#30340;ADMM&#26041;&#27861;&#35201;&#20040;&#19981;&#20801;&#35768;&#38750;&#20984;&#32422;&#26463;&#38598;&#21512;&#65292;&#35201;&#20040;&#19981;&#20801;&#35768;&#38750;&#20809;&#28369;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#21453;&#20043;&#65292;&#25105;&#20204;&#30340;&#22797;&#26434;&#24230;&#32467;&#26524;&#36866;&#29992;&#20110;&#21516;&#26102;&#20855;&#26377;&#38750;&#20809;&#28369;&#30446;&#26631;&#21644;&#27969;&#24418;&#32422;&#26463;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#25968;&#20540;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a class of Riemannian optimization problems where the objective is the sum of a smooth function and a nonsmooth function, considered in the ambient space. This class of problems finds important applications in machine learning and statistics such as the sparse principal component analysis, sparse spectral clustering, and orthogonal dictionary learning. We propose a Riemannian alternating direction method of multipliers (ADMM) to solve this class of problems. Our algorithm adopts easily computable steps in each iteration. The iteration complexity of the proposed algorithm for obtaining an $\epsilon$-stationary point is analyzed under mild assumptions. Existing ADMM for solving nonconvex problems either does not allow nonconvex constraint set, or does not allow nonsmooth objective function. In contrast, our complexity result is established for problems with simultaneous nonsmooth objective and manifold constraint. Numerical experiments are conducted to demonstrate the advanta
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Conformal Prediction&#26041;&#27861;&#23545;&#20110;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#26500;&#24314;&#21487;&#20197;&#27491;&#30830;&#35206;&#30422;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#30830;&#25511;&#21046;&#30340;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20043;&#22806;&#65292;&#20351;&#29992;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30028;&#23610;&#23544;&#22122;&#22768;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2209.14295</link><description>&lt;p&gt;
Conformal Prediction&#23545;&#20998;&#25955;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Conformal Prediction is Robust to Dispersive Label Noise. (arXiv:2209.14295v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.14295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;Conformal Prediction&#26041;&#27861;&#23545;&#20110;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#25214;&#20986;&#20102;&#26500;&#24314;&#21487;&#20197;&#27491;&#30830;&#35206;&#30422;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#30340;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20855;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#27491;&#30830;&#25511;&#21046;&#30340;&#35201;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20043;&#22806;&#65292;&#20351;&#29992;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#21487;&#20197;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#30028;&#23610;&#23544;&#22122;&#22768;&#20462;&#27491;&#30340;&#26041;&#27861;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#23545;&#26631;&#31614;&#22122;&#22768;&#20855;&#26377;&#40065;&#26834;&#24615;&#30340;Conformal Prediction&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#28085;&#30422;&#20102;&#22238;&#24402;&#21644;&#20998;&#31867;&#38382;&#39064;&#65292;&#23545;&#20110;&#22914;&#20309;&#26500;&#24314;&#33021;&#22815;&#27491;&#30830;&#35206;&#30422;&#26410;&#35266;&#23519;&#21040;&#30340;&#26080;&#22122;&#22768;&#30495;&#23454;&#26631;&#31614;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#36827;&#34892;&#20102;&#30028;&#23450;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#25105;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#27491;&#30830;&#25511;&#21046;&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#65288;&#22914;&#20551;&#38452;&#24615;&#27604;&#20363;&#65289;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#24102;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#65292;Conformal Prediction&#21644;&#39118;&#38505;&#25511;&#21046;&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#23545;&#24178;&#20928;&#30495;&#23454;&#26631;&#31614;&#30340;&#20445;&#23432;&#39118;&#38505;&#65292;&#38500;&#20102;&#22312;&#23545;&#25239;&#24615;&#26696;&#20363;&#20013;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#36890;&#36807;&#23545;Conformal Prediction&#31639;&#27861;&#36827;&#34892;&#26377;&#30028;&#23610;&#23544;&#30340;&#22122;&#22768;&#20462;&#27491;&#65292;&#20197;&#30830;&#20445;&#23454;&#29616;&#27491;&#30830;&#30340;&#30495;&#23454;&#26631;&#31614;&#39118;&#38505;&#65292;&#32780;&#26080;&#38656;&#32771;&#34385;&#20998;&#25968;&#25110;&#25968;&#25454;&#30340;&#35268;&#21017;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the robustness of conformal prediction, a powerful tool for uncertainty quantification, to label noise. Our analysis tackles both regression and classification problems, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels. We further extend our theory and formulate the requirements for correctly controlling a general loss function, such as the false negative proportion, with noisy labels. Our theory and experiments suggest that conformal prediction and risk-controlling techniques with noisy labels attain conservative risk over the clean ground truth labels except in adversarial cases. In such cases, we can also correct for noise of bounded size in the conformal prediction algorithm in order to ensure achieving the correct risk of the ground truth labels without score or data regularity.
&lt;/p&gt;</description></item></channel></rss>