<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>XpertAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#29305;&#23450;&#33539;&#22260;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#27169;&#22411;&#30340;&#26597;&#35810;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.07486</link><description>&lt;p&gt;
XpertAI&#65306;&#25581;&#31034;&#23376;&#27969;&#24418;&#30340;&#27169;&#22411;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
XpertAI: uncovering model strategies for sub-manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07486
&lt;/p&gt;
&lt;p&gt;
XpertAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#29305;&#23450;&#33539;&#22260;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#27169;&#22411;&#30340;&#26597;&#35810;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#20419;&#36827;&#20102;&#28145;&#20837;&#39564;&#35777;&#21644;&#30693;&#35782;&#25552;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#38024;&#23545;&#20998;&#31867;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#26377;XAI&#35299;&#20915;&#26041;&#26696;&#35299;&#20915;&#20102;&#29305;&#23450;&#20110;&#22238;&#24402;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#22238;&#24402;&#20013;&#65292;&#35299;&#37322;&#38656;&#35201;&#31934;&#30830;&#21046;&#23450;&#20197;&#24212;&#23545;&#29305;&#23450;&#29992;&#25143;&#26597;&#35810;&#65288;&#20363;&#22914;&#21306;&#20998;&#8220;&#20026;&#20160;&#20040;&#36755;&#20986;&#22823;&#20110;0&#65311;&#8221;&#21644;&#8220;&#20026;&#20160;&#20040;&#36755;&#20986;&#22823;&#20110;50&#65311;&#8221;&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24212;&#21453;&#26144;&#27169;&#22411;&#22312;&#30456;&#20851;&#25968;&#25454;&#23376;&#27969;&#24418;&#19978;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;XpertAI&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#33539;&#22260;&#29305;&#23450;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#23545;&#27169;&#22411;&#30340;&#31934;&#20934;&#26597;&#35810;&#65288;&#8220;&#34987;&#35299;&#37322;&#29289;&#8221;&#65289;&#30340;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#30340;&#26694;&#26550;&#12290;XpertAI&#36890;&#24120;&#21046;&#23450;&#21487;&#20197;&#19982;&#22522;&#20110;&#36974;&#25377;&#12289;&#26799;&#24230;&#38598;&#25104;&#25110;&#21453;&#21521;&#20256;&#25773;&#30340;&#27969;&#34892;XAI&#24402;&#22240;&#25216;&#26415;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07486v1 Announce Type: new  Abstract: In recent years, Explainable AI (XAI) methods have facilitated profound validation and knowledge extraction from ML models. While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models. In regression, explanations need to be precisely formulated to address specific user queries (e.g.\ distinguishing between `Why is the output above 0?' and `Why is the output above 50?'). They should furthermore reflect the model's behavior on the relevant data sub-manifold. In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum') as a linear combination of those sub-strategies. XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation. Qualitat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#27969;&#32593;&#32476;&#19982;&#34433;&#32676;&#20248;&#21270;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;ACO&#31639;&#27861;&#24182;&#19982;&#29305;&#23450;&#38382;&#39064;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.07041</link><description>&lt;p&gt;
&#20351;&#29992;GFlowNets&#30340;&#34433;&#32676;&#37319;&#26679;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Ant Colony Sampling with GFlowNets for Combinatorial Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#19968;&#31181;&#32467;&#21512;&#29983;&#25104;&#27969;&#32593;&#32476;&#19982;&#34433;&#32676;&#20248;&#21270;&#26041;&#27861;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#22312;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;ACO&#31639;&#27861;&#24182;&#19982;&#29305;&#23450;&#38382;&#39064;&#21551;&#21457;&#24335;&#26041;&#27861;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#29983;&#25104;&#27969;&#34433;&#32676;&#37319;&#26679;&#22120;&#65288;GFACS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#32452;&#21512;&#20248;&#21270;&#30340;&#31070;&#32463;&#24341;&#23548;&#20803;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;GFACS &#23558;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#19982;&#34433;&#32676;&#20248;&#21270;&#65288;ACO&#65289;&#26041;&#27861;&#30456;&#32467;&#21512;&#12290;GFlowNets &#26159;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#22312;&#32452;&#21512;&#31354;&#38388;&#20013;&#23398;&#20064;&#26500;&#36896;&#24615;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#36755;&#20837;&#22270;&#23454;&#20363;&#19978;&#25552;&#20379;&#20915;&#31574;&#21464;&#37327;&#30340;&#30693;&#24773;&#20808;&#39564;&#20998;&#24067;&#26469;&#22686;&#24378; ACO&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#25216;&#24039;&#32452;&#21512;&#65292;&#21253;&#25324;&#25628;&#32034;&#24341;&#23548;&#30340;&#23616;&#37096;&#25506;&#32034;&#12289;&#33021;&#37327;&#24402;&#19968;&#21270;&#21644;&#33021;&#37327;&#22609;&#24418;&#65292;&#20197;&#25552;&#39640; GFACS &#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GFACS &#22312;&#19971;&#20010;&#32452;&#21512;&#20248;&#21270;&#20219;&#21153;&#20013;&#20248;&#20110;&#22522;&#32447; ACO &#31639;&#27861;&#65292;&#24182;&#19988;&#22312;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#38382;&#39064;&#29305;&#23450;&#21551;&#21457;&#24335;&#26041;&#27861;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312; \url{https://github.com/ai4co/gfacs} &#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07041v1 Announce Type: new  Abstract: This paper introduces the Generative Flow Ant Colony Sampler (GFACS), a novel neural-guided meta-heuristic algorithm for combinatorial optimization. GFACS integrates generative flow networks (GFlowNets) with the ant colony optimization (ACO) methodology. GFlowNets, a generative model that learns a constructive policy in combinatorial spaces, enhance ACO by providing an informed prior distribution of decision variables conditioned on input graph instances. Furthermore, we introduce a novel combination of training tricks, including search-guided local exploration, energy normalization, and energy shaping to improve GFACS. Our experimental results demonstrate that GFACS outperforms baseline ACO algorithms in seven CO tasks and is competitive with problem-specific heuristics for vehicle routing problems. The source code is available at \url{https://github.com/ai4co/gfacs}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20854;&#38477;&#20302;&#20026;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#20004;&#20010;&#36890;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#19968;&#20010;&#22522;&#20110;&#20048;&#35266;&#31639;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#65292;&#27010;&#25324;&#20102;&#20197;&#24448;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#24182;&#35777;&#23454;&#20102;&#26032;&#30340;&#29702;&#35770;&#22312;&#20855;&#26377;&#26377;&#30028;&#21487;&#35206;&#30422;&#24615;&#30340;MDP&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.06323</link><description>&lt;p&gt;
&#20351;&#29992;&#20248;&#21270;&#31561;&#20215;&#35777;&#26126;&#38477;&#20302;&#21040;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#39118;&#38505;&#25935;&#24863;RL
&lt;/p&gt;
&lt;p&gt;
Risk-Sensitive RL with Optimized Certainty Equivalents via Reduction to Standard RL
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06323
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#20854;&#38477;&#20302;&#20026;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#25552;&#20986;&#20102;&#20004;&#20010;&#36890;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#19968;&#20010;&#22522;&#20110;&#20048;&#35266;&#31639;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#65292;&#27010;&#25324;&#20102;&#20197;&#24448;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#29702;&#35770;&#24182;&#35777;&#23454;&#20102;&#26032;&#30340;&#29702;&#35770;&#22312;&#20855;&#26377;&#26377;&#30028;&#21487;&#35206;&#30422;&#24615;&#30340;MDP&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20855;&#26377;&#20248;&#21270;&#31561;&#20215;&#35777;&#26126;&#65288;OCE&#65289;&#39118;&#38505;&#30340;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#65288;RSRL&#65289;&#65292;&#35813;&#39118;&#38505;&#27010;&#25324;&#20102;&#26465;&#20214;&#20540;&#39118;&#38505;&#65288;CVaR&#65289;&#12289;&#29109;&#39118;&#38505;&#21644;&#39532;&#31185;&#32500;&#33576;&#30340;&#22343;&#20540;-&#26041;&#24046;&#12290;&#36890;&#36807;&#22686;&#24378;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#36890;&#29992;&#30340;&#20803;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#20854;&#38477;&#20302;&#20026;&#26631;&#20934;RL&#65306;&#19968;&#20010;&#22522;&#20110;&#20048;&#35266;&#31639;&#27861;&#65292;&#21478;&#19968;&#20010;&#22522;&#20110;&#31574;&#30053;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#20048;&#35266;&#20803;&#31639;&#27861;&#27010;&#25324;&#20102;&#20960;&#20046;&#25152;&#26377;&#20043;&#21069;RSRL&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#20351;&#29992;&#29109;&#39118;&#38505;&#25110;CVaR&#12290;&#22312;&#31163;&#25955;&#22870;&#21169;&#19979;&#65292;&#25105;&#20204;&#30340;&#20048;&#35266;&#29702;&#35770;&#36824;&#35777;&#26126;&#20102;&#20855;&#26377;&#26377;&#30028;&#21487;&#35206;&#30422;&#24615;&#30340;MDP&#65288;&#20363;&#22914;&#22806;&#29983;&#22359;MDP&#65289;&#30340;&#31532;&#19968;&#20010;RSRL&#36951;&#25022;&#19978;&#30028;&#12290;&#22312;&#31163;&#25955;&#22870;&#21169;&#19979;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#20248;&#21270;&#20803;&#31639;&#27861;&#22312;&#19968;&#20010;&#26032;&#39062;&#30340;&#24230;&#37327;&#20013;&#20139;&#26377;&#20840;&#23616;&#25910;&#25947;&#24615;&#21644;&#23616;&#37096;&#25913;&#36827;&#20445;&#35777;&#65292;&#35813;&#24230;&#37327;&#19979;&#30028;&#20026;&#30495;&#23454;&#30340;OCE&#39118;&#38505;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;PPO&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#26500;&#24314;&#19968;&#20010;MDP&#65292;&#24182;&#23637;&#31034;&#23427;&#23398;&#20064;&#20102;&#26368;&#20248;&#30340;&#39118;&#38505;&#25935;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06323v1 Announce Type: new  Abstract: We study Risk-Sensitive Reinforcement Learning (RSRL) with the Optimized Certainty Equivalent (OCE) risk, which generalizes Conditional Value-at-risk (CVaR), entropic risk and Markowitz's mean-variance. Using an augmented Markov Decision Process (MDP), we propose two general meta-algorithms via reductions to standard RL: one based on optimistic algorithms and another based on policy optimization. Our optimistic meta-algorithm generalizes almost all prior RSRL theory with entropic risk or CVaR. Under discrete rewards, our optimistic theory also certifies the first RSRL regret bounds for MDPs with bounded coverability, e.g., exogenous block MDPs. Under discrete rewards, our policy optimization meta-algorithm enjoys both global convergence and local improvement guarantees in a novel metric that lower bounds the true OCE risk. Finally, we instantiate our framework with PPO, construct an MDP, and show that it learns the optimal risk-sensitive
&lt;/p&gt;</description></item><item><title>LoCoDL&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#35757;&#32451;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#20855;&#26377;&#21452;&#20493;&#21152;&#36895;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19968;&#33324;&#24322;&#26500;&#26465;&#20214;&#19979;&#30340;&#24378;&#20984;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.04348</link><description>&lt;p&gt;
LoCoDL: &#20855;&#26377;&#26412;&#22320;&#35757;&#32451;&#21644;&#21387;&#32553;&#30340;&#36890;&#20449;&#39640;&#25928;&#20998;&#24067;&#24335;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
LoCoDL: Communication-Efficient Distributed Learning with Local Training and Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04348
&lt;/p&gt;
&lt;p&gt;
LoCoDL&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#23398;&#20064;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#35757;&#32451;&#21644;&#21387;&#32553;&#25216;&#26415;&#65292;&#20855;&#26377;&#21452;&#20493;&#21152;&#36895;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#19968;&#33324;&#24322;&#26500;&#26465;&#20214;&#19979;&#30340;&#24378;&#20984;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#23398;&#20064;&#20013;&#65292;&#29978;&#33267;&#22312;&#29616;&#20195;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#30001;&#20110;&#36890;&#20449;&#36895;&#24230;&#24930;&#19988;&#25104;&#26412;&#39640;&#65292;&#36890;&#20449;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LoCoDL&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#20449;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#23427;&#21033;&#29992;&#20102;&#26412;&#22320;&#35757;&#32451;&#21644;&#21387;&#32553;&#36825;&#20004;&#31181;&#27969;&#34892;&#19988;&#26377;&#25928;&#30340;&#25216;&#26415;&#65292;&#26412;&#22320;&#35757;&#32451;&#38477;&#20302;&#20102;&#36890;&#20449;&#39057;&#29575;&#65292;&#21387;&#32553;&#21017;&#26159;&#21457;&#36865;&#30701;&#30340;&#27604;&#29305;&#27969;&#32780;&#19981;&#26159;&#23436;&#25972;&#30340;&#28014;&#28857;&#25968;&#21521;&#37327;&#12290;LoCoDL&#36866;&#29992;&#20110;&#22823;&#31867;&#21035;&#30340;&#26080;&#20559;&#21387;&#32553;&#22120;&#65292;&#20854;&#20013;&#21253;&#25324;&#24191;&#27867;&#20351;&#29992;&#30340;&#31232;&#30095;&#21270;&#21644;&#37327;&#21270;&#26041;&#27861;&#12290;LoCoDL&#22312;&#19968;&#33324;&#24322;&#26500;&#26465;&#20214;&#19979;&#20855;&#26377;&#21452;&#20493;&#21152;&#36895;&#30340;&#36890;&#20449;&#22797;&#26434;&#24230;&#20248;&#21183;&#65292;&#36825;&#21462;&#20915;&#20110;&#20989;&#25968;&#30340;&#26465;&#20214;&#25968;&#21644;&#27169;&#22411;&#32500;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#24378;&#20984;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#22312;&#23454;&#36341;&#20013;&#24471;&#21040;&#20102;&#39564;&#35777;&#65292;LoCoDL&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04348v1 Announce Type: cross  Abstract: In Distributed optimization and Learning, and even more in the modern framework of federated learning, communication, which is slow and costly, is critical. We introduce LoCoDL, a communication-efficient algorithm that leverages the two popular and effective techniques of Local training, which reduces the communication frequency, and Compression, in which short bitstreams are sent instead of full-dimensional vectors of floats. LoCoDL works with a large class of unbiased compressors that includes widely-used sparsification and quantization methods. LoCoDL provably benefits from local training and compression and enjoys a doubly-accelerated communication complexity, with respect to the condition number of the functions and the model dimension, in the general heterogenous regime with strongly convex functions. This is confirmed in practice, with LoCoDL outperforming existing algorithms.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.01570</link><description>&lt;p&gt;
SERVAL&#65306;&#22402;&#30452;&#27169;&#22411;&#21644;LLM&#20043;&#38388;&#30340;&#21327;&#21516;&#23398;&#20064;&#65292;&#23454;&#29616;&#38646;-shot&#32423;&#21035;&#30340;&#21307;&#23398;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01570
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23454;&#29616;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#26080;&#30417;&#30563;&#24320;&#21457;&#65292;&#20174;&#32780;&#25913;&#21892;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#30340;&#38646;-shot&#39044;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#23637;&#31034;&#20986;&#23545;&#36890;&#29992;&#21644;&#24120;&#35782;&#38382;&#39064;&#21331;&#36234;&#30340;&#38646;-shot&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#39046;&#22495;&#29305;&#23450;&#22402;&#30452;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#20173;&#28982;&#33853;&#21518;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22402;&#30452;&#30693;&#35782;&#26041;&#38754;&#30340;&#38382;&#39064;&#21644;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#22402;&#30452;&#25968;&#25454;&#27880;&#37322;&#36807;&#31243;&#36890;&#24120;&#38656;&#35201;&#21171;&#21160;&#23494;&#38598;&#22411;&#30340;&#19987;&#23478;&#21442;&#19982;&#65292;&#22240;&#27492;&#22686;&#21152;&#20102;&#22686;&#24378;&#27169;&#22411;&#22402;&#30452;&#33021;&#21147;&#30340;&#39069;&#22806;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SERVAL&#65292;&#19968;&#20010;&#21327;&#21516;&#23398;&#20064;&#27969;&#27700;&#32447;&#65292;&#26088;&#22312;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#65292;&#23545;LLMs&#21644;&#23567;&#27169;&#22411;&#30340;&#22402;&#30452;&#33021;&#21147;&#36827;&#34892;&#26080;&#30417;&#30563;&#24320;&#21457;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;SERVAL&#21033;&#29992;LLMs&#30340;&#38646;-shot&#36755;&#20986;&#20316;&#20026;&#27880;&#37322;&#65292;&#21033;&#29992;&#20854;&#32622;&#20449;&#24230;&#26469;&#20174;&#22836;&#24320;&#22987;&#25945;&#25480;&#19968;&#20010;&#24378;&#22823;&#30340;&#22402;&#30452;&#27169;&#22411;&#12290;&#21453;&#36807;&#26469;&#65292;&#35757;&#32451;&#26377;&#32032;&#30340;&#22402;&#30452;&#27169;&#22411;&#24341;&#23548;LLM&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#38646;-shot&#33021;&#21147;&#65292;&#36880;&#27493;&#25913;&#36827;&#20004;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01570v1 Announce Type: new  Abstract: Recent development of large language models (LLMs) has exhibited impressive zero-shot proficiency on generic and common sense questions. However, LLMs' application on domain-specific vertical questions still lags behind, primarily due to the humiliation problems and deficiencies in vertical knowledge. Furthermore, the vertical data annotation process often requires labor-intensive expert involvement, thereby presenting an additional challenge in enhancing the model's vertical capabilities. In this paper, we propose SERVAL, a synergy learning pipeline designed for unsupervised development of vertical capabilities in both LLMs and small models by mutual enhancement. Specifically, SERVAL utilizes the LLM's zero-shot outputs as annotations, leveraging its confidence to teach a robust vertical model from scratch. Reversely, the trained vertical model guides the LLM fine-tuning to enhance its zero-shot capability, progressively improving both 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;dropout&#29575;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#36825;&#36229;&#20986;&#20102;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.00946</link><description>&lt;p&gt;
&#20351;&#29992;&#38750;&#24120;&#22823;&#30340;Dropout&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning with Very Large Dropout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;dropout&#29575;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#20197;&#23454;&#29616;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#36825;&#36229;&#20986;&#20102;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#19981;&#21487;&#33021;&#20551;&#35013;&#26426;&#22120;&#23398;&#20064;&#23454;&#36341;&#19982;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#36981;&#24490;&#30456;&#21516;&#20998;&#24067;&#30340;&#35266;&#24565;&#26159;&#20860;&#23481;&#30340;&#12290;&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#20351;&#29992;&#38750;&#24120;&#39640;&#30340;&#20002;&#24323;&#29575;&#26469;&#33719;&#24471;&#36825;&#31181;&#20016;&#23500;&#34920;&#31034;&#65292;&#23613;&#31649;&#20351;&#29992;&#36825;&#26679;&#30340;&#20002;&#24323;&#29575;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#20960;&#20046;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#22312;&#36825;&#20123;&#26465;&#20214;&#19979;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#19981;&#20165;&#26159;&#21487;&#33021;&#30340;&#65292;&#32780;&#19988;&#23454;&#29616;&#20102;&#36229;&#36234;&#38598;&#25104;&#21644;&#26435;&#37325;&#24179;&#22343;&#26041;&#27861;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00946v1 Announce Type: new  Abstract: It is impossible today to pretend that the practice of machine learning is compatible with the idea that training and testing data follow the same distribution. Several authors have recently used ensemble techniques to show how scenarios involving multiple data distributions are best served by representations that are both richer than those obtained by regularizing for the best in-distribution performance, and richer than those obtained under the influence of the implicit sparsity bias of common stochastic gradient procedures.   This contribution investigates the use of very high dropout rates instead of ensembles to obtain such rich representations. Although training a deep network from scratch using such dropout rates is virtually impossible, fine-tuning a large pre-trained model under such conditions is not only possible but also achieves out-of-distribution performances that exceed those of both ensembles and weight averaging methods
&lt;/p&gt;</description></item><item><title>DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17812</link><description>&lt;p&gt;
DropBP&#65306;&#36890;&#36807;&#20002;&#24323;&#21453;&#21521;&#20256;&#25773;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17812
&lt;/p&gt;
&lt;p&gt;
DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#27491;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#23618;&#27425;&#20002;&#24323;&#25216;&#26415;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#24323;&#26576;&#20123;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22312;&#27491;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#20002;&#24323;&#23618;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DropBP&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;DropBP&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#65292;&#19981;&#24433;&#21709;&#27491;&#21521;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;DropBP&#35745;&#31639;&#27599;&#20010;&#23618;&#30340;&#25935;&#24863;&#24615;&#20197;&#20998;&#37197;&#36866;&#24403;&#30340;&#20002;&#22833;&#29575;&#65292;&#20174;&#32780;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;DropBP&#26088;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#23436;&#20840;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17812v1 Announce Type: cross  Abstract: Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropag
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;ODE&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#22122;&#22768;&#35266;&#23519;&#21644;&#21021;&#22987;&#26465;&#20214;&#20197;&#21450;&#21521;&#37327;&#22330;&#36827;&#34892;&#26144;&#23556;&#65292;&#24471;&#21040;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#30340;&#32467;&#26524;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#12289;&#21305;&#37197;&#21644;&#32452;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.07594</link><description>&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Foundational Inference Models for Dynamical Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07594
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;&#21160;&#24577;&#31995;&#32479;&#30340;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;&#36890;&#36807;&#29983;&#25104;&#22823;&#22411;ODE&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23558;&#22122;&#22768;&#35266;&#23519;&#21644;&#21021;&#22987;&#26465;&#20214;&#20197;&#21450;&#21521;&#37327;&#22330;&#36827;&#34892;&#26144;&#23556;&#65292;&#24471;&#21040;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#30340;&#32467;&#26524;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#22797;&#21046;&#12289;&#21305;&#37197;&#21644;&#32452;&#21512;&#65292;&#29992;&#20110;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#26500;&#25104;&#20102;&#20316;&#20026;&#33258;&#28982;&#21644;&#31038;&#20250;&#29616;&#35937;&#27169;&#22411;&#30340;&#21160;&#24577;&#31995;&#32479;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25512;&#26029;&#20986;&#26368;&#20339;&#25551;&#36848;&#32473;&#23450;&#29616;&#35937;&#30340;&#19968;&#32452;&#22122;&#22768;&#35266;&#23519;&#30340;ODE&#21487;&#33021;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29616;&#26377;&#30340;&#27169;&#22411;&#24448;&#24448;&#20063;&#38750;&#24120;&#19987;&#19994;&#21270;&#21644;&#22797;&#26434;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30417;&#30563;&#24335;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#20174;&#22122;&#22768;&#25968;&#25454;&#20013;&#38646;&#26679;&#26412;&#25512;&#29702;ODE&#12290;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#23545;&#21021;&#22987;&#26465;&#20214;&#31354;&#38388;&#21644;&#23450;&#20041;&#23427;&#20204;&#30340;&#21521;&#37327;&#22330;&#31354;&#38388;&#30340;&#20998;&#24067;&#36827;&#34892;&#37319;&#26679;&#65292;&#29983;&#25104;&#22823;&#22411;&#19968;&#32500;ODE&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23398;&#20064;&#23558;&#36825;&#20123;&#26041;&#31243;&#30340;&#35299;&#30340;&#22122;&#22768;&#35266;&#23519;&#19982;&#20854;&#30456;&#24212;&#30340;&#21021;&#22987;&#26465;&#20214;&#21644;&#21521;&#37327;&#22330;&#20043;&#38388;&#30340;&#31070;&#32463;&#26144;&#23556;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#27169;&#22411;&#31216;&#20026;&#22522;&#30784;&#25512;&#29702;&#27169;&#22411;&#65288;FIM&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#65288;i&#65289;&#27839;&#26102;&#38388;&#32500;&#22797;&#21046;&#21644;&#21305;&#37197;&#20197;&#22686;&#21152;&#20998;&#36776;&#29575;&#65307;&#65288;ii&#65289;&#22797;&#21046;&#21644;&#32452;&#21512;&#20197;&#26500;&#24314;&#20219;&#20309;&#32500;&#24230;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ordinary differential equations (ODEs) underlie dynamical systems which serve as models for a vast number of natural and social phenomena. Yet inferring the ODE that best describes a set of noisy observations on one such phenomenon can be remarkably challenging, and the models available to achieve it tend to be highly specialized and complex too. In this work we propose a novel supervised learning framework for zero-shot inference of ODEs from noisy data. We first generate large datasets of one-dimensional ODEs, by sampling distributions over the space of initial conditions, and the space of vector fields defining them. We then learn neural maps between noisy observations on the solutions of these equations, and their corresponding initial condition and vector fields. The resulting models, which we call foundational inference models (FIM), can be (i) copied and matched along the time dimension to increase their resolution; and (ii) copied and composed to build inference models of any d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2402.07033</link><description>&lt;p&gt;
Fiddler&#65306;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#24555;&#36895;&#25512;&#26029;&#30340;CPU-GPU&#32534;&#25490;
&lt;/p&gt;
&lt;p&gt;
Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07033
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;Mixture-of-Experts&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#36890;&#36807;CPU-GPU&#32534;&#25490;&#23454;&#29616;&#26368;&#23567;&#21270;&#25968;&#25454;&#20256;&#36755;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#20102;&#19968;&#20010;&#25968;&#37327;&#32423;&#30340;&#25512;&#26029;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Mixture-of-Experts&#65288;MoE&#65289;&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#29615;&#22659;&#19979;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#65292;&#21363;GPU&#20869;&#23384;&#36164;&#28304;&#19981;&#20016;&#23500;&#30340;&#24773;&#20917;&#19979;&#65292;&#30001;&#20110;&#27169;&#22411;&#35268;&#27169;&#24222;&#22823;&#65292;&#23384;&#22312;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#23558;&#27169;&#22411;&#26435;&#37325;&#21368;&#36733;&#21040;CPU&#20869;&#23384;&#30340;&#31995;&#32479;&#65292;&#30001;&#20110;&#39057;&#32321;&#22320;&#22312;CPU&#21644;GPU&#20043;&#38388;&#31227;&#21160;&#25968;&#25454;&#32780;&#23548;&#33268;&#26174;&#33879;&#30340;&#24320;&#38144;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Fiddler&#65292;&#19968;&#31181;&#29992;&#20110;MoE&#27169;&#22411;&#30340;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#24341;&#25806;&#65292;&#23454;&#29616;&#20102;CPU-GPU&#32534;&#25490;&#12290;Fiddler&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;CPU&#30340;&#35745;&#31639;&#33021;&#21147;&#26469;&#26368;&#23567;&#21270;CPU&#21644;GPU&#20043;&#38388;&#30340;&#25968;&#25454;&#20256;&#36755;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Fiddler&#33021;&#22815;&#22312;&#21333;&#20010;&#20855;&#26377;24GB&#20869;&#23384;&#30340;GPU&#19978;&#36816;&#34892;&#26410;&#21387;&#32553;&#30340;Mixtral-8x7B&#27169;&#22411;&#65288;&#21442;&#25968;&#36229;&#36807;90GB&#65289;&#65292;&#27599;&#31186;&#29983;&#25104;&#36229;&#36807;3&#20010;token&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#25552;&#39640;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;Fiddler&#30340;&#20195;&#30721;&#21487;&#20197;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/efeslab/fiddler}
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) based on Mixture-of-Experts (MoE) architecture are showing promising performance on various tasks. However, running them on resource-constrained settings, where GPU memory resources are not abundant, is challenging due to huge model sizes. Existing systems that offload model weights to CPU memory suffer from the significant overhead of frequently moving data between CPU and GPU. In this paper, we propose Fiddler, a resource-efficient inference engine with CPU-GPU orchestration for MoE models. The key idea of Fiddler is to use the computation ability of the CPU to minimize the data movement between the CPU and GPU. Our evaluation shows that Fiddler can run the uncompressed Mixtral-8x7B model, which exceeds 90GB in parameters, to generate over $3$ tokens per second on a single GPU with 24GB memory, showing an order of magnitude improvement over existing methods. The code of Fiddler is publicly available at \url{https://github.com/efeslab/fiddler}
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.03110</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;
&lt;/p&gt;
&lt;p&gt;
Non-Stationary Latent Auto-Regressive Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03110
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38750;&#24179;&#31283;&#28508;&#22312;&#33258;&#22238;&#24402;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#29615;&#22659;&#19979;&#21487;&#20197;&#36798;&#21040;&#36739;&#20302;&#30340;&#36951;&#25022;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20855;&#26377;&#38750;&#24179;&#31283;&#22870;&#21169;&#30340;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#38750;&#24179;&#31283;&#29615;&#22659;&#30340;&#20844;&#24335;&#65292;&#20854;&#20013;&#33218;&#30340;&#24179;&#22343;&#22870;&#21169;&#38543;&#26102;&#38388;&#21464;&#21270;&#26159;&#30001;&#19968;&#20123;&#26410;&#30693;&#30340;&#28508;&#22312;&#33258;&#22238;&#24402;(AR)&#29366;&#24577;&#30340;&#39034;&#24207;k&#20915;&#23450;&#30340;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26032;&#30340;&#29615;&#22659;&#31216;&#20026;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#12290;&#28508;&#22312;AR&#36172;&#21338;&#26426;&#30340;&#19981;&#21516;&#24418;&#24335;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#37117;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#34892;&#20026;&#20581;&#24247;&#25110;&#25945;&#32946;&#31561;&#26032;&#20852;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#36825;&#37324;&#32570;&#20047;&#23545;&#29615;&#22659;&#30340;&#26426;&#21046;&#24314;&#27169;&#12290;&#22914;&#26524;AR&#39034;&#24207;k&#24050;&#30693;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#27861;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31639;&#27861;&#34920;&#29616;&#20986;O(k&#8730;T)&#30340;&#36951;&#25022;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#26174;&#31034;&#65292;&#21363;&#20351;k&#34987;&#38169;&#35823;&#22320;&#20272;&#35745;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#22810;&#20010;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#20063;&#32988;&#36807;&#26631;&#20934;&#30340;UCB&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the stochastic multi-armed bandit problem with non-stationary rewards. We present a novel formulation of non-stationarity in the environment where changes in the mean reward of the arms over time are due to some unknown, latent, auto-regressive (AR) state of order $k$. We call this new environment the latent AR bandit. Different forms of the latent AR bandit appear in many real-world settings, especially in emerging scientific fields such as behavioral health or education where there are few mechanistic models of the environment. If the AR order $k$ is known, we propose an algorithm that achieves $\tilde{O}(k\sqrt{T})$ regret in this setting. Empirically, our algorithm outperforms standard UCB across multiple non-stationary environments, even if $k$ is mis-specified.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00234</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#33021;&#21542;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Generative AI systems Capable of Supporting Information Needs of Patients?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00234
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;AI&#31995;&#32479;&#34987;&#24212;&#29992;&#20110;&#25903;&#25345;&#24739;&#32773;&#20449;&#24687;&#38656;&#27714;&#30340;&#30740;&#31350;&#20013;&#65292;&#20197;&#25552;&#39640;&#24739;&#32773;&#23545;&#25918;&#23556;&#23398;&#25968;&#25454;&#30340;&#29702;&#35299;&#21644;&#31649;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#19982;&#24739;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#30340;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24739;&#26377;&#22797;&#26434;&#30142;&#30149;&#22914;&#30284;&#30151;&#30340;&#24739;&#32773;&#38754;&#20020;&#22797;&#26434;&#30340;&#20449;&#24687;&#25361;&#25112;&#65292;&#20182;&#20204;&#19981;&#20165;&#38656;&#35201;&#20102;&#35299;&#20182;&#20204;&#30340;&#30142;&#30149;&#65292;&#36824;&#38656;&#35201;&#23398;&#20250;&#22914;&#20309;&#31649;&#29702;&#23427;&#12290;&#19982;&#21307;&#30103;&#19987;&#23478;&#65288;&#25918;&#23556;&#31185;&#21307;&#24072;&#12289;&#32959;&#30244;&#31185;&#21307;&#24072;&#65289;&#23494;&#20999;&#20114;&#21160;&#21487;&#20197;&#25552;&#39640;&#24739;&#32773;&#30340;&#23398;&#20064;&#33021;&#21147;&#65292;&#20174;&#32780;&#25913;&#21892;&#30142;&#30149;&#39044;&#21518;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36164;&#28304;&#23494;&#38598;&#19988;&#21344;&#29992;&#20102;&#19987;&#23478;&#30340;&#26102;&#38388;&#65292;&#20351;&#20182;&#20204;&#26080;&#27861;&#23436;&#25104;&#20854;&#20182;&#20851;&#38190;&#20219;&#21153;&#12290;&#37492;&#20110;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#22312;&#25913;&#36827;&#21307;&#30103;&#31995;&#32479;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#30740;&#31350;&#20102;&#29983;&#25104;&#24335;&#35270;&#35273;&#38382;&#31572;&#31995;&#32479;&#22312;&#25918;&#23556;&#23398;&#25104;&#20687;&#25968;&#25454;&#32972;&#26223;&#19979;&#22914;&#20309;&#36127;&#36131;&#20219;&#22320;&#25903;&#25345;&#24739;&#32773;&#30340;&#20449;&#24687;&#38656;&#27714;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#24418;&#25104;&#24615;&#38656;&#27714;&#21457;&#29616;&#30740;&#31350;&#65292;&#21442;&#19982;&#32773;&#35752;&#35770;&#20102;&#19968;&#20010;&#34394;&#26500;&#36817;&#20146;&#30340;&#33016;&#37096;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#22270;&#20687;&#21644;&#30456;&#20851;&#30340;&#25918;&#23556;&#23398;&#25253;&#21578;&#12290;&#36890;&#36807;&#23545;&#21442;&#19982;&#32773;&#21644;&#21307;&#30103;&#19987;&#23478;&#20043;&#38388;&#30340;&#23545;&#35805;&#30340;&#20027;&#39064;&#20998;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#24120;&#35265;&#30340;&#21307;&#23398;&#20449;&#24687;&#38656;&#27714;&#21644;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patients managing a complex illness such as cancer face a complex information challenge where they not only must learn about their illness but also how to manage it. Close interaction with healthcare experts (radiologists, oncologists) can improve patient learning and thereby, their disease outcome. However, this approach is resource intensive and takes expert time away from other critical tasks. Given the recent advancements in Generative AI models aimed at improving the healthcare system, our work investigates whether and how generative visual question answering systems can responsibly support patient information needs in the context of radiology imaging data. We conducted a formative need-finding study in which participants discussed chest computed tomography (CT) scans and associated radiology reports of a fictitious close relative with a cardiothoracic radiologist. Using thematic analysis of the conversation between participants and medical experts, we identified commonly occurrin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;</title><link>https://arxiv.org/abs/2208.04284</link><description>&lt;p&gt;
&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#28145;&#24230;&#23398;&#20064;&#19968;&#33324;&#21270;&#30028;&#38480;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Rademacher Complexity-based Generalization Bounds for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.04284
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#22312;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#23569;&#31867;&#21035;&#22270;&#20687;&#20998;&#31867;&#26102;&#29983;&#25104;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#20854;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#26159;&#21457;&#23637;&#20102;&#38024;&#23545;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#30340;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22522;&#20110;Rademacher&#22797;&#26434;&#24230;&#30340;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#36827;&#34892;&#20998;&#31867;&#23569;&#37327;&#31867;&#21035;&#22270;&#20687;&#38750;&#31354;&#27867;&#21270;&#30028;&#38480;&#12290;&#26032;&#30340;Talagrand&#21387;&#32553;&#24341;&#29702;&#30340;&#21457;&#23637;&#23545;&#20110;&#39640;&#32500;&#26144;&#23556;&#20989;&#25968;&#31354;&#38388;&#21644;&#20855;&#26377;&#19968;&#33324;Lipschitz&#28608;&#27963;&#20989;&#25968;&#30340;CNNs&#26159;&#19968;&#20010;&#20851;&#38190;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Rademacher&#22797;&#26434;&#24230;&#19981;&#20381;&#36182;&#20110;CNNs&#30340;&#32593;&#32476;&#38271;&#24230;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#35832;&#22914;ReLU&#65292;Leaky ReLU&#65292;Parametric Rectifier Linear Unit&#65292;Sigmoid&#21644;Tanh&#31561;&#29305;&#23450;&#31867;&#22411;&#30340;&#28608;&#27963;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the Rademacher complexity-based approach can generate non-vacuous generalisation bounds on Convolutional Neural Networks (CNNs) for classifying a small number of classes of images. The development of new Talagrand's contraction lemmas for high-dimensional mappings between function spaces and CNNs for general Lipschitz activation functions is a key technical contribution. Our results show that the Rademacher complexity does not depend on the network length for CNNs with some special types of activation functions such as ReLU, Leaky ReLU, Parametric Rectifier Linear Unit, Sigmoid, and Tanh.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.10895</link><description>&lt;p&gt;
&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis. (arXiv:2401.10895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;(SCRA)&#32463;&#21382;&#20102;&#28145;&#21051;&#30340;&#28436;&#21464;&#65292;&#38761;&#26032;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;&#36825;&#31181;&#28436;&#21464;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#22312;&#29616;&#20195;&#20379;&#24212;&#38142;&#20013;&#30830;&#20445;&#36816;&#33829;&#30340;&#38887;&#24615;&#21644;&#36830;&#32493;&#24615;&#65292;&#38656;&#35201;&#31283;&#20581;&#30340;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#12290;&#20197;&#24448;&#30340;&#32508;&#36848;&#24050;&#32463;&#27010;&#36848;&#20102;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#20294;&#24573;&#35270;&#20102;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#29702;&#35299;&#20854;&#22312;SCRA&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;1717&#31687;&#35770;&#25991;&#65292;&#24182;&#20174;2014&#24180;&#33267;2023&#24180;&#20043;&#38388;&#21457;&#34920;&#30340;48&#31687;&#25991;&#31456;&#20013;&#33719;&#24471;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;&#35813;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#25506;&#31350;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12289;&#26041;&#27861;&#35770;&#12289;&#30740;&#31350;&#32467;&#26524;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain risk assessment (SCRA) has witnessed a profound evolution through the integration of artificial intelligence (AI) and machine learning (ML) techniques, revolutionizing predictive capabilities and risk mitigation strategies. The significance of this evolution stems from the critical role of robust risk management strategies in ensuring operational resilience and continuity within modern supply chains. Previous reviews have outlined established methodologies but have overlooked emerging AI/ML techniques, leaving a notable research gap in understanding their practical implications within SCRA. This paper conducts a systematic literature review combined with a comprehensive bibliometric analysis. We meticulously examined 1,717 papers and derived key insights from a select group of 48 articles published between 2014 and 2023. The review fills this research gap by addressing pivotal research questions, and exploring existing AI/ML techniques, methodologies, findings, and future 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;</title><link>http://arxiv.org/abs/2310.16152</link><description>&lt;p&gt;
FLTrojan: &#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#23545;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38544;&#31169;&#27844;&#38706;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
FLTrojan: Privacy Leakage Attacks against Federated Language Models Through Selective Weight Tampering. (arXiv:2310.16152v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;FLTrojan&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#24615;&#26435;&#37325;&#31713;&#25913;&#65292;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#12290;&#36890;&#36807;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#21487;&#20197;&#24341;&#36215;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#65292;&#24182;&#21457;&#29616;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(Federated learning, FL)&#27491;&#25104;&#20026;&#35768;&#22810;&#25216;&#26415;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;&#35821;&#35328;&#24314;&#27169;&#39046;&#22495;&#65292;&#20854;&#20013;&#20010;&#20307;FL&#21442;&#19982;&#32773;&#22312;&#20854;&#26412;&#22320;&#25968;&#25454;&#38598;&#20013;&#24448;&#24448;&#20855;&#26377;&#25935;&#24863;&#30340;&#25991;&#26412;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38544;&#31169;&#27844;&#38706;&#31243;&#24230;&#24182;&#19981;&#31616;&#21333;&#65292;&#29616;&#26377;&#30340;&#25915;&#20987;&#21482;&#26159;&#35797;&#22270;&#25552;&#21462;&#25968;&#25454;&#65292;&#32780;&#19981;&#32771;&#34385;&#25968;&#25454;&#30340;&#25935;&#24863;&#24615;&#25110;&#22825;&#30495;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20851;&#20110;&#20174;&#32852;&#37030;&#35821;&#35328;&#27169;&#22411;&#20013;&#27844;&#38706;&#38544;&#31169;&#25935;&#24863;&#29992;&#25143;&#25968;&#25454;&#30340;&#20004;&#20010;&#26032;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;FL&#20013;&#20013;&#38388;&#36718;&#27425;&#30340;&#27169;&#22411;&#24555;&#29031;&#27604;&#26368;&#32456;&#35757;&#32451;&#27169;&#22411;&#33021;&#22815;&#36896;&#25104;&#26356;&#22823;&#30340;&#38544;&#31169;&#27844;&#38706;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30830;&#23450;&#38544;&#31169;&#27844;&#38706;&#21487;&#20197;&#36890;&#36807;&#31713;&#25913;&#27169;&#22411;&#30340;&#36873;&#25321;&#24615;&#26435;&#37325;&#26469;&#21152;&#21095;&#65292;&#36825;&#20123;&#26435;&#37325;&#29305;&#21035;&#36127;&#36131;&#35760;&#24518;&#25935;&#24863;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24694;&#24847;&#23458;&#25143;&#31471;&#22914;&#20309;&#22312;FL&#20013;&#27844;&#38706;&#20854;&#20182;&#29992;&#25143;&#30340;&#38544;&#31169;&#25935;&#24863;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is becoming a key component in many technology-based applications including language modeling -- where individual FL participants often have privacy-sensitive text data in their local datasets. However, realizing the extent of privacy leakage in federated language models is not straightforward and the existing attacks only intend to extract data regardless of how sensitive or naive it is. To fill this gap, in this paper, we introduce two novel findings with regard to leaking privacy-sensitive user data from federated language models. Firstly, we make a key observation that model snapshots from the intermediate rounds in FL can cause greater privacy leakage than the final trained model. Secondly, we identify that privacy leakage can be aggravated by tampering with a model's selective weights that are specifically responsible for memorizing the sensitive training data. We show how a malicious client can leak the privacy-sensitive data of some other user in FL even
&lt;/p&gt;</description></item><item><title>MuseGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#25910;&#25947;&#30340;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#20943;&#23569;&#22522;&#20110;&#37319;&#26679;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#21516;&#26102;&#20316;&#20026;&#39044;&#27979;&#29305;&#24449;&#21644;&#33021;&#37327;&#20989;&#25968;&#26368;&#23567;&#21270;&#32773;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.12457</link><description>&lt;p&gt;
MuseGNN: &#21487;&#35299;&#37322;&#21644;&#21487;&#25910;&#25947;&#30340;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;
&lt;/p&gt;
&lt;p&gt;
MuseGNN: Interpretable and Convergent Graph Neural Network Layers at Scale. (arXiv:2310.12457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12457
&lt;/p&gt;
&lt;p&gt;
MuseGNN&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#25910;&#25947;&#30340;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#36890;&#36807;&#36845;&#20195;&#22320;&#20943;&#23569;&#22522;&#20110;&#37319;&#26679;&#30340;&#33021;&#37327;&#20989;&#25968;&#65292;&#21516;&#26102;&#20316;&#20026;&#39044;&#27979;&#29305;&#24449;&#21644;&#33021;&#37327;&#20989;&#25968;&#26368;&#23567;&#21270;&#32773;&#65292;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33021;&#22815;&#24314;&#27169;&#20855;&#26377;&#36328;&#23454;&#20363;&#20851;&#31995;&#30340;&#25968;&#25454;&#30340;&#35768;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26550;&#26500;&#20013;&#65292;&#19968;&#31867;&#37325;&#35201;&#30340;&#23376;&#31867;&#28041;&#21450;&#35774;&#35745;&#23618;&#65292;&#20854;&#27491;&#21521;&#20256;&#36882;&#36845;&#20195;&#22320;&#20943;&#23569;&#24863;&#20852;&#36259;&#30340;&#22270;&#27491;&#21017;&#21270;&#33021;&#37327;&#20989;&#25968;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#36755;&#20986;&#23618;&#20135;&#29983;&#30340;&#33410;&#28857;&#23884;&#20837;&#26082;&#21487;&#20316;&#20026;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65288;&#22914;&#33410;&#28857;&#20998;&#31867;&#65289;&#30340;&#39044;&#27979;&#29305;&#24449;&#65292;&#21448;&#21487;&#20316;&#20026;&#33021;&#37327;&#20989;&#25968;&#26368;&#23567;&#21270;&#32773;&#65292;&#32487;&#25215;&#20102;&#21487;&#38752;&#30340;&#24402;&#32435;&#20559;&#32622;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#20197;&#36825;&#31181;&#26041;&#24335;&#26500;&#24314;&#30340;GNN&#26550;&#26500;&#30340;&#25193;&#23637;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#27491;&#21521;&#20256;&#36882;&#30340;&#25910;&#25947;&#21487;&#33021;&#28041;&#21450;&#20855;&#26377;&#30456;&#24403;&#28145;&#24230;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37319;&#26679;&#30340;&#33021;&#37327;&#20989;&#25968;&#21644;&#21487;&#25193;&#23637;&#30340;GNN&#23618;&#65292;&#36890;&#36807;&#22312;&#26576;&#20123;&#35774;&#32622;&#20013;&#20855;&#26377;&#25910;&#25947;&#20445;&#35777;&#30340;&#25351;&#23548;&#65292;&#36845;&#20195;&#22320;&#20943;&#23569;&#23427;&#12290;&#25105;&#20204;&#36824;&#22522;&#20110;&#36825;&#20123;&#35774;&#35745;&#23454;&#20363;&#21270;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;GNN&#26550;&#26500;&#65292;&#35813;&#27169;&#22411;&#22312;&#20934;&#30830;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#22343;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Among the many variants of graph neural network (GNN) architectures capable of modeling data with cross-instance relations, an important subclass involves layers designed such that the forward pass iteratively reduces a graph-regularized energy function of interest. In this way, node embeddings produced at the output layer dually serve as both predictive features for solving downstream tasks (e.g., node classification) and energy function minimizers that inherit desirable inductive biases and interpretability. However, scaling GNN architectures constructed in this way remains challenging, in part because the convergence of the forward pass may involve models with considerable depth. To tackle this limitation, we propose a sampling-based energy function and scalable GNN layers that iteratively reduce it, guided by convergence guarantees in certain settings. We also instantiate a full GNN architecture based on these designs, and the model achieves competitive accuracy and scalability whe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#27969;&#20013;&#30340;&#20004;&#20010;&#26679;&#26412;&#26469;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#65292;&#21435;&#38500;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39069;&#22806;&#26435;&#37325;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.01170</link><description>&lt;p&gt;
&#30452;&#25509;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Direct Gradient Temporal Difference Learning. (arXiv:2308.01170v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01170
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#30452;&#25509;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#27969;&#20013;&#30340;&#20004;&#20010;&#26679;&#26412;&#26469;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#65292;&#21435;&#38500;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#30340;&#39069;&#22806;&#26435;&#37325;&#65292;&#20445;&#35777;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#25552;&#20379;&#20102;&#25910;&#25947;&#24615;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#23398;&#20064;&#20351;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20195;&#29702;&#33021;&#22815;&#21453;&#20107;&#23454;&#22320;&#25512;&#29702;&#26410;&#25191;&#34892;&#30340;&#31574;&#30053;&#65292;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#26368;&#37325;&#35201;&#30340;&#24605;&#24819;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#24403;&#19982;&#20989;&#25968;&#36924;&#36817;&#21644;&#33258;&#20030;&#36825;&#20004;&#20010;&#22312;&#22823;&#35268;&#27169;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#22240;&#32032;&#32467;&#21512;&#26102;&#65292;&#20250;&#23548;&#33268;&#19981;&#31283;&#23450;&#24615;&#12290;&#36825;&#23601;&#26159;&#33261;&#21517;&#26157;&#33879;&#30340;&#33268;&#21629;&#19977;&#20803;&#32452;&#12290;&#26799;&#24230;&#26102;&#38388;&#24046;&#20998;&#65288;GTD&#65289;&#26159;&#35299;&#20915;&#36825;&#20010;&#33268;&#21629;&#19977;&#20803;&#32452;&#30340;&#19968;&#31181;&#24378;&#22823;&#24037;&#20855;&#12290;&#23427;&#30340;&#25104;&#21151;&#26159;&#36890;&#36807;&#20351;&#29992;&#26435;&#37325;&#22797;&#21046;&#25110;Fenchel&#23545;&#20598;&#38388;&#25509;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#32780;&#23454;&#29616;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#26041;&#27861;&#26469;&#35299;&#20915;&#21452;&#37325;&#21462;&#26679;&#38382;&#39064;&#65292;&#21482;&#38656;&#22312;&#36880;&#28176;&#22686;&#21152;&#30340;&#39532;&#23572;&#21487;&#22827;&#25968;&#25454;&#27969;&#20013;&#20351;&#29992;&#20004;&#20010;&#26679;&#26412;&#12290;&#25152;&#24471;&#21040;&#30340;&#31639;&#27861;&#19982;GTD&#19968;&#26679;&#35745;&#31639;&#25928;&#29575;&#39640;&#65292;&#20294;&#25682;&#24323;&#20102;GTD&#30340;&#39069;&#22806;&#26435;&#37325;&#12290;&#25105;&#20204;&#25152;&#20184;&#20986;&#30340;&#21807;&#19968;&#20195;&#20215;&#26159;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#20869;&#23384;&#21576;&#23545;&#25968;&#22686;&#38271;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#28176;&#36817;&#21644;&#26377;&#38480;&#26679;&#26412;&#20998;&#26512;&#65292;&#20854;&#20013;&#25910;&#25947;&#24615;&#21487;&#20197;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Off-policy learning enables a reinforcement learning (RL) agent to reason counterfactually about policies that are not executed and is one of the most important ideas in RL. It, however, can lead to instability when combined with function approximation and bootstrapping, two arguably indispensable ingredients for large-scale reinforcement learning. This is the notorious deadly triad. Gradient Temporal Difference (GTD) is one powerful tool to solve the deadly triad. Its success results from solving a doubling sampling issue indirectly with weight duplication or Fenchel duality. In this paper, we instead propose a direct method to solve the double sampling issue by simply using two samples in a Markovian data stream with an increasing gap. The resulting algorithm is as computationally efficient as GTD but gets rid of GTD's extra weights. The only price we pay is a logarithmically increasing memory as time progresses. We provide both asymptotic and finite sample analysis, where the conver
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;</title><link>http://arxiv.org/abs/2305.15557</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#23398;&#20064;&#20855;&#26377;&#24555;&#36895;&#25910;&#25947;&#29575;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence. (arXiv:2305.15557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15557
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#20013;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#24555;&#36895;&#30340;&#25910;&#25947;&#29575;&#65292;&#20351;&#24471;&#23398;&#20064;&#36895;&#29575;&#38543;&#30528;&#26410;&#30693;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#21152;&#32039;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38750;&#21442;&#25968;&#23398;&#20064;&#33539;&#24335;&#26469;&#35782;&#21035;&#38750;&#32447;&#24615;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#65292;&#35813;&#33539;&#24335;&#20381;&#36182;&#20110;&#29366;&#24577;&#30340;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#23558;&#30456;&#24212;&#30340;Fokker-Planck&#26041;&#31243;&#30340;&#22522;&#20110;RKHS&#30340;&#36817;&#20284;&#25311;&#21512;&#21040;&#36825;&#20123;&#35266;&#27979;&#20540;&#65292;&#20174;&#32780;&#24471;&#20986;&#29702;&#35770;&#23398;&#20064;&#36895;&#29575;&#30340;&#20272;&#35745;&#20540;&#65292;&#36825;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#24403;&#26410;&#30693;&#28418;&#31227;&#21644;&#25193;&#25955;&#31995;&#25968;&#30340;&#20809;&#28369;&#24230;&#36234;&#39640;&#26102;&#65292;&#29702;&#35770;&#20272;&#35745;&#20540;&#36234;&#26469;&#36234;&#32039;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#20869;&#26680;&#30340;&#65292;&#22240;&#27492;&#31163;&#32447;&#39044;&#22788;&#29702;&#21487;&#20197;&#22312;&#21407;&#21017;&#19978;&#24471;&#21040;&#26377;&#25928;&#30340;&#25968;&#20540;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel non-parametric learning paradigm for the identification of drift and diffusion coefficients of non-linear stochastic differential equations, which relies upon discrete-time observations of the state. The key idea essentially consists of fitting a RKHS-based approximation of the corresponding Fokker-Planck equation to such observations, yielding theoretical estimates of learning rates which, unlike previous works, become increasingly tighter when the regularity of the unknown drift and diffusion coefficients becomes higher. Our method being kernel-based, offline pre-processing may in principle be profitably leveraged to enable efficient numerical implementation.
&lt;/p&gt;</description></item></channel></rss>