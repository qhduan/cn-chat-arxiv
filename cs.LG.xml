<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#23569;&#37327;&#27169;&#22411;&#23601;&#33021;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26435;&#37325;&#31354;&#38388;&#21644;&#23618;&#27425;&#21152;&#26435;&#24179;&#22343;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19522</link><description>&lt;p&gt;
&#27169;&#22411;&#24211;&#65306;&#25105;&#20204;&#21482;&#38656;&#35201;&#20960;&#20010;&#32463;&#36807;&#33391;&#22909;&#35843;&#25972;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Model Stock: All we need is just a few fine-tuned models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#65292;&#21482;&#20351;&#29992;&#23569;&#37327;&#27169;&#22411;&#23601;&#33021;&#33719;&#24471;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#36890;&#36807;&#26435;&#37325;&#31354;&#38388;&#21644;&#23618;&#27425;&#21152;&#26435;&#24179;&#22343;&#25216;&#26415;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#27169;&#22411;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#25552;&#20379;&#24378;&#22823;&#30340;&#20869;&#20998;&#24067;&#65288;ID&#65289;&#21644;&#22806;&#20998;&#24067;&#65288;OOD&#65289;&#24615;&#33021;&#12290;&#19982;&#38656;&#35201;&#22823;&#37327;&#24494;&#35843;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#30340;&#20256;&#32479;&#20570;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26356;&#23569;&#30340;&#27169;&#22411;&#26469;&#33719;&#24471;&#26368;&#32456;&#26435;&#37325;&#65292;&#21516;&#26102;&#20135;&#29983;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#20174;&#24494;&#35843;&#26435;&#37325;&#30340;&#26435;&#37325;&#31354;&#38388;&#20013;&#27762;&#21462;&#20851;&#38190;&#35265;&#35299;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#24615;&#33021;&#21644;&#25509;&#36817;&#26435;&#37325;&#31354;&#38388;&#20013;&#24515;&#30340;&#24378;&#36830;&#25509;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#26469;&#36817;&#20284;&#20013;&#24515;&#25509;&#36817;&#30340;&#26435;&#37325;&#65292;&#21487;&#22312;&#35757;&#32451;&#26399;&#38388;&#25110;&#20043;&#21518;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#30340;&#36880;&#23618;&#26435;&#37325;&#24179;&#22343;&#25216;&#26415;&#36229;&#36234;&#20102;Model Soup&#31561;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26041;&#27861;&#65292;&#20165;&#21033;&#29992;&#20004;&#20010;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#31181;&#31574;&#30053;&#21487;&#20197;&#34987;&#31216;&#20026;&#27169;&#22411;&#24211;&#65292;&#31361;&#20986;&#20102;&#23427;&#20381;&#36182;&#20110;&#36873;&#25321;&#23569;&#37327;&#27169;&#22411;&#26469;&#36827;&#34892;&#32508;&#21512;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19522v1 Announce Type: new  Abstract: This paper introduces an efficient fine-tuning method for large pre-trained models, offering strong in-distribution (ID) and out-of-distribution (OOD) performance. Breaking away from traditional practices that need a multitude of fine-tuned models for averaging, our approach employs significantly fewer models to achieve final weights yet yield superior accuracy. Drawing from key insights in the weight space of fine-tuned weights, we uncover a strong link between the performance and proximity to the center of weight space. Based on this, we introduce a method that approximates a center-close weight using only two fine-tuned models, applicable during or after training. Our innovative layer-wise weight averaging technique surpasses state-of-the-art model methods such as Model Soup, utilizing only two fine-tuned models. This strategy can be aptly coined Model Stock, highlighting its reliance on selecting a minimal number of models to draw a 
&lt;/p&gt;</description></item><item><title>&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02364</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#30340;&#21457;&#23637;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
The Developmental Landscape of In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02364
&lt;/p&gt;
&lt;p&gt;
&#22312;transformers&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#31163;&#25955;&#21457;&#23637;&#38454;&#27573;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#12290;&#25105;&#20204;&#20351;&#29992;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#39564;&#35777;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;transformers&#20013;&#65292;&#24403;&#23427;&#20204;&#36890;&#36807;&#35821;&#35328;&#24314;&#27169;&#25110;&#32447;&#24615;&#22238;&#24402;&#20219;&#21153;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#19978;&#19979;&#25991;&#23398;&#20064;&#26159;&#22914;&#20309;&#20197;&#31163;&#25955;&#30340;&#21457;&#23637;&#38454;&#27573;&#20986;&#29616;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#26816;&#27979;&#20998;&#38548;&#36825;&#20123;&#38454;&#27573;&#30340;&#20851;&#38190;&#37324;&#31243;&#30865;&#65292;&#36890;&#36807;&#25506;&#27979;&#21442;&#25968;&#31354;&#38388;&#21644;&#20989;&#25968;&#31354;&#38388;&#20013;&#31181;&#32676;&#25439;&#22833;&#30340;&#20960;&#20309;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#34892;&#20026;&#21644;&#32467;&#26500;&#24230;&#37327;&#30740;&#31350;&#36825;&#20123;&#26032;&#26041;&#27861;&#25581;&#31034;&#30340;&#38454;&#27573;&#65292;&#20197;&#24314;&#31435;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that in-context learning emerges in transformers in discrete developmental stages, when they are trained on either language modeling or linear regression tasks. We introduce two methods for detecting the milestones that separate these stages, by probing the geometry of the population loss in both parameter space and function space. We study the stages revealed by these new methods using a range of behavioral and structural metrics to establish their validity.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Bagged Regularized $k$-Distances for Anomaly Detection (BRDAD)&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#36317;&#31163;&#31639;&#27861;&#20013;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21253;&#38598;&#25104;&#26041;&#27861;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.01046</link><description>&lt;p&gt;
Bagged Regularized $k$-Distances&#29992;&#20110;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bagged Regularized $k$-Distances for Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Bagged Regularized $k$-Distances for Anomaly Detection (BRDAD)&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#36317;&#31163;&#31639;&#27861;&#20013;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#21253;&#38598;&#25104;&#26041;&#27861;&#35299;&#20915;&#20102;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#30340;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#30340;&#33539;&#24335;&#65292;&#21363;&#22312;&#27809;&#26377;&#26631;&#35760;&#30340;&#24773;&#20917;&#19979;&#35782;&#21035;&#25968;&#25454;&#38598;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#23613;&#31649;&#22522;&#20110;&#36317;&#31163;&#30340;&#26041;&#27861;&#23545;&#20110;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#23545;&#26368;&#36817;&#37051;&#25968;&#37327;&#30340;&#36873;&#25321;&#38750;&#24120;&#25935;&#24863;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#31639;&#27861;&#65292;&#31216;&#20026;Bagged Regularized $k$-Distances for Anomaly Detection (BRDAD)&#65292;&#23558;&#38750;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#36716;&#21270;&#20026;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;BRDAD&#31639;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#26367;&#20195;&#39118;&#38505;&#65288;&#21363;&#32463;&#39564;&#39118;&#38505;&#30340;&#26377;&#38480;&#26679;&#26412;&#19978;&#30028;&#65289;&#26469;&#36873;&#25321;&#26435;&#37325;&#65292;&#20197;&#29992;&#20110;&#23494;&#24230;&#20272;&#35745;&#30340;&#24102;&#26435;&#37325;&#30340;$k$-distances&#12290;&#36825;&#31181;&#26041;&#27861;&#25104;&#21151;&#35299;&#20915;&#20102;&#22522;&#20110;&#36317;&#31163;&#31639;&#27861;&#20013;&#36229;&#21442;&#25968;&#36873;&#25321;&#30340;&#25935;&#24863;&#24615;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#26102;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#36890;&#36807;&#21253;&#38598;&#25104;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the paradigm of unsupervised anomaly detection, which involves the identification of anomalies within a dataset in the absence of labeled examples. Though distance-based methods are top-performing for unsupervised anomaly detection, they suffer heavily from the sensitivity to the choice of the number of the nearest neighbors. In this paper, we propose a new distance-based algorithm called bagged regularized $k$-distances for anomaly detection (BRDAD) converting the unsupervised anomaly detection problem into a convex optimization problem. Our BRDAD algorithm selects the weights by minimizing the surrogate risk, i.e., the finite sample bound of the empirical risk of the bagged weighted $k$-distances for density estimation (BWDDE). This approach enables us to successfully address the sensitivity challenge of the hyperparameter choice in distance-based algorithms. Moreover, when dealing with large-scale datasets, the efficiency issues can be addressed by the incorporated baggi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#21644;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.01100</link><description>&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#23454;&#29616;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Scalable manifold learning by uniform landmark sampling and constrained locally linear embedding. (arXiv:2401.01100v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01100
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22343;&#21248;&#22320;&#26631;&#25277;&#26679;&#21644;&#32422;&#26463;&#23616;&#37096;&#32447;&#24615;&#23884;&#20837;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#35299;&#20915;&#20840;&#23616;&#32467;&#26500;&#22833;&#30495;&#21644;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#31185;&#23398;&#20013;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#26088;&#22312;&#25581;&#31034;&#39640;&#32500;&#31354;&#38388;&#20013;&#22797;&#26434;&#38750;&#32447;&#24615;&#27969;&#24418;&#20869;&#22312;&#30340;&#20302;&#32500;&#32467;&#26500;&#12290;&#36890;&#36807;&#21033;&#29992;&#27969;&#24418;&#20551;&#35774;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#38750;&#32447;&#24615;&#38477;&#32500;&#25216;&#26415;&#26469;&#20419;&#36827;&#21487;&#35270;&#21270;&#12289;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#33719;&#24471;&#20851;&#38190;&#27934;&#23519;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#27969;&#24418;&#23398;&#20064;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#20840;&#23616;&#32467;&#26500;&#20013;&#30340;&#22823;&#37327;&#22833;&#30495;&#38382;&#39064;&#65292;&#36825;&#38459;&#30861;&#20102;&#23545;&#24213;&#23618;&#27169;&#24335;&#30340;&#29702;&#35299;&#12290;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#20063;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#22823;&#35268;&#27169;&#25968;&#25454;&#30340;&#36866;&#29992;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#30340;&#27969;&#24418;&#23398;&#20064;(scML)&#26041;&#27861;&#65292;&#21487;&#20197;&#20197;&#26377;&#25928;&#30340;&#26041;&#24335;&#22788;&#29702;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#12290;&#23427;&#36890;&#36807;&#23547;&#25214;&#19968;&#32452;&#22320;&#26631;&#26469;&#26500;&#24314;&#25972;&#20010;&#25968;&#25454;&#30340;&#20302;&#32500;&#39592;&#26550;&#65292;&#28982;&#21518;&#23558;&#38750;&#22320;&#26631;&#24341;&#20837;&#22320;&#26631;&#31354;&#38388;&#20013;
&lt;/p&gt;
&lt;p&gt;
As a pivotal approach in machine learning and data science, manifold learning aims to uncover the intrinsic low-dimensional structure within complex nonlinear manifolds in high-dimensional space. By exploiting the manifold hypothesis, various techniques for nonlinear dimension reduction have been developed to facilitate visualization, classification, clustering, and gaining key insights. Although existing manifold learning methods have achieved remarkable successes, they still suffer from extensive distortions incurred in the global structure, which hinders the understanding of underlying patterns. Scalability issues also limit their applicability for handling large-scale data. Here, we propose a scalable manifold learning (scML) method that can manipulate large-scale and high-dimensional data in an efficient manner. It starts by seeking a set of landmarks to construct the low-dimensional skeleton of the entire data and then incorporates the non-landmarks into the landmark space based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15611</link><description>&lt;p&gt;
&#22522;&#20110;&#35889;&#35282;&#24230;&#21078;&#26512;&#29983;&#29289;&#25968;&#25454;&#20013;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#65306;&#35266;&#28857;&#21644;&#23454;&#36341;
&lt;/p&gt;
&lt;p&gt;
Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective. (arXiv:2305.15611v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15611
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35889;&#35282;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#65292;&#24182;&#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#21457;&#29616;GNNs&#22312;&#24230;&#20998;&#24067;&#21644;&#35889;&#20998;&#24067;&#20559;&#31227;&#26102;&#22343;&#34920;&#29616;&#25935;&#24863;&#65292;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#25581;&#31034;&#20102; GNNs&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476; (GNNs) &#26159;&#21542;&#20855;&#26377;&#20174;&#23567;&#22270;&#20013;&#23398;&#20064;&#30340;&#30693;&#35782;&#21487;&#25512;&#24191;&#21040;&#21516;&#19968;&#39046;&#22495;&#30340;&#22823;&#22270;&#20013;&#12290;&#20043;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19981;&#21516;&#22823;&#23567;&#30340;&#22270;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#23588;&#20854;&#26159;&#24230;&#20998;&#24067;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22270;&#20998;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#28982;&#32780;&#65292;&#22312;&#29983;&#29289;&#25968;&#25454;&#38598;&#20013;&#65292;&#24230;&#25968;&#26159;&#26377;&#30028;&#30340;&#65292;&#22240;&#27492;&#24230;&#20998;&#24067;&#30340;&#20559;&#31227;&#24456;&#23567;&#12290;&#21363;&#20351;&#24230;&#20998;&#24067;&#20559;&#31227;&#24456;&#23567;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;GNNs&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#30340;&#22823;&#22270;&#19978;&#30340;&#24615;&#33021;&#20173;&#28982;&#19979;&#38477;&#65292;&#26263;&#31034;&#26377;&#20854;&#20182;&#21407;&#22240;&#12290;&#20107;&#23454;&#19978;&#65292;&#20197;&#24448;&#23545;&#20110;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#21508;&#31181;&#22270;&#23610;&#23544;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#31867;&#22411;&#21644;&#23646;&#24615;&#30340;&#25506;&#32034;&#19981;&#36275;&#12290;&#27492;&#22806;&#65292;&#20197;&#21069;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#20998;&#26512;&#22823;&#22810;&#38598;&#20013;&#22312;&#31354;&#38388;&#39046;&#22495;&#12290;&#20026;&#22635;&#34917;&#36825;&#20123;&#31354;&#30333;&#65292;&#25105;&#20204;&#37319;&#29992;&#35889;&#35282;&#24230;&#21435;&#30740;&#31350;GNNs&#22312;&#29983;&#29289;&#22270;&#25968;&#25454;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#27169;&#25311;&#21508;&#31181;&#31867;&#22411;&#30340;&#24230;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#27979;&#35797;GNNs &#22312;&#30495;&#23454;&#29983;&#29289;&#25968;&#25454;&#38598;&#19978;&#30340;&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38500;&#20102;&#24230;&#20998;&#24067;&#20559;&#31227;&#22806;&#65292;GNNs &#36824;&#23545;&#22270;&#22823;&#23567;&#21464;&#21270;&#24341;&#36215;&#30340;&#35889;&#20998;&#24067;&#20559;&#31227;&#24456;&#25935;&#24863;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#19981;&#21516;&#30340;GNN&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#26126;&#65292;&#19968;&#20123;&#27169;&#22411;&#27604;&#20854;&#20182;&#27169;&#22411;&#26356;&#20855;&#26377;&#23610;&#23544;&#27867;&#21270;&#24615;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20851;&#20110;GNNs&#23610;&#23544;&#21487;&#27867;&#21270;&#24615;&#38382;&#39064;&#30340;&#26032;&#35266;&#28857;&#21644;&#23454;&#36341;&#65292;&#24182;&#20026;&#35813;&#39046;&#22495;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#30410;&#30340;&#27934;&#23519;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the question of whether the knowledge learned by graph neural networks (GNNs) from small graphs is generalizable to large graphs in the same domain. Prior works suggest that the distribution shift, particularly in the degree distribution, between graphs of different sizes can lead to performance degradation in the graph classification task. However, this may not be the case for biological datasets where the degrees are bounded and the distribution shift of degrees is small. Even with little degree distribution shift, our observations show that GNNs' performance on larger graphs from the same datasets still degrades, suggesting other causes. In fact, there has been a lack of exploration in real datasets to understand the types and properties of distribution shifts caused by various graph sizes. Furthermore, previous analyses of size generalizability mostly focus on the spatial domain.  To fill these gaps, we take the spectral perspective and study the size generalizabilit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.04095</link><description>&lt;p&gt;
&#22522;&#20110;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Gradient Leakage Defense with Key-Lock Module for Federated Learning. (arXiv:2305.04095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#65292;&#24182;&#21487;&#30830;&#20445;&#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#37319;&#29992;&#30340;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#31169;&#26377;&#25968;&#25454;&#20445;&#25345;&#26412;&#22320;&#65292;&#20801;&#35768;&#23433;&#20840;&#35745;&#31639;&#21644;&#26412;&#22320;&#27169;&#22411;&#26799;&#24230;&#19982;&#31532;&#19977;&#26041;&#21442;&#25968;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#20132;&#25442;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#36890;&#36807;&#20849;&#20139;&#30340;&#26799;&#24230;&#21487;&#33021;&#20250;&#21361;&#21450;&#38544;&#31169;&#24182;&#24674;&#22797;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#35814;&#32454;&#30340;&#20998;&#26512;&#21644;&#23545;&#26799;&#24230;&#27844;&#28431;&#38382;&#39064;&#30340;&#26032;&#35270;&#35282;&#12290;&#36825;&#20123;&#29702;&#35770;&#24037;&#20316;&#23548;&#33268;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#27844;&#38706;&#38450;&#24481;&#25216;&#26415;&#65292;&#20351;&#29992;&#31169;&#38053;&#38145;&#27169;&#22359;&#20445;&#25252;&#20219;&#24847;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#12290;&#21482;&#26377;&#38145;&#23450;&#30340;&#26799;&#24230;&#34987;&#20256;&#36755;&#21040;&#21442;&#25968;&#26381;&#21153;&#22120;&#36827;&#34892;&#20840;&#23616;&#27169;&#22411;&#32858;&#21512;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#23398;&#20064;&#26041;&#27861;&#23545;&#26799;&#24230;&#27844;&#38706;&#25915;&#20987;&#20855;&#26377;&#25269;&#25239;&#21147;&#65292;&#24182;&#19988;&#25152;&#35774;&#35745;&#21644;&#35757;&#32451;&#30340;&#23494;&#38053;&#38145;&#27169;&#22359;&#21487;&#20197;&#30830;&#20445;&#65292;&#27809;&#26377;&#23494;&#38053;&#38145;&#27169;&#22359;&#30340;&#31169;&#26377;&#20449;&#24687;&#65306;a) &#26080;&#27861;&#20174;&#20849;&#20139;&#30340;&#26799;&#24230;&#20013;&#37325;&#24314;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a widely adopted privacy-preserving machine learning approach where private data remains local, enabling secure computations and the exchange of local model gradients between local clients and third-party parameter servers. However, recent findings reveal that privacy may be compromised and sensitive information potentially recovered from shared gradients. In this study, we offer detailed analysis and a novel perspective on understanding the gradient leakage problem. These theoretical works lead to a new gradient leakage defense technique that secures arbitrary model architectures using a private key-lock module. Only the locked gradient is transmitted to the parameter server for global model aggregation. Our proposed learning method is resistant to gradient leakage attacks, and the key-lock module is designed and trained to ensure that, without the private information of the key-lock module: a) reconstructing private training data from the shared gradient is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#38376;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.16955</link><description>&lt;p&gt;
&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#38376;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Generative Modeling Approach Using Quantum Gates. (arXiv:2303.16955v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#38376;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#20123;&#24180;&#26469;&#65292;&#37327;&#23376;&#35745;&#31639;&#20316;&#20026;&#35299;&#20915;&#22797;&#26434;&#35745;&#31639;&#38382;&#39064;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#65292;&#24320;&#22987;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#29983;&#25104;&#24314;&#27169;&#26159;&#19968;&#31181;&#25216;&#26415;&#65292;&#21487;&#20197;&#35753;&#25105;&#20204;&#23398;&#20064;&#24182;&#29983;&#25104;&#31867;&#20284;&#20110;&#21407;&#25968;&#25454;&#38598;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37327;&#23376;&#38376;&#29983;&#25104;&#26032;&#26679;&#26412;&#30340;&#29983;&#25104;&#24314;&#27169;&#26041;&#27861;&#12290;&#25105;&#20204;&#20174;&#31616;&#35201;&#20171;&#32461;&#37327;&#23376;&#35745;&#31639;&#21644;&#29983;&#25104;&#24314;&#27169;&#24320;&#22987;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#21040;&#23558;&#25968;&#25454;&#38598;&#32534;&#30721;&#25104;&#37327;&#23376;&#24577;&#65292;&#24182;&#20351;&#29992;&#37327;&#23376;&#38376;&#26469;&#25805;&#20316;&#36825;&#20123;&#29366;&#24577;&#20197;&#29983;&#25104;&#26032;&#26679;&#26412;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25968;&#23398;&#32454;&#33410;&#65292;&#24182;&#36890;&#36807;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26469;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, quantum computing has emerged as a promising technology for solving complex computational problems. Generative modeling is a technique that allows us to learn and generate new data samples similar to the original dataset. In this paper, we propose a generative modeling approach using quantum gates to generate new samples from a given dataset. We start with a brief introduction to quantum computing and generative modeling. Then, we describe our proposed approach, which involves encoding the dataset into quantum states and using quantum gates to manipulate these states to generate new samples. We also provide mathematical details of our approach and demonstrate its effectiveness through experimental results on various datasets.
&lt;/p&gt;</description></item></channel></rss>