<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.16776</link><description>&lt;p&gt;
Diff-Def: &#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#30340;&#24418;&#21464;&#22330;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16776
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#22270;&#35889;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#21475;&#20998;&#26512;&#12290;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#38024;&#23545;&#36890;&#36807;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#25110;&#30149;&#29702;&#23398;&#65289;&#23450;&#20041;&#30340;&#29305;&#23450;&#23376;&#20154;&#21475;&#65292;&#24182;&#20801;&#35768;&#30740;&#31350;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#24418;&#24577;&#23398;&#24046;&#24322;&#31561;&#32454;&#31890;&#24230;&#35299;&#21078;&#23398;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#37197;&#20934;&#30340;&#26041;&#27861;&#25110;&#29983;&#25104;&#27169;&#22411;&#65292;&#21069;&#32773;&#26080;&#27861;&#22788;&#29702;&#22823;&#30340;&#35299;&#21078;&#23398;&#21464;&#24322;&#65292;&#21518;&#32773;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#19981;&#31283;&#23450;&#21644;&#24187;&#35273;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#20010;&#24120;&#35268;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#20195;&#34920;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#12290;&#36890;&#36807;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#24182;&#23558;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#27880;&#20876;&#21040;&#19968;&#32452;&#22270;&#20687;&#38468;&#36817;&#65292;&#25105;&#20204;&#30830;&#20445;&#32467;&#26500;&#30340;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#30452;&#25509;&#22270;&#20687;&#21512;&#25104;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16776v1 Announce Type: cross  Abstract: Anatomical atlases are widely used for population analysis. Conditional atlases target a particular sub-population defined via certain conditions (e.g. demographics or pathologies) and allow for the investigation of fine-grained anatomical differences - such as morphological changes correlated with age. Existing approaches use either registration-based methods that are unable to handle large anatomical variations or generative models, which can suffer from training instabilities and hallucinations. To overcome these limitations, we use latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. By generating a deformation field and registering the conditional atlas to a neighbourhood of images, we ensure structural plausibility and avoid hallucinations, which can occur during direct image synthesis. We compare our method to several state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.14606</link><description>&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#35201;&#32032;
&lt;/p&gt;
&lt;p&gt;
The Elements of Differentiable Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14606
&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#32534;&#31243;&#26159;&#19968;&#20010;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#65292;&#20351;&#24471;&#22797;&#26434;&#31243;&#24207;&#33021;&#22815;&#31471;&#23545;&#31471;&#22320;&#36827;&#34892;&#24494;&#20998;&#65292;&#23454;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#21442;&#25968;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26368;&#36817;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#36825;&#24471;&#30410;&#20110;&#22823;&#22411;&#27169;&#22411;&#12289;&#24222;&#22823;&#25968;&#25454;&#38598;&#12289;&#21152;&#36895;&#30828;&#20214;&#65292;&#20197;&#21450;&#21487;&#24494;&#20998;&#32534;&#31243;&#30340;&#21464;&#38761;&#24615;&#21147;&#37327;&#12290;&#36825;&#31181;&#26032;&#30340;&#32534;&#31243;&#33539;&#24335;&#20351;&#22797;&#26434;&#35745;&#31639;&#26426;&#31243;&#24207;&#65288;&#21253;&#25324;&#20855;&#26377;&#25511;&#21046;&#27969;&#21644;&#25968;&#25454;&#32467;&#26500;&#30340;&#31243;&#24207;&#65289;&#33021;&#22815;&#36827;&#34892;&#31471;&#23545;&#31471;&#30340;&#24494;&#20998;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#31243;&#24207;&#21442;&#25968;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#12290;&#19981;&#20165;&#20165;&#26159;&#31243;&#24207;&#30340;&#24494;&#20998;&#65292;&#21487;&#24494;&#20998;&#32534;&#31243;&#20063;&#21253;&#25324;&#20102;&#31243;&#24207;&#20248;&#21270;&#12289;&#27010;&#29575;&#31561;&#22810;&#20010;&#39046;&#22495;&#30340;&#27010;&#24565;&#12290;&#26412;&#20070;&#20171;&#32461;&#20102;&#21487;&#24494;&#20998;&#32534;&#31243;&#25152;&#38656;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24182;&#37319;&#29992;&#20102;&#20248;&#21270;&#21644;&#27010;&#29575;&#20004;&#20010;&#20027;&#35201;&#35270;&#35282;&#36827;&#34892;&#38416;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14606v1 Announce Type: new  Abstract: Artificial intelligence has recently experienced remarkable advances, fueled by large models, vast datasets, accelerated hardware, and, last but not least, the transformative power of differentiable programming. This new programming paradigm enables end-to-end differentiation of complex computer programs (including those with control flows and data structures), making gradient-based optimization of program parameters possible. As an emerging paradigm, differentiable programming builds upon several areas of computer science and applied mathematics, including automatic differentiation, graphical models, optimization and statistics. This book presents a comprehensive review of the fundamental concepts useful for differentiable programming. We adopt two main perspectives, that of optimization and that of probability, with clear analogies between the two. Differentiable programming is not merely the differentiation of programs, but also the t
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.12029</link><description>&lt;p&gt;
&#23545;&#40784;&#19982;&#25552;&#28860;&#65306;&#32479;&#19968;&#21644;&#25913;&#36827;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Align and Distill: Unifying and Improving Domain Adaptive Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12029
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;ALDI&#20197;&#21450;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;CFC-DAOD&#65292;&#35299;&#20915;&#20102;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#20013;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#24182;&#25903;&#25345;&#26410;&#26469;&#26041;&#27861;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#26816;&#27979;&#22120;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#20110;&#19982;&#20854;&#35757;&#32451;&#38598;&#19981;&#21516;&#30340;&#25968;&#25454;&#12290;&#26368;&#36817;&#65292;&#39046;&#22495;&#33258;&#36866;&#24212;&#30446;&#26631;&#26816;&#27979;&#65288;DAOD&#65289;&#26041;&#27861;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#19978;&#30340;&#24378;&#22823;&#32467;&#26524;&#12290;&#36951;&#25022;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#38519;&#38449;&#65292;&#36825;&#20123;&#38519;&#38449;&#23545;&#36807;&#21435;&#30340;&#32467;&#26524;&#25552;&#20986;&#36136;&#30097;&#24182;&#38459;&#30861;&#20102;&#36827;&#19968;&#27493;&#30340;&#36827;&#23637;&#65306;&#65288;a&#65289;&#30001;&#20110;&#22522;&#32447;&#19981;&#36275;&#23548;&#33268;&#24615;&#33021;&#39640;&#20272;&#65292;&#65288;b&#65289;&#19981;&#19968;&#33268;&#30340;&#23454;&#29616;&#23454;&#36341;&#38459;&#27490;&#20102;&#26041;&#27861;&#30340;&#36879;&#26126;&#27604;&#36739;&#65292;&#65288;c&#65289;&#30001;&#20110;&#36807;&#26102;&#30340;&#39592;&#24178;&#21644;&#22522;&#20934;&#27979;&#35797;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#23548;&#33268;&#32570;&#20047;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;&#20197;&#19979;&#38382;&#39064;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19968;&#20010;&#32479;&#19968;&#30340;&#22522;&#20934;&#27979;&#35797;&#21644;&#23454;&#29616;&#26694;&#26550;&#65292;Align and Distill&#65288;ALDI&#65289;&#65292;&#25903;&#25345;DAOD&#26041;&#27861;&#30340;&#27604;&#36739;&#24182;&#25903;&#25345;&#26410;&#26469;&#21457;&#23637;&#65292;&#65288;2&#65289;&#19968;&#20010;&#20844;&#24179;&#19988;&#29616;&#20195;&#30340;DAOD&#35757;&#32451;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#35299;&#20915;&#20102;&#22522;&#20934;&#27979;&#35797;&#30340;&#38519;&#38449;&#65292;&#65288;3&#65289;&#19968;&#20010;&#26032;&#30340;DAOD&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;CFC-DAOD&#65292;&#33021;&#22815;&#22312;&#22810;&#26679;&#21270;&#30340;&#30495;&#23454;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12029v1 Announce Type: cross  Abstract: Object detectors often perform poorly on data that differs from their training set. Domain adaptive object detection (DAOD) methods have recently demonstrated strong results on addressing this challenge. Unfortunately, we identify systemic benchmarking pitfalls that call past results into question and hamper further progress: (a) Overestimation of performance due to underpowered baselines, (b) Inconsistent implementation practices preventing transparent comparisons of methods, and (c) Lack of generality due to outdated backbones and lack of diversity in benchmarks. We address these problems by introducing: (1) A unified benchmarking and implementation framework, Align and Distill (ALDI), enabling comparison of DAOD methods and supporting future development, (2) A fair and modern training and evaluation protocol for DAOD that addresses benchmarking pitfalls, (3) A new DAOD benchmark dataset, CFC-DAOD, enabling evaluation on diverse real
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35843;&#25972;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.01471</link><description>&lt;p&gt;
&#20445;&#25345;&#30456;&#20851;&#24615;&#65306;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#32479;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Preserving correlations: A statistical method for generating synthetic data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01471
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#33021;&#22815;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20379;&#21487;&#35843;&#25972;&#30340;&#38544;&#31169;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#29983;&#25104;&#20855;&#26377;&#32479;&#35745;&#20195;&#34920;&#24615;&#30340;&#21512;&#25104;&#25968;&#25454;&#12290;&#20027;&#35201;&#30446;&#26631;&#26159;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#20013;&#20445;&#25345;&#21407;&#22987;&#25968;&#25454;&#38598;&#20013;&#23384;&#22312;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21516;&#26102;&#25552;&#20379;&#19968;&#20010;&#33298;&#36866;&#30340;&#38544;&#31169;&#32423;&#21035;&#65292;&#21487;&#20197;&#26681;&#25454;&#29305;&#23450;&#23458;&#25143;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#25105;&#20204;&#35814;&#32454;&#25551;&#36848;&#20102;&#25105;&#20204;&#29992;&#20110;&#20998;&#26512;&#21407;&#22987;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#28857;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#22823;&#22411;&#33021;&#28304;&#30456;&#20851;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#22312;&#23450;&#24615;&#65288;&#20363;&#22914;&#36890;&#36807;&#21487;&#35270;&#21270;&#30456;&#20851;&#24615;&#22270;&#65289;&#21644;&#23450;&#37327;&#65288;&#20197;&#36866;&#24403;&#30340;$\ell^1$&#31867;&#22411;&#35823;&#24046;&#33539;&#25968;&#20316;&#20026;&#35780;&#20272;&#25351;&#26631;&#65289;&#26041;&#38754;&#33719;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#26159;&#19968;&#33324;&#30340;&#65292;&#19981;&#20381;&#36182;&#20110;&#20351;&#29992;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26399;&#26395;&#23427;&#21487;&#36866;&#29992;&#20110;&#27604;&#27492;&#22788;&#25351;&#31034;&#30340;&#26356;&#24191;&#27867;&#30340;&#24773;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01471v1 Announce Type: new  Abstract: We propose a method to generate statistically representative synthetic data. The main goal is to be able to maintain in the synthetic dataset the correlations of the features present in the original one, while offering a comfortable privacy level that can be eventually tailored on specific customer demands.   We describe in detail our algorithm used both for the analysis of the original dataset and for the generation of the synthetic data points. The approach is tested using a large energy-related dataset. We obtain good results both qualitatively (e.g. via vizualizing correlation maps) and quantitatively (in terms of suitable $\ell^1$-type error norms used as evaluation metrics).   The proposed methodology is general in the sense that it does not rely on the used test dataset. We expect it to be applicable in a much broader context than indicated here.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2212.04672</link><description>&lt;p&gt;
Primal Dual Alternating Proximal Gradient&#31639;&#27861;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Primal Dual Alternating Proximal Gradient Algorithms for Nonsmooth Nonconvex Minimax Problems with Coupled Linear Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.04672
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20809;&#28369;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#20004;&#31181;&#31639;&#27861;&#65292;&#20998;&#21035;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#36817;&#24180;&#26469;&#22312;&#26426;&#22120;&#23398;&#20064;&#12289;&#20449;&#21495;&#22788;&#29702;&#21644;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35299;&#20915;&#38750;&#20809;&#28369;&#38750;&#20984;&#65288;&#24378;&#65289;&#20985;&#21644;&#38750;&#20984;&#32447;&#24615;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#21407;&#22987;&#23545;&#20598;&#20132;&#26367;&#36817;&#31471;&#26799;&#24230;&#65288;PDAPG&#65289;&#31639;&#27861;&#21644;&#21407;&#22987;&#23545;&#20598;&#36817;&#31471;&#26799;&#24230;&#65288;PDPG-L&#65289;&#31639;&#27861;&#65292;&#20998;&#21035;&#29992;&#20110;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#24773;&#20917;&#12290;&#36825;&#20004;&#31181;&#31639;&#27861;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#35777;&#26126;&#20026; $\mathcal{O}\left( \varepsilon ^{-2} \right)$ &#65288;&#23545;&#24212; $\mathcal{O}\left( \varepsilon ^{-4} \right)$&#65289;&#22312;&#38750;&#20984;&#24378;&#20985; &#65288;&#23545;&#24212;&#38750;&#20984;&#20985;&#65289;&#24773;&#20917;&#19979;&#65292;&#20197;&#21450; $\mathcal{O}\left( \varepsilon ^{-3} \right)$ &#22312;&#38750;&#20984;&#32447;&#24615;&#24773;&#20917;&#19979;&#65292;&#20998;&#21035;&#36798;&#21040; $\varepsilon$-&#31283;&#24577;&#28857;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23427;&#20204;&#26159;&#29992;&#20110;&#35299;&#20915;&#20855;&#26377;&#32806;&#21512;&#32447;&#24615;&#32422;&#26463;&#30340;&#38750;&#20984;&#26497;&#23567;&#26497;&#22823;&#38382;&#39064;&#30340;&#31532;&#19968;&#25209;&#20855;&#26377;&#36845;&#20195;&#22797;&#26434;&#24230;&#20445;&#35777;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.04672v3 Announce Type: replace-cross  Abstract: Nonconvex minimax problems have attracted wide attention in machine learning, signal processing and many other fields in recent years. In this paper, we propose a primal-dual alternating proximal gradient (PDAPG) algorithm and a primal-dual proximal gradient (PDPG-L) algorithm for solving nonsmooth nonconvex-(strongly) concave and nonconvex-linear minimax problems with coupled linear constraints, respectively. The iteration complexity of the two algorithms are proved to be $\mathcal{O}\left( \varepsilon ^{-2} \right)$ (resp. $\mathcal{O}\left( \varepsilon ^{-4} \right)$) under nonconvex-strongly concave (resp. nonconvex-concave) setting and $\mathcal{O}\left( \varepsilon ^{-3} \right)$ under nonconvex-linear setting to reach an $\varepsilon$-stationary point, respectively. To our knowledge, they are the first two algorithms with iteration complexity guarantees for solving the nonconvex minimax problems with coupled linear const
&lt;/p&gt;</description></item><item><title>Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.16316</link><description>&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#65306;&#23545;&#29305;&#24449;&#32452;&#30340;&#24544;&#23454;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts Models: Faithful Attributions for Groups of Features. (arXiv:2310.16316v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16316
&lt;/p&gt;
&lt;p&gt;
Sum-of-Parts&#27169;&#22411;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#29305;&#24449;&#32452;&#24402;&#22240;&#30340;&#24544;&#23454;&#24615;&#65292;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#20934;&#30830;&#21453;&#26144;&#20102;&#20854;&#20915;&#31574;&#36807;&#31243;&#65292;&#21017;&#34987;&#35748;&#20026;&#26159;&#8220;&#24544;&#23454;&#8221;&#30340;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20363;&#22914;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#24402;&#22240;&#31561;&#35299;&#37322;&#24182;&#19981;&#33021;&#20445;&#35777;&#24544;&#23454;&#65292;&#26377;&#21487;&#33021;&#20135;&#29983;&#20855;&#26377;&#35823;&#23548;&#24615;&#30340;&#35299;&#37322;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Sum-of-Parts&#65288;SOP&#65289;&#27169;&#22411;&#65292;&#23427;&#26159;&#19968;&#31867;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#20855;&#26377;&#36890;&#36807;&#26500;&#36896;&#20445;&#35777;&#24544;&#23454;&#30340;&#29305;&#24449;&#32452;&#24402;&#22240;&#12290;&#35813;&#27169;&#22411;&#23558;&#39044;&#27979;&#20998;&#35299;&#20026;&#21487;&#35299;&#37322;&#30340;&#20998;&#25968;&#20043;&#21644;&#65292;&#27599;&#20010;&#20998;&#25968;&#30452;&#25509;&#24402;&#22240;&#20110;&#19968;&#32452;&#31232;&#30095;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#21487;&#35299;&#37322;&#24615;&#25351;&#26631;&#23545;SOP&#36827;&#34892;&#35780;&#20272;&#65292;&#24182;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#21033;&#29992;SOP&#25552;&#20379;&#30340;&#24544;&#23454;&#35299;&#37322;&#24110;&#21161;&#22825;&#20307;&#29289;&#29702;&#23398;&#23478;&#21457;&#29616;&#20102;&#20851;&#20110;&#26143;&#31995;&#24418;&#25104;&#30340;&#26032;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
An explanation of a machine learning model is considered "faithful" if it accurately reflects the model's decision-making process. However, explanations such as feature attributions for deep learning are not guaranteed to be faithful, and can produce potentially misleading interpretations. In this work, we develop Sum-of-Parts (SOP), a class of models whose predictions come with grouped feature attributions that are faithful-by-construction. This model decomposes a prediction into an interpretable sum of scores, each of which is directly attributable to a sparse group of features. We evaluate SOP on benchmarks with standard interpretability metrics, and in a case study, we use the faithful explanations from SOP to help astrophysicists discover new knowledge about galaxy formation.
&lt;/p&gt;</description></item><item><title>DF2&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#19977;&#20010;&#29942;&#39048;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.05889</link><description>&lt;p&gt;
DF2: &#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DF2: Distribution-Free Decision-Focused Learning. (arXiv:2308.05889v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05889
&lt;/p&gt;
&lt;p&gt;
DF2&#26159;&#19968;&#31181;&#26080;&#20998;&#24067;&#30340;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#35299;&#20915;&#20102;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#19977;&#20010;&#29942;&#39048;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#65288;DFL&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#39044;&#27979;-&#20248;&#21270;&#38382;&#39064;&#26102;&#65292;&#36890;&#36807;&#23558;&#39044;&#27979;&#27169;&#22411;&#23450;&#21046;&#21040;&#19968;&#20010;&#19979;&#28216;&#20248;&#21270;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;DFL&#26041;&#27861;&#21463;&#21040;&#19977;&#20010;&#37325;&#35201;&#29942;&#39048;&#30340;&#21046;&#32422;&#65306;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#12289;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#21644;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#12290;&#27169;&#22411;&#19981;&#21305;&#37197;&#38169;&#35823;&#28304;&#20110;&#27169;&#22411;&#21442;&#25968;&#21270;&#30340;&#39044;&#27979;&#20998;&#24067;&#19982;&#30495;&#23454;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21327;&#35843;&#12290;&#26679;&#26412;&#24179;&#22343;&#36924;&#36817;&#35823;&#24046;&#26159;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#26469;&#36817;&#20284;&#26399;&#26395;&#20248;&#21270;&#30446;&#26631;&#26102;&#20135;&#29983;&#30340;&#12290;&#26799;&#24230;&#36924;&#36817;&#35823;&#24046;&#21457;&#29983;&#22312;DFL&#20381;&#38752;KKT&#26465;&#20214;&#36827;&#34892;&#31934;&#30830;&#26799;&#24230;&#35745;&#31639;&#26102;&#65292;&#32780;&#22823;&#22810;&#25968;&#26041;&#27861;&#22312;&#38750;&#20984;&#30446;&#26631;&#20013;&#36817;&#20284;&#26799;&#24230;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;DF2 - &#31532;&#19968;&#20010;&#26126;&#30830;&#35774;&#35745;&#26469;&#35299;&#20915;&#36825;&#19977;&#20010;&#29942;&#39048;&#30340;&#26080;&#20998;&#24067;&#20915;&#31574;&#28966;&#28857;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Decision-focused learning (DFL) has recently emerged as a powerful approach for predict-then-optimize problems by customizing a predictive model to a downstream optimization task. However, existing end-to-end DFL methods are hindered by three significant bottlenecks: model mismatch error, sample average approximation error, and gradient approximation error. Model mismatch error stems from the misalignment between the model's parameterized predictive distribution and the true probability distribution. Sample average approximation error arises when using finite samples to approximate the expected optimization objective. Gradient approximation error occurs as DFL relies on the KKT condition for exact gradient computation, while most methods approximate the gradient for backpropagation in non-convex objectives. In this paper, we present DF2 -- the first \textit{distribution-free} decision-focused learning method explicitly designed to address these three bottlenecks. Rather than depending 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.06213</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#22810;&#31867;&#20998;&#31867;&#40065;&#26834;&#24615;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Twin Parametric Margin Support Vector Machine for Multiclass Classification. (arXiv:2306.06213v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#20351;&#20854;&#26356;&#21152;&#40065;&#26834;&#12290;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21452;&#21442;&#25968;&#36793;&#30028;&#25903;&#25345;&#21521;&#37327;&#26426;(TPMSVM)&#27169;&#22411;&#26469;&#35299;&#20915;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#12290; &#23545;&#20110;&#27599;&#20010;&#31867;&#21035;&#65292;&#25105;&#20204;&#37319;&#29992;&#19968;&#23545;&#21106;&#24179;&#38754;&#30340;&#27169;&#24335;&#26500;&#24314;&#19968;&#20010;&#20998;&#31867;&#22120;&#12290;&#19968;&#26086;&#30830;&#23450;&#20102;&#25152;&#26377;&#20998;&#31867;&#22120;&#65292;&#21017;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#32508;&#21512;&#30340;&#20915;&#31574;&#20989;&#25968;&#12290;&#25105;&#20204;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#20869;&#26680;&#24341;&#36215;&#30340;&#20998;&#31867;&#22120;&#30340;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#25216;&#26415;&#22686;&#24378;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290; &#21021;&#27493;&#30340;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we present a Twin Parametric-Margin Support Vector Machine (TPMSVM) model to tackle the problem of multiclass classification. In the spirit of one-versus-all paradigm, for each class we construct a classifier by solving a TPMSVM-type model. Once all classifiers have been determined, they are combined into an aggregate decision function. We consider the cases of both linear and nonlinear kernel-induced classifiers. In addition, we robustify the proposed approach through robust optimization techniques. Indeed, in real-world applications observations are subject to measurement errors and noise, affecting the quality of the solutions. Consequently, data uncertainties need to be included within the model in order to prevent low accuracies in the classification process. Preliminary computational experiments on real-world datasets show the good performance of the proposed approach.
&lt;/p&gt;</description></item></channel></rss>