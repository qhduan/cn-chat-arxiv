<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#23376;&#32593;&#32476;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#24615;&#33021;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2309.13018</link><description>&lt;p&gt;
&#21160;&#24577;ASR&#36335;&#24452;&#65306;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#29992;&#20110;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#30340;&#39640;&#25928;&#20462;&#21098;
&lt;/p&gt;
&lt;p&gt;
Dynamic ASR Pathways: An Adaptive Masking Approach Towards Efficient Pruning of A Multilingual ASR Model. (arXiv:2309.13018v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13018
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#22320;&#21387;&#32553;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#21160;&#24577;&#36866;&#24212;&#23376;&#32593;&#32476;&#32467;&#26500;&#65292;&#33021;&#22815;&#22312;&#20943;&#23569;&#24615;&#33021;&#25439;&#22833;&#30340;&#24773;&#20917;&#19979;&#24471;&#21040;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20462;&#21098;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#24615;&#33021;&#25439;&#22833;&#26368;&#23567;&#30340;&#24773;&#20917;&#19979;&#21387;&#32553;&#22810;&#35821;&#31181;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#23545;&#27599;&#31181;&#35821;&#35328;&#36816;&#34892;&#22810;&#36718;&#20462;&#21098;&#21644;&#37325;&#26032;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#25513;&#34109;&#26041;&#27861;&#65292;&#20197;&#20004;&#31181;&#22330;&#26223;&#39640;&#25928;&#22320;&#20462;&#21098;&#22810;&#35821;&#31181;ASR&#27169;&#22411;&#65292;&#20998;&#21035;&#24471;&#21040;&#20102;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#25110;&#31232;&#30095;&#30340;&#22810;&#35821;&#31181;&#27169;&#22411;&#65288;&#31216;&#20026;&#21160;&#24577;ASR&#36335;&#24452;&#65289;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21160;&#24577;&#22320;&#36866;&#24212;&#23376;&#32593;&#32476;&#65292;&#36991;&#20813;&#23545;&#22266;&#23450;&#30340;&#23376;&#32593;&#32476;&#32467;&#26500;&#36827;&#34892;&#36807;&#26089;&#20915;&#31574;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#38024;&#23545;&#31232;&#30095;&#30340;&#21333;&#35821;&#31181;&#27169;&#22411;&#26102;&#20248;&#20110;&#29616;&#26377;&#30340;&#20462;&#21098;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35828;&#26126;&#20102;&#21160;&#24577;ASR&#36335;&#24452;&#36890;&#36807;&#33258;&#19981;&#21516;&#30340;&#23376;&#32593;&#32476;&#21021;&#22987;&#21270;&#36827;&#34892;&#35843;&#25972;&#65292;&#20849;&#21516;&#21457;&#29616;&#21644;&#35757;&#32451;&#26356;&#22909;&#30340;&#21333;&#19968;&#22810;&#35821;&#31181;&#27169;&#22411;&#30340;&#23376;&#32593;&#32476;&#65288;&#36335;&#24452;&#65289;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#23545;&#29305;&#23450;&#35821;&#35328;&#36827;&#34892;&#20462;&#21098;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network pruning offers an effective method for compressing a multilingual automatic speech recognition (ASR) model with minimal performance loss. However, it entails several rounds of pruning and re-training needed to be run for each language. In this work, we propose the use of an adaptive masking approach in two scenarios for pruning a multilingual ASR model efficiently, each resulting in sparse monolingual models or a sparse multilingual model (named as Dynamic ASR Pathways). Our approach dynamically adapts the sub-network, avoiding premature decisions about a fixed sub-network structure. We show that our approach outperforms existing pruning methods when targeting sparse monolingual models. Further, we illustrate that Dynamic ASR Pathways jointly discovers and trains better sub-networks (pathways) of a single multilingual model by adapting from different sub-network initializations, thereby reducing the need for language-specific pruning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#30340;&#19977;&#20010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#20419;&#36827;&#35757;&#32451;&#31639;&#27861;&#25928;&#29575;&#30340;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.07179</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Neural Network Training Algorithms. (arXiv:2306.07179v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07179
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#31639;&#27861;&#22522;&#20934;&#27979;&#35797;&#20013;&#23384;&#22312;&#30340;&#19977;&#20010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#20419;&#36827;&#35757;&#32451;&#31639;&#27861;&#25928;&#29575;&#30340;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#31639;&#27861;&#26159;&#27599;&#20010;&#28145;&#24230;&#23398;&#20064;&#27969;&#31243;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#25552;&#39640;&#35757;&#32451;&#31639;&#27861;&#30340;&#25928;&#29575;&#21487;&#20197;&#33410;&#30465;&#26102;&#38388;&#12289;&#35745;&#31639;&#36164;&#28304;&#65292;&#24182;&#24102;&#26469;&#26356;&#22909;&#12289;&#26356;&#20934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30446;&#21069;&#36824;&#26080;&#27861;&#21487;&#38752;&#22320;&#30830;&#23450;&#26368;&#20808;&#36827;&#30340;&#35757;&#32451;&#31639;&#27861;&#12290;&#26412;&#25991;&#36890;&#36807;&#20855;&#20307;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#21152;&#36895;&#35757;&#32451;&#30340;&#30495;&#27491;&#36827;&#23637;&#38656;&#35201;&#35299;&#20915;&#19977;&#20010;&#22522;&#26412;&#25361;&#25112;&#65306;&#22914;&#20309;&#30830;&#23450;&#35757;&#32451;&#20309;&#26102;&#32467;&#26463;&#24182;&#31934;&#30830;&#27979;&#37327;&#35757;&#32451;&#26102;&#38388;&#65292;&#22914;&#20309;&#22788;&#29702;&#27979;&#37327;&#23545;&#30830;&#20999;&#24037;&#20316;&#36127;&#36733;&#35814;&#24773;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#20844;&#24179;&#27604;&#36739;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#31639;&#27861;&#12290;&#20026;&#20102;&#22686;&#21152;&#23545;&#35757;&#32451;&#31639;&#27861;&#25928;&#29575;&#30340;&#20102;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35774;&#35745;&#20102;&#19968;&#20123;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training algorithms, broadly construed, are an essential part of every deep learning pipeline. Training algorithm improvements that speed up training across a wide variety of workloads (e.g., better update rules, tuning protocols, learning rate schedules, or data selection schemes) could save time, save computational resources, and lead to better, more accurate, models. Unfortunately, as a community, we are currently unable to reliably identify training algorithm improvements, or even determine the state-of-the-art training algorithm. In this work, using concrete experiments, we argue that real progress in speeding up training requires new benchmarks that resolve three basic challenges faced by empirical comparisons of training algorithms: (1) how to decide when training is complete and precisely measure training time, (2) how to handle the sensitivity of measurements to exact workload details, and (3) how to fairly compare algorithms that require hyperparameter tuning. In order to add
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20132;&#26367;&#20027;&#20307;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#27714;&#35299;&#21644;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.17992</link><description>&lt;p&gt;
&#19968;&#31181;&#24555;&#36895;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A fast Multiplicative Updates algorithm for Non-negative Matrix Factorization. (arXiv:2303.17992v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#20132;&#26367;&#20027;&#20307;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#36739;&#24555;&#30340;&#27714;&#35299;&#21644;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#36924;&#36817;&#31934;&#24230;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#26080;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#21487;&#20197;&#23558;&#25968;&#25454;&#30697;&#38453;&#20998;&#35299;&#20026;&#26131;&#20110;&#35299;&#37322;&#30340;&#37096;&#20998;&#12290;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#20986;&#29616;&#20102;&#35768;&#22810;&#31639;&#27861;&#65292;&#20854;&#20013;&#19968;&#31181;&#24191;&#20026;&#20154;&#30693;&#30340;&#26041;&#27861;&#26159;&#30001;&#26446;&#39134;&#39134;&#21644;&#25165;&#21326;&#27178;&#28322;&#20110;2002&#24180;&#25552;&#20986;&#30340;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#22312;&#35768;&#22810;&#39046;&#22495;&#34920;&#29616;&#33391;&#22909;&#65292;&#20855;&#26377;&#31616;&#21333;&#26131;&#23454;&#29616;&#21644;&#21487;&#36866;&#24212;&#27969;&#34892;&#21464;&#20307;&#30340;&#29305;&#28857;&#12290;&#26412;&#25991;&#24314;&#35758;&#36890;&#36807;&#20026;&#27599;&#20010;&#26367;&#20195;&#23376;&#38382;&#39064;&#21046;&#20316;&#26356;&#32039;&#23494;&#30340;Hessian&#30697;&#38453;&#30340;&#19978;&#38480;&#26469;&#25913;&#36827;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#65292;&#24182;&#23558;&#20854;&#35270;&#20026;&#20132;&#26367;&#20027;&#20307;&#21270;&#26368;&#23567;&#21270;&#31639;&#27861;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#23454;&#38469;&#25968;&#25454;&#19978;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#65292;&#25152;&#25552;&#20986;&#30340;fastMU&#31639;&#27861;&#36890;&#24120;&#27604;&#21407;&#22987;&#30340;&#20056;&#27861;&#26356;&#26032;&#31639;&#27861;&#24555;&#25968;&#20493;&#65292;&#21516;&#26102;&#22312;&#36924;&#36817;&#31934;&#24230;&#26041;&#38754;&#23454;&#29616;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#25910;&#25947;&#20173;&#28982;&#24471;&#21040;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nonnegative Matrix Factorization is an important tool in unsupervised machine learning to decompose a data matrix into a product of parts that are often interpretable. Many algorithms have been proposed during the last three decades. A well-known method is the Multiplicative Updates algorithm proposed by Lee and Seung in 2002. Multiplicative updates have many interesting features: they are simple to implement and can be adapted to popular variants such as sparse Nonnegative Matrix Factorization, and, according to recent benchmarks, is state-of-the-art for many problems where the loss function is not the Frobenius norm. In this manuscript, we propose to improve the Multiplicative Updates algorithm seen as an alternating majorization minimization algorithm by crafting a tighter upper bound of the Hessian matrix for each alternate subproblem. Convergence is still ensured and we observe in practice on both synthetic and real world dataset that the proposed fastMU algorithm is often several
&lt;/p&gt;</description></item></channel></rss>