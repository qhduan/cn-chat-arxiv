<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;TSAP&#26041;&#27861;&#26469;&#33258;&#21160;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#31471;&#21040;&#31471;&#30340;&#33258;&#35843;&#33410;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02865</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#33258;&#35843;&#33410;&#33258;&#30417;&#30563;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
End-To-End Self-tuning Self-supervised Time Series Anomaly Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02865
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;TSAP&#26041;&#27861;&#26469;&#33258;&#21160;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#65292;&#20026;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24102;&#26469;&#20102;&#31471;&#21040;&#31471;&#30340;&#33258;&#35843;&#33410;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#22312;&#30417;&#25511;&#29615;&#22659;&#20256;&#24863;&#22120;&#12289;&#34892;&#19994;KPI&#12289;&#24739;&#32773;&#29983;&#29289;&#26631;&#24535;&#29289;&#31561;&#26041;&#38754;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;TSAD&#30340;&#19968;&#20010;&#21452;&#37325;&#25361;&#25112;&#26159;&#38656;&#35201;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#26080;&#30417;&#30563;&#27169;&#22411;&#65292;&#33021;&#22815;&#26816;&#27979;&#21508;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#65288;&#23574;&#23792;&#12289;&#19981;&#36830;&#32493;&#12289;&#36235;&#21183;&#21464;&#21270;&#31561;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TSAP&#26469;&#25191;&#34892;TSA&#8220;&#33258;&#21160;&#39550;&#39542;&#8221;&#65292;&#21487;&#20197;&#31471;&#21040;&#31471;&#33258;&#21160;&#35843;&#25972;&#25968;&#25454;&#22686;&#24378;&#30340;&#36229;&#21442;&#25968;&#65292;&#33258;&#36866;&#24212;&#36873;&#25321;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02865v1 Announce Type: new  Abstract: Time series anomaly detection (TSAD) finds many applications such as monitoring environmental sensors, industry KPIs, patient biomarkers, etc. A two-fold challenge for TSAD is a versatile and unsupervised model that can detect various different types of time series anomalies (spikes, discontinuities, trend shifts, etc.) without any labeled data. Modern neural networks have outstanding ability in modeling complex time series. Self-supervised models in particular tackle unsupervised TSAD by transforming the input via various augmentations to create pseudo anomalies for training. However, their performance is sensitive to the choice of augmentation, which is hard to choose in practice, while there exists no effort in the literature on data augmentation tuning for TSAD without labels. Our work aims to fill this gap. We introduce TSAP for TSA "on autoPilot", which can (self-)tune augmentation hyperparameters end-to-end. It stands on two key c
&lt;/p&gt;</description></item><item><title>MKL&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#22797;&#26434;&#30340;&#30417;&#30563;&#24335;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#31454;&#20105;</title><link>https://arxiv.org/abs/2403.18355</link><description>&lt;p&gt;
&#30417;&#30563;&#22810;&#26680;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#25104;
&lt;/p&gt;
&lt;p&gt;
Supervised Multiple Kernel Learning approaches for multi-omics data integration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18355
&lt;/p&gt;
&lt;p&gt;
MKL&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#26377;&#25928;&#30340;&#22810;&#32452;&#23398;&#25968;&#25454;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#19982;&#22797;&#26434;&#30340;&#30417;&#30563;&#24335;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36890;&#37327;&#25216;&#26415;&#30340;&#36827;&#23637;&#23548;&#33268;&#36234;&#26469;&#36234;&#22810;&#30340;&#32452;&#23398;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#22810;&#31181;&#24322;&#36136;&#25968;&#25454;&#28304;&#30340;&#38598;&#25104;&#30446;&#21069;&#26159;&#29983;&#29289;&#23398;&#21644;&#29983;&#29289;&#20449;&#24687;&#23398;&#39046;&#22495;&#30340;&#19968;&#20010;&#38382;&#39064;&#12290;&#22810;&#26680;&#23398;&#20064;&#65288;MKL&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#28789;&#27963;&#21644;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#32771;&#34385;&#22810;&#32452;&#23398;&#36755;&#20837;&#30340;&#22810;&#26679;&#24615;&#65292;&#23613;&#31649;&#23427;&#22312;&#22522;&#22240;&#32452;&#25968;&#25454;&#25366;&#25496;&#20013;&#26159;&#19968;&#31181;&#19981;&#24120;&#29992;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#19981;&#21516;&#26680;&#34701;&#21512;&#31574;&#30053;&#30340;&#26032;&#39062;MKL&#26041;&#27861;&#12290;&#20026;&#20102;&#20174;&#36755;&#20837;&#26680;&#30340;&#20803;&#26680;&#20013;&#23398;&#20064;&#65292;&#25105;&#20204;&#23558;&#26080;&#30417;&#30563;&#38598;&#25104;&#31639;&#27861;&#35843;&#25972;&#20026;&#25903;&#25345;&#21521;&#37327;&#26426;&#30340;&#30417;&#30563;&#20219;&#21153;&#12290;&#25105;&#20204;&#36824;&#27979;&#35797;&#20102;&#29992;&#20110;&#26680;&#34701;&#21512;&#21644;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;MKL&#30340;&#27169;&#22411;&#21487;&#20197;&#19982;&#26356;&#22797;&#26434;&#12289;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#24335;&#22810;&#32452;&#23398;&#25972;&#21512;&#26041;&#27861;&#31454;&#20105;&#12290;&#22810;&#26680;&#23398;&#20064;&#20026;&#22810;&#32452;&#23398;&#22522;&#22240;&#32452;&#25968;&#25454;&#20013;&#30340;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18355v1 Announce Type: cross  Abstract: Advances in high-throughput technologies have originated an ever-increasing availability of omics datasets. The integration of multiple heterogeneous data sources is currently an issue for biology and bioinformatics. Multiple kernel learning (MKL) has shown to be a flexible and valid approach to consider the diverse nature of multi-omics inputs, despite being an underused tool in genomic data mining.We provide novel MKL approaches based on different kernel fusion strategies.To learn from the meta-kernel of input kernels, we adaptedunsupervised integration algorithms for supervised tasks with support vector machines.We also tested deep learning architectures for kernel fusion and classification.The results show that MKL-based models can compete with more complex, state-of-the-art, supervised multi-omics integrative approaches. Multiple kernel learning offers a natural framework for predictive models in multi-omics genomic data. Our resu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.16970</link><description>&lt;p&gt;
&#32852;&#21512;&#33016;&#37096;X&#20809;&#35786;&#26029;&#21644;&#20020;&#24202;&#35270;&#35273;&#27880;&#24847;&#21147;&#39044;&#27979;&#30340;&#22810;&#38454;&#27573;&#21327;&#20316;&#23398;&#20064;&#65306;&#22686;&#24378;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint chest X-ray diagnosis and clinical visual attention prediction with multi-stage cooperative learning: enhancing interpretability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16970
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#65292;&#36890;&#36807;&#35774;&#35745;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#24182;&#21033;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#26469;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25104;&#20026;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#20915;&#31574;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20020;&#24202;&#37096;&#32626;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#22312;&#36825;&#19968;&#39046;&#22495;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#22312;&#25918;&#23556;&#23398;&#31579;&#26597;&#36807;&#31243;&#20013;&#20020;&#24202;&#21307;&#29983;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#22270;&#20026;&#25552;&#20379;&#37325;&#35201;&#27934;&#23519;&#25552;&#20379;&#20102;&#29420;&#29305;&#30340;&#36164;&#20135;&#65292;&#24182;&#26377;&#21487;&#33021;&#25552;&#39640;&#35745;&#31639;&#36741;&#21161;&#35786;&#26029;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#36825;&#31687;&#35770;&#25991;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#21512;&#30142;&#30149;&#35786;&#26029;&#21644;&#33016;&#37096;X&#20809;&#25195;&#25551;&#23545;&#24212;&#35270;&#35273;&#26174;&#33879;&#24615;&#22270;&#30340;&#39044;&#27979;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32534;&#30721;&#22120;&#22810;&#20219;&#21153;UNet&#65292;&#21033;&#29992;&#20102;DenseNet201&#20027;&#24178;&#21644;&#22522;&#20110;&#27531;&#24046;&#21644;&#33192;&#32960;&#28608;&#21169;&#22359;&#30340;&#32534;&#30721;&#22120;&#26469;&#25552;&#21462;&#29992;&#20110;&#26174;&#33879;&#24615;&#22270;&#39044;&#27979;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#34701;&#21512;&#20998;&#31867;&#22120;&#36827;&#34892;&#30142;&#30149;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16970v1 Announce Type: cross  Abstract: As deep learning has become the state-of-the-art for computer-assisted diagnosis, interpretability of the automatic decisions is crucial for clinical deployment. While various methods were proposed in this domain, visual attention maps of clinicians during radiological screening offer a unique asset to provide important insights and can potentially enhance the quality of computer-assisted diagnosis. With this paper, we introduce a novel deep-learning framework for joint disease diagnosis and prediction of corresponding visual saliency maps for chest X-ray scans. Specifically, we designed a novel dual-encoder multi-task UNet, which leverages both a DenseNet201 backbone and a Residual and Squeeze-and-Excitation block-based encoder to extract diverse features for saliency map prediction, and a multi-scale feature-fusion classifier to perform disease classification. To tackle the issue of asynchronous training schedules of individual tasks
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reduced Label&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#26631;&#31614;&#35774;&#32622;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#38271;&#23614;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#23614;&#37096;&#26679;&#26412;&#30417;&#30563;&#20449;&#24687;&#30340;&#19979;&#38477;&#65292;&#38477;&#20302;&#20102;&#26631;&#31614;&#25104;&#26412;</title><link>https://arxiv.org/abs/2403.16469</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#20943;&#23569;&#26631;&#31614;&#30340;&#38271;&#23614;&#25968;&#25454;&#20013;
&lt;/p&gt;
&lt;p&gt;
Learning from Reduced Labels for Long-Tailed Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16469
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reduced Label&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#26631;&#31614;&#35774;&#32622;&#65292;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#38271;&#23614;&#25968;&#25454;&#65292;&#36991;&#20813;&#20102;&#23614;&#37096;&#26679;&#26412;&#30417;&#30563;&#20449;&#24687;&#30340;&#19979;&#38477;&#65292;&#38477;&#20302;&#20102;&#26631;&#31614;&#25104;&#26412;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#23614;&#25968;&#25454;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#20998;&#31867;&#20219;&#21153;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#20005;&#37325;&#20381;&#36182;&#30417;&#30563;&#20449;&#24687;&#65292;&#36825;&#20351;&#24471;&#27880;&#37322;&#36807;&#31243;&#24322;&#24120;&#32791;&#26102;&#19988;&#36153;&#21147;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20943;&#23569;&#26631;&#27880;&#25104;&#26412;&#26159;&#32531;&#35299;&#26631;&#31614;&#25104;&#26412;&#30340;&#24120;&#35265;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#30340;&#24369;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#20805;&#20998;&#20445;&#30041;&#23614;&#37096;&#26679;&#26412;&#30340;&#30417;&#30563;&#20449;&#24687;&#65292;&#23548;&#33268;&#23614;&#37096;&#31867;&#21035;&#30340;&#20934;&#30830;&#29575;&#19979;&#38477;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Reduced Label&#30340;&#26032;&#22411;&#24369;&#30417;&#30563;&#26631;&#31614;&#35774;&#32622;&#12290;&#25152;&#25552;&#20986;&#30340;&#26631;&#31614;&#35774;&#32622;&#19981;&#20165;&#36991;&#20813;&#20102;&#23614;&#37096;&#26679;&#26412;&#30340;&#30417;&#30563;&#20449;&#24687;&#19979;&#38477;&#65292;&#36824;&#20943;&#23569;&#20102;&#19982;&#38271;&#23614;&#25968;&#25454;&#30456;&#20851;&#30340;&#26631;&#31614;&#25104;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30452;&#35266;&#19988;&#39640;&#25928;&#30340;&#26080;&#20559;&#26694;&#26550;&#65292;&#20855;&#26377;&#24378;&#22823;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#21487;&#20197;&#20174;&#36825;&#20123;Reduced Labels&#20013;&#23398;&#20064;&#12290;&#22312;&#21253;&#25324;Imag&#22312;&#20869;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16469v1 Announce Type: new  Abstract: Long-tailed data is prevalent in real-world classification tasks and heavily relies on supervised information, which makes the annotation process exceptionally labor-intensive and time-consuming. Unfortunately, despite being a common approach to mitigate labeling costs, existing weakly supervised learning methods struggle to adequately preserve supervised information for tail samples, resulting in a decline in accuracy for the tail classes. To alleviate this problem, we introduce a novel weakly supervised labeling setting called Reduced Label. The proposed labeling setting not only avoids the decline of supervised information for the tail samples, but also decreases the labeling costs associated with long-tailed data. Additionally, we propose an straightforward and highly efficient unbiased framework with strong theoretical guarantees to learn from these Reduced Labels. Extensive experiments conducted on benchmark datasets including Imag
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.15112</link><description>&lt;p&gt;
&#20351;&#29992;LLM&#23884;&#20837;&#36827;&#34892;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Text clustering with LLM embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15112
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#23884;&#20837;&#33021;&#22815;&#25429;&#25417;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#65292;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#65292;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32858;&#31867;&#26159;&#32452;&#32455;&#19981;&#26029;&#22686;&#38271;&#30340;&#25968;&#23383;&#20869;&#23481;&#30340;&#37325;&#35201;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#32467;&#26500;&#21270;&#21644;&#21457;&#29616;&#26410;&#20998;&#31867;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#19981;&#21516;&#25991;&#26412;&#23884;&#20837;&#65288;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#20013;&#20351;&#29992;&#30340;&#65289;&#21644;&#32858;&#31867;&#31639;&#27861;&#22914;&#20309;&#24433;&#21709;&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#26041;&#24335;&#12290;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#23454;&#39564;&#20197;&#35780;&#20272;&#23884;&#20837;&#26159;&#22914;&#20309;&#24433;&#21709;&#32858;&#31867;&#32467;&#26524;&#30340;&#65292;&#20197;&#21450;&#36890;&#36807;&#25688;&#35201;&#36827;&#34892;&#38477;&#32500;&#21644;&#23884;&#20837;&#22823;&#23567;&#35843;&#25972;&#30340;&#20316;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;LLM&#23884;&#20837;&#22312;&#25429;&#33719;&#32467;&#26500;&#21270;&#35821;&#35328;&#30340;&#32454;&#24494;&#24046;&#21035;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;BERT&#22312;&#24615;&#33021;&#19978;&#39046;&#20808;&#20110;&#36731;&#37327;&#32423;&#36873;&#39033;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21644;&#25688;&#35201;&#25216;&#26415;&#24182;&#19981;&#19968;&#33268;&#22320;&#25552;&#39640;&#32858;&#31867;&#25928;&#29575;&#65292;&#36825;&#34920;&#26126;&#36825;&#20123;&#31574;&#30053;&#38656;&#35201;&#20180;&#32454;&#20998;&#26512;&#25165;&#33021;&#22312;&#23454;&#38469;&#27169;&#22411;&#20013;&#20351;&#29992;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#20986;&#20102;&#19968;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15112v1 Announce Type: cross  Abstract: Text clustering is an important approach for organising the growing amount of digital content, helping to structure and find hidden patterns in uncategorised data. In this research, we investigated how different textual embeddings - particularly those used in large language models (LLMs) - and clustering algorithms affect how text datasets are clustered. A series of experiments were conducted to assess how embeddings influence clustering results, the role played by dimensionality reduction through summarisation, and embedding size adjustment. Results reveal that LLM embeddings excel at capturing the nuances of structured language, while BERT leads the lightweight options in performance. In addition, we find that increasing embedding dimensionality and summarisation techniques do not uniformly improve clustering efficiency, suggesting that these strategies require careful analysis to use in real-life models. These results highlight a co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#32534;&#30721;&#24067;&#26009;&#27169;&#25311;&#30340;&#29289;&#29702;&#29305;&#24449;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#23454;&#26102;&#27169;&#25311;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26032;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#27979;&#35797;&#34920;&#29616;&#20986;&#19982;&#22522;&#32447;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12820</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#24067;&#26009;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
A Physics-embedded Deep Learning Framework for Cloth Simulation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#30452;&#25509;&#32534;&#30721;&#24067;&#26009;&#27169;&#25311;&#30340;&#29289;&#29702;&#29305;&#24449;&#65292;&#23454;&#29616;&#24555;&#36895;&#21644;&#23454;&#26102;&#27169;&#25311;&#65292;&#24182;&#22312;&#19981;&#20351;&#29992;&#26032;&#25968;&#25454;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#36890;&#36807;&#27979;&#35797;&#34920;&#29616;&#20986;&#19982;&#22522;&#32447;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#32454;&#30340;&#24067;&#26009;&#27169;&#25311;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#25152;&#26399;&#26395;&#30340;&#12290;&#20026;&#25913;&#36827;&#21463;&#21147;&#20132;&#20114;&#12289;&#30896;&#25758;&#22788;&#29702;&#21644;&#25968;&#20540;&#31215;&#20998;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#12290;&#28145;&#24230;&#23398;&#20064;&#26377;&#28508;&#21147;&#23454;&#29616;&#24555;&#36895;&#21644;&#23454;&#26102;&#27169;&#25311;&#65292;&#20294;&#24120;&#35265;&#30340;&#31070;&#32463;&#32593;&#32476;&#32467;&#26500;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#21442;&#25968;&#26469;&#25429;&#33719;&#24067;&#26009;&#21160;&#21147;&#23398;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#32534;&#30721;&#24067;&#26009;&#27169;&#25311;&#29289;&#29702;&#29305;&#24449;&#30340;&#29289;&#29702;&#23884;&#20837;&#23398;&#20064;&#26694;&#26550;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#34920;&#31034;&#36136;&#28857;-&#24377;&#31783;&#31995;&#32479;&#30340;&#31354;&#38388;&#30456;&#20851;&#24615;&#65292;&#20043;&#21518;&#35774;&#35745;&#20102;&#19977;&#20010;&#20998;&#25903;&#26469;&#23398;&#20064;&#24067;&#26009;&#29289;&#29702;&#30340;&#32447;&#24615;&#12289;&#38750;&#32447;&#24615;&#21644;&#26102;&#38388;&#23548;&#25968;&#29305;&#24449;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#36890;&#36807;&#20256;&#32479;&#27169;&#25311;&#22120;&#25110;&#23376;&#31070;&#32463;&#32593;&#32476;&#19982;&#20854;&#20182;&#22806;&#37096;&#21147;&#21644;&#30896;&#25758;&#22788;&#29702;&#36827;&#34892;&#38598;&#25104;&#12290;&#27169;&#22411;&#22312;&#19981;&#20351;&#29992;&#26032;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19981;&#21516;&#30340;&#24067;&#26009;&#21160;&#30011;&#26696;&#20363;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#19982;&#22522;&#32447;&#30340;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12820v1 Announce Type: cross  Abstract: Delicate cloth simulations have long been desired in computer graphics. Various methods were proposed to improve engaged force interactions, collision handling, and numerical integrations. Deep learning has the potential to achieve fast and real-time simulation, but common neural network structures often demand many parameters to capture cloth dynamics. This paper proposes a physics-embedded learning framework that directly encodes physical features of cloth simulation. The convolutional neural network is used to represent spatial correlations of the mass-spring system, after which three branches are designed to learn linear, nonlinear, and time derivate features of cloth physics. The framework can also integrate with other external forces and collision handling through either traditional simulators or sub neural networks. The model is tested across different cloth animation cases, without training with new data. Agreement with baselin
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10182</link><description>&lt;p&gt;
&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10182
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#31609;&#23398;(OR)&#20013;&#65292;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#39046;&#22495;&#30340;&#20986;&#33394;&#24615;&#33021;&#20351;&#20854;&#22312;OR&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;OOD&#25968;&#25454;&#26102;&#65292;NNs&#24448;&#24448;&#20250;&#20570;&#20986;&#33258;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20026;&#33258;&#20449;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#24403;&#36755;&#20986;&#24212;(&#19981;&#24212;)&#34987;&#20449;&#20219;&#26102;&#36827;&#34892;&#36890;&#20449;&#12290;&#22240;&#27492;&#65292;&#22312;OR&#39046;&#22495;&#20013;&#65292;NNs&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#22810;&#20010;&#29420;&#31435;NNs&#32452;&#25104;&#30340;&#28145;&#24230;&#38598;&#21512;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#36824;&#33021;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#30001;&#20110;&#36739;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22522;&#30784;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;NN&#38598;&#25104;&#65292;&#21363;sna
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10182v1 Announce Type: new  Abstract: In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the sna
&lt;/p&gt;</description></item><item><title>AutoGuide&#36890;&#36807;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#29983;&#25104;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#65292;&#20174;&#32780;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20026;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08978</link><description>&lt;p&gt;
AutoGuide: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#30340;&#33258;&#21160;&#29983;&#25104;&#21644;&#36873;&#25321;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
AutoGuide: Automated Generation and Selection of State-Aware Guidelines for Large Language Model Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08978
&lt;/p&gt;
&lt;p&gt;
AutoGuide&#36890;&#36807;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#65292;&#29983;&#25104;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#65292;&#20174;&#32780;&#24357;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#65292;&#20026;&#20195;&#29702;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20027;&#35201;&#23616;&#38480;&#24615;&#26159;&#23427;&#20204;&#23545;&#19990;&#30028;&#30340;&#29702;&#35299;&#21463;&#38480;&#12290;&#36825;&#32473;&#22522;&#20110;LLMs&#30340;&#20195;&#29702;&#24102;&#26469;&#20102;&#37325;&#22823;&#22256;&#38590;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#35757;&#32451;&#30340;LLMs&#32570;&#20047;&#36275;&#22815;&#30693;&#35782;&#30340;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;AutoGuide&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#31163;&#32447;&#32463;&#39564;&#20013;&#30340;&#38544;&#21547;&#30693;&#35782;&#26469;&#24357;&#21512;&#39044;&#35757;&#32451;LLMs&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;AutoGuide&#36890;&#36807;&#25552;&#21462;&#19968;&#32452;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#26377;&#25928;&#22320;&#25552;&#21462;&#23884;&#20837;&#22312;&#31163;&#32447;&#25968;&#25454;&#20013;&#30340;&#30693;&#35782;&#12290;&#27599;&#20010;&#29366;&#24577;&#24863;&#30693;&#25351;&#21335;&#20197;&#31616;&#27905;&#30340;&#33258;&#28982;&#35821;&#35328;&#34920;&#36798;&#65292;&#24182;&#36981;&#24490;&#26465;&#20214;&#32467;&#26500;&#65292;&#28165;&#26224;&#25551;&#36848;&#36866;&#29992;&#30340;&#29366;&#24577;&#12290;&#22240;&#27492;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#25351;&#21335;&#20026;&#21521;&#20195;&#29702;&#24403;&#21069;&#30340;&#20915;&#31574;&#36807;&#31243;&#25552;&#20379;&#26377;&#29992;&#30340;&#30693;&#35782;&#25552;&#20379;&#20102;&#19968;&#31181;&#21407;&#21017;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#39034;&#24207;&#20219;&#21153;&#20013;&#22823;&#24133;&#39046;&#20808;&#20110;&#31454;&#20105;&#30340;&#22522;&#20110;LLMs&#30340;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08978v1 Announce Type: new  Abstract: The primary limitation of large language models (LLMs) is their restricted understanding of the world. This poses significant difficulties for LLM-based agents, particularly in domains where pre-trained LLMs lack sufficient knowledge. In this paper, we introduce a novel framework, called AutoGuide, that bridges the knowledge gap in pre-trained LLMs by leveraging implicit knowledge in offline experiences. Specifically, AutoGuide effectively extracts knowledge embedded in offline data by extracting a set of state-aware guidelines. Importantly, each state-aware guideline is expressed in concise natural language and follows a conditional structure, clearly describing the state where it is applicable. As such, the resulting guidelines enable a principled way to provide helpful knowledge pertinent to an agent's current decision-making process. We show that our approach outperforms competitive LLM-based baselines by a large margin in sequential
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.08802</link><description>&lt;p&gt;
&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Governance of Generative Artificial Intelligence for Companies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#26377;&#20851;&#20225;&#19994;&#20013;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#27835;&#29702;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#65292;&#29305;&#21035;&#26159;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#36805;&#36895;&#36827;&#20837;&#20225;&#19994;&#65292;&#20294;&#32570;&#20047;&#20805;&#20998;&#30340;&#27835;&#29702;&#65292;&#24102;&#26469;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#23613;&#31649;&#23545;GenAI&#20855;&#26377;&#21464;&#38761;&#24615;&#36136;&#21644;&#30417;&#31649;&#25514;&#26045;&#30340;&#24191;&#27867;&#35752;&#35770;&#65292;&#20294;&#26377;&#38480;&#30340;&#30740;&#31350;&#28041;&#21450;&#32452;&#32455;&#27835;&#29702;&#65292;&#21253;&#25324;&#25216;&#26415;&#21644;&#19994;&#21153;&#35270;&#35282;&#12290;&#26412;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#65292;&#35843;&#26597;&#20102;&#26368;&#36817;&#30340;&#30740;&#31350;&#12290;&#23427;&#19981;&#20165;&#20165;&#26159;&#24635;&#32467;&#65292;&#36824;&#36890;&#36807;&#21046;&#23450;&#36866;&#29992;&#20110;&#20225;&#19994;&#20869;&#30340;GenAI&#27835;&#29702;&#26694;&#26550;&#26469;&#36827;&#34892;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#35814;&#32454;&#25551;&#36848;&#20102;&#33539;&#22260;&#12289;&#30446;&#26631;&#21644;&#27835;&#29702;&#26426;&#21046;&#65292;&#26088;&#22312;&#21033;&#29992;&#19994;&#21153;&#26426;&#20250;&#24182;&#20943;&#36731;&#19982;GenAI&#25972;&#21512;&#30456;&#20851;&#39118;&#38505;&#12290;&#35813;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#31181;&#19987;&#27880;&#20110;GenAI&#27835;&#29702;&#30340;&#26041;&#27861;&#65292;&#20026;&#20225;&#19994;&#22312;&#36127;&#36131;&#20219;&#30340;AI&#37319;&#29992;&#25361;&#25112;&#20013;&#25552;&#20379;&#20102;&#23454;&#29992;&#35265;&#35299;&#12290;&#23545;&#20110;&#25216;&#26415;&#20154;&#21592;&#26469;&#35828;&#65292;&#20063;&#26377;&#21161;&#20110;&#25299;&#23485;&#20182;&#20204;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08802v1 Announce Type: new  Abstract: Generative Artificial Intelligence (GenAI), specifically large language models like ChatGPT, has swiftly entered organizations without adequate governance, posing both opportunities and risks. Despite extensive debates on GenAI's transformative nature and regulatory measures, limited research addresses organizational governance, encompassing technical and business perspectives. This review paper fills this gap by surveying recent works. It goes beyond mere summarization by developing a framework for GenAI governance within companies. Our framework outlines the scope, objectives, and governance mechanisms tailored to harness business opportunities and mitigate risks associated with GenAI integration. This research contributes a focused approach to GenAI governance, offering practical insights for companies navigating the challenges of responsible AI adoption. It is also valuable for a technical audience to broaden their perspective as inc
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#26126;&#30830;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#21464;&#25442;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.01946</link><description>&lt;p&gt;
&#19968;&#31181;&#23545;&#31216;&#21464;&#25442;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Generative Model of Symmetry Transformations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01946
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#19968;&#31181;&#29983;&#25104;&#27169;&#22411;&#26469;&#26126;&#30830;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#21464;&#25442;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#25429;&#25417;&#25968;&#25454;&#30340;&#23545;&#31216;&#21464;&#25442;&#21487;&#20197;&#23548;&#33268;&#20855;&#26377;&#24378;&#22823;&#27867;&#21270;&#33021;&#21147;&#30340;&#39640;&#25928;&#27169;&#22411;&#65292;&#23613;&#31649;&#28041;&#21450;&#23545;&#31216;&#24615;&#30340;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20808;&#39564;&#30693;&#35782;&#12290;&#26368;&#36817;&#22312;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#36825;&#20123;&#23545;&#31216;&#24615;&#26041;&#38754;&#24050;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#21028;&#21035;&#35774;&#32622;&#19978;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#26126;&#30830;&#26088;&#22312;&#25429;&#25417;&#25968;&#25454;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#20010;&#20197;&#21487;&#35299;&#37322;&#26041;&#24335;&#23398;&#20064;&#25968;&#25454;&#20013;&#23384;&#22312;&#21738;&#20123;&#23545;&#31216;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#31639;&#27861;&#26469;&#26377;&#25928;&#23398;&#20064;&#25105;&#20204;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#20223;&#23556;&#21644;&#39068;&#33394;&#21464;&#25442;&#19979;&#25429;&#25417;&#23545;&#31216;&#24615;&#30340;&#33021;&#21147;&#12290;&#23558;&#25105;&#20204;&#30340;&#23545;&#31216;&#27169;&#22411;&#19982;&#29616;&#26377;&#30340;&#29983;&#25104;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#36793;&#38469;&#27979;&#35797;&#23545;&#25968;&#20284;&#28982;&#21644;&#23545;&#25968;&#25454;&#31232;&#30095;&#24615;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01946v1 Announce Type: new  Abstract: Correctly capturing the symmetry transformations of data can lead to efficient models with strong generalization capabilities, though methods incorporating symmetries often require prior knowledge. While recent advancements have been made in learning those symmetries directly from the dataset, most of this work has focused on the discriminative setting. In this paper, we construct a generative model that explicitly aims to capture symmetries in the data, resulting in a model that learns which symmetries are present in an interpretable way. We provide a simple algorithm for efficiently learning our generative model and demonstrate its ability to capture symmetries under affine and color transformations. Combining our symmetry model with existing generative models results in higher marginal test-log-likelihoods and robustness to data sparsification.
&lt;/p&gt;</description></item><item><title>&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.16886</link><description>&lt;p&gt;
&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20316;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#30340;&#30740;&#31350;&#65292;&#20197;&#21307;&#30103;&#25968;&#25454;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Using text embedding models and vector databases as text classifiers with the example of medical data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16886
&lt;/p&gt;
&lt;p&gt;
&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;&#20026;&#25991;&#26412;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#25968;&#25454;&#27169;&#24335;&#65292;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#39046;&#22495;&#20013;&#24320;&#22987;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#26159;&#20196;&#20154;&#20852;&#22859;&#30340;&#65292;&#24182;&#24050;&#22312;&#35768;&#22810;&#39046;&#22495;&#25214;&#21040;&#24212;&#29992;&#65292;&#20294;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#26631;&#20934;&#35201;&#27714;&#38750;&#24120;&#39640;&#12290;&#19982;LLMs&#37197;&#21512;&#20351;&#29992;&#65292;&#21521;&#37327;&#23884;&#20837;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#30340;&#26041;&#24335;&#26469;&#34920;&#36798;&#21508;&#31181;&#25968;&#25454;&#27169;&#24335;&#65292;&#36825;&#20123;&#25968;&#25454;&#27169;&#24335;&#23481;&#26131;&#34987;&#20856;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#29702;&#35299;&#12290;&#38500;&#20102;&#26041;&#20415;&#22320;&#21521;&#36825;&#20123;&#21521;&#37327;&#25968;&#25454;&#24211;&#28155;&#21152;&#20449;&#24687;&#12289;&#30693;&#35782;&#21644;&#25968;&#25454;&#22806;&#65292;&#23427;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#20196;&#20154;&#20449;&#26381;&#30340;&#29702;&#30001;&#65292;&#21363;&#23558;&#20854;&#24212;&#29992;&#20110;&#36890;&#24120;&#30001;&#20154;&#31867;&#23436;&#25104;&#30340;&#26816;&#32034;&#20449;&#24687;&#20219;&#21153;&#30340;&#21508;&#31181;&#39046;&#22495;&#12290;Google&#30340;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20102;&#19968;&#20010;&#28165;&#26224;&#30340;&#26367;&#20195;&#27169;&#22411;Med-PaLM&#65292;&#19987;&#38376;&#26088;&#22312;&#19982;&#20020;&#24202;&#21307;&#24072;&#30340;&#21307;&#23398;&#30693;&#35782;&#27700;&#24179;&#21305;&#37197;&#12290;&#22312;&#35757;&#32451;&#20998;&#31867;&#22120;&#21644;&#24320;&#21457;&#27169;&#22411;&#26102;&#65292;&#20445;&#25345;&#20107;&#23454;&#21644;&#20943;&#23569;&#20559;&#35265;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#23884;&#20837;&#27169;&#22411;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16886v1 Announce Type: cross  Abstract: The advent of Large Language Models (LLMs) is promising and has found application in numerous fields, but as it often is with the medical field, the bar is typically quite high [5]. In tandem with LLMs, vector embedding models and vector databases provide a robust way of expressing numerous modes of data that are easily digestible by typical machine learning models. Along with the ease of adding information, knowledge, and data to these vector databases, they provide a compelling reason to apply them in numerous fields where the task of retrieving information is typically done by humans. Researchers at Google have developed a clear alternative model, Med-PaLM [6] specifically designed to match a clinician's level of accuracy when it comes to medical knowledge. When training classifiers, and developing models, it is imperative to maintain factuality and reduce bias [4]. Here, we explore the use of vector databases and embedding models a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#39044;&#22788;&#29702;&#31639;&#27861;&#35782;&#21035;&#23637;&#29616;&#23545;&#31216;&#24615;&#30340;&#27491;&#21017;&#23376;&#36229;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#36229;&#22270;&#22312;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.11339</link><description>&lt;p&gt;
&#36890;&#36807;&#36229;&#22270;&#23545;&#31216;&#24615;&#25171;&#30772;&#36827;&#34892;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Expressive Higher-Order Link Prediction through Hypergraph Symmetry Breaking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11339
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#39044;&#22788;&#29702;&#31639;&#27861;&#35782;&#21035;&#23637;&#29616;&#23545;&#31216;&#24615;&#30340;&#27491;&#21017;&#23376;&#36229;&#22270;&#65292;&#20174;&#32780;&#25552;&#39640;&#36229;&#22270;&#22312;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21306;&#20998;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#36229;&#22270;&#30001;&#19968;&#32452;&#33410;&#28857;&#20197;&#21450;&#31216;&#20026;&#36229;&#36793;&#30340;&#33410;&#28857;&#23376;&#38598;&#21512;&#32452;&#25104;&#12290;&#26356;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#19968;&#20010;&#36229;&#22270;&#20013;&#26159;&#21542;&#23384;&#22312;&#32570;&#22833;&#30340;&#36229;&#36793;&#30340;&#20219;&#21153;&#12290;&#20026;&#39640;&#38454;&#38142;&#25509;&#39044;&#27979;&#23398;&#20064;&#30340;&#36229;&#36793;&#34920;&#31034;&#22312;&#21516;&#26500;&#19979;&#19981;&#22833;&#21435;&#21306;&#20998;&#33021;&#21147;&#26102;&#20855;&#26377;&#23436;&#20840;&#34920;&#36798;&#24615;&#12290;&#35768;&#22810;&#29616;&#26377;&#30340;&#36229;&#22270;&#34920;&#31034;&#23398;&#20064;&#22120;&#21463;&#21040;&#24191;&#20041;Weisfeiler Lehman-1&#65288;GWL-1&#65289;&#31639;&#27861;&#30340;&#34920;&#36798;&#33021;&#21147;&#38480;&#21046;&#65292;&#23427;&#26159;Weisfeiler Lehman-1&#31639;&#27861;&#30340;&#25512;&#24191;&#12290;&#28982;&#32780;&#65292;GWL-1&#30340;&#34920;&#36798;&#33021;&#21147;&#26377;&#38480;&#12290;&#20107;&#23454;&#19978;&#65292;&#20855;&#26377;&#30456;&#21516;GWL-1&#20540;&#33410;&#28857;&#30340;&#35825;&#23548;&#23376;&#36229;&#22270;&#26159;&#26080;&#27861;&#21306;&#20998;&#30340;&#12290;&#27492;&#22806;&#65292;&#22312;&#36229;&#22270;&#19978;&#36827;&#34892;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#24050;&#32463;&#22312;GPU&#20869;&#23384;&#19978;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#21487;&#20197;&#35782;&#21035;&#20986;&#23637;&#29616;&#23545;&#31216;&#24615;&#30340;&#29305;&#23450;&#27491;&#21017;&#23376;&#36229;&#22270;&#30340;&#39044;&#22788;&#29702;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11339v1 Announce Type: new  Abstract: A hypergraph consists of a set of nodes along with a collection of subsets of the nodes called hyperedges. Higher-order link prediction is the task of predicting the existence of a missing hyperedge in a hypergraph. A hyperedge representation learned for higher order link prediction is fully expressive when it does not lose distinguishing power up to an isomorphism. Many existing hypergraph representation learners, are bounded in expressive power by the Generalized Weisfeiler Lehman-1 (GWL-1) algorithm, a generalization of the Weisfeiler Lehman-1 algorithm. However, GWL-1 has limited expressive power. In fact, induced subhypergraphs with identical GWL-1 valued nodes are indistinguishable. Furthermore, message passing on hypergraphs can already be computationally expensive, especially on GPU memory. To address these limitations, we devise a preprocessing algorithm that can identify certain regular subhypergraphs exhibiting symmetry. Our p
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;</title><link>https://arxiv.org/abs/2402.10946</link><description>&lt;p&gt;
&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
CultureLLM: Incorporating Cultural Differences into Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CultureLLM&#30340;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20351;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#26469;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25104;&#21151;&#24494;&#35843;&#24471;&#21040;&#20102;&#28085;&#30422;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;9&#31181;&#25991;&#21270;&#29305;&#23450;LLMs&#20197;&#21450;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25253;&#36947;&#20559;&#21521;&#20110;&#26576;&#20123;&#25991;&#21270;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20027;&#35201;&#26469;&#33258;&#33521;&#35821;&#35821;&#26009;&#24211;&#12290;&#30001;&#20110;&#22810;&#35821;&#31181;&#25991;&#21270;&#25968;&#25454;&#36890;&#24120;&#36739;&#38590;&#25910;&#38598;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#25110;&#29305;&#23450;&#25991;&#21270;&#30340;&#39044;&#35757;&#32451;&#26469;&#22788;&#29702;&#36825;&#19968;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#24573;&#35270;&#20102;&#20302;&#36164;&#28304;&#25991;&#21270;&#30340;&#30693;&#35782;&#32570;&#20047;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CultureLLM&#65292;&#36825;&#26159;&#19968;&#20010;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#23558;&#25991;&#21270;&#24046;&#24322;&#32435;&#20837;LLMs&#20013;&#12290;CultureLLM&#37319;&#29992;&#19990;&#30028;&#20215;&#20540;&#35843;&#26597;&#65288;WVS&#65289;&#20316;&#20026;&#31181;&#23376;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#30340;&#35821;&#20041;&#25968;&#25454;&#22686;&#24378;&#29983;&#25104;&#35821;&#20041;&#31561;&#25928;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#20165;&#20351;&#29992;&#26469;&#33258;WVS&#30340;50&#20010;&#31181;&#23376;&#26679;&#26412;&#21644;&#22686;&#24378;&#25968;&#25454;&#65292;&#25105;&#20204;&#23545;9&#31181;&#21253;&#25324;&#23500;&#35029;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;&#25991;&#21270;&#29305;&#23450;LLMs&#21644;&#19968;&#20010;&#32479;&#19968;&#27169;&#22411;&#65288;CultureLLM-One&#65289;&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#23545;60&#20010;&#19982;&#25991;&#21270;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;CultureLLM&#22312;&#22686;&#24378;LLM&#30340;&#25991;&#21270;&#29305;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10946v1 Announce Type: cross  Abstract: Large language models (LLMs) are reported to be partial to certain cultures owing to the training data dominance from the English corpora. Since multilingual cultural data are often expensive to collect, existing efforts handle this by prompt engineering or culture-specific pre-training. However, they might overlook the knowledge deficiency of low-resource culture and require extensive computing resources. In this paper, we propose CultureLLM, a cost-effective solution to incorporate cultural differences into LLMs. CultureLLM adopts World Value Survey (WVS) as seed data and generates semantically equivalent training data via the proposed semantic data augmentation. Using only 50 seed samples from WVS with augmented data, we fine-tune culture-specific LLMs and one unified model (CultureLLM-One) for 9 cultures covering rich and low-resource languages. Extensive experiments on 60 culture-related datasets demonstrate that CultureLLM signif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Nystr\"om&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#26680;&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#30340;&#22320;&#26631;&#36873;&#25321;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.06763</link><description>&lt;p&gt;
&#20351;&#29992;Nystr\"om&#36817;&#20284;&#30340;&#21487;&#25193;&#23637;&#26680;&#36923;&#36753;&#22238;&#24402;&#65306;&#29702;&#35770;&#20998;&#26512;&#21644;&#31163;&#25955;&#36873;&#25321;&#24314;&#27169;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Scalable Kernel Logistic Regression with Nystr\"om Approximation: Theoretical Analysis and Application to Discrete Choice Modelling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Nystr\"om&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#26680;&#36923;&#36753;&#22238;&#24402;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#30740;&#31350;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#24182;&#39564;&#35777;&#20102;&#19981;&#21516;&#30340;&#22320;&#26631;&#36873;&#25321;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22522;&#20110;&#26680;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#24212;&#29992;&#20110;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#31163;&#25955;&#36873;&#25321;&#24314;&#27169;&#26102;&#65292;&#32463;&#24120;&#38754;&#20020;&#23384;&#20648;&#38656;&#27714;&#21644;&#27169;&#22411;&#20013;&#28041;&#21450;&#30340;&#22823;&#37327;&#21442;&#25968;&#30340;&#25361;&#25112;&#12290;&#36825;&#31181;&#22797;&#26434;&#24615;&#24433;&#21709;&#20102;&#22823;&#35268;&#27169;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;Nystr\"om&#36817;&#20284;&#26041;&#27861;&#35299;&#20915;&#20102;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#30340;&#26680;&#36923;&#36753;&#22238;&#24402;&#12290;&#30740;&#31350;&#39318;&#20808;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#20854;&#20013;&#65306;i) &#23545;KLR&#35299;&#30340;&#38598;&#21512;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;ii) &#32473;&#20986;&#20102;&#20351;&#29992;Nystr\"om&#36817;&#20284;&#30340;KLR&#35299;&#30340;&#19978;&#30028;&#65292;&#24182;&#26368;&#21518;&#25551;&#36848;&#20102;&#19987;&#38376;&#29992;&#20110;Nystr\"om KLR&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#29305;&#21270;&#12290;&#20043;&#21518;&#65292;&#23545;Nystr\"om KLR&#36827;&#34892;&#20102;&#35745;&#31639;&#39564;&#35777;&#12290;&#27979;&#35797;&#20102;&#22235;&#31181;&#22320;&#26631;&#36873;&#25321;&#26041;&#27861;&#65292;&#21253;&#25324;&#22522;&#26412;&#22343;&#21248;&#37319;&#26679;&#12289;k-means&#37319;&#26679;&#31574;&#30053;&#21644;&#22522;&#20110;&#26464;&#26438;&#24471;&#20998;&#30340;&#20004;&#31181;&#38750;&#22343;&#21248;&#26041;&#27861;&#12290;&#36825;&#20123;&#31574;&#30053;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The application of kernel-based Machine Learning (ML) techniques to discrete choice modelling using large datasets often faces challenges due to memory requirements and the considerable number of parameters involved in these models. This complexity hampers the efficient training of large-scale models. This paper addresses these problems of scalability by introducing the Nystr\"om approximation for Kernel Logistic Regression (KLR) on large datasets. The study begins by presenting a theoretical analysis in which: i) the set of KLR solutions is characterised, ii) an upper bound to the solution of KLR with Nystr\"om approximation is provided, and finally iii) a specialisation of the optimisation algorithms to Nystr\"om KLR is described. After this, the Nystr\"om KLR is computationally validated. Four landmark selection methods are tested, including basic uniform sampling, a k-means sampling strategy, and two non-uniform methods grounded in leverage scores. The performance of these strategi
&lt;/p&gt;</description></item><item><title>PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.03687</link><description>&lt;p&gt;
Pard: &#20855;&#26377;&#32622;&#25442;&#19981;&#21464;&#24615;&#30340;&#33258;&#22238;&#24402;&#25193;&#25955;&#29992;&#20110;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Pard: Permutation-Invariant Autoregressive Diffusion for Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03687
&lt;/p&gt;
&lt;p&gt;
PARD&#26159;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#22270;&#20013;&#30340;&#37096;&#20998;&#39034;&#24207;&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#22238;&#24402;&#27169;&#22411;&#23545;&#20110;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#65292;&#20294;&#20854;&#31616;&#21333;&#26377;&#25928;&#65292;&#22312;&#22270;&#29983;&#25104;&#39046;&#22495;&#19968;&#30452;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#22240;&#20854;&#32622;&#25442;&#19981;&#21464;&#24615;&#32780;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#30446;&#21069;&#30340;&#22270;&#25193;&#25955;&#27169;&#22411;&#19968;&#27425;&#24615;&#29983;&#25104;&#22270;&#65292;&#20294;&#38656;&#35201;&#39069;&#22806;&#30340;&#29305;&#24449;&#21644;&#25104;&#21315;&#19978;&#19975;&#27493;&#30340;&#21435;&#22122;&#25165;&#33021;&#36798;&#21040;&#26368;&#20339;&#24615;&#33021;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;PARD&#65292;&#19968;&#31181;&#23558;&#25193;&#25955;&#27169;&#22411;&#19982;&#33258;&#22238;&#24402;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;&#33258;&#22238;&#24402;&#25193;&#25955;&#27169;&#22411;&#12290;PARD&#21033;&#29992;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#25928;&#26524;&#21644;&#25928;&#29575;&#65292;&#21516;&#26102;&#20445;&#25345;&#32622;&#25442;&#19981;&#21464;&#24615;&#65292;&#26080;&#38656;&#20851;&#27880;&#22270;&#30340;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#38598;&#21512;&#19981;&#21516;&#65292;&#22270;&#20013;&#30340;&#20803;&#32032;&#24182;&#19981;&#26159;&#23436;&#20840;&#26080;&#24207;&#30340;&#65292;&#33410;&#28857;&#21644;&#36793;&#26377;&#19968;&#20010;&#29420;&#29305;&#30340;&#37096;&#20998;&#39034;&#24207;&#12290;&#21033;&#29992;&#36825;&#20010;&#37096;&#20998;&#39034;&#24207;&#65292;PARD&#20197;&#22359;&#36880;&#22359;&#30340;&#33258;&#22238;&#24402;&#26041;&#24335;&#29983;&#25104;&#22270;&#65292;&#20854;&#20013;&#27599;&#20010;&#22359;&#30340;&#27010;&#29575;&#20026;c&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#20316;&#20026;&#24402;&#19968;&#21270;&#27969;&#26500;&#36896;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#36890;&#36807;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;&#38382;&#39064;&#21644;&#25968;&#20540;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#26368;&#32456;&#32467;&#26524;&#26377;&#21161;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#12290;</title><link>http://arxiv.org/abs/2311.01404</link><description>&lt;p&gt;
&#20351;&#29992;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#23558;&#24402;&#19968;&#21270;&#27969;&#20316;&#20026;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;
&lt;/p&gt;
&lt;p&gt;
Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs. (arXiv:2311.01404v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01404
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#20316;&#20026;&#24402;&#19968;&#21270;&#27969;&#26500;&#36896;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#36890;&#36807;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;&#38382;&#39064;&#21644;&#25968;&#20540;&#26041;&#26696;&#65292;&#23454;&#29616;&#20102;&#23545;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#30340;&#36817;&#20284;&#12290;&#26368;&#32456;&#32467;&#26524;&#26377;&#21161;&#20110;&#26500;&#24314;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
"&#24402;&#19968;&#21270;&#27969;"&#19968;&#35789;&#19982;&#36890;&#36807;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26500;&#24314;&#27010;&#29575;&#27979;&#24230;&#20043;&#38388;&#30340;&#21487;&#36870;&#20256;&#36755;&#26144;&#23556;&#30456;&#20851;&#12290;&#26412;&#25991;&#32771;&#34385;&#23558;$W_2$-&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;$T$&#24674;&#22797;&#20026;&#32447;&#24615;&#25511;&#21046;&#31070;&#32463;ODE&#30340;&#27969;&#21160;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22312;&#21512;&#36866;&#30340;&#20551;&#35774;&#19979;&#65292;&#23545;&#20110;&#32477;&#23545;&#36830;&#32493;&#27979;&#24230;$\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$&#21644;&#21463;&#25511;&#21521;&#37327;&#22330;&#65292;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;&#21253;&#21547;&#22312;&#31995;&#32479;&#20135;&#29983;&#30340;&#27969;&#21160;&#30340;$C^0_c$&#38381;&#21253;&#20013;&#12290;&#20551;&#35774;&#21407;&#22987;&#27979;&#24230;$\mu,\nu$&#30340;&#31163;&#25955;&#36817;&#20284;$\mu_N,\nu_N$&#21487;&#29992;&#65292;&#25105;&#20204;&#20351;&#29992;&#31163;&#25955;&#26368;&#20248;&#32806;&#21512;$\gamma_N$&#26469;&#23450;&#20041;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#12290;&#36890;&#36807;$\Gamma$-&#25910;&#25947;&#35770;&#35777;&#65292;&#25105;&#20204;&#35777;&#26126;&#20854;&#35299;&#23545;&#24212;&#20110;&#36817;&#20284;&#26368;&#20248;&#20256;&#36755;&#26144;&#23556;$T$&#30340;&#27969;&#21160;&#12290;&#26368;&#21518;&#65292;&#21033;&#29992;Pontryagin&#26368;&#22823;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#25968;&#20540;&#26041;&#26696;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The term "Normalizing Flows" is related to the task of constructing invertible transport maps between probability measures by means of deep neural networks. In this paper, we consider the problem of recovering the $W_2$-optimal transport map $T$ between absolutely continuous measures $\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neural ODE. We first show that, under suitable assumptions on $\mu,\nu$ and on the controlled vector fields, the optimal transport map is contained in the $C^0_c$-closure of the flows generated by the system. Assuming that discrete approximations $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available, we use a discrete optimal coupling $\gamma_N$ to define an optimal control problem. With a $\Gamma$-convergence argument, we prove that its solutions correspond to flows that approximate the optimal transport map $T$. Finally, taking advantage of the Pontryagin Maximum Principle, we propose an iterative numerical scheme for the reso
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#24335;&#25805;&#20316;&#32593;&#32476;&#30340;&#25391;&#21160;&#22768;&#23398;&#39057;&#21709;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20195;&#34920;&#24615;&#25391;&#21160;&#22768;&#23398;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#39057;&#21709;&#27169;&#25311;&#65292;&#26377;&#21161;&#20110;&#35774;&#35745;&#20248;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2310.05469</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#24335;&#25805;&#20316;&#32593;&#32476;&#30340;&#25391;&#21160;&#22768;&#23398;&#39057;&#21709;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vibroacoustic Frequency Response Prediction with Query-based Operator Networks. (arXiv:2310.05469v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05469
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#24335;&#25805;&#20316;&#32593;&#32476;&#30340;&#25391;&#21160;&#22768;&#23398;&#39057;&#21709;&#39044;&#27979;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20195;&#34920;&#24615;&#25391;&#21160;&#22768;&#23398;&#38382;&#39064;&#30340;&#32467;&#26500;&#21270;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#21152;&#36895;&#39057;&#21709;&#27169;&#25311;&#65292;&#26377;&#21161;&#20110;&#35774;&#35745;&#20248;&#21270;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#39134;&#26426;&#12289;&#27773;&#36710;&#21644;&#25151;&#23627;&#31561;&#26426;&#26800;&#32467;&#26500;&#20013;&#30340;&#25391;&#21160;&#22768;&#23398;&#27874;&#20256;&#25773;&#23545;&#20445;&#35777;&#29992;&#25143;&#30340;&#20581;&#24247;&#21644;&#33298;&#36866;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20123;&#31995;&#32479;&#65292;&#35774;&#35745;&#24072;&#21644;&#24037;&#31243;&#24072;&#20027;&#35201;&#32771;&#34385;&#39057;&#22495;&#20013;&#30340;&#21160;&#24577;&#21709;&#24212;&#65292;&#36825;&#36890;&#36807;&#20687;&#26377;&#38480;&#20803;&#26041;&#27861;&#36825;&#26679;&#30340;&#26114;&#36149;&#25968;&#20540;&#27169;&#25311;&#26469;&#35745;&#31639;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#22522;&#20110;&#25968;&#25454;&#30340;&#26367;&#20195;&#27169;&#22411;&#25215;&#35834;&#21152;&#36895;&#36825;&#20123;&#27169;&#25311;&#65292;&#20174;&#32780;&#20419;&#36827;&#35774;&#35745;&#20248;&#21270;&#12289;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;&#31561;&#20219;&#21153;&#30340;&#23454;&#26045;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#22522;&#20934;&#27979;&#35797;&#29992;&#20110;&#20195;&#34920;&#24615;&#25391;&#21160;&#22768;&#23398;&#38382;&#39064;&#65306;&#39044;&#27979;&#24102;&#26377;&#19981;&#21516;&#24418;&#24335;&#38262;&#36793;&#30340;&#25391;&#21160;&#26495;&#30340;&#39057;&#21709;&#12290;&#35813;&#22522;&#20934;&#27979;&#35797;&#21253;&#21547;&#20102;&#20849;&#35745;12,000&#20010;&#26495;&#20960;&#20309;&#24418;&#29366;&#20197;&#21450;&#30456;&#24212;&#30340;&#25968;&#20540;&#35299;&#65292;&#24182;&#24341;&#20837;&#20102;&#35780;&#20272;&#25351;&#26631;&#20197;&#37327;&#21270;&#39044;&#27979;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#39057;&#21709;&#39044;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39057;&#29575;&#26597;&#35810;&#25805;&#20316;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding vibroacoustic wave propagation in mechanical structures like airplanes, cars and houses is crucial to ensure health and comfort of their users. To analyze such systems, designers and engineers primarily consider the dynamic response in the frequency domain, which is computed through expensive numerical simulations like the finite element method. In contrast, data-driven surrogate models offer the promise of speeding up these simulations, thereby facilitating tasks like design optimization, uncertainty quantification, and design space exploration. We present a structured benchmark for a representative vibroacoustic problem: Predicting the frequency response for vibrating plates with varying forms of beadings. The benchmark features a total of 12,000 plate geometries with an associated numerical solution and introduces evaluation metrics to quantify the prediction quality. To address the frequency response prediction task, we propose a novel frequency query operator model, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.02583</link><description>&lt;p&gt;
&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Sequential Volumetric Design Tasks. (arXiv:2309.02583v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39034;&#24207;&#20307;&#31215;&#35774;&#35745;&#20219;&#21153;&#30340;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;transformer&#27169;&#22411;&#20174;&#19987;&#23478;&#30340;&#35774;&#35745;&#24207;&#21015;&#20013;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#26469;&#25552;&#39640;&#33258;&#21160;&#29983;&#25104;&#20307;&#31215;&#35774;&#35745;&#30340;&#36136;&#37327;&#65292;&#20197;&#21450;&#25903;&#25345;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20307;&#31215;&#35774;&#35745;&#65292;&#20063;&#31216;&#20026;&#36136;&#37327;&#35774;&#35745;&#65292;&#26159;&#19987;&#19994;&#24314;&#31569;&#35774;&#35745;&#20013;&#30340;&#31532;&#19968;&#27493;&#20851;&#38190;&#24615;&#20219;&#21153;&#65292;&#20855;&#26377;&#39034;&#24207;&#24615;&#12290;&#30001;&#20110;&#20307;&#31215;&#35774;&#35745;&#36807;&#31243;&#22797;&#26434;&#65292;&#39034;&#24207;&#21270;&#35774;&#35745;&#36807;&#31243;&#20013;&#21253;&#21547;&#20102;&#23545;&#35774;&#35745;&#24072;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#35768;&#22810;&#21162;&#21147;&#24050;&#32463;&#34987;&#25237;&#20837;&#21040;&#33258;&#21160;&#29983;&#25104;&#21512;&#29702;&#30340;&#20307;&#31215;&#35774;&#35745;&#19978;&#65292;&#20294;&#29983;&#25104;&#30340;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#23384;&#22312;&#24046;&#24322;&#65292;&#24182;&#19988;&#35780;&#20272;&#19968;&#20010;&#35774;&#35745;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#38656;&#35201;&#19968;&#22871;&#36807;&#20110;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#35201;&#20040;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#21147;&#19987;&#19994;&#30693;&#35782;&#12290;&#32780;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#23398;&#20064;&#26368;&#32456;&#35774;&#35745;&#65292;&#32780;&#19981;&#26159;&#39034;&#24207;&#35774;&#35745;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19987;&#23478;&#25110;&#39640;&#24615;&#33021;&#35774;&#35745;&#24207;&#21015;&#30340;&#35774;&#35745;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;transformer&#30340;&#27169;&#22411;&#25552;&#21462;&#26377;&#29992;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#25152;&#23398;&#30340;&#34920;&#31034;&#22312;&#20851;&#38190;&#30340;&#19979;&#28216;&#24212;&#29992;&#20013;&#65292;&#22914;&#35774;&#35745;&#20559;&#22909;&#35780;&#20272;&#21644;&#31243;&#24207;&#21270;&#35774;&#35745;&#29983;&#25104;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;prefer
&lt;/p&gt;
&lt;p&gt;
Volumetric design, also called massing design, is the first and critical step in professional building design which is sequential in nature. As the volumetric design process is complex, the underlying sequential design process encodes valuable information for designers. Many efforts have been made to automatically generate reasonable volumetric designs, but the quality of the generated design solutions varies, and evaluating a design solution requires either a prohibitively comprehensive set of metrics or expensive human expertise. While previous approaches focused on learning only the final design instead of sequential design tasks, we propose to encode the design knowledge from a collection of expert or high-performing design sequences and extract useful representations using transformer-based models. Later we propose to utilize the learned representations for crucial downstream applications such as design preference evaluation and procedural design generation. We develop the prefere
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; Vision Transformer &#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#19978;&#36827;&#34892;&#20102;&#26032;&#30340;&#25506;&#31350;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.17235</link><description>&lt;p&gt;
COMCAT&#65306;&#39640;&#25928;&#21387;&#32553;&#21644;&#33258;&#23450;&#20041;&#27880;&#24847;&#21147;&#35270;&#35273;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models. (arXiv:2305.17235v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; Vision Transformer &#21387;&#32553;&#26041;&#27861;&#65292;&#22312;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#19978;&#36827;&#34892;&#20102;&#26032;&#30340;&#25506;&#31350;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#26356;&#23569;&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27169;&#22411;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#20363;&#22914;Vision Transformer&#65288;ViT&#65289;&#21450;&#20854;&#21464;&#20307;&#65292;&#22312;&#21508;&#31181;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26032;&#20852;&#30340;&#26550;&#26500;&#23384;&#22312;&#30528;&#27169;&#22411;&#23610;&#23544;&#22823;&#21644;&#39640;&#35745;&#31639;&#25104;&#26412;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#39640;&#25928;&#30340;&#27169;&#22411;&#21387;&#32553;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25506;&#31350;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#21387;&#32553;&#26041;&#27861;&#65292;&#20197;&#20016;&#23500;&#33719;&#21462;&#32039;&#20945;&#30340;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#35270;&#35273;&#27169;&#22411;&#30340;&#24037;&#20855;&#38598;&#12290;&#22522;&#20110;&#23545;&#22810;&#22836;&#27880;&#24847;&#21147;&#23618;&#30340;&#26032;&#35265;&#35299;&#65292;&#25105;&#20204;&#24320;&#21457;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;ViT&#21387;&#32553;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#21098;&#26525;&#26041;&#27861;&#12290;&#22312;ImageNet&#19978;&#23545;DeiT-small&#21644;DeiT-base&#27169;&#22411;&#36827;&#34892;&#21387;&#32553;&#65292;&#25105;&#20204;&#30340;&#25552;&#35758;&#26041;&#27861;&#21363;&#20351;&#20351;&#29992;&#26356;&#23569;&#30340;&#21442;&#25968;&#65292;&#20173;&#28982;&#33021;&#22815;&#23454;&#29616;&#27604;&#29616;&#26377;&#26041;&#27861;&#39640;0.45&#65285;&#21644;0.76&#65285;&#30340;top-1&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.02109</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;O-RAN&#30340;&#21327;&#21516;&#65306;&#38754;&#21521;&#22810;&#20010;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#24377;&#24615;&#34394;&#25311;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26368;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#21253;&#25324;&#32593;&#32476;&#26465;&#20214;&#30340;&#21160;&#24577;&#24615;&#12289;&#31995;&#32479;&#20013;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;/&#20219;&#21153;&#30340;&#24182;&#23384;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#19982;&#20854;&#20182;&#32593;&#32476;&#26381;&#21153;&#30340;&#24182;&#34892;&#25191;&#34892;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#27867;&#22411;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#26469;&#35299;&#20915;DMS-FL&#20013;&#30340;&#19977;&#20010;&#26410;&#25506;&#32034;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#65292;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#39640;&#20986;55&#65285;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2305.00386</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#30340;&#37325;&#35201;&#24615;&#21152;&#26435;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Importance Weighted Expectation-Maximization for Protein Sequence Design. (arXiv:2305.00386v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00386
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#30456;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#65292;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#39640;&#20986;55&#65285;&#65292;&#24182;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#39046;&#22495;&#65292;&#35774;&#35745;&#20855;&#26377;&#25152;&#38656;&#29983;&#29289;&#21151;&#33021;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#38750;&#24120;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20351;&#29992;&#20195;&#29702;&#24207;&#21015;-&#21151;&#33021;&#27169;&#22411;&#26367;&#20195;&#26114;&#36149;&#30340;&#28287;&#23454;&#39564;&#39564;&#35777;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;IsEM-Pro&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26681;&#25454;&#32473;&#23450;&#30340;&#36866;&#24212;&#24615;&#26631;&#20934;&#29983;&#25104;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;&#23427;&#26159;&#19968;&#20010;&#28508;&#22312;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#21463;&#21040;&#21478;&#22806;&#19968;&#20010;&#23398;&#20064;&#30340;&#39532;&#23572;&#21487;&#22827;&#38543;&#26426;&#22330;&#32467;&#26500;&#29305;&#24449;&#30340;&#22686;&#24378;&#12290;&#30740;&#31350;&#32773;&#20351;&#29992;&#33945;&#29305;&#21345;&#32599;&#26399;&#26395;&#26368;&#22823;&#21270;&#26041;&#27861;&#65288;MCEM&#65289;&#26469;&#23398;&#20064;&#36825;&#20010;&#27169;&#22411;&#12290;&#22312;&#25512;&#29702;&#26399;&#38388;&#65292;&#20174;&#20854;&#28508;&#22312;&#31354;&#38388;&#37319;&#26679;&#21487;&#20197;&#22686;&#21152;&#22810;&#26679;&#24615;&#65292;&#32780;&#20854;MRF&#29305;&#24449;&#21017;&#25351;&#23548;&#20102;&#25506;&#32034;&#39640;&#36866;&#24212;&#24615;&#21306;&#22495;&#12290;&#22312;&#20843;&#39033;&#34507;&#30333;&#36136;&#24207;&#21015;&#35774;&#35745;&#20219;&#21153;&#20013;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;IsEM-Pro&#30340;&#24179;&#22343;&#36866;&#24212;&#24615;&#24471;&#20998;&#33267;&#23569;&#27604;&#20808;&#21069;&#26368;&#20339;&#26041;&#27861;&#39640;55&#65285;&#65292;&#24182;&#19988;&#29983;&#25104;&#20102;&#26356;&#22810;&#26679;&#21270;&#21644;&#26032;&#39062;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing protein sequences with desired biological function is crucial in biology and chemistry. Recent machine learning methods use a surrogate sequence-function model to replace the expensive wet-lab validation. How can we efficiently generate diverse and novel protein sequences with high fitness? In this paper, we propose IsEM-Pro, an approach to generate protein sequences towards a given fitness criterion. At its core, IsEM-Pro is a latent generative model, augmented by combinatorial structure features from a separately learned Markov random fields (MRFs). We develop an Monte Carlo Expectation-Maximization method (MCEM) to learn the model. During inference, sampling from its latent space enhances diversity while its MRFs features guide the exploration in high fitness regions. Experiments on eight protein sequence design tasks show that our IsEM-Pro outperforms the previous best methods by at least 55% on average fitness score and generates more diverse and novel protein sequences.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#25152;&#23450;&#20041;&#30340;&#65292;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.04613</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;
&lt;/p&gt;
&lt;p&gt;
The Descriptive Complexity of Graph Neural Networks. (arXiv:2303.04613v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04613
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#25152;&#23450;&#20041;&#30340;&#65292;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#24067;&#23572;&#30005;&#36335;&#22797;&#26434;&#24615;&#21644;&#25551;&#36848;&#24615;&#22797;&#26434;&#24615;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#39033;&#24335;&#35268;&#27169;&#26377;&#30028;&#28145;&#24230;&#30340;GNN&#26063;&#26063;&#21487;&#20197;&#35745;&#31639;&#30340;&#22270;&#26597;&#35810;&#27491;&#26159;&#37027;&#20123;&#29992;&#24102;&#35745;&#25968;&#21644;&#20869;&#32622;&#20851;&#31995;&#30340;&#19968;&#38454;&#36923;&#36753;&#21463;&#20445;&#25252;&#30340;&#29255;&#26029;GFO+C&#23450;&#20041;&#30340;&#12290;&#36825;&#23558;GNN&#25918;&#22312;&#30005;&#36335;&#22797;&#26434;&#24615;&#31867;TC^0&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GNN&#23478;&#26063;&#21487;&#20197;&#20351;&#29992;&#20219;&#24847;&#23454;&#25968;&#26435;&#20540;&#21644;&#21253;&#25324;&#26631;&#20934;ReLU&#12289;Logistic&#8220;sigmod&#8221;&#21644;&#21452;&#26354;&#27491;&#20999;&#20989;&#25968;&#22312;&#20869;&#30340;&#24191;&#27867;&#28608;&#27963;&#20989;&#25968;&#31867;&#12290;&#22914;&#26524;GNN&#34987;&#20801;&#35768;&#20351;&#29992;&#38543;&#26426;&#21021;&#22987;&#21270;&#21644;&#20840;&#23616;&#35835;&#21462;&#65288;&#36825;&#20123;&#37117;&#26159;GNN&#22312;&#23454;&#36341;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26631;&#20934;&#21151;&#33021;&#65289;&#65292;&#23427;&#20204;&#21487;&#20197;&#35745;&#31639;&#19982;&#38408;&#38376;&#30340;&#26377;&#30028;&#28145;&#24230;&#24067;&#23572;&#30005;&#36335;&#23436;&#20840;&#30456;&#21516;&#30340;&#26597;&#35810;&#65292;&#21363;&#22312;TC^0&#20013;&#30340;&#26597;&#35810;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#24102;&#20998;&#27573;&#32447;&#24615;&#28608;&#27963;&#21644;&#26377;&#29702;&#26435;&#37325;&#30340;&#21333;&#20010;GNN&#21487;&#20197;&#22312;&#19981;&#24314;&#36896;&#20869;&#37096;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#30001;GFO+C&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We analyse the power of graph neural networks (GNNs) in terms of Boolean circuit complexity and descriptive complexity.  We prove that the graph queries that can be computed by a polynomial-size bounded-depth family of GNNs are exactly those definable in the guarded fragment GFO+C of first-order logic with counting and with built-in relations. This puts GNNs in the circuit complexity class TC^0. Remarkably, the GNN families may use arbitrary real weights and a wide class of activation functions that includes the standard ReLU, logistic "sigmod", and hyperbolic tangent functions. If the GNNs are allowed to use random initialisation and global readout (both standard features of GNNs widely used in practice), they can compute exactly the same queries as bounded depth Boolean circuits with threshold gates, that is, exactly the queries in TC^0.  Moreover, we show that queries computable by a single GNN with piecewise linear activations and rational weights are definable in GFO+C without bui
&lt;/p&gt;</description></item></channel></rss>