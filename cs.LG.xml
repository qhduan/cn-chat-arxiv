<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#26032;&#23450;&#20041;&#30340;&#24809;&#32602;&#25439;&#22833;&#23398;&#20064;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;FBF&#31639;&#27861;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#65292;&#20197;&#35299;&#20915;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.00390</link><description>&lt;p&gt;
&#23398;&#20064;&#30495;&#27491;&#21333;&#35843;&#31639;&#23376;&#21450;&#20854;&#22312;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Learning truly monotone operators with applications to nonlinear inverse problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00390
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#23450;&#20041;&#30340;&#24809;&#32602;&#25439;&#22833;&#23398;&#20064;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#65292;&#35299;&#20915;&#22270;&#20687;&#22788;&#29702;&#20013;&#30340;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;FBF&#31639;&#27861;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#65292;&#20197;&#35299;&#20915;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26032;&#23450;&#20041;&#30340;&#24809;&#32602;&#25439;&#22833;&#26469;&#23398;&#20064;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#35299;&#20915;&#19968;&#31867;&#21464;&#20998;&#38382;&#39064;&#20013;&#29305;&#21035;&#26377;&#25928;&#65292;&#29305;&#21035;&#26159;&#22270;&#20687;&#22788;&#29702;&#20219;&#21153;&#20013;&#24120;&#36935;&#21040;&#30340;&#21333;&#35843;&#21253;&#21547;&#38382;&#39064;&#12290;&#37319;&#29992;&#21069;-&#21518;-&#21069;&#65288;FBF&#65289;&#31639;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#31070;&#32463;&#32593;&#32476;&#30340;Lipschitz&#24120;&#25968;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#25552;&#20379;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;FBF&#31639;&#27861;&#22312;&#23398;&#20064;&#31639;&#23376;&#21333;&#35843;&#30340;&#26465;&#20214;&#19979;&#25552;&#20379;&#25910;&#25947;&#20445;&#35777;&#12290;&#20511;&#37492;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#36825;&#20123;&#26032;&#23398;&#20064;&#30340;&#31639;&#23376;&#24212;&#29992;&#20110;&#35299;&#20915;&#38750;&#32447;&#24615;&#36870;&#38382;&#39064;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23558;&#38382;&#39064;&#21046;&#23450;&#20026;&#19968;&#20010;&#21464;&#20998;&#21253;&#21547;&#38382;&#39064;&#65292;&#38543;&#21518;&#35757;&#32451;&#19968;&#20010;&#21333;&#35843;&#31070;&#32463;&#32593;&#32476;&#26469;&#36924;&#36817;&#19968;&#20010;&#26412;&#36136;&#19978;&#21487;&#33021;&#19981;&#26159;&#21333;&#35843;&#30340;&#31639;&#23376;&#12290;&#21033;&#29992;FBF&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00390v1 Announce Type: cross  Abstract: This article introduces a novel approach to learning monotone neural networks through a newly defined penalization loss. The proposed method is particularly effective in solving classes of variational problems, specifically monotone inclusion problems, commonly encountered in image processing tasks. The Forward-Backward-Forward (FBF) algorithm is employed to address these problems, offering a solution even when the Lipschitz constant of the neural network is unknown. Notably, the FBF algorithm provides convergence guarantees under the condition that the learned operator is monotone. Building on plug-and-play methodologies, our objective is to apply these newly learned operators to solving non-linear inverse problems. To achieve this, we initially formulate the problem as a variational inclusion problem. Subsequently, we train a monotone neural network to approximate an operator that may not inherently be monotone. Leveraging the FBF al
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20984;&#32452;&#21512;&#30340;&#26041;&#27861;&#20272;&#35745;&#39640;&#32500;&#31354;&#38388;&#20013;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32500;&#22343;&#20540;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26435;&#37325;&#30830;&#23450;&#31574;&#30053;&#65306;&#19968;&#31181;&#36890;&#36807;&#27979;&#35797;&#31243;&#24207;&#35782;&#21035;&#20302;&#26041;&#24046;&#30340;&#30456;&#37051;&#22343;&#20540;&#65292;&#25552;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#25554;&#34917;&#20844;&#24335;&#65307;&#21478;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#32622;&#20449;&#30028;&#30830;&#23450;&#26435;&#37325;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24471;&#20986;&#26041;&#27861;&#23545;&#32463;&#39564;&#22343;&#20540;&#30340;&#20108;&#27425;&#39118;&#38505;&#25913;&#36827;&#65292;&#22312;&#32500;&#24230;&#28176;&#36817;&#30340;&#35282;&#24230;&#19978;&#28176;&#36817;&#22320;&#25509;&#36817; Oracle&#65288;Minimax&#65289;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.15038</link><description>&lt;p&gt;
&#39640;&#32500;&#24773;&#20917;&#19979;&#22810;&#20010;&#22343;&#20540;&#21521;&#37327;&#30340;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of multiple mean vectors in high dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15038
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20984;&#32452;&#21512;&#30340;&#26041;&#27861;&#20272;&#35745;&#39640;&#32500;&#31354;&#38388;&#20013;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32500;&#22343;&#20540;&#65292;&#24341;&#20837;&#20102;&#20004;&#31181;&#26435;&#37325;&#30830;&#23450;&#31574;&#30053;&#65306;&#19968;&#31181;&#36890;&#36807;&#27979;&#35797;&#31243;&#24207;&#35782;&#21035;&#20302;&#26041;&#24046;&#30340;&#30456;&#37051;&#22343;&#20540;&#65292;&#25552;&#20986;&#20102;&#23553;&#38381;&#24418;&#24335;&#25554;&#34917;&#20844;&#24335;&#65307;&#21478;&#19968;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#32622;&#20449;&#30028;&#30830;&#23450;&#26435;&#37325;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#24471;&#20986;&#26041;&#27861;&#23545;&#32463;&#39564;&#22343;&#20540;&#30340;&#20108;&#27425;&#39118;&#38505;&#25913;&#36827;&#65292;&#22312;&#32500;&#24230;&#28176;&#36817;&#30340;&#35282;&#24230;&#19978;&#28176;&#36817;&#22320;&#25509;&#36817; Oracle&#65288;Minimax&#65289;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#22522;&#20110;&#29420;&#31435;&#26679;&#26412;&#22312;&#19968;&#20010;&#20849;&#21516;&#31354;&#38388;&#20013;&#20272;&#35745;&#26469;&#33258;&#19981;&#21516;&#27010;&#29575;&#20998;&#24067;&#30340;&#22810;&#32500;&#22343;&#20540;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36890;&#36807;&#23545;&#36825;&#20123;&#26679;&#26412;&#23548;&#20986;&#30340;&#32463;&#39564;&#22343;&#20540;&#36827;&#34892;&#20984;&#32452;&#21512;&#26469;&#24418;&#25104;&#20272;&#35745;&#37327;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#25214;&#21040;&#36866;&#24403;&#30340;&#20381;&#36182;&#20110;&#25968;&#25454;&#30340;&#20984;&#32452;&#21512;&#26435;&#37325;&#65306;&#31532;&#19968;&#31181;&#21033;&#29992;&#27979;&#35797;&#31243;&#24207;&#26469;&#35782;&#21035;&#20855;&#26377;&#20302;&#26041;&#24046;&#30340;&#30456;&#37051;&#22343;&#20540;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#20851;&#20110;&#26435;&#37325;&#30340;&#23553;&#38381;&#24418;&#24335;&#25554;&#34917;&#20844;&#24335;&#65307;&#31532;&#20108;&#31181;&#36890;&#36807;&#26368;&#23567;&#21270;&#20108;&#27425;&#39118;&#38505;&#30340;&#19978;&#32622;&#20449;&#21306;&#38388;&#26469;&#30830;&#23450;&#26435;&#37325;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#32463;&#39564;&#22343;&#20540;&#25552;&#20379;&#30340;&#20108;&#27425;&#39118;&#38505;&#25913;&#36827;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#38598;&#20013;&#22312;&#32500;&#24230;&#28176;&#36817;&#30340;&#35282;&#24230;&#19978;&#65292;&#26174;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#30340;&#26377;&#25928;&#32500;&#24230;&#22686;&#21152;&#26102;&#28176;&#36817;&#22320;&#25509;&#36817;&#20110;&#19968;&#20010; Oracle&#65288;Minimax&#65289;&#25913;&#36827;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22343;&#20540;&#20272;&#35745;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15038v1 Announce Type: cross  Abstract: We endeavour to estimate numerous multi-dimensional means of various probability distributions on a common space based on independent samples. Our approach involves forming estimators through convex combinations of empirical means derived from these samples. We introduce two strategies to find appropriate data-dependent convex combination weights: a first one employing a testing procedure to identify neighbouring means with low variance, which results in a closed-form plug-in formula for the weights, and a second one determining weights via minimization of an upper confidence bound on the quadratic risk.Through theoretical analysis, we evaluate the improvement in quadratic risk offered by our methods compared to the empirical means. Our analysis focuses on a dimensional asymptotics perspective, showing that our methods asymptotically approach an oracle (minimax) improvement as the effective dimension of the data increases.We demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;</title><link>https://arxiv.org/abs/2403.09193</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#26159;&#32441;&#29702;&#20559;&#35265;&#36824;&#26159;&#24418;&#29366;&#20559;&#35265;&#65292;&#25105;&#20204;&#21487;&#20197;&#24341;&#23548;&#23427;&#20204;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Vision Language Models Texture or Shape Biased and Can We Steer Them?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09193
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24191;&#27867;&#24212;&#29992;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#27604;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#24418;&#29366;&#65292;&#26263;&#31034;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20250;&#21463;&#21040;&#25991;&#26412;&#30340;&#35843;&#33410;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#30701;&#30701;&#20960;&#24180;&#20869;&#24443;&#24213;&#25913;&#21464;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#26684;&#23616;&#65292;&#24320;&#21551;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#24212;&#29992;&#65292;&#20174;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#21040;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;&#65292;&#20877;&#21040;&#35270;&#35273;&#38382;&#31572;&#12290;&#19982;&#32431;&#35270;&#35273;&#27169;&#22411;&#19981;&#21516;&#65292;&#23427;&#20204;&#25552;&#20379;&#20102;&#36890;&#36807;&#35821;&#35328;&#25552;&#31034;&#35775;&#38382;&#35270;&#35273;&#20869;&#23481;&#30340;&#30452;&#35266;&#26041;&#24335;&#12290;&#36825;&#31181;&#27169;&#22411;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#24341;&#21457;&#25105;&#20204;&#24605;&#32771;&#23427;&#20204;&#26159;&#21542;&#20063;&#19982;&#20154;&#31867;&#35270;&#35273;&#19968;&#33268; - &#20855;&#20307;&#26469;&#35828;&#65292;&#23427;&#20204;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#26377;&#22810;&#22823;&#31243;&#24230;&#22320;&#37319;&#29992;&#20102;&#20154;&#31867;&#24341;&#23548;&#30340;&#35270;&#35273;&#20559;&#35265;&#65292;&#25110;&#32773;&#23427;&#20204;&#26159;&#21542;&#21482;&#26159;&#20174;&#32431;&#35270;&#35273;&#27169;&#22411;&#20013;&#32487;&#25215;&#20102;&#20559;&#35265;&#12290;&#20854;&#20013;&#19968;&#20010;&#37325;&#35201;&#30340;&#35270;&#35273;&#20559;&#35265;&#26159;&#32441;&#29702;&#19982;&#24418;&#29366;&#20559;&#35265;&#65292;&#21363;&#23616;&#37096;&#20449;&#24687;&#30340;&#20027;&#23548;&#22320;&#20301;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#27969;&#34892;&#30340;VLMs&#20013;&#30340;&#36825;&#31181;&#20559;&#35265;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;VLMs&#36890;&#24120;&#27604;&#23427;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#26356;&#20559;&#21521;&#20110;&#24418;&#29366;&#65292;&#36825;&#34920;&#26126;&#35270;&#35273;&#20559;&#35265;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#36890;&#36807;&#25991;&#26412;&#36827;&#34892;&#35843;&#33410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09193v1 Announce Type: cross  Abstract: Vision language models (VLMs) have drastically changed the computer vision model landscape in only a few years, opening an exciting array of new applications from zero-shot image classification, over to image captioning, and visual question answering. Unlike pure vision models, they offer an intuitive way to access visual content through language prompting. The wide applicability of such models encourages us to ask whether they also align with human vision - specifically, how far they adopt human-induced visual biases through multimodal fusion, or whether they simply inherit biases from pure vision models. One important visual bias is the texture vs. shape bias, or the dominance of local over global information. In this paper, we study this bias in a wide range of popular VLMs. Interestingly, we find that VLMs are often more shape-biased than their vision encoders, indicating that visual biases are modulated to some extent through text
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.08352</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21450;&#19982;&#20256;&#32479;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08352
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#34987;&#35748;&#20026;&#26159;&#24120;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26368;&#37325;&#35201;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#23427;&#20027;&#35201;&#28041;&#21450;&#24212;&#29992;&#36866;&#24403;&#30340;&#25968;&#25454;&#36716;&#25442;&#25805;&#20316;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#19968;&#36807;&#31243;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25163;&#21160;&#21019;&#24314;&#21644;&#27979;&#35797;&#19981;&#21516;&#20505;&#36873;&#22686;&#24378;&#21450;&#20854;&#36229;&#21442;&#25968;&#38656;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21407;&#21017;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20110;AutoML&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;AutoML&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#25805;&#20316;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08352v1 Announce Type: cross  Abstract: Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of technique
&lt;/p&gt;</description></item><item><title>&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.00025</link><description>&lt;p&gt;
&#20851;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#25361;&#25112;&#19982;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
On the Challenges and Opportunities in Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00025
&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#23384;&#22312;&#20851;&#38190;&#30340;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#22914;&#20309;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23558;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#65292;&#24182;&#20026;&#30740;&#31350;&#26041;&#21521;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#39046;&#22495;&#36817;&#24180;&#26469;&#22686;&#38271;&#36805;&#36895;&#32780;&#31283;&#23450;&#12290;&#38543;&#30528;&#28023;&#37327;&#35757;&#32451;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#20197;&#21450;&#21487;&#25193;&#23637;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#33539;&#24335;&#30340;&#36827;&#27493;&#65292;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#29983;&#25104;&#27169;&#22411;&#23637;&#29616;&#20986;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#25991;&#26412;&#20197;&#21450;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#22914;&#35270;&#39057;&#21644;&#20998;&#23376;&#65289;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35748;&#20026;&#24403;&#21069;&#22823;&#35268;&#27169;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#33509;&#24178;&#22522;&#26412;&#38382;&#39064;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#23450;&#29616;&#20195;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#33539;&#20363;&#20013;&#30340;&#20851;&#38190;&#26410;&#35299;&#20915;&#25361;&#25112;&#65292;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#23427;&#20204;&#30340;&#33021;&#21147;&#12289;&#22810;&#21151;&#33021;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#35782;&#21035;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#26088;&#22312;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#65292;&#25506;&#32034;&#26377;&#30410;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21152;&#24378;&#22823;&#21644;&#21487;&#35775;&#38382;&#30340;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00025v1 Announce Type: cross  Abstract: The field of deep generative modeling has grown rapidly and consistently over the years. With the availability of massive amounts of training data coupled with advances in scalable unsupervised learning paradigms, recent large-scale generative models show tremendous promise in synthesizing high-resolution images and text, as well as structured data such as videos and molecules. However, we argue that current large-scale generative AI models do not sufficiently address several fundamental issues that hinder their widespread adoption across domains. In this work, we aim to identify key unresolved challenges in modern generative AI paradigms that should be tackled to further enhance their capabilities, versatility, and reliability. By identifying these challenges, we aim to provide researchers with valuable insights for exploring fruitful research directions, thereby fostering the development of more robust and accessible generative AI so
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.14973</link><description>&lt;p&gt;
GenCeption&#65306;&#20351;&#29992;&#26410;&#26631;&#35760;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#22810;&#27169;&#24577;LLM
&lt;/p&gt;
&lt;p&gt;
GenCeption: Evaluate Multimodal LLMs with Unlabeled Unimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14973
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenCeption&#30340;&#26032;&#22411;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#21487;&#20197;&#20165;&#21033;&#29992;&#21333;&#27169;&#24577;&#25968;&#25454;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#26377;&#25928;&#21453;&#26144;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#30456;&#20851;&#24615;&#21644;&#28508;&#21147;&#20110;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#36890;&#24120;&#20351;&#29992;&#26114;&#36149;&#30340;&#24102;&#26631;&#27880;&#30340;&#22810;&#27169;&#24577;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20934;&#36890;&#24120;&#38590;&#20197;&#36319;&#19978;MLLM&#35780;&#20272;&#30340;&#24555;&#36895;&#21457;&#23637;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GenCeption&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26080;&#38656;&#27880;&#37322;&#30340;MLLM&#35780;&#20272;&#26694;&#26550;&#65292;&#20165;&#38656;&#35201;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#35780;&#20272;&#36328;&#27169;&#24577;&#35821;&#20041;&#19968;&#33268;&#24615;&#65292;&#24182;&#21453;&#26144;&#20986;&#27169;&#22411;&#20135;&#29983;&#24187;&#35273;&#30340;&#20542;&#21521;&#12290;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;DrawCeption&#28216;&#25103;&#65292;GenCeption&#20174;&#19968;&#20010;&#38750;&#25991;&#26412;&#26679;&#26412;&#24320;&#22987;&#65292;&#24182;&#32463;&#21382;&#19968;&#31995;&#21015;&#36845;&#20195;&#30340;&#25551;&#36848;&#21644;&#29983;&#25104;&#27493;&#39588;&#12290;&#36845;&#20195;&#20043;&#38388;&#30340;&#35821;&#20041;&#28418;&#31227;&#20351;&#29992;GC@T&#25351;&#26631;&#36827;&#34892;&#37327;&#21270;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#39564;&#35777;&#20102;GenCeption&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;&#20986;&#19982;&#27969;&#34892;&#30340;MLLM&#22522;&#20934;&#32467;&#26524;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;GenCeption&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#26222;&#36941;&#23384;&#22312;&#19988;&#20197;&#21069;&#26410;&#35265;&#30340;&#21333;&#27169;&#24577;&#25968;&#25454;&#26469;&#25193;&#23637;&#65292;&#20197;&#20943;&#36731;&#35757;&#32451;&#25968;&#25454;&#30340;&#27745;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14973v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) are commonly evaluated using costly annotated multimodal benchmarks. However, these benchmarks often struggle to keep pace with the rapidly advancing requirements of MLLM evaluation. We propose GenCeption, a novel and annotation-free MLLM evaluation framework that merely requires unimodal data to assess inter-modality semantic coherence and inversely reflects the models' inclination to hallucinate. Analogous to the popular DrawCeption game, GenCeption initiates with a non-textual sample and undergoes a series of iterative description and generation steps. Semantic drift across iterations is quantified using the GC@T metric. Our empirical findings validate GenCeption's efficacy, showing strong correlations with popular MLLM benchmarking results. GenCeption may be extended to mitigate training data contamination by utilizing ubiquitous, previously unseen unimodal data.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#37327;&#21270;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#38544;&#31169;&#38450;&#24481;&#25514;&#26045;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10065</link><description>&lt;p&gt;
&#27599;&#20010;&#25968;&#25454;&#28857;&#27844;&#38706;&#24744;&#38544;&#31169;&#30340;&#31243;&#24230;&#26377;&#22810;&#22823;&#65311;&#37327;&#21270;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;
&lt;/p&gt;
&lt;p&gt;
How Much Does Each Datapoint Leak Your Privacy? Quantifying the Per-datum Membership Leakage
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65292;&#37327;&#21270;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#65292;&#24182;&#35780;&#20272;&#20102;&#20004;&#31181;&#38544;&#31169;&#38450;&#24481;&#25514;&#26045;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#65292;&#20854;&#20013;&#25915;&#20987;&#32773;&#26088;&#22312;&#25512;&#26029;&#20986;&#19968;&#20010;&#22266;&#23450;&#30446;&#26631;&#25968;&#25454;&#26159;&#21542;&#24050;&#21253;&#21547;&#22312;&#31639;&#27861;&#30340;&#36755;&#20837;&#25968;&#25454;&#38598;&#20013;&#65292;&#20174;&#32780;&#20405;&#29359;&#38544;&#31169;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23450;&#20041;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#20026;&#26368;&#20248;&#23545;&#25163;&#36776;&#35782;&#23427;&#30340;&#20248;&#21183;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#32463;&#39564;&#22343;&#20540;&#30340;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#65292;&#24182;&#34920;&#26126;&#23427;&#21462;&#20915;&#20110;&#30446;&#26631;&#25968;&#25454;&#28857;&#21644;&#25968;&#25454;&#29983;&#25104;&#20998;&#24067;&#20043;&#38388;&#30340;&#39532;&#27663;&#36317;&#31163;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35780;&#20272;&#20102;&#20004;&#31181;&#38544;&#31169;&#38450;&#24481;&#25514;&#26045;&#30340;&#25928;&#26524;&#65292;&#21363;&#28155;&#21152;&#39640;&#26031;&#22122;&#22768;&#21644;&#23376;&#37319;&#26679;&#12290;&#25105;&#20204;&#20934;&#30830;&#22320;&#37327;&#21270;&#20102;&#23427;&#20204;&#37117;&#22914;&#20309;&#38477;&#20302;&#27599;&#20010;&#25968;&#25454;&#28857;&#30340;&#25104;&#21592;&#27844;&#38706;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#19968;&#20010;&#32467;&#21512;&#20102;&#20284;&#28982;&#27604;&#26816;&#39564;&#30340;Edgeworth&#23637;&#24320;&#21644;Lindeberg-Feller&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#30340;&#26032;&#22411;&#35777;&#26126;&#25216;&#26415;&#19978;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36830;&#25509;&#20102;&#29616;&#26377;&#30340;&#20284;&#28982;&#27604;&#21644;&#26631;&#37327;&#20056;&#31215;&#25915;&#20987;&#65292;&#24182;&#23545;&#36825;&#20123;&#25915;&#20987;&#36827;&#34892;&#20102;&#35770;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10065v1 Announce Type: new  Abstract: We study the per-datum Membership Inference Attacks (MIAs), where an attacker aims to infer whether a fixed target datum has been included in the input dataset of an algorithm and thus, violates privacy. First, we define the membership leakage of a datum as the advantage of the optimal adversary targeting to identify it. Then, we quantify the per-datum membership leakage for the empirical mean, and show that it depends on the Mahalanobis distance between the target datum and the data-generating distribution. We further assess the effect of two privacy defences, i.e. adding Gaussian noise and sub-sampling. We quantify exactly how both of them decrease the per-datum membership leakage. Our analysis builds on a novel proof technique that combines an Edgeworth expansion of the likelihood ratio test and a Lindeberg-Feller central limit theorem. Our analysis connects the existing likelihood ratio and scalar product attacks, and also justifies 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25551;&#36848;&#20102;CNN&#20013;&#21367;&#31215;&#29942;&#39048;&#65288;CBN&#65289;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#22312;&#21069;&#20960;&#23618;&#23558;&#36755;&#20837;&#34920;&#31034;&#36716;&#25442;&#20026;&#22312;&#23569;&#25968;&#39057;&#29575;&#21644;&#36890;&#36947;&#19978;&#21463;&#25903;&#25345;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#21518;&#20960;&#23618;&#26144;&#23556;&#22238;&#36755;&#20986;&#12290;CBN&#31209;&#23450;&#20041;&#20102;&#20445;&#30041;&#22312;&#29942;&#39048;&#20013;&#30340;&#39057;&#29575;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#37096;&#20998;&#35777;&#26126;&#20102;&#21442;&#25968;&#33539;&#25968;&#19982;&#28145;&#24230;&#21644;CBN&#31209;&#30340;&#27604;&#20363;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32593;&#32476;&#30340;&#21442;&#25968;&#33539;&#25968;&#20381;&#36182;&#20110;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20219;&#20309;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#33539;&#25968;&#30340;&#32593;&#32476;&#37117;&#20250;&#23637;&#31034;&#20986;CBN&#32467;&#26500;&#65292;&#36825;&#35299;&#37322;&#20102;&#19979;&#37319;&#26679;&#30340;&#24120;&#35265;&#23454;&#36341;&#65307;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;CBN&#32467;&#26500;&#22312;&#19979;&#37319;&#26679;&#19979;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CBN&#32467;&#26500;&#26469;&#35299;&#37322;...&#65288;&#25688;&#35201;&#23436;&#25972;&#20869;&#23481;&#35831;&#35265;&#27491;&#25991;&#65289;</title><link>https://arxiv.org/abs/2402.08010</link><description>&lt;p&gt;
CNN&#38656;&#35201;&#21738;&#20123;&#39057;&#29575;&#65311;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#32039;&#24613;&#29942;&#39048;&#32467;&#26500;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;CNN&#20013;&#21367;&#31215;&#29942;&#39048;&#65288;CBN&#65289;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#22312;&#21069;&#20960;&#23618;&#23558;&#36755;&#20837;&#34920;&#31034;&#36716;&#25442;&#20026;&#22312;&#23569;&#25968;&#39057;&#29575;&#21644;&#36890;&#36947;&#19978;&#21463;&#25903;&#25345;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#26368;&#21518;&#20960;&#23618;&#26144;&#23556;&#22238;&#36755;&#20986;&#12290;CBN&#31209;&#23450;&#20041;&#20102;&#20445;&#30041;&#22312;&#29942;&#39048;&#20013;&#30340;&#39057;&#29575;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#37096;&#20998;&#35777;&#26126;&#20102;&#21442;&#25968;&#33539;&#25968;&#19982;&#28145;&#24230;&#21644;CBN&#31209;&#30340;&#27604;&#20363;&#25104;&#27491;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#32593;&#32476;&#30340;&#21442;&#25968;&#33539;&#25968;&#20381;&#36182;&#20110;&#20989;&#25968;&#30340;&#35268;&#21017;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20219;&#20309;&#20855;&#26377;&#25509;&#36817;&#26368;&#20248;&#21442;&#25968;&#33539;&#25968;&#30340;&#32593;&#32476;&#37117;&#20250;&#23637;&#31034;&#20986;CBN&#32467;&#26500;&#65292;&#36825;&#35299;&#37322;&#20102;&#19979;&#37319;&#26679;&#30340;&#24120;&#35265;&#23454;&#36341;&#65307;&#25105;&#20204;&#36824;&#39564;&#35777;&#20102;CBN&#32467;&#26500;&#22312;&#19979;&#37319;&#26679;&#19979;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CBN&#32467;&#26500;&#26469;&#35299;&#37322;...&#65288;&#25688;&#35201;&#23436;&#25972;&#20869;&#23481;&#35831;&#35265;&#27491;&#25991;&#65289;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;CNN&#20013;&#21367;&#31215;&#29942;&#39048;&#65288;CBN&#65289;&#32467;&#26500;&#30340;&#20986;&#29616;&#65292;&#32593;&#32476;&#20351;&#29992;&#20854;&#21069;&#20960;&#23618;&#23558;&#36755;&#20837;&#34920;&#31034;&#36716;&#25442;&#20026;&#20165;&#22312;&#20960;&#20010;&#39057;&#29575;&#21644;&#36890;&#36947;&#19978;&#21463;&#25903;&#25345;&#30340;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#26368;&#21518;&#20960;&#23618;&#23558;&#20854;&#26144;&#23556;&#22238;&#36755;&#20986;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;CBN&#31209;&#65292;&#25551;&#36848;&#20102;&#20445;&#30041;&#22312;&#29942;&#39048;&#20869;&#30340;&#39057;&#29575;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#65292;&#24182;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#35777;&#26126;&#20102;&#34920;&#31034;&#20989;&#25968;$f$&#25152;&#38656;&#30340;&#21442;&#25968;&#33539;&#25968;&#25353;&#28145;&#24230;&#20056;&#20197;CBN&#31209;$f$&#30340;&#27604;&#20363;&#32553;&#25918;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#21442;&#25968;&#33539;&#25968;&#22312;&#19979;&#19968;&#38454;&#20013;&#20381;&#36182;&#20110;$f$&#30340;&#27491;&#21017;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#21442;&#25968;&#33539;&#25968;&#30340;&#32593;&#32476;&#37117;&#20250;&#22312;&#26435;&#37325;&#21644;&#65288;&#22312;&#32593;&#32476;&#23545;&#22823;&#23398;&#20064;&#29575;&#31283;&#23450;&#30340;&#20551;&#35774;&#19979;&#65289;&#28608;&#27963;&#20013;&#34920;&#29616;&#20986;CBN&#32467;&#26500;&#65292;&#36825;&#20419;&#20351;&#20102;&#19979;&#37319;&#26679;&#30340;&#24120;&#35265;&#20570;&#27861;&#65307;&#24182;&#19988;&#25105;&#20204;&#39564;&#35777;&#20102;CBN&#32467;&#26500;&#22312;&#19979;&#37319;&#26679;&#19979;&#20173;&#28982;&#25104;&#31435;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;CBN&#32467;&#26500;&#26469;&#35299;&#37322;...
&lt;/p&gt;
&lt;p&gt;
We describe the emergence of a Convolution Bottleneck (CBN) structure in CNNs, where the network uses its first few layers to transform the input representation into a representation that is supported only along a few frequencies and channels, before using the last few layers to map back to the outputs. We define the CBN rank, which describes the number and type of frequencies that are kept inside the bottleneck, and partially prove that the parameter norm required to represent a function $f$ scales as depth times the CBN rank $f$. We also show that the parameter norm depends at next order on the regularity of $f$. We show that any network with almost optimal parameter norm will exhibit a CBN structure in both the weights and - under the assumption that the network is stable under large learning rate - the activations, which motivates the common practice of down-sampling; and we verify that the CBN results still hold with down-sampling. Finally we use the CBN structure to interpret the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#20248;&#21270;&#38544;&#21547;&#20998;&#24067;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#24490;&#29615;&#20013;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#21644;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05468</link><description>&lt;p&gt;
&#38544;&#24335;&#25193;&#25955;: &#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#23454;&#29616;&#39640;&#25928;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit Diffusion: Efficient Optimization through Stochastic Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05468
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#37319;&#26679;&#20248;&#21270;&#38544;&#21547;&#20998;&#24067;&#30340;&#26032;&#31639;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#21333;&#20010;&#24490;&#29615;&#20013;&#21516;&#26102;&#36827;&#34892;&#20248;&#21270;&#21644;&#37319;&#26679;&#27493;&#39588;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21442;&#25968;&#21270;&#38543;&#26426;&#25193;&#25955;&#38544;&#24335;&#23450;&#20041;&#30340;&#20998;&#24067;&#26469;&#36827;&#34892;&#20248;&#21270;&#30340;&#26032;&#31639;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#36825;&#20123;&#21442;&#25968;&#65292;&#21487;&#20197;&#20462;&#25913;&#37319;&#26679;&#36807;&#31243;&#30340;&#32467;&#26524;&#20998;&#24067;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#38024;&#23545;&#36825;&#20123;&#36807;&#31243;&#30340;&#19968;&#38454;&#20248;&#21270;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#21333;&#20010;&#24490;&#29615;&#20013;&#36827;&#34892;&#20248;&#21270;&#21644;&#37319;&#26679;&#27493;&#39588;&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#26041;&#27861;&#21463;&#21040;&#21452;&#23618;&#20248;&#21270;&#21644;&#33258;&#21160;&#38544;&#24335;&#24494;&#20998;&#30340;&#26368;&#26032;&#36827;&#23637;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#37319;&#26679;&#20316;&#20026;&#22312;&#27010;&#29575;&#20998;&#24067;&#31354;&#38388;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25105;&#20204;&#26041;&#27861;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#21450;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new algorithm to optimize distributions defined implicitly by parameterized stochastic diffusions. Doing so allows us to modify the outcome distribution of sampling processes by optimizing over their parameters. We introduce a general framework for first-order optimization of these processes, that performs jointly, in a single loop, optimization and sampling steps. This approach is inspired by recent advances in bilevel optimization and automatic implicit differentiation, leveraging the point of view of sampling as optimization over the space of probability distributions. We provide theoretical guarantees on the performance of our method, as well as experimental results demonstrating its effectiveness in real-world settings.
&lt;/p&gt;</description></item><item><title>PQMass&#26159;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#36136;&#37327;&#20272;&#35745;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#20840;&#38754;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#19981;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.04355</link><description>&lt;p&gt;
PQMass: &#20351;&#29992;&#27010;&#29575;&#36136;&#37327;&#20272;&#35745;&#30340;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#27010;&#29575;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04355
&lt;/p&gt;
&lt;p&gt;
PQMass&#26159;&#19968;&#31181;&#20351;&#29992;&#27010;&#29575;&#36136;&#37327;&#20272;&#35745;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#36136;&#37327;&#30340;&#20840;&#38754;&#26041;&#27861;&#65292;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#19981;&#20381;&#36182;&#20110;&#20551;&#35774;&#25110;&#35757;&#32451;&#20854;&#20182;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#22522;&#20110;&#26679;&#26412;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#20272;&#35745;&#20004;&#20010;&#26679;&#26412;&#38598;&#21512;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#30340;&#27010;&#29575;&#65292;&#20026;&#35780;&#20272;&#21333;&#20010;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#25110;&#27604;&#36739;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22810;&#20010;&#31454;&#20105;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#35745;&#19978;&#20005;&#26684;&#30340;&#26041;&#27861;&#12290;&#35813;&#27604;&#36739;&#21487;&#20197;&#36890;&#36807;&#23558;&#31354;&#38388;&#21010;&#20998;&#20026;&#38750;&#37325;&#21472;&#30340;&#21306;&#22495;&#24182;&#27604;&#36739;&#27599;&#20010;&#21306;&#22495;&#20013;&#30340;&#25968;&#25454;&#26679;&#26412;&#25968;&#37327;&#26469;&#36827;&#34892;&#12290;&#35813;&#26041;&#27861;&#20165;&#38656;&#35201;&#29983;&#25104;&#27169;&#22411;&#21644;&#27979;&#35797;&#25968;&#25454;&#30340;&#26679;&#26412;&#12290;&#23427;&#33021;&#22815;&#30452;&#25509;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#26080;&#38656;&#38477;&#32500;&#12290;&#26174;&#33879;&#30340;&#26159;&#65292;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#20851;&#20110;&#30495;&#23454;&#20998;&#24067;&#23494;&#24230;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;&#35757;&#32451;&#25110;&#25311;&#21512;&#20219;&#20309;&#36741;&#21161;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#23427;&#30528;&#37325;&#20110;&#36817;&#20284;&#35745;&#31639;&#23494;&#24230;&#30340;&#31215;&#20998;&#65288;&#27010;&#29575;&#36136;&#37327;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a comprehensive sample-based method for assessing the quality of generative models. The proposed approach enables the estimation of the probability that two sets of samples are drawn from the same distribution, providing a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models trained on the same dataset. This comparison can be conducted by dividing the space into non-overlapping regions and comparing the number of data samples in each region. The method only requires samples from the generative model and the test data. It is capable of functioning directly on high-dimensional data, obviating the need for dimensionality reduction. Significantly, the proposed method does not depend on assumptions regarding the density of the true distribution, and it does not rely on training or fitting any auxiliary models. Instead, it focuses on approximating the integral of the density (probability mass) acros
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03448</link><description>&lt;p&gt;
&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65306;&#20855;&#26377;&#24191;&#20041;&#25910;&#25947;&#20445;&#35777;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Decentralized Sporadic Federated Learning: A Unified Methodology with Generalized Convergence Guarantees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#32479;&#19968;&#20102;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#12289;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#31561;&#33879;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#12290;&#26681;&#25454;&#20998;&#26512;&#32467;&#26524;&#65292;DSpodFL&#33021;&#22815;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#36798;&#21040;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26368;&#20339;&#24615;&#24046;&#36317;&#30340;&#21305;&#37197;&#12290;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36817;&#26469;&#21463;&#21040;&#20102;&#37325;&#35201;&#30340;&#30740;&#31350;&#20851;&#27880;&#65292;&#28085;&#30422;&#20102;&#27169;&#22411;&#26356;&#26032;&#21644;&#27169;&#22411;&#32858;&#21512;&#36825;&#20004;&#20010;&#20851;&#38190;&#32852;&#37030;&#23398;&#20064;&#36807;&#31243;&#37117;&#30001;&#23458;&#25143;&#31471;&#36827;&#34892;&#30340;&#35774;&#32622;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#25955;&#24335;&#38388;&#27463;&#32852;&#37030;&#23398;&#20064;&#65288;DSpodFL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;DFL&#26041;&#27861;&#65292;&#23427;&#22312;&#36825;&#20004;&#20010;&#36807;&#31243;&#20013;&#24191;&#20041;&#21270;&#20102;&#38388;&#27463;&#24615;&#30340;&#27010;&#24565;&#65292;&#24314;&#27169;&#20102;&#22312;&#23454;&#38469;DFL&#35774;&#32622;&#20013;&#20986;&#29616;&#30340;&#19981;&#21516;&#24418;&#24335;&#30340;&#24322;&#36136;&#24615;&#30340;&#24433;&#21709;&#12290;DSpodFL&#23558;&#35768;&#22810;&#30528;&#21517;&#30340;&#20998;&#25955;&#20248;&#21270;&#26041;&#27861;&#65292;&#22914;&#20998;&#24067;&#24335;&#26799;&#24230;&#19979;&#38477;&#65288;DGD&#65289;&#65292;&#38543;&#26426;&#38386;&#35805;&#65288;RG&#65289;&#21644;&#20998;&#25955;&#24335;&#32852;&#37030;&#24179;&#22343;&#65288;DFedAvg&#65289;&#65292;&#32479;&#19968;&#21040;&#19968;&#20010;&#24314;&#27169;&#26694;&#26550;&#19979;&#12290;&#25105;&#20204;&#23545;DSpodFL&#30340;&#25910;&#25947;&#34892;&#20026;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#26174;&#31034;&#20986;&#21487;&#20197;&#22312;&#26356;&#19968;&#33324;&#30340;&#20551;&#35774;&#19979;&#65292;&#23558;&#20960;&#20309;&#25910;&#25947;&#36895;&#29575;&#19982;&#26377;&#38480;&#30340;&#26368;&#20339;&#24615;&#24046;&#36317;&#30456;&#21305;&#37197;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Decentralized Federated Learning (DFL) has received significant recent research attention, capturing settings where both model updates and model aggregations -- the two key FL processes -- are conducted by the clients. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology which generalizes the notion of sporadicity in both of these processes, modeling the impact of different forms of heterogeneity that manifest in realistic DFL settings. $\texttt{DSpodFL}$ unifies many of the prominent decentralized optimization methods, e.g., distributed gradient descent (DGD), randomized gossip (RG), and decentralized federated averaging (DFedAvg), under a single modeling framework. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$, showing, among other insights, that we can match a geometric convergence rate to a finite optimality gap under more general assumptions than in existing works. Through experiments, we demonstra
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#21644;&#36741;&#21161;&#26799;&#24230;&#22312;&#35757;&#32451;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#27491;&#20132;&#25237;&#24433;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;EMA&#65288;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65289;&#21487;&#20197;&#25913;&#36827;&#26799;&#24230;&#25163;&#26415;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#31649;&#36947;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02998</link><description>&lt;p&gt;
&#23567;&#24515;&#20351;&#29992;&#25163;&#26415;&#20992;&#65306;&#20351;&#29992;EMA&#25913;&#36827;&#26799;&#24230;&#25163;&#26415;
&lt;/p&gt;
&lt;p&gt;
Careful with that Scalpel: Improving Gradient Surgery with an EMA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02998
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#21644;&#36741;&#21161;&#26799;&#24230;&#22312;&#35757;&#32451;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#27491;&#20132;&#25237;&#24433;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#29992;EMA&#65288;&#25351;&#25968;&#31227;&#21160;&#24179;&#22343;&#65289;&#21487;&#20197;&#25913;&#36827;&#26799;&#24230;&#25163;&#26415;&#65292;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#31649;&#36947;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20272;&#35745;&#31649;&#36947;&#20013;&#65292;&#38500;&#20102;&#26368;&#23567;&#21270;&#21333;&#19968;&#30340;&#35757;&#32451;&#25439;&#22833;&#22806;&#65292;&#36824;&#20381;&#36182;&#20110;&#36741;&#21161;&#30446;&#26631;&#26469;&#37327;&#21270;&#21644;&#40723;&#21169;&#27169;&#22411;&#30340;&#21487;&#21462;&#23646;&#24615;&#65288;&#20363;&#22914;&#22312;&#21478;&#19968;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#40065;&#26834;&#24615;&#65292;&#19982;&#20808;&#21069;&#30340;&#19968;&#33268;&#24615;&#65289;&#12290;&#34429;&#28982;&#23558;&#36741;&#21161;&#25439;&#22833;&#19982;&#35757;&#32451;&#25439;&#22833;&#30456;&#21152;&#20316;&#20026;&#27491;&#21017;&#21270;&#30340;&#26368;&#31616;&#21333;&#26041;&#27861;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#28151;&#21512;&#26799;&#24230;&#32780;&#19981;&#20165;&#20165;&#26159;&#31616;&#21333;&#30456;&#21152;&#65292;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65307;&#36825;&#34987;&#31216;&#20026;&#26799;&#24230;&#25163;&#26415;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#30475;&#20316;&#26159;&#19968;&#20010;&#32422;&#26463;&#26368;&#23567;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#36741;&#21161;&#30446;&#26631;&#22312;&#35757;&#32451;&#25439;&#22833;&#30340;&#26368;&#23567;&#21270;&#38598;&#21512;&#20013;&#34987;&#26368;&#23567;&#21270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#21452;&#23618;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#21442;&#25968;&#26356;&#26032;&#26041;&#21521;&#65292;&#23427;&#23558;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#21644;&#36741;&#21161;&#26799;&#24230;&#22312;&#35757;&#32451;&#26799;&#24230;&#26041;&#21521;&#19978;&#30340;&#27491;&#20132;&#25237;&#24433;&#32467;&#21512;&#36215;&#26469;&#12290;&#22312;&#26799;&#24230;&#26469;&#33258;&#23567;&#25209;&#27425;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#20351;&#29992;&#35757;&#32451;&#25439;&#22833;&#26799;&#24230;&#30340;&#31227;&#21160;&#24179;&#22343;&#26469;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Beyond minimizing a single training loss, many deep learning estimation pipelines rely on an auxiliary objective to quantify and encourage desirable properties of the model (e.g. performance on another dataset, robustness, agreement with a prior). Although the simplest approach to incorporating an auxiliary loss is to sum it with the training loss as a regularizer, recent works have shown that one can improve performance by blending the gradients beyond a simple sum; this is known as gradient surgery. We cast the problem as a constrained minimization problem where the auxiliary objective is minimized among the set of minimizers of the training loss. To solve this bilevel problem, we follow a parameter update direction that combines the training loss gradient and the orthogonal projection of the auxiliary gradient to the training gradient. In a setting where gradients come from mini-batches, we explain how, using a moving average of the training loss gradients, we can carefully maintain
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.01879</link><description>&lt;p&gt;
$\sigma$-zero: &#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$-&#33539;&#25968;&#23545;&#25239;&#26679;&#26412;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
$\sigma$-zero: Gradient-based Optimization of $\ell_0$-norm Adversarial Examples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26041;&#27861;$\sigma$-zero&#65292;&#20854;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#21487;&#24494;&#36817;&#20284;&#21644;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#65292;&#33021;&#22815;&#22312;&#38750;&#20984;&#21644;&#38750;&#21487;&#24494;&#30340;&#32422;&#26463;&#19979;&#20248;&#21270;&#65292;&#20174;&#32780;&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#31232;&#30095;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#23545;&#22522;&#20110;&#26799;&#24230;&#25915;&#20987;&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#25915;&#20987;&#32771;&#34385;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#32422;&#26463;&#26469;&#21046;&#36896;&#36755;&#20837;&#25200;&#21160;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#20102;&#31232;&#30095;&#30340;$\ell_1$&#21644;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#12290;&#29305;&#21035;&#26159;&#65292;&#30001;&#20110;&#22312;&#38750;&#20984;&#19988;&#38750;&#21487;&#24494;&#32422;&#26463;&#19978;&#36827;&#34892;&#20248;&#21270;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#65292;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#26159;&#30740;&#31350;&#26368;&#23569;&#30340;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#36825;&#20123;&#25915;&#20987;&#35780;&#20272;&#23545;&#25239;&#40065;&#26834;&#24615;&#21487;&#20197;&#25581;&#31034;&#22312;&#26356;&#20256;&#32479;&#30340;$\ell_2$&#21644;$\ell_\infty$&#33539;&#25968;&#25915;&#20987;&#20013;&#26410;&#33021;&#27979;&#35797;&#20986;&#30340;&#24369;&#28857;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;$\ell_0$&#33539;&#25968;&#25915;&#20987;&#65292;&#31216;&#20026;$\sigma$-zero&#65292;&#23427;&#21033;&#29992;&#20102;$\ell_0$&#33539;&#25968;&#30340;&#19968;&#20010;&#29305;&#27530;&#21487;&#24494;&#36817;&#20284;&#26469;&#20419;&#36827;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#25237;&#24433;&#36816;&#31639;&#31526;&#21160;&#24577;&#35843;&#25972;&#25439;&#22833;&#26368;&#23567;&#21270;&#21644;&#25200;&#21160;&#31232;&#30095;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#36890;&#36807;&#22312;MNIST&#12289;CIFAR10&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#35780;&#20272;&#65292;&#21253;&#25324;...
&lt;/p&gt;
&lt;p&gt;
Evaluating the adversarial robustness of deep networks to gradient-based attacks is challenging. While most attacks consider $\ell_2$- and $\ell_\infty$-norm constraints to craft input perturbations, only a few investigate sparse $\ell_1$- and $\ell_0$-norm attacks. In particular, $\ell_0$-norm attacks remain the least studied due to the inherent complexity of optimizing over a non-convex and non-differentiable constraint. However, evaluating adversarial robustness under these attacks could reveal weaknesses otherwise left untested with more conventional $\ell_2$- and $\ell_\infty$-norm attacks. In this work, we propose a novel $\ell_0$-norm attack, called $\sigma$-zero, which leverages an ad hoc differentiable approximation of the $\ell_0$ norm to facilitate gradient-based optimization, and an adaptive projection operator to dynamically adjust the trade-off between loss minimization and perturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet datasets, involving
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;</title><link>https://arxiv.org/abs/2303.14537</link><description>&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#65306;&#22312;&#28608;&#27963;&#31354;&#38388;&#20013;&#20351;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Deep Augmentation: Self-Supervised Learning with Transformations in Activation Space
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.14537
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#22686;&#24378;&#26159;&#19968;&#31181;&#21033;&#29992;dropout&#25110;PCA&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#36716;&#25442;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65292;&#26377;&#25928;&#25913;&#21892;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#22312;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#22522;&#30784;&#27169;&#22411;&#19978;&#65292;&#36890;&#36807;&#28145;&#24230;&#22686;&#24378;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30417;&#30563;&#38382;&#39064;&#19978;&#25928;&#26524;&#30456;&#21453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28145;&#24230;&#22686;&#24378;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#36749;&#23398;&#25110;PCA&#26469;&#36716;&#25442;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#30446;&#26631;&#23618;&#65292;&#20197;&#25552;&#39640;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#22270;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#19978;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#26469;&#23637;&#31034;&#28145;&#24230;&#22686;&#24378;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#30784;&#27169;&#22411;&#20013;&#65292;&#22914;Transformers&#12289;ResNets&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#19978;&#28145;&#24230;&#22686;&#24378;&#33021;&#22815;&#24102;&#26469;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#65292;&#20294;&#22312;&#30456;&#24212;&#30340;&#30417;&#30563;&#38382;&#39064;&#19978;&#35266;&#23519;&#21040;&#30456;&#21453;&#30340;&#25928;&#26524;&#12290; &#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#28145;&#24230;&#22686;&#24378;&#20943;&#36731;&#20102;&#23618;&#20043;&#38388;&#30340;&#30456;&#20114;&#36866;&#24212;&#65292;&#21363;"&#23849;&#28291;"&#24418;&#24335;&#30340;&#38382;&#39064;&#12290; &#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#21046;&#23450;&#20102;&#19968;&#31181;&#36873;&#25321;&#30446;&#26631;&#23618;&#30340;&#26041;&#27861;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#29992;&#28145;&#24230;&#22686;&#24378;&#23450;&#20301;&#26356;&#28145;&#23618;&#27425;&#30340;&#23618;&#35201;&#20248;&#20110;&#22686;&#24378;&#36755;&#20837;&#25968;&#25454;&#12290; &#36825;&#31181;&#26041;&#27861;&#30340;&#31616;&#21333;&#32593;&#32476;&#21644;&#27169;&#24577;&#26080;&#20851;&#24615;&#20351;&#20854;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.14537v2 Announce Type: replace-cross  Abstract: We introduce Deep Augmentation, an approach to implicit data augmentation using dropout or PCA to transform a targeted layer within a neural network to improve performance and generalization. We demonstrate Deep Augmentation through extensive experiments on contrastive learning tasks in NLP, computer vision, and graph learning. We observe substantial performance gains with Transformers, ResNets, and Graph Neural Networks as the underlying models in contrastive learning, but observe inverse effects on the corresponding supervised problems. Our analysis suggests that Deep Augmentation alleviates co-adaption between layers, a form of "collapse." We use this observation to formulate a method for selecting which layer to target; in particular, our experimentation reveals that targeting deeper layers with Deep Augmentation outperforms augmenting the input data. The simple network- and modality-agnostic nature of this approach enables
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;MFCPL&#65292;&#36890;&#36807;&#23436;&#25972;&#30340;&#21407;&#22411;&#25552;&#20379;&#22810;&#26679;&#30340;&#27169;&#24577;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32570;&#22833;&#27169;&#24577;&#24102;&#26469;&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.13898</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#21407;&#22411;&#22522;&#30784;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#22312;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Cross-Modal Prototype based Multimodal Federated Learning under Severely Missing Modality. (arXiv:2401.13898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13898
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;MFCPL&#65292;&#36890;&#36807;&#23436;&#25972;&#30340;&#21407;&#22411;&#25552;&#20379;&#22810;&#26679;&#30340;&#27169;&#24577;&#30693;&#35782;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#32570;&#22833;&#27169;&#24577;&#24102;&#26469;&#30340;&#31283;&#20581;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#65288;MFL&#65289;&#20316;&#20026;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#20363;&#24050;&#32463;&#20986;&#29616;&#65292;&#23427;&#20801;&#35768;&#20855;&#26377;&#19981;&#21516;&#27169;&#24577;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#31169;&#20154;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#20316;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#36328;&#22810;&#26679;&#30340;&#25968;&#25454;&#28304;&#12290;&#28982;&#32780;&#65292;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#31561;&#25361;&#25112;&#32473;MFL&#30340;&#31283;&#20581;&#24615;&#24102;&#26469;&#37325;&#35201;&#38459;&#30861;&#65292;&#20005;&#37325;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20005;&#37325;&#32570;&#22833;&#27169;&#24577;&#19979;&#30340;&#22810;&#27169;&#24577;&#32852;&#37030;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#22810;&#27169;&#24577;&#32852;&#37030;&#20132;&#21449;&#21407;&#22411;&#23398;&#20064;&#65288;MFCPL&#65289;&#65292;&#36890;&#36807;&#23545;&#27169;&#24577;&#20849;&#20139;&#32423;&#21035;&#36827;&#34892;&#23436;&#25972;&#30340;&#21407;&#22411;&#26469;&#25552;&#20379;&#22810;&#26679;&#30340;&#27169;&#24577;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal federated learning (MFL) has emerged as a decentralized machine learning paradigm, allowing multiple clients with different modalities to collaborate on training a machine learning model across diverse data sources without sharing their private data. However, challenges, such as data heterogeneity and severely missing modalities, pose crucial hindrances to the robustness of MFL, significantly impacting the performance of global model. The absence of a modality introduces misalignment during the local training phase, stemming from zero-filling in the case of clients with missing modalities. Consequently, achieving robust generalization in global model becomes imperative, especially when dealing with clients that have incomplete data. In this paper, we propose Multimodal Federated Cross Prototype Learning (MFCPL), a novel approach for MFL under severely missing modalities by conducting the complete prototypes to provide diverse modality knowledge in modality-shared level with 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35268;&#33539;&#21270;&#27491;&#24577;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#27969;&#24418;&#23398;&#20064;&#12290;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#36870;&#21464;&#25442;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#27969;&#24418;&#19978;&#35745;&#31639;&#27010;&#29575;&#23494;&#24230;&#24182;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#34920;&#31034;&#20013;&#23384;&#22312;&#30528;&#19982;&#27969;&#24418;&#20851;&#32852;&#19988;&#36864;&#21270;&#30340;&#20869;&#22312;&#22522;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12743</link><description>&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#30340;&#35268;&#33539;&#21270;&#27491;&#24577;&#27969;
&lt;/p&gt;
&lt;p&gt;
Canonical normalizing flows for manifold learning. (arXiv:2310.12743v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12743
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35268;&#33539;&#21270;&#27491;&#24577;&#27969;&#26041;&#27861;&#65292;&#29992;&#20110;&#27969;&#24418;&#23398;&#20064;&#12290;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#36870;&#21464;&#25442;&#23558;&#25968;&#25454;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#27969;&#24418;&#19978;&#35745;&#31639;&#27010;&#29575;&#23494;&#24230;&#24182;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#22312;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#34920;&#31034;&#20013;&#23384;&#22312;&#30528;&#19982;&#27969;&#24418;&#20851;&#32852;&#19988;&#36864;&#21270;&#30340;&#20869;&#22312;&#22522;&#20989;&#25968;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24418;&#23398;&#20064;&#27969;&#26159;&#19968;&#31867;&#29983;&#25104;&#24314;&#27169;&#25216;&#26415;&#65292;&#20551;&#35774;&#25968;&#25454;&#20855;&#26377;&#20302;&#32500;&#27969;&#24418;&#25551;&#36848;&#12290;&#36890;&#36807;&#21487;&#23398;&#20064;&#30340;&#21487;&#36870;&#21464;&#25442;&#23558;&#36825;&#31181;&#27969;&#24418;&#23884;&#20837;&#21040;&#25968;&#25454;&#30340;&#39640;&#32500;&#31354;&#38388;&#20013;&#12290;&#22240;&#27492;&#65292;&#19968;&#26086;&#36890;&#36807;&#37325;&#26500;&#25439;&#22833;&#27491;&#30830;&#23545;&#40784;&#27969;&#24418;&#65292;&#27969;&#24418;&#19978;&#30340;&#27010;&#29575;&#23494;&#24230;&#23601;&#26159;&#21487;&#35745;&#31639;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#26368;&#22823;&#20284;&#28982;&#26469;&#20248;&#21270;&#32593;&#32476;&#21442;&#25968;&#12290;&#33258;&#28982;&#22320;&#65292;&#25968;&#25454;&#30340;&#20302;&#32500;&#34920;&#31034;&#38656;&#35201;&#26159;&#21333;&#23556;&#26144;&#23556;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#24314;&#27169;&#30340;&#27969;&#24418;&#19978;&#23545;&#23494;&#24230;&#36827;&#34892;&#23545;&#20934;&#65292;&#24182;&#22312;&#23884;&#20837;&#21040;&#39640;&#32500;&#31354;&#38388;&#26102;&#39640;&#25928;&#35745;&#31639;&#23494;&#24230;&#20307;&#31215;&#21464;&#21270;&#39033;&#12290;&#28982;&#32780;&#65292;&#38500;&#38750;&#21333;&#23556;&#26144;&#23556;&#22312;&#35299;&#26512;&#19978;&#39044;&#23450;&#20041;&#65292;&#21542;&#21017;&#23398;&#20064;&#21040;&#30340;&#27969;&#24418;&#19981;&#19968;&#23450;&#26159;&#25968;&#25454;&#30340;&#26377;&#25928;&#34920;&#31034;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#36825;&#31181;&#27169;&#22411;&#30340;&#28508;&#22312;&#32500;&#24230;&#32463;&#24120;&#20250;&#23398;&#20064;&#21040;&#19982;&#27969;&#24418;&#30456;&#20851;&#24182;&#19988;&#36864;&#21270;&#30340;&#20869;&#22312;&#22522;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manifold learning flows are a class of generative modelling techniques that assume a low-dimensional manifold description of the data. The embedding of such manifold into the high-dimensional space of the data is achieved via learnable invertible transformations. Therefore, once the manifold is properly aligned via a reconstruction loss, the probability density is tractable on the manifold and maximum likelihood can be used optimize the network parameters. Naturally, the lower-dimensional representation of the data requires an injective-mapping. Recent approaches were able to enforce that density aligns with the modelled manifold, while efficiently calculating the density volume-change term when embedding to the higher-dimensional space. However, unless the injective-mapping is analytically predefined, the learned manifold is not necessarily an efficient representation of the data. Namely, the latent dimensions of such models frequently learn an entangled intrinsic basis with degenerat
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#33539;&#24335;&#19982;&#22343;&#22330;&#20998;&#24067;&#34920;&#31034;&#37197;&#23545;&#65292;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#22343;&#22330;&#21338;&#24328;&#21644;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#20174;&#20998;&#24067;&#20013;&#33719;&#21462;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#26080;&#38480;&#26102;&#22495;&#26694;&#26550;&#19979;&#20351;&#29992;&#32447;&#24615;&#20108;&#27425;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.10953</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36830;&#32493;&#31354;&#38388;&#26080;&#38480;&#26102;&#22495;&#22343;&#22330;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning for Infinite Horizon Mean Field Problems in Continuous Spaces. (arXiv:2309.10953v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10953
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#33539;&#24335;&#19982;&#22343;&#22330;&#20998;&#24067;&#34920;&#31034;&#37197;&#23545;&#65292;&#26469;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#20013;&#30340;&#22343;&#22330;&#21338;&#24328;&#21644;&#22343;&#22330;&#25511;&#21046;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#20174;&#20998;&#24067;&#20013;&#33719;&#21462;&#26679;&#26412;&#12290;&#35813;&#31639;&#27861;&#22312;&#28176;&#36817;&#26080;&#38480;&#26102;&#22495;&#26694;&#26550;&#19979;&#20351;&#29992;&#32447;&#24615;&#20108;&#27425;&#22522;&#20934;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#32479;&#19968;&#35299;&#20915;&#36830;&#32493;&#31354;&#38388;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#21644;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#38382;&#39064;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#20998;&#26512;&#21644;&#21457;&#23637;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65288;AC&#65289;&#33539;&#24335;&#19982;&#36890;&#36807;&#21442;&#25968;&#21270;&#35780;&#20998;&#20989;&#25968;&#34920;&#31034;&#30340;&#22343;&#22330;&#20998;&#24067;&#37197;&#23545;&#65292;&#21487;&#20197;&#20197;&#22312;&#32447;&#26041;&#24335;&#26377;&#25928;&#22320;&#26356;&#26032;&#65292;&#24182;&#20351;&#29992;&#26391;&#20043;&#19975;&#21160;&#21147;&#23398;&#20174;&#24471;&#21040;&#30340;&#20998;&#24067;&#20013;&#33719;&#24471;&#26679;&#26412;&#12290;AC&#20195;&#29702;&#21644;&#35780;&#20998;&#20989;&#25968;&#25353;&#36845;&#20195;&#26041;&#24335;&#36827;&#34892;&#26356;&#26032;&#65292;&#20197;&#25910;&#25947;&#21040;&#32473;&#23450;&#22343;&#22330;&#38382;&#39064;&#30340;MFG&#24179;&#34913;&#25110;MFC&#26368;&#20248;&#35299;&#65292;&#20855;&#20307;&#21462;&#20915;&#20110;&#23398;&#20064;&#29575;&#30340;&#36873;&#25321;&#12290;&#31639;&#27861;&#30340;&#31616;&#21333;&#20462;&#25913;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#20915;&#28151;&#21512;&#22343;&#22330;&#25511;&#21046;&#21338;&#24328;&#65288;MFCG&#65289;&#12290;&#25105;&#20204;&#20351;&#29992;&#28176;&#36817;&#26080;&#38480;&#26102;&#22495;&#26694;&#26550;&#20013;&#30340;&#32447;&#24615;&#20108;&#27425;&#22522;&#20934;&#35780;&#20272;&#25105;&#20204;&#30340;&#31639;&#27861;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present the development and analysis of a reinforcement learning (RL) algorithm designed to solve continuous-space mean field game (MFG) and mean field control (MFC) problems in a unified manner. The proposed approach pairs the actor-critic (AC) paradigm with a representation of the mean field distribution via a parameterized score function, which can be efficiently updated in an online fashion, and uses Langevin dynamics to obtain samples from the resulting distribution. The AC agent and the score function are updated iteratively to converge, either to the MFG equilibrium or the MFC optimum for a given mean field problem, depending on the choice of learning rates. A straightforward modification of the algorithm allows us to solve mixed mean field control games (MFCGs). The performance of our algorithm is evaluated using linear-quadratic benchmarks in the asymptotic infinite horizon framework.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;</title><link>http://arxiv.org/abs/2211.09619</link><description>&lt;p&gt;
&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#31616;&#20171;
&lt;/p&gt;
&lt;p&gt;
Introduction to Online Nonstochastic Control. (arXiv:2211.09619v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09619
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#21160;&#24577;&#31995;&#32479;&#25511;&#21046;&#19982;&#21487;&#24494;&#24378;&#21270;&#23398;&#20064;&#33539;&#24335;&#8212;&#8212;&#22312;&#32447;&#38750;&#38543;&#26426;&#25511;&#21046;&#65292;&#24182;&#24212;&#29992;&#22312;&#32447;&#20984;&#20248;&#21270;&#21644;&#20984;&#26494;&#24347;&#25216;&#26415;&#24471;&#21040;&#20102;&#20855;&#26377;&#21487;&#35777;&#26126;&#20445;&#35777;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#26368;&#20339;&#21644;&#40065;&#26834;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#19982;&#20854;&#20182;&#26694;&#26550;&#19981;&#21516;&#65292;&#35813;&#26041;&#27861;&#30340;&#30446;&#26631;&#26159;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#22312;&#26080;&#27861;&#39044;&#27979;&#25200;&#21160;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#19968;&#32452;&#31574;&#30053;&#20013;&#23547;&#25214;&#20302;&#21518;&#24724;&#65292;&#33719;&#24471;&#23545;&#26368;&#20248;&#31574;&#30053;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.  The primary distinction between online nonstochastic control and other frameworks is the objective. In optimal control, robust control, and other control methodologies that assume stochastic noise, the goal is to perform comparably to an offline optimal strategy. In online nonstochastic control, both the cost functions as well as the perturbations from the assumed dynamical model are chosen by an adversary. Thus the optimal policy is not defined a priori. Rather, the target is to attain low regret against the best policy in hindsight from a benchmark class of policies.  This objective suggests the use of the decision making frame
&lt;/p&gt;</description></item><item><title>&#35813;&#25945;&#31243;&#20171;&#32461;&#20102;&#20998;&#25674;&#20248;&#21270;&#30340;&#22522;&#30784;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2202.00665</link><description>&lt;p&gt;
&#20851;&#20110;&#20998;&#25674;&#20248;&#21270;&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Tutorial on amortized optimization. (arXiv:2202.00665v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00665
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25945;&#31243;&#20171;&#32461;&#20102;&#20998;&#25674;&#20248;&#21270;&#30340;&#22522;&#30784;&#65292;&#24182;&#24635;&#32467;&#20102;&#20854;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#19968;&#31181;&#26222;&#36941;&#30340;&#24314;&#27169;&#24037;&#20855;&#65292;&#32463;&#24120;&#22312;&#21453;&#22797;&#35299;&#20915;&#30456;&#21516;&#38382;&#39064;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#12290;&#20998;&#25674;&#20248;&#21270;&#26041;&#27861;&#20351;&#29992;&#23398;&#20064;&#26469;&#39044;&#27979;&#36825;&#20123;&#35774;&#32622;&#20013;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#30456;&#20284;&#38382;&#39064;&#23454;&#20363;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#12290;&#36825;&#20123;&#26041;&#27861;&#22312;&#21464;&#20998;&#25512;&#26029;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#33021;&#22815;&#27604;&#19981;&#20351;&#29992;&#20998;&#25674;&#30340;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#24555;&#20960;&#20010;&#25968;&#37327;&#32423;&#22320;&#35299;&#20915;&#20248;&#21270;&#38382;&#39064;&#12290;&#26412;&#27425;&#25945;&#31243;&#20171;&#32461;&#20102;&#36825;&#20123;&#36827;&#27493;&#32972;&#21518;&#30340;&#20998;&#25674;&#20248;&#21270;&#22522;&#30784;&#65292;&#24182;&#27010;&#36848;&#20102;&#23427;&#20204;&#22312;&#21464;&#20998;&#25512;&#26029;&#12289;&#31232;&#30095;&#32534;&#30721;&#12289;&#22522;&#20110;&#26799;&#24230;&#30340;&#20803;&#23398;&#20064;&#12289;&#25511;&#21046;&#12289;&#24378;&#21270;&#23398;&#20064;&#12289;&#20984;&#20248;&#21270;&#12289;&#26368;&#20248;&#20256;&#36755;&#21644;&#28145;&#24230;&#24179;&#34913;&#32593;&#32476;&#20013;&#30340;&#24212;&#29992;&#12290;&#26412;&#25945;&#31243;&#30340;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/facebookresearch/amortized-optimization-tutorial&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is a ubiquitous modeling tool and is often deployed in settings which repeatedly solve similar instances of the same problem. Amortized optimization methods use learning to predict the solutions to problems in these settings, exploiting the shared structure between similar problem instances. These methods have been crucial in variational inference and reinforcement learning and are capable of solving optimization problems many orders of magnitudes times faster than traditional optimization methods that do not use amortization. This tutorial presents an introduction to the amortized optimization foundations behind these advancements and overviews their applications in variational inference, sparse coding, gradient-based meta-learning, control, reinforcement learning, convex optimization, optimal transport, and deep equilibrium networks. The source code for this tutorial is available at https://github.com/facebookresearch/amortized-optimization-tutorial.
&lt;/p&gt;</description></item></channel></rss>