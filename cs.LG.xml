<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#23376;&#22330;&#35770;&#20013;&#39640;&#38454;&#36153;&#26364;&#22270;&#30340;&#35745;&#31639;&#22270;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#32455;&#25104;&#24352;&#37327;&#25805;&#20316;&#30340;&#20998;&#24418;&#32467;&#26500;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#20887;&#20313;&#65292;&#38598;&#25104;&#20102;Taylor-mode&#33258;&#21160;&#24494;&#20998;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#36153;&#26364;&#22270;&#32534;&#35793;&#22120;&#20197;&#20248;&#21270;&#35745;&#31639;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.18840</link><description>&lt;p&gt;
&#36153;&#26364;&#22270;&#20316;&#20026;&#35745;&#31639;&#22270;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Feynman Diagrams as Computational Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18840
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#37327;&#23376;&#22330;&#35770;&#20013;&#39640;&#38454;&#36153;&#26364;&#22270;&#30340;&#35745;&#31639;&#22270;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32452;&#32455;&#25104;&#24352;&#37327;&#25805;&#20316;&#30340;&#20998;&#24418;&#32467;&#26500;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#20887;&#20313;&#65292;&#38598;&#25104;&#20102;Taylor-mode&#33258;&#21160;&#24494;&#20998;&#25216;&#26415;&#65292;&#24320;&#21457;&#20102;&#36153;&#26364;&#22270;&#32534;&#35793;&#22120;&#20197;&#20248;&#21270;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#31354;&#38388;&#12289;&#26102;&#38388;&#12289;&#21160;&#37327;&#21644;&#39057;&#29575;&#39046;&#22495;&#30340;&#39640;&#38454;&#36153;&#26364;&#22270;&#30340;&#35745;&#31639;&#22270;&#34920;&#31034;&#65292;&#36866;&#29992;&#20110;&#37327;&#23376;&#22330;&#35770;&#65288;QFT&#65289;&#12290;&#21033;&#29992;&#25140;&#26862;-&#26045;&#28201;&#26684;&#26041;&#31243;&#21644;&#26641;&#22270;&#26041;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#23558;&#36825;&#20123;&#22270;&#32452;&#32455;&#25104;&#24352;&#37327;&#25805;&#20316;&#30340;&#20998;&#24418;&#32467;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#20887;&#20313;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#31616;&#21270;&#20102;&#22797;&#26434;&#22270;&#30340;&#35780;&#20272;&#65292;&#36824;&#20419;&#36827;&#20102;&#22330;&#35770;&#37325;&#25972;&#21270;&#26041;&#26696;&#30340;&#39640;&#25928;&#23454;&#26045;&#65292;&#23545;&#22686;&#24378;&#24494;&#25200;QFT&#35745;&#31639;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#19968;&#36827;&#23637;&#30340;&#20851;&#38190;&#22312;&#20110;&#38598;&#25104;&#20102;Taylor-mode&#33258;&#21160;&#24494;&#20998;&#65292;&#36825;&#26159;&#26426;&#22120;&#23398;&#20064;&#21253;&#20013;&#29992;&#20110;&#22312;&#35745;&#31639;&#22270;&#19978;&#39640;&#25928;&#35745;&#31639;&#39640;&#38454;&#23548;&#25968;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#25805;&#20316;&#21270;&#36825;&#20123;&#27010;&#24565;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36153;&#26364;&#22270;&#32534;&#35793;&#22120;&#65292;&#20248;&#21270;&#20102;&#21508;&#31181;&#35745;&#31639;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18840v1 Announce Type: cross  Abstract: We propose a computational graph representation of high-order Feynman diagrams in Quantum Field Theory (QFT), applicable to any combination of spatial, temporal, momentum, and frequency domains. Utilizing the Dyson-Schwinger and parquet equations, our approach effectively organizes these diagrams into a fractal structure of tensor operations, significantly reducing computational redundancy. This approach not only streamlines the evaluation of complex diagrams but also facilitates an efficient implementation of the field-theoretic renormalization scheme, crucial for enhancing perturbative QFT calculations. Key to this advancement is the integration of Taylor-mode automatic differentiation, a key technique employed in machine learning packages to compute higher-order derivatives efficiently on computational graphs. To operationalize these concepts, we develop a Feynman diagram compiler that optimizes diagrams for various computational pl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#27010;&#29575;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;Prob-PSENN&#65289;&#65292;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#21462;&#20195;&#28857;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#26356;&#28789;&#27963;&#30340;&#21407;&#22411;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.13740</link><description>&lt;p&gt;
&#36890;&#36807;&#27010;&#29575;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#35748;&#30693;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Explanations Through Probabilistic Self-Explainable Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13740
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#27010;&#29575;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;Prob-PSENN&#65289;&#65292;&#36890;&#36807;&#27010;&#29575;&#20998;&#24067;&#21462;&#20195;&#28857;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#26356;&#28789;&#27963;&#30340;&#21407;&#22411;&#23398;&#20064;&#65292;&#25552;&#20379;&#20102;&#23454;&#29992;&#30340;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#36879;&#26126;&#24615;&#25345;&#32493;&#38480;&#21046;&#20854;&#21487;&#38752;&#24615;&#21644;&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#27010;&#29575;&#33258;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65288;Prob-PSENN&#65289;&#65292;&#37319;&#29992;&#27010;&#29575;&#20998;&#24067;&#20195;&#26367;&#21407;&#22411;&#30340;&#28857;&#20272;&#35745;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#21407;&#22411;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13740v1 Announce Type: new  Abstract: The lack of transparency of Deep Neural Networks continues to be a limitation that severely undermines their reliability and usage in high-stakes applications. Promising approaches to overcome such limitations are Prototype-Based Self-Explainable Neural Networks (PSENNs), whose predictions rely on the similarity between the input at hand and a set of prototypical representations of the output classes, offering therefore a deep, yet transparent-by-design, architecture. So far, such models have been designed by considering pointwise estimates for the prototypes, which remain fixed after the learning phase of the model. In this paper, we introduce a probabilistic reformulation of PSENNs, called Prob-PSENN, which replaces point estimates for the prototypes with probability distributions over their values. This provides not only a more flexible framework for an end-to-end learning of prototypes, but can also capture the explanatory uncertaint
&lt;/p&gt;</description></item><item><title>XpertAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#29305;&#23450;&#33539;&#22260;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#27169;&#22411;&#30340;&#26597;&#35810;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.07486</link><description>&lt;p&gt;
XpertAI&#65306;&#25581;&#31034;&#23376;&#27969;&#24418;&#30340;&#27169;&#22411;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
XpertAI: uncovering model strategies for sub-manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07486
&lt;/p&gt;
&lt;p&gt;
XpertAI&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#29305;&#23450;&#33539;&#22260;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#27169;&#22411;&#30340;&#26597;&#35810;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#24050;&#32463;&#20419;&#36827;&#20102;&#28145;&#20837;&#39564;&#35777;&#21644;&#30693;&#35782;&#25552;&#21462;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#38024;&#23545;&#20998;&#31867;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#24456;&#23569;&#26377;XAI&#35299;&#20915;&#26041;&#26696;&#35299;&#20915;&#20102;&#29305;&#23450;&#20110;&#22238;&#24402;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#22312;&#22238;&#24402;&#20013;&#65292;&#35299;&#37322;&#38656;&#35201;&#31934;&#30830;&#21046;&#23450;&#20197;&#24212;&#23545;&#29305;&#23450;&#29992;&#25143;&#26597;&#35810;&#65288;&#20363;&#22914;&#21306;&#20998;&#8220;&#20026;&#20160;&#20040;&#36755;&#20986;&#22823;&#20110;0&#65311;&#8221;&#21644;&#8220;&#20026;&#20160;&#20040;&#36755;&#20986;&#22823;&#20110;50&#65311;&#8221;&#65289;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#24212;&#21453;&#26144;&#27169;&#22411;&#22312;&#30456;&#20851;&#25968;&#25454;&#23376;&#27969;&#24418;&#19978;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;XpertAI&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#39044;&#27979;&#31574;&#30053;&#35299;&#24320;&#20026;&#22810;&#20010;&#33539;&#22260;&#29305;&#23450;&#30340;&#23376;&#31574;&#30053;&#65292;&#24182;&#20801;&#35768;&#23558;&#23545;&#27169;&#22411;&#30340;&#31934;&#20934;&#26597;&#35810;&#65288;&#8220;&#34987;&#35299;&#37322;&#29289;&#8221;&#65289;&#30340;&#21046;&#23450;&#20026;&#36825;&#20123;&#23376;&#31574;&#30053;&#30340;&#32447;&#24615;&#32452;&#21512;&#30340;&#26694;&#26550;&#12290;XpertAI&#36890;&#24120;&#21046;&#23450;&#21487;&#20197;&#19982;&#22522;&#20110;&#36974;&#25377;&#12289;&#26799;&#24230;&#38598;&#25104;&#25110;&#21453;&#21521;&#20256;&#25773;&#30340;&#27969;&#34892;XAI&#24402;&#22240;&#25216;&#26415;&#19968;&#36215;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07486v1 Announce Type: new  Abstract: In recent years, Explainable AI (XAI) methods have facilitated profound validation and knowledge extraction from ML models. While extensively studied for classification, few XAI solutions have addressed the challenges specific to regression models. In regression, explanations need to be precisely formulated to address specific user queries (e.g.\ distinguishing between `Why is the output above 0?' and `Why is the output above 50?'). They should furthermore reflect the model's behavior on the relevant data sub-manifold. In this paper, we introduce XpertAI, a framework that disentangles the prediction strategy into multiple range-specific sub-strategies and allows the formulation of precise queries about the model (the `explanandum') as a linear combination of those sub-strategies. XpertAI is formulated generally to work alongside popular XAI attribution techniques, based on occlusion, gradient integration, or reverse propagation. Qualitat
&lt;/p&gt;</description></item><item><title>GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14009</link><description>&lt;p&gt;
&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Geometry-Informed Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14009
&lt;/p&gt;
&lt;p&gt;
GINNs&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#29983;&#25104;&#22810;&#26679;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26080;&#38656;&#35757;&#32451;&#25968;&#25454;&#65292;&#37319;&#29992;&#26174;&#24335;&#22810;&#26679;&#24615;&#25439;&#22833;&#20197;&#21450;&#21487;&#24494;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#20102;&#20854;&#22312;&#21508;&#31181;&#22797;&#26434;&#24615;&#22330;&#26223;&#20013;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#20309;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;GINNs&#65289;&#30340;&#27010;&#24565;&#65292;&#28085;&#30422;&#20102;&#65288;i&#65289;&#22312;&#20960;&#20309;&#32422;&#26463;&#19979;&#23398;&#20064;&#65292;&#65288;ii&#65289;&#31070;&#32463;&#22330;&#20316;&#20026;&#21512;&#36866;&#30340;&#34920;&#31034;&#65292;&#65288;iii&#65289;&#29983;&#25104;&#22312;&#20960;&#20309;&#20219;&#21153;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#27424;&#23450;&#31995;&#32479;&#30340;&#22810;&#26679;&#35299;&#20915;&#26041;&#26696;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;GINN&#30340;&#26500;&#24314;&#19981;&#38656;&#35201;&#35757;&#32451;&#25968;&#25454;&#65292;&#22240;&#27492;&#21487;&#20197;&#34987;&#32431;&#32422;&#26463;&#39537;&#21160;&#22320;&#35270;&#20026;&#29983;&#25104;&#24314;&#27169;&#12290;&#25105;&#20204;&#22686;&#21152;&#20102;&#26174;&#24335;&#30340;&#22810;&#26679;&#24615;&#25439;&#22833;&#26469;&#20943;&#36731;&#27169;&#24577;&#22349;&#32553;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#20960;&#31181;&#32422;&#26463;&#65292;&#29305;&#21035;&#26159;&#32452;&#20214;&#30340;&#36830;&#36890;&#24615;&#65292;&#25105;&#20204;&#36890;&#36807;&#33707;&#23572;&#26031;&#29702;&#35770;&#23558;&#20854;&#36716;&#21270;&#20026;&#21487;&#24494;&#25439;&#22833;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#26029;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#20108;&#32500;&#21644;&#19977;&#32500;&#22330;&#26223;&#20013;&#65292;GINN&#23398;&#20064;&#33539;&#24335;&#30340;&#39640;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14009v1 Announce Type: new  Abstract: We introduce the concept of geometry-informed neural networks (GINNs), which encompass (i) learning under geometric constraints, (ii) neural fields as a suitable representation, and (iii) generating diverse solutions to under-determined systems often encountered in geometric tasks. Notably, the GINN formulation does not require training data, and as such can be considered generative modeling driven purely by constraints. We add an explicit diversity loss to mitigate mode collapse. We consider several constraints, in particular, the connectedness of components which we convert to a differentiable loss through Morse theory. Experimentally, we demonstrate the efficacy of the GINN learning paradigm across a range of two and three-dimensional scenarios with increasing levels of complexity.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#65288;STL&#65289;&#25512;&#26029;&#21644;&#25511;&#21046;&#21512;&#25104;&#30340;&#26032;&#39062;&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#34920;&#31034;&#20219;&#21153;&#20026;STL&#20844;&#24335;&#65292;&#21516;&#26102;&#36890;&#36807;&#20154;&#20026;&#35843;&#25972;STL&#20844;&#24335;&#23454;&#29616;&#23545;&#20154;&#31867;&#30693;&#35782;&#30340;&#32435;&#20837;&#21644;&#26032;&#22330;&#26223;&#30340;&#36866;&#24212;&#65292;&#36824;&#37319;&#29992;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#32553;&#23567;&#20102;&#19987;&#23478;&#31574;&#30053;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;</title><link>https://arxiv.org/abs/2402.10310</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Interpretable Generative Adversarial Imitation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10310
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#65288;STL&#65289;&#25512;&#26029;&#21644;&#25511;&#21046;&#21512;&#25104;&#30340;&#26032;&#39062;&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26126;&#30830;&#34920;&#31034;&#20219;&#21153;&#20026;STL&#20844;&#24335;&#65292;&#21516;&#26102;&#36890;&#36807;&#20154;&#20026;&#35843;&#25972;STL&#20844;&#24335;&#23454;&#29616;&#23545;&#20154;&#31867;&#30693;&#35782;&#30340;&#32435;&#20837;&#21644;&#26032;&#22330;&#26223;&#30340;&#36866;&#24212;&#65292;&#36824;&#37319;&#29992;&#20102;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#26377;&#25928;&#32553;&#23567;&#20102;&#19987;&#23478;&#31574;&#30053;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#24050;&#32463;&#36890;&#36807;&#19987;&#23478;&#28436;&#31034;&#22312;&#25945;&#25480;&#33258;&#20027;&#31995;&#32479;&#22797;&#26434;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#30456;&#24403;&#22823;&#30340;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#22312;&#20110;&#23427;&#20204;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#23398;&#20064;&#20195;&#29702;&#35797;&#22270;&#23436;&#25104;&#30340;&#20855;&#20307;&#20219;&#21153;&#26041;&#38754;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#20449;&#21495;&#26102;&#24207;&#36923;&#36753;&#65288;STL&#65289;&#25512;&#26029;&#21644;&#25511;&#21046;&#21512;&#25104;&#30340;&#26032;&#39062;&#20223;&#30495;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#20219;&#21153;&#21487;&#20197;&#26126;&#30830;&#34920;&#31034;&#20026;STL&#20844;&#24335;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#21487;&#20197;&#28165;&#26224;&#22320;&#29702;&#35299;&#20219;&#21153;&#65292;&#36824;&#21487;&#20197;&#36890;&#36807;&#25163;&#21160;&#35843;&#25972;STL&#20844;&#24335;&#26469;&#23558;&#20154;&#31867;&#30693;&#35782;&#32435;&#20837;&#24182;&#36866;&#24212;&#26032;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21463;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21551;&#21457;&#30340;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#25512;&#26029;&#21644;&#25511;&#21046;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#32553;&#23567;&#20102;&#19987;&#23478;&#31574;&#30053;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10310v1 Announce Type: new  Abstract: Imitation learning methods have demonstrated considerable success in teaching autonomous systems complex tasks through expert demonstrations. However, a limitation of these methods is their lack of interpretability, particularly in understanding the specific task the learning agent aims to accomplish. In this paper, we propose a novel imitation learning method that combines Signal Temporal Logic (STL) inference and control synthesis, enabling the explicit representation of the task as an STL formula. This approach not only provides a clear understanding of the task but also allows for the incorporation of human knowledge and adaptation to new scenarios through manual adjustments of the STL formulae. Additionally, we employ a Generative Adversarial Network (GAN)-inspired training approach for both the inference and the control policy, effectively narrowing the gap between the expert and learned policies. The effectiveness of our algorithm
&lt;/p&gt;</description></item><item><title>&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2311.04938</link><description>&lt;p&gt;
&#20351;&#29992;&#30697;&#21305;&#37197;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#25913;&#36827;&#20102;DDIM&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.04938
&lt;/p&gt;
&lt;p&gt;
&#22312;DDIM&#26694;&#26550;&#20013;&#20351;&#29992;GMM&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#21487;&#20197;&#33719;&#24471;&#36136;&#37327;&#26356;&#39640;&#30340;&#26679;&#26412;&#12290;&#22312;&#26080;&#26465;&#20214;&#27169;&#22411;&#21644;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#25913;&#36827;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#22312;Denoising Diffusion Implicit Models (DDIM)&#26694;&#26550;&#20013;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65288;GMM&#65289;&#20316;&#20026;&#21453;&#21521;&#36716;&#31227;&#31639;&#23376;&#65288;&#20869;&#26680;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20174;&#39044;&#35757;&#32451;&#30340;Denoising Diffusion Probabilistic Models (DDPM)&#20013;&#21152;&#36895;&#37319;&#26679;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#20043;&#19968;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#32422;&#26463;GMM&#30340;&#21442;&#25968;&#65292;&#21305;&#37197;DDPM&#21069;&#21521;&#36793;&#38469;&#30340;&#19968;&#38454;&#21644;&#20108;&#38454;&#20013;&#24515;&#30697;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#30697;&#21305;&#37197;&#65292;&#21487;&#20197;&#33719;&#24471;&#19982;&#20351;&#29992;&#39640;&#26031;&#26680;&#30340;&#21407;&#22987;DDIM&#30456;&#21516;&#25110;&#26356;&#22909;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;CelebAHQ&#21644;FFHQ&#30340;&#26080;&#26465;&#20214;&#27169;&#22411;&#20197;&#21450;ImageNet&#25968;&#25454;&#38598;&#30340;&#31867;&#26465;&#20214;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#23454;&#39564;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#37319;&#26679;&#27493;&#39588;&#36739;&#23569;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;GMM&#20869;&#26680;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#36890;&#36807;FID&#21644;IS&#25351;&#26631;&#34913;&#37327;&#30340;&#12290;&#20363;&#22914;&#65292;&#22312;ImageNet 256x256&#19978;&#65292;&#20351;&#29992;10&#20010;&#37319;&#26679;&#27493;&#39588;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;FID&#20540;&#20026;...
&lt;/p&gt;
&lt;p&gt;
We propose using a Gaussian Mixture Model (GMM) as reverse transition operator (kernel) within the Denoising Diffusion Implicit Models (DDIM) framework, which is one of the most widely used approaches for accelerated sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM). Specifically we match the first and second order central moments of the DDPM forward marginals by constraining the parameters of the GMM. We see that moment matching is sufficient to obtain samples with equal or better quality than the original DDIM with Gaussian kernels. We provide experimental results with unconditional models trained on CelebAHQ and FFHQ and class-conditional models trained on ImageNet datasets respectively. Our results suggest that using the GMM kernel leads to significant improvements in the quality of the generated samples when the number of sampling steps is small, as measured by FID and IS metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a FID of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#22120;&#36716;&#25442;&#20026;&#31561;&#25928;&#36719;&#20915;&#31574;&#26641;&#25511;&#21046;&#22120;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19988;&#33410;&#32422;&#25104;&#26412;&#30340;&#36716;&#25442;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21253;&#25324;ReLU&#28608;&#27963;&#20989;&#25968;&#22312;&#20869;&#30340;&#31163;&#25955;&#36755;&#20986;NN&#25511;&#21046;&#22120;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#24418;&#24335;&#39564;&#35777;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.06049</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#21040;&#20915;&#31574;&#26641;&#25511;&#21046;&#22120;&#30340;&#31934;&#30830;&#19988;&#33410;&#32422;&#25104;&#26412;&#30340;&#33258;&#21160;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Exact and Cost-Effective Automated Transformation of Neural Network Controllers to Decision Tree Controllers. (arXiv:2304.06049v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25511;&#21046;&#22120;&#36716;&#25442;&#20026;&#31561;&#25928;&#36719;&#20915;&#31574;&#26641;&#25511;&#21046;&#22120;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#19988;&#33410;&#32422;&#25104;&#26412;&#30340;&#36716;&#25442;&#31639;&#27861;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#21253;&#25324;ReLU&#28608;&#27963;&#20989;&#25968;&#22312;&#20869;&#30340;&#31163;&#25955;&#36755;&#20986;NN&#25511;&#21046;&#22120;&#65292;&#24182;&#33021;&#22815;&#25552;&#39640;&#24418;&#24335;&#39564;&#35777;&#30340;&#36816;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#30340;&#25511;&#21046;&#22120;&#22312;&#21508;&#31181;&#20915;&#31574;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#30528;&#30340;&#21151;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#40657;&#30418;&#29305;&#24615;&#21644;&#24847;&#22806;&#34892;&#20026;&#21644;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#30340;&#39118;&#38505;&#23545;&#20110;&#22312;&#20855;&#26377;&#27491;&#30830;&#24615;&#21644;&#23433;&#20840;&#24615;&#24378;&#20445;&#35777;&#30340;&#30495;&#23454;&#19990;&#30028;&#31995;&#32479;&#20013;&#30340;&#37096;&#32626;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#23558;&#22522;&#20110;NN&#30340;&#25511;&#21046;&#22120;&#36716;&#25442;&#20026;&#31561;&#25928;&#30340;&#36719;&#20915;&#31574;&#26641;&#65288;SDT&#65289;&#25511;&#21046;&#22120;&#21450;&#20854;&#23545;&#21487;&#39564;&#35777;&#24615;&#30340;&#24433;&#21709;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#31163;&#25955;&#36755;&#20986;NN&#25511;&#21046;&#22120;&#65292;&#21253;&#25324;&#25972;&#27969;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#20989;&#25968;&#20197;&#21450;argmax&#25805;&#20316;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#31934;&#30830;&#20294;&#33410;&#30465;&#25104;&#26412;&#30340;&#36716;&#25442;&#31639;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#33258;&#21160;&#21024;&#38500;&#22810;&#20313;&#30340;&#20998;&#25903;&#12290;&#25105;&#20204;&#20351;&#29992;OpenAI Gym&#29615;&#22659;&#30340;&#20004;&#20010;&#22522;&#20934;&#27979;&#35797;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;SDT&#36716;&#25442;&#21487;&#20197;&#20351;&#24418;&#24335;&#39564;&#35777;&#21463;&#30410;&#65292;&#26174;&#31034;&#36816;&#34892;&#26102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, neural network (NN)-based controllers have demonstrated remarkable efficacy in a variety of decision-making tasks. However, their black-box nature and the risk of unexpected behaviors and surprising results pose a challenge to their deployment in real-world systems with strong guarantees of correctness and safety. We address these limitations by investigating the transformation of NN-based controllers into equivalent soft decision tree (SDT)-based controllers and its impact on verifiability. Differently from previous approaches, we focus on discrete-output NN controllers including rectified linear unit (ReLU) activation functions as well as argmax operations. We then devise an exact but cost-effective transformation algorithm, in that it can automatically prune redundant branches. We evaluate our approach using two benchmarks from the OpenAI Gym environment. Our results indicate that the SDT transformation can benefit formal verification, showing runtime improveme
&lt;/p&gt;</description></item></channel></rss>