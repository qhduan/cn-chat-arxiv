<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01454</link><description>&lt;p&gt;
&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;: &#19968;&#31181;&#32479;&#35745;&#22240;&#26524;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01454
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22240;&#26524;&#21457;&#29616;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#19982;&#30693;&#35782;&#22686;&#24378;&#30456;&#32467;&#21512;&#65292;&#21487;&#20197;&#20351;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#30340;&#32479;&#35745;&#22240;&#26524;&#21457;&#29616;&#65288;SCD&#65289;&#20013;&#65292;&#23558;&#39046;&#22495;&#19987;&#23478;&#30693;&#35782;&#20316;&#20026;&#32422;&#26463;&#23884;&#20837;&#21040;&#31639;&#27861;&#20013;&#34987;&#24191;&#27867;&#25509;&#21463;&#65292;&#22240;&#20026;&#36825;&#23545;&#20110;&#21019;&#24314;&#19968;&#33268;&#26377;&#24847;&#20041;&#30340;&#22240;&#26524;&#27169;&#22411;&#26159;&#37325;&#35201;&#30340;&#65292;&#23613;&#31649;&#35782;&#21035;&#32972;&#26223;&#30693;&#35782;&#30340;&#25361;&#25112;&#34987;&#35748;&#21487;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22240;&#26524;&#25512;&#26029;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;LLM&#30340;&#8220;&#32479;&#35745;&#22240;&#26524;&#25552;&#31034;&#65288;SCP&#65289;&#8221;&#19982;SCD&#26041;&#27861;&#21644;&#22522;&#20110;&#30693;&#35782;&#30340;&#22240;&#26524;&#25512;&#26029;&#65288;KBCI&#65289;&#30456;&#32467;&#21512;&#65292;&#23545;SCD&#36827;&#34892;&#20808;&#39564;&#30693;&#35782;&#22686;&#24378;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;GPT-4&#21487;&#20197;&#20351;LLM-KBCI&#30340;&#36755;&#20986;&#19982;&#24102;&#26377;LLM-KBCI&#30340;&#20808;&#39564;&#30693;&#35782;&#30340;SCD&#32467;&#26524;&#25509;&#36817;&#30495;&#23454;&#24773;&#20917;&#65292;&#22914;&#26524;GPT-4&#32463;&#21382;&#20102;SCP&#65292;&#37027;&#20040;SCD&#30340;&#32467;&#26524;&#36824;&#21487;&#20197;&#36827;&#19968;&#27493;&#25913;&#21892;&#12290;&#32780;&#19988;&#65292;&#21363;&#20351;LLM&#19981;&#21547;&#26377;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#65292;LLM&#20173;&#28982;&#21487;&#20197;&#36890;&#36807;&#20854;&#32972;&#26223;&#30693;&#35782;&#26469;&#25913;&#36827;SCD&#12290;
&lt;/p&gt;
&lt;p&gt;
In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is widely accepted as significant for creating consistent meaningful causal models, despite the recognized challenges in systematic acquisition of the background knowledge. To overcome these challenges, this paper proposes a novel methodology for causal inference, in which SCD methods and knowledge based causal inference (KBCI) with a large language model (LLM) are synthesized through "statistical causal prompting (SCP)" for LLMs and prior knowledge augmentation for SCD. Experiments have revealed that GPT-4 can cause the output of the LLM-KBCI and the SCD result with prior knowledge from LLM-KBCI to approach the ground truth, and that the SCD result can be further improved, if GPT-4 undergoes SCP. Furthermore, it has been clarified that an LLM can improve SCD with its background knowledge, even if the LLM does not contain information on the dataset. The proposed approach
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#21033;&#29992;&#38544;&#24335;&#26799;&#24230;&#20256;&#36755;&#21644;&#22522;&#20110;Hessian&#30340;&#25216;&#26415;&#65292;&#20998;&#21035;&#30830;&#20445;&#20102;$\tilde{\mathcal{O}}(T^{3/5})$&#21644;$\tilde{\mathcal{O}}(\sqrt{T})$&#25968;&#37327;&#32423;&#30340;&#26399;&#26395;&#21518;&#24724;&#12290;</title><link>https://arxiv.org/abs/2404.02108</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#26041;&#24046;&#32553;&#20943;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Variance-Reduced Policy Gradient Approaches for Infinite Horizon Average Reward Markov Decision Processes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02108
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#20998;&#21035;&#21033;&#29992;&#38544;&#24335;&#26799;&#24230;&#20256;&#36755;&#21644;&#22522;&#20110;Hessian&#30340;&#25216;&#26415;&#65292;&#20998;&#21035;&#30830;&#20445;&#20102;$\tilde{\mathcal{O}}(T^{3/5})$&#21644;$\tilde{\mathcal{O}}(\sqrt{T})$&#25968;&#37327;&#32423;&#30340;&#26399;&#26395;&#21518;&#24724;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#26080;&#38480;&#26102;&#22495;&#24179;&#22343;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#32972;&#26223;&#19979;&#65292;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#19968;&#33324;&#21442;&#25968;&#21270;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20351;&#29992;&#38544;&#24335;&#26799;&#24230;&#20256;&#36755;&#36827;&#34892;&#26041;&#24046;&#32553;&#20943;&#65292;&#30830;&#20445;&#26399;&#26395;&#21518;&#24724;&#30340;&#25968;&#37327;&#32423;&#20026;$\tilde{\mathcal{O}}(T^{3/5})$&#12290;&#31532;&#20108;&#31181;&#26041;&#27861;&#65292;&#26681;&#26893;&#20110;&#22522;&#20110;Hessian&#30340;&#25216;&#26415;&#65292;&#30830;&#20445;&#26399;&#26395;&#21518;&#24724;&#30340;&#25968;&#37327;&#32423;&#20026;$\tilde{\mathcal{O}}(\sqrt{T})$&#12290;&#36825;&#20123;&#32467;&#26524;&#26174;&#33879;&#25913;&#36827;&#20102;&#35813;&#38382;&#39064;&#30340;&#29616;&#26377;&#25216;&#26415;&#27700;&#24179;&#65292;&#20854;&#21518;&#24724;&#29575;&#20026;$\tilde{\mathcal{O}}(T^{3/4})&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02108v1 Announce Type: new  Abstract: We present two Policy Gradient-based methods with general parameterization in the context of infinite horizon average reward Markov Decision Processes. The first approach employs Implicit Gradient Transport for variance reduction, ensuring an expected regret of the order $\tilde{\mathcal{O}}(T^{3/5})$. The second approach, rooted in Hessian-based techniques, ensures an expected regret of the order $\tilde{\mathcal{O}}(\sqrt{T})$. These results significantly improve the state of the art of the problem, which achieves a regret of $\tilde{\mathcal{O}}(T^{3/4})$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#36827;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.12338</link><description>&lt;p&gt;
&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Stochastic Halpern iteration in normed spaces and applications to reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12338
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;Halpern&#36845;&#20195;&#22312;&#36171;&#33539;&#31354;&#38388;&#20013;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#31639;&#27861;&#22797;&#26434;&#24230;&#65292;&#36827;&#32780;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21516;&#27493;&#31639;&#27861;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20998;&#26512;&#20102;&#20855;&#26377;&#26041;&#24046;&#20943;&#23569;&#30340;&#38543;&#26426;Halpern&#36845;&#20195;&#30340;Oracle&#22797;&#26434;&#24230;&#65292;&#26088;&#22312;&#36817;&#20284;&#26377;&#30028;&#21644;&#25910;&#32553;&#31639;&#23376;&#30340;&#19981;&#21160;&#28857;&#22312;&#19968;&#20010;&#26377;&#38480;&#32500;&#36171;&#33539;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22914;&#26524;&#24213;&#23618;&#30340;&#38543;&#26426;Oracle&#20855;&#26377;&#19968;&#33268;&#26377;&#30028;&#30340;&#26041;&#24046;&#65292;&#21017;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20986;&#24635;&#30340;Oracle&#22797;&#26434;&#24230;&#20026;$ \tilde{O} (\varepsilon^{-5})$&#65292;&#25913;&#36827;&#20102;&#26368;&#36817;&#20026;&#38543;&#26426;Krasnoselskii-Mann&#36845;&#20195;&#24314;&#31435;&#30340;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102; $\Omega (\varepsilon^{-3})$&#30340;&#19979;&#30028;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#33539;&#22260;&#30340;&#31639;&#27861;&#65292;&#21253;&#25324;&#25152;&#26377;&#24102;&#26377;&#23567;&#25209;&#22788;&#29702;&#30340;&#24179;&#22343;&#36845;&#20195;&#12290;&#36890;&#36807;&#36866;&#24403;&#20462;&#25913;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#20102;&#22312;&#31639;&#23376;&#20026; $\gamma$-&#25910;&#32553;&#30340;&#24773;&#20917;&#19979;&#19968;&#20010; $O(\varepsilon^{-2}(1-\gamma)^{-3})$&#22797;&#26434;&#24230;&#19978;&#30028;&#12290;&#20316;&#20026;&#19968;&#20010;&#24212;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#29992;&#20110;&#24179;&#22343;&#22870;&#21169;&#21644;&#25240;&#25187;&#22870;&#21169;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#30340;&#21516;&#27493;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12338v1 Announce Type: cross  Abstract: We analyze the oracle complexity of the stochastic Halpern iteration with variance reduction, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space. We show that if the underlying stochastic oracle is with uniformly bounded variance, our method exhibits an overall oracle complexity of $\tilde{O}(\varepsilon^{-5})$, improving recent rates established for the stochastic Krasnoselskii-Mann iteration. Also, we establish a lower bound of $\Omega(\varepsilon^{-3})$, which applies to a wide range of algorithms, including all averaged iterations even with minibatching. Using a suitable modification of our approach, we derive a $O(\varepsilon^{-2}(1-\gamma)^{-3})$ complexity bound in the case in which the operator is a $\gamma$-contraction. As an application, we propose new synchronous algorithms for average reward and discounted reward Markov decision processes. In particular, f
&lt;/p&gt;</description></item><item><title>LOOPer&#26159;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#24314;&#31435;&#25104;&#26412;&#27169;&#22411;&#26469;&#25351;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#25628;&#32034;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#32534;&#35793;&#22120;&#22312;&#36873;&#25321;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.11522</link><description>&lt;p&gt;
LOOPer: &#19968;&#20010;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11522
&lt;/p&gt;
&lt;p&gt;
LOOPer&#26159;&#38024;&#23545;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#30340;&#23398;&#20064;&#22411;&#33258;&#21160;&#20195;&#30721;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#24314;&#31435;&#25104;&#26412;&#27169;&#22411;&#26469;&#25351;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#25628;&#32034;&#65292;&#31361;&#30772;&#20102;&#20256;&#32479;&#32534;&#35793;&#22120;&#22312;&#36873;&#25321;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#22312;&#23454;&#29616;&#39640;&#32423;&#20195;&#30721;&#36716;&#25442;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#25104;&#21151;&#65292;&#20294;&#22312;&#36873;&#25321;&#33021;&#22815;&#24102;&#26469;&#26368;&#20339;&#21152;&#36895;&#30340;&#26368;&#26377;&#21033;&#36716;&#25442;&#26041;&#38754;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#36825;&#20419;&#20351;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26500;&#24314;&#25104;&#26412;&#27169;&#22411;&#26469;&#24341;&#23548;&#22810;&#38754;&#20307;&#20248;&#21270;&#30340;&#25628;&#32034;&#12290;&#26368;&#20808;&#36827;&#30340;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#24050;&#32463;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#27010;&#24565;&#39564;&#35777;&#12290;&#34429;&#28982;&#36825;&#31181;&#27010;&#24565;&#39564;&#35777;&#26174;&#31034;&#20986;&#20102;&#24076;&#26395;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#26174;&#33879;&#38480;&#21046;&#12290;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#26368;&#20808;&#36827;&#22810;&#38754;&#20307;&#32534;&#35793;&#22120;&#21482;&#25903;&#25345;&#23569;&#37327;&#20223;&#23556;&#21464;&#25442;&#30340;&#23376;&#38598;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#24212;&#29992;&#22797;&#26434;&#20195;&#30721;&#21464;&#25442;&#30340;&#33021;&#21147;&#12290;&#23427;&#20204;&#36824;&#21482;&#25903;&#25345;&#20855;&#26377;&#21333;&#20010;&#24490;&#29615;&#23884;&#22871;&#21644;&#30697;&#24418;&#36845;&#20195;&#22495;&#30340;&#31616;&#21333;&#31243;&#24207;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#35768;&#22810;&#31243;&#24207;&#30340;&#36866;&#29992;&#24615;&#12290;&#36825;&#20123;&#38480;&#21046;&#26174;&#33879;&#24433;&#21709;&#20102;&#36825;&#26679;&#30340;&#32534;&#35793;&#22120;&#21644;&#33258;&#21160;&#35843;&#24230;&#22120;&#30340;&#36890;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11522v1 Announce Type: cross  Abstract: While polyhedral compilers have shown success in implementing advanced code transformations, they still have challenges in selecting the most profitable transformations that lead to the best speedups. This has motivated the use of machine learning to build cost models to guide the search for polyhedral optimizations. State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of this approach. While such a proof-of-concept has shown promise, it still has significant limitations. State-of-the-art polyhedral compilers that use a deep-learning cost model only support a small subset of affine transformations, limiting their ability to apply complex code transformations. They also only support simple programs that have a single loop nest and a rectangular iteration domain, limiting their applicability to many programs. These limitations significantly impact the generality of such compilers and autoschedulers and put in
&lt;/p&gt;</description></item><item><title>&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.10266</link><description>&lt;p&gt;
DSP&#65306;&#22810;&#32500;Transformer&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10266
&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#36890;&#36807;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#32500;&#24230;&#23454;&#29616;&#23545;&#22810;&#32500;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26412;&#25991;&#20171;&#32461;&#30340;&#21160;&#24577;&#24207;&#21015;&#24182;&#34892;&#24615;&#65288;DSP&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20026;&#22810;&#32500;Transformer&#27169;&#22411;&#23454;&#29616;&#39640;&#25928;&#30340;&#24207;&#21015;&#24182;&#34892;&#24615;&#12290;&#20854;&#20851;&#38190;&#24605;&#24819;&#26159;&#26681;&#25454;&#24403;&#21069;&#35745;&#31639;&#38454;&#27573;&#21160;&#24577;&#20999;&#25442;&#24182;&#34892;&#24615;&#32500;&#24230;&#65292;&#21033;&#29992;&#22810;&#32500;&#27880;&#24847;&#21147;&#30340;&#28508;&#22312;&#29305;&#24615;&#12290;&#36825;&#31181;&#21160;&#24577;&#32500;&#24230;&#20999;&#25442;&#20351;&#24471;&#24207;&#21015;&#24182;&#34892;&#24615;&#22312;&#22810;&#32500;&#27169;&#22411;&#20013;&#20855;&#26377;&#26368;&#23567;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10266v1 Announce Type: cross  Abstract: Scaling large models with long sequences across applications like language generation, video generation and multimodal tasks requires efficient sequence parallelism. However, existing sequence parallelism methods all assume a single sequence dimension and fail to adapt to multi-dimensional transformer architectures that perform attention calculations across different dimensions. This paper introduces Dynamic Sequence Parallelism (DSP), a novel approach to enable efficient sequence parallelism for multi-dimensional transformer models. The key idea is to dynamically switch the parallelism dimension according to the current computation stage, leveraging the potential characteristics of multi-dimensional attention. This dynamic dimension switching allows sequence parallelism with minimal communication overhead compared to applying traditional single-dimension parallelism to multi-dimensional models. Experiments show DSP improves end-to-end
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24067;&#23616;&#26159;&#21542;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#23545;&#21253;&#21547;&#20167;&#24680;&#35328;&#35770;&#21477;&#23376;&#30340;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#35299;&#37322;&#24067;&#23616;&#22312;&#35302;&#21457;&#21442;&#19982;&#32773;&#25552;&#20379;&#32416;&#27491;&#24615;&#21453;&#39304;&#21644;&#35780;&#20272;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.05581</link><description>&lt;p&gt;
&#35299;&#37322;&#24067;&#23616;&#21487;&#20197;&#24433;&#21709;&#20154;&#23545;&#20882;&#29359;&#24615;&#21477;&#23376;&#30340;&#24863;&#30693;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Interpretability Layouts Influence Human Perception of Offensive Sentences?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#35299;&#37322;&#24067;&#23616;&#26159;&#21542;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#23545;&#21253;&#21547;&#20167;&#24680;&#35328;&#35770;&#21477;&#23376;&#30340;&#35780;&#20215;&#65292;&#32467;&#26524;&#34920;&#26126;&#35299;&#37322;&#24067;&#23616;&#22312;&#35302;&#21457;&#21442;&#19982;&#32773;&#25552;&#20379;&#32416;&#27491;&#24615;&#21453;&#39304;&#21644;&#35780;&#20272;&#27169;&#22411;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#19968;&#39033;&#29992;&#25143;&#30740;&#31350;&#65292;&#35780;&#20272;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35299;&#37322;&#24067;&#23616;&#26159;&#21542;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#35780;&#20272;&#21253;&#21547;&#20167;&#24680;&#35328;&#35770;&#30340;&#21477;&#23376;&#26102;&#30340;&#35266;&#28857;&#65292;&#37325;&#28857;&#20851;&#27880;&#8220;&#21388;&#24694;&#22899;&#24615;&#8221;&#21644;&#8220;&#31181;&#26063;&#20027;&#20041;&#8221;&#20004;&#31867;&#12290;&#37492;&#20110;&#25991;&#29486;&#20013;&#23384;&#22312;&#20998;&#27495;&#30340;&#32467;&#35770;&#65292;&#25105;&#20204;&#36890;&#36807;&#32479;&#35745;&#21644;&#23450;&#24615;&#20998;&#26512;&#38382;&#21367;&#35843;&#26597;&#22238;&#24212;&#30340;&#23454;&#35777;&#35777;&#25454;&#65292;&#25506;&#35752;&#22312;&#22312;&#32447;&#31038;&#21306;&#20013;&#20351;&#29992;ML&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#12290;&#24191;&#20041;&#21487;&#21152;&#27169;&#22411;&#20272;&#35745;&#21442;&#19982;&#32773;&#30340;&#35780;&#32423;&#65292;&#34701;&#21512;&#20102;&#32452;&#20869;&#35774;&#35745;&#21644;&#32452;&#38388;&#35774;&#35745;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#32479;&#35745;&#20998;&#26512;&#34920;&#26126;&#65292;&#27809;&#26377;&#20219;&#20309;&#35299;&#37322;&#24067;&#23616;&#26174;&#33879;&#24433;&#21709;&#21442;&#19982;&#32773;&#30340;&#35266;&#28857;&#65292;&#20294;&#25105;&#20204;&#30340;&#23450;&#24615;&#20998;&#26512;&#34920;&#26126;ML&#35299;&#37322;&#24615;&#30340;&#20248;&#21183;&#65306;1&#65289;&#35302;&#21457;&#21442;&#19982;&#32773;&#22312;&#20182;&#20204;&#30340;&#35266;&#28857;&#19982;&#27169;&#22411;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#26102;&#25552;&#20379;&#32416;&#27491;&#24615;&#21453;&#39304;&#65292;2&#65289;&#25552;&#20379;&#35780;&#20272;&#27169;&#22411;&#30340;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05581v1 Announce Type: cross  Abstract: This paper conducts a user study to assess whether three machine learning (ML) interpretability layouts can influence participants' views when evaluating sentences containing hate speech, focusing on the "Misogyny" and "Racism" classes. Given the existence of divergent conclusions in the literature, we provide empirical evidence on using ML interpretability in online communities through statistical and qualitative analyses of questionnaire responses. The Generalized Additive Model estimates participants' ratings, incorporating within-subject and between-subject designs. While our statistical analysis indicates that none of the interpretability layouts significantly influences participants' views, our qualitative analysis demonstrates the advantages of ML interpretability: 1) triggering participants to provide corrective feedback in case of discrepancies between their views and the model, and 2) providing insights to evaluate a model's 
&lt;/p&gt;</description></item><item><title>&#22312;&#37325;&#23614;&#25200;&#21160;&#19979;&#65292;&#22122;&#22768;SGD&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#31867;&#65292;&#29305;&#21035;&#26159;&#38750;&#20984;&#20989;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.02051</link><description>&lt;p&gt;
&#22312;&#37325;&#23614;&#25200;&#21160;&#19979;&#22122;&#22768;(S)GD&#30340;&#24046;&#20998;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Differential Privacy of Noisy (S)GD under Heavy-Tailed Perturbations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02051
&lt;/p&gt;
&lt;p&gt;
&#22312;&#37325;&#23614;&#25200;&#21160;&#19979;&#65292;&#22122;&#22768;SGD&#23454;&#29616;&#20102;&#24046;&#20998;&#38544;&#31169;&#20445;&#35777;&#65292;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#25439;&#22833;&#20989;&#25968;&#31867;&#65292;&#29305;&#21035;&#26159;&#38750;&#20984;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#37325;&#23614;&#22122;&#22768;&#27880;&#20837;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;(SGD)&#30340;&#36845;&#20195;&#20013;&#24050;&#32463;&#24341;&#36215;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#23545;&#23548;&#33268;&#30340;&#31639;&#27861;&#30340;&#21508;&#31181;&#29702;&#35770;&#24615;&#36136;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#20027;&#35201;&#26469;&#33258;&#23398;&#20064;&#29702;&#35770;&#21644;&#20248;&#21270;&#35270;&#35282;&#65292;&#20294;&#23427;&#20204;&#30340;&#38544;&#31169;&#20445;&#25252;&#24615;&#36136;&#23578;&#26410;&#24314;&#31435;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#32570;&#21475;&#65292;&#25105;&#20204;&#20026;&#22122;&#22768;SGD&#25552;&#20379;&#24046;&#20998;&#38544;&#31169;(DP)&#20445;&#35777;&#65292;&#24403;&#27880;&#20837;&#30340;&#22122;&#22768;&#36981;&#24490;$\alpha$-&#31283;&#23450;&#20998;&#24067;&#26102;&#65292;&#35813;&#20998;&#24067;&#21253;&#25324;&#19968;&#31995;&#21015;&#37325;&#23614;&#20998;&#24067;(&#20855;&#26377;&#26080;&#38480;&#26041;&#24046;)&#20197;&#21450;&#39640;&#26031;&#20998;&#24067;&#12290;&#32771;&#34385;$(\epsilon,\delta)$-DP&#26694;&#26550;&#65292;&#25105;&#20204;&#34920;&#26126;&#24102;&#26377;&#37325;&#23614;&#25200;&#21160;&#30340;SGD&#23454;&#29616;&#20102;$(0,\tilde{\mathcal{O}}(1/n))$-DP&#30340;&#24191;&#27867;&#25439;&#22833;&#20989;&#25968;&#31867;&#65292;&#36825;&#20123;&#20989;&#25968;&#21487;&#20197;&#26159;&#38750;&#20984;&#30340;&#65292;&#36825;&#37324;$n$&#26159;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#12290;&#20316;&#20026;&#19968;&#39033;&#26174;&#30528;&#30340;&#21103;&#20135;&#21697;&#65292;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#35813;&#24037;&#20316;&#35201;&#27714;&#26377;&#30028;se
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02051v1 Announce Type: cross  Abstract: Injecting heavy-tailed noise to the iterates of stochastic gradient descent (SGD) has received increasing attention over the past few years. While various theoretical properties of the resulting algorithm have been analyzed mainly from learning theory and optimization perspectives, their privacy preservation properties have not yet been established. Aiming to bridge this gap, we provide differential privacy (DP) guarantees for noisy SGD, when the injected noise follows an $\alpha$-stable distribution, which includes a spectrum of heavy-tailed distributions (with infinite variance) as well as the Gaussian distribution. Considering the $(\epsilon, \delta)$-DP framework, we show that SGD with heavy-tailed perturbations achieves $(0, \tilde{\mathcal{O}}(1/n))$-DP for a broad class of loss functions which can be non-convex, where $n$ is the number of data points. As a remarkable byproduct, contrary to prior work that necessitates bounded se
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28508;&#22312;&#37096;&#20998;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#35782;&#21035;&#28508;&#22312;&#32806;&#21512;&#21464;&#37327;&#26041;&#38754;&#30340;&#20248;&#31168;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.06223</link><description>&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37096;&#20998;&#22240;&#26524;&#27169;&#22411;&#25581;&#31034;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Revealing Multimodal Contrastive Representation Learning through Latent Partial Causal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06223
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28508;&#22312;&#37096;&#20998;&#22240;&#26524;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#35782;&#21035;&#28508;&#22312;&#32806;&#21512;&#21464;&#37327;&#26041;&#38754;&#30340;&#20248;&#31168;&#33021;&#21147;&#65292;&#24182;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#36890;&#36807;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#30001;&#20110;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#29616;&#35937;&#30340;&#26377;&#24847;&#20041;&#30340;&#20849;&#20139;&#34920;&#31034;&#12290;&#20026;&#20102;&#22686;&#24378;&#23545;&#36825;&#20123;&#33719;&#24471;&#30340;&#34920;&#31034;&#30340;&#28145;&#24230;&#20998;&#26512;&#21644;&#29702;&#35299;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#29305;&#21035;&#38024;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#35774;&#35745;&#30340;&#32479;&#19968;&#22240;&#26524;&#27169;&#22411;&#12290;&#36890;&#36807;&#30740;&#31350;&#36825;&#20010;&#27169;&#22411;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24335;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#22312;&#35782;&#21035;&#22312;&#25552;&#20986;&#30340;&#32479;&#19968;&#27169;&#22411;&#20013;&#30340;&#28508;&#22312;&#32806;&#21512;&#21464;&#37327;&#26041;&#38754;&#30340;&#20248;&#31168;&#33021;&#21147;&#65292;&#21363;&#20351;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#23548;&#33268;&#30340;&#32447;&#24615;&#25110;&#32622;&#25442;&#21464;&#25442;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;&#39044;&#35757;&#32451;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#36890;&#36807;&#32447;&#24615;&#29420;&#31435;&#20998;&#37327;&#20998;&#26512;&#36825;&#19968;&#20196;&#20154;&#24778;&#35766;&#30340;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#24037;&#20855;&#23398;&#20064;&#20998;&#31163;&#34920;&#31034;&#30340;&#28508;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#22312;&#34987;&#36829;&#21453;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#65292;&#20063;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#23398;&#20064;&#30142;&#30149;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal contrastive representation learning methods have proven successful across a range of domains, partly due to their ability to generate meaningful shared representations of complex phenomena. To enhance the depth of analysis and understanding of these acquired representations, we introduce a unified causal model specifically designed for multimodal data. By examining this model, we show that multimodal contrastive representation learning excels at identifying latent coupled variables within the proposed unified model, up to linear or permutation transformations resulting from different assumptions. Our findings illuminate the potential of pre-trained multimodal models, eg, CLIP, in learning disentangled representations through a surprisingly simple yet highly effective tool: linear independent component analysis. Experiments demonstrate the robustness of our findings, even when the assumptions are violated, and validate the effectiveness of the proposed method in learning dise
&lt;/p&gt;</description></item><item><title>&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26410;&#36798;&#21040;&#31561;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#22270;&#32593;&#32476;&#27169;&#22411;&#23545;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;</title><link>https://arxiv.org/abs/2312.16560</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#28040;&#24687;&#20256;&#36882;&#65306;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26410;&#36798;&#21040;&#30340;&#36890;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16560
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26694;&#26550;&#26159;&#19968;&#20010;&#36890;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26410;&#36798;&#21040;&#31561;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#28145;&#24230;&#22270;&#32593;&#32476;&#27169;&#22411;&#23545;&#38271;&#31243;&#20381;&#36182;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#22312;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#27491;&#30830;&#25551;&#36848;&#22797;&#26434;&#31995;&#32479;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#20013;&#21253;&#21547;&#23427;&#20204;&#30340;&#20195;&#20215;&#26159;&#25972;&#20307;&#35745;&#31639;&#25104;&#26412;&#30340;&#24613;&#21095;&#22686;&#21152;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#22270;&#32593;&#32476;&#34987;&#29992;&#20316;&#39640;&#25928;&#30340;&#12289;&#25968;&#25454;&#39537;&#21160;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#34920;&#31034;&#20026;&#22270;&#30340;&#22797;&#26434;&#31995;&#32479;&#30340;&#24615;&#36136;&#12290;&#36825;&#20123;&#27169;&#22411;&#20381;&#36182;&#20110;&#19968;&#31181;&#23616;&#37096;&#30340;&#36845;&#20195;&#28040;&#24687;&#20256;&#36882;&#31574;&#30053;&#65292;&#29702;&#35770;&#19978;&#24212;&#35813;&#33021;&#22815;&#25429;&#33719;&#38271;&#31243;&#20449;&#24687;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#22320;&#23545;&#30456;&#24212;&#30340;&#30456;&#20114;&#20316;&#29992;&#36827;&#34892;&#24314;&#27169;&#12290;&#28982;&#32780;&#22312;&#23454;&#36341;&#20013;&#65292;&#22823;&#22810;&#25968;&#28145;&#24230;&#22270;&#32593;&#32476;&#30001;&#20110;&#65288;&#21516;&#27493;&#65289;&#28040;&#24687;&#20256;&#36882;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#21363;&#36807;&#24230;&#24179;&#28369;&#12289;&#36807;&#24230;&#21387;&#32553;&#21644;&#26410;&#36798;&#21040;&#32780;&#19981;&#33021;&#30495;&#27491;&#23545;&#38271;&#31243;&#20381;&#36182;&#36827;&#34892;&#24314;&#27169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#32531;&#35299;&#36825;&#20123;&#38480;&#21046;&#30340;&#36890;&#29992;&#26694;&#26550;&#65306;&#22312;&#21464;&#20998;&#25512;&#29702;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#36171;&#20104;&#28040;&#24687;&#20256;&#36882;&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16560v2 Announce Type: replace  Abstract: Long-range interactions are essential for the correct description of complex systems in many scientific fields. The price to pay for including them in the calculations, however, is a dramatic increase in the overall computational costs. Recently, deep graph networks have been employed as efficient, data-driven surrogate models for predicting properties of complex systems represented as graphs. These models rely on a local and iterative message passing strategy that should, in principle, capture long-range information without explicitly modeling the corresponding interactions. In practice, most deep graph networks cannot really model long-range dependencies due to the intrinsic limitations of (synchronous) message passing, namely oversmoothing, oversquashing, and underreaching. This work proposes a general framework that learns to mitigate these limitations: within a variational inference framework, we endow message passing architectu
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#25511;&#21046;&#36719;&#24418;&#21464;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;DittoGym&#65292;&#35813;&#31995;&#32479;&#38656;&#35201;&#23545;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#36827;&#34892;&#32454;&#31890;&#24230;&#21464;&#21270;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2401.13231</link><description>&lt;p&gt;
DittoGym:&#23398;&#20064;&#25511;&#21046;&#36719;&#24418;&#21464;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DittoGym: Learning to Control Soft Shape-Shifting Robots. (arXiv:2401.13231v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13231
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#23398;&#20064;&#25511;&#21046;&#36719;&#24418;&#21464;&#26426;&#22120;&#20154;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#31995;&#32479;DittoGym&#65292;&#35813;&#31995;&#32479;&#38656;&#35201;&#23545;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#36827;&#34892;&#32454;&#31890;&#24230;&#21464;&#21270;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#20849;&#21516;&#35774;&#35745;&#65292;&#20854;&#20013;&#26426;&#22120;&#20154;&#30340;&#24418;&#24577;&#20248;&#21270;&#19982;&#23398;&#20064;&#30340;&#31574;&#30053;&#20849;&#21516;&#35299;&#20915;&#29305;&#23450;&#20219;&#21153;&#65292;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23545;&#20110;&#36719;&#26426;&#22120;&#20154;&#26469;&#35828;&#65292;&#36825;&#19968;&#39046;&#22495;&#20855;&#26377;&#29305;&#21035;&#30340;&#28508;&#21147;&#65292;&#22240;&#20026;&#36719;&#26426;&#22120;&#20154;&#21487;&#20197;&#36890;&#36807;&#26032;&#39062;&#30340;&#21046;&#36896;&#25216;&#26415;&#23454;&#29616;&#23398;&#20064;&#21040;&#30340;&#24418;&#24577;&#21644;&#25191;&#34892;&#22120;&#12290;&#21463;&#33258;&#28982;&#30028;&#21644;&#26368;&#36817;&#30340;&#26032;&#22411;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#26356;&#36827;&#19968;&#27493;&#25506;&#32034;&#26032;&#22411;&#21487;&#37325;&#26500;&#26426;&#22120;&#20154;&#65292;&#21363;&#22312;&#20854;&#23551;&#21629;&#20869;&#21487;&#20197;&#25913;&#21464;&#24418;&#24577;&#30340;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#23558;&#21487;&#37325;&#26500;&#36719;&#26426;&#22120;&#20154;&#30340;&#25511;&#21046;&#24418;&#24335;&#21270;&#20026;&#39640;&#32500;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#21516;&#19968;action&#31354;&#38388;&#20013;&#32479;&#19968;&#24418;&#24577;&#21464;&#21270;&#12289;&#36816;&#21160;&#21644;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#65292;&#24182;&#24341;&#20837;&#21512;&#36866;&#30340;&#31895;&#21040;&#32454;&#30340;&#35838;&#31243;&#34920;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#21457;&#29616;&#23454;&#29616;&#23545;&#26368;&#32456;&#26426;&#22120;&#20154;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;DittoGym&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#21487;&#37325;&#26500;&#36719;&#26426;&#22120;&#20154;&#30340;&#20840;&#38754;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#65292;&#38656;&#35201;&#23545;&#24418;&#24577;&#36827;&#34892;&#32454;&#31890;&#24230;&#21464;&#21270;&#26469;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore the novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning (RL) problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables us to discover policies that accomplish fine-grained control of the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23545;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#23618;&#26799;&#24230;&#33539;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26080;&#27861;&#27867;&#21270;&#21040;&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#65292;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.08909</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#20934;&#30830;&#24615;&#20272;&#35745;&#19979;&#20998;&#24067;&#20559;&#31227;&#30340;&#26799;&#24230;&#29305;&#24449;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Characterising Gradients for Unsupervised Accuracy Estimation under Distribution Shift. (arXiv:2401.08909v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#65292;&#21033;&#29992;&#26799;&#24230;&#20449;&#24687;&#23545;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20998;&#26512;&#20998;&#31867;&#23618;&#26799;&#24230;&#33539;&#25968;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#26080;&#27861;&#27867;&#21270;&#21040;&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#65292;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#21270;&#30340;&#27979;&#35797;&#29615;&#22659;&#19979;&#65292;&#26080;&#27861;&#35775;&#38382;&#30495;&#23454;&#27979;&#35797;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#27979;&#35797;&#20934;&#30830;&#24615;&#26159;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23433;&#20840;&#37096;&#32626;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#26497;&#20854;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#25110;&#25552;&#21462;&#29305;&#24449;&#30340;&#20449;&#24687;&#26469;&#24314;&#31435;&#19982;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#30456;&#20851;&#30340;&#20272;&#35745;&#20998;&#25968;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#21644;&#29702;&#35770;&#30740;&#31350;&#25506;&#35752;&#20102;&#26799;&#24230;&#20449;&#24687;&#22914;&#20309;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#23545;&#30495;&#23454;&#27979;&#35797;&#20934;&#30830;&#24615;&#36827;&#34892;&#39044;&#27979;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#32463;&#36807;&#19968;&#27425;&#26799;&#24230;&#27493;&#38271;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#20989;&#25968;&#21518;&#21453;&#21521;&#20256;&#25773;&#30340;&#20998;&#31867;&#23618;&#26799;&#24230;&#33539;&#25968;&#26469;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#65292;&#22312;&#27169;&#22411;&#22312;&#20998;&#24067;&#20559;&#31227;&#19979;&#26080;&#27861;&#27867;&#21270;&#21040;&#27979;&#35797;&#25968;&#25454;&#38598;&#26102;&#65292;&#24212;&#24403;&#35843;&#25972;&#27169;&#22411;&#20197;&#33719;&#24471;&#26356;&#22823;&#30340;&#26799;&#24230;&#33539;&#25968;&#12290;&#25105;&#20204;&#25552;&#20379;&#29702;&#35770;&#35265;&#35299;&#65292;&#31361;&#20986;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20027;&#35201;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating test accuracy without access to the ground-truth test labels under varying test environments is a challenging, yet extremely important problem in the safe deployment of machine learning algorithms. Existing works rely on the information from either the outputs or the extracted features of neural networks to formulate an estimation score correlating with the ground-truth test accuracy. In this paper, we investigate--both empirically and theoretically--how the information provided by the gradients can be predictive of the ground-truth test accuracy even under a distribution shift. Specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our key idea is that the model should be adjusted with a higher magnitude of gradients when it does not generalize to the test dataset with a distribution shift. We provide theoretical insights highlighting the main ingredients of such an approach en
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#28369;&#26680;&#31215;&#20998;&#21464;&#25442;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#32500;&#24230;&#32553;&#20943;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.02890</link><description>&lt;p&gt;
&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Nonlinear functional regression by functional deep neural network with kernel embedding. (arXiv:2401.02890v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24179;&#28369;&#26680;&#31215;&#20998;&#21464;&#25442;&#21644;&#25968;&#25454;&#30456;&#20851;&#30340;&#32500;&#24230;&#32553;&#20943;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#39044;&#27979;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#22312;&#35821;&#38899;&#35782;&#21035;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#39046;&#22495;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#23427;&#20063;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#20998;&#26512;&#20013;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26080;&#38480;&#32500;&#30340;&#36755;&#20837;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#24378;&#22823;&#30340;&#32500;&#24230;&#32553;&#20943;&#26041;&#27861;&#26469;&#22788;&#29702;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#22522;&#20110;&#24179;&#28369;&#26680;&#31215;&#20998;&#21464;&#25442;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#39640;&#25928;&#19988;&#23436;&#20840;&#25968;&#25454;&#20381;&#36182;&#30340;&#32500;&#24230;&#32553;&#20943;&#26041;&#27861;&#30340;&#20989;&#25968;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20989;&#25968;&#32593;&#32476;&#30001;&#20197;&#19979;&#27493;&#39588;&#32452;&#25104;&#65306;&#26680;&#23884;&#20837;&#27493;&#39588;&#65306;&#21033;&#29992;&#25968;&#25454;&#30456;&#20851;&#30340;&#24179;&#28369;&#26680;&#36827;&#34892;&#31215;&#20998;&#21464;&#25442;&#65307;&#25237;&#24433;&#27493;&#39588;&#65306;&#36890;&#36807;&#22522;&#20110;&#23884;&#20837;&#26680;&#30340;&#29305;&#24449;&#20989;&#25968;&#22522;&#24213;&#36827;&#34892;&#32500;&#24230;&#32553;&#20943;&#65307;&#26368;&#21518;&#26159;&#19968;&#20010;&#34920;&#36798;&#20016;&#23500;&#30340;&#28145;&#24230;ReLU&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of deep learning in various fields of science and technology, such as speech recognition, image classification, and natural language processing, recently it is also widely applied in the functional data analysis (FDA) with some empirical success. However, due to the infinite dimensional input, we need a powerful dimension reduction method for functional learning tasks, especially for the nonlinear functional regression. In this paper, based on the idea of smooth kernel integral transformation, we propose a functional deep neural network with an efficient and fully data-dependent dimension reduction method. The architecture of our functional net consists of a kernel embedding step: an integral transformation with a data-dependent smooth kernel; a projection step: a dimension reduction by projection with eigenfunction basis based on the embedding kernel; and finally an expressive deep ReLU neural network for the prediction. The utilization of smooth kernel embe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13786</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limits of Membership Inference Attacks on Machine Learning Models. (arXiv:2310.13786v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#21253;&#25324;&#25512;&#23548;&#20102;&#25928;&#26524;&#21644;&#25104;&#21151;&#29575;&#30340;&#32479;&#35745;&#37327;&#65292;&#24182;&#25552;&#20379;&#20102;&#20960;&#31181;&#24773;&#20917;&#19979;&#30340;&#30028;&#38480;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIA&#65289;&#21487;&#20197;&#25581;&#31034;&#29305;&#23450;&#25968;&#25454;&#28857;&#26159;&#21542;&#26159;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#65292;&#21487;&#33021;&#26292;&#38706;&#20010;&#20154;&#30340;&#25935;&#24863;&#20449;&#24687;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20851;&#20110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19978;MIA&#30340;&#22522;&#26412;&#32479;&#35745;&#38480;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25512;&#23548;&#20102;&#32479;&#35745;&#37327;&#65292;&#35813;&#32479;&#35745;&#37327;&#20915;&#23450;&#20102;&#36825;&#31181;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#21644;&#25104;&#21151;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20960;&#31181;&#24773;&#20917;&#65292;&#24182;&#23545;&#36825;&#20010;&#24863;&#20852;&#36259;&#30340;&#32479;&#35745;&#37327;&#25552;&#20379;&#20102;&#30028;&#38480;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#26681;&#25454;&#26679;&#26412;&#25968;&#37327;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;&#20854;&#20182;&#32467;&#26500;&#21442;&#25968;&#25512;&#26029;&#28508;&#22312;&#25915;&#20987;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#20197;&#30452;&#25509;&#20174;&#25968;&#25454;&#38598;&#20013;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article explores the fundamental statistical limitations associated with MIAs on machine learning models. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. Then, we investigate several situations for which we provide bounds on this quantity of interest. This allows us to infer the accuracy of potential attacks as a function of the number of samples and other structural parameters of learning models, which in some cases can be directly estimated from the dataset.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2310.13548</link><description>&lt;p&gt;
&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding Sycophancy in Language Models. (arXiv:2310.13548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13548
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#25216;&#26415;&#65292;&#21457;&#29616;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#26102;&#36807;&#20110;&#35844;&#23194;&#65292;&#32780;&#19981;&#26159;&#22374;&#35802;&#65292;&#36890;&#36807;&#20998;&#26512;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#24471;&#20986;&#20102;&#36825;&#19968;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12300;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#12301;&#26159;&#35757;&#32451;&#39640;&#36136;&#37327;AI&#21161;&#25163;&#30340;&#19968;&#31181;&#27969;&#34892;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;RLHF&#21487;&#33021;&#20250;&#40723;&#21169;&#27169;&#22411;&#36890;&#36807;&#19982;&#29992;&#25143;&#20449;&#24565;&#30456;&#31526;&#30340;&#22238;&#31572;&#26469;&#20195;&#26367;&#30495;&#23454;&#22238;&#31572;&#65292;&#36825;&#31181;&#34892;&#20026;&#34987;&#31216;&#20026;&#35844;&#23194;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;RLHF&#35757;&#32451;&#27169;&#22411;&#20013;&#35844;&#23194;&#34892;&#20026;&#30340;&#26222;&#36941;&#24615;&#20197;&#21450;&#20154;&#31867;&#20559;&#22909;&#21028;&#26029;&#26159;&#21542;&#36215;&#21040;&#20102;&#20316;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20116;&#20010;&#26368;&#20808;&#36827;&#30340;AI&#21161;&#25163;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#33258;&#30001;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#19968;&#36143;&#34920;&#29616;&#20986;&#35844;&#23194;&#34892;&#20026;&#12290;&#20026;&#20102;&#29702;&#35299;&#20154;&#31867;&#20559;&#22909;&#26159;&#21542;&#39537;&#21160;&#20102;RLHF&#27169;&#22411;&#30340;&#36825;&#31181;&#24191;&#27867;&#34892;&#20026;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#29616;&#26377;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#24403;&#22238;&#31572;&#19982;&#29992;&#25143;&#30340;&#35266;&#28857;&#30456;&#31526;&#26102;&#65292;&#23427;&#26356;&#26377;&#21487;&#33021;&#34987;&#36873;&#20013;&#12290;&#27492;&#22806;&#65292;&#20154;&#31867;&#21644;&#20559;&#22909;&#27169;&#22411;&#65288;PMs&#65289;&#23558;&#26377;&#35828;&#26381;&#21147;&#30340;&#35844;&#23194;&#22238;&#31572;&#19982;&#27491;&#30830;&#22238;&#31572;&#30456;&#27604;&#65292;&#26377;&#26102;&#20960;&#20046;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#22320;&#36873;&#25321;&#20102;&#35844;&#23194;&#22238;&#31572;&#12290;&#20248;&#21270;&#27169;&#22411;&#36755;&#20986;&#20197;&#28385;&#36275;PMs&#26377;&#26102;&#20063;&#20250;&#22312;&#30495;&#23454;&#24615;&#21644;&#35844;&#23194;&#34892;&#20026;&#20043;&#38388;&#20570;&#20986;&#21462;&#33293;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) is a popular technique for training high-quality AI assistants. However, RLHF may also encourage model responses that match user beliefs over truthful responses, a behavior known as sycophancy. We investigate the prevalence of sycophancy in RLHF-trained models and whether human preference judgements are responsible. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophantic behavior across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior of RLHF models, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Over
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.12563</link><description>&lt;p&gt;
&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#23545;&#29983;&#29702;&#20449;&#21495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36890;&#24120;&#22312;&#23398;&#26415;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#21463;&#21040;&#25511;&#21046;&#23454;&#39564;&#26465;&#20214;&#19979;&#30340;&#28165;&#27905;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22122;&#22768;&#30340;&#25361;&#25112;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#22312;&#24863;&#30693;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#20013;&#28145;&#20837;&#30740;&#31350;&#20102;&#26631;&#31614;&#32423;&#22122;&#22768;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#23454;&#29992;&#30340;&#31471;&#21040;&#31471;&#26080;&#30417;&#30563;TSAD&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#24322;&#24120;&#30340;&#24773;&#20917;&#19979;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;TSAD-C&#65292;&#20854;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#38656;&#35201;&#35775;&#38382;&#24322;&#24120;&#26631;&#31614;&#12290;TSAD-C&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21435;&#27745;&#22120;&#29992;&#20110;&#32416;&#27491;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#24322;&#24120;&#65288;&#20063;&#31216;&#20026;&#22122;&#22768;&#65289;&#65292;&#19968;&#20010;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#27169;&#22359;&#29992;&#20110;&#25429;&#25417;&#21435;&#27745;&#21518;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20869;&#37096;&#21644;&#36328;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#35270;&#20026;&#26367;&#20195;&#24615;&#30340;&#24322;&#24120;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2306.15907</link><description>&lt;p&gt;
&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#27700;&#20301;&#39044;&#27979;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Deep Learning Models for Water Stage Predictions in South Florida. (arXiv:2306.15907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#19979;&#28216;&#30340;&#27700;&#20301;&#65292;&#24182;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21644;&#39044;&#27979;&#27827;&#27969;&#31995;&#32479;&#30340;&#27700;&#20301;&#23545;&#20110;&#27946;&#27700;&#35686;&#25253;&#12289;&#27700;&#21147;&#25805;&#20316;&#21644;&#27946;&#27700;&#20943;&#36731;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#24037;&#31243;&#39046;&#22495;&#20013;&#65292;&#20351;&#29992;HEC-RAS&#12289;MIKE&#21644;SWMM&#31561;&#24037;&#20855;&#24314;&#31435;&#35814;&#32454;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27700;&#25991;&#21644;&#27700;&#21147;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#25972;&#20010;&#27969;&#22495;&#65292;&#20174;&#32780;&#39044;&#27979;&#31995;&#32479;&#20013;&#20219;&#24847;&#28857;&#30340;&#27700;&#20301;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#35745;&#31639;&#37327;&#22823;&#65292;&#23588;&#20854;&#23545;&#20110;&#22823;&#27969;&#22495;&#21644;&#38271;&#26102;&#38388;&#27169;&#25311;&#26469;&#35828;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20960;&#20010;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#27169;&#22411;&#20316;&#20026;&#20195;&#29702;&#27169;&#22411;&#65292;&#24555;&#36895;&#39044;&#27979;&#27700;&#20301;&#12290;&#26412;&#25991;&#20197;&#21335;&#20315;&#32599;&#37324;&#36798;&#24030;&#36808;&#38463;&#23494;&#27827;&#30340;&#19979;&#28216;&#27700;&#20301;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25968;&#25454;&#38598;&#26469;&#33258;&#21335;&#20315;&#32599;&#37324;&#36798;&#27700;&#31649;&#29702;&#21306;&#65288;SFWMD&#65289;&#30340;DBHYDRO&#25968;&#25454;&#24211;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2010&#24180;1&#26376;1&#26085;&#33267;2020&#24180;12&#26376;31&#26085;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;DL&#27169;&#22411;&#30340;&#24615;&#33021;&#19982;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating and predicting water levels in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. In the engineering field, tools such as HEC-RAS, MIKE, and SWMM are used to build detailed physics-based hydrological and hydraulic computational models to simulate the entire watershed, thereby predicting the water stage at any point in the system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. The downstream stage of the Miami River in South Florida is chosen as a case study for this paper. The dataset is from January 1, 2010, to December 31, 2020, downloaded from the DBHYDRO database of the South Florida Water Management District (SFWMD). Extensive experiments show that the performance of the DL models is comparable to that of the physics-bas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#20197;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2303.13326</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#21435;&#20013;&#24515;&#21270;&#23545;&#25239;&#24615;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Decentralized Adversarial Training over Graphs. (arXiv:2303.13326v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13326
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#30340;&#21435;&#20013;&#24515;&#21270;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#21033;&#29992;&#25193;&#25955;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#65292;&#22686;&#24378;&#20102;&#22810;&#20010;&#20195;&#29702;&#30340;&#40065;&#26834;&#24615;&#20197;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23545;&#25239;&#25915;&#20987;&#30340;&#28431;&#27934;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#29420;&#31435;&#21333;&#19968;&#20195;&#29702;&#23398;&#20064;&#32773;&#30340;&#34892;&#20026;&#19978;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22270;&#19978;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#65292;&#20854;&#20013;&#21508;&#20010;&#21333;&#29420;&#30340;&#20195;&#29702;&#20250;&#21463;&#21040;&#31354;&#38388;&#20013;&#19981;&#21516;&#24378;&#24230;&#30340;&#25200;&#21160;&#12290;&#39044;&#26399;&#36890;&#36807;&#38142;&#25509;&#20195;&#29702;&#21644;&#21487;&#33021;&#22312;&#22270;&#19978;&#23454;&#29616;&#30340;&#25915;&#20987;&#27169;&#22411;&#30340;&#24322;&#36136;&#24615;&#65292;&#21327;&#35843;&#25972;&#20010;&#22242;&#38431;&#30340;&#24378;&#22823;&#21327;&#21516;&#20316;&#29992;&#21487;&#20197;&#24110;&#21161;&#22686;&#24378;&#40065;&#26834;&#24615;&#12290;&#26412;&#25991;&#20351;&#29992;&#25193;&#25955;&#23398;&#20064;&#30340;&#26497;&#23567;-&#26497;&#22823;&#20844;&#24335;&#65292;&#20026;&#22810;&#20195;&#29702;&#31995;&#32479;&#24320;&#21457;&#20102;&#19968;&#31181;&#21435;&#20013;&#24515;&#21270;&#30340;&#23545;&#25239;&#24615;&#35757;&#32451;&#26694;&#26550;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#35813;&#26041;&#26696;&#22312;&#20984;&#21644;&#38750;&#20984;&#29615;&#22659;&#19979;&#30340;&#25910;&#25947;&#29305;&#24615;&#65292;&#24182;&#35828;&#26126;&#20102;&#22686;&#24378;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
The vulnerability of machine learning models to adversarial attacks has been attracting considerable attention in recent years. Most existing studies focus on the behavior of stand-alone single-agent learners. In comparison, this work studies adversarial training over graphs, where individual agents are subjected to perturbations of varied strength levels across space. It is expected that interactions by linked agents, and the heterogeneity of the attack models that are possible over the graph, can help enhance robustness in view of the coordination power of the group. Using a min-max formulation of diffusion learning, we develop a decentralized adversarial training framework for multi-agent systems. We analyze the convergence properties of the proposed scheme for both convex and non-convex environments, and illustrate the enhanced robustness to adversarial attacks.
&lt;/p&gt;</description></item><item><title>FedEBA+&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;FedEBA+&#20248;&#20110;&#20854;&#20182;&#20844;&#24179;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12407</link><description>&lt;p&gt;
FedEBA+&#65306;&#22522;&#20110;&#29109;&#30340;&#27169;&#22411;&#23454;&#29616;&#20844;&#24179;&#21644;&#26377;&#25928;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FedEBA+: Towards Fair and Effective Federated Learning via Entropy-Based Model. (arXiv:2301.12407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12407
&lt;/p&gt;
&lt;p&gt;
FedEBA+&#26159;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#65292;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;FedEBA+&#20248;&#20110;&#20854;&#20182;&#20844;&#24179;&#24615;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20844;&#24179;&#24615;&#26159;&#32852;&#37030;&#23398;&#20064;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#23427;&#20351;&#27169;&#22411;&#22312;&#25152;&#26377;&#23458;&#25143;&#31471;&#19978;&#20445;&#25345;&#19968;&#33268;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#19968;&#31181;&#21487;&#20197;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#21644;&#20419;&#36827;&#20844;&#24179;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;&#20173;&#28982;&#26159;&#19968;&#20010;&#33392;&#24040;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#23454;&#29616;&#21518;&#32773;&#36890;&#24120;&#38656;&#35201;&#19982;&#21069;&#32773;&#30340;&#26435;&#34913;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32852;&#37030;&#23398;&#20064;&#31639;&#27861;FedEBA+&#65292;&#23427;&#22312;&#21516;&#26102;&#25552;&#39640;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#30340;&#21516;&#26102;&#25552;&#39640;&#20844;&#24179;&#24615;&#65292;&#35813;&#31639;&#27861;&#37319;&#29992;&#20844;&#24179;&#32858;&#21512;&#26041;&#26696;&#21644;&#23545;&#40784;&#26356;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#25910;&#25947;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;FedEBA+&#30340;&#20844;&#24179;&#24615;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;FedEBA+&#22312;&#20844;&#24179;&#24615;&#21644;&#20840;&#23616;&#27169;&#22411;&#24615;&#33021;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;SOTA&#30340;&#20844;&#24179;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring fairness is a crucial aspect of Federated Learning (FL), which enables the model to perform consistently across all clients. However, designing an FL algorithm that simultaneously improves global model performance and promotes fairness remains a formidable challenge, as achieving the latter often necessitates a trade-off with the former.To address this challenge, we propose a new FL algorithm, FedEBA+, which enhances fairness while simultaneously improving global model performance. FedEBA+ incorporates a fair aggregation scheme that assigns higher weights to underperforming clients and an alignment update method. In addition, we provide theoretical convergence analysis and show the fairness of FedEBA+. Extensive experiments demonstrate that FedEBA+ outperforms other SOTA fairness FL methods in terms of both fairness and global model performance.
&lt;/p&gt;</description></item></channel></rss>