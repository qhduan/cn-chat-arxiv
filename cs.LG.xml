<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#23545;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#27169;&#22411;&#36866;&#24212;&#26032;&#20998;&#24067;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.18888</link><description>&lt;p&gt;
&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#31163;&#25955;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Based Extensible Codebook for Discrete Federated Learning in Heterogeneous Data Silos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18888
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#24212;&#23545;&#24322;&#26500;&#25968;&#25454;&#23396;&#23707;&#20013;&#27169;&#22411;&#36866;&#24212;&#26032;&#20998;&#24067;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26088;&#22312;&#21033;&#29992;&#24191;&#27867;&#20998;&#24067;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#32852;&#37030;&#23398;&#20064;(FL)&#38754;&#20020;&#30528;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#19981;&#21516;&#23396;&#23707;&#38388;&#25968;&#25454;&#30340;&#24322;&#26500;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#20174;FL&#23548;&#20986;&#30340;&#27169;&#22411;&#22312;&#24212;&#29992;&#20110;&#20855;&#26377;&#38476;&#29983;&#20998;&#24067;&#30340;&#25968;&#25454;&#23396;&#23707;&#26102;&#20250;&#34920;&#29616;&#20986;&#26126;&#26174;&#22686;&#21152;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#32780;&#31616;&#21333;&#30340;&#36845;&#20195;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#21487;&#25299;&#23637;&#32534;&#30721;&#26412;&#32852;&#37030;&#23398;&#20064;(UEFL)&#12290;&#35813;&#26694;&#26550;&#21160;&#24577;&#22320;&#23558;&#28508;&#22312;&#29305;&#24449;&#26144;&#23556;&#21040;&#21487;&#35757;&#32451;&#30340;&#31163;&#25955;&#21521;&#37327;&#65292;&#35780;&#20272;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#38024;&#23545;&#34920;&#29616;&#20986;&#39640;&#19981;&#30830;&#23450;&#24615;&#30340;&#23396;&#23707;&#29305;&#21035;&#22320;&#25193;&#23637;&#31163;&#25955;&#21270;&#35789;&#20856;&#25110;&#32534;&#30721;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18888v1 Announce Type: new  Abstract: Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance a
&lt;/p&gt;</description></item><item><title>&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09448</link><description>&lt;p&gt;
&#26222;&#36890;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Conventional and Tripolar EEG for High-Performance Reach-to-Grasp BCI Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09448
&lt;/p&gt;
&lt;p&gt;
&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#39640;&#24615;&#33021;&#21040;&#39076;&#25235;&#25569;BCI&#31995;&#32479;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#12289;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#27604;&#36739;&#20256;&#32479;EEG&#19982;&#19977;&#26497;EEG&#22312;&#25552;&#21319;&#36816;&#21160;&#38556;&#30861;&#20010;&#20307;&#30340;BCI&#24212;&#29992;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#26159;&#35299;&#35835;&#21644;&#35299;&#30721;&#21508;&#31181;&#25235;&#25569;&#21160;&#20316;&#65292;&#22914;&#21147;&#25569;&#21644;&#31934;&#30830;&#25569;&#25345;&#12290;&#30446;&#26631;&#26159;&#30830;&#23450;&#21738;&#31181;EEG&#25216;&#26415;&#22312;&#22788;&#29702;&#21644;&#32763;&#35793;&#19982;&#25235;&#25569;&#30456;&#20851;&#30340;&#33041;&#30005;&#20449;&#21495;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#12290;&#30740;&#31350;&#28041;&#21450;&#23545;&#21313;&#21517;&#20581;&#24247;&#21442;&#19982;&#32773;&#36827;&#34892;&#23454;&#39564;&#65292;&#21442;&#19982;&#32773;&#36827;&#34892;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25569;&#25345;&#36816;&#21160;&#65306;&#21147;&#25569;&#21644;&#31934;&#30830;&#25569;&#25345;&#65292;&#26080;&#36816;&#21160;&#26465;&#20214;&#20316;&#20026;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22312;&#35299;&#30721;&#25235;&#25569;&#21160;&#20316;&#26041;&#38754;&#23545;EEG&#21644;&#19977;&#26497;EEG&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;&#35813;&#27604;&#36739;&#28085;&#30422;&#20102;&#20960;&#20010;&#20851;&#38190;&#21442;&#25968;&#65292;&#21253;&#25324;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#12289;&#36890;&#36807;&#21151;&#33021;&#36830;&#25509;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#12289;ERPs&#21644;&#23567;&#27874;&#26102;&#39057;&#20998;&#26512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36824;&#28041;&#21450;&#20174;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09448v1 Announce Type: cross  Abstract: This study aims to enhance BCI applications for individuals with motor impairments by comparing the effectiveness of tripolar EEG (tEEG) with conventional EEG. The focus is on interpreting and decoding various grasping movements, such as power grasp and precision grasp. The goal is to determine which EEG technology is more effective in processing and translating grasp related neural signals. The approach involved experimenting on ten healthy participants who performed two distinct grasp movements: power grasp and precision grasp, with a no movement condition serving as the baseline. Our research presents a thorough comparison between EEG and tEEG in decoding grasping movements. This comparison spans several key parameters, including signal to noise ratio (SNR), spatial resolution via functional connectivity, ERPs, and wavelet time frequency analysis. Additionally, our study involved extracting and analyzing statistical features from th
&lt;/p&gt;</description></item><item><title>ViewFusion &#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471;&#29983;&#25104;&#26041;&#27861;&#65292;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#21516;&#26102;&#24212;&#29992;&#25193;&#25955;&#21435;&#22122;&#21644;&#20687;&#32032;&#21152;&#26435;&#25513;&#27169;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02906</link><description>&lt;p&gt;
ViewFusion: &#23398;&#20064;&#21487;&#32452;&#21512;&#30340;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
ViewFusion: Learning Composable Diffusion Models for Novel View Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02906
&lt;/p&gt;
&lt;p&gt;
ViewFusion &#26159;&#19968;&#31181;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471;&#29983;&#25104;&#26041;&#27861;&#65292;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#28789;&#27963;&#24615;&#65292;&#36890;&#36807;&#21516;&#26102;&#24212;&#29992;&#25193;&#25955;&#21435;&#22122;&#21644;&#20687;&#32032;&#21152;&#26435;&#25513;&#27169;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20026;&#26032;&#35270;&#35282;&#21512;&#25104;&#36825;&#20010;&#32769;&#38382;&#39064;&#25552;&#20379;&#20102;&#20016;&#23500;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#22522;&#20110;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#30340;&#26041;&#27861;&#21040;&#31471;&#21040;&#31471;&#30340;&#39118;&#26684;&#26550;&#26500;&#12290;&#27599;&#31181;&#26041;&#27861;&#37117;&#20855;&#26377;&#29305;&#23450;&#30340;&#20248;&#21183;&#65292;&#20294;&#20063;&#20855;&#26377;&#29305;&#23450;&#30340;&#36866;&#29992;&#24615;&#38480;&#21046;&#12290;&#36825;&#39033;&#24037;&#20316;&#24341;&#20837;&#20102;ViewFusion&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;&#26080;&#19982;&#20262;&#27604;&#30340;&#28789;&#27963;&#24615;&#30340;&#26368;&#26032;&#31471;&#21040;&#31471;&#29983;&#25104;&#26041;&#27861;&#65292;&#29992;&#20110;&#26032;&#35270;&#35282;&#21512;&#25104;&#12290;ViewFusion&#21516;&#26102;&#23545;&#22330;&#26223;&#30340;&#20219;&#24847;&#25968;&#37327;&#30340;&#36755;&#20837;&#35270;&#35282;&#24212;&#29992;&#25193;&#25955;&#21435;&#22122;&#27493;&#39588;&#65292;&#28982;&#21518;&#23558;&#27599;&#20010;&#35270;&#35282;&#24471;&#21040;&#30340;&#22122;&#22768;&#26799;&#24230;&#19982;&#65288;&#25512;&#26029;&#24471;&#21040;&#30340;&#65289;&#20687;&#32032;&#21152;&#26435;&#25513;&#27169;&#30456;&#32467;&#21512;&#65292;&#30830;&#20445;&#23545;&#20110;&#30446;&#26631;&#22330;&#26223;&#30340;&#27599;&#20010;&#21306;&#22495;&#65292;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36755;&#20837;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#20197;&#19979;&#26041;&#24335;&#35299;&#20915;&#20102;&#20808;&#21069;&#26041;&#27861;&#30340;&#20960;&#20010;&#23616;&#38480;&#24615;&#65306;&#65288;1&#65289;&#21487;&#35757;&#32451;&#19988;&#33021;&#22815;&#27867;&#21270;&#21040;&#22810;&#20010;&#22330;&#26223;&#21644;&#29289;&#20307;&#31867;&#21035;&#65292;&#65288;2&#65289;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26102;&#33258;&#36866;&#24212;&#22320;&#37319;&#29992;&#21487;&#21464;&#25968;&#37327;&#30340;&#26080;&#23039;&#24577;&#35270;&#22270;&#65292;&#65288;3&#65289;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning is providing a wealth of new approaches to the old problem of novel view synthesis, from Neural Radiance Field (NeRF) based approaches to end-to-end style architectures. Each approach offers specific strengths but also comes with specific limitations in their applicability. This work introduces ViewFusion, a state-of-the-art end-to-end generative approach to novel view synthesis with unparalleled flexibility. ViewFusion consists in simultaneously applying a diffusion denoising step to any number of input views of a scene, then combining the noise gradients obtained for each view with an (inferred) pixel-weighting mask, ensuring that for each region of the target scene only the most informative input views are taken into account. Our approach resolves several limitations of previous approaches by (1) being trainable and generalizing across multiple scenes and object classes, (2) adaptively taking in a variable number of pose-free views at both train and test time, (3) gene
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;State-Dependent Causal Inference&#65288;SDCI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19968;&#31867;&#23485;&#27867;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#25104;&#21151;&#22320;&#22238;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2110.06257</link><description>&lt;p&gt;
&#20174;&#26377;&#26465;&#20214;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#20013;&#36827;&#34892;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Conditionally Stationary Time Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.06257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;State-Dependent Causal Inference&#65288;SDCI&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19968;&#31867;&#23485;&#27867;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#25104;&#21151;&#22320;&#22238;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#65292;&#21363;&#20174;&#35266;&#27979;&#25968;&#25454;&#25512;&#26029;&#28508;&#22312;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#24050;&#34987;&#35777;&#26126;&#23545;AI&#31995;&#32479;&#20855;&#26377;&#26497;&#22823;&#25361;&#25112;&#12290;&#22312;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#32972;&#26223;&#19979;&#65292;&#20256;&#32479;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#20027;&#35201;&#32771;&#34385;&#20855;&#26377;&#23436;&#20840;&#35266;&#27979;&#21464;&#37327;&#21644;/&#25110;&#26469;&#33258;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#30340;&#25968;&#25454;&#30340;&#21463;&#38480;&#22330;&#26223;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#26469;&#22788;&#29702;&#19968;&#31867;&#23485;&#27867;&#30340;&#38750;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#21363;&#22312;&#26465;&#20214;&#19978;&#26159;&#24179;&#31283;&#30340;&#26465;&#20214;&#24179;&#31283;&#26102;&#38388;&#24207;&#21015;&#65292;&#20854;&#20013;&#38750;&#24179;&#31283;&#34892;&#20026;&#34987;&#24314;&#27169;&#20026;&#22312;&#19968;&#32452;&#65288;&#21487;&#33021;&#26159;&#38544;&#34255;&#30340;&#65289;&#29366;&#24577;&#21464;&#37327;&#19978;&#30340;&#24179;&#31283;&#24615;&#12290;&#21629;&#21517;&#20026;State-Dependent Causal Inference&#65288;SDCI&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#21487;&#35777;&#22320;&#22238;&#22797;&#20986;&#28508;&#22312;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#65292;&#35777;&#26126;&#22312;&#23436;&#20840;&#35266;&#23519;&#21040;&#30340;&#29366;&#24577;&#19979;&#65292;&#24182;&#22312;&#23384;&#22312;&#38544;&#34255;&#29366;&#24577;&#26102;&#32463;&#39564;&#24615;&#22320;&#23454;&#29616;&#12290;&#21518;&#32773;&#36890;&#36807;&#23545;&#21512;&#25104;&#32447;&#24615;&#31995;&#32479;&#21644;&#38750;&#32447;&#24615;&#31890;&#23376;&#30456;&#20114;&#20316;&#29992;&#25968;&#25454;&#30340;&#23454;&#39564;&#36827;&#34892;&#39564;&#35777;&#65292;SDCI&#23454;&#29616;&#20102;&#20248;&#20110;&#22522;&#32447;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.06257v2 Announce Type: replace  Abstract: Causal discovery, i.e., inferring underlying causal relationships from observational data, has been shown to be highly challenging for AI systems. In time series modeling context, traditional causal discovery methods mainly consider constrained scenarios with fully observed variables and/or data from stationary time-series. We develop a causal discovery approach to handle a wide class of non-stationary time-series that are conditionally stationary, where the non-stationary behaviour is modeled as stationarity conditioned on a set of (possibly hidden) state variables. Named State-Dependent Causal Inference (SDCI), our approach is able to recover the underlying causal dependencies, provably with fully-observed states and empirically with hidden states. The latter is confirmed by experiments on synthetic linear system and nonlinear particle interaction data, where SDCI achieves superior performance over baseline causal discovery methods
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2001.01095</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;: &#36890;&#36807;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Independence Testing via Maximum and Average Distance Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2001.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#22810;&#20803;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#34920;&#24449;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#36793;&#38469;&#30456;&#20851;&#32500;&#24230;&#25968;&#37327;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#35780;&#20272;&#20102;&#27599;&#20010;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#20248;&#21183;&#65292;&#26816;&#26597;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#38646;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#26816;&#27979;&#31243;&#24207;&#12290;&#24471;&#20986;&#30340;&#26816;&#39564;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#20316;&#20026;&#24213;&#23618;&#24230;&#37327;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#25152;&#25552;&#20986;&#30340;&#27979;&#35797;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#20803;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#26368;&#22823;&#36317;&#31163;&#30456;&#20851;&#24615;&#12289;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#21644;&#21407;&#22987;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#23454;&#35777;&#34920;&#29616;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#20197;&#26816;&#27979;&#20154;&#31867;&#34880;&#27974;&#20013;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#21644;&#32957;&#27700;&#24179;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces and investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, assess the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.
&lt;/p&gt;</description></item><item><title>Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.15098</link><description>&lt;p&gt;
Hi-Core: &#38754;&#21521;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#30340;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Hi-Core: Hierarchical Knowledge Transfer for Continual Reinforcement Learning. (arXiv:2401.15098v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15098
&lt;/p&gt;
&lt;p&gt;
Hi-Core&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#23618;&#27425;&#21270;&#30340;&#30693;&#35782;&#36801;&#31227;&#26469;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;Hi-Core&#23637;&#29616;&#20102;&#36739;&#24378;&#30340;&#30693;&#35782;&#36801;&#31227;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#65288;Continual Reinforcement Learning, CRL&#65289;&#36171;&#20104;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#20445;&#30041;&#20808;&#21069;&#30340;&#30693;&#35782;&#24182;&#21033;&#29992;&#23427;&#26469;&#20419;&#36827;&#26410;&#26469;&#30340;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#19987;&#27880;&#20110;&#22312;&#31867;&#20284;&#20219;&#21153;&#20043;&#38388;&#20256;&#36755;&#20302;&#23618;&#27425;&#30340;&#30693;&#35782;&#65292;&#24573;&#35270;&#20102;&#20154;&#31867;&#35748;&#30693;&#25511;&#21046;&#30340;&#23618;&#27425;&#32467;&#26500;&#65292;&#23548;&#33268;&#22312;&#21508;&#31181;&#20219;&#21153;&#20043;&#38388;&#30340;&#30693;&#35782;&#36801;&#31227;&#19981;&#36275;&#12290;&#20026;&#20102;&#22686;&#24378;&#39640;&#23618;&#27425;&#30340;&#30693;&#35782;&#36801;&#31227;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning)&#30340;&#26032;&#26694;&#26550;&#65292;&#23427;&#30001;&#20004;&#23618;&#32467;&#26500;&#32452;&#25104;&#65306;1) &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Large Language Model, LLM&#65289;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#35774;&#23450;&#30446;&#26631;&#30340;&#39640;&#23618;&#31574;&#30053;&#21046;&#23450;&#21644;2) &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#25353;&#29031;&#39640;&#23618;&#30446;&#26631;&#23548;&#21521;&#30340;&#20302;&#23618;&#31574;&#30053;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#30693;&#35782;&#24211;&#65288;&#31574;&#30053;&#24211;&#65289;&#26469;&#23384;&#20648;&#21487;&#20197;&#29992;&#20110;&#23618;&#27425;&#21270;&#30693;&#35782;&#36801;&#31227;&#30340;&#31574;&#30053;&#12290;&#22312;MiniGr&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual reinforcement learning (CRL) empowers RL agents with the ability to learn from a sequence of tasks, preserving previous knowledge and leveraging it to facilitate future learning. However, existing methods often focus on transferring low-level knowledge across similar tasks, which neglects the hierarchical structure of human cognitive control, resulting in insufficient knowledge transfer across diverse tasks. To enhance high-level knowledge transfer, we propose a novel framework named Hi-Core (Hierarchical knowledge transfer for Continual reinforcement learning), which is structured in two layers: 1) the high-level policy formulation which utilizes the powerful reasoning ability of the Large Language Model (LLM) to set goals and 2) the low-level policy learning through RL which is oriented by high-level goals. Moreover, the knowledge base (policy library) is constructed to store policies that can be retrieved for hierarchical knowledge transfer. Experiments conducted in MiniGr
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#25299;&#25169;&#20449;&#24687;&#20272;&#35745;&#23431;&#23449;&#21442;&#25968;&#12290;&#36890;&#36807;&#21442;&#25968;&#24674;&#22797;&#27979;&#35797;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26356;&#20934;&#30830;&#21644;&#31934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2308.02636</link><description>&lt;p&gt;
&#20174;&#25299;&#25169;&#23398;&#20013;&#23398;&#20064;&#65306;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#23431;&#23449;&#21442;&#25968;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Learning from Topology: Cosmological Parameter Estimation from the Large-scale Structure. (arXiv:2308.02636v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.02636
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25345;&#32493;&#21516;&#35843;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#25299;&#25169;&#20449;&#24687;&#20272;&#35745;&#23431;&#23449;&#21442;&#25968;&#12290;&#36890;&#36807;&#21442;&#25968;&#24674;&#22797;&#27979;&#35797;&#65292;&#21457;&#29616;&#35813;&#26041;&#27861;&#27604;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#26356;&#20934;&#30830;&#21644;&#31934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23431;&#23449;&#22823;&#23610;&#24230;&#32467;&#26500;&#30340;&#25299;&#25169;&#21253;&#21547;&#30528;&#26377;&#20851;&#22522;&#30784;&#23431;&#23449;&#21442;&#25968;&#30340;&#23453;&#36149;&#20449;&#24687;&#12290;&#34429;&#28982;&#25345;&#32493;&#21516;&#35843;&#21487;&#20197;&#25552;&#21462;&#36825;&#31181;&#25299;&#25169;&#20449;&#24687;&#65292;&#20294;&#22914;&#20309;&#26368;&#20339;&#22320;&#20174;&#36825;&#20010;&#24037;&#20855;&#20013;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#23558;&#25345;&#32493;&#22270;&#20687;&#26144;&#23556;&#21040;&#23431;&#23449;&#21442;&#25968;&#12290;&#36890;&#36807;&#21442;&#25968;&#24674;&#22797;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#32780;&#31934;&#30830;&#22320;&#20272;&#35745;&#65292;&#26126;&#26174;&#20248;&#20110;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The topology of the large-scale structure of the universe contains valuable information on the underlying cosmological parameters. While persistent homology can extract this topological information, the optimal method for parameter estimation from the tool remains an open question. To address this, we propose a neural network model to map persistence images to cosmological parameters. Through a parameter recovery test, we demonstrate that our model makes accurate and precise estimates, considerably outperforming conventional Bayesian inference approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;KL&#35757;&#32451;&#27861;&#21644;Fisher&#35757;&#32451;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#23454;&#29616;&#20102;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#22823;&#25209;&#37327;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2306.04952</link><description>&lt;p&gt;
&#22522;&#20110;&#29109;&#30340;&#35757;&#32451;&#26041;&#27861;&#29992;&#20110;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;
&lt;/p&gt;
&lt;p&gt;
Entropy-based Training Methods for Scalable Neural Implicit Sampler. (arXiv:2306.04952v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04952
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#24182;&#24341;&#20837;&#20102;KL&#35757;&#32451;&#27861;&#21644;Fisher&#35757;&#32451;&#27861;&#26469;&#35757;&#32451;&#23427;&#65292;&#23454;&#29616;&#20102;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#22823;&#25209;&#37327;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#20174;&#38750;&#26631;&#20934;&#30446;&#26631;&#20998;&#24067;&#20013;&#37319;&#26679;&#26159;&#31185;&#23398;&#35745;&#31639;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#22914;&#39532;&#23572;&#31185;&#22827;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#21487;&#20445;&#35777;&#20174;&#36825;&#20123;&#20998;&#24067;&#20013;&#28176;&#36827;&#26080;&#20559;&#37319;&#26679;&#65292;&#20294;&#22312;&#22788;&#29702;&#39640;&#32500;&#30446;&#26631;&#26102;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#65292;&#38656;&#35201;&#22810;&#27425;&#36845;&#20195;&#29983;&#25104;&#19968;&#25209;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#25193;&#23637;&#30340;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#36890;&#36807;&#21033;&#29992;&#30452;&#25509;&#23558;&#26131;&#20110;&#37319;&#26679;&#30340;&#28508;&#22312;&#21521;&#37327;&#26144;&#23556;&#21040;&#30446;&#26631;&#26679;&#26412;&#30340;&#31070;&#32463;&#21464;&#25442;&#65292;&#21487;&#20197;&#22312;&#20302;&#35745;&#31639;&#25104;&#26412;&#19979;&#29983;&#25104;&#22823;&#25209;&#37327;&#26679;&#26412;&#12290;&#20026;&#20102;&#35757;&#32451;&#31070;&#32463;&#38544;&#24335;&#37319;&#26679;&#22120;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#26041;&#27861;&#65306;KL&#35757;&#32451;&#27861;&#21644;Fisher&#35757;&#32451;&#27861;&#12290;&#21069;&#32773;&#26368;&#23567;&#21270;Kullback-Leibler&#25955;&#24230;&#65292;&#32780;&#21518;&#32773;&#21017;&#26368;&#23567;&#21270;Fisher&#25955;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently sampling from un-normalized target distributions is a fundamental problem in scientific computing and machine learning. Traditional approaches like Markov Chain Monte Carlo (MCMC) guarantee asymptotically unbiased samples from such distributions but suffer from computational inefficiency, particularly when dealing with high-dimensional targets, as they require numerous iterations to generate a batch of samples. In this paper, we propose an efficient and scalable neural implicit sampler that overcomes these limitations. Our sampler can generate large batches of samples with low computational costs by leveraging a neural transformation that directly maps easily sampled latent vectors to target samples without the need for iterative procedures. To train the neural implicit sampler, we introduce two novel methods: the KL training method and the Fisher training method. The former minimizes the Kullback-Leibler divergence, while the latter minimizes the Fisher divergence. By empl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#32780;&#26159;&#21033;&#29992;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#20248;&#21270;&#31574;&#30053;&#65292;&#22240;&#27492;&#33021;&#22815;&#36866;&#24212;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#28436;&#21464;&#21644;&#20542;&#21521;&#24615;&#20943;&#24369;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.09900</link><description>&lt;p&gt;
&#26080;&#20132;&#21472;&#31574;&#30053;&#23398;&#20064;&#65306;&#24754;&#35266;&#21644;&#24191;&#20041;&#32463;&#39564;Bernstein&#19981;&#31561;&#24335;
&lt;/p&gt;
&lt;p&gt;
Policy learning "without'' overlap: Pessimism and generalized empirical Bernstein's inequality. (arXiv:2212.09900v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#32780;&#26159;&#21033;&#29992;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#20248;&#21270;&#31574;&#30053;&#65292;&#22240;&#27492;&#33021;&#22815;&#36866;&#24212;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#28436;&#21464;&#21644;&#20542;&#21521;&#24615;&#20943;&#24369;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#31574;&#30053;&#23398;&#20064;&#65292;&#26088;&#22312;&#21033;&#29992;&#20808;&#21069;&#25910;&#38598;&#21040;&#30340;&#35266;&#27979;&#65288;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#25110;&#26159;&#36866;&#24212;&#28436;&#21464;&#30340;&#34892;&#20026;&#31574;&#30053;&#65289;&#26469;&#23398;&#20064;&#32473;&#23450;&#31867;&#21035;&#20013;&#30340;&#26368;&#20248;&#20010;&#24615;&#21270;&#20915;&#31574;&#35268;&#21017;&#12290;&#29616;&#26377;&#30340;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#32479;&#19968;&#20132;&#21472;&#20551;&#35774;&#65292;&#21363;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#25506;&#32034;&#25152;&#26377;&#20010;&#24615;&#21270;&#29305;&#24449;&#30340;&#25152;&#26377;&#21160;&#20316;&#30340;&#20542;&#21521;&#24615;&#19979;&#30028;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#26368;&#22351;&#30340;&#20542;&#21521;&#24615;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#19981;&#21463;&#25511;&#21046;&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#19981;&#22826;&#29616;&#23454;&#65292;&#29305;&#21035;&#26159;&#24403;&#20801;&#35768;&#34892;&#20026;&#31574;&#30053;&#38543;&#26102;&#38388;&#28436;&#21464;&#24182;&#19988;&#20542;&#21521;&#24615;&#20943;&#24369;&#26102;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#23427;&#20248;&#21270;&#31574;&#30053;&#20215;&#20540;&#30340;&#19979;&#38480;&#32622;&#20449;&#21306;&#38388;&#65288;LCBs&#65289;&#8212;&#8212;&#32780;&#19981;&#26159;&#28857;&#20272;&#35745;&#12290;LCBs&#36890;&#36807;&#37327;&#21270;&#22686;&#24378;&#20498;&#25968;&#20542;&#21521;&#26435;&#37325;&#30340;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26469;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies offline policy learning, which aims at utilizing observations collected a priori (from either fixed or adaptively evolving behavior policies) to learn the optimal individualized decision rule in a given class. Existing policy learning methods rely on a uniform overlap assumption, i.e., the propensities of exploring all actions for all individual characteristics are lower bounded in the offline dataset. In other words, the performance of these methods depends on the worst-case propensity in the offline dataset. As one has no control over the data collection process, this assumption can be unrealistic in many situations, especially when the behavior policies are allowed to evolve over time with diminishing propensities.  In this paper, we propose a new algorithm that optimizes lower confidence bounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs are constructed by quantifying the estimation uncertainty of the augmented inverse propensity weight
&lt;/p&gt;</description></item></channel></rss>