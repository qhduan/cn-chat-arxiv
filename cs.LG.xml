<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;</title><link>https://rss.arxiv.org/abs/2402.01528</link><description>&lt;p&gt;
&#35299;&#30721;&#25512;&#27979;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
Decoding Speculative Decoding
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01528
&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#30340;&#25216;&#26415;&#65292;&#20294;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36873;&#25321;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#36234;&#39640;&#65292;&#21534;&#21520;&#37327;&#36234;&#20302;&#12290;&#25105;&#20204;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#20998;&#26512;&#20102;&#21508;&#31181;&#22240;&#32032;&#23545;&#25512;&#27979;&#35299;&#30721;&#25928;&#26524;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#27979;&#35299;&#30721;&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#26029;&#65292;&#32780;&#19981;&#20462;&#25913;&#20854;&#32467;&#26524;&#12290;&#22312;&#23545;LLM&#36827;&#34892;&#25512;&#26029;&#26102;&#65292;&#25512;&#27979;&#35299;&#30721;&#20351;&#29992;&#36739;&#23567;&#30340;&#33609;&#31295;&#27169;&#22411;&#29983;&#25104;&#25512;&#27979;&#20196;&#29260;&#65292;&#28982;&#21518;&#20351;&#29992;&#30446;&#26631;LLM&#39564;&#35777;&#36825;&#20123;&#33609;&#31295;&#20196;&#29260;&#12290;&#25512;&#27979;&#35299;&#30721;&#25552;&#20379;&#30340;&#21152;&#36895;&#21462;&#20915;&#20110;&#33609;&#31295;&#27169;&#22411;&#30340;&#36873;&#25321;&#12290;&#26222;&#36941;&#24314;&#35758;&#36873;&#25321;&#19968;&#20010;&#33609;&#31295;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;LLM&#25509;&#21463;&#30340;&#27010;&#29575;&#24456;&#39640;&#65292;&#20197;&#23454;&#29616;&#26368;&#39640;&#21534;&#21520;&#37327;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#19982;&#20043;&#30456;&#21453;&#65292;&#38543;&#30528;&#29983;&#25104;&#30340;&#20196;&#29260;&#34987;&#30446;&#26631;&#27169;&#22411;&#25509;&#21463;&#30340;&#27010;&#29575;&#22686;&#21152;&#65292;&#21534;&#21520;&#37327;&#20943;&#23569;&#12290;&#20026;&#20102;&#29702;&#35299;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23545;&#24433;&#21709;&#25512;&#27979;&#35299;&#30721;&#30340;&#19981;&#21516;&#22240;&#32032;&#36827;&#34892;&#20102;&#34920;&#24449;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22240;&#32032;&#22914;&#20309;&#30456;&#20114;&#20316;&#29992;&#21644;&#24433;&#21709;&#21152;&#36895;&#25928;&#26524;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#20010;&#20998;&#26512;&#27169;&#22411;&#65292;&#21487;&#20197;&#20351;&#29992;&#35813;&#27169;&#22411;&#26469;&#36827;&#34892;&#20915;&#31574;&#65292;&#25552;&#39640;&#25512;&#27979;&#35299;&#30721;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Speculative Decoding is a widely used technique to speed up inference for Large Language Models (LLMs) without modifying its outcome. When performing inference on an LLM, speculative decoding uses a smaller draft model which generates speculative tokens and then uses the target LLM to verify those draft tokens. The speedup provided by speculative decoding heavily depends on the choice of the draft model. It has been widely suggested to select a draft model that provides a high probability of the generated token being accepted by the LLM to achieve the highest throughput. However, our experiments indicate the contrary with throughput diminishing as the probability of generated tokens to be accepted by the target model increases. To understand this phenomenon, we perform extensive experiments to characterize the different factors that affect speculative decoding and how those factors interact and affect the speedups. Based on our experiments we describe an analytical model which can be u
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#33258;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#20998;&#37327;&#23376;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#21367;&#31215;&#26680;&#34920;&#31034;&#28145;&#23618;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#32593;&#32476;&#12290;</title><link>https://arxiv.org/abs/2403.14753</link><description>&lt;p&gt;
&#19982;SASQuaTCh&#23398;&#20064;&#65306;&#22522;&#20110;&#26680;&#33258;&#27880;&#24847;&#21147;&#30340;&#26032;&#22411;&#21464;&#20998;&#37327;&#23376;&#21464;&#21387;&#22120;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Learning with SASQuaTCh: a Novel Variational Quantum Transformer Architecture with Kernel-Based Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14753
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26680;&#33258;&#27880;&#24847;&#21147;&#26469;&#23398;&#20064;&#30340;&#26032;&#22411;&#21464;&#20998;&#37327;&#23376;&#21464;&#21387;&#22120;&#26550;&#26500;&#65292;&#21487;&#20197;&#29992;&#31616;&#21333;&#30340;&#21367;&#31215;&#26680;&#34920;&#31034;&#28145;&#23618;&#30340;&#35270;&#35273;&#21464;&#21387;&#22120;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#26222;&#21450;&#30340;&#24191;&#27867;&#27969;&#34892;&#30340;&#21464;&#21387;&#22120;&#32593;&#32476;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#39044;&#27979;&#25991;&#26412;&#21644;&#22270;&#20687;&#12289;&#20998;&#31867;&#65292;&#29978;&#33267;&#39044;&#27979;&#29289;&#29702;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#35299;&#12290;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#33021;&#22815;&#36890;&#36807;&#22522;&#20110;&#26680;&#30340;&#36816;&#31639;&#31526;&#23398;&#20064;&#35270;&#35282;&#39640;&#25928;&#34920;&#36798;&#33258;&#25105;&#27880;&#24847;&#26426;&#21046;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#22312;&#36825;&#20010;&#35270;&#35282;&#19979;&#65292;&#25105;&#20204;&#33021;&#22815;&#20351;&#29992;&#31616;&#21333;&#30340;&#21367;&#31215;&#26680;&#26469;&#34920;&#31034;&#35270;&#35273;&#21464;&#21387;&#22120;&#32593;&#32476;&#30340;&#28145;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14753v1 Announce Type: cross  Abstract: The widely popular transformer network popularized by the generative pre-trained transformer (GPT) has a large field of applicability, including predicting text and images, classification, and even predicting solutions to the dynamics of physical systems. In the latter context, the continuous analog of the self-attention mechanism at the heart of transformer networks has been applied to learning the solutions of partial differential equations and reveals a convolution kernel nature that can be exploited by the Fourier transform. It is well known that many quantum algorithms that have provably demonstrated a speedup over classical algorithms utilize the quantum Fourier transform. In this work, we explore quantum circuits that can efficiently express a self-attention mechanism through the perspective of kernel-based operator learning. In this perspective, we are able to represent deep layers of a vision transformer network using simple g
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.03726</link><description>&lt;p&gt;
&#34507;&#30333;&#36136;&#24207;&#21015;&#29983;&#25104;&#30340;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Diffusion on language model embeddings for protein sequence generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03726
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;DiMA&#27169;&#22411;&#65292;&#22312;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#25193;&#25955;&#26469;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#65292;&#27604;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#35774;&#35745;&#36873;&#25321;&#30340;&#24433;&#21709;&#26469;&#37327;&#21270;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34507;&#30333;&#35774;&#35745;&#38656;&#35201;&#23545;&#34507;&#30333;&#36136;&#23431;&#23449;&#22266;&#26377;&#22797;&#26434;&#24615;&#30340;&#28145;&#20837;&#20102;&#35299;&#12290;&#23613;&#31649;&#35768;&#22810;&#24037;&#20316;&#20542;&#21521;&#20110;&#26377;&#26465;&#20214;&#30340;&#29983;&#25104;&#25110;&#19987;&#27880;&#20110;&#29305;&#23450;&#34507;&#30333;&#36136;&#23478;&#26063;&#65292;&#20294;&#26080;&#26465;&#20214;&#29983;&#25104;&#30340;&#22522;&#30784;&#20219;&#21153;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#21644;&#37325;&#35270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#32034;&#36825;&#20010;&#20851;&#38190;&#39046;&#22495;&#65292;&#24341;&#20837;&#20102;DiMA&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#20174;&#34507;&#30333;&#35821;&#35328;&#27169;&#22411;ESM-2&#34893;&#29983;&#30340;&#23884;&#20837;&#36827;&#34892;&#36830;&#32493;&#25193;&#25955;&#20197;&#29983;&#25104;&#27688;&#22522;&#37240;&#24207;&#21015;&#30340;&#27169;&#22411;&#12290;DiMA&#36229;&#36234;&#20102;&#21253;&#25324;&#33258;&#22238;&#24402;&#21464;&#25442;&#22120;&#21644;&#31163;&#25955;&#25193;&#25955;&#27169;&#22411;&#22312;&#20869;&#30340;&#20027;&#35201;&#35299;&#20915;&#26041;&#26696;&#65292;&#25105;&#20204;&#23450;&#37327;&#22320;&#35828;&#26126;&#20102;&#23548;&#33268;&#20854;&#21331;&#36234;&#24615;&#33021;&#30340;&#35774;&#35745;&#36873;&#25321;&#25152;&#24102;&#26469;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#25351;&#26631;&#36328;&#22810;&#31181;&#24418;&#24335;&#24191;&#27867;&#35780;&#20272;&#29983;&#25104;&#24207;&#21015;&#30340;&#36136;&#37327;&#12289;&#22810;&#26679;&#24615;&#12289;&#20998;&#24067;&#30456;&#20284;&#24615;&#21644;&#29983;&#29289;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20135;&#29983;&#26032;&#39062;&#12289;&#22810;&#26679;&#21270;&#30340;&#34507;&#30333;&#36136;&#24207;&#21015;&#65292;&#31934;&#20934;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03726v1 Announce Type: cross  Abstract: Protein design requires a deep understanding of the inherent complexities of the protein universe. While many efforts lean towards conditional generation or focus on specific families of proteins, the foundational task of unconditional generation remains underexplored and undervalued. Here, we explore this pivotal domain, introducing DiMA, a model that leverages continuous diffusion on embeddings derived from the protein language model, ESM-2, to generate amino acid sequences. DiMA surpasses leading solutions, including autoregressive transformer-based and discrete diffusion models, and we quantitatively illustrate the impact of the design choices that lead to its superior performance. We extensively evaluate the quality, diversity, distribution similarity, and biological relevance of the generated sequences using multiple metrics across various modalities. Our approach consistently produces novel, diverse protein sequences that accura
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02780</link><description>&lt;p&gt;
&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Data Collaboration Analysis Over Matrix Manifolds
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02780
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35752;&#35770;&#20102;&#22312;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#25968;&#25454;&#21327;&#20316;&#20998;&#26512;&#65292;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;&#26469;&#22788;&#29702;&#22810;&#26469;&#28304;&#25968;&#25454;&#30340;&#36947;&#24503;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;(ML)&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#19982;&#20854;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#23494;&#20999;&#30456;&#20851;&#12290;&#25913;&#36827;&#30340;&#25968;&#25454;&#38598;&#65292;&#26631;&#24535;&#30528;&#20248;&#36234;&#30340;&#36136;&#37327;&#65292;&#22686;&#24378;&#20102;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#25193;&#23637;&#20102;&#27169;&#22411;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30340;&#36866;&#29992;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#32463;&#24120;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#30340;&#25968;&#25454;&#65292;&#20197;&#20943;&#36731;&#21333;&#19968;&#26469;&#28304;&#25968;&#25454;&#38598;&#30340;&#20559;&#35265;&#21644;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#24191;&#27867;&#30340;&#25968;&#25454;&#34701;&#21512;&#24341;&#21457;&#20102;&#37325;&#22823;&#30340;&#36947;&#24503;&#20851;&#20999;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#29992;&#25143;&#38544;&#31169;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#25968;&#25454;&#25259;&#38706;&#39118;&#38505;&#12290;&#24050;&#24314;&#31435;&#20102;&#21508;&#31181;&#20840;&#29699;&#31435;&#27861;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38544;&#31169;&#38382;&#39064;&#12290;&#34429;&#28982;&#36825;&#20123;&#27861;&#35268;&#23545;&#20445;&#25252;&#38544;&#31169;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;ML&#25216;&#26415;&#30340;&#23454;&#38469;&#37096;&#32626;&#21464;&#24471;&#22797;&#26434;&#12290;&#38544;&#31169;&#20445;&#25252;&#26426;&#22120;&#23398;&#20064;(PPML)&#36890;&#36807;&#20445;&#25252;&#20174;&#20581;&#24247;&#35760;&#24405;&#21040;&#22320;&#29702;&#20301;&#32622;&#25968;&#25454;&#31561;&#25935;&#24863;&#20449;&#24687;&#65292;&#21516;&#26102;&#23454;&#29616;&#23433;&#20840;&#20351;&#29992;&#36825;&#20123;&#20449;&#24687;&#65292;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02780v1 Announce Type: new  Abstract: The effectiveness of machine learning (ML) algorithms is deeply intertwined with the quality and diversity of their training datasets. Improved datasets, marked by superior quality, enhance the predictive accuracy and broaden the applicability of models across varied scenarios. Researchers often integrate data from multiple sources to mitigate biases and limitations of single-source datasets. However, this extensive data amalgamation raises significant ethical concerns, particularly regarding user privacy and the risk of unauthorized data disclosure. Various global legislative frameworks have been established to address these privacy issues. While crucial for safeguarding privacy, these regulations can complicate the practical deployment of ML technologies. Privacy-Preserving Machine Learning (PPML) addresses this challenge by safeguarding sensitive information, from health records to geolocation data, while enabling the secure use of th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;</title><link>https://arxiv.org/abs/2403.02233</link><description>&lt;p&gt;
Transformers&#22312;Masked Image Modeling&#20013;&#33021;&#22815;&#35777;&#26126;&#23398;&#20064;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
Transformers Provably Learn Feature-Position Correlations in Masked Image Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#65292;&#25581;&#31034;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Masked image modeling (MIM)&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#33258;&#30417;&#30563;&#35270;&#35273;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#23427;&#20174;&#26410;&#23631;&#34109;&#30340;&#22270;&#20687;&#20013;&#39044;&#27979;&#38543;&#26426;&#23631;&#34109;&#30340;&#34917;&#19969;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#22522;&#20110;transformers&#30340;MIM&#30340;&#29702;&#35770;&#29702;&#35299;&#30456;&#24403;&#26377;&#38480;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#20851;&#20351;&#29992;MIM&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#23398;&#20064;&#19968;&#23618;transformers&#30340;&#39318;&#20010;&#31471;&#21040;&#31471;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;transformers&#22914;&#20309;&#23398;&#20064;&#21040;&#22312;&#20855;&#26377;&#31354;&#38388;&#32467;&#26500;&#30340;&#25968;&#25454;&#20998;&#24067;&#19978;&#31361;&#26174;&#29305;&#24449;-&#20301;&#32622;&#30456;&#20851;&#24615;&#30340;&#26412;&#22320;&#21644;&#22810;&#26679;&#21270;&#27880;&#24847;&#27169;&#24335;&#30340;&#29702;&#35770;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02233v1 Announce Type: new  Abstract: Masked image modeling (MIM), which predicts randomly masked patches from unmasked ones, has emerged as a promising approach in self-supervised vision pretraining. However, the theoretical understanding of MIM is rather limited, especially with the foundational architecture of transformers. In this paper, to the best of our knowledge, we provide the first end-to-end theory of learning one-layer transformers with softmax attention in MIM self-supervised pretraining. On the conceptual side, we posit a theoretical mechanism of how transformers, pretrained with MIM, produce empirically observed local and diverse attention patterns on data distributions with spatial structures that highlight feature-position correlations. On the technical side, our end-to-end analysis of the training dynamics of softmax-based transformers accommodates both input and position embeddings simultaneously, which is developed based on a novel approach to track the i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.00165</link><description>&lt;p&gt;
TELEClass: &#31246;&#21153;&#23398;&#20016;&#23500;&#21644;LLM&#22686;&#24378;&#30340;&#26368;&#23567;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
TELEClass: Taxonomy Enrichment and LLM-Enhanced Hierarchical Text Classification with Minimal Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#23567;&#30417;&#30563;&#30340;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65292;&#21033;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#21807;&#19968;&#30417;&#30563;&#65292;&#21516;&#26102;&#32467;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26088;&#22312;&#23558;&#27599;&#20010;&#25991;&#26723;&#20998;&#31867;&#20026;&#26631;&#31614;Taxonomy&#20013;&#30340;&#19968;&#32452;&#31867;&#21035;&#12290;&#26412;&#25991;&#26088;&#22312;&#30740;&#31350;&#20351;&#29992;&#26368;&#23569;&#30417;&#30563;&#65306;&#20165;&#20351;&#29992;&#27599;&#20010;&#33410;&#28857;&#30340;&#21807;&#19968;&#31867;&#21517;&#20316;&#20026;&#30417;&#30563;&#26469;&#36827;&#34892;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38646;&#25552;&#31034;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#33021;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20998;&#23618;&#35774;&#32622;&#20013;&#34920;&#29616;&#36739;&#24046;&#65292;&#22240;&#20026;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#22823;&#32780;&#32467;&#26500;&#21270;&#30340;&#26631;&#31614;&#31354;&#38388;&#26159;&#26080;&#25928;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#20197;&#21069;&#30340;&#24369;&#30417;&#30563;&#20998;&#23618;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20165;&#21033;&#29992;&#21407;&#22987;&#30340;Taxonomy&#39592;&#26550;&#65292;&#24573;&#30053;&#20102;&#25991;&#26412;&#35821;&#26009;&#24211;&#20013;&#38544;&#34255;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#29992;&#20316;&#39069;&#22806;&#30340;&#31867;&#21035;&#25351;&#31034;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00165v1 Announce Type: new  Abstract: Hierarchical text classification aims to categorize each document into a set of classes in a label taxonomy. Most earlier works focus on fully or semi-supervised methods that require a large amount of human annotated data which is costly and time-consuming to acquire. To alleviate human efforts, in this paper, we work on hierarchical text classification with the minimal amount of supervision: using the sole class name of each node as the only supervision. Recently, large language models (LLM) show competitive performance on various tasks through zero-shot prompting, but this method performs poorly in the hierarchical setting, because it is ineffective to include the large and structured label space in a prompt. On the other hand, previous weakly-supervised hierarchical text classification methods only utilize the raw taxonomy skeleton and ignore the rich information hidden in the text corpus that can serve as additional class-indicative 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.18213</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21487;&#24494;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-objective Differentiable Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18213
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#65288;MOO&#65289;&#20013;&#30340;Pareto&#21069;&#27839;&#36718;&#24275;&#21078;&#26512;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#23588;&#20854;&#26159;&#22312;&#20687;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36825;&#26679;&#30340;&#26114;&#36149;&#30446;&#26631;&#20013;&#12290; &#30456;&#23545;&#20110;&#20256;&#32479;&#30340;NAS&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;NAS&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#19968;&#20010;&#25628;&#32034;&#36816;&#34892;&#20013;&#32534;&#30721;&#29992;&#25143;&#23545;&#24615;&#33021;&#21644;&#30828;&#20214;&#25351;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#20559;&#22909;&#65292;&#24182;&#29983;&#25104;&#31934;&#24515;&#36873;&#25321;&#30340;&#22810;&#35774;&#22791;&#26550;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#20010;&#36229;&#32593;&#32476;&#21442;&#25968;&#21270;&#36328;&#22810;&#20010;&#35774;&#22791;&#21644;&#22810;&#20010;&#30446;&#26631;&#30340;&#32852;&#21512;&#26550;&#26500;&#20998;&#24067;&#65292;&#36229;&#32593;&#32476;&#21487;&#20197;&#26681;&#25454;&#30828;&#20214;&#29305;&#24449;&#21644;&#20559;&#22909;&#21521;&#37327;&#36827;&#34892;&#26465;&#20214;&#21270;&#65292;&#23454;&#29616;&#38646;&#27425;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18213v1 Announce Type: new  Abstract: Pareto front profiling in multi-objective optimization (MOO), i.e. finding a diverse set of Pareto optimal solutions, is challenging, especially with expensive objectives like neural network training. Typically, in MOO neural architecture search (NAS), we aim to balance performance and hardware metrics across devices. Prior NAS approaches simplify this task by incorporating hardware constraints into the objective function, but profiling the Pareto front necessitates a search for each constraint. In this work, we propose a novel NAS algorithm that encodes user preferences for the trade-off between performance and hardware metrics, and yields representative and diverse architectures across multiple devices in just one search run. To this end, we parameterize the joint architectural distribution across devices and multiple objectives via a hypernetwork that can be conditioned on hardware features and preference vectors, enabling zero-shot t
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#27973;&#23618;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#26469;&#23398;&#20064;&#37327;&#23376;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#20581;&#22766;&#30340;&#27973;&#24433;&#21327;&#35758;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#35299;&#20915;&#37327;&#23376;&#22122;&#22768;&#21644;&#20559;&#24046;&#25361;&#25112;</title><link>https://arxiv.org/abs/2402.17911</link><description>&lt;p&gt;
&#29992;&#27973;&#24433;&#23398;&#20064;&#23637;&#31034;&#20581;&#22766;&#39640;&#25928;&#30340;&#37327;&#23376;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Demonstration of Robust and Efficient Quantum Property Learning with Shallow Shadows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17911
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#27973;&#23618;&#38543;&#26426;&#37327;&#23376;&#30005;&#36335;&#26469;&#23398;&#20064;&#37327;&#23376;&#24615;&#36136;&#65292;&#25552;&#20986;&#20102;&#20581;&#22766;&#30340;&#27973;&#24433;&#21327;&#35758;&#65292;&#21033;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#26469;&#35299;&#20915;&#37327;&#23376;&#22122;&#22768;&#21644;&#20559;&#24046;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#37327;&#23376;&#31995;&#32479;&#20013;&#39640;&#25928;&#25552;&#21462;&#20449;&#24687;&#26159;&#37327;&#23376;&#20449;&#24687;&#22788;&#29702;&#20219;&#21153;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38543;&#26426;&#21270;&#27979;&#37327;&#65292;&#25110;&#31216;&#32463;&#20856;&#38452;&#24433;&#65292;&#33021;&#22815;&#20351;&#29992;&#23569;&#37327;&#27979;&#37327;&#26469;&#39044;&#27979;&#20219;&#24847;&#37327;&#23376;&#24577;&#30340;&#35768;&#22810;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17911v1 Announce Type: cross  Abstract: Extracting information efficiently from quantum systems is a major component of quantum information processing tasks. Randomized measurements, or classical shadows, enable predicting many properties of arbitrary quantum states using few measurements. While random single qubit measurements are experimentally friendly and suitable for learning low-weight Pauli observables, they perform poorly for nonlocal observables. Prepending a shallow random quantum circuit before measurements maintains this experimental friendliness, but also has favorable sample complexities for observables beyond low-weight Paulis, including high-weight Paulis and global low-rank properties such as fidelity. However, in realistic scenarios, quantum noise accumulated with each additional layer of the shallow circuit biases the results. To address these challenges, we propose the robust shallow shadows protocol. Our protocol uses Bayesian inference to learn the expe
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#36825;&#19968;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#12290;</title><link>https://arxiv.org/abs/2402.16105</link><description>&lt;p&gt;
&#36890;&#30693;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Informed Meta-Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16105
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#36825;&#19968;&#26032;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#65292;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#30427;&#34892;&#30340;&#22024;&#26434;&#21644;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#20010;&#31361;&#20986;&#30340;&#25361;&#25112;&#22312;&#20110;&#26377;&#25928;&#22320;&#34701;&#21512;&#20419;&#36827;&#25968;&#25454;&#25928;&#29575;&#21644;&#31283;&#20581;&#24615;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#20803;&#23398;&#20064;&#21644;&#36890;&#30693;&#26426;&#22120;&#23398;&#20064;&#26159;&#20004;&#31181;&#23558;&#20808;&#39564;&#30693;&#35782;&#32435;&#20837;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#26041;&#27861;&#12290;&#21069;&#32773;&#20381;&#36182;&#20110;&#19968;&#31181;&#32431;&#25968;&#25454;&#39537;&#21160;&#30340;&#20808;&#39564;&#26469;&#28304;&#65292;&#32780;&#21518;&#32773;&#21463;&#19987;&#23478;&#30693;&#35782;&#30340;&#24418;&#24335;&#21270;&#34920;&#31034;&#24341;&#23548;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28151;&#21512;&#33539;&#24335;&#65292;&#36890;&#30693;&#20803;&#23398;&#20064;&#65292;&#26088;&#22312;&#23454;&#29616;&#20154;&#31867;&#21644;&#26426;&#22120;&#20043;&#38388;&#36328;&#20219;&#21153;&#30693;&#35782;&#20849;&#20139;&#30340;&#20114;&#34917;&#24615;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#25552;&#20986;&#20102;&#36825;&#19968;&#26694;&#26550;&#30340;&#20855;&#20307;&#23454;&#20363;--&#36890;&#30693;&#31070;&#32463;&#36807;&#31243;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35828;&#26126;&#24615;&#21644;&#26356;&#22823;&#35268;&#27169;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#30693;&#20803;&#23398;&#20064;&#22312;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#21644;&#25269;&#24481;&#35266;&#27979;&#22122;&#22768;&#26041;&#38754;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16105v1 Announce Type: new  Abstract: In noisy and low-data regimes prevalent in real-world applications, an outstanding challenge of machine learning lies in effectively incorporating inductive biases that promote data efficiency and robustness. Meta-learning and informed ML stand out as two approaches for incorporating prior knowledge into the ML pipeline. While the former relies on a purely data-driven source of priors, the latter is guided by a formal representation of expert knowledge. This paper introduces a novel hybrid paradigm, informed meta-learning, seeking complementarity in cross-task knowledge sharing of humans and machines. We establish the foundational components of informed meta-learning and present a concrete instantiation of this framework--the Informed Neural Process. Through a series of illustrative and larger-scale experiments, we demonstrate the potential benefits of informed meta-learning in improving data efficiency and robustness to observational no
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#22312;&#25152;&#26377;&#39640;&#25928;&#31639;&#27861;&#30340;&#24179;&#22343;&#24773;&#20917;&#22256;&#38590;&#24615;&#30340;&#35777;&#25454;&#65292;&#20551;&#35774;&#26684;&#38382;&#39064;&#30340;&#26368;&#22351;&#24773;&#20917;&#22256;&#38590;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14645</link><description>&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#21644;&#26684;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Sparse Linear Regression and Lattice Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#22312;&#25152;&#26377;&#39640;&#25928;&#31639;&#27861;&#30340;&#24179;&#22343;&#24773;&#20917;&#22256;&#38590;&#24615;&#30340;&#35777;&#25454;&#65292;&#20551;&#35774;&#26684;&#38382;&#39064;&#30340;&#26368;&#22351;&#24773;&#20917;&#22256;&#38590;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#32447;&#24615;&#22238;&#24402;&#65288;SLR&#65289;&#26159;&#32479;&#35745;&#23398;&#20013;&#19968;&#20010;&#30740;&#31350;&#33391;&#22909;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#32473;&#23450;&#35774;&#35745;&#30697;&#38453; $X\in\mathbb{R}^{m\times n}$ &#21644;&#21709;&#24212;&#21521;&#37327; $y=X\theta^*+w$&#65292;&#20854;&#20013; $\theta^*$ &#26159; $k$-&#31232;&#30095;&#21521;&#37327;&#65288;&#21363;&#65292;$\|\theta^*\|_0\leq k$&#65289;&#65292;$w$ &#26159;&#23567;&#30340;&#12289;&#20219;&#24847;&#30340;&#22122;&#22768;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010; $k$-&#31232;&#30095;&#30340; $\widehat{\theta} \in \mathbb{R}^n$&#65292;&#20351;&#24471;&#22343;&#26041;&#39044;&#27979;&#35823;&#24046; $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$ &#26368;&#23567;&#21270;&#12290;&#34429;&#28982; $\ell_1$-&#26494;&#24347;&#26041;&#27861;&#22914;&#22522; Pursuit&#12289;Lasso &#21644; Dantzig &#36873;&#25321;&#22120;&#22312;&#35774;&#35745;&#30697;&#38453;&#26465;&#20214;&#33391;&#22909;&#26102;&#35299;&#20915;&#20102; SLR&#65292;&#20294;&#27809;&#26377;&#24050;&#30693;&#36890;&#29992;&#31639;&#27861;&#65292;&#20063;&#27809;&#26377;&#20219;&#20309;&#20851;&#20110;&#22312;&#25152;&#26377;&#39640;&#25928;&#31639;&#27861;&#30340;&#24179;&#22343;&#24773;&#20917;&#35774;&#32622;&#20013;&#30340;&#22256;&#38590;&#24615;&#30340;&#27491;&#24335;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14645v1 Announce Type: new  Abstract: Sparse linear regression (SLR) is a well-studied problem in statistics where one is given a design matrix $X\in\mathbb{R}^{m\times n}$ and a response vector $y=X\theta^*+w$ for a $k$-sparse vector $\theta^*$ (that is, $\|\theta^*\|_0\leq k$) and small, arbitrary noise $w$, and the goal is to find a $k$-sparse $\widehat{\theta} \in \mathbb{R}^n$ that minimizes the mean squared prediction error $\frac{1}{m}\|X\widehat{\theta}-X\theta^*\|^2_2$. While $\ell_1$-relaxation methods such as basis pursuit, Lasso, and the Dantzig selector solve SLR when the design matrix is well-conditioned, no general algorithm is known, nor is there any formal evidence of hardness in an average-case setting with respect to all efficient algorithms.   We give evidence of average-case hardness of SLR w.r.t. all efficient algorithms assuming the worst-case hardness of lattice problems. Specifically, we give an instance-by-instance reduction from a variant of the bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;</title><link>https://arxiv.org/abs/2402.12062</link><description>&lt;p&gt;
&#22240;&#26524;&#24179;&#31561;&#20445;&#25252;&#19982;&#31639;&#27861;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
Causal Equal Protection as Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21407;&#21017;&#8212;&#8212;&#24179;&#31561;&#20445;&#25252;&#65292;&#20854;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#22343;&#31561;&#21270;&#65292;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20256;&#32479;&#20998;&#31867;&#24179;&#31561;&#21407;&#21017;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#21313;&#24180;&#65292;&#35745;&#31639;&#26426;&#31185;&#23398;&#21644;&#21746;&#23398;&#30340;&#25991;&#29486;&#24418;&#25104;&#20102;&#19981;&#21516;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#26631;&#20934;&#12290;&#20854;&#20013;&#26368;&#21463;&#20105;&#35758;&#30340;&#20998;&#31867;&#24179;&#31561;&#35201;&#27714;&#65292;&#39044;&#27979;&#31639;&#27861;&#30340;&#38169;&#35823;&#20998;&#31867;&#22312;&#34987;&#20445;&#25252;&#29305;&#24449;&#25152;&#25351;&#31034;&#30340;&#32676;&#20307;&#20013;&#20197;&#30456;&#31561;&#39057;&#29575;&#21457;&#29983;&#12290;&#23613;&#31649;&#20998;&#31867;&#24179;&#31561;&#20855;&#26377;&#30452;&#35266;&#21560;&#24341;&#21147;&#65292;&#20294;&#24050;&#21463;&#21040;&#25915;&#20987;&#12290;&#25105;&#20204;&#36716;&#21521;&#19968;&#20010;&#30456;&#20851;&#21407;&#21017;&#65292;&#21363;&#24179;&#31561;&#20445;&#25252;&#65292;&#35813;&#21407;&#21017;&#26368;&#21021;&#26159;&#22312;&#21009;&#20107;&#21496;&#27861;&#39046;&#22495;&#21457;&#23637;&#36215;&#26469;&#30340;&#12290;&#24179;&#31561;&#20445;&#25252;&#30340;&#20851;&#38190;&#22312;&#20110;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#39118;&#38505;&#65288;&#23558;&#22312;&#35268;&#23450;&#30340;&#24847;&#20041;&#19978;&#20855;&#20307;&#35828;&#26126;&#65289;&#36827;&#34892;&#22343;&#31561;&#21270;&#65292;&#32780;&#19981;&#26159;&#23558;&#38169;&#35823;&#20998;&#31867;&#30340;&#27604;&#29575;&#22343;&#31561;&#21270;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24179;&#31561;&#20445;&#25252;&#36991;&#20813;&#20102;&#35768;&#22810;&#23545;&#20998;&#31867;&#24179;&#31561;&#30340;&#21453;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12062v1 Announce Type: cross  Abstract: Over the last ten years the literature in computer science and philosophy has formulated different criteria of algorithmic fairness. One of the most discussed, classification parity, requires that the erroneous classifications of a predictive algorithm occur with equal frequency for groups picked out by protected characteristics. Despite its intuitive appeal, classification parity has come under attack. Multiple scenarios can be imagined in which - intuitively - a predictive algorithm does not treat any individual unfairly, and yet classification parity is violated. To make progress, we turn to a related principle, equal protection, originally developed in the context of criminal justice. Key to equal protection is equalizing the risks of erroneous classifications (in a sense to be specified) as opposed to equalizing the rates of erroneous classifications. We show that equal protection avoids many of the counterexamples to classificati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#21270;&#23398;&#21453;&#24212;&#25910;&#29575;&#39044;&#27979;&#12290;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#65292;&#20934;&#30830;&#30340;&#39640;&#25910;&#29575;&#39044;&#27979;&#23545;&#20110;&#21270;&#23398;&#23478;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#23548;&#33268;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#25910;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2402.05971</link><description>&lt;p&gt;
&#25105;&#20204;&#21462;&#24471;&#20102;&#22810;&#23569;&#36827;&#23637;&#65311;&#20174;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#21270;&#23398;&#21453;&#24212;&#25910;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Are we making much progress? Revisiting chemical reaction yield prediction from an imbalanced regression perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05971
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#19981;&#24179;&#34913;&#22238;&#24402;&#30340;&#35282;&#24230;&#37325;&#26032;&#23457;&#35270;&#20102;&#21270;&#23398;&#21453;&#24212;&#25910;&#29575;&#39044;&#27979;&#12290;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#65292;&#20934;&#30830;&#30340;&#39640;&#25910;&#29575;&#39044;&#27979;&#23545;&#20110;&#21270;&#23398;&#23478;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#23548;&#33268;&#20102;&#29616;&#26377;&#26041;&#27861;&#22312;&#39640;&#25910;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21270;&#23398;&#21453;&#24212;&#30340;&#25910;&#29575;&#26159;&#25351;&#30446;&#26631;&#20135;&#29289;&#24418;&#25104;&#30340;&#30334;&#20998;&#27604;&#19982;&#21270;&#23398;&#21453;&#24212;&#36807;&#31243;&#20013;&#28040;&#32791;&#30340;&#21453;&#24212;&#29289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20934;&#30830;&#30340;&#25910;&#29575;&#39044;&#27979;&#21487;&#20197;&#22312;&#21512;&#25104;&#35268;&#21010;&#20013;&#25351;&#23548;&#21270;&#23398;&#23478;&#36873;&#25321;&#39640;&#25910;&#29575;&#21453;&#24212;&#65292;&#22312;&#25237;&#20837;&#26102;&#38388;&#21644;&#36164;&#28304;&#36827;&#34892;&#28287;&#23454;&#39564;&#20043;&#21069;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#34429;&#28982;&#26368;&#36817;&#22312;&#25910;&#29575;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#25972;&#20307;&#24615;&#33021;&#30340;&#25913;&#36827;&#65292;&#20294;&#22312;&#25552;&#39640;&#39640;&#25910;&#29575;&#21453;&#24212;&#30340;&#39044;&#27979;&#26041;&#38754;&#20173;&#23384;&#22312;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#65292;&#36825;&#23545;&#20110;&#21270;&#23398;&#23478;&#26469;&#35828;&#26356;&#20026;&#37325;&#35201;&#12290;&#26412;&#25991;&#35748;&#20026;&#39640;&#25910;&#29575;&#39044;&#27979;&#26041;&#38754;&#30340;&#24615;&#33021;&#24046;&#36317;&#26159;&#30001;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#19981;&#24179;&#34913;&#20998;&#24067;&#25152;&#33268;&#65292;&#36825;&#20123;&#25968;&#25454;&#20559;&#21521;&#20110;&#20302;&#25910;&#29575;&#21453;&#24212;&#65292;&#36890;&#24120;&#26159;&#30001;&#20110;&#26410;&#21453;&#24212;&#30340;&#36215;&#22987;&#29289;&#36136;&#21644;&#21453;&#24212;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#27495;&#20041;&#24615;&#12290;&#23613;&#31649;&#23384;&#22312;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#29616;&#26377;&#30340;&#25910;&#29575;&#39044;&#27979;&#26041;&#27861;&#32487;&#32493;&#23558;&#19981;&#21516;&#25910;&#29575;&#33539;&#22260;&#35270;&#20026;&#24179;&#34913;&#30340;&#35757;&#32451;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
The yield of a chemical reaction quantifies the percentage of the target product formed in relation to the reactants consumed during the chemical reaction. Accurate yield prediction can guide chemists toward selecting high-yield reactions during synthesis planning, offering valuable insights before dedicating time and resources to wet lab experiments. While recent advancements in yield prediction have led to overall performance improvement across the entire yield range, an open challenge remains in enhancing predictions for high-yield reactions, which are of greater concern to chemists. In this paper, we argue that the performance gap in high-yield predictions results from the imbalanced distribution of real-world data skewed towards low-yield reactions, often due to unreacted starting materials and inherent ambiguities in the reaction processes. Despite this data imbalance, existing yield prediction methods continue to treat different yield ranges equally, assuming a balanced training
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35745;&#21010;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#24314;&#27169;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#19978;&#19979;&#25991;&#29615;&#22659;&#21644;&#38750;&#31283;&#24577;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26410;&#30693;&#30340;&#36716;&#31227;&#21160;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.04933</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#20855;&#26377;&#19978;&#19979;&#25991;&#29615;&#22659;&#30340;&#19981;&#23433;&#23425;&#36172;&#21338;&#26426;&#20013;&#30340;&#24212;&#29992;&#20110;&#20844;&#20849;&#21355;&#29983;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04933
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#26041;&#27861;&#30340;&#22312;&#32447;&#23398;&#20064;&#22312;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35745;&#21010;&#20013;&#30340;&#36164;&#28304;&#20998;&#37197;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36125;&#21494;&#26031;&#24314;&#27169;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#25216;&#26415;&#65292;&#33021;&#22815;&#28789;&#27963;&#22320;&#22788;&#29702;&#19978;&#19979;&#25991;&#29615;&#22659;&#21644;&#38750;&#31283;&#24577;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#24555;&#36895;&#23398;&#20064;&#26410;&#30693;&#30340;&#36716;&#31227;&#21160;&#24577;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#23433;&#23425;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMABs&#65289;&#29992;&#20110;&#24314;&#27169;&#20844;&#20849;&#21355;&#29983;&#24178;&#39044;&#35745;&#21010;&#20013;&#30340;&#39034;&#24207;&#36164;&#28304;&#20998;&#37197;&#12290;&#22312;&#36825;&#20123;&#24773;&#26223;&#20013;&#65292;&#28508;&#22312;&#30340;&#36716;&#31227;&#21160;&#24577;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#38656;&#35201;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;RMAB&#22312;&#32447;RL&#26041;&#27861;&#26080;&#27861;&#25972;&#21512;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#20844;&#20849;&#21355;&#29983;&#24212;&#29992;&#20013;&#24120;&#35265;&#30340;&#23646;&#24615;&#65292;&#22914;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#38750;&#31283;&#24577;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#36125;&#21494;&#26031;&#27169;&#22411;&#21644;&#27748;&#26222;&#26862;&#25277;&#26679;&#30340;&#19978;&#19979;&#25991;RMAB&#30340;&#36125;&#21494;&#26031;&#23398;&#20064;&#65288;BCoR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#32447;RL&#26041;&#27861;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#27169;&#25311;&#21508;&#31181;&#22797;&#26434;&#30340;RMAB&#35774;&#32622;&#65292;&#22914;&#19978;&#19979;&#25991;&#21644;&#38750;&#31283;&#24577;&#30340;RMAB&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#36129;&#29486;&#26159;&#22312;&#39044;&#31639;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20869;&#37096;&#21644;&#21508;&#20010;&#33218;&#20043;&#38388;&#30340;&#20849;&#20139;&#20449;&#24687;&#65292;&#22312;&#30456;&#23545;&#30701;&#30340;&#26102;&#38388;&#33539;&#22260;&#20869;&#24555;&#36895;&#23398;&#20064;&#26410;&#30693;&#30340;RMAB&#36716;&#31227;&#21160;&#24577;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;BCoR&#22312;&#26377;&#38480;&#30340;&#26102;&#38388;&#20869;&#23454;&#29616;&#20102;&#26174;&#33879;&#26356;&#39640;&#30340;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-armed bandits (RMABs) are used to model sequential resource allocation in public health intervention programs. In these settings, the underlying transition dynamics are often unknown a priori, requiring online reinforcement learning (RL). However, existing methods in online RL for RMABs cannot incorporate properties often present in real-world public health applications, such as contextual information and non-stationarity. We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model a wide range of complex RMAB settings, such as contextual and non-stationary RMABs. A key contribution of our approach is its ability to leverage shared information within and between arms to learn unknown RMAB transition dynamics quickly in budget-constrained settings with relatively short time horizons. Empirically, we show that BCoR achieves substantially higher finit
&lt;/p&gt;</description></item><item><title>DyExpert&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#21382;&#21490;&#28436;&#21270;&#36807;&#31243;&#24182;&#32467;&#21512;&#38142;&#25509;&#39044;&#27979;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#22270;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02168</link><description>&lt;p&gt;
&#36328;&#22495;&#21160;&#24577;&#38142;&#25509;&#39044;&#27979;&#30340;&#19968;&#31181;&#22270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
One Graph Model for Cross-domain Dynamic Link Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02168
&lt;/p&gt;
&lt;p&gt;
DyExpert&#26159;&#19968;&#31181;&#29992;&#20110;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#65292;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#21382;&#21490;&#28436;&#21270;&#36807;&#31243;&#24182;&#32467;&#21512;&#38142;&#25509;&#39044;&#27979;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#22270;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#22312;&#21508;&#20010;&#39046;&#22495;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;DyExpert&#65292;&#19968;&#31181;&#29992;&#20110;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#30340;&#21160;&#24577;&#22270;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#26126;&#30830;&#22320;&#24314;&#27169;&#21382;&#21490;&#28436;&#21270;&#36807;&#31243;&#65292;&#23398;&#20064;&#29305;&#23450;&#19979;&#28216;&#22270;&#30340;&#28436;&#21270;&#27169;&#24335;&#65292;&#24182;&#36827;&#32780;&#36827;&#34892;&#29305;&#23450;&#27169;&#24335;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;DyExpert&#37319;&#29992;&#20102;&#35299;&#30721;&#22120;&#20248;&#21270;&#30340;transformer&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#28436;&#21270;&#24314;&#27169;&#21644;&#38142;&#25509;&#39044;&#27979;&#30340;&#8220;&#26465;&#20214;&#38142;&#25509;&#29983;&#25104;&#8221;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#24182;&#34892;&#35757;&#32451;&#21644;&#25512;&#26029;&#12290;DyExpert&#22312;&#21253;&#21547;6&#30334;&#19975;&#20010;&#21160;&#24577;&#36793;&#30340;&#24191;&#27867;&#21160;&#24577;&#22270;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#22312;&#20843;&#20010;&#26410;&#35757;&#32451;&#30340;&#22270;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#26174;&#31034;DyExpert&#22312;&#36328;&#22495;&#38142;&#25509;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#19982;&#30456;&#21516;&#35774;&#32622;&#19979;&#30340;&#20808;&#36827;&#22522;&#20934;&#30456;&#27604;&#65292;DyExpert&#22312;&#20843;&#20010;&#22270;&#19978;&#30340;&#24179;&#22343;&#31934;&#30830;&#24230;&#25552;&#39640;&#20102;11.40&#65285;&#12290;&#26356;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26159;&#65292;&#22312;&#20845;&#20010;&#26410;&#35757;&#32451;&#30340;&#22270;&#19978;&#65292;&#23427;&#36229;&#36807;&#20102;&#20843;&#20010;&#20808;&#36827;&#22522;&#32447;&#30340;&#20840;&#30417;&#30563;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes DyExpert, a dynamic graph model for cross-domain link prediction. It can explicitly model historical evolving processes to learn the evolution pattern of a specific downstream graph and subsequently make pattern-specific link predictions. DyExpert adopts a decode-only transformer and is capable of efficiently parallel training and inference by \textit{conditioned link generation} that integrates both evolution modeling and link prediction. DyExpert is trained by extensive dynamic graphs across diverse domains, comprising 6M dynamic edges. Extensive experiments on eight untrained graphs demonstrate that DyExpert achieves state-of-the-art performance in cross-domain link prediction. Compared to the advanced baseline under the same setting, DyExpert achieves an average of 11.40% improvement Average Precision across eight graphs. More impressive, it surpasses the fully supervised performance of 8 advanced baselines on 6 untrained graphs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20165;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#20840;&#21442;&#25968;&#37325;&#26032;&#35757;&#32451;&#30340;&#20570;&#27861;&#65292;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#25110;&#29978;&#33267;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;PERP&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2312.15230</link><description>&lt;p&gt;
PERP: &#22312;LLMs&#26102;&#20195;&#37325;&#26032;&#24605;&#32771;&#20462;&#21098;-&#37325;&#26032;&#35757;&#32451;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PERP: Rethinking the Prune-Retrain Paradigm in the Era of LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15230
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#20165;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#20840;&#21442;&#25968;&#37325;&#26032;&#35757;&#32451;&#30340;&#20570;&#27861;&#65292;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#25110;&#29978;&#33267;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;PERP&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#20462;&#21098;&#23454;&#29616;&#39640;&#25928;&#21387;&#32553;&#65292;&#26174;&#33879;&#20943;&#23569;&#23384;&#20648;&#21644;&#35745;&#31639;&#38656;&#27714;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#20687;&#36845;&#20195;&#24133;&#20540;&#20462;&#21098;&#65288;IMP&#65292;Han&#31561;&#65292;2015&#65289;&#36825;&#26679;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#21487;&#20197;&#21435;&#38500;&#19981;&#37325;&#35201;&#30340;&#21442;&#25968;&#65292;&#24182;&#38656;&#35201;&#26114;&#36149;&#30340;&#37325;&#26032;&#35757;&#32451;&#36807;&#31243;&#20197;&#22312;&#20462;&#21098;&#21518;&#24674;&#22797;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20852;&#36215;&#65292;&#30001;&#20110;&#20869;&#23384;&#21644;&#35745;&#31639;&#38480;&#21046;&#65292;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#21464;&#24471;&#19981;&#21487;&#34892;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#37325;&#26032;&#35757;&#32451;&#25152;&#26377;&#21442;&#25968;&#30340;&#20570;&#27861;&#65292;&#36890;&#36807;&#35777;&#26126;&#21482;&#26356;&#26032;&#23569;&#37096;&#20998;&#39640;&#24230;&#34920;&#36798;&#21147;&#30340;&#21442;&#25968;&#36890;&#24120;&#36275;&#20197;&#24674;&#22797;&#29978;&#33267;&#25552;&#39640;&#24615;&#33021;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#20165;&#37325;&#26032;&#35757;&#32451;GPT-&#32467;&#26500;&#30340;0.27%-0.35%&#30340;&#21442;&#25968;&#21363;&#21487;&#22312;&#19981;&#21516;&#31232;&#30095;&#27700;&#24179;&#19978;&#23454;&#29616;&#19982;&#19968;&#27425;&#24615;IMP&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;&#20462;&#21098;&#21518;&#21442;&#25968;&#39640;&#25928;&#37325;&#26032;&#35757;&#32451;&#65288;PERP&#65289;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural Networks can be efficiently compressed through pruning, significantly reducing storage and computational demands while maintaining predictive performance. Simple yet effective methods like Iterative Magnitude Pruning (IMP, Han et al., 2015) remove less important parameters and require a costly retraining procedure to recover performance after pruning. However, with the rise of Large Language Models (LLMs), full retraining has become infeasible due to memory and compute constraints. In this study, we challenge the practice of retraining all parameters by demonstrating that updating only a small subset of highly expressive parameters is often sufficient to recover or even improve performance compared to full retraining. Surprisingly, retraining as little as 0.27%-0.35% of the parameters of GPT-architectures achieves comparable performance to One Shot IMP across various sparsity levels. Our approach, Parameter-Efficient Retraining after Pruning (PERP), drastically reduces compute a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.02501</link><description>&lt;p&gt;
&#32454;&#32990;&#20449;&#21495;&#20256;&#23548;&#32467;&#26500;&#21644;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
The cell signaling structure function. (arXiv:2401.02501v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02501
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#20808;&#39564;&#30340;&#39044;&#26399;&#27169;&#24335;&#21160;&#21147;&#23398;&#21644;&#35757;&#32451;&#25968;&#25454;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#65292;&#36890;&#36807;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#21644;&#21608;&#22260;&#32454;&#32990;&#36136;&#20043;&#38388;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#36890;&#36807;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#23558;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#34920;&#31034;&#20026;&#20302;&#32500;&#23884;&#20837;&#20013;&#30340;&#28857;&#65292;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#25429;&#25417;&#21040;&#30340;&#20116;&#32500;$(x,y,z,channel,time)$&#35270;&#39057;&#26174;&#31034;&#20102;&#32454;&#32990;&#36816;&#21160;&#21644;&#20449;&#21495;&#21160;&#21147;&#23398;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20986;&#19968;&#31181;&#22312;&#20116;&#32500;&#27963;&#20307;&#32454;&#32990;&#26174;&#24494;&#38236;&#35270;&#39057;&#20013;&#23547;&#25214;&#32454;&#32990;&#20449;&#21495;&#21160;&#21147;&#23398;&#26102;&#31354;&#27169;&#24335;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#19981;&#38656;&#35201;&#39044;&#20808;&#20102;&#35299;&#39044;&#26399;&#30340;&#27169;&#24335;&#21160;&#21147;&#23398;&#20197;&#21450;&#27809;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#32454;&#32990;&#20449;&#21495;&#32467;&#26500;&#20989;&#25968;&#65288;SSF&#65289;&#26159;&#19968;&#31181;Kolmogorov&#32467;&#26500;&#20989;&#25968;&#65292;&#21487;&#20197;&#36890;&#36807;&#26680;&#24515;&#21306;&#22495;&#30456;&#23545;&#20110;&#21608;&#22260;&#32454;&#32990;&#36136;&#30340;&#26680;&#31958;&#20307;&#24378;&#24230;&#26469;&#26368;&#20248;&#22320;&#27979;&#37327;&#32454;&#32990;&#20449;&#21495;&#29366;&#24577;&#65292;&#30456;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26680;&#31958;&#20307;&#19982;&#32454;&#32990;&#26680;&#27604;&#20540;&#26377;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24230;&#37327;&#24402;&#19968;&#21270;&#21387;&#32553;&#36317;&#31163;&#65288;NCD&#65289;&#26469;&#35782;&#21035;&#30456;&#20284;&#30340;&#27169;&#24335;&#12290;NCD&#26159;&#19968;&#20010;&#29992;&#20110;&#34920;&#31034;&#36755;&#20837;&#30340;SSF&#26500;&#22270;&#22312;&#20302;&#32500;&#23884;&#20837;&#20013;&#20316;&#20026;&#28857;&#30340;Hilbert&#31354;&#38388;&#30340;&#20877;&#29983;&#26680;&#65292;&#21487;&#20197;&#26368;&#20248;&#22320;&#25429;&#25417;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display patterns of cellular motion and signaling dynamics. We present here an approach to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell microscopy movies unique in requiring no \emph{a priori} knowledge of expected pattern dynamics, and no training data. The proposed cell signaling structure function (SSF) is a Kolmogorov structure function that optimally measures cell signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a significant improvement compared to the current state-of-the-art cytonuclear ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value, or a functional output such as velocity. Patterns of similarity are identified via the metric normalized compression distance (NCD). The NCD is a reproducing kernel for a Hilbert space that represents the input SSF kymographs as points in a low dimensional embedding that optimally captures the pattern
&lt;/p&gt;</description></item><item><title>HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.15318</link><description>&lt;p&gt;
HetGPT: &#21033;&#29992;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#25552;&#31034;&#35843;&#25972;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15318
&lt;/p&gt;
&lt;p&gt;
HetGPT&#26159;&#19968;&#31181;&#39044;&#35757;&#32451;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#26469;&#35299;&#20915;&#39044;&#35757;&#32451;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#29616;&#20026;&#34920;&#31034;&#21644;&#20998;&#26512;Web&#20013;&#30340;&#22797;&#26434;&#27169;&#24335;&#21644;&#20016;&#23500;&#20449;&#24687;&#30340;&#33258;&#28982;&#36873;&#25321;&#65292;&#20351;&#24471;&#22312;&#32447;&#39029;&#38754;&#20998;&#31867;&#21644;&#31038;&#20132;&#25512;&#33616;&#31561;&#24212;&#29992;&#25104;&#20026;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#8220;&#39044;&#35757;&#32451;&#65292;&#24494;&#35843;&#8221;&#33539;&#24335;&#22312;&#22270;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#26377;&#38480;&#26631;&#35760;&#33410;&#28857;&#30340;&#24773;&#20917;&#19979;&#65292;&#24448;&#24448;&#23384;&#22312;&#39044;&#35757;&#32451;&#30446;&#26631;&#20219;&#21153;&#19982;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#12290;&#36825;&#31181;&#24046;&#36317;&#21487;&#33021;&#23548;&#33268;&#8220;&#36127;&#36716;&#31227;&#8221;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#20852;&#36215;&#34920;&#26126;&#20102;&#23558;&#8220;&#39044;&#35757;&#32451;&#65292;&#25552;&#31034;&#8221;&#33539;&#24335;&#24212;&#29992;&#20110;&#22270;&#24418;&#30340;&#28508;&#21147;&#65292;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22270;&#24418;&#25552;&#31034;&#25216;&#26415;&#38024;&#23545;&#30340;&#26159;&#21516;&#36136;&#22270;&#65292;&#24573;&#35270;&#20102;Web&#22270;&#30340;&#20869;&#22312;&#24322;&#26500;&#24615;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HetGPT&#65292;
&lt;/p&gt;
&lt;p&gt;
Graphs have emerged as a natural choice to represent and analyze the intricate patterns and rich information of the Web, enabling applications such as online page classification and social recommendation. The prevailing "pre-train, fine-tune" paradigm has been widely adopted in graph machine learning tasks, particularly in scenarios with limited labeled nodes. However, this approach often exhibits a misalignment between the training objectives of pretext tasks and those of downstream tasks. This gap can result in the "negative transfer" problem, wherein the knowledge gained from pre-training adversely affects performance in the downstream tasks. The surge in prompt-based learning within Natural Language Processing (NLP) suggests the potential of adapting a "pre-train, prompt" paradigm to graphs as an alternative. However, existing graph prompting techniques are tailored to homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To bridge this gap, we propose HetGPT, a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;</title><link>http://arxiv.org/abs/2309.02705</link><description>&lt;p&gt;
&#35777;&#26126;LLM&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certifying LLM Safety against Adversarial Prompting. (arXiv:2309.02705v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#65292;&#29992;&#20110;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#12290;&#36890;&#36807;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#65292;&#30830;&#20445;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#37117;&#33021;&#34987;&#27491;&#30830;&#26631;&#35782;&#20026;&#26377;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#23433;&#20840;&#65292;&#20844;&#24320;&#20351;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#24341;&#20837;&#20102;&#25152;&#35859;&#30340;&#8220;&#27169;&#22411;&#23545;&#40784;&#8221;&#38450;&#25252;&#25514;&#26045;&#12290;&#19968;&#20010;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;&#24212;&#35813;&#25298;&#32477;&#29992;&#25143;&#30340;&#35831;&#27714;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#23433;&#20840;&#25514;&#26045;&#23481;&#26131;&#21463;&#21040;&#25932;&#23545;&#25552;&#31034;&#30340;&#25915;&#20987;&#65292;&#25932;&#23545;&#25552;&#31034;&#21253;&#21547;&#24694;&#24847;&#35774;&#35745;&#30340;&#26631;&#35760;&#24207;&#21015;&#65292;&#20197;&#35268;&#36991;&#27169;&#22411;&#30340;&#23433;&#20840;&#38450;&#25252;&#24182;&#23548;&#33268;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#39564;&#35777;&#23433;&#20840;&#20445;&#35777;&#30340;&#31532;&#19968;&#20010;&#23545;&#25239;&#25932;&#23545;&#25552;&#31034;&#30340;&#26694;&#26550;&#8212;&#8212;&#28040;&#38500;&#21644;&#26816;&#26597;&#12290;&#25105;&#20204;&#36880;&#20010;&#28040;&#38500;&#26631;&#35760;&#65292;&#24182;&#20351;&#29992;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#26597;&#29983;&#25104;&#30340;&#23376;&#24207;&#21015;&#12290;&#22914;&#26524;&#23433;&#20840;&#36807;&#28388;&#22120;&#26816;&#27979;&#21040;&#20219;&#20309;&#23376;&#24207;&#21015;&#25110;&#36755;&#20837;&#25552;&#31034;&#26377;&#23475;&#65292;&#25105;&#20204;&#30340;&#36807;&#31243;&#23558;&#23558;&#36755;&#20837;&#25552;&#31034;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#36825;&#20445;&#35777;&#20102;&#23545;&#20110;&#26576;&#20010;&#29305;&#23450;&#22823;&#23567;&#30340;&#26377;&#23475;&#36755;&#20837;&#25552;&#31034;&#30340;&#20219;&#20309;&#25932;&#23545;&#20462;&#25913;&#20063;&#23558;&#34987;&#26631;&#35760;&#20026;&#26377;&#23475;&#12290;&#25105;&#20204;&#23545;&#25239;&#19977;&#31181;&#25915;&#20987;&#27169;&#24335;&#65306;i)&#25932;&#23545;&#21518;&#32512;&#65292;&#21363;&#38468;&#21152;&#25932;&#23545;&#24207;&#21015;&#8230;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) released for public use incorporate guardrails to ensure their output is safe, often referred to as "model alignment." An aligned language model should decline a user's request to produce harmful content. However, such safety measures are vulnerable to adversarial prompts, which contain maliciously designed token sequences to circumvent the model's safety guards and cause it to produce harmful content. In this work, we introduce erase-and-check, the first framework to defend against adversarial prompts with verifiable safety guarantees. We erase tokens individually and inspect the resulting subsequences using a safety filter. Our procedure labels the input prompt as harmful if any subsequences or the input prompt are detected as harmful by the filter. This guarantees that any adversarial modification of a harmful prompt up to a certain size is also labeled harmful. We defend against three attack modes: i) adversarial suffix, which appends an adversarial seq
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#28041;&#21450;&#22823;&#37327;&#29420;&#31435;&#20174;&#23646;&#32773;&#30340;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#19968;&#20010;&#37319;&#26679;&#30340;&#23376;&#38598;&#20013;&#20272;&#35745;&#26410;&#37319;&#26679;&#20174;&#23646;&#32773;&#30340;&#30446;&#26631;&#20540;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#33324;&#20174;&#23646;&#32773;&#29305;&#24449;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2209.09404</link><description>&lt;p&gt;
&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#27714;&#35299;&#22823;&#35268;&#27169;&#21452;&#23618;&#21644;&#38543;&#26426;&#35268;&#21010;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;: &#20197;&#33258;&#34892;&#36710;&#32593;&#32476;&#35774;&#35745;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach to Solving Large Bilevel and Stochastic Programs: Application to Cycling Network Design. (arXiv:2209.09404v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.09404
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#28041;&#21450;&#22823;&#37327;&#29420;&#31435;&#20174;&#23646;&#32773;&#30340;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#20174;&#19968;&#20010;&#37319;&#26679;&#30340;&#23376;&#38598;&#20013;&#20272;&#35745;&#26410;&#37319;&#26679;&#20174;&#23646;&#32773;&#30340;&#30446;&#26631;&#20540;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23545;&#19968;&#33324;&#20174;&#23646;&#32773;&#29305;&#24449;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#27714;&#35299;&#28041;&#21450;&#22823;&#37327;&#29420;&#31435;&#20174;&#23646;&#32773;&#30340;&#21452;&#23618;&#35268;&#21010;&#38382;&#39064;&#65292;&#20854;&#20013;&#29305;&#27530;&#24773;&#20917;&#21253;&#25324;&#20004;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20248;&#21270;&#27169;&#22411;&#65292;&#26126;&#30830;&#32771;&#34385;&#21040;&#20174;&#19968;&#20010;&#37319;&#26679;&#30340;&#23376;&#38598;&#30340;&#20174;&#23646;&#32773;&#65292;&#24182;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20272;&#35745;&#26410;&#37319;&#26679;&#30340;&#20174;&#23646;&#32773;&#30340;&#30446;&#26631;&#20540;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#23558;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#23884;&#20837;&#21040;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#36825;&#20801;&#35768;&#25105;&#20204;&#20351;&#29992;&#26080;&#27861;&#36890;&#36807;&#39046;&#23548;&#32773;&#20915;&#31574;&#34920;&#31034;&#30340;&#19968;&#33324;&#20174;&#23646;&#32773;&#29305;&#24449;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29983;&#25104;&#30340;&#39046;&#23548;&#32773;&#20915;&#31574;&#22312;&#21407;&#22987;&#30446;&#26631;&#20989;&#25968;&#30340;&#26368;&#20248;&#38388;&#38553;&#19978;&#30340;&#30028;&#38480;&#65292;&#35813;&#30446;&#26631;&#20989;&#25968;&#32771;&#34385;&#21040;&#23436;&#25972;&#30340;&#20174;&#23646;&#32773;&#38598;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20174;&#23646;&#32773;&#37319;&#26679;&#31639;&#27861;&#26469;&#32553;&#23567;&#30028;&#38480;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#26469;&#23398;&#20064;&#20174;&#23646;&#32773;&#29305;&#24449;&#65292;&#36825;&#20123;&#29305;&#24449;&#21487;&#20197;&#29992;&#20316;&#23884;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#33258;&#34892;&#36710;&#32593;&#32476;&#35774;&#35745;&#38382;&#39064;&#30340;&#21512;&#25104;&#23454;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
We present a novel machine learning-based approach to solving bilevel programs that involve a large number of independent followers, which as a special case include two-stage stochastic programming. We propose an optimization model that explicitly considers a sampled subset of followers and exploits a machine learning model to estimate the objective values of unsampled followers. Unlike existing approaches, we embed machine learning model training into the optimization problem, which allows us to employ general follower features that can not be represented using leader decisions. We prove bounds on the optimality gap of the generated leader decision as measured by the original objective function that considers the full follower set. We then develop follower sampling algorithms to tighten the bounds and a representation learning approach to learn follower features, which can be used as inputs to the embedded machine learning model. Using synthetic instances of a cycling network design p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#38543;&#26426;&#24615;&#26469;&#25214;&#21040;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25910;&#25947;&#24615;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.05923</link><description>&lt;p&gt;
&#19968;&#31181;&#20195;&#25968;&#25910;&#25947;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#29992;&#20110;&#20840;&#23616;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Algebraically Converging Stochastic Gradient Descent Algorithm for Global Optimization. (arXiv:2204.05923v3 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.05923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#35843;&#25972;&#38543;&#26426;&#24615;&#26469;&#25214;&#21040;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#65292;&#24182;&#36890;&#36807;&#20195;&#25968;&#25910;&#25947;&#24615;&#35777;&#26126;&#20102;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#36890;&#36807;&#28155;&#21152;&#38543;&#26426;&#39033;&#26469;&#25214;&#21040;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#30340;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#26681;&#25454;&#30446;&#26631;&#20989;&#25968;&#30340;&#20540;&#33258;&#36866;&#24212;&#35843;&#25972;&#38543;&#26426;&#24615;&#12290;&#22312;&#27169;&#25311;&#36864;&#28779;&#30340;&#26415;&#35821;&#20013;&#65292;&#28201;&#24230;&#26159;&#19982;&#29366;&#24577;&#30456;&#20851;&#30340;&#12290;&#20511;&#27492;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#31639;&#27861;&#22312;&#27010;&#29575;&#21644;&#21442;&#25968;&#31354;&#38388;&#20013;&#20855;&#26377;&#20195;&#25968;&#25910;&#25947;&#24615;&#65292;&#36825;&#27604;&#20165;&#20351;&#29992;&#26356;&#31616;&#21333;&#30340;&#22122;&#22768;&#39033;&#25511;&#21046;&#30340;&#32463;&#20856;&#25910;&#25947;&#36895;&#29575;&#26377;&#20102;&#26174;&#33879;&#25913;&#36827;&#12290;&#25910;&#25947;&#35777;&#26126;&#22522;&#20110;&#31639;&#27861;&#30340;&#23454;&#38469;&#31163;&#25955;&#35774;&#32622;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#22914;&#25991;&#29486;&#20013;&#36890;&#24120;&#20570;&#30340;&#36830;&#32493;&#26497;&#38480;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20960;&#20010;&#25968;&#20540;&#31034;&#20363;&#65292;&#20197;&#23637;&#31034;&#31639;&#27861;&#23545;&#20110;&#30456;&#23545;&#22797;&#26434;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new gradient descent algorithm with added stochastic terms for finding the global optimizers of nonconvex optimization problems. A key component in the algorithm is the adaptive tuning of the randomness based on the value of the objective function. In the language of simulated annealing, the temperature is state-dependent. With this, we prove the global convergence of the algorithm with an algebraic rate both in probability and in the parameter space. This is a significant improvement over the classical rate from using a more straightforward control of the noise term. The convergence proof is based on the actual discrete setup of the algorithm, not just its continuous limit as often done in the literature. We also present several numerical examples to demonstrate the efficiency and robustness of the algorithm for reasonably complex objective functions.
&lt;/p&gt;</description></item></channel></rss>