<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.14802</link><description>&lt;p&gt;
&#22312;&#24322;&#36136;&#24615;&#19979;&#30340;&#38142;&#36335;&#39044;&#27979;: &#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Link Prediction under Heterophily: A Physics-Inspired Graph Neural Network Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14802
&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#24322;&#36136;&#22270;&#19978;&#30340;&#38142;&#36335;&#39044;&#27979;&#38754;&#20020;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;&#26041;&#27861;&#20197;&#22686;&#24378;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#30001;&#20110;&#20854;&#22312;&#23545;&#22270;&#34920;&#31034;&#30340;&#30495;&#23454;&#19990;&#30028;&#29616;&#35937;&#24314;&#27169;&#26041;&#38754;&#30340;&#28789;&#27963;&#24615;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#24050;&#25104;&#20026;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#30340;&#20107;&#23454;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;GNNs&#30340;&#28040;&#24687;&#20256;&#36882;&#26426;&#21046;&#22312;&#23398;&#20064;&#33021;&#21147;&#21644;&#34920;&#36798;&#33021;&#21147;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#65292;&#36825;&#38480;&#21046;&#20102;&#22312;&#24322;&#36136;&#22270;&#19978;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#33021;&#21147;&#65292;&#20854;&#20013;&#30456;&#37051;&#33410;&#28857;&#32463;&#24120;&#20855;&#26377;&#19981;&#21516;&#30340;&#26631;&#31614;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#23616;&#38480;&#20110;&#38024;&#23545;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#29305;&#23450;&#22522;&#20934;&#12290;&#36825;&#31181;&#29421;&#31364;&#30340;&#28966;&#28857;&#38480;&#21046;&#20102;&#38142;&#36335;&#39044;&#27979;&#22312;&#22810;&#20010;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#12290;&#20363;&#22914;&#65292;&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#65292;&#20004;&#20010;&#29992;&#25143;&#21487;&#33021;&#30001;&#20110;&#26576;&#31181;&#28508;&#22312;&#21407;&#22240;&#32780;&#36830;&#25509;&#65292;&#36825;&#20351;&#24471;&#25552;&#21069;&#39044;&#27979;&#36825;&#31181;&#36830;&#25509;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21463;&#29289;&#29702;&#21551;&#21457;&#30340;GNNs&#65288;&#22914;GRAFF&#65289;&#23545;&#25552;&#39640;&#33410;&#28857;&#20998;&#31867;&#24615;&#33021;&#25552;&#20379;&#20102;&#26174;&#33879;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14802v1 Announce Type: new  Abstract: In the past years, Graph Neural Networks (GNNs) have become the `de facto' standard in various deep learning domains, thanks to their flexibility in modeling real-world phenomena represented as graphs. However, the message-passing mechanism of GNNs faces challenges in learnability and expressivity, hindering high performance on heterophilic graphs, where adjacent nodes frequently have different labels. Most existing solutions addressing these challenges are primarily confined to specific benchmarks focused on node classification tasks. This narrow focus restricts the potential impact that link prediction under heterophily could offer in several applications, including recommender systems. For example, in social networks, two users may be connected for some latent reason, making it challenging to predict such connections in advance. Physics-Inspired GNNs such as GRAFF provided a significant contribution to enhance node classification perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#32422;&#31616;&#26041;&#27861;&#65292;&#21033;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#20102;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2402.02239</link><description>&lt;p&gt;
&#20998;&#24067;&#32422;&#31616;&#65306;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#38477;&#32500;&#21644;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Distributional Reduction: Unifying Dimensionality Reduction and Clustering with Gromov-Wasserstein Projection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#32422;&#31616;&#26041;&#27861;&#65292;&#21033;&#29992;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#25237;&#24433;&#32479;&#19968;&#20102;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#38477;&#32500;&#21644;&#32858;&#31867;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#39046;&#22495;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#26088;&#22312;&#25429;&#25417;&#28508;&#22312;&#30340;&#22823;&#35268;&#27169;&#21644;&#39640;&#32500;&#25968;&#25454;&#38598;&#30340;&#32467;&#26500;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#28041;&#21450;&#20351;&#29992;&#38477;&#32500;&#26041;&#27861;&#23558;&#25968;&#25454;&#25237;&#24433;&#21040;&#21487;&#35299;&#37322;&#30340;&#31354;&#38388;&#19978;&#65292;&#25110;&#23558;&#25968;&#25454;&#28857;&#32452;&#32455;&#25104;&#26377;&#24847;&#20041;&#30340;&#32858;&#31867;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#26159;&#25353;&#39034;&#24207;&#20351;&#29992;&#30340;&#65292;&#32780;&#19981;&#33021;&#20445;&#35777;&#32858;&#31867;&#19982;&#38477;&#32500;&#30456;&#19968;&#33268;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35266;&#28857;&#65306;&#20351;&#29992;&#20998;&#24067;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#20248;&#36755;&#36816;&#30340;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#26684;&#32599;&#33707;&#22827;-&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#65292;&#25105;&#20204;&#23558;&#32858;&#31867;&#21644;&#38477;&#32500;&#32479;&#19968;&#20026;&#19968;&#20010;&#31216;&#20026;&#20998;&#24067;&#32422;&#31616;&#30340;&#21333;&#19968;&#26694;&#26550;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#21333;&#20010;&#20248;&#21270;&#38382;&#39064;&#21516;&#26102;&#35299;&#20915;&#32858;&#31867;&#21644;&#38477;&#32500;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#22522;&#22240;&#32452;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised learning aims to capture the underlying structure of potentially large and high-dimensional datasets. Traditionally, this involves using dimensionality reduction methods to project data onto interpretable spaces or organizing points into meaningful clusters. In practice, these methods are used sequentially, without guaranteeing that the clustering aligns well with the conducted dimensionality reduction. In this work, we offer a fresh perspective: that of distributions. Leveraging tools from optimal transport, particularly the Gromov-Wasserstein distance, we unify clustering and dimensionality reduction into a single framework called distributional reduction. This allows us to jointly address clustering and dimensionality reduction with a single optimization problem. Through comprehensive experiments, we highlight the versatility and interpretability of our method and show that it outperforms existing approaches across a variety of image and genomics datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32858;&#31867;&#23558;&#22810;&#27169;&#24577;&#26679;&#26412;&#38598;&#20998;&#21106;&#25104;&#22810;&#20010;&#21333;&#27169;&#24577;&#26679;&#26412;&#38598;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;KDE&#20272;&#35745;&#26469;&#20272;&#35745;&#25972;&#20307;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#12289;&#38750;&#27491;&#24577;&#21644;&#39640;&#30456;&#20851;&#20998;&#24067;&#20272;&#35745;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.10566</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Robust Multi-Modal Density Estimation. (arXiv:2401.10566v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ROME&#30340;&#40065;&#26834;&#22810;&#27169;&#24577;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#32858;&#31867;&#23558;&#22810;&#27169;&#24577;&#26679;&#26412;&#38598;&#20998;&#21106;&#25104;&#22810;&#20010;&#21333;&#27169;&#24577;&#26679;&#26412;&#38598;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#30340;KDE&#20272;&#35745;&#26469;&#20272;&#35745;&#25972;&#20307;&#20998;&#24067;&#12290;&#36825;&#31181;&#26041;&#27861;&#35299;&#20915;&#20102;&#22810;&#27169;&#24577;&#12289;&#38750;&#27491;&#24577;&#21644;&#39640;&#30456;&#20851;&#20998;&#24067;&#20272;&#35745;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#27010;&#29575;&#39044;&#27979;&#27169;&#22411;&#30340;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#32508;&#21512;&#35780;&#20272;&#25351;&#26631;&#30340;&#38656;&#27714;&#12290;&#34429;&#28982;&#26377;&#20960;&#20010;&#25351;&#26631;&#21487;&#20197;&#34920;&#24449;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65288;&#20363;&#22914;&#65292;&#36127;&#23545;&#25968;&#20284;&#28982;&#12289;Jensen-Shannon&#25955;&#24230;&#65289;&#65292;&#20294;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#20316;&#29992;&#20110;&#27010;&#29575;&#23494;&#24230;&#19978;&#12290;&#22240;&#27492;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#32431;&#31929;&#22522;&#20110;&#26679;&#26412;&#30340;&#39044;&#27979;&#27169;&#22411;&#38656;&#35201;&#20272;&#35745;&#24213;&#23618;&#23494;&#24230;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#26041;&#27861;&#22914;&#26680;&#23494;&#24230;&#20272;&#35745;&#65288;KDE&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#32780;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#22312;&#22810;&#27169;&#24577;&#20272;&#35745;&#38382;&#39064;&#20013;&#23578;&#26410;&#24471;&#21040;&#35780;&#20272;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#21442;&#25968;&#30340;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;ROME&#65288;RObust Multi-modal density Estimator&#65289;&#65292;&#23427;&#35299;&#20915;&#20102;&#20272;&#35745;&#22810;&#27169;&#24577;&#12289;&#38750;&#27491;&#24577;&#21644;&#39640;&#30456;&#20851;&#20998;&#24067;&#30340;&#25361;&#25112;&#12290;ROME&#21033;&#29992;&#32858;&#31867;&#23558;&#22810;&#27169;&#24577;&#26679;&#26412;&#38598;&#20998;&#21106;&#25104;&#22810;&#20010;&#21333;&#27169;&#24577;&#26679;&#26412;&#38598;&#65292;&#28982;&#21518;&#32467;&#21512;&#31616;&#21333;&#30340;KDE&#20272;&#35745;&#26469;&#24471;&#21040;&#24635;&#20307;&#30340;&#20272;&#35745;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development of multi-modal, probabilistic prediction models has lead to a need for comprehensive evaluation metrics. While several metrics can characterize the accuracy of machine-learned models (e.g., negative log-likelihood, Jensen-Shannon divergence), these metrics typically operate on probability densities. Applying them to purely sample-based prediction models thus requires that the underlying density function is estimated. However, common methods such as kernel density estimation (KDE) have been demonstrated to lack robustness, while more complex methods have not been evaluated in multi-modal estimation problems. In this paper, we present ROME (RObust Multi-modal density Estimator), a non-parametric approach for density estimation which addresses the challenge of estimating multi-modal, non-normal, and highly correlated distributions. ROME utilizes clustering to segment a multi-modal set of samples into multiple uni-modal ones and then combines simple KDE estimates obtained for i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;O-RAN&#20013;&#32593;&#32476;&#20999;&#29255;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#20010;xAPPs&#65292;&#20998;&#21035;&#22788;&#29702;&#21151;&#29575;&#25511;&#21046;&#21644;&#29289;&#29702;&#36164;&#28304;&#22359;&#20998;&#37197;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#29992;&#25143;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#26368;&#22823;&#21270;&#30340;&#21152;&#26435;&#21534;&#21520;&#37327;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#22686;&#24378;&#22411;&#31227;&#21160;&#23485;&#24102;&#21644;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#36825;&#20004;&#31181;&#26381;&#21153;&#31867;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.08861</link><description>&lt;p&gt;
O-RAN&#20013;&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#32593;&#32476;&#20999;&#29255;&#30340;&#36164;&#28304;&#20998;&#37197;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Learning Approach for Efficient Resource Allocation with Network Slicing in O-RAN. (arXiv:2401.08861v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08861
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;O-RAN&#20013;&#32593;&#32476;&#20999;&#29255;&#21644;&#36164;&#28304;&#20998;&#37197;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#35774;&#35745;&#20004;&#20010;xAPPs&#65292;&#20998;&#21035;&#22788;&#29702;&#21151;&#29575;&#25511;&#21046;&#21644;&#29289;&#29702;&#36164;&#28304;&#22359;&#20998;&#37197;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#29992;&#25143;&#35774;&#22791;&#20043;&#38388;&#23454;&#29616;&#26368;&#22823;&#21270;&#30340;&#21152;&#26435;&#21534;&#21520;&#37327;&#65292;&#24182;&#20248;&#20808;&#32771;&#34385;&#22686;&#24378;&#22411;&#31227;&#21160;&#23485;&#24102;&#21644;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#36825;&#20004;&#31181;&#26381;&#21153;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#26080;&#32447;&#25509;&#20837;&#32593;&#32476;&#65288;O-RAN&#65289;&#25216;&#26415;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#32593;&#32476;&#36816;&#33829;&#21830;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#25918;&#21644;&#26377;&#21033;&#30340;&#29615;&#22659;&#12290;&#22312;O-RAN&#20869;&#30830;&#20445;&#26377;&#25928;&#22320;&#21327;&#35843;x&#24212;&#29992;&#31243;&#24207;&#65288;xAPPs&#65289;&#23545;&#20110;&#32593;&#32476;&#20999;&#29255;&#21644;&#36164;&#28304;&#20998;&#37197;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#36164;&#28304;&#20998;&#37197;&#26041;&#27861;&#65292;&#26088;&#22312;&#21327;&#35843;O-RAN&#20013;&#22810;&#20010;&#29420;&#31435;xAPPs&#30340;&#21327;&#35843;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#22312;&#29992;&#25143;&#35774;&#22791;&#65288;UE&#65289;&#20043;&#38388;&#26368;&#22823;&#21270;&#21152;&#26435;&#21534;&#21520;&#37327;&#65292;&#24182;&#20998;&#37197;&#29289;&#29702;&#36164;&#28304;&#22359;&#65288;PRBs&#65289;&#12290;&#25105;&#20204;&#20248;&#20808;&#32771;&#34385;&#22686;&#24378;&#22411;&#31227;&#21160;&#23485;&#24102;&#21644;&#36229;&#21487;&#38752;&#20302;&#24310;&#36831;&#36890;&#20449;&#36825;&#20004;&#31181;&#26381;&#21153;&#31867;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;xAPPs&#65306;&#27599;&#20010;UE&#30340;&#21151;&#29575;&#25511;&#21046;xAPP&#21644;PRB&#20998;&#37197;xAPP&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#30340;&#35757;&#32451;&#38454;&#27573;&#65292;&#20854;&#20013;&#31532;&#19968;&#37096;&#20998;&#20351;&#29992;&#24102;&#26377;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#30340;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Open Radio Access Network (O-RAN) technology has emerged as a promising solution for network operators, providing them with an open and favorable environment. Ensuring effective coordination of x-applications (xAPPs) is crucial to enhance flexibility and optimize network performance within the O-RAN. In this paper, we introduce an innovative approach to the resource allocation problem, aiming to coordinate multiple independent xAPPs for network slicing and resource allocation in O-RAN. Our proposed method focuses on maximizing the weighted throughput among user equipments (UE), as well as allocating physical resource blocks (PRBs). We prioritize two service types, namely enhanced Mobile Broadband and Ultra Reliable Low Latency Communication. To achieve this, we have designed two xAPPs: a power control xAPP for each UE and a PRB allocation xAPP. The proposed method consists of a two-part training phase, where the first part uses supervised learning with a Variational Autoencoder tra
&lt;/p&gt;</description></item></channel></rss>