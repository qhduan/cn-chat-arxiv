<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#30340;&#39640;&#25928;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#38024;&#23545;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#21644;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#26469;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#30828;&#20214;&#21451;&#22909;&#24615;&#12290;</title><link>https://arxiv.org/abs/2404.02882</link><description>&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#21270;
&lt;/p&gt;
&lt;p&gt;
Linear Attention Sequence Parallelism
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02882
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#30340;&#39640;&#25928;&#24207;&#21015;&#24182;&#34892;&#26041;&#27861;&#65292;&#38024;&#23545;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#36890;&#36807;&#35774;&#35745;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#21644;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#26469;&#38477;&#20302;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#23454;&#29616;&#30828;&#20214;&#21451;&#22909;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#24182;&#34892;&#65288;SP&#65289;&#20316;&#20026;&#19968;&#31181;&#22788;&#29702;&#36229;&#20986;&#21333;&#20010;GPU&#20869;&#23384;&#38480;&#21046;&#30340;&#38271;&#24207;&#21015;&#30340;&#27969;&#34892;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;SP&#26041;&#27861;&#24182;&#26410;&#21033;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#29305;&#24615;&#65292;&#23548;&#33268;&#22312;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#24182;&#34892;&#25928;&#29575;&#21644;&#21487;&#29992;&#24615;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32447;&#24615;&#27880;&#24847;&#21147;&#24207;&#21015;&#24182;&#34892;&#65288;LASP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;&#22522;&#20110;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#35821;&#35328;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#39640;&#25928;SP&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#26426;&#21046;&#65292;&#20197;&#21033;&#29992;&#32447;&#24615;&#27880;&#24847;&#21147;&#30340;&#21491;&#20056;&#20869;&#26680;&#25216;&#24039;&#65292;&#20174;&#32780;&#26174;&#30528;&#38477;&#20302;SP&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#25191;&#34892;&#20869;&#26680;&#34701;&#21512;&#21644;&#20013;&#38388;&#29366;&#24577;&#32531;&#23384;&#26469;&#22686;&#24378;LASP&#30340;&#23454;&#38469;&#25928;&#29575;&#65292;&#20351;LASP&#22312;GPU&#38598;&#32676;&#19978;&#30340;&#30828;&#20214;&#21451;&#22909;&#24615;&#24471;&#21040;&#25552;&#21319;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#31934;&#24515;&#30830;&#20445;&#24207;&#21015;&#32423;LASP&#19982;&#25152;&#26377;&#31867;&#22411;&#30340;&#25209;&#32423;&#25968;&#25454;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02882v1 Announce Type: cross  Abstract: Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data par
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20287</link><description>&lt;p&gt;
&#22522;&#20934;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Counterfactual Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#22312;&#29702;&#35299;&#21464;&#37327;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#29983;&#25104;&#26080;&#20559;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#22270;&#20687;&#29983;&#25104;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#35780;&#20272;&#23545;&#29031;&#29983;&#25104;&#30340;&#38656;&#27714;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#23545;&#29031;&#24773;&#26223;&#26159;&#27809;&#26377;&#21487;&#35266;&#27979;&#22522;&#20934;&#20107;&#23454;&#30340;&#20551;&#35774;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#39062;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20391;&#37325;&#20110;&#35780;&#20272;&#23545;&#29031;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20363;&#22914;&#32452;&#25104;&#12289;&#26377;&#25928;&#24615;&#12289;&#24178;&#39044;&#30340;&#26368;&#23567;&#24615;&#21644;&#22270;&#20687;&#36924;&#30495;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#33539;&#24335;&#30340;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#31867;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#37197;&#22791;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20287v1 Announce Type: cross  Abstract: Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further eval
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#31209;&#30340;&#29305;&#24449;&#20016;&#23500;&#24615;&#22686;&#24378;&#65288;RFR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26377;&#25928;&#31209;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#21069;&#21521;&#20860;&#23481;&#24615;&#30340;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#22312;&#21518;&#21521;&#20860;&#23481;&#24615;&#21644;&#21069;&#21521;&#20860;&#23481;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21452;&#37325;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.15517</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#31209;&#21644;&#29305;&#24449;&#20016;&#23500;&#24615;&#26469;&#25913;&#21892;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#21069;&#21521;&#20860;&#23481;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improving Forward Compatibility in Class Incremental Learning by Increasing Representation Rank and Feature Richness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15517
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#31209;&#30340;&#29305;&#24449;&#20016;&#23500;&#24615;&#22686;&#24378;&#65288;RFR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26377;&#25928;&#31209;&#65292;&#23454;&#29616;&#20102;&#25552;&#39640;&#21069;&#21521;&#20860;&#23481;&#24615;&#30340;&#30446;&#26631;&#12290;&#21516;&#26102;&#65292;&#22312;&#21518;&#21521;&#20860;&#23481;&#24615;&#21644;&#21069;&#21521;&#20860;&#23481;&#24615;&#26041;&#38754;&#21462;&#24471;&#20102;&#21452;&#37325;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26159;&#36830;&#32493;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#30041;&#20808;&#21069;&#20219;&#21153;&#30693;&#35782;&#30340;&#21516;&#26102;&#36880;&#28176;&#23398;&#20064;&#26032;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#25928;&#31209;&#30340;&#29305;&#24449;&#20016;&#23500;&#24615;&#22686;&#24378;&#65288;RFR&#65289;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#21069;&#21521;&#20860;&#23481;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#21152;&#22522;&#30784;&#38454;&#27573;&#34920;&#31034;&#30340;&#26377;&#25928;&#31209;&#65292;&#20174;&#32780;&#26377;&#21161;&#20110;&#21512;&#24182;&#26356;&#22810;&#19982;&#26410;&#35265;&#26032;&#20219;&#21153;&#30456;&#20851;&#30340;&#20449;&#24687;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;RFR&#22312;&#21518;&#21521;&#20860;&#23481;&#24615;&#21644;&#21069;&#21521;&#20860;&#23481;&#24615;&#26041;&#38754;&#23454;&#29616;&#20102;&#21452;&#37325;&#30446;&#26631;&#65306;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23567;&#29305;&#24449;&#25552;&#21462;&#22120;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15517v1 Announce Type: new  Abstract: Class Incremental Learning (CIL) constitutes a pivotal subfield within continual learning, aimed at enabling models to progressively learn new classification tasks while retaining knowledge obtained from prior tasks. Although previous studies have predominantly focused on backward compatible approaches to mitigate catastrophic forgetting, recent investigations have introduced forward compatible methods to enhance performance on novel tasks and complement existing backward compatible methods. In this study, we introduce an effective-Rank based Feature Richness enhancement (RFR) method, designed for improving forward compatibility. Specifically, this method increases the effective rank of representations during the base session, thereby facilitating the incorporation of more informative features pertinent to unseen novel tasks. Consequently, RFR achieves dual objectives in backward and forward compatibility: minimizing feature extractor mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.12094</link><description>&lt;p&gt;
LLMs&#26159;&#19968;&#20010;&#22909;&#30340;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#27714;&#35299;&#22120;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are LLMs Good Cryptic Crossword Solvers?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24314;&#31435;&#20102;&#19977;&#31181;&#27969;&#34892;LLMs&#30340;&#22522;&#20934;&#32467;&#26524;&#65292;&#34920;&#26126;&#23427;&#20204;&#22312;&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38590;&#35299;&#22635;&#23383;&#28216;&#25103;&#26159;&#19968;&#31181;&#35868;&#39064;&#65292;&#19981;&#20165;&#20381;&#36182;&#20110;&#19968;&#33324;&#30693;&#35782;&#65292;&#36824;&#20381;&#36182;&#20110;&#27714;&#35299;&#32773;&#22312;&#19981;&#21516;&#23618;&#38754;&#19978;&#25805;&#32437;&#35821;&#35328;&#24182;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#25991;&#23383;&#28216;&#25103;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;&#29616;&#20195;NLP&#27169;&#22411;&#26469;&#35828;&#65292;&#35299;&#20915;&#36825;&#31867;&#35868;&#39064;&#20063;&#26159;&#19968;&#39033;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#23578;&#26410;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20026;&#19977;&#31181;&#27969;&#34892;&#30340;LLMs -- LLaMA2&#12289;Mistral&#21644;ChatGPT&#24314;&#31435;&#20102;&#22522;&#20934;&#32467;&#26524;&#65292;&#26174;&#31034;&#23427;&#20204;&#22312;&#36825;&#19968;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20173;&#36828;&#36828;&#19981;&#21450;&#20154;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12094v1 Announce Type: new  Abstract: Cryptic crosswords are puzzles that rely not only on general knowledge but also on the solver's ability to manipulate language on different levels and deal with various types of wordplay. Previous research suggests that solving such puzzles is a challenge even for modern NLP models. However, the abilities of large language models (LLMs) have not yet been tested on this task. In this paper, we establish the benchmark results for three popular LLMs -- LLaMA2, Mistral, and ChatGPT -- showing that their performance on this task is still far from that of humans.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.10799</link><description>&lt;p&gt;
&#20351;&#29992;&#33258;&#36866;&#24212;&#20272;&#35745;&#34701;&#21512;&#39640;&#25928;&#21098;&#26525;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Efficient Pruning of Large Language Model with Adaptive Estimation Fusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10799
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#21098;&#26525;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#26681;&#25454;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#29983;&#25104;&#24615;&#19979;&#28216;&#20219;&#21153;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#23548;&#33268;&#22312;&#36164;&#28304;&#21463;&#38480;&#35774;&#22791;&#19978;&#39640;&#25928;&#37096;&#32626;&#23427;&#20204;&#25104;&#20026;&#19981;&#21487;&#36991;&#20813;&#30340;&#36235;&#21183;&#21644;&#37325;&#22823;&#25361;&#25112;&#12290;&#32467;&#26500;&#21270;&#21098;&#26525;&#26159;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#30340;&#24191;&#27867;&#24212;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;&#22797;&#26434;&#32467;&#26500;&#26102;&#65292;&#36890;&#24120;&#30340;&#26041;&#27861;&#24448;&#24448;&#37319;&#29992;&#24120;&#35265;&#30340;&#20272;&#35745;&#26041;&#27861;&#36827;&#34892;&#21098;&#26525;&#12290;&#36825;&#20123;&#26041;&#27861;&#23548;&#33268;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#31934;&#24230;&#19979;&#38477;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#33258;&#36866;&#24212;&#22320;&#27169;&#25311;&#27599;&#20010;&#23376;&#32467;&#26500;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#22522;&#20110;&#22797;&#26434;&#21644;&#22810;&#23618;&#32467;&#26500;&#30340;&#32467;&#26524;&#65292;&#33258;&#36866;&#24212;&#22320;&#34701;&#21512;&#31895;&#31890;&#24230;&#21644;&#32454;&#31890;&#24230;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#25152;&#26377;&#26041;&#38754;&#37117;&#26080;&#32541;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#30340;&#21098;&#26525;&#26694;&#26550;&#20013;&#12290;&#19982;&#20027;&#27969;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10799v1 Announce Type: cross  Abstract: Large language models (LLMs) have become crucial for many generative downstream tasks, leading to an inevitable trend and significant challenge to deploy them efficiently on resource-constrained devices. Structured pruning is a widely used method to address this challenge. However, when dealing with the complex structure of the multiple decoder layers, general methods often employ common estimation approaches for pruning. These approaches lead to a decline in accuracy for specific downstream tasks. In this paper, we introduce a simple yet efficient method that adaptively models the importance of each substructure. Meanwhile, it can adaptively fuse coarse-grained and finegrained estimations based on the results from complex and multilayer structures. All aspects of our design seamlessly integrate into the endto-end pruning framework. Our experimental results, compared with state-of-the-art methods on mainstream datasets, demonstrate ave
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.10182</link><description>&lt;p&gt;
&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#65306;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Reliable uncertainty with cheaper neural network ensembles: a case study in industrial parts classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10182
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#22312;&#24037;&#19994;&#38646;&#37096;&#20214;&#20998;&#31867;&#20013;&#25506;&#35752;&#20102;&#21033;&#29992;&#26356;&#20415;&#23452;&#30340;&#31070;&#32463;&#32593;&#32476;&#38598;&#25104;&#23454;&#29616;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36816;&#31609;&#23398;(OR)&#20013;&#65292;&#39044;&#27979;&#27169;&#22411;&#32463;&#24120;&#20250;&#36935;&#21040;&#25968;&#25454;&#20998;&#24067;&#19982;&#35757;&#32451;&#25968;&#25454;&#20998;&#24067;&#19981;&#21516;&#30340;&#22330;&#26223;&#12290;&#36817;&#24180;&#26469;&#65292;&#31070;&#32463;&#32593;&#32476;(NNs)&#22312;&#22270;&#20687;&#20998;&#31867;&#31561;&#39046;&#22495;&#30340;&#20986;&#33394;&#24615;&#33021;&#20351;&#20854;&#22312;OR&#20013;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#23545;OOD&#25968;&#25454;&#26102;&#65292;NNs&#24448;&#24448;&#20250;&#20570;&#20986;&#33258;&#20449;&#20294;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20026;&#33258;&#20449;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#24403;&#36755;&#20986;&#24212;(&#19981;&#24212;)&#34987;&#20449;&#20219;&#26102;&#36827;&#34892;&#36890;&#20449;&#12290;&#22240;&#27492;&#65292;&#22312;OR&#39046;&#22495;&#20013;&#65292;NNs&#20013;&#30340;&#21487;&#38752;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#22810;&#20010;&#29420;&#31435;NNs&#32452;&#25104;&#30340;&#28145;&#24230;&#38598;&#21512;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#25552;&#20379;&#24378;&#22823;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#36824;&#33021;&#21487;&#38752;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#30001;&#20110;&#36739;&#22823;&#30340;&#35745;&#31639;&#38656;&#27714;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#22522;&#30784;&#30740;&#31350;&#25552;&#20986;&#20102;&#26356;&#39640;&#25928;&#30340;NN&#38598;&#25104;&#65292;&#21363;sna
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10182v1 Announce Type: new  Abstract: In operations research (OR), predictive models often encounter out-of-distribution (OOD) scenarios where the data distribution differs from the training data distribution. In recent years, neural networks (NNs) are gaining traction in OR for their exceptional performance in fields such as image classification. However, NNs tend to make confident yet incorrect predictions when confronted with OOD data. Uncertainty estimation offers a solution to overconfident models, communicating when the output should (not) be trusted. Hence, reliable uncertainty quantification in NNs is crucial in the OR domain. Deep ensembles, composed of multiple independent NNs, have emerged as a promising approach, offering not only strong predictive accuracy but also reliable uncertainty estimation. However, their deployment is challenging due to substantial computational demands. Recent fundamental research has proposed more efficient NN ensembles, namely the sna
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22312;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#19979;&#24314;&#31435;&#20102;&#20851;&#20110;&#19968;&#33324;&#38750;&#20984;&#21644;$\mu$-PL&#38750;&#20984;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#21160;&#37327;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#30028;&#38480;&#65292;&#35206;&#30422;&#20102;&#19968;&#33324;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#26799;&#24230;&#20272;&#35745;&#26377;&#20559;&#26102;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#24433;&#21709;&#65292;&#21363;&#22312;&#20803;&#23398;&#20064;&#21644;&#26799;&#24230;&#34987;&#21387;&#32553;&#25110;&#21098;&#20999;&#26102;&#12290;</title><link>https://arxiv.org/abs/2403.00853</link><description>&lt;p&gt;
&#22312;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#19979;&#30340;&#20998;&#24067;&#24335;&#21160;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Distributed Momentum Methods Under Biased Gradient Estimations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00853
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20559;&#24046;&#26799;&#24230;&#20272;&#35745;&#19979;&#24314;&#31435;&#20102;&#20851;&#20110;&#19968;&#33324;&#38750;&#20984;&#21644;$\mu$-PL&#38750;&#20984;&#38382;&#39064;&#30340;&#20998;&#24067;&#24335;&#21160;&#37327;&#26041;&#27861;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#30028;&#38480;&#65292;&#35206;&#30422;&#20102;&#19968;&#33324;&#20998;&#24067;&#24335;&#20248;&#21270;&#38382;&#39064;&#30340;&#20998;&#26512;&#65292;&#24182;&#25581;&#31034;&#20102;&#26799;&#24230;&#20272;&#35745;&#26377;&#20559;&#26102;&#30340;&#29305;&#27530;&#24773;&#20917;&#19979;&#30340;&#24433;&#21709;&#65292;&#21363;&#22312;&#20803;&#23398;&#20064;&#21644;&#26799;&#24230;&#34987;&#21387;&#32553;&#25110;&#21098;&#20999;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#38543;&#26426;&#26799;&#24230;&#26041;&#27861;&#22312;&#35299;&#20915;&#28041;&#21450;&#20998;&#24067;&#22312;&#22810;&#20010;&#33410;&#28857;&#19978;&#30340;&#25968;&#25454;&#30340;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#26085;&#30410;&#21463;&#21040;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#33719;&#24471;&#26080;&#20559;&#30340;&#38543;&#26426;&#26799;&#24230;&#65292;&#36825;&#26159;&#22823;&#22810;&#25968;&#29702;&#35770;&#30740;&#31350;&#30340;&#37325;&#28857;&#65292;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26799;&#24230;&#20272;&#35745;&#24456;&#23481;&#26131;&#21464;&#24471;&#26377;&#20559;&#65292;&#20363;&#22914;&#65292;&#22312;&#26799;&#24230;&#34987;&#21387;&#32553;&#25110;&#21098;&#20999;&#26102;&#65292;&#25968;&#25454;&#34987;&#27927;&#29260;&#26102;&#65292;&#20197;&#21450;&#22312;&#20803;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00853v1 Announce Type: new  Abstract: Distributed stochastic gradient methods are gaining prominence in solving large-scale machine learning problems that involve data distributed across multiple nodes. However, obtaining unbiased stochastic gradients, which have been the focus of most theoretical research, is challenging in many distributed machine learning applications. The gradient estimations easily become biased, for example, when gradients are compressed or clipped, when data is shuffled, and in meta-learning and reinforcement learning. In this work, we establish non-asymptotic convergence bounds on distributed momentum methods under biased gradient estimation on both general non-convex and $\mu$-PL non-convex problems. Our analysis covers general distributed optimization problems, and we work out the implications for special cases where gradient estimates are biased, i.e., in meta-learning and when the gradients are compressed or clipped. Our numerical experiments on 
&lt;/p&gt;</description></item><item><title>DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.17812</link><description>&lt;p&gt;
DropBP&#65306;&#36890;&#36807;&#20002;&#24323;&#21453;&#21521;&#20256;&#25773;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
DropBP: Accelerating Fine-Tuning of Large Language Models by Dropping Backward Propagation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17812
&lt;/p&gt;
&lt;p&gt;
DropBP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#26469;&#21152;&#36895;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#28041;&#21450;&#27491;&#21521;&#21644;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#22823;&#37327;&#35745;&#31639;&#25104;&#26412;&#12290;&#20256;&#32479;&#30340;&#23618;&#27425;&#20002;&#24323;&#25216;&#26415;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20002;&#24323;&#26576;&#20123;&#23618;&#20197;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;&#28982;&#32780;&#65292;&#22312;&#27491;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#20002;&#24323;&#23618;&#20250;&#23545;&#35757;&#32451;&#36807;&#31243;&#20135;&#29983;&#19981;&#21033;&#24433;&#21709;&#65292;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DropBP&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21516;&#26102;&#20445;&#25345;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;DropBP&#22312;&#21453;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#38543;&#26426;&#20002;&#24323;&#23618;&#65292;&#19981;&#24433;&#21709;&#27491;&#21521;&#20256;&#25773;&#12290;&#27492;&#22806;&#65292;DropBP&#35745;&#31639;&#27599;&#20010;&#23618;&#30340;&#25935;&#24863;&#24615;&#20197;&#20998;&#37197;&#36866;&#24403;&#30340;&#20002;&#22833;&#29575;&#65292;&#20174;&#32780;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#12290;DropBP&#26088;&#22312;&#36890;&#36807;&#21453;&#21521;&#20256;&#25773;&#22686;&#24378;&#35757;&#32451;&#36807;&#31243;&#30340;&#25928;&#29575;&#65292;&#20174;&#32780;&#21152;&#36895;&#20351;&#29992;&#21453;&#21521;&#20256;&#25773;&#36827;&#34892;&#23436;&#20840;&#24494;&#35843;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17812v1 Announce Type: cross  Abstract: Training deep neural networks typically involves substantial computational costs during both forward and backward propagation. The conventional layer dropping techniques drop certain layers during training for reducing the computations burden. However, dropping layers during forward propagation adversely affects the training process by degrading accuracy. In this paper, we propose Dropping Backward Propagation (DropBP), a novel approach designed to reduce computational costs while maintaining accuracy. DropBP randomly drops layers during the backward propagation, which does not deviate forward propagation. Moreover, DropBP calculates the sensitivity of each layer to assign appropriate drop rate, thereby stabilizing the training process. DropBP is designed to enhance the efficiency of the training process with backpropagation, thereby enabling the acceleration of both full fine-tuning and parameter-efficient fine-tuning using backpropag
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23398;&#20064;&#30524;&#24213;&#22270;&#20687;&#20013;&#30524;&#37096;&#30142;&#30149;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.16865</link><description>&lt;p&gt;
&#36890;&#36807;&#23558;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#25552;&#39640;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Improve Robustness of Eye Disease Detection by including Learnable Probabilistic Discrete Latent Variables into Machine Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16865
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#21487;&#23398;&#20064;&#30340;&#27010;&#29575;&#31163;&#25955;&#28508;&#21464;&#37327;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30524;&#37096;&#30142;&#30149;&#26816;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#26469;&#23398;&#20064;&#30524;&#24213;&#22270;&#20687;&#20013;&#30524;&#37096;&#30142;&#30149;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30524;&#37096;&#30142;&#30149;&#20174;&#31958;&#23615;&#30149;&#24615;&#35270;&#32593;&#33180;&#30149;&#21464;&#21040;&#38738;&#20809;&#30524;&#31561;&#65292;&#30001;&#20110;&#20854;&#39640;&#21457;&#30149;&#29575;&#21644;&#21487;&#33021;&#23548;&#33268;&#35270;&#21147;&#25439;&#23475;&#65292;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#20844;&#20849;&#21355;&#29983;&#25361;&#25112;&#12290;&#21450;&#26089;&#21644;&#20934;&#30830;&#30340;&#35786;&#26029;&#23545;&#20110;&#26377;&#25928;&#27835;&#30103;&#21644;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#20998;&#26512;&#21307;&#23398;&#22270;&#20687;&#65288;&#21253;&#25324;&#30524;&#37096;&#22270;&#20687;&#65289;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#21644;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#65292;&#36825;&#23545;&#20020;&#24202;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;GFlowOut&#30340;&#26032;&#39062;&#24212;&#29992;&#65292;&#21033;&#29992;&#29983;&#25104;&#27969;&#32593;&#32476;&#65288;GFlowNets&#65289;&#30340;&#27010;&#29575;&#26694;&#26550;&#26469;&#23398;&#20064;&#20851;&#20110;&#36749;&#23398;&#25513;&#30721;&#30340;&#21518;&#39564;&#20998;&#24067;&#65292;&#29992;&#20110;&#20351;&#29992;&#30524;&#24213;&#22270;&#20687;&#23545;&#30524;&#37096;&#30142;&#30149;&#36827;&#34892;&#20998;&#31867;&#21644;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31283;&#20581;&#19988;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20197;ResNet18&#21644;ViT&#27169;&#22411;&#20026;&#20027;&#24178;&#30340;GFlowOut&#26469;&#35782;&#21035;&#21508;&#31181;&#30524;&#37096;&#29366;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16865v1 Announce Type: cross  Abstract: Ocular diseases, ranging from diabetic retinopathy to glaucoma, present a significant public health challenge due to their prevalence and potential for causing vision impairment. Early and accurate diagnosis is crucial for effective treatment and management.In recent years, deep learning models have emerged as powerful tools for analysing medical images, including ocular imaging . However, challenges persist in model interpretability and uncertainty estimation, which are critical for clinical decision-making. This study introduces a novel application of GFlowOut, leveraging the probabilistic framework of Generative Flow Networks (GFlowNets) to learn the posterior distribution over dropout masks, for the classification and analysis of ocular diseases using eye fundus images. We develop a robust and generalizable method that utilizes GFlowOut integrated with ResNet18 and ViT models as backbone in identifying various ocular conditions. Th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.13699</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#37327;&#23376;&#28857;&#22120;&#20214;&#27979;&#37327;&#20998;&#31867;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Explainable Classification Techniques for Quantum Dot Device Measurements
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13699
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25216;&#26415;&#65292;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23454;&#29616;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#20013;&#36739;&#39640;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29289;&#29702;&#31185;&#23398;&#20013;&#65292;&#23545;&#22270;&#20687;&#25968;&#25454;&#30340;&#31283;&#20581;&#29305;&#24449;&#34920;&#31034;&#38656;&#27714;&#22686;&#21152;&#65306;&#22270;&#20687;&#37319;&#38598;&#65292;&#22312;&#24191;&#20041;&#19978;&#25351;&#20108;&#32500;&#25968;&#25454;&#65292;&#29616;&#22312;&#22312;&#35768;&#22810;&#39046;&#22495;&#24191;&#27867;&#24212;&#29992;&#65292;&#21253;&#25324;&#25105;&#20204;&#22312;&#27492;&#32771;&#34385;&#30340;&#37327;&#23376;&#20449;&#24687;&#31185;&#23398;&#12290;&#34429;&#28982;&#22312;&#36825;&#20123;&#24773;&#20917;&#19979;&#24191;&#27867;&#20351;&#29992;&#20256;&#32479;&#22270;&#20687;&#29305;&#24449;&#65292;&#20294;&#23427;&#20204;&#30340;&#20351;&#29992;&#27491;&#22312;&#36805;&#36895;&#34987;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#25152;&#21462;&#20195;&#65292;&#21518;&#32773;&#24448;&#24448;&#20197;&#29306;&#29298;&#21487;&#35299;&#37322;&#24615;&#20026;&#20195;&#20215;&#25442;&#21462;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#31181;&#26435;&#34913;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#25104;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#20135;&#29983;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#21033;&#29992;&#21487;&#35299;&#37322;&#24615;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#23637;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#25552;&#20379;&#20102;&#21331;&#36234;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#19981;&#20250;&#38477;&#20302;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#37327;&#23376;&#28857;&#35843;&#35856;&#30340;&#32972;&#26223;&#19979;&#65292;&#36825;&#31181;&#25216;&#26415;&#24102;&#26469;&#20102;&#23454;&#36136;&#24615;&#30340;&#30410;&#22788;&#65292;&#24403;&#21069;&#21457;&#23637;&#38454;&#27573;&#38656;&#35201;&#20154;&#31867;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13699v1 Announce Type: cross  Abstract: In the physical sciences, there is an increased need for robust feature representations of image data: image acquisition, in the generalized sense of two-dimensional data, is now widespread across a large number of fields, including quantum information science, which we consider here. While traditional image features are widely utilized in such cases, their use is rapidly being supplanted by Neural Network-based techniques that often sacrifice explainability in exchange for high accuracy. To ameliorate this trade-off, we propose a synthetic data-based technique that results in explainable features. We show, using Explainable Boosting Machines (EBMs), that this method offers superior explainability without sacrificing accuracy. Specifically, we show that there is a meaningful benefit to this technique in the context of quantum dot tuning, where human intervention is necessary at the current stage of development.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;"&#27169;&#25311;&#21463;&#25511;&#32452;&#35013;"&#26041;&#27861;&#65292;&#25104;&#21151;&#35774;&#35745;&#20986;&#22810;&#31181;&#31283;&#23450;&#30340;2D Skyrm&#31163;&#23376;&#36229;&#26448;&#26009;&#12290;</title><link>https://arxiv.org/abs/2402.10874</link><description>&lt;p&gt;
&#36890;&#36807;&#21463;&#25511;&#32452;&#35013;&#35774;&#35745;2D Skyrm&#31163;&#23376;&#36229;&#26448;&#26009;
&lt;/p&gt;
&lt;p&gt;
Design of 2D Skyrmionic Metamaterial Through Controlled Assembly
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10874
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;"&#27169;&#25311;&#21463;&#25511;&#32452;&#35013;"&#26041;&#27861;&#65292;&#25104;&#21151;&#35774;&#35745;&#20986;&#22810;&#31181;&#31283;&#23450;&#30340;2D Skyrm&#31163;&#23376;&#36229;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#30913;&#24615;Skyrmion&#21644;&#21453;Skyrmion&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#65292;&#21363;&#22914;&#20309;&#21046;&#36896;&#20855;&#26377;&#19981;&#21516;&#29978;&#33267;&#23450;&#21046;&#25299;&#25169;&#32467;&#26500;&#30340;&#38750;&#24179;&#20961;&#39640;&#38454;Skyrm&#31163;&#23376;&#32441;&#29702;&#12290;&#25105;&#20204;&#36890;&#36807;&#38598;&#20013;&#22312;&#21333;&#23618;&#34180;&#33180;&#20869;Skyrmion&#36229;&#26448;&#26009;&#30340;&#26500;&#24314;&#36884;&#24452;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#31283;&#23450;&#30340;&#32593;&#26684;&#29366;&#12289;&#34180;&#29255;&#29366;&#21644;&#32454;&#32990;&#29366;Skyrm&#31163;&#23376;&#36229;&#26448;&#26009;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#8220;&#27169;&#25311;&#21463;&#25511;&#32452;&#35013;&#8221;&#27010;&#24565;&#65292;&#31616;&#32780;&#35328;&#20043;&#65292;&#36825;&#26159;&#21463;&#8220;&#28857;&#20987;&#21270;&#23398;&#8221;&#21551;&#21457;&#30340;&#19968;&#31181;&#21327;&#35758;&#65292;&#20801;&#35768;&#22312;&#21916;&#27426;&#30340;&#20301;&#32622;&#25918;&#32622;&#25299;&#25169;&#30913;&#32467;&#26500;&#65292;&#28982;&#21518;&#36890;&#36807;&#33021;&#37327;&#26368;&#23567;&#21270;&#26469;&#38416;&#26126;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10874v1 Announce Type: cross  Abstract: Despite extensive research on magnetic skyrmions and antiskyrmions, a significant challenge remains in crafting nontrivial high-order skyrmionic textures with varying, or even tailor-made, topologies. We address this challenge, by focusing on a construction pathway of skyrmionics metamaterial within a monolayer thin film and suggest several promising lattice-like, flakes-like, and cell-like skyrmionic metamaterials that are surprisingly stable. Central to our approach is the concept of 'simulated controlled assembly', in short, a protocol inspired by 'click chemistry' that allows for positioning topological magnetic structures where one likes, and then allowing for energy minimization to elucidate the stability. Utilizing high-throughput atomistic-spin-dynamic (ASD) simulations alongside state-of-the-art AI-driven tools, we have isolated skyrmions (topological charge Q=1), antiskyrmions (Q=-1), and skyrmionium (Q=0). These entities ser
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25506;&#32034;-&#20877;&#30830;&#23450;&#31639;&#27861;&#21644;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#20197;&#21450;&#35880;&#24910;&#36873;&#25321;&#32622;&#20449;&#21306;&#38388;&#30340;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#26102;&#38388;&#30028;&#36275;&#22815;&#22823;&#26102;&#65292;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#39069;&#22806;&#20195;&#20215;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.07391</link><description>&lt;p&gt;
&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#65292;&#21487;&#22797;&#21046;&#24615;&#28176;&#36827;&#33258;&#30001;
&lt;/p&gt;
&lt;p&gt;
Replicability is Asymptotically Free in Multi-armed Bandits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07391
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25506;&#32034;-&#20877;&#30830;&#23450;&#31639;&#27861;&#21644;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#20197;&#21450;&#35880;&#24910;&#36873;&#25321;&#32622;&#20449;&#21306;&#38388;&#30340;&#24133;&#24230;&#65292;&#23454;&#29616;&#20102;&#21487;&#22797;&#21046;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#26102;&#38388;&#30028;&#36275;&#22815;&#22823;&#26102;&#65292;&#21487;&#22797;&#21046;&#31639;&#27861;&#30340;&#39069;&#22806;&#20195;&#20215;&#26159;&#19981;&#24517;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21463;&#21487;&#22797;&#21046;&#30340;&#26426;&#22120;&#23398;&#20064;&#38656;&#27714;&#30340;&#25512;&#21160;&#65292;&#30740;&#31350;&#20102;&#38543;&#26426;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#30830;&#20445;&#31639;&#27861;&#30340;&#25805;&#20316;&#24207;&#21015;&#19981;&#21463;&#25968;&#25454;&#38598;&#22266;&#26377;&#38543;&#26426;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#29616;&#26377;&#31639;&#27861;&#25152;&#38656;&#30340;&#36951;&#25022;&#20540;&#27604;&#19981;&#21487;&#22797;&#21046;&#31639;&#27861;&#22810;$O(1/\rho^2)$&#20493;&#65292;&#20854;&#20013;$\rho$&#26159;&#38750;&#22797;&#21046;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#32473;&#23450;&#30340;$\rho$&#19979;&#26102;&#38388;&#30028;$T$&#36275;&#22815;&#22823;&#26102;&#65292;&#27492;&#39069;&#22806;&#20195;&#20215;&#26159;&#19981;&#24517;&#35201;&#30340;&#65292;&#21069;&#25552;&#26159;&#35880;&#24910;&#36873;&#25321;&#32622;&#20449;&#21306;&#38388;&#30340;&#24133;&#24230;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20808;&#25506;&#32034;&#21518;&#20915;&#31574;&#30340;&#31639;&#27861;&#65292;&#22312;&#20915;&#31574;&#20043;&#21069;&#22343;&#21248;&#36873;&#25321;&#21160;&#20316;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#36830;&#32493;&#28120;&#27760;&#31639;&#27861;&#65292;&#22312;&#27599;&#20010;&#38454;&#27573;&#32467;&#26463;&#26102;&#28120;&#27760;&#27425;&#20248;&#21160;&#20316;&#12290;&#20026;&#20102;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25105;&#20204;&#23558;&#38543;&#26426;&#24615;&#24341;&#20837;&#20915;&#31574;&#21046;&#23450;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work is motivated by the growing demand for reproducible machine learning. We study the stochastic multi-armed bandit problem. In particular, we consider a replicable algorithm that ensures, with high probability, that the algorithm's sequence of actions is not affected by the randomness inherent in the dataset. We observe that existing algorithms require $O(1/\rho^2)$ times more regret than nonreplicable algorithms, where $\rho$ is the level of nonreplication. However, we demonstrate that this additional cost is unnecessary when the time horizon $T$ is sufficiently large for a given $\rho$, provided that the magnitude of the confidence bounds is chosen carefully. We introduce an explore-then-commit algorithm that draws arms uniformly before committing to a single arm. Additionally, we examine a successive elimination algorithm that eliminates suboptimal arms at the end of each phase. To ensure the replicability of these algorithms, we incorporate randomness into their decision-ma
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;ProFITi&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#26465;&#20214;&#19979;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#21487;&#36870;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.06293</link><description>&lt;p&gt;
&#36890;&#36807;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Forecasting of Irregular Time Series via Conditional Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06293
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26465;&#20214;&#27969;&#36827;&#34892;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#30340;&#27010;&#29575;&#39044;&#27979;&#30340;&#26032;&#27169;&#22411;ProFITi&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#26465;&#20214;&#19979;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#39044;&#27979;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#36890;&#36807;&#24341;&#20837;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#21487;&#36870;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#35813;&#27169;&#22411;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#35268;&#21017;&#37319;&#26679;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#27010;&#29575;&#39044;&#27979;&#26159;&#35768;&#22810;&#39046;&#22495;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#21253;&#25324;&#21307;&#30103;&#20445;&#20581;&#12289;&#22825;&#25991;&#23398;&#21644;&#27668;&#20505;&#23398;&#12290;&#30446;&#21069;&#35813;&#20219;&#21153;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#20165;&#20272;&#35745;&#21333;&#20010;&#36890;&#36947;&#21644;&#21333;&#20010;&#26102;&#38388;&#28857;&#19978;&#35266;&#27979;&#20540;&#30340;&#36793;&#38469;&#20998;&#24067;&#65292;&#20551;&#35774;&#20102;&#19968;&#20010;&#22266;&#23450;&#24418;&#29366;&#30340;&#21442;&#25968;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;ProFITi&#65292;&#29992;&#20110;&#20351;&#29992;&#26465;&#20214;&#24402;&#19968;&#21270;&#27969;&#23545;&#20855;&#26377;&#32570;&#22833;&#20540;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#27010;&#29575;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#23398;&#20064;&#20102;&#22312;&#36807;&#21435;&#35266;&#27979;&#21644;&#26597;&#35810;&#30340;&#36890;&#36947;&#21644;&#26102;&#38388;&#19978;&#26465;&#20214;&#19979;&#26102;&#38388;&#24207;&#21015;&#26410;&#26469;&#20540;&#30340;&#32852;&#21512;&#20998;&#24067;&#65292;&#32780;&#19981;&#20551;&#35774;&#24213;&#23618;&#20998;&#24067;&#30340;&#22266;&#23450;&#24418;&#29366;&#12290;&#20316;&#20026;&#27169;&#22411;&#32452;&#20214;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#19977;&#35282;&#24418;&#27880;&#24847;&#21147;&#23618;&#21644;&#19968;&#20010;&#21487;&#36870;&#30340;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#22312;&#25972;&#20010;&#23454;&#25968;&#32447;&#19978;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#27169;&#22411;&#30340;&#25552;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic forecasting of irregularly sampled multivariate time series with missing values is an important problem in many fields, including health care, astronomy, and climate. State-of-the-art methods for the task estimate only marginal distributions of observations in single channels and at single timepoints, assuming a fixed-shape parametric distribution. In this work, we propose a novel model, ProFITi, for probabilistic forecasting of irregularly sampled time series with missing values using conditional normalizing flows. The model learns joint distributions over the future values of the time series conditioned on past observations and queried channels and times, without assuming any fixed shape of the underlying distribution. As model components, we introduce a novel invertible triangular attention layer and an invertible non-linear activation function on and onto the whole real line. We conduct extensive experiments on four datasets and demonstrate that the proposed model pro
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;UNICORN&#65292;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.02429</link><description>&lt;p&gt;
&#26397;&#30528;&#22522;&#20110;&#20449;&#24687;&#29702;&#35770;&#30340;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Towards an Information Theoretic Framework of Context-Based Offline Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#23558;&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19981;&#21516;&#26041;&#27861;&#25972;&#21512;&#36215;&#26469;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;UNICORN&#65292;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;OMRL&#65289;&#20316;&#20026;&#31163;&#32447;RL&#21644;&#20803;RL&#30340;&#32467;&#21512;&#65292;&#22312;&#23454;&#29616;RL&#26234;&#33021;&#20307;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#21644;&#24555;&#36895;&#36866;&#24212;&#20197;&#21450;&#23433;&#20840;&#33719;&#21462;&#30693;&#35782;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#20854;&#20013;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;OMRL&#65288;COMRL&#65289;&#20316;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#33539;&#24335;&#65292;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#22522;&#20110;&#26377;&#25928;&#20219;&#21153;&#34920;&#31034;&#30340;&#36890;&#29992;&#31574;&#30053;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#36890;&#36807;&#30740;&#31350;COMRL&#39046;&#22495;&#30340;&#20960;&#20010;&#20851;&#38190;&#37324;&#31243;&#30865;&#65292;&#25105;&#20204;&#25552;&#35758;&#23558;&#36825;&#20123;&#30475;&#20284;&#29420;&#31435;&#30340;&#26041;&#27861;&#25972;&#21512;&#21040;&#19968;&#20010;&#32479;&#19968;&#30340;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#20013;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29616;&#26377;&#30340;COMRL&#31639;&#27861;&#26412;&#36136;&#19978;&#26159;&#36890;&#36807;&#23454;&#29616;&#21508;&#31181;&#36817;&#20284;&#30028;&#38480;&#26469;&#20248;&#21270;&#20219;&#21153;&#21464;&#37327;$\boldsymbol{M}$&#21644;&#20854;&#28508;&#22312;&#34920;&#31034;$\boldsymbol{Z}$&#20043;&#38388;&#30340;&#30456;&#20114;&#20449;&#24687;&#30446;&#26631;&#12290;&#22522;&#20110;&#29702;&#35770;&#27934;&#23519;&#21147;&#21644;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;UNICORN&#65292;&#23637;&#29616;&#20102;&#22312;&#24191;&#27867;&#30340;R&#38382;&#39064;&#35889;&#19978;&#30340;&#26174;&#33879;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As a marriage between offline RL and meta-RL, the advent of offline meta-reinforcement learning (OMRL) has shown great promise in enabling RL agents to multi-task and quickly adapt while acquiring knowledge safely. Among which, Context-based OMRL (COMRL) as a popular paradigm, aims to learn a universal policy conditioned on effective task representations. In this work, by examining several key milestones in the field of COMRL, we propose to integrate these seemingly independent methodologies into a unified information theoretic framework. Most importantly, we show that the pre-existing COMRL algorithms are essentially optimizing the same mutual information objective between the task variable $\boldsymbol{M}$ and its latent representation $\boldsymbol{Z}$ by implementing various approximate bounds. Based on the theoretical insight and the information bottleneck principle, we arrive at a novel algorithm dubbed UNICORN, which exhibits remarkable generalization across a broad spectrum of R
&lt;/p&gt;</description></item><item><title>SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2401.11791</link><description>&lt;p&gt;
SemPLeS: &#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#29992;&#20110;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SemPLeS: Semantic Prompt Learning for Weakly-Supervised Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11791
&lt;/p&gt;
&lt;p&gt;
SemPLeS&#26694;&#26550;&#21033;&#29992;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23398;&#20064;&#26377;&#25928;&#25552;&#31034;&#26469;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#65288;WSSS&#65289;&#26088;&#22312;&#21033;&#29992;&#20165;&#20855;&#26377;&#22270;&#20687;&#32423;&#30417;&#30563;&#30340;&#22270;&#20687;&#25968;&#25454;&#26469;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#12290;&#30001;&#20110;&#26080;&#27861;&#33719;&#24471;&#31934;&#30830;&#30340;&#20687;&#32032;&#32423;&#26631;&#27880;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#20391;&#37325;&#20110;&#36890;&#36807;&#20248;&#21270;CAM&#26679;&#24335;&#30340;&#28909;&#22270;&#26469;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#21106;&#27169;&#22411;&#30340;&#20266;&#26631;&#35760;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#28909;&#22270;&#21487;&#33021;&#20165;&#25429;&#33719;&#23545;&#35937;&#31867;&#21035;&#30340;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#22270;&#20687;&#21306;&#22495;&#25110;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;WSSS&#30340;&#35821;&#20041;&#25552;&#31034;&#23398;&#20064;&#65288;SemPLeS&#65289;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#26377;&#25928;&#22320;&#25552;&#31034;CLIP&#28508;&#31354;&#38388;&#20197;&#22686;&#24378;&#20998;&#21106;&#21306;&#22495;&#19982;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#20043;&#38388;&#30340;&#35821;&#20041;&#23545;&#20934;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25552;&#31034;&#23398;&#20064;&#21644;&#25552;&#31034;&#24341;&#23548;&#30340;&#35821;&#20041;&#32454;&#21270;&#65292;&#20197;&#23398;&#20064;&#36866;&#24403;&#25551;&#36848;&#21644;&#25233;&#21046;&#19982;&#27599;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30456;&#20851;&#30340;&#20849;&#21516;&#20986;&#29616;&#30340;&#32972;&#26223;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11791v2 Announce Type: replace-cross  Abstract: Weakly-Supervised Semantic Segmentation (WSSS) aims to train segmentation models using image data with only image-level supervision. Since precise pixel-level annotations are not accessible, existing methods typically focus on producing pseudo masks for training segmentation models by refining CAM-like heatmaps. However, the produced heatmaps may capture only the discriminative image regions of object categories or the associated co-occurring backgrounds. To address the issues, we propose a Semantic Prompt Learning for WSSS (SemPLeS) framework, which learns to effectively prompt the CLIP latent space to enhance the semantic alignment between the segmented regions and the target object categories. More specifically, we propose Contrastive Prompt Learning and Prompt-guided Semantic Refinement to learn the prompts that adequately describe and suppress the co-occurring backgrounds associated with each target object category. In thi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#37327;&#23376;&#31995;&#32479;&#22522;&#24577;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#38271;&#31243;&#21644;&#31561;&#21464;&#29305;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20998;&#23376;&#21644;&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#25351;&#25968;&#32423;&#22797;&#26434;&#24230;&#30340;&#35823;&#24046;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.17019</link><description>&lt;p&gt;
&#39640;&#25928;&#23398;&#20064;&#38271;&#31243;&#21644;&#31561;&#21464;&#37327;&#37327;&#23376;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Long-Range and Equivariant Quantum Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#37327;&#23376;&#31995;&#32479;&#22522;&#24577;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#38271;&#31243;&#21644;&#31561;&#21464;&#29305;&#24615;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#29616;&#26377;&#32467;&#26524;&#65292;&#20351;&#20854;&#36866;&#29992;&#20110;&#20998;&#23376;&#21644;&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#25552;&#20379;&#20102;&#20855;&#26377;&#25351;&#25968;&#32423;&#22797;&#26434;&#24230;&#30340;&#35823;&#24046;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#37327;&#23376;&#22810;&#20307;&#29289;&#29702;&#23398;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;-&#25214;&#21040;&#21644;&#23398;&#20064;&#37327;&#23376;&#21704;&#23494;&#39039;&#37327;&#30340;&#22522;&#24577;&#21450;&#20854;&#24615;&#36136;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#25968;&#25454;&#26469;&#39044;&#27979;&#20960;&#20309;&#23616;&#37096;&#21487;&#35266;&#27979;&#37327;&#30340;&#22522;&#24577;&#26399;&#26395;&#20540;&#30340;&#20219;&#21153;&#12290;&#23545;&#20110;&#30701;&#31243;&#32570;&#38519;&#21704;&#23494;&#39039;&#37327;&#65292;&#24471;&#21040;&#20102;&#26679;&#26412;&#22797;&#26434;&#24230;&#22312;&#37327;&#23376;&#20301;&#25968;&#30340;&#23545;&#25968;&#21644;&#35823;&#24046;&#30340;&#20934;&#22810;&#39033;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23558;&#36825;&#20123;&#32467;&#26524;&#25193;&#23637;&#21040;&#36229;&#20986;&#21704;&#23494;&#39039;&#37327;&#21644;&#35266;&#27979;&#37327;&#30340;&#23616;&#37096;&#35201;&#27714;&#65292;&#36825;&#26159;&#30001;&#20998;&#23376;&#21644;&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#38271;&#31243;&#30456;&#20114;&#20316;&#29992;&#30340;&#30456;&#20851;&#24615;&#25152;&#39537;&#21160;&#30340;&#12290;&#23545;&#20110;&#25351;&#25968;&#22823;&#20110;&#31995;&#32479;&#32500;&#25968;&#20004;&#20493;&#30340;&#24130;&#24459;&#34928;&#20943;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#30456;&#21516;&#30340;&#39640;&#25928;&#23545;&#25968;&#26631;&#24230;&#20851;&#20110;&#37327;&#23376;&#20301;&#25968;&#30340;&#20381;&#36182;&#24615;&#65292;&#20294;&#35823;&#24046;&#30340;&#20381;&#36182;&#24615;&#24694;&#21270;&#21040;&#20102;&#25351;&#25968;&#32423;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#30456;&#20114;&#20316;&#29992;&#36229;&#22270;&#30340;&#33258;&#21516;&#26500;&#32676;&#19979;&#31561;&#21464;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we consider a fundamental task in quantum many-body physics - finding and learning ground states of quantum Hamiltonians and their properties. Recent works have studied the task of predicting the ground state expectation value of sums of geometrically local observables by learning from data. For short-range gapped Hamiltonians, a sample complexity that is logarithmic in the number of qubits and quasipolynomial in the error was obtained. Here we extend these results beyond the local requirements on both Hamiltonians and observables, motivated by the relevance of long-range interactions in molecular and atomic systems. For interactions decaying as a power law with exponent greater than twice the dimension of the system, we recover the same efficient logarithmic scaling with respect to the number of qubits, but the dependence on the error worsens to exponential. Further, we show that learning algorithms equivariant under the automorphism group of the interaction hypergraph a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.09982</link><description>&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
ACPO: AI-Enabled Compiler-Driven Program Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.09982
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#26694;&#26550;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#32473;LLVM&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#23454;&#29616;&#32534;&#35793;&#22120;&#39537;&#21160;&#30340;&#31243;&#24207;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ACPO&#65306;AI-Enabled Compiler-driven Program Optimization&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#20026;LLVM&#25552;&#20379;&#31616;&#21333;&#20840;&#38754;&#30340;&#24037;&#20855;&#65292;&#20197;&#20174;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#36827;&#34892;&#19981;&#21516;&#30340;&#20248;&#21270;&#36890;&#36335;&#20013;&#33719;&#30410;&#12290;&#39318;&#20808;&#23637;&#31034;&#20102;ACPO&#30340;&#39640;&#23618;&#35270;&#22270;&#12289;&#31867;&#23618;&#27425;&#32467;&#26500;&#21644;&#21151;&#33021;&#65292;&#28982;&#21518;&#36890;&#36807;&#23558;&#24490;&#29615;&#23637;&#24320;&#21644;&#20989;&#25968;&#20869;&#32852;&#20256;&#36882;&#30340;ML&#20351;&#33021;&#21270;&#65292;&#23637;&#31034;&#20102;ACPO&#30340;&#19968;&#20123;&#29992;&#20363;&#65292;&#25551;&#36848;&#20102;ACPO&#22914;&#20309;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.09982v2 Announce Type: replace-cross  Abstract: The key to performance optimization of a program is to decide correctly when a certain transformation should be applied by a compiler. This is an ideal opportunity to apply machine-learning models to speed up the tuning process; while this realization has been around since the late 90s, only recent advancements in ML enabled a practical application of ML to compilers as an end-to-end framework.   This paper presents ACPO: \textbf{\underline{A}}I-Enabled \textbf{\underline{C}}ompiler-driven \textbf{\underline{P}}rogram \textbf{\underline{O}}ptimization; a novel framework to provide LLVM with simple and comprehensive tools to benefit from employing ML models for different optimization passes. We first showcase the high-level view, class hierarchy, and functionalities of ACPO and subsequently, demonstrate a couple of use cases of ACPO by ML-enabling the Loop Unroll and Function Inlining passes and describe how ACPO can be leverage
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2401.10895</link><description>&lt;p&gt;
&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19968;&#39033;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
AI in Supply Chain Risk Assessment: A Systematic Literature Review and Bibliometric Analysis. (arXiv:2401.10895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#21644;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#65292;&#22635;&#34917;&#20102;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;&#20013;&#26032;&#20852;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#30740;&#31350;&#31354;&#30333;&#65292;&#20026;&#20102;&#35299;&#36825;&#20123;&#25216;&#26415;&#22312;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#25552;&#20379;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20379;&#24212;&#38142;&#39118;&#38505;&#35780;&#20272;(SCRA)&#32463;&#21382;&#20102;&#28145;&#21051;&#30340;&#28436;&#21464;&#65292;&#38761;&#26032;&#20102;&#39044;&#27979;&#33021;&#21147;&#21644;&#39118;&#38505;&#32531;&#35299;&#31574;&#30053;&#12290;&#36825;&#31181;&#28436;&#21464;&#30340;&#37325;&#35201;&#24615;&#22312;&#20110;&#22312;&#29616;&#20195;&#20379;&#24212;&#38142;&#20013;&#30830;&#20445;&#36816;&#33829;&#30340;&#38887;&#24615;&#21644;&#36830;&#32493;&#24615;&#65292;&#38656;&#35201;&#31283;&#20581;&#30340;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#12290;&#20197;&#24448;&#30340;&#32508;&#36848;&#24050;&#32463;&#27010;&#36848;&#20102;&#24050;&#24314;&#31435;&#30340;&#26041;&#27861;&#65292;&#20294;&#24573;&#35270;&#20102;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#22312;&#29702;&#35299;&#20854;&#22312;SCRA&#20013;&#30340;&#23454;&#38469;&#24433;&#21709;&#26041;&#38754;&#23384;&#22312;&#26126;&#26174;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#26412;&#25991;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24182;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;&#25991;&#29486;&#35745;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#20180;&#32454;&#30740;&#31350;&#20102;1717&#31687;&#35770;&#25991;&#65292;&#24182;&#20174;2014&#24180;&#33267;2023&#24180;&#20043;&#38388;&#21457;&#34920;&#30340;48&#31687;&#25991;&#31456;&#20013;&#33719;&#24471;&#20102;&#20851;&#38190;&#35265;&#35299;&#12290;&#35813;&#32508;&#36848;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#36890;&#36807;&#22238;&#31572;&#20851;&#38190;&#30740;&#31350;&#38382;&#39064;&#65292;&#25506;&#31350;&#20102;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#12289;&#26041;&#27861;&#35770;&#12289;&#30740;&#31350;&#32467;&#26524;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supply chain risk assessment (SCRA) has witnessed a profound evolution through the integration of artificial intelligence (AI) and machine learning (ML) techniques, revolutionizing predictive capabilities and risk mitigation strategies. The significance of this evolution stems from the critical role of robust risk management strategies in ensuring operational resilience and continuity within modern supply chains. Previous reviews have outlined established methodologies but have overlooked emerging AI/ML techniques, leaving a notable research gap in understanding their practical implications within SCRA. This paper conducts a systematic literature review combined with a comprehensive bibliometric analysis. We meticulously examined 1,717 papers and derived key insights from a select group of 48 articles published between 2014 and 2023. The review fills this research gap by addressing pivotal research questions, and exploring existing AI/ML techniques, methodologies, findings, and future 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#25512;&#24191;&#21040;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#65292;&#21516;&#26102;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2309.02422</link><description>&lt;p&gt;
&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#36935;&#19978;&#31070;&#32463;&#32593;&#32476;&#65306;Radon-Kolmogorov-Smirnov&#26816;&#39564;
&lt;/p&gt;
&lt;p&gt;
Maximum Mean Discrepancy Meets Neural Networks: The Radon-Kolmogorov-Smirnov Test. (arXiv:2309.02422v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02422
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#23558;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#26368;&#22823;&#21270;&#30340;&#38382;&#39064;&#25512;&#24191;&#21040;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#65292;&#21516;&#26102;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#22343;&#24046;&#30456;&#20284;&#24230;&#65288;MMD&#65289;&#26159;&#19968;&#31867;&#22522;&#20110;&#26368;&#22823;&#21270;&#20004;&#20010;&#20998;&#24067;$P$&#21644;$Q$&#20043;&#38388;&#26679;&#26412;&#22343;&#20540;&#24046;&#24322;&#30340;&#38750;&#21442;&#25968;&#21452;&#26679;&#26412;&#26816;&#39564;&#65292;&#20854;&#20013;&#32771;&#34385;&#20102;&#25152;&#26377;&#22312;&#26576;&#20010;&#20989;&#25968;&#31354;&#38388;$\mathcal{F}$&#20013;&#30340;&#25968;&#25454;&#21464;&#25442;$f$&#30340;&#36873;&#25321;&#12290;&#21463;&#21040;&#26368;&#36817;&#23558;&#25152;&#35859;&#30340;Radon&#26377;&#30028;&#21464;&#24046;&#20989;&#25968;&#65288;RBV&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#32852;&#31995;&#36215;&#26469;&#30340;&#24037;&#20316;&#30340;&#21551;&#21457;&#65288;Parhi&#21644;Nowak, 2021, 2023&#65289;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;$\mathcal{F}$&#21462;&#20026;&#32473;&#23450;&#24179;&#28369;&#24230;&#39034;&#24207;$k \geq 0$&#19979;&#30340;RBV&#31354;&#38388;&#20013;&#30340;&#21333;&#20301;&#29699;&#30340;MMD&#12290;&#36825;&#20010;&#26816;&#39564;&#34987;&#31216;&#20026;Radon-Kolmogorov-Smirnov&#65288;RKS&#65289;&#26816;&#39564;&#65292;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#22810;&#32500;&#31354;&#38388;&#21644;&#26356;&#39640;&#24179;&#28369;&#24230;&#39034;&#24207;&#30340;&#32463;&#20856;Kolmogorov-Smirnov&#65288;KS&#65289;&#26816;&#39564;&#30340;&#19968;&#33324;&#21270;&#12290;&#23427;&#36824;&#19982;&#31070;&#32463;&#32593;&#32476;&#23494;&#20999;&#30456;&#20851;&#65306;&#25105;&#20204;&#35777;&#26126;RKS&#26816;&#39564;&#20013;&#30340;&#35777;&#25454;&#20989;&#25968;$f$&#65292;&#21363;&#36798;&#21040;&#26368;&#22823;&#22343;&#24046;&#30340;&#20989;&#25968;&#65292;&#24635;&#26159;&#19968;&#20010;&#20108;&#27425;&#26679;&#26465;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Maximum mean discrepancy (MMD) refers to a general class of nonparametric two-sample tests that are based on maximizing the mean difference over samples from one distribution $P$ versus another $Q$, over all choices of data transformations $f$ living in some function space $\mathcal{F}$. Inspired by recent work that connects what are known as functions of $\textit{Radon bounded variation}$ (RBV) and neural networks (Parhi and Nowak, 2021, 2023), we study the MMD defined by taking $\mathcal{F}$ to be the unit ball in the RBV space of a given smoothness order $k \geq 0$. This test, which we refer to as the $\textit{Radon-Kolmogorov-Smirnov}$ (RKS) test, can be viewed as a generalization of the well-known and classical Kolmogorov-Smirnov (KS) test to multiple dimensions and higher orders of smoothness. It is also intimately connected to neural networks: we prove that the witness in the RKS test -- the function $f$ achieving the maximum mean difference -- is always a ridge spline of degree
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2306.11207</link><description>&lt;p&gt;
Quilt-1M: &#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Quilt-1M: One Million Image-Text Pairs for Histopathology. (arXiv:2306.11207v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026; Quilt-1M &#30340;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21644;&#25991;&#23383;&#23545;&#30340;&#30334;&#19975;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992; YouTube &#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#20026;&#20027;&#35201;&#26469;&#28304;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20351;&#24471;&#30284;&#30151;&#32452;&#32455;&#23398;&#39046;&#22495;&#30340;&#34920;&#24449;&#23398;&#20064;&#21462;&#24471;&#31867;&#20284;&#20110;&#20854;&#20182;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#24212;&#29992;&#30340;&#21152;&#36895;&#20351;&#24471;&#22312;&#32447;&#22270;&#20687;&#21644;&#25991;&#23383;&#25968;&#25454;&#22823;&#37327;&#28044;&#29616;&#65292;&#20294;&#21307;&#23398;&#39046;&#22495;&#65288;&#29305;&#21035;&#26159;&#30284;&#30151;&#32452;&#32455;&#23398;&#65289;&#31867;&#20284;&#30340;&#25968;&#25454;&#21364;&#24456;&#31232;&#23569;&#65292;&#36825;&#38459;&#30861;&#20102;&#21307;&#23398;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#26412;&#25991;&#21033;&#29992;YouTube&#19978;&#30340;&#19987;&#23478;&#21307;&#29983;&#25945;&#31243;&#35270;&#39057;&#65292;&#20174;&#20013;&#36873;&#25321;&#20102; 1,087 &#23567;&#26102;&#30340;&#21307;&#23398;&#32452;&#32455;&#23398;&#35270;&#39057;&#65292;&#20197;&#27492;&#33258;&#21160;&#31579;&#36873;&#20986;&#20849;&#21253;&#21547; 768,826 &#20010;&#30284;&#30151;&#32452;&#32455;&#23398;&#22270;&#20687;&#21450;&#20854;&#23545;&#24212;&#30340;&#25991;&#23383;&#23545;&#30340; Quilt &#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has halted comparable progress. To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians. From YouTube, we curate Quilt: a large-scale vision-language dataset consisting of $768,826$ image and text pairs. Quilt was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition. In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples. We combine Quilt with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;</title><link>http://arxiv.org/abs/2306.07886</link><description>&lt;p&gt;
&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#19982;&#20020;&#30028;&#28857;
&lt;/p&gt;
&lt;p&gt;
Symmetry &amp; Critical Points for Symmetric Tensor Decompositions Problems. (arXiv:2306.07886v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24471;&#21040;&#20102;&#31934;&#30830;&#30340;&#20998;&#26512;&#20272;&#35745;&#65292;&#24182;&#21457;&#29616;&#20102;&#21508;&#31181;&#38459;&#30861;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20960;&#20309;&#38556;&#30861;&#21644;&#30001;&#20110;&#23545;&#31216;&#24615;&#23548;&#33268;&#30340;&#20016;&#23500;&#30340;&#20020;&#30028;&#28857;&#38598;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#23558;&#19968;&#20010;&#23454;&#23545;&#31216;&#24352;&#37327;&#20998;&#35299;&#25104;&#31209;&#20026;1&#39033;&#20043;&#21644;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#23545;&#31216;&#32467;&#26500;&#65292;&#23548;&#20986;Puiseux&#32423;&#25968;&#34920;&#31034;&#30340;&#19968;&#31995;&#21015;&#20020;&#30028;&#28857;&#65292;&#24182;&#33719;&#24471;&#20102;&#20851;&#20110;&#20020;&#30028;&#20540;&#21644;Hessian&#35889;&#30340;&#31934;&#30830;&#20998;&#26512;&#20272;&#35745;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#21508;&#31181;&#20960;&#20309;&#38556;&#30861;&#65292;&#38459;&#30861;&#20102;&#23616;&#37096;&#20248;&#21270;&#26041;&#27861;&#30340;&#20351;&#29992;&#65292;&#26368;&#21518;&#65292;&#21033;&#29992;&#19968;&#20010;&#29275;&#39039;&#22810;&#38754;&#20307;&#35770;&#35777;&#20102;&#22266;&#23450;&#23545;&#31216;&#24615;&#30340;&#25152;&#26377;&#20020;&#30028;&#28857;&#30340;&#23436;&#20840;&#26522;&#20030;&#65292;&#24182;&#35777;&#26126;&#20102;&#19982;&#20840;&#23616;&#26368;&#23567;&#20540;&#30340;&#38598;&#21512;&#30456;&#27604;&#65292;&#30001;&#20110;&#23545;&#31216;&#24615;&#30340;&#23384;&#22312;&#65292;&#20020;&#30028;&#28857;&#30340;&#38598;&#21512;&#21487;&#33021;&#20250;&#26174;&#31034;&#20986;&#32452;&#21512;&#30340;&#20016;&#23500;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the non-convex optimization problem associated with the decomposition of a real symmetric tensor into a sum of rank one terms. Use is made of the rich symmetry structure to derive Puiseux series representations of families of critical points, and so obtain precise analytic estimates on the critical values and the Hessian spectrum. The sharp results make possible an analytic characterization of various geometric obstructions to local optimization methods, revealing in particular a complex array of saddles and local minima which differ by their symmetry, structure and analytic properties. A desirable phenomenon, occurring for all critical points considered, concerns the index of a point, i.e., the number of negative Hessian eigenvalues, increasing with the value of the objective function. Lastly, a Newton polytope argument is used to give a complete enumeration of all critical points of fixed symmetry, and it is shown that contrarily to the set of global minima which remains 
&lt;/p&gt;</description></item><item><title>GFairHint&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#23398;&#20064;&#20844;&#24179;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#22270;&#23884;&#20837;&#36830;&#25509;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.15622</link><description>&lt;p&gt;
GFairHint&#65306;&#36890;&#36807;&#20844;&#24179;&#24615;&#25552;&#31034;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
GFairHint: Improving Individual Fairness for Graph Neural Networks via Fairness Hint. (arXiv:2305.15622v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15622
&lt;/p&gt;
&lt;p&gt;
GFairHint&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36741;&#21161;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#23398;&#20064;&#20844;&#24179;&#34920;&#31034;&#65292;&#24182;&#23558;&#20854;&#19982;&#21407;&#22987;&#22270;&#23884;&#20837;&#36830;&#25509;&#20197;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#21516;&#26102;&#19981;&#29306;&#29298;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#38382;&#39064;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#20197;&#21450;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#25968;&#25454;&#23398;&#20064;&#19978;&#30340;&#21331;&#36234;&#34920;&#29616;&#65292;GNN&#20013;&#30340;&#31639;&#27861;&#20844;&#24179;&#24615;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#34429;&#28982;&#35768;&#22810;&#29616;&#26377;&#30340;&#30740;&#31350;&#22312;&#32676;&#20307;&#23618;&#38754;&#19978;&#25913;&#21892;&#20102;&#20844;&#24179;&#24615;&#65292;&#20294;&#21482;&#26377;&#23569;&#25968;&#24037;&#20316;&#20419;&#36827;&#20102;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#36825;&#20351;&#24471;&#30456;&#20284;&#30340;&#20010;&#20307;&#20855;&#26377;&#30456;&#20284;&#30340;&#32467;&#26524;&#12290;&#20419;&#36827;&#20010;&#20307;&#20844;&#24179;&#24615;&#30340;&#29702;&#24819;&#26694;&#26550;&#24212;&#35813;&#65288;1&#65289;&#22312;&#20844;&#24179;&#24615;&#21644;&#24615;&#33021;&#20043;&#38388;&#24179;&#34913;&#65292;&#65288;2&#65289;&#36866;&#24212;&#20004;&#31181;&#24120;&#29992;&#30340;&#20010;&#20307;&#30456;&#20284;&#24615;&#24230;&#37327;&#65288;&#20174;&#22806;&#37096;&#27880;&#37322;&#21644;&#20174;&#36755;&#20837;&#29305;&#24449;&#35745;&#31639;&#65289;&#65292;&#65288;3&#65289;&#27178;&#36328;&#21508;&#31181;GNN&#27169;&#22411;&#36827;&#34892;&#25512;&#24191;&#65292;&#65288;4&#65289;&#20855;&#26377;&#39640;&#25928;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#20043;&#21069;&#30340;&#24037;&#20316;&#37117;&#27809;&#26377;&#23454;&#29616;&#25152;&#26377;&#30340;&#29702;&#24819;&#26465;&#20214;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;GFairHint&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#36741;&#21161;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#23398;&#20064;&#20844;&#24179;&#34920;&#31034;&#65292;&#28982;&#21518;&#23558;&#20854;&#19982;&#21407;&#22987;&#22270;&#23884;&#20837;&#36830;&#25509;&#20197;&#22686;&#24378;&#20010;&#20307;&#20844;&#24179;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;GFairHint&#22312;&#19981;&#29306;&#29298;&#22826;&#22810;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#20102;&#20010;&#20307;&#20844;&#24179;&#24615;&#65292;&#24182;&#19988;&#20248;&#20110;&#30446;&#21069;&#30340;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the growing concerns about fairness in machine learning and the impressive performance of Graph Neural Networks (GNNs) on graph data learning, algorithmic fairness in GNNs has attracted significant attention. While many existing studies improve fairness at the group level, only a few works promote individual fairness, which renders similar outcomes for similar individuals. A desirable framework that promotes individual fairness should (1) balance between fairness and performance, (2) accommodate two commonly-used individual similarity measures (externally annotated and computed from input features), (3) generalize across various GNN models, and (4) be computationally efficient. Unfortunately, none of the prior work achieves all the desirables. In this work, we propose a novel method, GFairHint, which promotes individual fairness in GNNs and achieves all aforementioned desirables. GFairHint learns fairness representations through an auxiliary link prediction task, and then concate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2305.03123</link><description>&lt;p&gt;
ChatGPT &#38656;&#35201;&#36827;&#34892;SPADE&#65288;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65289;&#35780;&#20272;&#65306;&#19968;&#39033;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Needs SPADE (Sustainability, PrivAcy, Digital divide, and Ethics) Evaluation: A Review. (arXiv:2305.03123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03123
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20851;&#27880;ChatGPT&#38754;&#20020;&#30340;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;SPADE&#35780;&#20272;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#32473;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#21478;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#30001;&#20110;&#20854;&#24615;&#33021;&#21644;&#26377;&#25928;&#30340;&#23545;&#35805;&#33021;&#21147;&#65292;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#20013;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#20851;&#27880;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#30740;&#31350;&#24050;&#32463;&#21457;&#34920;&#65292;&#20197;&#23637;&#31034;ChatGPT&#21644;&#20854;&#20182;LLMs&#30340;&#26377;&#25928;&#24615;&#12289;&#25928;&#29575;&#12289;&#38598;&#25104;&#21644;&#24773;&#24863;&#12290;&#30456;&#21453;&#65292;&#26412;&#30740;&#31350;&#20851;&#27880;&#30340;&#26159;&#22823;&#22810;&#25968;&#34987;&#24573;&#35270;&#30340;&#37325;&#35201;&#26041;&#38754;&#65292;&#21363;&#21487;&#25345;&#32493;&#24615;&#12289;&#38544;&#31169;&#12289;&#25968;&#23383;&#40511;&#27807;&#21644;&#20262;&#29702;&#65292;&#24182;&#24314;&#35758;&#19981;&#20165;&#20165;&#26159;ChatGPT&#65292;&#32780;&#26159;&#22312;&#23545;&#35805;&#26426;&#22120;&#20154;&#31867;&#21035;&#20013;&#30340;&#27599;&#19968;&#20010;&#21518;&#32493;&#20837;&#21475;&#37117;&#24212;&#35813;&#36827;&#34892;SPADE&#35780;&#20272;&#12290;&#26412;&#25991;&#35814;&#32454;&#35752;&#35770;&#20102;&#20851;&#20110;ChatGPT&#30340;&#38382;&#39064;&#21644;&#20851;&#27880;&#28857;&#19982;&#19978;&#36848;&#29305;&#24449;&#19968;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#20123;&#21021;&#27493;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#21487;&#35270;&#21270;&#20197;&#21450;&#20551;&#35774;&#30340;&#20107;&#23454;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#20551;&#35774;&#12290;&#25105;&#20204;&#36824;&#20026;&#27599;&#20010;&#38382;&#39064;&#25552;&#20986;&#20102;&#32531;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20123;&#26410;&#26469;&#26041;&#21521;&#21644;&#24320;&#25918;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is another large language model (LLM) inline but due to its performance and ability to converse effectively, it has gained a huge popularity amongst research as well as industrial community. Recently, many studies have been published to show the effectiveness, efficiency, integration, and sentiments of chatGPT and other LLMs. In contrast, this study focuses on the important aspects that are mostly overlooked, i.e. sustainability, privacy, digital divide, and ethics and suggests that not only chatGPT but every subsequent entry in the category of conversational bots should undergo Sustainability, PrivAcy, Digital divide, and Ethics (SPADE) evaluation. This paper discusses in detail about the issues and concerns raised over chatGPT in line with aforementioned characteristics. We support our hypothesis by some preliminary data collection and visualizations along with hypothesized facts. We also suggest mitigations and recommendations for each of the concerns. Furthermore, we also s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.03807</link><description>&lt;p&gt;
&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Privacy-Preserving CNN Training with Transfer Learning. (arXiv:2304.03807v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36801;&#31227;&#23398;&#20064;&#23454;&#29616;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#19979;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#30340;&#26041;&#26696;&#65292;&#36890;&#36807;&#36716;&#25442;&#24605;&#24819;&#21644;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;&#65292;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#20445;&#25252;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24050;&#32463;&#24471;&#21040;&#24456;&#22909;&#30340;&#30740;&#31350;&#65292;&#21516;&#26102;&#20445;&#25345;&#21516;&#24577;CNN&#35757;&#32451;&#20173;&#28982;&#26159;&#19968;&#39033;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#26469;&#23454;&#29616;&#22522;&#20110;&#21516;&#24577;&#21152;&#23494;&#25216;&#26415;&#30340;&#38544;&#31169;&#20445;&#25252;CNN&#35757;&#32451;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#27425;&#25104;&#21151;&#31361;&#30772;&#36825;&#20010;&#38590;&#39064;&#65292;&#20197;&#21069;&#27809;&#26377;&#20219;&#20309;&#24037;&#20316;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#12290;&#37319;&#29992;&#20102;&#20960;&#31181;&#25216;&#26415;&#65306;&#65288;1&#65289;&#36890;&#36807;&#36801;&#31227;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;&#38544;&#31169;&#20445;&#25252;&#30340;CNN&#35757;&#32451;&#31616;&#21270;&#20026;&#21516;&#24577;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#29978;&#33267;&#26159;&#22810;&#31867;&#36923;&#36753;&#22238;&#24402;&#65288;MLR&#65289;&#35757;&#32451;&#65307;&#65288;2&#65289;&#36890;&#36807;&#26356;&#24555;&#30340;&#26799;&#24230;&#21464;&#20307;$\texttt{Quadratic Gradient}$&#65292;&#24212;&#29992;&#20110;MLR&#30340;&#22686;&#24378;&#26799;&#24230;&#26041;&#27861;&#65292;&#22312;&#25910;&#25947;&#36895;&#24230;&#26041;&#38754;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65307;&#65288;3&#65289;&#25105;&#20204;&#37319;&#29992;&#25968;&#23398;&#20013;&#30340;&#21464;&#25442;&#24605;&#24819;&#65292;&#23558;&#21152;&#23494;&#22495;&#20013;&#30340;&#36817;&#20284;Softmax&#20989;&#25968;&#36716;&#25442;&#25104;&#24050;&#32463;&#30740;&#31350;&#36807;&#30340;&#36924;&#36817;&#26041;&#27861;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Privacy-preserving nerual network inference has been well studied while homomorphic CNN training still remains an open challenging task. In this paper, we present a practical solution to implement privacy-preserving CNN training based on mere Homomorphic Encryption (HE) technique. To our best knowledge, this is the first attempt successfully to crack this nut and no work ever before has achieved this goal. Several techniques combine to make it done: (1) with transfer learning, privacy-preserving CNN training can be reduced to homomorphic neural network training, or even multiclass logistic regression (MLR) training; (2) via a faster gradient variant called $\texttt{Quadratic Gradient}$, an enhanced gradient method for MLR with a state-of-the-art performance in converge speed is applied in this work to achieve high performance; (3) we employ the thought of transformation in mathematics to transform approximating Softmax function in encryption domain to the well-studied approximation of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#27880;&#24847;&#21147;&#22522;&#20934;&#27979;&#35797;&#65288;CAB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#24314;&#27169;&#38271;&#24207;&#21015;&#26102;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26041;&#27861;&#12290;CAB&#21253;&#25324;&#20102;&#32454;&#31890;&#24230;&#30340;&#27880;&#24847;&#21147;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#38750;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#38750;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#22235;&#31181;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#37319;&#38598;&#20102;&#19971;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2210.07661</link><description>&lt;p&gt;
CAB: &#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#20840;&#38754;&#27880;&#24847;&#21147;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling. (arXiv:2210.07661v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#27880;&#24847;&#21147;&#22522;&#20934;&#27979;&#35797;&#65288;CAB&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#22312;&#24314;&#27169;&#38271;&#24207;&#21015;&#26102;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#26041;&#27861;&#12290;CAB&#21253;&#25324;&#20102;&#32454;&#31890;&#24230;&#30340;&#27880;&#24847;&#21147;&#20998;&#31867;&#20307;&#31995;&#65292;&#28085;&#30422;&#20102;&#38750;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#38750;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#22235;&#31181;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#24182;&#37319;&#38598;&#20102;&#19971;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#22312;&#35821;&#35328;&#12289;&#22270;&#20687;&#21644;&#35821;&#38899;&#22788;&#29702;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#25552;&#20986;&#20102;&#21508;&#31181;&#39640;&#25928;&#30340;&#27880;&#24847;&#21147;&#26550;&#26500;&#65292;&#20197;&#25552;&#39640;Transformer&#22312;&#24314;&#27169;&#38271;&#24207;&#21015;&#26102;&#30340;&#25928;&#29575;&#65292;&#21516;&#26102;&#22823;&#24133;&#20445;&#30041;&#20854;&#34920;&#29616;&#21147;&#12290;&#19968;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#29992;&#20110;&#27979;&#35797;&#36825;&#20123;&#39640;&#25928;&#26041;&#27861;&#22312;&#38271;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#19978;&#30340;&#22522;&#20934;&#26159;&#38271;&#36317;&#31163;&#31454;&#25216;&#22330;&#65288;LRA&#65289;&#12290;&#28982;&#32780;&#65292;LRA&#21482;&#20851;&#27880;&#26631;&#20934;&#30340;&#21452;&#21521;&#65288;&#25110;&#38750;&#22240;&#26524;&#65289;&#33258;&#27880;&#24847;&#21147;&#65292;&#23436;&#20840;&#24573;&#30053;&#20102;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#21333;&#21521;&#65288;&#25110;&#22240;&#26524;&#65289;&#27880;&#24847;&#21147;&#65292;&#32780;&#36825;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#21516;&#26679;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#38754;&#30340;&#27880;&#24847;&#21147;&#22522;&#20934;&#27979;&#35797;&#65288;CAB&#65289;&#65292;&#37319;&#29992;&#20102;&#32454;&#31890;&#24230;&#30340;&#27880;&#24847;&#21147;&#20998;&#31867;&#20307;&#31995;&#65292;&#21253;&#25324;&#38750;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#12289;&#38750;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22240;&#26524;&#20132;&#21449;&#27880;&#24847;&#21147;&#22235;&#31181;&#21487;&#21306;&#20998;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;CAB&#25910;&#38598;&#20102;&#26469;&#33258;&#19981;&#21516;&#30740;&#31350;&#39046;&#22495;&#30340;&#19971;&#20010;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#65292;&#20197;&#35780;&#20272;&#22312;&#22235;&#31181;&#27880;&#24847;&#21147;&#27169;&#24335;&#19979;&#30340;&#39640;&#25928;&#27880;&#24847;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer has achieved remarkable success in language, image, and speech processing. Recently, various efficient attention architectures have been proposed to improve transformer's efficiency while largely preserving its efficacy, especially in modeling long sequences. A widely-used benchmark to test these efficient methods' capability on long-range modeling is Long Range Arena (LRA). However, LRA only focuses on the standard bidirectional (or noncausal) self attention, and completely ignores cross attentions and unidirectional (or causal) attentions, which are equally important to downstream applications. In this paper, we propose Comprehensive Attention Benchmark (CAB) under a fine-grained attention taxonomy with four distinguishable attention patterns, namely, noncausal self, causal self, noncausal cross, and causal cross attentions. CAB collects seven real-world tasks from different research areas to evaluate efficient attentions under the four attention patterns. Among these tas
&lt;/p&gt;</description></item></channel></rss>