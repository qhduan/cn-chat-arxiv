<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2402.19411</link><description>&lt;p&gt;
PaECTER&#65306;&#20351;&#29992;&#24341;&#25991;&#20449;&#24687;&#30340;&#19987;&#21033;&#32423;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
PaECTER: Patent-level Representation Learning using Citation-informed Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19411
&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#19987;&#20026;&#19987;&#21033;&#35774;&#35745;&#30340;&#24320;&#25918;&#28304;&#30721;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#21033;&#29992;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#29983;&#25104;&#19987;&#21033;&#25991;&#26723;&#30340;&#25968;&#20540;&#34920;&#31034;&#65292;&#24182;&#22312;&#19987;&#21033;&#39046;&#22495;&#30340;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PaECTER&#26159;&#19968;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#12289;&#38754;&#21521;&#19987;&#21033;&#30340;&#25991;&#26723;&#32423;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#21033;&#29992;&#23457;&#26680;&#21592;&#28155;&#21152;&#30340;&#24341;&#25991;&#20449;&#24687;&#23545;BERT&#36827;&#34892;&#24494;&#35843;&#65292;&#20026;&#19987;&#21033;&#25991;&#26723;&#29983;&#25104;&#25968;&#20540;&#34920;&#31034;&#12290;&#19982;&#19987;&#21033;&#39046;&#22495;&#20013;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;PaECTER&#22312;&#30456;&#20284;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19987;&#21033;&#24341;&#25991;&#39044;&#27979;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#20004;&#31181;&#19981;&#21516;&#30340;&#25490;&#21517;&#35780;&#20272;&#25351;&#26631;&#19978;&#22343;&#20248;&#20110;&#19979;&#19968;&#20010;&#26368;&#20339;&#19987;&#21033;&#29305;&#23450;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;&#19987;&#21033;BERT&#65289;&#12290;&#19982;25&#20010;&#19981;&#30456;&#20851;&#30340;&#19987;&#21033;&#30456;&#27604;&#65292;PaECTER&#22312;&#24179;&#22343;&#25490;&#21517;1.32&#22788;&#39044;&#27979;&#21040;&#33267;&#23569;&#19968;&#20010;&#26368;&#30456;&#20284;&#30340;&#19987;&#21033;&#12290;PaECTER&#20174;&#19987;&#21033;&#25991;&#26412;&#29983;&#25104;&#30340;&#25968;&#20540;&#34920;&#31034;&#21487;&#29992;&#20110;&#20998;&#31867;&#12289;&#36861;&#36394;&#30693;&#35782;&#27969;&#21160;&#25110;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#31561;&#19979;&#28216;&#20219;&#21153;&#12290;&#35821;&#20041;&#30456;&#20284;&#24615;&#25628;&#32034;&#22312;&#21457;&#26126;&#20154;&#21644;&#19987;&#21033;&#30340;&#20808;&#21069;&#25216;&#26415;&#25628;&#32034;&#32972;&#26223;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19411v1 Announce Type: cross  Abstract: PaECTER is a publicly available, open-source document-level encoder specific for patents. We fine-tune BERT for Patents with examiner-added citation information to generate numerical representations for patent documents. PaECTER performs better in similarity tasks than current state-of-the-art models used in the patent domain. More specifically, our model outperforms the next-best patent specific pre-trained language model (BERT for Patents) on our patent citation prediction test dataset on two different rank evaluation metrics. PaECTER predicts at least one most similar patent at a rank of 1.32 on average when compared against 25 irrelevant patents. Numerical representations generated by PaECTER from patent text can be used for downstream tasks such as classification, tracing knowledge flows, or semantic similarity search. Semantic similarity search is especially relevant in the context of prior art search for both inventors and paten
&lt;/p&gt;</description></item><item><title>&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#36755;&#20837;&#20013;&#20559;&#35265;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25935;&#24863;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.12715</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Spurious Correlations in Machine Learning: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12715
&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#36755;&#20837;&#20013;&#20559;&#35265;&#29305;&#24449;&#19982;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25935;&#24863;&#65292;&#26412;&#25991;&#22238;&#39038;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#26368;&#26032;&#26041;&#27861;&#65292;&#21516;&#26102;&#24635;&#32467;&#20102;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#23545;&#36755;&#20837;&#20013;&#20559;&#35265;&#29305;&#24449;&#65288;&#20363;&#22914;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#27425;&#35201;&#23545;&#35937;&#65289;&#19982;&#30456;&#24212;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#25935;&#24863;&#12290;&#36825;&#20123;&#29305;&#24449;&#21450;&#20854;&#19982;&#26631;&#31614;&#30340;&#30456;&#20851;&#24615;&#34987;&#31216;&#20026;&#8220;&#34394;&#20551;&#8221;&#65292;&#22240;&#20026;&#23427;&#20204;&#24448;&#24448;&#38543;&#30528;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#32780;&#25913;&#21464;&#65292;&#36825;&#21487;&#33021;&#23545;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#40065;&#26834;&#24615;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#23457;&#26597;&#20102;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20851;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#34394;&#20551;&#30456;&#20851;&#24615;&#30340;&#24403;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#20998;&#31867;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#21644;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#24110;&#21161;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#26368;&#21518;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#26410;&#26469;&#30740;&#31350;&#25361;&#25112;&#65292;&#26088;&#22312;&#20026;&#30456;&#20851;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12715v1 Announce Type: new  Abstract: Machine learning systems are known to be sensitive to spurious correlations between biased features of the inputs (e.g., background, texture, and secondary objects) and the corresponding labels. These features and their correlations with the labels are known as "spurious" because they tend to change with shifts in real-world data distributions, which can negatively impact the model's generalization and robustness. In this survey, we provide a comprehensive review of this issue, along with a taxonomy of current state-of-the-art methods for addressing spurious correlations in machine learning models. Additionally, we summarize existing datasets, benchmarks, and metrics to aid future research. The paper concludes with a discussion of the recent advancements and future research challenges in this field, aiming to provide valuable insights for researchers in the related domains.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#25105;&#20204;&#30340;ReAlnet&#27169;&#22411;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17231</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#31070;&#32463;&#34920;&#31034;&#23545;&#40784;&#23454;&#29616;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;ReAlnet&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ReAlnet: Achieving More Human Brain-Like Vision via Human Neural Representational Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17231
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#65292;&#25105;&#20204;&#30340;ReAlnet&#27169;&#22411;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#65292;&#25552;&#20379;&#20102;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;&#30340;&#29289;&#20307;&#35782;&#21035;&#27169;&#22411;&#22312;&#27169;&#25311;&#20154;&#33041;&#35270;&#35273;&#20449;&#24687;&#22788;&#29702;&#26426;&#21046;&#26041;&#38754;&#20173;&#28982;&#33853;&#21518;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#21033;&#29992;&#31070;&#32463;&#25968;&#25454;&#26469;&#27169;&#20223;&#22823;&#33041;&#22788;&#29702;&#30340;&#28508;&#21147;&#65307;&#28982;&#32780;&#65292;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#20381;&#36182;&#20110;&#23545;&#38750;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#30340;&#20405;&#20837;&#24615;&#31070;&#32463;&#35760;&#24405;&#65292;&#36825;&#22312;&#25105;&#20204;&#23545;&#20154;&#31867;&#35270;&#35273;&#24863;&#30693;&#21644;&#24320;&#21457;&#26356;&#31867;&#20284;&#20154;&#31867;&#22823;&#33041;&#35270;&#35273;&#27169;&#22411;&#30340;&#29702;&#35299;&#19978;&#23384;&#22312;&#30528;&#37325;&#35201;&#30340;&#32570;&#21475;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#8220;Re(presentational)Al(ignment)net&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#20197;&#38750;&#20405;&#20837;&#24615;&#33041;&#30005;&#22270;&#35760;&#24405;&#20026;&#22522;&#30784;&#30340;&#19982;&#20154;&#33041;&#27963;&#21160;&#30456;&#23545;&#40784;&#30340;&#35270;&#35273;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#19982;&#20154;&#33041;&#34920;&#31034;&#26356;&#39640;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#22270;&#20687;&#21040;&#33041;&#22810;&#23618;&#32534;&#30721;&#23545;&#40784;&#26694;&#26550;&#19981;&#20165;&#20248;&#21270;&#20102;&#27169;&#22411;&#30340;&#22810;&#20010;&#23618;&#27425;&#65292;&#26631;&#24535;&#30528;&#31070;&#32463;&#23545;&#40784;&#26041;&#38754;&#30340;&#37325;&#22823;&#31361;&#30772;&#65292;&#32780;&#19988;&#36824;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#39640;&#25928;&#22320;&#23398;&#20064;&#21644;&#27169;&#20223;&#20154;&#33041;&#30340;&#35270;&#35273;&#24863;&#30693;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the remarkable strides made in artificial intelligence, current object recognition models still lag behind in emulating the mechanism of visual information processing in human brains. Recent studies have highlighted the potential of using neural data to mimic brain processing; however, these often reply on invasive neural recordings from non-human subjects, leaving a critical gap in our understanding of human visual perception and the development of more human brain-like vision models. Addressing this gap, we present, for the first time, "Re(presentational)Al(ignment)net", a vision model aligned with human brain activity based on non-invasive EEG recordings, demonstrating a significantly higher similarity to human brain representations. Our innovative image-to-brain multi-layer encoding alignment framework not only optimizes multiple layers of the model, marking a substantial leap in neural alignment, but also enables the model to efficiently learn and mimic human brain's visua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#65288;SMaRt&#65289;&#26469;&#25913;&#36827;GANs&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25345;&#32493;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#28857;&#25512;&#21521;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18208</link><description>&lt;p&gt;
SMaRt: &#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#25913;&#36827;GANs
&lt;/p&gt;
&lt;p&gt;
SMaRt: Improving GANs with Score Matching Regularity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#65288;SMaRt&#65289;&#26469;&#25913;&#36827;GANs&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#36890;&#36807;&#25345;&#32493;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#28857;&#25512;&#21521;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#65292;&#25552;&#39640;&#20102;&#21512;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#36890;&#24120;&#22312;&#23398;&#20064;&#39640;&#24230;&#22810;&#26679;&#21270;&#30340;&#22797;&#26434;&#25968;&#25454;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;GANs&#30340;&#25968;&#23398;&#22522;&#30784;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#25581;&#31034;&#20102;GAN&#35757;&#32451;&#30340;&#21407;&#22987;&#23545;&#25239;&#25439;&#22833;&#19981;&#33021;&#35299;&#20915;&#29983;&#25104;&#25968;&#25454;&#27969;&#24418;&#30340;&#27491;&#27979;&#24230;&#23376;&#38598;&#33853;&#22312;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#20043;&#22806;&#30340;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#21457;&#29616;&#20998;&#25968;&#21305;&#37197;&#21487;&#20197;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25345;&#32493;&#23558;&#29983;&#25104;&#30340;&#25968;&#25454;&#28857;&#25512;&#21521;&#30495;&#23454;&#25968;&#25454;&#27969;&#24418;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#20998;&#25968;&#21305;&#37197;&#35268;&#21017;&#65288;SMaRt&#65289;&#26469;&#25913;&#36827;GANs&#30340;&#20248;&#21270;&#12290;&#23545;&#20110;&#32463;&#39564;&#35777;&#25454;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#20010;&#29609;&#20855;&#31034;&#20363;&#26469;&#23637;&#31034;&#36890;&#36807;&#36741;&#21161;&#19968;&#20010;&#30495;&#23454;&#24471;&#20998;&#20989;&#25968;&#26469;&#35757;&#32451;GANs&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#20877;&#29616;&#30495;&#23454;&#25968;&#25454;&#20998;&#24067;&#65292;&#28982;&#21518;&#30830;&#35748;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25345;&#32493;&#25552;&#21319;&#21508;&#31181;&#29366;&#24577;&#30340;&#21512;&#25104;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative adversarial networks (GANs) usually struggle in learning from highly diverse data, whose underlying manifold is complex. In this work, we revisit the mathematical foundations of GANs, and theoretically reveal that the native adversarial loss for GAN training is insufficient to fix the problem of subsets with positive Lebesgue measure of the generated data manifold lying out of the real data manifold. Instead, we find that score matching serves as a promising solution to this issue thanks to its capability of persistently pushing the generated data points towards the real data manifold. We thereby propose to improve the optimization of GANs with score matching regularity (SMaRt). Regarding the empirical evidences, we first design a toy example to show that training GANs by the aid of a ground-truth score function can help reproduce the real data distribution more accurately, and then confirm that our approach can consistently boost the synthesis performance of various state-o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"Inhibitor"&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;ReLU&#21644;&#21152;&#27861;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25110;&#26367;&#20195;&#31639;&#27861;&#31995;&#32479;&#19978;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25191;&#34892;&#21644;&#25903;&#25345;&#26356;&#22823;&#30340;&#37327;&#21270;Transformer&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30456;&#27604;&#65292;&#35813;&#26426;&#21046;&#22312;&#39044;&#27979;&#24471;&#20998;&#19978;&#34920;&#29616;&#30456;&#24403;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;&#36825;&#19968;&#21019;&#26032;&#21487;&#33021;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#24212;&#29992;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2310.02041</link><description>&lt;p&gt;
&#29992;&#20110;&#39640;&#25928;Transformer&#30340;"Inhibitor"&#65306;ReLU&#21644;&#21152;&#27861;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
The Inhibitor: ReLU and Addition-Based Attention for Efficient Transformers. (arXiv:2310.02041v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"Inhibitor"&#26426;&#21046;&#65292;&#36890;&#36807;&#20351;&#29992;ReLU&#21644;&#21152;&#27861;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#22686;&#24378;&#35745;&#31639;&#25928;&#29575;&#12290;&#36825;&#31181;&#26426;&#21046;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25110;&#26367;&#20195;&#31639;&#27861;&#31995;&#32479;&#19978;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25191;&#34892;&#21644;&#25903;&#25345;&#26356;&#22823;&#30340;&#37327;&#21270;Transformer&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30456;&#27604;&#65292;&#35813;&#26426;&#21046;&#22312;&#39044;&#27979;&#24471;&#20998;&#19978;&#34920;&#29616;&#30456;&#24403;&#65292;&#24182;&#19988;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;&#36825;&#19968;&#21019;&#26032;&#21487;&#33021;&#22312;&#38544;&#31169;&#20445;&#25252;&#30340;&#24212;&#29992;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#22686;&#24378;&#37327;&#21270;Transformer&#30340;&#35745;&#31639;&#25928;&#29575;&#65292;&#25105;&#20204;&#29992;&#21482;&#28041;&#21450;&#21152;&#27861;&#21644;ReLU&#28608;&#27963;&#30340;&#26367;&#20195;&#26426;&#21046;&#26469;&#21462;&#20195;&#22522;&#20110;&#28857;&#31215;&#21644;Softmax&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#36825;&#26679;&#21487;&#20197;&#36991;&#20813;&#30697;&#38453;&#20056;&#27861;&#20013;&#24120;&#38656;&#35201;&#30340;&#21452;&#31934;&#24230;&#25193;&#23637;&#21644;&#26114;&#36149;&#30340;Softmax&#35745;&#31639;&#65292;&#20294;&#20173;&#20445;&#30041;&#20102;&#20256;&#32479;&#30340;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#26680;&#24515;&#21151;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#25110;&#21516;&#24577;&#21152;&#23494;&#31561;&#26367;&#20195;&#31639;&#27861;&#31995;&#32479;&#19978;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#25191;&#34892;&#21644;&#25903;&#25345;&#26356;&#22823;&#30340;&#37327;&#21270;Transformer&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;&#24120;&#35265;&#30340;&#22522;&#20934;&#20219;&#21153;&#19978;&#30340;&#35757;&#32451;&#23454;&#39564;&#26174;&#31034;&#65292;&#27979;&#35797;&#38598;&#30340;&#39044;&#27979;&#24471;&#20998;&#19982;&#37319;&#29992;&#28857;&#31215;&#27880;&#24847;&#21147;&#30340;&#20256;&#32479;Transformer&#30456;&#24403;&#12290;&#25105;&#20204;&#30340;&#32553;&#25918;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#22312;&#26126;&#25991;&#21644;&#21152;&#23494;&#19979;&#37117;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#35745;&#31639;&#33410;&#30465;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30456;&#20449;&#26412;&#25991;&#20171;&#32461;&#30340;&#22522;&#20110;ReLU&#21644;&#21152;&#27861;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#33021;&#20250;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;A
&lt;/p&gt;
&lt;p&gt;
To enhance the computational efficiency of quantized Transformers, we replace the dot-product and Softmax-based attention with an alternative mechanism involving addition and ReLU activation only. This side-steps the expansion to double precision often required by matrix multiplication and avoids costly Softmax evaluations but maintains much of the core functionality of conventional dot-product attention. It can enable more efficient execution and support larger quantized Transformer models on resource-constrained hardware or alternative arithmetic systems like homomorphic encryption. Training experiments on four common benchmark tasks show test set prediction scores comparable to those of conventional Transformers with dot-product attention. Our scaling experiments also suggest significant computational savings, both in plaintext and under encryption. In particular, we believe that the ReLU and addition-based attention mechanism introduced in this paper may enable privacy-preserving A
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#29305;&#24449;&#22312;&#30456;&#21516;&#20449;&#24687;&#22833;&#30495;&#27700;&#24179;&#19979;&#27604;&#36755;&#20837;&#34920;&#31034;&#26356;&#21152;&#38887;&#24615;&#65292;&#24182;&#19988;&#23545;&#25239;&#24615;&#38887;&#24615;&#30001;&#29305;&#24449;&#32500;&#24230;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20849;&#21516;&#20915;&#23450;&#12290;</title><link>http://arxiv.org/abs/2309.17401</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarial Machine Learning in Latent Representations of Neural Networks. (arXiv:2309.17401v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17401
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#29305;&#24449;&#22312;&#30456;&#21516;&#20449;&#24687;&#22833;&#30495;&#27700;&#24179;&#19979;&#27604;&#36755;&#20837;&#34920;&#31034;&#26356;&#21152;&#38887;&#24615;&#65292;&#24182;&#19988;&#23545;&#25239;&#24615;&#38887;&#24615;&#30001;&#29305;&#24449;&#32500;&#24230;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20849;&#21516;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#38477;&#20302;&#36793;&#32536;&#35745;&#31639;&#22330;&#26223;&#20013;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#24310;&#36831;&#12290;&#23613;&#31649;&#24050;&#32463;&#23545;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#20998;&#26512;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#26469;&#22635;&#34917;&#29616;&#26377;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#32622;&#20110;&#20449;&#24687;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#34913;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#22833;&#30495;&#21644;&#38887;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#34920;&#26126;&#65306;&#65288;i&#65289;&#22312;&#20551;&#35774;&#20855;&#26377;&#30456;&#21516;&#20449;&#24687;&#22833;&#30495;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#65292;&#28508;&#22312;&#29305;&#24449;&#22987;&#32456;&#27604;&#36755;&#20837;&#34920;&#31034;&#26356;&#21152;&#38887;&#24615;&#65307;&#65288;ii&#65289;&#23545;&#25239;&#24615;&#38887;&#24615;&#21516;&#26102;&#30001;&#29305;&#24449;&#32500;&#24230;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20915;&#23450;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#32771;&#34385;&#20102;6&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed deep neural networks (DNNs) have been shown to reduce the computational burden of mobile devices and decrease the end-to-end inference latency in edge computing scenarios. While distributed DNNs have been studied, to the best of our knowledge the resilience of distributed DNNs to adversarial action still remains an open problem. In this paper, we fill the existing research gap by rigorously analyzing the robustness of distributed DNNs against adversarial action. We cast this problem in the context of information theory and introduce two new measurements for distortion and robustness. Our theoretical findings indicate that (i) assuming the same level of information distortion, latent features are always more robust than input representations; (ii) the adversarial robustness is jointly determined by the feature dimension and the generalization capability of the DNN. To test our theoretical findings, we perform extensive experimental analysis by considering 6 different DNN arc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoQGAN&#30340;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20114;&#20449;&#24687;&#31070;&#32463;&#20272;&#35745;&#22120;&#65288;MINE&#65289;&#35299;&#20915;&#20102;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.01363</link><description>&lt;p&gt;
&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#30340;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21450;&#20854;&#22312;&#37329;&#34701;&#39046;&#22495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mutual Information Maximizing Quantum Generative Adversarial Network and Its Applications in Finance. (arXiv:2309.01363v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01363
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoQGAN&#30340;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20114;&#20449;&#24687;&#31070;&#32463;&#20272;&#35745;&#22120;&#65288;MINE&#65289;&#35299;&#20915;&#20102;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#37329;&#34701;&#39046;&#22495;&#20855;&#26377;&#24212;&#29992;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;NISQ&#65288;&#22122;&#22768;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65289;&#35745;&#31639;&#26102;&#20195;&#65292;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#26368;&#20855;&#21069;&#26223;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#27604;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#20855;&#26377;&#26174;&#33879;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#34987;&#35748;&#20026;&#22312;&#22270;&#20687;&#29983;&#25104;&#12289;&#37329;&#34701;&#21644;&#27010;&#29575;&#20998;&#24067;&#24314;&#27169;&#31561;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#30340;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#32593;&#32476;&#38656;&#35201;&#35299;&#20915;&#22266;&#26377;&#30340;&#25361;&#25112;&#65292;&#22914;&#27169;&#24335;&#23849;&#28291;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36890;&#36807;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26799;&#24230;&#19979;&#38477;&#26469;&#23454;&#29616;&#39640;&#32500;&#36830;&#32493;&#38543;&#26426;&#21464;&#37327;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;InfoQGAN&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#20114;&#20449;&#24687;&#31070;&#32463;&#20272;&#35745;&#22120;&#65288;MINE&#65289;&#24212;&#29992;&#20110;&#37327;&#23376;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#26694;&#26550;&#20013;&#65292;&#20197;&#35299;&#20915;&#27169;&#24335;&#23849;&#28291;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#22914;&#20309;&#23558;&#27492;&#26041;&#27861;&#24212;&#29992;&#20110;&#37329;&#34701;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most promising applications in the era of NISQ (Noisy Intermediate-Scale Quantum) computing is quantum machine learning. Quantum machine learning offers significant quantum advantages over classical machine learning across various domains. Specifically, generative adversarial networks have been recognized for their potential utility in diverse fields such as image generation, finance, and probability distribution modeling. However, these networks necessitate solutions for inherent challenges like mode collapse. In this study, we capitalize on the concept that the estimation of mutual information between high-dimensional continuous random variables can be achieved through gradient descent using neural networks. We introduce a novel approach named InfoQGAN, which employs the Mutual Information Neural Estimator (MINE) within the framework of quantum generative adversarial networks to tackle the mode collapse issue. Furthermore, we elaborate on how this approach can be applied t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#19979;&#30028;&#30340;Fisher-Rao&#26799;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#19982;&#30446;&#26631;&#20998;&#24067;&#30340;Kullback-Leibler&#25955;&#24230;&#26799;&#24230;&#30340;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#19982;&#26368;&#22823;&#21270;ELBO&#30340;&#31561;&#20215;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.11249</link><description>&lt;p&gt;
&#20851;&#20110;&#35777;&#25454;&#19979;&#30028;&#30340;Fisher-Rao&#26799;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Fisher-Rao Gradient of the Evidence Lower Bound. (arXiv:2307.11249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#19979;&#30028;&#30340;Fisher-Rao&#26799;&#24230;&#65292;&#25581;&#31034;&#20102;&#23427;&#19982;&#30446;&#26631;&#20998;&#24067;&#30340;Kullback-Leibler&#25955;&#24230;&#26799;&#24230;&#30340;&#20851;&#31995;&#65292;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#19982;&#26368;&#22823;&#21270;ELBO&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35777;&#25454;&#19979;&#30028;&#65288;ELBO&#65289;&#30340;Fisher-Rao&#26799;&#24230;&#65292;&#20063;&#31216;&#20026;&#33258;&#28982;&#26799;&#24230;&#65292;&#23427;&#22312;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#29702;&#35770;&#12289;Helmholtz&#26426;&#21644;&#33258;&#30001;&#33021;&#21407;&#29702;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;ELBO&#30340;&#33258;&#28982;&#26799;&#24230;&#19982;&#30446;&#26631;&#20998;&#24067;&#30340;Kullback-Leibler&#25955;&#24230;&#30340;&#33258;&#28982;&#26799;&#24230;&#30456;&#20851;&#65292;&#21518;&#32773;&#26159;&#23398;&#20064;&#30340;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#12290;&#22522;&#20110;&#20449;&#24687;&#20960;&#20309;&#20013;&#26799;&#24230;&#30340;&#19981;&#21464;&#24615;&#29305;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#24213;&#23618;&#27169;&#22411;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#26368;&#23567;&#21270;&#20027;&#35201;&#30446;&#26631;&#20989;&#25968;&#19982;&#26368;&#22823;&#21270;ELBO&#30340;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article studies the Fisher-Rao gradient, also referred to as the natural gradient, of the evidence lower bound, the ELBO, which plays a crucial role within the theory of the Variational Autonecoder, the Helmholtz Machine and the Free Energy Principle. The natural gradient of the ELBO is related to the natural gradient of the Kullback-Leibler divergence from a target distribution, the prime objective function of learning. Based on invariance properties of gradients within information geometry, conditions on the underlying model are provided that ensure the equivalence of minimising the prime objective function and the maximisation of the ELBO.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#23450;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#25237;&#24433;&#12289;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#37327;&#35268;&#30340;&#26032;&#29305;&#24449;&#36798;&#21040;&#20102;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#20248;&#21152;&#36895;&#25910;&#25947;&#20445;&#35777; $O(1/T)$&#12289;&#24179;&#28369;&#38382;&#39064;&#30340; $O(1/T^2)$&#65292;&#20197;&#21450;&#20004;&#32773;&#37117;&#28385;&#36275;&#30340;&#21152;&#36895;&#32447;&#24615;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2303.05037</link><description>&lt;p&gt;
&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#30340;&#37327;&#35268;&#21644;&#21152;&#36895;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Gauges and Accelerated Optimization over Smooth and/or Strongly Convex Sets. (arXiv:2303.05037v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#23450;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#25237;&#24433;&#12289;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#37327;&#35268;&#30340;&#26032;&#29305;&#24449;&#36798;&#21040;&#20102;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#20248;&#21152;&#36895;&#25910;&#25947;&#20445;&#35777; $O(1/T)$&#12289;&#24179;&#28369;&#38382;&#39064;&#30340; $O(1/T^2)$&#65292;&#20197;&#21450;&#20004;&#32773;&#37117;&#28385;&#36275;&#30340;&#21152;&#36895;&#32447;&#24615;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#22312;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#19978;&#23450;&#20041;&#30340;&#21487;&#34892;&#24615;&#21644;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#12290;&#36825;&#20123;&#27010;&#24565;&#19982;&#23427;&#20204;&#21463;&#27426;&#36814;&#30340;&#20989;&#25968;&#23545;&#24212;&#29289;&#30456;&#20284;&#65292;&#20294;&#22312;&#19968;&#38454;&#20248;&#21270;&#25991;&#29486;&#20013;&#30740;&#31350;&#36739;&#23569;&#12290;&#25105;&#20204;&#22312;&#36825;&#20123;&#35774;&#32622;&#20013;&#25552;&#20986;&#20102;&#26032;&#30340;&#21487;&#25193;&#23637;&#12289;&#26080;&#25237;&#24433;&#12289;&#21152;&#36895;&#19968;&#38454;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#32447;&#24615;&#20248;&#21270;&#25110;&#25237;&#24433;&#39044;&#35328;&#26426;&#65292;&#20165;&#20351;&#29992;&#20415;&#23452;&#30340;&#19968;&#32500;&#32447;&#25628;&#32034;&#21644;&#27861;&#21521;&#37327;&#35745;&#31639;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#24378;&#20984;&#38382;&#39064;&#30340;&#26368;&#20248;&#21152;&#36895;&#25910;&#25947;&#20445;&#35777; $O(1/T)$&#12289;&#24179;&#28369;&#38382;&#39064;&#30340; $O(1/T^2)$&#65292;&#20197;&#21450;&#20004;&#32773;&#37117;&#28385;&#36275;&#30340;&#21152;&#36895;&#32447;&#24615;&#25910;&#25947;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21644;&#20998;&#26512;&#22522;&#20110;&#24179;&#28369;&#21644;/&#25110;&#24378;&#20984;&#38598;&#21512;&#30340;&#38389;&#21487;&#22827;&#26031;&#22522;&#37327;&#30340;&#26032;&#29305;&#24449;&#65292;&#36825;&#21487;&#33021;&#20855;&#26377;&#29420;&#31435;&#30340;&#20852;&#36259;&#65306;&#23613;&#31649;&#37327;&#35268;&#26082;&#19981;&#26159;&#24179;&#28369;&#30340;&#20063;&#19981;&#26159;&#24378;&#20984;&#30340;&#65292;&#20294;&#25105;&#20204;&#26174;&#31034;&#20102;&#35268;&#27169;&#30340;&#21152;&#24179;&#26041;&#22312;&#38598;&#21512;&#20013;&#32487;&#25215;&#20219;&#20309;&#23384;&#22312;&#30340;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider feasibility and constrained optimization problems defined over smooth and/or strongly convex sets. These notions mirror their popular function counterparts but are much less explored in the first-order optimization literature. We propose new scalable, projection-free, accelerated first-order methods in these settings. Our methods avoid linear optimization or projection oracles, only using cheap one-dimensional linesearches and normal vector computations. Despite this, we derive optimal accelerated convergence guarantees of $O(1/T)$ for strongly convex problems, $O(1/T^2)$ for smooth problems, and accelerated linear convergence given both. Our algorithms and analysis are based on novel characterizations of the Minkowski gauge of smooth and/or strongly convex sets, which may be of independent interest: although the gauge is neither smooth nor strongly convex, we show the gauge squared inherits any structure present in the set.
&lt;/p&gt;</description></item></channel></rss>