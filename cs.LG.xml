<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Dyna-LfLH&#65292;&#36890;&#36807;&#23398;&#20064;&#24187;&#35273;&#20013;&#30340;&#21160;&#24577;&#29615;&#22659;&#65292;&#23433;&#20840;&#22320;&#23398;&#20064;&#22320;&#38754;&#26426;&#22120;&#20154;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#28789;&#27963;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.17231</link><description>&lt;p&gt;
Dyna-LfLH:&#20174;&#23398;&#21040;&#30340;&#24187;&#35273;&#20013;&#23398;&#20250;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#23398;&#20064;&#28789;&#27963;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from Learned Hallucination
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17231
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;Dyna-LfLH&#65292;&#36890;&#36807;&#23398;&#20064;&#24187;&#35273;&#20013;&#30340;&#21160;&#24577;&#29615;&#22659;&#65292;&#23433;&#20840;&#22320;&#23398;&#20064;&#22320;&#38754;&#26426;&#22120;&#20154;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#28789;&#27963;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#23433;&#20840;&#22320;&#23398;&#20064;&#22320;&#38754;&#26426;&#22120;&#20154;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#65292;&#20197;&#22312;&#23494;&#38598;&#19988;&#21160;&#24577;&#30340;&#38556;&#30861;&#29289;&#29615;&#22659;&#20013;&#23548;&#33322;&#12290;&#38024;&#23545;&#39640;&#24230;&#28151;&#20081;&#12289;&#24555;&#36895;&#31227;&#21160;&#12289;&#38590;&#20197;&#39044;&#27979;&#30340;&#38556;&#30861;&#29289;&#65292;&#20256;&#32479;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#21487;&#33021;&#26080;&#27861;&#36319;&#19978;&#26377;&#38480;&#30340;&#26426;&#36733;&#35745;&#31639;&#12290;&#23545;&#20110;&#22522;&#20110;&#23398;&#20064;&#30340;&#35268;&#21010;&#22120;&#65292;&#24456;&#38590;&#33719;&#21462;&#39640;&#36136;&#37327;&#30340;&#28436;&#31034;&#20197;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#65292;&#21516;&#26102;&#24378;&#21270;&#23398;&#20064;&#22312;&#25506;&#32034;&#36807;&#31243;&#20013;&#30001;&#20110;&#39640;&#30896;&#25758;&#27010;&#29575;&#32780;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#23433;&#20840;&#26377;&#25928;&#22320;&#25552;&#20379;&#35757;&#32451;&#25968;&#25454;&#65292;LfH&#26041;&#27861;&#22522;&#20110;&#36807;&#21435;&#25104;&#21151;&#30340;&#23548;&#33322;&#32463;&#39564;&#22312;&#30456;&#23545;&#31616;&#21333;&#25110;&#23436;&#20840;&#24320;&#25918;&#30340;&#29615;&#22659;&#20013;&#32508;&#21512;&#22256;&#38590;&#30340;&#23548;&#33322;&#29615;&#22659;&#65292;&#20294;&#36951;&#25022;&#30340;&#26159;&#26080;&#27861;&#35299;&#20915;&#21160;&#24577;&#38556;&#30861;&#29289;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;Dyna-LfLH&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#24182;&#23398;&#20064;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#20998;&#24067;&#21644;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17231v1 Announce Type: cross  Abstract: This paper presents a self-supervised learning method to safely learn a motion planner for ground robots to navigate environments with dense and dynamic obstacles. When facing highly-cluttered, fast-moving, hard-to-predict obstacles, classical motion planners may not be able to keep up with limited onboard computation. For learning-based planners, high-quality demonstrations are difficult to acquire for imitation learning while reinforcement learning becomes inefficient due to the high probability of collision during exploration. To safely and efficiently provide training data, the Learning from Hallucination (LfH) approaches synthesize difficult navigation environments based on past successful navigation experiences in relatively easy or completely open ones, but unfortunately cannot address dynamic obstacles. In our new Dynamic Learning from Learned Hallucination (Dyna-LfLH), we design and learn a novel latent distribution and sample
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.08955</link><description>&lt;p&gt;
&#26397;&#21521;&#39640;&#25928;&#30340;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#65306;&#19968;&#20010;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Risk-Sensitive Policy Gradient: An Iteration Complexity Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#21457;&#29616;&#20854;&#33021;&#22815;&#36890;&#36807;&#20351;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#36798;&#21040;&#36739;&#20302;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20351;&#24471;&#33258;&#20027;&#26234;&#33021;&#20307;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#23398;&#20064;&#26368;&#20339;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#22312;&#36845;&#20195;&#22797;&#26434;&#24230;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#24179;&#34913;&#20102;&#26399;&#26395;&#22238;&#25253;&#21644;&#39118;&#38505;&#65292;&#20855;&#26377;&#20135;&#29983;&#27010;&#29575;&#40065;&#26834;&#31574;&#30053;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#39118;&#38505;&#25935;&#24863;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#20998;&#26512;&#65292;&#37325;&#28857;&#20851;&#27880;REINFORCE&#31639;&#27861;&#24182;&#37319;&#29992;&#25351;&#25968;&#25928;&#29992;&#20989;&#25968;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#19968;&#20010;$\mathcal{O}(\epsilon^{-2})$&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#65292;&#20197;&#36798;&#21040;$\epsilon$-&#36817;&#20284;&#30340;&#19968;&#38454;&#31283;&#23450;&#28857;&#65288;FOSP&#65289;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#26159;&#21542;&#21487;&#20197;&#27604;&#39118;&#38505;&#20013;&#24615;&#31639;&#27861;&#23454;&#29616;&#26356;&#22909;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08955v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has shown exceptional performance across various applications, enabling autonomous agents to learn optimal policies through interaction with their environments. However, traditional RL frameworks often face challenges in terms of iteration complexity and robustness. Risk-sensitive RL, which balances expected return and risk, has been explored for its potential to yield probabilistically robust policies, yet its iteration complexity analysis remains underexplored. In this study, we conduct a thorough iteration complexity analysis for the risk-sensitive policy gradient method, focusing on the REINFORCE algorithm and employing the exponential utility function. We obtain an iteration complexity of $\mathcal{O}(\epsilon^{-2})$ to reach an $\epsilon$-approximate first-order stationary point (FOSP). We investigate whether risk-sensitive algorithms can achieve better iteration complexity compared to their risk-neutr
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.09698</link><description>&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#36807;&#28388;&#22120;&#20013;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Combining Evidence Across Filtrations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09698
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#21512;&#24182;&#20351;&#29992;&#19981;&#21516;&#36807;&#28388;&#22120;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39034;&#24207;&#25512;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20219;&#20309;&#26102;&#21051;&#26377;&#25928;&#30340;&#39034;&#24207;&#25512;&#29702;&#20013;&#65292;&#24050;&#30693;&#20219;&#20309;&#21487;&#25509;&#21463;&#30340;&#25512;&#29702;&#26041;&#27861;&#24517;&#39035;&#22522;&#20110;&#27979;&#35797;&#38789;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#24191;&#20041;&#21270;&#65292;&#31216;&#20026;e&#36827;&#31243;&#65292;&#23427;&#20204;&#26159;&#38750;&#36127;&#36827;&#31243;&#65292;&#20854;&#22312;&#20219;&#20309;&#20219;&#24847;&#20572;&#26102;&#30340;&#26399;&#26395;&#19978;&#30028;&#19981;&#36229;&#36807;&#19968;&#12290;e&#36827;&#31243;&#37327;&#21270;&#20102;&#38024;&#23545;&#22797;&#21512;&#38646;&#20551;&#35774;&#30340;&#19968;&#31995;&#21015;&#32467;&#26524;&#30340;&#32047;&#31215;&#35777;&#25454;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#19981;&#21516;&#20449;&#24687;&#38598;&#65288;&#21363;&#36807;&#28388;&#22120;&#65289;&#35745;&#31639;&#30340;e&#36827;&#31243;&#30340;&#21512;&#24182;&#26041;&#27861;&#65292;&#38024;&#23545;&#19968;&#20010;&#38646;&#20551;&#35774;&#12290;&#23613;&#31649;&#22312;&#30456;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#21487;&#20197;&#36731;&#26494;&#22320;&#21512;&#24182;&#65288;&#20363;&#22914;&#65292;&#36890;&#36807;&#24179;&#22343;&#65289;&#65292;&#20294;&#22312;&#19981;&#21516;&#36807;&#28388;&#22120;&#19978;&#26500;&#24314;&#30340;e&#36827;&#31243;&#19981;&#33021;&#37027;&#20040;&#23481;&#26131;&#22320;&#21512;&#24182;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#36739;&#31895;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#19981;&#33021;&#36716;&#25442;&#20026;&#22312;&#26356;&#32454;&#30340;&#36807;&#28388;&#22120;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25991;&#29486;&#20013;&#19977;&#20010;&#20855;&#20307;&#20363;&#23376;&#65306;&#21487;&#20132;&#25442;&#24615;&#27979;&#35797;&#65292;&#29420;&#31435;&#24615;&#27979;&#35797;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09698v1 Announce Type: cross  Abstract: In anytime-valid sequential inference, it is known that any admissible inference procedure must be based on test martingales and their composite generalization, called e-processes, which are nonnegative processes whose expectation at any arbitrary stopping time is upper-bounded by one. An e-process quantifies the accumulated evidence against a composite null hypothesis over a sequence of outcomes. This paper studies methods for combining e-processes that are computed using different information sets, i.e., filtrations, for a null hypothesis. Even though e-processes constructed on the same filtration can be combined effortlessly (e.g., by averaging), e-processes constructed on different filtrations cannot be combined as easily because their validity in a coarser filtration does not translate to validity in a finer filtration. We discuss three concrete examples of such e-processes in the literature: exchangeability tests, independence te
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.04660</link><description>&lt;p&gt;
&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#25552;&#39640;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial Robustness Through Artifact Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04660
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33402;&#26415;&#35774;&#35745;&#23454;&#29616;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24494;&#23567;&#26356;&#25913;&#29616;&#26377;&#35268;&#33539;&#26469;&#25269;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#20986;&#29616;&#32473;&#26426;&#22120;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#38459;&#30861;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#22823;&#22810;&#25968;&#38450;&#24481;&#26041;&#27861;&#37117;&#25913;&#21464;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#26041;&#24335;&#65288;&#22914;&#23545;&#25239;&#24615;&#35757;&#32451;&#65289;&#25110;&#25512;&#29702;&#36807;&#31243;&#65288;&#22914;&#38543;&#26426;&#24179;&#28369;&#65289;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#20294;&#27169;&#22411;&#20173;&#28982;&#26497;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#24433;&#21709;&#12290;&#22312;&#26576;&#20123;&#39046;&#22495;&#22914;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#35937;&#26159;&#25353;&#29031;&#35268;&#33539;&#26469;&#35774;&#35745;&#65288;&#22914;&#26631;&#24535;&#35268;&#33539;&#65289;&#12290;&#20026;&#20102;&#25913;&#21892;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37325;&#26032;&#23450;&#20041;&#35268;&#33539;&#30340;&#26041;&#27861;&#65292;&#23545;&#29616;&#26377;&#35268;&#33539;&#36827;&#34892;&#24494;&#23567;&#30340;&#26356;&#25913;&#65292;&#20197;&#38450;&#24481;&#23545;&#25239;&#24615;&#31034;&#20363;&#12290;&#25105;&#20204;&#23558;&#33402;&#26415;&#35774;&#35745;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#26799;&#24230;&#21644;&#36138;&#23146;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#23427;&#12290;&#25105;&#20204;&#22312;&#20132;&#36890;&#26631;&#24535;&#35782;&#21035;&#39046;&#22495;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#20854;&#33021;&#22815;&#25913;&#21464;&#20132;&#36890;&#26631;&#24535;&#20013;&#30340;&#35937;&#24418;&#22270;&#26631;&#65288;&#21363;&#26631;&#24535;&#20869;&#30340;&#31526;&#21495;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial examples arose as a challenge for machine learning. To hinder them, most defenses alter how models are trained (e.g., adversarial training) or inference is made (e.g., randomized smoothing). Still, while these approaches markedly improve models' adversarial robustness, models remain highly susceptible to adversarial examples. Identifying that, in certain domains such as traffic-sign recognition, objects are implemented per standards specifying how artifacts (e.g., signs) should be designed, we propose a novel approach for improving adversarial robustness. Specifically, we offer a method to redefine standards, making minor changes to existing ones, to defend against adversarial examples. We formulate the problem of artifact design as a robust optimization problem, and propose gradient-based and greedy search methods to solve it. We evaluated our approach in the domain of traffic-sign recognition, allowing it to alter traffic-sign pictograms (i.e., symbols within the signs) a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#28040;&#38500;&#32593;&#32476;&#65288;GENs&#65289;&#65292;&#20854;&#36890;&#36807;&#28040;&#38500;&#37051;&#22495;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#20887;&#20313;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#28145;&#23618;&#27425;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;GENs&#21487;&#20197;&#22686;&#24378;&#33410;&#28857;&#23545;&#36828;&#36317;&#31163;&#37051;&#22495;&#30340;&#24863;&#30693;&#65292;&#24182;&#25193;&#23637;&#32593;&#32476;&#20256;&#25773;&#30340;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.01233</link><description>&lt;p&gt;
&#22270;&#28040;&#38500;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Graph Elimination Networks. (arXiv:2401.01233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22270;&#28040;&#38500;&#32593;&#32476;&#65288;GENs&#65289;&#65292;&#20854;&#36890;&#36807;&#28040;&#38500;&#37051;&#22495;&#20256;&#25773;&#36807;&#31243;&#20013;&#30340;&#20887;&#20313;&#26469;&#35299;&#20915;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#28145;&#23618;&#27425;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#38382;&#39064;&#12290;GENs&#21487;&#20197;&#22686;&#24378;&#33410;&#28857;&#23545;&#36828;&#36317;&#31163;&#37051;&#22495;&#30340;&#24863;&#30693;&#65292;&#24182;&#25193;&#23637;&#32593;&#32476;&#20256;&#25773;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#20294;&#22312;&#28145;&#23618;&#27425;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#29616;&#26377;&#30740;&#31350;&#36890;&#24120;&#23558;&#36825;&#20010;&#38382;&#39064;&#24402;&#22240;&#20110;&#33410;&#28857;&#36807;&#24230;&#24179;&#28369;&#65292;&#21363;&#22312;&#22810;&#36718;&#20256;&#25773;&#20043;&#21518;&#65292;&#33410;&#28857;&#34920;&#31034;&#21464;&#24471;&#26080;&#27861;&#21306;&#20998;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;GNN&#30340;&#37051;&#22495;&#20256;&#25773;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;GNN&#22312;&#28145;&#23618;&#27425;&#19978;&#24615;&#33021;&#19979;&#38477;&#30340;&#30495;&#27491;&#26681;&#26412;&#21407;&#22240;&#22312;&#20110;&#37051;&#22495;&#29305;&#24449;&#20256;&#25773;&#30340;&#26080;&#25928;&#24615;&#12290;&#36825;&#31181;&#20256;&#25773;&#22312;&#27599;&#19968;&#27493;&#20256;&#25773;&#20013;&#23548;&#33268;&#33410;&#28857;&#24403;&#21069;&#34920;&#31034;&#30340;&#25351;&#25968;&#22686;&#38271;&#65292;&#20351;&#24471;&#25429;&#25417;&#38271;&#36317;&#31163;&#33410;&#28857;&#20043;&#38388;&#30340;&#26377;&#20215;&#20540;&#20381;&#36182;&#20851;&#31995;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22270;&#28040;&#38500;&#32593;&#32476;&#65288;GENs&#65289;&#65292;&#23427;&#20351;&#29992;&#19968;&#31181;&#29305;&#23450;&#30340;&#31639;&#27861;&#22312;&#37051;&#22495;&#20256;&#25773;&#36807;&#31243;&#20013;&#28040;&#38500;&#20887;&#20313;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GENs&#21487;&#20197;&#22686;&#24378;&#33410;&#28857;&#23545;&#36828;&#36317;&#31163;&#37051;&#22495;&#30340;&#24863;&#30693;&#65292;&#24182;&#25193;&#23637;&#32593;&#32476;&#20256;&#25773;&#30340;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are widely applied across various domains, yet they perform poorly in deep layers. Existing research typically attributes this problem to node over-smoothing, where node representations become indistinguishable after multiple rounds of propagation. In this paper, we delve into the neighborhood propagation mechanism of GNNs and discover that the real root cause of GNNs' performance degradation in deep layers lies in ineffective neighborhood feature propagation. This propagation leads to an exponential growth of a node's current representation at every propagation step, making it extremely challenging to capture valuable dependencies between long-distance nodes. To address this issue, we introduce Graph Elimination Networks (GENs), which employ a specific algorithm to eliminate redundancies during neighborhood propagation. We demonstrate that GENs can enhance nodes' perception of distant neighborhoods and extend the depth of network propagation. Extensive exp
&lt;/p&gt;</description></item><item><title>ODTlearn&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#21644;&#22788;&#26041;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#38382;&#39064;&#21644;&#31639;&#27861;&#30340;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2307.15691</link><description>&lt;p&gt;
ODTlearn: &#19968;&#20010;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#21644;&#22788;&#26041;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#30340;&#21253;
&lt;/p&gt;
&lt;p&gt;
ODTlearn: A Package for Learning Optimal Decision Trees for Prediction and Prescription. (arXiv:2307.15691v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.15691
&lt;/p&gt;
&lt;p&gt;
ODTlearn&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#29992;&#20110;&#23398;&#20064;&#39044;&#27979;&#21644;&#22788;&#26041;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#12290;&#23427;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21270;&#26041;&#27861;&#65292;&#24182;&#25903;&#25345;&#21508;&#31181;&#38382;&#39064;&#21644;&#31639;&#27861;&#30340;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ODTLearn&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#21253;&#65292;&#25552;&#20379;&#20102;&#22522;&#20110;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;(MIO)&#26694;&#26550;&#30340;&#39640;&#39118;&#38505;&#39044;&#27979;&#21644;&#22788;&#26041;&#20219;&#21153;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#23398;&#20064;&#26041;&#27861;&#12290;&#35813;&#21253;&#30340;&#24403;&#21069;&#29256;&#26412;&#25552;&#20379;&#20102;&#23398;&#20064;&#26368;&#20248;&#20998;&#31867;&#26641;&#12289;&#20844;&#24179;&#26368;&#20248;&#20998;&#31867;&#26641;&#12289;&#40065;&#26834;&#26368;&#20248;&#20998;&#31867;&#26641;&#21644;&#20174;&#35266;&#27979;&#25968;&#25454;&#23398;&#20064;&#26368;&#20248;&#22788;&#26041;&#26641;&#30340;&#23454;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#35813;&#21253;&#20197;&#20415;&#20110;&#32500;&#25252;&#21644;&#25193;&#23637;&#65292;&#24403;&#24341;&#20837;&#26032;&#30340;&#26368;&#20248;&#20915;&#31574;&#26641;&#38382;&#39064;&#31867;&#12289;&#37325;&#26500;&#31574;&#30053;&#21644;&#35299;&#20915;&#31639;&#27861;&#26102;&#65292;&#21487;&#20197;&#36731;&#26494;&#26356;&#26032;&#12290;&#20026;&#27492;&#65292;&#35813;&#21253;&#36981;&#24490;&#38754;&#21521;&#23545;&#35937;&#30340;&#35774;&#35745;&#21407;&#21017;&#65292;&#24182;&#25903;&#25345;&#21830;&#19994;(Gurobi)&#21644;&#24320;&#28304;(COIN-OR branch and cut)&#27714;&#35299;&#22120;&#12290;&#21253;&#30340;&#25991;&#26723;&#21644;&#35814;&#32454;&#29992;&#25143;&#25351;&#21335;&#21487;&#20197;&#22312;https://d3m-research-group.github.io/odtlearn/&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
ODTLearn is an open-source Python package that provides methods for learning optimal decision trees for high-stakes predictive and prescriptive tasks based on the mixed-integer optimization (MIO) framework proposed in Aghaei et al. (2019) and several of its extensions. The current version of the package provides implementations for learning optimal classification trees, optimal fair classification trees, optimal classification trees robust to distribution shifts, and optimal prescriptive trees from observational data. We have designed the package to be easy to maintain and extend as new optimal decision tree problem classes, reformulation strategies, and solution algorithms are introduced. To this end, the package follows object-oriented design principles and supports both commercial (Gurobi) and open source (COIN-OR branch and cut) solvers. The package documentation and an extensive user guide can be found at https://d3m-research-group.github.io/odtlearn/. Additionally, users can view
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#25928;&#29992;Q&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#24739;&#32773;&#20559;&#22909;&#32435;&#20837;&#22797;&#21512;&#32467;&#26524;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#26102;&#38388;&#28857;&#21644;&#32467;&#26524;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12022</link><description>&lt;p&gt;
&#23558;&#24739;&#32773;&#20559;&#22909;&#32435;&#20837;Q&#23398;&#20064;&#30340;&#28789;&#27963;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Flexible Framework for Incorporating Patient Preferences Into Q-Learning. (arXiv:2307.12022v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12022
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#25928;&#29992;Q&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#23558;&#24739;&#32773;&#20559;&#22909;&#32435;&#20837;&#22797;&#21512;&#32467;&#26524;&#30340;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696;&#20013;&#65292;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#23545;&#26102;&#38388;&#28857;&#21644;&#32467;&#26524;&#25968;&#37327;&#30340;&#38480;&#21046;&#65292;&#33021;&#22815;&#23454;&#29616;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21307;&#30103;&#38382;&#39064;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22810;&#20010;&#31454;&#20105;&#24615;&#30340;&#20851;&#27880;&#28857;&#65292;&#22914;&#27835;&#30103;&#30103;&#25928;&#21644;&#21103;&#20316;&#29992;&#20005;&#37325;&#31243;&#24230;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#20272;&#35745;&#21160;&#24577;&#27835;&#30103;&#26041;&#26696; (DTRs) &#30340;&#32479;&#35745;&#26041;&#27861;&#36890;&#24120;&#20551;&#35774;&#21482;&#26377;&#19968;&#20010;&#20851;&#27880;&#28857;&#65292;&#32780;&#22788;&#29702;&#22797;&#21512;&#32467;&#26524;&#30340;&#26041;&#27861;&#24456;&#23569;&#65292;&#23384;&#22312;&#37325;&#35201;&#38480;&#21046;&#65292;&#21253;&#25324;&#23545;&#21333;&#20010;&#26102;&#38388;&#28857;&#21644;&#20004;&#20010;&#32467;&#26524;&#30340;&#38480;&#21046;&#12289;&#26080;&#27861;&#32435;&#20837;&#24739;&#32773;&#30340;&#33258;&#36848;&#20559;&#22909;&#20197;&#21450;&#26377;&#38480;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#28508;&#22312;&#25928;&#29992;Q&#23398;&#20064;(LUQ-Learning)&#12290;LUQ-Learning&#37319;&#29992;&#28508;&#22312;&#27169;&#22411;&#26041;&#27861;&#65292;&#33258;&#28982;&#22320;&#23558;Q&#23398;&#20064;&#25193;&#23637;&#21040;&#22797;&#21512;&#32467;&#26524;&#35774;&#32622;&#65292;&#24182;&#20026;&#27599;&#20010;&#24739;&#32773;&#36873;&#25321;&#29702;&#24819;&#30340;&#32467;&#26524;&#26435;&#34913;&#12290;&#19982;&#20043;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20801;&#35768;&#20219;&#24847;&#25968;&#37327;&#30340;&#26102;&#38388;&#28857;&#21644;&#32467;&#26524;&#65292;&#32435;&#20837;&#38472;&#36848;&#30340;&#20559;&#22909;&#65292;&#24182;&#23454;&#29616;&#24378;&#22823;&#30340;&#28176;&#36817;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world healthcare problems, there are often multiple competing outcomes of interest, such as treatment efficacy and side effect severity. However, statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a single outcome of interest, and the few methods that deal with composite outcomes suffer from important limitations. This includes restrictions to a single time point and two outcomes, the inability to incorporate self-reported patient preferences and limited theoretical guarantees. To this end, we propose a new method to address these limitations, which we dub Latent Utility Q-Learning (LUQ-Learning). LUQ-Learning uses a latent model approach to naturally extend Q-learning to the composite outcome setting and adopt the ideal trade-off between outcomes to each patient. Unlike previous approaches, our framework allows for an arbitrary number of time points and outcomes, incorporates stated preferences and achieves strong asymptotic performance with rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-&#24352;&#37327;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#29305;&#24449;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;</title><link>http://arxiv.org/abs/2306.06534</link><description>&lt;p&gt;
K-Tensors&#65306;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06534
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#27491;&#21322;&#23450;&#30697;&#38453;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-&#24352;&#37327;&#65289;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#29305;&#24449;&#32467;&#26500;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#19968;&#33268;&#24615;&#32858;&#31867;&#31639;&#27861;&#65288;K-Tensors&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#23427;&#20204;&#30340;&#29305;&#24449;&#32467;&#26500;&#23558;&#27491;&#21322;&#23450;&#30697;&#38453;&#36827;&#34892;&#20998;&#21306;&#12290;&#30001;&#20110;&#27491;&#21322;&#23450;&#30697;&#38453;&#21487;&#20197;&#22312; p&#8805;2 &#30340;&#31354;&#38388;&#20013;&#34920;&#31034;&#20026;&#26925;&#29699;&#20307;&#65292;&#22240;&#27492;&#20445;&#25345;&#23427;&#20204;&#30340;&#32467;&#26500;&#20449;&#24687;&#20197;&#36827;&#34892;&#26377;&#25928;&#30340;&#32858;&#31867;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#30697;&#38453;&#32858;&#31867;&#31639;&#27861;&#24120;&#24120;&#28041;&#21450;&#23558;&#30697;&#38453;&#21521;&#37327;&#21270;&#65292;&#23548;&#33268;&#20851;&#38190;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27491;&#21322;&#23450;&#30697;&#38453;&#32467;&#26500;&#20449;&#24687;&#30340;&#36317;&#31163;&#24230;&#37327;&#26469;&#36827;&#34892;&#32858;&#31867;&#12290;&#36825;&#31181;&#36317;&#31163;&#24230;&#37327;&#20351;&#24471;&#32858;&#31867;&#31639;&#27861;&#33021;&#22815;&#32771;&#34385;&#27491;&#21322;&#23450;&#30697;&#38453;&#19982;&#23427;&#20204;&#22312;&#30001;&#19968;&#32452;&#27491;&#21322;&#23450;&#30697;&#38453;&#23450;&#20041;&#30340;&#27491;&#20132;&#21521;&#37327;&#24352;&#25104;&#30340;&#20849;&#21516;&#31354;&#38388;&#19978;&#30340;&#25237;&#24433;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#32858;&#31867;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel self-consistency clustering algorithm ($K$-Tensors) designed for {partitioning a distribution of} positive-semidefinite matrices based on their eigenstructures. As positive semi-definite matrices can be represented as ellipsoids in $\Re^p$, $p \ge 2$, it is critical to maintain their structural information to perform effective clustering. However, traditional clustering algorithms {applied to matrices} often {involve vectorization of} the matrices, resulting in a loss of essential structural information. To address this issue, we propose a distance metric {for clustering} that is specifically based on the structural information of positive semi-definite matrices. This distance metric enables the clustering algorithm to consider the differences between positive semi-definite matrices and their projections onto {a} common space spanned by \thadJulyTen{orthonormal vectors defined from a set of} positive semi-definite matrices. This innovative approach to clus
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25193;&#25955;&#21644;&#38543;&#26426;&#23450;&#20301;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#26631;&#20934;&#21435;&#22122;&#25193;&#25955;&#26159;&#19968;&#31181;&#38543;&#26426;&#23450;&#20301;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25968;&#27493;&#39588;&#20869;&#20174; Ising &#27169;&#22411;&#30340; Gibbs &#27979;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10690</link><description>&lt;p&gt;
&#37319;&#26679;&#65292;&#25193;&#25955;&#21644;&#38543;&#26426;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Sampling, Diffusions, and Stochastic Localization. (arXiv:2305.10690v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10690
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#25193;&#25955;&#21644;&#38543;&#26426;&#23450;&#20301;&#30340;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#26631;&#20934;&#21435;&#22122;&#25193;&#25955;&#26159;&#19968;&#31181;&#38543;&#26426;&#23450;&#20301;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23545;&#25968;&#27493;&#39588;&#20869;&#20174; Ising &#27169;&#22411;&#30340; Gibbs &#27979;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#26159;&#20174;&#39640;&#32500;&#20998;&#24067;&#20013;&#25277;&#26679;&#30340;&#25104;&#21151;&#25216;&#26415;&#65292;&#21487;&#20197;&#26126;&#30830;&#32473;&#20986;&#25110;&#20174;&#26679;&#26412;&#38598;&#20013;&#23398;&#20064;&#12290;&#23427;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#25193;&#25955;&#36807;&#31243;&#65292;&#20854;&#31471;&#28857;&#26159;&#30446;&#26631;&#20998;&#24067;&#30340;&#26679;&#26412;&#65292;&#28418;&#31227;&#36890;&#24120;&#34920;&#31034;&#20026;&#31070;&#32463;&#32593;&#32476;&#12290;&#38543;&#26426;&#23450;&#20301;&#26159;&#22312;&#39640;&#32500;&#20013;&#35777;&#26126;&#39532;&#23572;&#31185;&#22827;&#38142;&#21644;&#20854;&#20182;&#20989;&#25968;&#19981;&#31561;&#24335;&#28151;&#21512;&#30340;&#25104;&#21151;&#25216;&#26415;&#12290;[EAMS2022]&#20013;&#24341;&#20837;&#20102;&#38543;&#26426;&#23450;&#20301;&#30340;&#31639;&#27861;&#29256;&#26412;&#65292;&#20197;&#33719;&#24471;&#20174;&#26576;&#20123;&#32479;&#35745;&#21147;&#23398;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#31639;&#27861;&#12290;&#26412;&#25991;&#26377;&#19977;&#20010;&#30446;&#26631;&#65306;&#65288;i&#65289;&#23558;[EAMS2022]&#30340;&#26500;&#36896;&#25512;&#24191;&#21040;&#20854;&#20182;&#38543;&#26426;&#23450;&#20301;&#36807;&#31243;&#65307;&#65288;ii&#65289;&#28548;&#28165;&#25193;&#25955;&#21644;&#38543;&#26426;&#23450;&#20301;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26631;&#20934;&#21435;&#22122;&#25193;&#25955;&#26159;&#38543;&#26426;&#23450;&#20301;&#65292;&#20294;&#20854;&#20182;&#36890;&#36807;&#25152;&#25552;&#20986;&#30340;&#35270;&#35282;&#33258;&#28982;&#25552;&#20986;&#30340;&#31034;&#20363;&#65307;&#65288;iii&#65289;&#25551;&#36848;&#20174;&#36825;&#31181;&#32852;&#31995;&#20013;&#24471;&#20986;&#30340;&#19968;&#20123;&#35265;&#35299;&#65307;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#23545;&#25968;&#27493;&#39588;&#20869;&#20174; Ising &#27169;&#22411;&#30340; Gibbs &#27979;&#24230;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusions are a successful technique to sample from high-dimensional distributions can be either explicitly given or learnt from a collection of samples. They implement a diffusion process whose endpoint is a sample from the target distribution and whose drift is typically represented as a neural network. Stochastic localization is a successful technique to prove mixing of Markov Chains and other functional inequalities in high dimension. An algorithmic version of stochastic localization was introduced in [EAMS2022], to obtain an algorithm that samples from certain statistical mechanics models.  This notes have three objectives: (i) Generalize the construction [EAMS2022] to other stochastic localization processes; (ii) Clarify the connection between diffusions and stochastic localization. In particular we show that standard denoising diffusions are stochastic localizations but other examples that are naturally suggested by the proposed viewpoint; (iii) Describe some insights that foll
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.04228</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification. (arXiv:2305.04228v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20998;&#31867;&#26159;&#31243;&#24207;&#29702;&#35299;&#21644;&#33258;&#21160;&#32534;&#30721;&#20013;&#30340;&#19968;&#20010;&#38590;&#39064;&#12290;&#30001;&#20110;&#31243;&#24207;&#30340;&#27169;&#31946;&#35821;&#27861;&#21644;&#22797;&#26434;&#35821;&#20041;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25216;&#26415;&#21019;&#24314;&#20195;&#30721;&#34920;&#31034;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;&#12290;&#36825;&#20123;&#25216;&#26415;&#21033;&#29992;&#20195;&#30721;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#21482;&#32771;&#34385;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;AST&#20013;&#33410;&#28857;&#20043;&#38388;&#24050;&#32463;&#23384;&#22312;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#20195;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#65288;HDHG&#65289;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HDHGN&#65289;&#22788;&#29702;&#22270;&#24418;&#12290;HDHG&#20445;&#30041;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#24182;&#26356;&#20840;&#38754;&#22320;&#32534;&#30721;&#20102;AST&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;HDHGN&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#33410;&#28857;&#30340;&#29305;&#24449;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#22788;&#29702;&#26469;&#23545;AST&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HDHG&#21644;HDHGN&#22312;&#20195;&#30721;&#20998;&#31867;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural network (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order correlations that already exist between nodes in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose to represent AST as a heterogeneous directed hypergraph (HDHG) and process the graph by hetero
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FeDepth&#30340;&#20869;&#23384;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20869;&#23384;&#39044;&#31639;&#23558;&#23436;&#25972;&#27169;&#22411;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#25104;&#22359;&#65292;&#24182;&#20381;&#27425;&#35757;&#32451;&#36825;&#20123;&#22359;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#35774;&#22791;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.04887</link><description>&lt;p&gt;
&#21487;&#21464;&#28145;&#24230;&#24322;&#26500;&#32852;&#37030;&#23398;&#20064;&#30340;&#20869;&#23384;&#33258;&#36866;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Memory-adaptive Depth-wise Heterogenous Federated Learning. (arXiv:2303.04887v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04887
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FeDepth&#30340;&#20869;&#23384;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20869;&#23384;&#39044;&#31639;&#23558;&#23436;&#25972;&#27169;&#22411;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#25104;&#22359;&#65292;&#24182;&#20381;&#27425;&#35757;&#32451;&#36825;&#20123;&#22359;&#65292;&#20197;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#35774;&#22791;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#33539;&#24335;&#65292;&#20801;&#35768;&#22810;&#20010;&#23458;&#25143;&#31471;&#22312;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21327;&#21516;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23384;&#22312;&#24322;&#26500;&#35774;&#22791;&#65292;&#22914;&#25163;&#26426;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#30340;&#20869;&#23384;&#33021;&#21147;&#19981;&#21516;&#65292;&#20250;&#38480;&#21046;&#27169;&#22411;&#33021;&#22815;&#35757;&#32451;&#30340;&#35268;&#27169;&#21644;&#24615;&#33021;&#12290;&#20027;&#35201;&#35299;&#20915;&#20869;&#23384;&#38480;&#21046;&#30340;&#26041;&#27861;&#38598;&#20013;&#22312;&#20943;&#23569;&#23485;&#24230;&#30340;&#25216;&#26415;&#19978;&#65292;&#21363;&#19981;&#21516;&#23458;&#25143;&#31471;&#22312;&#26412;&#22320;&#35757;&#32451;&#20943;&#23485;&#24230;&#30340;&#23376;&#32593;&#32476;&#65292;&#28982;&#21518;&#26381;&#21153;&#22120;&#32858;&#21512;&#36825;&#20123;&#23376;&#32593;&#32476;&#12290;&#30001;&#20110;&#22788;&#29702;&#32858;&#21512;&#38454;&#27573;&#20013;&#19981;&#21516;&#23376;&#32593;&#32476;&#23485;&#24230;&#21464;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#36825;&#20123;&#26041;&#27861;&#20135;&#29983;&#30340;&#20840;&#23616;&#27169;&#22411;&#20250;&#21463;&#21040;&#24615;&#33021;&#30340;&#38477;&#20302;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;FeDepth&#30340;&#20869;&#23384;&#33258;&#36866;&#24212;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#26681;&#25454;&#27599;&#20010;&#23458;&#25143;&#31471;&#30340;&#20869;&#23384;&#39044;&#31639;&#23558;&#23436;&#25972;&#27169;&#22411;&#33258;&#36866;&#24212;&#22320;&#20998;&#35299;&#25104;&#22359;&#65292;&#24182;&#20381;&#27425;&#35757;&#32451;&#36825;&#20123;&#22359;&#65292;&#20197;&#33719;&#21462;&#26356;&#22909;&#30340;&#24615;&#33021;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obt
&lt;/p&gt;</description></item></channel></rss>