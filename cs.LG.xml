<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;&#21644;&#26368;&#23567;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#21487;&#20197;&#34920;&#31034;&#20026;&#32447;&#24615;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.03353</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#20551;&#35774;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Hypothesis Spaces for Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03353
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65292;&#30740;&#31350;&#20102;&#27491;&#21017;&#21270;&#23398;&#20064;&#21644;&#26368;&#23567;&#25554;&#20540;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#21487;&#20197;&#34920;&#31034;&#20026;&#32447;&#24615;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24212;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#28145;&#24230;&#23398;&#20064;&#20551;&#35774;&#31354;&#38388;&#12290;&#36890;&#36807;&#23558;DNN&#35270;&#20026;&#20004;&#20010;&#21464;&#37327;&#30340;&#20989;&#25968;&#65292;&#21363;&#29289;&#29702;&#21464;&#37327;&#21644;&#21442;&#25968;&#21464;&#37327;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;DNNs&#30340;&#21407;&#22987;&#38598;&#21512;&#65292;&#21442;&#25968;&#21464;&#37327;&#20301;&#20110;&#30001;DNNs&#30340;&#26435;&#37325;&#30697;&#38453;&#21644;&#20559;&#32622;&#20915;&#23450;&#30340;&#19968;&#32452;&#28145;&#24230;&#21644;&#23485;&#24230;&#20013;&#12290;&#28982;&#21518;&#22312;&#24369;*&#25299;&#25169;&#20013;&#23436;&#25104;&#21407;&#22987;DNN&#38598;&#21512;&#30340;&#32447;&#24615;&#36328;&#24230;&#65292;&#20197;&#26500;&#24314;&#19968;&#20010;&#29289;&#29702;&#21464;&#37327;&#20989;&#25968;&#30340;Banach&#31354;&#38388;&#12290;&#25105;&#20204;&#35777;&#26126;&#25152;&#26500;&#36896;&#30340;Banach&#31354;&#38388;&#26159;&#19968;&#20010;&#20877;&#29983;&#26680;Banach&#31354;&#38388;&#65288;RKBS&#65289;&#65292;&#24182;&#26500;&#36896;&#20854;&#20877;&#29983;&#26680;&#12290;&#36890;&#36807;&#20026;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#24314;&#31435;&#34920;&#36798;&#23450;&#29702;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#23398;&#20064;&#27169;&#22411;&#65292;&#27491;&#21017;&#21270;&#23398;&#20064;&#21644;&#26368;&#23567;&#25554;&#20540;&#38382;&#39064;&#22312;&#32467;&#26524;RKBS&#20013;&#12290;&#34920;&#36798;&#23450;&#29702;&#25581;&#31034;&#20102;&#36825;&#20123;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#21487;&#20197;&#34920;&#31034;&#20026;&#32447;&#24615;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03353v1 Announce Type: cross  Abstract: This paper introduces a hypothesis space for deep learning that employs deep neural networks (DNNs). By treating a DNN as a function of two variables, the physical variable and parameter variable, we consider the primitive set of the DNNs for the parameter variable located in a set of the weight matrices and biases determined by a prescribed depth and widths of the DNNs. We then complete the linear span of the primitive DNN set in a weak* topology to construct a Banach space of functions of the physical variable. We prove that the Banach space so constructed is a reproducing kernel Banach space (RKBS) and construct its reproducing kernel. We investigate two learning models, regularized learning and minimum interpolation problem in the resulting RKBS, by establishing representer theorems for solutions of the learning models. The representer theorems unfold that solutions of these learning models can be expressed as linear combination of
&lt;/p&gt;</description></item><item><title>&#20248;&#21270;&#30340;Transformer&#27169;&#22411;DistilBERT&#29992;&#20110;&#26816;&#27979;&#38035;&#40060;&#37038;&#20214;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#25216;&#26415;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;</title><link>https://arxiv.org/abs/2402.13871</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22522;&#20110;Transformer&#30340;&#38035;&#40060;&#37038;&#20214;&#26816;&#27979;&#27169;&#22411;&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Explainable Transformer-based Model for Phishing Email Detection: A Large Language Model Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13871
&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#30340;Transformer&#27169;&#22411;DistilBERT&#29992;&#20110;&#26816;&#27979;&#38035;&#40060;&#37038;&#20214;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#25216;&#26415;&#35299;&#20915;&#20102;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38035;&#40060;&#37038;&#20214;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#32593;&#32476;&#23041;&#32961;&#65292;&#35797;&#22270;&#36890;&#36807;&#21457;&#36865;&#34394;&#20551;&#37038;&#20214;&#26469;&#27450;&#39575;&#29992;&#25143;&#65292;&#24847;&#22270;&#26159;&#31363;&#21462;&#26426;&#23494;&#20449;&#24687;&#25110;&#36896;&#25104;&#36130;&#21153;&#25439;&#22833;&#12290;&#25915;&#20987;&#32773;&#24120;&#24120;&#20882;&#20805;&#21487;&#20449;&#23454;&#20307;&#65292;&#21033;&#29992;&#25216;&#26415;&#36827;&#27493;&#21644;&#22797;&#26434;&#24615;&#20351;&#24471;&#38035;&#40060;&#30340;&#26816;&#27979;&#21644;&#39044;&#38450;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#23613;&#31649;&#36827;&#34892;&#20102;&#22823;&#37327;&#23398;&#26415;&#30740;&#31350;&#65292;&#20294;&#38035;&#40060;&#37038;&#20214;&#26816;&#27979;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20173;&#28982;&#26159;&#19968;&#20010;&#25345;&#32493;&#19988;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#25513;&#30422;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#25317;&#26377;&#24040;&#22823;&#28508;&#21147;&#65292;&#33021;&#22815;&#25552;&#20379;&#21019;&#26032;&#35299;&#20915;&#26041;&#26696;&#26469;&#35299;&#20915;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#30740;&#31350;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;&#12289;&#32463;&#36807;&#24494;&#35843;&#30340;&#22522;&#20110;Transformer&#30340;DistilBERT&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#38035;&#40060;&#37038;&#20214;&#12290;&#22312;&#26816;&#27979;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#32452;&#38035;&#40060;&#37038;&#20214;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#39044;&#22788;&#29702;&#25216;&#26415;&#26469;&#28165;&#29702;&#21644;&#35299;&#20915;&#31867;&#21035;&#19981;&#24179;&#34913;&#38382;&#39064;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#23454;&#39564;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13871v1 Announce Type: cross  Abstract: Phishing email is a serious cyber threat that tries to deceive users by sending false emails with the intention of stealing confidential information or causing financial harm. Attackers, often posing as trustworthy entities, exploit technological advancements and sophistication to make detection and prevention of phishing more challenging. Despite extensive academic research, phishing detection remains an ongoing and formidable challenge in the cybersecurity landscape. Large Language Models (LLMs) and Masked Language Models (MLMs) possess immense potential to offer innovative solutions to address long-standing challenges. In this research paper, we present an optimized, fine-tuned transformer-based DistilBERT model designed for the detection of phishing emails. In the detection process, we work with a phishing email dataset and utilize the preprocessing techniques to clean and solve the imbalance class issues. Through our experiments, 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24352;&#37327;&#25918;&#26494;&#26041;&#27861;(CTRA)&#65292;&#29992;&#20110;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23547;&#25214;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;CTRA&#36890;&#36807;&#23545;&#31163;&#25955;&#20915;&#31574;&#21464;&#37327;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#35299;&#20915;&#20102;&#23547;&#25214;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.02190</link><description>&lt;p&gt;
&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23547;&#25214;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#36830;&#32493;&#24352;&#37327;&#25918;&#26494;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continuous Tensor Relaxation for Finding Diverse Solutions in Combinatorial Optimization Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02190
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36830;&#32493;&#24352;&#37327;&#25918;&#26494;&#26041;&#27861;(CTRA)&#65292;&#29992;&#20110;&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#23547;&#25214;&#22810;&#26679;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;CTRA&#36890;&#36807;&#23545;&#31163;&#25955;&#20915;&#31574;&#21464;&#37327;&#36827;&#34892;&#36830;&#32493;&#25918;&#26494;&#65292;&#35299;&#20915;&#20102;&#23547;&#25214;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#20013;&#65292;&#23547;&#25214;&#26368;&#20339;&#35299;&#26159;&#26368;&#24120;&#35265;&#30340;&#30446;&#26631;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#21487;&#33021;&#19981;&#36866;&#29992;&#65292;&#22240;&#20026;&#30446;&#26631;&#20989;&#25968;&#21644;&#32422;&#26463;&#26465;&#20214;&#21482;&#26159;&#21407;&#22987;&#29616;&#23454;&#19990;&#30028;&#24773;&#20917;&#30340;&#36817;&#20284;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#23547;&#25214;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#21644;&#32422;&#26463;&#20005;&#37325;&#24615;&#30340;&#21464;&#21270;&#25104;&#20026;&#33258;&#28982;&#30340;&#26041;&#21521;&#12290;&#36825;&#31181;&#31574;&#30053;&#25552;&#20379;&#20102;&#22312;&#21518;&#22788;&#29702;&#36807;&#31243;&#20013;&#36873;&#25321;&#21512;&#36866;&#35299;&#20915;&#26041;&#26696;&#30340;&#28789;&#27963;&#24615;&#12290;&#28982;&#32780;&#65292;&#21457;&#29616;&#36825;&#20123;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#27604;&#30830;&#23450;&#21333;&#19968;&#35299;&#20915;&#26041;&#26696;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#36830;&#32493;&#24352;&#37327;&#26494;&#24347;&#36864;&#28779; (CTRA) &#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#26080;&#30417;&#30563;&#23398;&#20064;&#30340;&#32452;&#21512;&#20248;&#21270;&#27714;&#35299;&#22120;&#12290;CTRA&#36890;&#36807;&#25193;&#23637;&#36830;&#32493;&#26494;&#24347;&#26041;&#27861;&#65292;&#23558;&#31163;&#25955;&#20915;&#31574;&#21464;&#37327;&#36716;&#25442;&#20026;&#36830;&#32493;&#24352;&#37327;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#22810;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#25214;&#21040;&#20102;&#19981;&#21516;&#29305;&#24449;&#30340;&#22810;&#26679;&#21270;&#35299;&#20915;&#26041;&#26696;&#21644;&#32422;&#26463;&#20005;&#37325;&#24615;&#30340;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the best solution is the most common objective in combinatorial optimization (CO) problems. However, a single solution may not be suitable in practical scenarios, as the objective functions and constraints are only approximations of original real-world situations. To tackle this, finding (i) "heterogeneous solutions", diverse solutions with distinct characteristics, and (ii) "penalty-diversified solutions", variations in constraint severity, are natural directions. This strategy provides the flexibility to select a suitable solution during post-processing. However, discovering these diverse solutions is more challenging than identifying a single solution. To overcome this challenge, this study introduces Continual Tensor Relaxation Annealing (CTRA) for unsupervised-learning-based CO solvers. CTRA addresses various problems simultaneously by extending the continual relaxation approach, which transforms discrete decision variables into continual tensors. This method finds heterog
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#21270;&#30340;&#25512;&#23548;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#22320;&#24809;&#32602;&#22833;&#36133;&#27169;&#24335;&#30340;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#26550;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2401.00873</link><description>&lt;p&gt;
&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Unification of Self-Supervised Clustering and Energy-Based Models. (arXiv:2401.00873v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.00873
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#29992;&#36125;&#21494;&#26031;&#26041;&#27861;&#32479;&#19968;&#33258;&#30417;&#30563;&#32858;&#31867;&#21644;&#33021;&#37327;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26631;&#20934;&#21270;&#30340;&#25512;&#23548;&#26041;&#27861;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21487;&#38752;&#22320;&#24809;&#32602;&#22833;&#36133;&#27169;&#24335;&#30340;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#26550;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#27969;&#34892;&#19988;&#24378;&#22823;&#30340;&#26041;&#27861;&#65292;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#21508;&#31181;&#35757;&#32451;&#30446;&#26631;&#12290;&#26412;&#30740;&#31350;&#23545;&#26368;&#20808;&#36827;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30446;&#26631;&#36827;&#34892;&#36125;&#21494;&#26031;&#20998;&#26512;&#65292;&#38416;&#26126;&#20102;&#27599;&#20010;&#31867;&#21035;&#20013;&#28508;&#22312;&#30340;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#22522;&#26412;&#21407;&#29702;&#20986;&#21457;&#25512;&#23548;&#36825;&#20123;&#27169;&#22411;&#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#20998;&#26512;&#36824;&#34920;&#26126;&#20102;&#23558;&#33258;&#30417;&#30563;&#23398;&#20064;&#19982;&#22522;&#20110;&#20284;&#28982;&#30340;&#29983;&#25104;&#27169;&#22411;&#33258;&#28982;&#25972;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#32858;&#31867;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#33021;&#37327;&#27169;&#22411;&#39046;&#22495;&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#27010;&#24565;&#65292;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#19979;&#30028;&#65292;&#32463;&#35777;&#26126;&#33021;&#21487;&#38752;&#22320;&#24809;&#32602;&#26368;&#37325;&#35201;&#30340;&#22833;&#36133;&#27169;&#24335;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#26032;&#25552;&#20986;&#30340;&#19979;&#30028;&#20351;&#24471;&#33021;&#22815;&#35757;&#32451;&#19968;&#20010;&#26631;&#20934;&#30340;&#39592;&#24178;&#26550;&#26500;&#65292;&#32780;&#26080;&#38656;&#20351;&#29992;&#35832;&#22914;&#20572;&#27490;&#26799;&#24230;&#12289;&#21160;&#37327;&#32534;&#30721;&#22120;&#25110;&#19987;&#38376;&#30340;&#32858;&#31867;&#31561;&#38750;&#23545;&#31216;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning is a popular and powerful method for utilizing large amounts of unlabeled data, for which a wide variety of training objectives have been proposed in the literature. In this study, we perform a Bayesian analysis of state-of-the-art self-supervised learning objectives, elucidating the underlying probabilistic graphical models in each class and presenting a standardized methodology for their derivation from first principles. The analysis also indicates a natural means of integrating self-supervised learning with likelihood-based generative models. We instantiate this concept within the realm of cluster-based self-supervised learning and energy models, introducing a novel lower bound which is proven to reliably penalize the most important failure modes. Furthermore, this newly proposed lower bound enables the training of a standard backbone architecture without the necessity for asymmetric elements such as stop gradients, momentum encoders, or specialized clusteri
&lt;/p&gt;</description></item></channel></rss>