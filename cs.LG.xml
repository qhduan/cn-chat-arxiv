<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#40723;&#21169;&#22810;&#27169;&#24577;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21333;&#27169;&#24577;&#27169;&#22411;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02359</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#33539;&#24335;&#30340;&#24402;&#22240;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Attribution Regularization for Multimodal Paradigms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02359
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#40723;&#21169;&#22810;&#27169;&#24577;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#20449;&#24687;&#65292;&#20197;&#35299;&#20915;&#22810;&#27169;&#24577;&#23398;&#20064;&#20013;&#21333;&#27169;&#24577;&#27169;&#22411;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#26426;&#22120;&#23398;&#20064;&#36817;&#24180;&#26469;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#33021;&#25972;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#20197;&#22686;&#24378;&#23398;&#20064;&#21644;&#20915;&#31574;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#36890;&#24120;&#35266;&#23519;&#21040;&#21333;&#27169;&#24577;&#27169;&#22411;&#20248;&#20110;&#22810;&#27169;&#24577;&#27169;&#22411;&#65292;&#23613;&#31649;&#21518;&#32773;&#21487;&#20197;&#35775;&#38382;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#21333;&#20010;&#27169;&#24577;&#30340;&#24433;&#21709;&#24120;&#24120;&#20027;&#23548;&#20915;&#31574;&#36807;&#31243;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#36825;&#20010;&#30740;&#31350;&#39033;&#30446;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#27491;&#21017;&#21270;&#39033;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35813;&#39033;&#40723;&#21169;&#22810;&#27169;&#24577;&#27169;&#22411;&#22312;&#20570;&#20986;&#20915;&#31574;&#26102;&#26377;&#25928;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#30340;&#20449;&#24687;&#12290;&#35813;&#39033;&#30446;&#30340;&#37325;&#28857;&#22312;&#20110;&#35270;&#39057;-&#38899;&#39057;&#39046;&#22495;&#65292;&#23613;&#31649;&#25152;&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#22312;&#28041;&#21450;&#22810;&#20010;&#27169;&#24577;&#30340;&#20307;&#29616;AI&#30740;&#31350;&#20013;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#21069;&#26223;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#27491;&#21017;&#21270;&#39033;&#65292;&#25552;&#20986;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02359v1 Announce Type: new  Abstract: Multimodal machine learning has gained significant attention in recent years due to its potential for integrating information from multiple modalities to enhance learning and decision-making processes. However, it is commonly observed that unimodal models outperform multimodal models, despite the latter having access to richer information. Additionally, the influence of a single modality often dominates the decision-making process, resulting in suboptimal performance. This research project aims to address these challenges by proposing a novel regularization term that encourages multimodal models to effectively utilize information from all modalities when making decisions. The focus of this project lies in the video-audio domain, although the proposed regularization technique holds promise for broader applications in embodied AI research, where multiple modalities are involved. By leveraging this regularization term, the proposed approach
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.02353</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#22312;&#22270;&#20687;&#20013;&#36827;&#34892;&#35821;&#20041;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Semantic Augmentation in Images using Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02353
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#22823;&#35268;&#27169;&#26631;&#35760;&#25968;&#25454;&#38598;&#65292;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#29983;&#25104;&#22270;&#20687;&#22686;&#24378;&#25968;&#25454;&#38598;&#20197;&#25913;&#36827;&#27169;&#22411;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#38750;&#24120;&#24222;&#22823;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#36827;&#34892;&#30417;&#30563;&#23398;&#20064;&#65292;&#32570;&#20047;&#36825;&#20123;&#25968;&#25454;&#38598;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#24182;&#38480;&#21046;&#20854;&#27867;&#21270;&#21040;&#29616;&#23454;&#19990;&#30028;&#31034;&#20363;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#20351;&#24471;&#33021;&#22815;&#22522;&#20110;&#25991;&#26412;&#36755;&#20837;&#29983;&#25104;&#36924;&#30495;&#30340;&#22270;&#20687;&#12290;&#21033;&#29992;&#29992;&#20110;&#35757;&#32451;&#36825;&#20123;&#25193;&#25955;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#30340;&#22270;&#20687;&#26469;&#22686;&#24378;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#26377;&#25928;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#36328;&#39046;&#22495;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02353v1 Announce Type: cross  Abstract: Deep Learning models are incredibly data-hungry and require very large labeled datasets for supervised learning. As a consequence, these models often suffer from overfitting, limiting their ability to generalize to real-world examples. Recent advancements in diffusion models have enabled the generation of photorealistic images based on textual inputs. Leveraging the substantial datasets used to train these diffusion models, we propose a technique to utilize generated images to augment existing datasets. This paper explores various strategies for effective data augmentation to improve the out-of-domain generalization capabilities of deep learning models.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#20809;&#28369;&#20989;&#25968;&#21644;&#23545;&#25968;&#20985;&#25277;&#26679;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#20248;&#21270;&#21644;&#25277;&#26679;&#20013;&#24212;&#29992;&#36817;&#31471;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36817;&#31471;&#26144;&#23556;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;</title><link>https://arxiv.org/abs/2404.02239</link><description>&lt;p&gt;
&#20248;&#21270;&#21644;&#25277;&#26679;&#30340;&#36817;&#31471;&#39044;&#35328;
&lt;/p&gt;
&lt;p&gt;
Proximal Oracles for Optimization and Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02239
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38750;&#20809;&#28369;&#20989;&#25968;&#21644;&#23545;&#25968;&#20985;&#25277;&#26679;&#30340;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#22312;&#20248;&#21270;&#21644;&#25277;&#26679;&#20013;&#24212;&#29992;&#36817;&#31471;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#36817;&#31471;&#26144;&#23556;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20855;&#26377;&#38750;&#20809;&#28369;&#30446;&#26631;&#20989;&#25968;&#21644;&#23545;&#25968;&#20985;&#25277;&#26679;&#65288;&#24102;&#38750;&#20809;&#28369;&#28508;&#21183;&#65292;&#21363;&#36127;&#23545;&#25968;&#23494;&#24230;&#65289;&#30340;&#20984;&#20248;&#21270;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#20855;&#20307;&#35774;&#32622;&#65292;&#20854;&#20013;&#20984;&#30446;&#26631;/&#28508;&#21183;&#20989;&#25968;&#35201;&#20040;&#26159;&#21322;&#20809;&#28369;&#30340;&#65292;&#35201;&#20040;&#26159;&#22797;&#21512;&#24418;&#24335;&#65292;&#20316;&#20026;&#21322;&#20809;&#28369;&#20998;&#37327;&#30340;&#26377;&#38480;&#21644;&#12290;&#20026;&#20102;&#20811;&#26381;&#30001;&#20110;&#38750;&#20809;&#28369;&#24615;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#20248;&#21270;&#21644;&#25277;&#26679;&#20013;&#37319;&#29992;&#20102;&#20004;&#31181;&#24378;&#22823;&#30340;&#36817;&#31471;&#26694;&#26550;&#65306;&#20248;&#21270;&#20013;&#30340;&#36817;&#31471;&#28857;&#26694;&#26550;&#21644;&#26367;&#20195;&#25277;&#26679;&#26694;&#26550;&#65288;ASF&#65289;&#65292;&#35813;&#26694;&#26550;&#22312;&#22686;&#24191;&#20998;&#24067;&#19978;&#20351;&#29992;Gibbs&#25277;&#26679;&#12290;&#20248;&#21270;&#21644;&#25277;&#26679;&#31639;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#36890;&#36807;&#27491;&#21017;&#21270;&#20999;&#24179;&#38754;&#26041;&#27861;&#39640;&#25928;&#23454;&#29616;&#36817;&#31471;&#26144;&#23556;&#12290;&#25105;&#20204;&#22312;&#21322;&#20809;&#28369;&#21644;&#22797;&#21512;&#35774;&#32622;&#20013;&#24314;&#31435;&#20102;&#36817;&#31471;&#26144;&#23556;&#30340;&#36845;&#20195;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38750;&#20809;&#28369;&#20248;&#21270;&#30340;&#33258;&#36866;&#24212;&#36817;&#31471;&#25414;&#32465;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02239v1 Announce Type: cross  Abstract: We consider convex optimization with non-smooth objective function and log-concave sampling with non-smooth potential (negative log density). In particular, we study two specific settings where the convex objective/potential function is either semi-smooth or in composite form as the finite sum of semi-smooth components. To overcome the challenges caused by non-smoothness, our algorithms employ two powerful proximal frameworks in optimization and sampling: the proximal point framework for optimization and the alternating sampling framework (ASF) that uses Gibbs sampling on an augmented distribution. A key component of both optimization and sampling algorithms is the efficient implementation of the proximal map by the regularized cutting-plane method. We establish the iteration-complexity of the proximal map in both semi-smooth and composite settings. We further propose an adaptive proximal bundle method for non-smooth optimization. The 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#36229;&#20998;&#36776;&#29575;&#21644;&#32463;&#36807;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;YOLOv5&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33322;&#31354;&#24433;&#20687;&#20013;&#23567;&#32780;&#23494;&#38598;&#29289;&#20307;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;YOLOv5&#27169;&#22411;&#37319;&#29992;Transformer&#32534;&#30721;&#22120;&#22359;&#65292;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#39640;&#23494;&#24230;&#12289;&#36974;&#25377;&#26465;&#20214;&#19979;&#25552;&#39640;&#26816;&#27979;&#32467;&#26524;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#19981;&#20165;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#32780;&#19988;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#39640;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.14661</link><description>&lt;p&gt;
&#20174;&#27169;&#31946;&#21040;&#26126;&#20142;&#30340;&#26816;&#27979;&#65306;&#22522;&#20110;YOLOv5&#30340;&#36229;&#20998;&#36776;&#29575;&#33322;&#31354;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Blurry to Brilliant Detection: YOLOv5-Based Aerial Object Detection with Super Resolution. (arXiv:2401.14661v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14661
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36229;&#20998;&#36776;&#29575;&#21644;&#32463;&#36807;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;YOLOv5&#26550;&#26500;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#33322;&#31354;&#24433;&#20687;&#20013;&#23567;&#32780;&#23494;&#38598;&#29289;&#20307;&#26816;&#27979;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;YOLOv5&#27169;&#22411;&#37319;&#29992;Transformer&#32534;&#30721;&#22120;&#22359;&#65292;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#22312;&#39640;&#23494;&#24230;&#12289;&#36974;&#25377;&#26465;&#20214;&#19979;&#25552;&#39640;&#26816;&#27979;&#32467;&#26524;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#19981;&#20165;&#20934;&#30830;&#24615;&#26356;&#39640;&#65292;&#32780;&#19988;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#39640;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26080;&#20154;&#26426;&#21644;&#21355;&#26143;&#25216;&#26415;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23545;&#33322;&#31354;&#24433;&#20687;&#20013;&#20934;&#30830;&#29289;&#20307;&#26816;&#27979;&#30340;&#38656;&#27714;&#22823;&#22823;&#22686;&#21152;&#12290;&#20256;&#32479;&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#20559;&#21521;&#22823;&#29289;&#20307;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#23545;&#20110;&#33322;&#31354;&#22330;&#26223;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#23567;&#32780;&#23494;&#38598;&#30340;&#29289;&#20307;&#38590;&#20197;&#21457;&#25381;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#36229;&#20998;&#36776;&#29575;&#21644;&#32463;&#36807;&#35843;&#25972;&#30340;&#36731;&#37327;&#32423;YOLOv5&#26550;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;VisDrone-2023&#12289;SeaDroneSee&#12289;VEDAI&#21644;NWPU VHR-10&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#36229;&#20998;&#36776;&#29575;YOLOv5&#26550;&#26500;&#37319;&#29992;Transformer&#32534;&#30721;&#22120;&#22359;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#21040;&#20840;&#23616;&#32972;&#26223;&#21644;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20174;&#32780;&#25552;&#39640;&#26816;&#27979;&#32467;&#26524;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#23494;&#24230;&#12289;&#36974;&#25377;&#26465;&#20214;&#19979;&#12290;&#36825;&#31181;&#36731;&#37327;&#32423;&#27169;&#22411;&#19981;&#20165;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#36824;&#30830;&#20445;&#20102;&#36164;&#28304;&#30340;&#26377;&#25928;&#21033;&#29992;&#65292;&#38750;&#24120;&#36866;&#21512;&#23454;&#26102;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#33322;&#31354;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demand for accurate object detection in aerial imagery has surged with the widespread use of drones and satellite technology. Traditional object detection models, trained on datasets biased towards large objects, struggle to perform optimally in aerial scenarios where small, densely clustered objects are prevalent. To address this challenge, we present an innovative approach that combines super-resolution and an adapted lightweight YOLOv5 architecture. We employ a range of datasets, including VisDrone-2023, SeaDroneSee, VEDAI, and NWPU VHR-10, to evaluate our model's performance. Our Super Resolved YOLOv5 architecture features Transformer encoder blocks, allowing the model to capture global context and context information, leading to improved detection results, especially in high-density, occluded conditions. This lightweight model not only delivers improved accuracy but also ensures efficient resource utilization, making it well-suited for real-time applications. Our experimental 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.03572</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#21487;&#35777;&#25928;&#29575;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Provably Efficient Learning in Partially Observable Contextual Bandit. (arXiv:2308.03572v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#22240;&#26524;&#25928;&#24212;&#30340;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#22240;&#26524;&#32422;&#26463;&#26469;&#25913;&#36827;&#36718;&#30424;&#36172;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#24773;&#22659;&#36718;&#30424;&#36172;&#20013;&#30340;&#36716;&#31227;&#23398;&#20064;&#38382;&#39064;&#65292;&#20854;&#20013;&#20195;&#29702;&#20154;&#20165;&#26377;&#26469;&#33258;&#20854;&#20182;&#20195;&#29702;&#20154;&#30340;&#26377;&#38480;&#30693;&#35782;&#65292;&#24182;&#19988;&#23545;&#38544;&#34255;&#30340;&#28151;&#28102;&#22240;&#32032;&#21482;&#26377;&#37096;&#20998;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#38382;&#39064;&#36716;&#21270;&#20026;&#36890;&#36807;&#20248;&#21270;&#38382;&#39064;&#26469;&#35782;&#21035;&#25110;&#37096;&#20998;&#35782;&#21035;&#34892;&#20026;&#21644;&#22870;&#21169;&#20043;&#38388;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20248;&#21270;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#26410;&#30693;&#20998;&#24067;&#30340;&#21407;&#22987;&#21151;&#33021;&#32422;&#26463;&#31163;&#25955;&#21270;&#20026;&#32447;&#24615;&#32422;&#26463;&#65292;&#24182;&#36890;&#36807;&#39034;&#24207;&#35299;&#32447;&#24615;&#35268;&#21010;&#26469;&#37319;&#26679;&#20860;&#23481;&#30340;&#22240;&#26524;&#27169;&#22411;&#65292;&#20197;&#32771;&#34385;&#20272;&#35745;&#35823;&#24046;&#24471;&#21040;&#22240;&#26524;&#32422;&#26463;&#12290;&#25105;&#20204;&#30340;&#37319;&#26679;&#31639;&#27861;&#20026;&#36866;&#24403;&#30340;&#37319;&#26679;&#20998;&#24067;&#25552;&#20379;&#20102;&#29702;&#24819;&#30340;&#25910;&#25947;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#22240;&#26524;&#32422;&#26463;&#24212;&#29992;&#20110;&#25913;&#36827;&#32463;&#20856;&#30340;&#36718;&#30424;&#36172;&#31639;&#27861;&#65292;&#24182;&#20197;&#34892;&#21160;&#38598;&#21644;&#20989;&#25968;&#31354;&#38388;&#35268;&#27169;&#20026;&#21442;&#32771;&#25913;&#21464;&#20102;&#36951;&#25022;&#20540;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#20801;&#35768;&#25105;&#20204;&#22788;&#29702;&#19968;&#33324;&#24773;&#22659;&#20998;&#24067;&#30340;&#20989;&#25968;&#36924;&#36817;&#20219;&#21153;&#20013;
&lt;/p&gt;
&lt;p&gt;
In this paper, we investigate transfer learning in partially observable contextual bandits, where agents have limited knowledge from other agents and partial information about hidden confounders. We first convert the problem to identifying or partially identifying causal effects between actions and rewards through optimization problems. To solve these optimization problems, we discretize the original functional constraints of unknown distributions into linear constraints, and sample compatible causal models via sequentially solving linear programmings to obtain causal bounds with the consideration of estimation error. Our sampling algorithms provide desirable convergence results for suitable sampling distributions. We then show how causal bounds can be applied to improving classical bandit algorithms and affect the regrets with respect to the size of action sets and function spaces. Notably, in the task with function approximation which allows us to handle general context distributions
&lt;/p&gt;</description></item><item><title>PyPOTS&#26159;&#19968;&#20010;Python&#24037;&#20855;&#31665;&#65292;&#29992;&#20110;&#23545;&#37096;&#20998;&#35266;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25554;&#20540;&#12289;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#39044;&#27979;&#31561;&#22235;&#20010;&#20219;&#21153;&#65292;&#31639;&#27861;&#31181;&#31867;&#32321;&#22810;&#65292;&#36866;&#29992;&#20110;&#23398;&#26415;&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.18811</link><description>&lt;p&gt;
PyPOTS&#65306;&#29992;&#20110;&#37096;&#20998;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#25366;&#25496;&#30340;Python&#24037;&#20855;&#31665;
&lt;/p&gt;
&lt;p&gt;
PyPOTS: A Python Toolbox for Data Mining on Partially-Observed Time Series. (arXiv:2305.18811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18811
&lt;/p&gt;
&lt;p&gt;
PyPOTS&#26159;&#19968;&#20010;Python&#24037;&#20855;&#31665;&#65292;&#29992;&#20110;&#23545;&#37096;&#20998;&#35266;&#27979;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25554;&#20540;&#12289;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#39044;&#27979;&#31561;&#22235;&#20010;&#20219;&#21153;&#65292;&#31639;&#27861;&#31181;&#31867;&#32321;&#22810;&#65292;&#36866;&#29992;&#20110;&#23398;&#26415;&#30740;&#31350;&#21644;&#24037;&#19994;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyPOTS&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;Python&#24211;&#65292;&#33268;&#21147;&#20110;&#22312;&#22810;&#20803;&#37096;&#20998;&#35266;&#27979;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#19978;&#36827;&#34892;&#25968;&#25454;&#25366;&#25496;&#21644;&#20998;&#26512;&#65292;&#21363;&#38024;&#23545;&#23384;&#22312;&#32570;&#22833;&#20540;&#30340;&#19981;&#23436;&#25972;&#26102;&#38388;&#24207;&#21015;&#65292;&#20063;&#31216;&#20026;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#12290;&#29305;&#21035;&#22320;&#65292;&#23427;&#25552;&#20379;&#20102;&#23545;&#22235;&#20010;&#20219;&#21153;&#20998;&#31867;&#30340;&#19981;&#21516;&#31639;&#27861;&#30340;&#26131;&#29992;&#24615;&#25903;&#25345;&#65306;&#25554;&#20540;&#12289;&#20998;&#31867;&#12289;&#32858;&#31867;&#21644;&#39044;&#27979;&#12290;&#23427;&#21253;&#21547;&#20102;&#27010;&#29575;&#26041;&#27861;&#21644;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#33391;&#22909;&#12289;&#23436;&#25972;&#25991;&#26723;&#30340;&#32534;&#31243;&#25509;&#21475;&#65292;&#20379;&#23398;&#26415;&#30740;&#31350;&#20154;&#21592;&#21644;&#24037;&#19994;&#19987;&#19994;&#20154;&#21592;&#20351;&#29992;&#12290;&#35813;&#24037;&#20855;&#21253;&#30340;&#35774;&#35745;&#29702;&#24565;&#26159;&#40065;&#26834;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#24320;&#21457;&#36807;&#31243;&#20013;&#36981;&#24490;&#20102;&#36719;&#20214;&#26500;&#24314;&#30340;&#26368;&#20339;&#23454;&#36341;&#65292;&#20363;&#22914;&#21333;&#20803;&#27979;&#35797;&#12289;&#25345;&#32493;&#38598;&#25104;&#65288;CI&#65289;&#21644;&#25345;&#32493;&#20132;&#20184;&#65288;CD&#65289;&#12289;&#20195;&#30721;&#35206;&#30422;&#29575;&#12289;&#21487;&#32500;&#25252;&#24615;&#35780;&#20272;&#12289;&#20132;&#20114;&#24335;&#25945;&#31243;&#21644;&#24182;&#34892;&#21270;&#31561;&#21407;&#21017;&#12290;&#35813;&#24037;&#20855;&#31665;&#21487;&#22312;Python&#21253;&#32034;&#24341;&#65288;PyPI&#65289;&#21644;Anaconda&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
PyPOTS is an open-source Python library dedicated to data mining and analysis on multivariate partially-observed time series, i.e. incomplete time series with missing values, A.K.A. irregularlysampled time series. Particularly, it provides easy access to diverse algorithms categorized into four tasks: imputation, classification, clustering, and forecasting. The included models contain probabilistic approaches as well as neural-network methods, with a well-designed and fully-documented programming interface for both academic researchers and industrial professionals to use. With robustness and scalability in its design philosophy, best practices of software construction, for example, unit testing, continuous integration (CI) and continuous delivery (CD), code coverage, maintainability evaluation, interactive tutorials, and parallelization, are carried out as principles during the development of PyPOTS. The toolkit is available on both Python Package Index (PyPI) and Anaconda. PyPOTS is o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#39044;&#27979;&#31639;&#27861;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#21644;&#20266;&#38543;&#26426;&#24615;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#22810;&#26657;&#20934;&#31639;&#27861;&#21644;&#23454;&#20540;&#20989;&#25968;&#26680;&#24341;&#29702;&#35777;&#26126;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.08837</link><description>&lt;p&gt;
&#20174;&#20266;&#38543;&#26426;&#24615;&#21040;&#22810;&#32452;&#20844;&#24179;&#24615;&#20877;&#21040;&#22238;&#26469;
&lt;/p&gt;
&lt;p&gt;
From Pseudorandomness to Multi-Group Fairness and Back. (arXiv:2301.08837v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.08837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#39044;&#27979;&#31639;&#27861;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#21644;&#20266;&#38543;&#26426;&#24615;&#30340;&#32852;&#31995;&#65292;&#25552;&#20379;&#20102;&#26032;&#30340;&#22810;&#26657;&#20934;&#31639;&#27861;&#21644;&#23454;&#20540;&#20989;&#25968;&#26680;&#24341;&#29702;&#35777;&#26126;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#39044;&#27979;&#31639;&#27861;&#20013;&#22810;&#32452;&#20844;&#24179;&#24615;&#21644;&#27844;&#38706;-&#38887;&#24615;&#21644;&#22270;&#24418;&#35268;&#21017;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22312;&#19968;&#20123;&#21442;&#25968;&#33539;&#22260;&#20869;&#25552;&#20379;&#20102;&#26032;&#30340;&#22810;&#26657;&#20934;&#21644;&#23454;&#20540;&#20989;&#25968;&#26680;&#24341;&#29702;&#35777;&#26126;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We identify and explore connections between the recent literature on multi-group fairness for prediction algorithms and the pseudorandomness notions of leakage-resilience and graph regularity. We frame our investigation using new, statistical distance-based variants of multicalibration that are closely related to the concept of outcome indistinguishability. Adopting this perspective leads us naturally not only to our graph theoretic results, but also to new, more efficient algorithms for multicalibration in certain parameter regimes and a novel proof of a hardcore lemma for real-valued functions.
&lt;/p&gt;</description></item></channel></rss>