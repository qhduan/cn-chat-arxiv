<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#32479;&#35745;&#37327;&#65292;&#29983;&#25104;&#19968;&#20010;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#27010;&#24565;&#20043;&#38388;&#30340;&#32467;&#26500;&#24182;&#36882;&#24402;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#30340;&#31614;&#21517;&#26469;&#25214;&#21040;&#30456;&#20851;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.12143</link><description>&lt;p&gt;
&#31616;&#21333;&#26426;&#21046;&#29992;&#20110;&#34920;&#31034;&#12289;&#32034;&#24341;&#21644;&#25805;&#20316;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Simple Mechanisms for Representing, Indexing and Manipulating Concepts. (arXiv:2310.12143v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12143
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#32479;&#35745;&#37327;&#65292;&#29983;&#25104;&#19968;&#20010;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#65292;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#27010;&#24565;&#20043;&#38388;&#30340;&#32467;&#26500;&#24182;&#36882;&#24402;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#65292;&#21516;&#26102;&#21487;&#20197;&#36890;&#36807;&#27010;&#24565;&#30340;&#31614;&#21517;&#26469;&#25214;&#21040;&#30456;&#20851;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#32593;&#32476;&#36890;&#24120;&#36890;&#36807;&#20998;&#31867;&#22120;&#23398;&#20064;&#27010;&#24565;&#65292;&#36825;&#28041;&#21450;&#35774;&#32622;&#27169;&#22411;&#24182;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#23427;&#20197;&#36866;&#24212;&#20855;&#26377;&#26631;&#35760;&#27010;&#24565;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#19968;&#20010;&#19981;&#21516;&#30340;&#35266;&#28857;&#65292;&#21363;&#21487;&#20197;&#36890;&#36807;&#26597;&#30475;&#27010;&#24565;&#30340;&#30697;&#38453;&#30697;&#38453;&#32479;&#35745;&#37327;&#26469;&#29983;&#25104;&#27010;&#24565;&#30340;&#20855;&#20307;&#34920;&#31034;&#25110;&#31614;&#21517;&#12290;&#36825;&#20123;&#31614;&#21517;&#21487;&#20197;&#29992;&#20110;&#21457;&#29616;&#19968;&#32452;&#27010;&#24565;&#30340;&#32467;&#26500;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#20174;&#36825;&#20123;&#31614;&#21517;&#20013;&#23398;&#20064;&#35813;&#32467;&#26500;&#26469;&#36882;&#24402;&#22320;&#20135;&#29983;&#26356;&#39640;&#32423;&#30340;&#27010;&#24565;&#12290;&#24403;&#27010;&#24565;"&#30456;&#20132;"&#26102;&#65292;&#27010;&#24565;&#30340;&#31614;&#21517;&#21487;&#20197;&#29992;&#20110;&#22312;&#19968;&#20123;&#30456;&#20851;&#30340;"&#30456;&#20132;"&#27010;&#24565;&#20013;&#25214;&#21040;&#19968;&#20010;&#20849;&#21516;&#30340;&#20027;&#39064;&#12290;&#36825;&#20010;&#36807;&#31243;&#21487;&#20197;&#29992;&#20110;&#20445;&#25345;&#19968;&#20010;&#27010;&#24565;&#23383;&#20856;&#65292;&#20197;&#20415;&#36755;&#20837;&#33021;&#22815;&#27491;&#30830;&#35782;&#21035;&#24182;&#34987;&#36335;&#30001;&#21040;&#19982;&#36755;&#20837;&#30340;(&#28508;&#22312;)&#29983;&#25104;&#30456;&#20851;&#30340;&#27010;&#24565;&#38598;&#21512;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep networks typically learn concepts via classifiers, which involves setting up a model and training it via gradient descent to fit the concept-labeled data. We will argue instead that learning a concept could be done by looking at its moment statistics matrix to generate a concrete representation or signature of that concept. These signatures can be used to discover structure across the set of concepts and could recursively produce higher-level concepts by learning this structure from those signatures. When the concepts are `intersected', signatures of the concepts can be used to find a common theme across a number of related `intersected' concepts. This process could be used to keep a dictionary of concepts so that inputs could correctly identify and be routed to the set of concepts involved in the (latent) generation of the input.
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;80&#65285;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#26356;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#38477;&#20302;&#35760;&#24518;&#25991;&#26412;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.08637</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20986;&#30340;&#35780;&#20272;&#65306;&#35805;&#35821;&#21644;&#35760;&#24518;
&lt;/p&gt;
&lt;p&gt;
An Evaluation on Large Language Model Outputs: Discourse and Memorization. (arXiv:2304.08637v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08637
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#20102;&#20061;&#20010;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#65292;&#21457;&#29616;&#20854;&#20013;80&#65285;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#26356;&#21487;&#33021;&#26159;&#39640;&#36136;&#37327;&#30340;&#12290;&#25552;&#20986;&#20102;&#32531;&#35299;&#31574;&#30053;&#20197;&#38477;&#20302;&#35760;&#24518;&#25991;&#26412;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;&#20061;&#20010;&#26368;&#24191;&#27867;&#21487;&#29992;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#21508;&#31181;&#36755;&#20986;&#36827;&#34892;&#20102;&#32463;&#39564;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#25104;&#30340;&#24037;&#20855;&#36827;&#34892;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#19982;&#36755;&#20986;&#30149;&#24577;&#65288;&#20363;&#22914;&#65292;&#21453;&#20107;&#23454;&#21644;&#36923;&#36753;&#19978;&#30340;&#38169;&#35823;&#38472;&#36848;&#65289;&#20197;&#21450;&#19981;&#20445;&#25345;&#20027;&#39064;&#31561;&#26041;&#38754;&#30340;&#20851;&#31995;&#20013;&#65292;&#35760;&#24518;&#25991;&#26412;&#30334;&#20998;&#27604;&#12289;&#29420;&#29305;&#25991;&#26412;&#30334;&#20998;&#27604;&#21644;&#25972;&#20307;&#36755;&#20986;&#36136;&#37327;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;80.0&#65285;&#30340;&#36755;&#20986;&#21253;&#21547;&#35760;&#24518;&#25968;&#25454;&#65292;&#20294;&#21253;&#21547;&#26368;&#22810;&#35760;&#24518;&#20869;&#23481;&#30340;&#36755;&#20986;&#20063;&#26356;&#26377;&#21487;&#33021;&#34987;&#35748;&#20026;&#20855;&#26377;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#21644;&#35780;&#20272;&#20102;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#26174;&#31034;&#65292;&#22312;&#35780;&#20272;&#30340;&#27169;&#22411;&#20013;&#65292;&#36755;&#20986;&#30340;&#35760;&#24518;&#25991;&#26412;&#29575;&#26377;&#25152;&#38477;&#20302;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23601;&#23398;&#20064;&#12289;&#35760;&#24518;&#21644;&#35780;&#20272;&#20248;&#36136;&#25991;&#26412;&#30340;&#28508;&#22312;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an empirical evaluation of various outputs generated by nine of the most widely-available large language models (LLMs). Our analysis is done with off-the-shelf, readily-available tools. We find a correlation between percentage of memorized text, percentage of unique text, and overall output quality, when measured with respect to output pathologies such as counterfactual and logically-flawed statements, and general failures like not staying on topic. Overall, 80.0% of the outputs evaluated contained memorized data, but outputs containing the most memorized content were also more likely to be considered of high quality. We discuss and evaluate mitigation strategies, showing that, in the models evaluated, the rate of memorized text being output is reduced. We conclude with a discussion on potential implications around what it means to learn, to memorize, and to evaluate quality text.
&lt;/p&gt;</description></item></channel></rss>