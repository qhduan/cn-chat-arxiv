<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;CounterfacTS&#24037;&#20855;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#25506;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.03508</link><description>&lt;p&gt;
&#25506;&#31350;&#20351;&#29992;CounterfacTS&#25506;&#31350;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Probing the Robustness of Time-series Forecasting Models with CounterfacTS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03508
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;CounterfacTS&#24037;&#20855;&#65292;&#36890;&#36807;&#21453;&#20107;&#23454;&#25506;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24212;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26102;&#38754;&#20020;&#30340;&#19968;&#20010;&#24120;&#35265;&#38382;&#39064;&#26159;&#25968;&#25454;&#20998;&#24067;&#30340;&#26102;&#38388;&#28436;&#21270;&#65288;&#21363;&#27010;&#24565;&#28418;&#31227;&#65289;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#35757;&#32451;&#25968;&#25454;&#27809;&#26377;&#21453;&#26144;&#36825;&#20123;&#21464;&#21270;&#65292;&#27169;&#22411;&#22312;&#26032;&#30340;&#20998;&#24067;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#24456;&#24046;&#65292;&#22240;&#27492;&#65292;&#27492;&#31867;&#20107;&#20214;&#30340;&#24433;&#21709;&#20107;&#21069;&#26080;&#27861;&#21487;&#38752;&#22320;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#20844;&#24320;&#21457;&#24067;CounterfacTS&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#21453;&#20107;&#23454;&#25506;&#31350;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#20013;&#40065;&#26834;&#24615;&#30340;&#24037;&#20855;&#12290;CounterfacTS&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#20801;&#35768;&#29992;&#25143;&#21487;&#35270;&#21270;&#12289;&#27604;&#36739;&#21644;&#37327;&#21270;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#21450;&#20854;&#39044;&#27979;&#32467;&#26524;&#65292;&#36866;&#29992;&#20110;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#29992;&#25143;&#21487;&#20197;&#23545;&#26102;&#38388;&#24207;&#21015;&#24212;&#29992;&#21508;&#31181;&#21464;&#25442;&#65292;&#24182;&#20197;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#25506;&#32034;&#39044;&#27979;&#32467;&#26524;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#31034;&#20363;&#26696;&#20363;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;CounterfacTS&#22914;&#20309;&#29992;&#20110;&#65306;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03508v1 Announce Type: new  Abstract: A common issue for machine learning models applied to time-series forecasting is the temporal evolution of the data distributions (i.e., concept drift). Because most of the training data does not reflect such changes, the models present poor performance on the new out-of-distribution scenarios and, therefore, the impact of such events cannot be reliably anticipated ahead of time. We present and publicly release CounterfacTS, a tool to probe the robustness of deep learning models in time-series forecasting tasks via counterfactuals. CounterfacTS has a user-friendly interface that allows the user to visualize, compare and quantify time series data and their forecasts, for a number of datasets and deep learning models. Furthermore, the user can apply various transformations to the time series and explore the resulting changes in the forecasts in an interpretable manner. Through example cases, we illustrate how CounterfacTS can be used to i)
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.01759</link><description>&lt;p&gt;
&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#65306;&#22238;&#39038;&#19982;&#26032;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Open-world Machine Learning: A Review and New Outlooks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01759
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#65292;&#26412;&#25991;&#25299;&#23637;&#20102;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#22810;&#20010;&#28508;&#22312;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#22522;&#20110;&#23553;&#38381;&#19990;&#30028;&#20551;&#35774;&#65292;&#21363;&#20551;&#23450;&#29615;&#22659;&#26159;&#38745;&#24577;&#30340;&#65292;&#27169;&#22411;&#19968;&#26086;&#37096;&#32626;&#23601;&#26159;&#22266;&#23450;&#30340;&#12290;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#65292;&#36825;&#31181;&#22522;&#26412;&#19988;&#30456;&#24403;&#24188;&#31258;&#30340;&#20551;&#35774;&#21487;&#33021;&#19981;&#25104;&#31435;&#65292;&#22240;&#20026;&#24320;&#25918;&#29615;&#22659;&#22797;&#26434;&#12289;&#21160;&#24577;&#19988;&#20805;&#28385;&#26410;&#30693;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25298;&#32477;&#26410;&#30693;&#12289;&#21457;&#29616;&#26032;&#22855;&#28857;&#65292;&#28982;&#21518;&#36880;&#27493;&#23398;&#20064;&#65292;&#21487;&#20197;&#20351;&#27169;&#22411;&#20687;&#29983;&#29289;&#31995;&#32479;&#19968;&#26679;&#23433;&#20840;&#22320;&#24182;&#25345;&#32493;&#36827;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26410;&#30693;&#25298;&#32477;&#12289;&#26032;&#31867;&#21035;&#21457;&#29616;&#21644;&#31867;&#21035;&#22686;&#37327;&#23398;&#20064;&#22312;&#32479;&#19968;&#33539;&#24335;&#20013;&#65292;&#25552;&#20379;&#20102;&#23545;&#24320;&#25918;&#19990;&#30028;&#26426;&#22120;&#23398;&#20064;&#30340;&#25972;&#20307;&#35266;&#28857;&#12290;&#35814;&#32454;&#35752;&#35770;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#25361;&#25112;&#12289;&#21407;&#21017;&#21644;&#23616;&#38480;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#20221;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01759v1 Announce Type: new  Abstract: Machine learning has achieved remarkable success in many applications. However, existing studies are largely based on the closed-world assumption, which assumes that the environment is stationary, and the model is fixed once deployed. In many real-world applications, this fundamental and rather naive assumption may not hold because an open environment is complex, dynamic, and full of unknowns. In such cases, rejecting unknowns, discovering novelties, and then incrementally learning them, could enable models to be safe and evolve continually as biological systems do. This paper provides a holistic view of open-world machine learning by investigating unknown rejection, novel class discovery, and class-incremental learning in a unified paradigm. The challenges, principles, and limitations of current methodologies are discussed in detail. Finally, we discuss several potential directions for future research. This paper aims to provide a compr
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2402.19186</link><description>&lt;p&gt;
&#29992;&#29983;&#25104;&#27169;&#22411;&#35299;&#24320;&#35270;&#32593;&#33180;&#22270;&#20687;&#30340;&#34920;&#24449;
&lt;/p&gt;
&lt;p&gt;
Disentangling representations of retinal images with generative models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.19186
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#24320;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#23454;&#29616;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#22312;&#26089;&#26399;&#26816;&#27979;&#30524;&#37096;&#30142;&#30149;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#29978;&#33267;&#34920;&#26126;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#22270;&#20687;&#36824;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#24515;&#34880;&#31649;&#39118;&#38505;&#22240;&#32032;&#21644;&#31070;&#32463;&#31995;&#32479;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#20687;&#21463;&#25216;&#26415;&#22240;&#32032;&#30340;&#24433;&#21709;&#21487;&#33021;&#23545;&#30524;&#31185;&#39046;&#22495;&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#26500;&#25104;&#25361;&#25112;&#12290;&#20363;&#22914;&#65292;&#22823;&#22411;&#24213;&#22270;&#38431;&#21015;&#24448;&#24448;&#21463;&#21040;&#30456;&#26426;&#31867;&#22411;&#12289;&#22270;&#20687;&#36136;&#37327;&#25110;&#29031;&#26126;&#27700;&#24179;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23384;&#22312;&#23398;&#20064;&#24555;&#25463;&#26041;&#24335;&#32780;&#19981;&#26159;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#32972;&#21518;&#22240;&#26524;&#20851;&#31995;&#30340;&#39118;&#38505;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35270;&#32593;&#33180;&#24213;&#22270;&#20687;&#32676;&#20307;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#35299;&#24320;&#20102;&#24739;&#32773;&#23646;&#24615;&#19982;&#30456;&#26426;&#25928;&#26524;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21487;&#25511;&#19988;&#39640;&#24230;&#36924;&#30495;&#30340;&#22270;&#20687;&#29983;&#25104;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#26032;&#39062;&#35299;&#24320;&#25439;&#22833;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.19186v1 Announce Type: cross  Abstract: Retinal fundus images play a crucial role in the early detection of eye diseases and, using deep learning approaches, recent studies have even demonstrated their potential for detecting cardiovascular risk factors and neurological disorders. However, the impact of technical factors on these images can pose challenges for reliable AI applications in ophthalmology. For example, large fundus cohorts are often confounded by factors like camera type, image quality or illumination level, bearing the risk of learning shortcuts rather than the causal relationships behind the image generation process. Here, we introduce a novel population model for retinal fundus images that effectively disentangles patient attributes from camera effects, thus enabling controllable and highly realistic image generation. To achieve this, we propose a novel disentanglement loss based on distance correlation. Through qualitative and quantitative analyses, we demon
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; $L^*LM$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21644;&#28436;&#31034;&#23398;&#20064; DFA&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.07051</link><description>&lt;p&gt;
$L^*LM$: &#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#31034;&#20363;&#23398;&#20064;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
$L^*LM$: Learning Automata from Examples using Natural Language Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07051
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026; $L^*LM$ &#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#21644;&#28436;&#31034;&#23398;&#20064; DFA&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#25928;&#29575;&#65292;&#20855;&#22791;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19987;&#23478;&#28436;&#31034;&#24050;&#34987;&#35777;&#26126;&#26159;&#31616;&#21270;&#38388;&#25509;&#25351;&#23450;&#22797;&#26434;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#31639;&#27861;&#29978;&#33267;&#25903;&#25345;&#20174;&#28436;&#31034;&#20013;&#25552;&#21462;&#26126;&#30830;&#30340;&#24418;&#24335;&#35268;&#33539;&#65292;&#22914;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426;&#65288;DFA&#65289;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#25216;&#26415;&#36890;&#24120;&#19981;&#20855;&#22791;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; $L^*LM$ &#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#28436;&#31034;&#21644;&#33258;&#28982;&#35821;&#35328;&#20013;&#23398;&#20064; DFA&#12290;&#30001;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20174;&#19987;&#23478;&#28436;&#31034;&#20013;&#23398;&#20064; DFA &#30340;&#25968;&#25454;&#25928;&#29575;&#26174;&#33879;&#25552;&#39640;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;$L^*LM$ &#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#22238;&#31572;&#20851;&#20110;&#24213;&#23618;&#20219;&#21153;&#30340;&#25104;&#21592;&#26597;&#35810;&#12290;&#28982;&#21518;&#23558;&#20854;&#19982;&#26368;&#36817;&#30340;&#28436;&#31034;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#23558;&#23398;&#20064;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#24102;&#26631;&#31614;&#31034;&#20363;&#23398;&#20064;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#20004;&#31181;&#27169;&#24577;&#30456;&#20114;&#34917;&#20805;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Expert demonstrations have proven an easy way to indirectly specify complex tasks. Recent algorithms even support extracting unambiguous formal specifications, e.g. deterministic finite automata (DFA), from demonstrations. Unfortunately, these techniques are generally not sample efficient. In this work, we introduce $L^*LM$, an algorithm for learning DFAs from both demonstrations and natural language. Due to the expressivity of natural language, we observe a significant improvement in the data efficiency of learning DFAs from expert demonstrations. Technically, $L^*LM$ leverages large language models to answer membership queries about the underlying task. This is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems. In our experiments, we observe the two modalities complement each other, yielding a powerful few-shot learner.
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.01763</link><description>&lt;p&gt;
&#24403;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36935;&#19978;&#21521;&#37327;&#25968;&#25454;&#24211;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
When Large Language Models Meet Vector Databases: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35770;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#32780;&#21521;&#37327;&#25968;&#25454;&#24211;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#26174;&#33879;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#30772;&#22312;&#20154;&#31867;&#25991;&#23383;&#22788;&#29702;&#21644;&#29983;&#25104;&#26041;&#38754;&#24320;&#21551;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#23427;&#20204;&#30340;&#26174;&#33879;&#22686;&#38271;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#21253;&#25324;&#24187;&#35273;&#12289;&#20559;&#35265;&#12289;&#23454;&#26102;&#30693;&#35782;&#26356;&#26032;&#20197;&#21450;&#22312;&#21830;&#19994;&#29615;&#22659;&#20013;&#23454;&#26045;&#21644;&#32500;&#25252;&#30340;&#39640;&#25104;&#26412;&#31561;&#37325;&#35201;&#25361;&#25112;&#12290;&#32780;&#21478;&#19968;&#31181;&#26085;&#30410;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#21521;&#37327;&#25968;&#25454;&#24211;&#21017;&#20026;&#36825;&#20123;&#25361;&#25112;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#25968;&#25454;&#24211;&#25797;&#38271;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#65292;&#24182;&#19988;&#23545;&#20110;&#39640;&#25928;&#30340;&#20449;&#24687;&#26816;&#32034;&#21644;&#35821;&#20041;&#25628;&#32034;&#31561;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25972;&#21512;&#65292;&#23427;&#20204;&#26174;&#33879;&#22686;&#24378;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#31649;&#29702;&#21644;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#22810;&#26679;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21521;&#37327;&#25968;&#25454;&#24211;&#20043;&#38388;&#30340;&#20132;&#21449;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#32780;&#29420;&#29305;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent burst in Large Language Models has opened new frontiers in human-like text processing and generation. However, alongside their remarkable growth, Large Language Models have encountered critical challenges including issues of hallucination, bias, real-time knowledge updates, and the high costs of implementation and maintenance in commercial settings. Vector Databases, another increasingly popular tool, offer potential solutions to these challenges. These databases are adept at handling high-dimensional data and are crucial for tasks such as efficient information retrieval and semantic search. By integrating with Large Language Models, they significantly enhance AI systems' ability to manage and utilize diverse data more effectively. This survey paper provides an in-depth and unique analysis of the intersection between Large Language Models and Vector Databases.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;</title><link>https://arxiv.org/abs/2402.01744</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#23618;&#22270;&#35299;&#37322;&#25581;&#31034;&#20998;&#23376;&#25104;&#20998;
&lt;/p&gt;
&lt;p&gt;
Unveiling Molecular Moieties through Hierarchical Graph Explainability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29983;&#29289;&#27963;&#24615;&#24182;&#25214;&#21040;&#19982;&#20043;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#22312;&#25903;&#25345;&#20307;&#22806;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#24050;&#32463;&#20986;&#29616;&#22810;&#24180;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22270;&#21367;&#31215;&#26550;&#26500;&#23454;&#29616;&#39640;&#31934;&#24230;&#22810;&#38774;&#26631;&#31579;&#36873;&#30340;GNN&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#20998;&#23618;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#36890;&#36807;&#21033;&#29992;&#20449;&#24687;&#20256;&#36882;&#26426;&#21046;&#65292;&#22312;&#21407;&#23376;&#12289;&#29615;&#21644;&#25972;&#20010;&#20998;&#23376;&#23618;&#38754;&#19978;&#30452;&#25509;&#25429;&#33719;&#20449;&#24687;&#65292;&#20174;&#32780;&#25214;&#21040;&#19982;&#29983;&#29289;&#27963;&#24615;&#39044;&#27979;&#30456;&#20851;&#30340;&#26368;&#37325;&#35201;&#30340;&#25104;&#20998;&#12290;&#32467;&#26524;&#65306;&#25105;&#20204;&#22312;&#25903;&#25345;&#34394;&#25311;&#31579;&#36873;&#26041;&#38754;&#30340;&#20108;&#21313;&#20010;&#32454;&#32990;&#21608;&#26399;&#20381;&#36182;&#24615;&#28608;&#37238;&#38774;&#26631;&#19978;&#25253;&#36947;&#20102;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;GNN&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#20998;&#31867;&#22120;&#36229;&#36234;&#20102;&#20316;&#32773;&#25552;&#20986;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#20165;&#38024;&#23545;CDK1&#30340;&#39640;&#28789;&#25935;&#24230;&#29256;&#26412;&#30340;GNN&#65292;&#20197;&#20351;&#29992;&#25105;&#20204;&#30340;&#35299;&#37322;&#22120;&#26469;&#36991;&#20813;&#22810;&#31867;&#21035;&#27169;&#22411;&#22266;&#26377;&#30340;&#20559;&#24046;&#12290;&#20998;&#23618;&#35299;&#37322;&#22120;&#24050;&#32463;&#30001;&#19968;&#20301;&#19987;&#23478;&#21270;&#23398;&#23478;&#22312;19&#20010;CDK1&#25209;&#20934;&#33647;&#29289;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Background: Graph Neural Networks (GNN) have emerged in very recent years as a powerful tool for supporting in silico Virtual Screening. In this work we present a GNN which uses Graph Convolutional architectures to achieve very accurate multi-target screening. We also devised a hierarchical Explainable Artificial Intelligence (XAI) technique to catch information directly at atom, ring, and whole molecule level by leveraging the message passing mechanism. In this way, we find the most relevant moieties involved in bioactivity prediction. Results: We report a state-of-the-art GNN classifier on twenty Cyclin-dependent Kinase targets in support of VS. Our classifier outperforms previous SOTA approaches proposed by the authors. Moreover, a CDK1-only high-sensitivity version of the GNN has been designed to use our explainer in order to avoid the inherent bias of multi-class models. The hierarchical explainer has been validated by an expert chemist on 19 approved drugs on CDK1. Our explainer 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2401.00691</link><description>&lt;p&gt;
&#28155;&#21152;&#38750;&#21442;&#25968;&#22238;&#24402;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;
&lt;/p&gt;
&lt;p&gt;
Stochastic Gradient Descent for Additive Nonparametric Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.00691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#21487;&#20197;&#23454;&#29616;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#35757;&#32451;&#21152;&#24615;&#27169;&#22411;&#30340;&#36845;&#20195;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20855;&#26377;&#33391;&#22909;&#30340;&#20869;&#23384;&#23384;&#20648;&#21644;&#35745;&#31639;&#35201;&#27714;&#12290;&#35813;&#31639;&#27861;&#21487;&#20197;&#30475;&#20316;&#26159;&#23545;&#32452;&#20214;&#20989;&#25968;&#30340;&#25130;&#26029;&#22522;&#25193;&#23637;&#30340;&#31995;&#25968;&#24212;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#30340;&#20989;&#25968;&#23545;&#24212;&#29289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24471;&#21040;&#30340;&#20272;&#35745;&#37327;&#28385;&#36275;&#19968;&#20010;&#22885;&#25289;&#20811;&#19981;&#31561;&#24335;&#65292;&#20801;&#35768;&#27169;&#22411;&#38169;&#35823;&#35268;&#33539;&#12290;&#22312;&#35268;&#33539;&#24456;&#22909;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#30340;&#19977;&#20010;&#19981;&#21516;&#38454;&#27573;&#20180;&#32454;&#36873;&#25321;&#23398;&#20064;&#29575;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20854;&#39118;&#38505;&#22312;&#25968;&#25454;&#32500;&#24230;&#21644;&#35757;&#32451;&#26679;&#26412;&#22823;&#23567;&#30340;&#20381;&#36182;&#26041;&#38754;&#26159;&#26368;&#23567;&#21644;&#26368;&#20248;&#30340;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#23558;&#35813;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#21453;&#21521;&#25311;&#21512;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35828;&#26126;&#20102;&#35745;&#31639;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces an iterative algorithm for training additive models that enjoys favorable memory storage and computational requirements. The algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. We show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. In the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. We further illustrate the computational benefits by comparing the approach with traditional backfitting on two real-world datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#20197;&#35299;&#20915;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#24310;&#36831;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04585</link><description>&lt;p&gt;
&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26469;&#23454;&#29616;&#24357;&#25955;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models. (arXiv:2401.04585v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#20998;&#24067;&#23545;&#40784;&#26041;&#27861;&#20197;&#35299;&#20915;&#21518;&#35757;&#32451;&#37327;&#21270;&#23545;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#20998;&#24067;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#20302;&#24310;&#36831;&#24212;&#29992;&#20013;&#20855;&#26377;&#36739;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#33021;&#26377;&#25928;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#22122;&#22768;&#20272;&#35745;&#65292;&#25193;&#25955;&#27169;&#22411;&#22312;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#32321;&#37325;&#30340;&#21435;&#22122;&#36807;&#31243;&#21644;&#22797;&#26434;&#30340;&#31070;&#32463;&#32593;&#32476;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#20302;&#24310;&#36831;&#24212;&#29992;&#12290;&#37327;&#21270;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#27169;&#22411;&#22797;&#26434;&#24230;&#65292;&#32780;&#21518;&#35757;&#32451;&#37327;&#21270;(PTQ)&#22312;&#21152;&#36895;&#21435;&#22122;&#36807;&#31243;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#28508;&#21147;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#24494;&#35843;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#30001;&#20110;&#19981;&#21516;&#21435;&#22122;&#27493;&#39588;&#20013;&#28608;&#27963;&#30340;&#39640;&#24230;&#21160;&#24577;&#20998;&#24067;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;PTQ&#26041;&#27861;&#22312;&#26657;&#20934;&#26679;&#26412;&#21644;&#37325;&#26500;&#36755;&#20986;&#20004;&#20010;&#23618;&#38754;&#19978;&#37117;&#23384;&#22312;&#20998;&#24067;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#65292;&#23548;&#33268;&#24615;&#33021;&#36828;&#20302;&#20110;&#20196;&#20154;&#28385;&#24847;&#30340;&#27700;&#24179;&#65292;&#29305;&#21035;&#26159;&#22312;&#20302;&#20301;&#24773;&#20917;&#19979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#20998;&#24067;&#23545;&#40784;&#29992;&#20110;&#24357;&#25955;&#27169;&#22411;&#30340;&#21518;&#35757;&#32451;&#37327;&#21270;(EDA-DM)&#26469;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#22312;&#26657;&#20934;&#26679;&#26412;&#23618;&#38754;&#65292;&#25105;&#20204;&#22522;&#20110;...[&#32570;&#30465;]
&lt;/p&gt;
&lt;p&gt;
Diffusion models have achieved great success in image generation tasks through iterative noise estimation. However, the heavy denoising process and complex neural networks hinder their low-latency applications in real-world scenarios. Quantization can effectively reduce model complexity, and post-training quantization (PTQ), which does not require fine-tuning, is highly promising in accelerating the denoising process. Unfortunately, we find that due to the highly dynamic distribution of activations in different denoising steps, existing PTQ methods for diffusion models suffer from distribution mismatch issues at both calibration sample level and reconstruction output level, which makes the performance far from satisfactory, especially in low-bit cases. In this paper, we propose Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models (EDA-DM) to address the above issues. Specifically, at the calibration sample level, we select calibration samples based on the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2401.01259</link><description>&lt;p&gt;
&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#26159;&#21542;&#36981;&#24490;&#23616;&#37096;&#24615;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Concept Bottleneck Models Obey Locality?. (arXiv:2401.01259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#26159;&#21542;&#33021;&#22815;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#20110;&#27010;&#24565;&#23616;&#37096;&#24615;&#20043;&#22806;&#29305;&#24449;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#27010;&#24565;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#22522;&#30784;&#23398;&#20064;&#36890;&#36807;&#35299;&#37322;&#20854;&#39044;&#27979;&#32467;&#26524;&#20351;&#29992;&#20154;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#65292;&#25913;&#21892;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#19979;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20005;&#37325;&#20381;&#36182;&#20110;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#29420;&#31435;&#20110;&#20854;&#20182;&#27010;&#24565;&#30340;&#32473;&#23450;&#27010;&#24565;&#30340;&#23384;&#22312;&#25110;&#19981;&#23384;&#22312;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#28872;&#26263;&#31034;&#36825;&#31181;&#20551;&#35774;&#21487;&#33021;&#22312;&#27010;&#24565;&#29942;&#39048;&#27169;&#22411;&#65288;CBMs&#65289;&#36825;&#19968;&#20856;&#22411;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#21487;&#35299;&#37322;&#26550;&#26500;&#20013;&#19981;&#33021;&#25104;&#31435;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24403;&#36825;&#20123;&#27010;&#24565;&#26082;&#22312;&#31354;&#38388;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#23436;&#20840;&#30001;&#22266;&#23450;&#23376;&#38598;&#30340;&#29305;&#24449;&#23450;&#20041;&#65289;&#21448;&#22312;&#35821;&#20041;&#19978;&#65288;&#36890;&#36807;&#23427;&#20204;&#30340;&#20540;&#20165;&#19982;&#39044;&#23450;&#20041;&#30340;&#22266;&#23450;&#23376;&#38598;&#30340;&#27010;&#24565;&#30456;&#20851;&#32852;&#65289;&#23450;&#20301;&#26102;&#65292;CBMs&#26159;&#21542;&#27491;&#30830;&#25429;&#25417;&#21040;&#27010;&#24565;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#31243;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#23616;&#37096;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#27010;&#24565;&#20043;&#22806;&#30340;&#29305;&#24449;&#21464;&#21270;&#23545;&#27010;&#24565;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concept-based learning improves a deep learning model's interpretability by explaining its predictions via human-understandable concepts. Deep learning models trained under this paradigm heavily rely on the assumption that neural networks can learn to predict the presence or absence of a given concept independently of other concepts. Recent work, however, strongly suggests that this assumption may fail to hold in Concept Bottleneck Models (CBMs), a quintessential family of concept-based interpretable architectures. In this paper, we investigate whether CBMs correctly capture the degree of conditional independence across concepts when such concepts are localised both spatially, by having their values entirely defined by a fixed subset of features, and semantically, by having their values correlated with only a fixed subset of predefined concepts. To understand locality, we analyse how changes to features outside of a concept's spatial or semantic locality impact concept predictions. Our
&lt;/p&gt;</description></item><item><title>DSAC-C&#26159;&#19968;&#31181;&#32422;&#26463;&#26368;&#22823;&#29109;&#30340;&#40065;&#26834;&#31163;&#25955;&#36719;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#39069;&#22806;&#30340;&#32479;&#35745;&#32422;&#26463;&#25552;&#20379;&#20102;&#23545;&#28508;&#22312;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#21644;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.17173</link><description>&lt;p&gt;
DSAC-C: &#32422;&#26463;&#26368;&#22823;&#29109;&#29992;&#20110;&#40065;&#26834;&#24615;&#31163;&#25955;&#21270;&#36719;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
DSAC-C: Constrained Maximum Entropy for Robust Discrete Soft-Actor Critic. (arXiv:2310.17173v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17173
&lt;/p&gt;
&lt;p&gt;
DSAC-C&#26159;&#19968;&#31181;&#32422;&#26463;&#26368;&#22823;&#29109;&#30340;&#40065;&#26834;&#31163;&#25955;&#36719;-&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#36890;&#36807;&#39069;&#22806;&#30340;&#32479;&#35745;&#32422;&#26463;&#25552;&#20379;&#20102;&#23545;&#28508;&#22312;&#39046;&#22495;&#21464;&#21270;&#30340;&#40065;&#26834;&#24615;&#21644;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;Soft Actor-Critic (SAC)&#31639;&#27861;&#30340;&#25193;&#23637;&#12290;&#25105;&#20204;&#35748;&#20026;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#29702;&#65292;&#21487;&#20197;&#36890;&#36807;&#20174;&#26367;&#20195;&#35780;&#35770;&#23478;&#31574;&#30053;&#20013;&#24471;&#21040;&#30340;&#39069;&#22806;&#32479;&#35745;&#32422;&#26463;&#26469;&#36827;&#19968;&#27493;&#25913;&#36827;&#31163;&#25955;&#30340;SAC&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#32422;&#26463;&#23545;&#20110;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#23433;&#20840;&#37096;&#32626;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#24182;&#25552;&#20379;&#20102;&#23545;&#20302;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;Atari 2600&#28216;&#25103;&#30340;&#20998;&#24067;&#20869;&#21644;&#20998;&#24067;&#22806;&#21464;&#20307;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension to the family of Soft Actor-Critic (SAC) algorithms. We argue that based on the Maximum Entropy Principle, discrete SAC can be further improved via additional statistical constraints derived from a surrogate critic policy. Furthermore, our findings suggests that these constraints provide an added robustness against potential domain shifts, which are essential for safe deployment of reinforcement learning agents in the real-world. We provide theoretical analysis and show empirical results on low data regimes for both in-distribution and out-of-distribution variants of Atari 2600 games.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.06417</link><description>&lt;p&gt;
&#29992;&#20110;&#22270;&#23398;&#20064;&#20013;&#30340;&#25299;&#25169;&#27010;&#25324;&#30340;&#27969;&#21160;&#25193;&#25955;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Advective Diffusion Transformers for Topological Generalization in Graph Learning. (arXiv:2310.06417v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.06417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22312;&#19981;&#21516;&#30340;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#65292;&#22270;&#25193;&#25955;&#26041;&#31243;&#22914;&#20309;&#23545;GNN&#36827;&#34892;&#22806;&#25512;&#21644;&#27010;&#25324;&#65292;&#25581;&#31034;&#20102;&#22522;&#20110;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#25193;&#25955;&#26041;&#31243;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23494;&#20999;&#30456;&#20851;&#65292;&#24182;&#19988;&#26368;&#36817;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#20998;&#26512;GNN&#21160;&#21147;&#23398;&#12289;&#24418;&#24335;&#21270;&#20854;&#34920;&#36798;&#33021;&#21147;&#21644;&#35777;&#26126;&#26550;&#26500;&#36873;&#25321;&#30340;&#26377;&#21407;&#21017;&#30340;&#26694;&#26550;&#12290;&#22270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;GNN&#30340;&#27010;&#25324;&#33021;&#21147;&#12290;&#24403;&#21069;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#22312;&#20110;&#20551;&#35774;&#35757;&#32451;&#38598;&#21644;&#27979;&#35797;&#38598;&#20013;&#30340;&#22270;&#25299;&#25169;&#26469;&#33258;&#30456;&#21516;&#30340;&#20998;&#24067;&#12290;&#26412;&#25991;&#36890;&#36807;&#25506;&#32034;&#22270;&#25193;&#25955;&#26041;&#31243;&#22312;&#19981;&#21516;&#22270;&#25299;&#25169;&#23384;&#22312;&#19979;&#30340;&#22806;&#25512;&#21644;&#27010;&#25324;&#33021;&#21147;&#65292;&#36808;&#20986;&#20102;&#35299;&#26512;GNN&#27010;&#25324;&#24615;&#30340;&#19968;&#27493;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#22522;&#20110;&#22270;&#19978;&#23616;&#37096;&#25193;&#25955;&#30340;&#29616;&#26377;&#27169;&#22411;&#22312;&#27010;&#25324;&#33021;&#21147;&#19978;&#30340;&#19981;&#36275;&#65292;&#36825;&#26159;&#30001;&#20110;&#23545;&#25299;&#25169;&#21464;&#21270;&#30340;&#25351;&#25968;&#25935;&#24863;&#24615;&#24341;&#36215;&#30340;&#12290;&#38543;&#21518;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#38750;&#23616;&#37096;&#25193;&#25955;&#30340;&#28508;&#21147;&#65292;&#23427;&#20513;&#23548;&#23545;&#23436;&#20840;&#36830;&#25509;&#30340;&#28508;&#22312;&#22270;&#36827;&#34892;&#29305;&#24449;&#20256;&#25773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph diffusion equations are intimately related to graph neural networks (GNNs) and have recently attracted attention as a principled framework for analyzing GNN dynamics, formalizing their expressive power, and justifying architectural choices. One key open questions in graph learning is the generalization capabilities of GNNs. A major limitation of current approaches hinges on the assumption that the graph topologies in the training and test sets come from the same distribution. In this paper, we make steps towards understanding the generalization of GNNs by exploring how graph diffusion equations extrapolate and generalize in the presence of varying graph topologies. We first show deficiencies in the generalization capability of existing models built upon local diffusion on graphs, stemming from the exponential sensitivity to topology variation. Our subsequent analysis reveals the promise of non-local diffusion, which advocates for feature propagation over fully-connected latent gr
&lt;/p&gt;</description></item><item><title>LieDetect&#26159;&#19968;&#31181;&#20174;&#32039;&#33268;Lie&#32676;&#30340;&#26377;&#38480;&#26679;&#26412;&#36712;&#36947;&#20013;&#20272;&#35745;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;&#19982;&#20854;&#20182;&#25216;&#26415;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26816;&#32034;&#31934;&#30830;&#30340;&#34920;&#31034;&#31867;&#22411;&#65292;&#24182;&#37325;&#24314;&#20854;&#36712;&#36947;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#35813;&#20316;&#29992;&#30340;Lie&#32676;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32039;&#33268;Lie&#32676;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.03086</link><description>&lt;p&gt;
LieDetect: &#20174;&#28857;&#20113;&#20013;&#26816;&#27979;&#32039;&#33268;Lie&#32676;&#30340;&#34920;&#31034;&#36712;&#36947;
&lt;/p&gt;
&lt;p&gt;
LieDetect: Detection of representation orbits of compact Lie groups from point clouds. (arXiv:2309.03086v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03086
&lt;/p&gt;
&lt;p&gt;
LieDetect&#26159;&#19968;&#31181;&#20174;&#32039;&#33268;Lie&#32676;&#30340;&#26377;&#38480;&#26679;&#26412;&#36712;&#36947;&#20013;&#20272;&#35745;&#34920;&#31034;&#30340;&#26032;&#31639;&#27861;&#12290;&#19982;&#20854;&#20182;&#25216;&#26415;&#19981;&#21516;&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26816;&#32034;&#31934;&#30830;&#30340;&#34920;&#31034;&#31867;&#22411;&#65292;&#24182;&#37325;&#24314;&#20854;&#36712;&#36947;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#35813;&#20316;&#29992;&#30340;Lie&#32676;&#12290;&#35813;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32039;&#33268;Lie&#32676;&#65292;&#24182;&#22312;&#22810;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20174;&#32039;&#33268;Lie&#32676;&#30340;&#26377;&#38480;&#26679;&#26412;&#36712;&#36947;&#20013;&#20272;&#35745;&#34920;&#31034;&#12290;&#19982;&#20854;&#20182;&#25253;&#36947;&#30340;&#25216;&#26415;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#26816;&#32034;&#31934;&#30830;&#30340;&#34920;&#31034;&#31867;&#22411;&#65292;&#20316;&#20026;&#19981;&#21487;&#32422;&#34920;&#31034;&#30340;&#30452;&#21644;&#12290;&#32780;&#19988;&#65292;&#23545;&#34920;&#31034;&#31867;&#22411;&#30340;&#20102;&#35299;&#21487;&#20197;&#37325;&#24314;&#20854;&#36712;&#36947;&#65292;&#26377;&#21161;&#20110;&#35782;&#21035;&#29983;&#25104;&#35813;&#20316;&#29992;&#30340;Lie&#32676;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#32039;&#33268;Lie&#32676;&#65292;&#20294;&#21482;&#32771;&#34385;&#20102;SO(2), T^d, SU(2)&#21644;SO(3)&#30340;&#23454;&#20363;&#21270;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#22312;Hausdorff&#21644;Wasserstein&#36317;&#31163;&#26041;&#38754;&#30340;&#40065;&#26834;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#24037;&#20855;&#26469;&#33258;&#20110;&#20960;&#20309;&#27979;&#24230;&#29702;&#35770;&#65292;&#35745;&#31639;&#20960;&#20309;&#21644;&#30697;&#38453;&#27969;&#24418;&#19978;&#30340;&#20248;&#21270;&#12290;&#31639;&#27861;&#22312;&#39640;&#36798;16&#32500;&#30340;&#21512;&#25104;&#25968;&#25454;&#20197;&#21450;&#22270;&#20687;&#20998;&#26512;&#65292;&#35856;&#27874;&#20998;&#26512;&#21644;&#32463;&#20856;&#21147;&#23398;&#31995;&#32479;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21462;&#24471;&#20102;&#38750;&#24120;&#20934;&#30830;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We suggest a new algorithm to estimate representations of compact Lie groups from finite samples of their orbits. Different from other reported techniques, our method allows the retrieval of the precise representation type as a direct sum of irreducible representations. Moreover, the knowledge of the representation type permits the reconstruction of its orbit, which is useful to identify the Lie group that generates the action. Our algorithm is general for any compact Lie group, but only instantiations for SO(2), T^d, SU(2) and SO(3) are considered. Theoretical guarantees of robustness in terms of Hausdorff and Wasserstein distances are derived. Our tools are drawn from geometric measure theory, computational geometry, and optimization on matrix manifolds. The algorithm is tested for synthetic data up to dimension 16, as well as real-life applications in image analysis, harmonic analysis, and classical mechanics systems, achieving very accurate results.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#36941;&#21382;&#25968;&#25454;&#24207;&#21015;&#19978;&#35757;&#32451;&#26102;&#30340;&#26680;&#26497;&#38480;&#65292;&#21033;&#29992;&#25968;&#23398;&#26041;&#27861;&#23545;&#20854;&#28176;&#36817;&#29305;&#24615;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#35777;&#26126;&#20102;RNN&#25910;&#25947;&#21040;&#19982;&#38543;&#26426;&#20195;&#25968;&#26041;&#31243;&#30340;&#19981;&#21160;&#28857;&#32806;&#21512;&#30340;&#26080;&#31351;&#32500;ODE&#30340;&#35299;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#25913;&#36827;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.14555</link><description>&lt;p&gt;
&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#36941;&#21382;&#25968;&#25454;&#24207;&#21015;&#19978;&#35757;&#32451;&#30340;&#26680;&#26497;&#38480;
&lt;/p&gt;
&lt;p&gt;
Kernel Limit of Recurrent Neural Networks Trained on Ergodic Data Sequences. (arXiv:2308.14555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#36941;&#21382;&#25968;&#25454;&#24207;&#21015;&#19978;&#35757;&#32451;&#26102;&#30340;&#26680;&#26497;&#38480;&#65292;&#21033;&#29992;&#25968;&#23398;&#26041;&#27861;&#23545;&#20854;&#28176;&#36817;&#29305;&#24615;&#36827;&#34892;&#20102;&#25551;&#36848;&#65292;&#24182;&#35777;&#26126;&#20102;RNN&#25910;&#25947;&#21040;&#19982;&#38543;&#26426;&#20195;&#25968;&#26041;&#31243;&#30340;&#19981;&#21160;&#28857;&#32806;&#21512;&#30340;&#26080;&#31351;&#32500;ODE&#30340;&#35299;&#12290;&#36825;&#23545;&#20110;&#29702;&#35299;&#21644;&#25913;&#36827;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#25968;&#23398;&#26041;&#27861;&#26469;&#25551;&#36848;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#30340;&#28176;&#36817;&#29305;&#24615;&#65292;&#20854;&#20013;&#38544;&#34255;&#21333;&#20803;&#30340;&#25968;&#37327;&#12289;&#24207;&#21015;&#20013;&#30340;&#25968;&#25454;&#26679;&#26412;&#12289;&#38544;&#34255;&#29366;&#24577;&#30340;&#26356;&#26032;&#21644;&#35757;&#32451;&#27493;&#39588;&#21516;&#26102;&#36235;&#20110;&#26080;&#31351;&#22823;&#12290;&#23545;&#20110;&#20855;&#26377;&#31616;&#21270;&#26435;&#37325;&#30697;&#38453;&#30340;RNN&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;RNN&#25910;&#25947;&#21040;&#19982;&#38543;&#26426;&#20195;&#25968;&#26041;&#31243;&#30340;&#19981;&#21160;&#28857;&#32806;&#21512;&#30340;&#26080;&#31351;&#32500;ODE&#30340;&#35299;&#12290;&#20998;&#26512;&#38656;&#35201;&#35299;&#20915;RNN&#25152;&#29305;&#26377;&#30340;&#20960;&#20010;&#25361;&#25112;&#12290;&#22312;&#20856;&#22411;&#30340;&#22343;&#22330;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65289;&#65292;&#31163;&#25955;&#30340;&#26356;&#26032;&#37327;&#20026;$\mathcal{O}(\frac{1}{N})$&#65292;&#26356;&#26032;&#30340;&#27425;&#25968;&#20026;$\mathcal{O}(N)$&#12290;&#22240;&#27492;&#65292;&#31995;&#32479;&#21487;&#20197;&#34920;&#31034;&#20026;&#36866;&#24403;ODE/PDE&#30340;Euler&#36924;&#36817;&#65292;&#24403;$N \rightarrow \infty$&#26102;&#25910;&#25947;&#21040;&#35813;ODE/PDE&#12290;&#28982;&#32780;&#65292;RNN&#30340;&#38544;&#34255;&#23618;&#26356;&#26032;&#20026;$\mathcal{O}(1)$&#12290;&#22240;&#27492;&#65292;RNN&#19981;&#33021;&#34920;&#31034;&#20026;ODE/PDE&#30340;&#31163;&#25955;&#21270;&#21644;&#26631;&#20934;&#22343;&#22330;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mathematical methods are developed to characterize the asymptotics of recurrent neural networks (RNN) as the number of hidden units, data samples in the sequence, hidden state updates, and training steps simultaneously grow to infinity. In the case of an RNN with a simplified weight matrix, we prove the convergence of the RNN to the solution of an infinite-dimensional ODE coupled with the fixed point of a random algebraic equation. The analysis requires addressing several challenges which are unique to RNNs. In typical mean-field applications (e.g., feedforward neural networks), discrete updates are of magnitude $\mathcal{O}(\frac{1}{N})$ and the number of updates is $\mathcal{O}(N)$. Therefore, the system can be represented as an Euler approximation of an appropriate ODE/PDE, which it will converge to as $N \rightarrow \infty$. However, the RNN hidden layer updates are $\mathcal{O}(1)$. Therefore, RNNs cannot be represented as a discretization of an ODE/PDE and standard mean-field tec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#20351;&#29992;Wasserstein&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#26377;&#25928;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2308.14048</link><description>&lt;p&gt;
&#19968;&#31181;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#29992;&#20110;&#29983;&#25104;&#27169;&#22411;&#65306;&#20351;&#29992;Wasserstein&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#38598;&#25104;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
A Bayesian Non-parametric Approach to Generative Models: Integrating Variational Autoencoder and Generative Adversarial Networks using Wasserstein and Maximum Mean Discrepancy. (arXiv:2308.14048v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#30340;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#20351;&#29992;Wasserstein&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#23545;&#28508;&#22312;&#31354;&#38388;&#30340;&#26377;&#25928;&#23398;&#20064;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#24050;&#25104;&#20026;&#19968;&#31181;&#20135;&#29983;&#19982;&#30495;&#23454;&#22270;&#20687;&#38590;&#20197;&#21306;&#20998;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#26377;&#21069;&#36884;&#30340;&#25216;&#26415;&#12290;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#26159;&#26368;&#20026;&#37325;&#35201;&#19988;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;&#20004;&#31181;&#29983;&#25104;&#27169;&#22411;&#12290;GAN&#22312;&#29983;&#25104;&#36924;&#30495;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;VAE&#21017;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;GAN&#24573;&#35270;&#20102;&#22823;&#37096;&#20998;&#21487;&#33021;&#30340;&#36755;&#20986;&#31354;&#38388;&#65292;&#36825;&#23548;&#33268;&#19981;&#33021;&#23436;&#20840;&#20307;&#29616;&#30446;&#26631;&#20998;&#24067;&#30340;&#22810;&#26679;&#24615;&#65292;&#32780;VAE&#21017;&#24120;&#24120;&#29983;&#25104;&#27169;&#31946;&#22270;&#20687;&#12290;&#20026;&#20102;&#20805;&#20998;&#21457;&#25381;&#20004;&#31181;&#27169;&#22411;&#30340;&#20248;&#28857;&#24182;&#20943;&#36731;&#23427;&#20204;&#30340;&#24369;&#28857;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#36125;&#21494;&#26031;&#38750;&#21442;&#25968;&#26041;&#27861;&#23558;GAN&#21644;VAE&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25439;&#22833;&#20989;&#25968;&#20013;&#21516;&#26102;&#20351;&#29992;&#20102;Wasserstein&#21644;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#24230;&#37327;&#65292;&#20197;&#26377;&#25928;&#23398;&#20064;&#28508;&#22312;&#31354;&#38388;&#24182;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative models have emerged as a promising technique for producing high-quality images that are indistinguishable from real images. Generative adversarial networks (GANs) and variational autoencoders (VAEs) are two of the most prominent and widely studied generative models. GANs have demonstrated excellent performance in generating sharp realistic images and VAEs have shown strong abilities to generate diverse images. However, GANs suffer from ignoring a large portion of the possible output space which does not represent the full diversity of the target distribution, and VAEs tend to produce blurry images. To fully capitalize on the strengths of both models while mitigating their weaknesses, we employ a Bayesian non-parametric (BNP) approach to merge GANs and VAEs. Our procedure incorporates both Wasserstein and maximum mean discrepancy (MMD) measures in the loss function to enable effective learning of the latent space and generate diverse and high-quality samples. By fusing the di
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#22522;&#20934;&#21270;&#23454;&#39564;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#20173;&#28982;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#21644;&#39318;&#36873;&#30340;&#36873;&#25321;&#65292;&#24182;&#25581;&#31034;&#20102;&#27169;&#22411;&#20860;&#23481;&#24615;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.08954</link><description>&lt;p&gt;
&#37325;&#26032;&#22522;&#20934;&#21270;&#38754;&#21521;&#20108;&#20998;&#31867;&#30340;&#22522;&#20110;&#27744;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Re-Benchmarking Pool-Based Active Learning for Binary Classification. (arXiv:2306.08954v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#22522;&#20934;&#21270;&#23454;&#39564;&#35777;&#26126;&#20102;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#20173;&#28982;&#26159;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#26377;&#25928;&#21644;&#39318;&#36873;&#30340;&#36873;&#25321;&#65292;&#24182;&#25581;&#31034;&#20102;&#27169;&#22411;&#20860;&#23481;&#24615;&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#21160;&#23398;&#20064;&#26159;&#19968;&#31181;&#26174;&#33879;&#25552;&#21319;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#24615;&#33021;&#30340;&#33539;&#24335;&#65292;&#24403;&#33719;&#21462;&#26631;&#35760;&#25968;&#25454;&#20195;&#20215;&#26114;&#36149;&#26102;&#29305;&#21035;&#26377;&#29992;&#12290;&#23613;&#31649;&#23384;&#22312;&#22810;&#20010;&#29992;&#20110;&#35780;&#20272;&#20027;&#21160;&#23398;&#20064;&#31574;&#30053;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#23427;&#20204;&#30340;&#21457;&#29616;&#23384;&#22312;&#19968;&#23450;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#36825;&#31181;&#24046;&#24322;&#28608;&#21457;&#25105;&#20204;&#20026;&#31038;&#21306;&#24320;&#21457;&#19968;&#20010;&#36879;&#26126;&#19988;&#21487;&#22797;&#29616;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#21162;&#21147;&#32467;&#26524;&#26159;&#19968;&#20010;&#21487;&#38752;&#19988;&#21487;&#25193;&#23637;&#29992;&#20110;&#26410;&#26469;&#30740;&#31350;&#30340;&#24320;&#28304;&#23454;&#29616;&#65288;https://github.com/ariapoy/active-learning-benchmark&#65289;&#12290;&#36890;&#36807;&#36827;&#34892;&#24443;&#24213;&#30340;&#37325;&#26032;&#22522;&#20934;&#21270;&#23454;&#39564;&#65292;&#25105;&#20204;&#19981;&#20165;&#32416;&#27491;&#20102;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#35823;&#37197;&#32622;&#38382;&#39064;&#65292;&#36824;&#25581;&#31034;&#20102;&#27169;&#22411;&#20860;&#23481;&#24615;&#36825;&#20010;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#38382;&#39064;&#65292;&#36825;&#30452;&#25509;&#23548;&#33268;&#20102;&#35266;&#23519;&#21040;&#30340;&#19981;&#19968;&#33268;&#24615;&#12290;&#35299;&#20915;&#36825;&#20010;&#24046;&#24322;&#20351;&#24471;&#19981;&#30830;&#23450;&#24615;&#37319;&#26679;&#31574;&#30053;&#20445;&#25345;&#20102;&#22312;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#19978;&#26159;&#19968;&#20010;&#26377;&#25928;&#19988;&#39318;&#36873;&#30340;&#36873;&#25321;&#12290;&#25105;&#20204;&#30340;&#32463;&#39564;&#24378;&#35843;&#20102;&#23558;&#30740;&#31350;&#21162;&#21147;&#25237;&#20837;&#21040;&#37325;&#26032;&#22522;&#20934;&#21270;&#19978;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Active learning is a paradigm that significantly enhances the performance of machine learning models when acquiring labeled data is expensive. While several benchmarks exist for evaluating active learning strategies, their findings exhibit some misalignment. This discrepancy motivates us to develop a transparent and reproducible benchmark for the community. Our efforts result in an open-sourced implementation (https://github.com/ariapoy/active-learning-benchmark) that is reliable and extensible for future research. By conducting thorough re-benchmarking experiments, we have not only rectified misconfigurations in existing benchmark but also shed light on the under-explored issue of model compatibility, which directly causes the observed discrepancy. Resolving the discrepancy reassures that the uncertainty sampling strategy of active learning remains an effective and preferred choice for most datasets. Our experience highlights the importance of dedicating research efforts towards re-be
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#65292;&#24182;&#35201;&#27714;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#12289;&#20351;&#29992;&#20844;&#35748;&#25968;&#25454;&#38598;&#20197;&#21450;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;</title><link>http://arxiv.org/abs/2305.04532</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#26368;&#26032;&#36235;&#21183;&#65306;&#19968;&#20010;&#33539;&#22260;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Latest Trends in Artificial Intelligence Technology: A Scoping Review. (arXiv:2305.04532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#65292;&#24182;&#35201;&#27714;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#12289;&#20351;&#29992;&#20844;&#35748;&#25968;&#25454;&#38598;&#20197;&#21450;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#22810;&#20010;&#39046;&#22495;&#12290;&#26234;&#33021;&#25163;&#26426;&#12289;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#12289;&#25628;&#32034;&#24341;&#25806;&#21644;&#33258;&#20027;&#39550;&#39542;&#36710;&#36742;&#31561;&#24212;&#29992;&#31243;&#24207;&#37117;&#21033;&#29992;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#20197;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#26412;&#30740;&#31350;&#25353;&#29031; PRISMA &#26694;&#26550;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36827;&#34892;&#20102;&#33539;&#22260;&#35780;&#20272;&#12290;&#30446;&#26631;&#26159;&#23547;&#25214;&#24212;&#29992;&#20110;&#19981;&#21516;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30740;&#31350;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#12290;&#20174;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#36873;&#21462;&#20102;&#19977;&#20010;&#30693;&#21517;&#26399;&#21002;&#65306;&#12298;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#26434;&#24535;&#12299;&#12289;&#12298;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#26434;&#24535;&#12299;&#21644;&#12298;&#26426;&#22120;&#23398;&#20064;&#12299;&#65292;&#24182;&#35266;&#23519;&#20102;2022&#24180;&#21457;&#34920;&#30340;&#25991;&#31456;&#12290;&#23545;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#19968;&#23450;&#30340;&#36164;&#26684;&#35201;&#27714;&#65306;&#25216;&#26415;&#24517;&#39035;&#38024;&#23545;&#21487;&#27604;&#36739;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#27979;&#35797;&#65292;&#24517;&#39035;&#20351;&#29992;&#20844;&#35748;&#25110;&#20854;&#20182;&#20805;&#20998;&#35777;&#26126;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#24212;&#29992;&#65292;&#24182;&#30830;&#20445;&#32467;&#26524;&#21487;&#22797;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence is more ubiquitous in multiple domains. Smartphones, social media platforms, search engines, and autonomous vehicles are just a few examples of applications that utilize artificial intelligence technologies to enhance their performance. This study carries out a scoping review of the current state-of-the-art artificial intelligence technologies following the PRISMA framework. The goal was to find the most advanced technologies used in different domains of artificial intelligence technology research. Three recognized journals were used from artificial intelligence and machine learning domain: Journal of Artificial Intelligence Research, Journal of Machine Learning Research, and Machine Learning, and articles published in 2022 were observed. Certain qualifications were laid for the technological solutions: the technology must be tested against comparable solutions, commonly approved or otherwise well justified datasets must be used while applying, and results must 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#38598;&#65292;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10805</link><description>&lt;p&gt;
RPLKG: &#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RPLKG: Robust Prompt Learning with Knowledge Graph. (arXiv:2304.10805v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#38598;&#65292;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#21487;&#36801;&#31227;&#30340;&#65292;&#24182;&#19988;&#23545;&#26410;&#30693;&#25968;&#25454;&#38598;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#35832;&#22914;CLIP&#20043;&#31867;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#24403;&#26631;&#35760;&#25968;&#25454;&#38598;&#26377;&#38480;&#26102;&#65292;&#26032;&#25968;&#25454;&#38598;&#25110;&#39046;&#22495;&#30340;&#27867;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#21508;&#31181;&#21162;&#21147;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#21644;&#36866;&#37197;&#22120;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#33258;&#36866;&#24212;&#26041;&#27861;&#19981;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#65288;RPLKG&#65289;&#12290;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#33258;&#21160;&#35774;&#35745;&#20986;&#21508;&#31181;&#21487;&#35299;&#37322;&#21644;&#26377;&#24847;&#20041;&#30340;&#25552;&#31034;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19968;&#27425;&#27491;&#21521;&#20256;&#36882;&#21518;&#33719;&#24471;&#25552;&#31034;&#38598;&#30340;&#32531;&#23384;&#23884;&#20837;&#12290;&#20043;&#21518;&#65292;&#27169;&#22411;&#20351;&#29992;GumbelSoftmax&#20248;&#21270;&#25552;&#31034;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained models have been known that they are transferable, and they generalize well on the unseen dataset. Recently, multimodal pre-trained models such as CLIP show significant performance improvement in diverse experiments. However, when the labeled dataset is limited, the generalization of a new dataset or domain is still challenging. To improve the generalization performance on few-shot learning, there have been diverse efforts, such as prompt learning and adapter. However, the current few-shot adaptation methods are not interpretable, and they require a high computation cost for adaptation. In this study, we propose a new method, robust prompt learning with knowledge graph (RPLKG). Based on the knowledge graph, we automatically design diverse interpretable and meaningful prompt sets. Our model obtains cached embeddings of prompt sets after one forwarding from a large pre-trained model. After that, model optimizes the prompt selection processes with GumbelSoftmax. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2304.10640</link><description>&lt;p&gt;
&#35770;&#25968;&#25454;&#24322;&#26500;&#24615;&#23545;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#27714;&#35299;&#22120;&#25910;&#25947;&#36895;&#24230;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Effects of Data Heterogeneity on the Convergence Rates of Distributed Linear System Solvers. (arXiv:2304.10640v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#27714;&#35299;&#20998;&#24067;&#24335;&#32447;&#24615;&#31995;&#32479;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#23545;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;(APC&#21644;D-HBM)&#30340;&#25910;&#25947;&#36895;&#24230;&#36827;&#34892;&#20102;&#32422;&#26463;&#21644;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#35299;&#20915;&#22823;&#35268;&#27169;&#32447;&#24615;&#26041;&#31243;&#32452;&#30340;&#22522;&#26412;&#38382;&#39064;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#32771;&#34385;&#20219;&#21153;&#36127;&#36131;&#20154;&#25171;&#31639;&#22312;&#19968;&#32452;&#20855;&#26377;&#19968;&#20123;&#26041;&#31243;&#32452;&#23376;&#38598;&#30340;&#26426;&#22120;&#30340;&#20998;&#24067;&#24335;/&#32852;&#21512;&#24110;&#21161;&#19979;&#35299;&#20915;&#35813;&#31995;&#32479;&#30340;&#35774;&#32622;&#12290;&#34429;&#28982;&#26377;&#20960;&#31181;&#26041;&#27861;&#29992;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#32570;&#23569;&#23545;&#25237;&#24433;&#26041;&#27861;&#21644;&#20248;&#21270;&#26041;&#27861;&#25910;&#25947;&#36895;&#24230;&#30340;&#20005;&#26684;&#27604;&#36739;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#24182;&#27604;&#36739;&#36825;&#20004;&#31867;&#31639;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#27599;&#20010;&#31867;&#21035;&#20013;&#26368;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21363;&#26368;&#36817;&#25552;&#20986;&#30340;&#21152;&#36895;&#25237;&#24433;&#19968;&#33268;&#24615;(APC)&#21644;&#20998;&#24067;&#24335;&#37325;&#29699;&#26041;&#27861;(D-HBM)&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#31216;&#20026;&#35282;&#24322;&#26500;&#24615;&#30340;&#20960;&#20309;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20854;&#26222;&#36941;&#24615;&#12290;&#20351;&#29992;&#35813;&#27010;&#24565;&#65292;&#25105;&#20204;&#32422;&#26463;&#24182;&#27604;&#36739;&#25152;&#30740;&#31350;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#25429;&#25417;&#20004;&#31181;&#26041;&#27861;&#30340;&#24322;&#26500;&#25968;&#25454;&#30340;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the fundamental problem of solving a large-scale system of linear equations. In particular, we consider the setting where a taskmaster intends to solve the system in a distributed/federated fashion with the help of a set of machines, who each have a subset of the equations. Although there exist several approaches for solving this problem, missing is a rigorous comparison between the convergence rates of the projection-based methods and those of the optimization-based ones. In this paper, we analyze and compare these two classes of algorithms with a particular focus on the most efficient method from each class, namely, the recently proposed Accelerated Projection-Based Consensus (APC) and the Distributed Heavy-Ball Method (D-HBM). To this end, we first propose a geometric notion of data heterogeneity called angular heterogeneity and discuss its generality. Using this notion, we bound and compare the convergence rates of the studied algorithms and capture the effects of both 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#19979;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#37327;$O(\lambda/n)$&#26469;&#19978;&#30028;&#20272;&#35745;&#27867;&#21270;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2303.14658</link><description>&lt;p&gt;
&#20851;&#20110;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the tightness of information-theoretic bounds on generalization error of learning algorithms. (arXiv:2303.14658v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#30340;&#32039;&#23494;&#24615;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#20551;&#35774;&#65292;&#21487;&#20197;&#22312;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#19979;&#20351;&#29992;&#20449;&#24687;&#29702;&#35770;&#37327;$O(\lambda/n)$&#26469;&#19978;&#30028;&#20272;&#35745;&#27867;&#21270;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Russo&#21644;Xu&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35777;&#26126;&#23398;&#20064;&#31639;&#27861;&#30340;&#27867;&#21270;&#35823;&#24046;&#21487;&#20197;&#36890;&#36807;&#20449;&#24687;&#24230;&#37327;&#36827;&#34892;&#19978;&#30028;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25910;&#25947;&#36895;&#24230;&#36890;&#24120;&#34987;&#35748;&#20026;&#26159;&#8220;&#24930;&#8221;&#30340;&#65292;&#22240;&#20026;&#23427;&#30340;&#26399;&#26395;&#25910;&#25947;&#36895;&#24230;&#30340;&#24418;&#24335;&#20026;$O(\sqrt{\lambda/n})$&#65292;&#20854;&#20013;$\lambda$&#26159;&#19968;&#20123;&#20449;&#24687;&#29702;&#35770;&#37327;&#12290;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#35777;&#26126;&#20102;&#26681;&#21495;&#24182;&#19981;&#19968;&#23450;&#24847;&#21619;&#30528;&#25910;&#25947;&#36895;&#24230;&#24930;&#65292;&#21487;&#20197;&#22312;&#36866;&#24403;&#30340;&#20551;&#35774;&#19979;&#20351;&#29992;&#36825;&#20010;&#30028;&#38480;&#26469;&#24471;&#21040;$O(\lambda/n)$&#30340;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36798;&#21040;&#24555;&#36895;&#25910;&#25947;&#36895;&#24230;&#30340;&#20851;&#38190;&#26465;&#20214;&#65292;&#21363;&#25152;&#35859;&#30340;$(\eta,c)$-&#20013;&#24515;&#26465;&#20214;&#12290;&#22312;&#36825;&#20010;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#32473;&#20986;&#20102;&#23398;&#20064;&#31639;&#27861;&#27867;&#21270;&#35823;&#24046;&#30340;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent line of works, initiated by Russo and Xu, has shown that the generalization error of a learning algorithm can be upper bounded by information measures. In most of the relevant works, the convergence rate of the expected generalization error is in the form of $O(\sqrt{\lambda/n})$ where $\lambda$ is some information-theoretic quantities such as the mutual information or conditional mutual information between the data and the learned hypothesis. However, such a learning rate is typically considered to be ``slow", compared to a ``fast rate" of $O(\lambda/n)$ in many learning scenarios. In this work, we first show that the square root does not necessarily imply a slow rate, and a fast rate result can still be obtained using this bound under appropriate assumptions. Furthermore, we identify the critical conditions needed for the fast rate generalization error, which we call the $(\eta,c)$-central condition. Under this condition, we give information-theoretic bounds on the generaliz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.11536</link><description>&lt;p&gt;
&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Indeterminate Probability Neural Network. (arXiv:2303.11536v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11536
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#8212;&#8212;&#19981;&#23450;&#27010;&#29575;&#31070;&#32463;&#32593;&#32476;&#65307;&#23427;&#21487;&#20197;&#36827;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#21644;&#20351;&#29992;&#24456;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22823;&#35268;&#27169;&#20998;&#31867;&#65292;&#20854;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;IPNN&#30340;&#26032;&#22411;&#36890;&#29992;&#27169;&#22411;&#65292;&#23427;&#23558;&#31070;&#32463;&#32593;&#32476;&#21644;&#27010;&#29575;&#35770;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#22312;&#20256;&#32479;&#27010;&#29575;&#35770;&#20013;&#65292;&#27010;&#29575;&#30340;&#35745;&#31639;&#26159;&#22522;&#20110;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#32780;&#36825;&#22312;&#24403;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#20013;&#20960;&#20046;&#19981;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#65292;&#23427;&#26159;&#32463;&#20856;&#27010;&#29575;&#35770;&#30340;&#25193;&#23637;&#65292;&#24182;&#20351;&#32463;&#20856;&#27010;&#29575;&#35770;&#25104;&#20026;&#25105;&#20204;&#29702;&#35770;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#25105;&#20204;&#25552;&#20986;&#30340;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#65292;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20986;&#34987;&#23450;&#20041;&#20026;&#27010;&#29575;&#20107;&#20214;&#65292;&#24182;&#22522;&#20110;&#36825;&#20123;&#20107;&#20214;&#30340;&#32479;&#35745;&#20998;&#26512;&#65292;&#25512;&#23548;&#20986;&#20998;&#31867;&#20219;&#21153;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;IPNN&#23637;&#29616;&#20102;&#26032;&#30340;&#29305;&#24615;&#65306;&#23427;&#22312;&#36827;&#34892;&#20998;&#31867;&#30340;&#21516;&#26102;&#21487;&#20197;&#25191;&#34892;&#26080;&#30417;&#30563;&#32858;&#31867;&#12290;&#27492;&#22806;&#65292;IPNN&#33021;&#22815;&#20351;&#29992;&#38750;&#24120;&#23567;&#30340;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#38750;&#24120;&#22823;&#30340;&#20998;&#31867;&#65292;&#20363;&#22914;100&#20010;&#36755;&#20986;&#33410;&#28857;&#30340;&#27169;&#22411;&#21487;&#20197;&#20998;&#31867;10&#20159;&#31867;&#21035;&#12290;&#29702;&#35770;&#20248;&#21183;&#20307;&#29616;&#22312;&#26032;&#30340;&#27010;&#29575;&#29702;&#35770;&#21644;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#24182;&#19988;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;IPNN&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new general model called IPNN - Indeterminate Probability Neural Network, which combines neural network and probability theory together. In the classical probability theory, the calculation of probability is based on the occurrence of events, which is hardly used in current neural networks. In this paper, we propose a new general probability theory, which is an extension of classical probability theory, and makes classical probability theory a special case to our theory. Besides, for our proposed neural network framework, the output of neural network is defined as probability events, and based on the statistical analysis of these events, the inference model for classification task is deduced. IPNN shows new property: It can perform unsupervised clustering while doing classification. Besides, IPNN is capable of making very large classification with very small neural network, e.g. model with 100 output nodes can classify 10 billion categories. Theoretical advantages are refl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#21644;&#37325;&#29699;&#21160;&#37327;&#36827;&#34892;&#21152;&#36895;&#65292;&#22312;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#20013;&#20445;&#25345;&#24555;&#36895;&#25910;&#25947;&#29575;&#12290;</title><link>http://arxiv.org/abs/2206.07553</link><description>&lt;p&gt;
&#35770;&#23567;&#25209;&#37327;&#37325;&#29699;&#21160;&#37327;&#27861;&#30340;&#24555;&#36895;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the fast convergence of minibatch heavy ball momentum. (arXiv:2206.07553v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07553
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#65292;&#20351;&#29992;&#23567;&#25209;&#37327;&#21644;&#37325;&#29699;&#21160;&#37327;&#36827;&#34892;&#21152;&#36895;&#65292;&#22312;&#20108;&#27425;&#20248;&#21270;&#38382;&#39064;&#20013;&#20445;&#25345;&#24555;&#36895;&#25910;&#25947;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#30340;&#38543;&#26426;&#21160;&#37327;&#26041;&#27861;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20248;&#21270;&#20013;&#65292;&#20294;&#30001;&#20110;&#36824;&#27809;&#26377;&#21152;&#36895;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#36825;&#19982;&#23427;&#20204;&#22312;&#23454;&#36341;&#20013;&#30340;&#33391;&#22909;&#24615;&#33021;&#24182;&#19981;&#30456;&#31526;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23637;&#31034;&#65292;&#38543;&#26426;&#37325;&#29699;&#21160;&#37327;&#22312;&#20108;&#27425;&#26368;&#20248;&#21270;&#38382;&#39064;&#20013;&#20445;&#25345;&#65288;&#30830;&#23450;&#24615;&#65289;&#37325;&#29699;&#21160;&#37327;&#30340;&#24555;&#36895;&#32447;&#24615;&#29575;&#65292;&#33267;&#23569;&#22312;&#20351;&#29992;&#36275;&#22815;&#22823;&#30340;&#25209;&#37327;&#22823;&#23567;&#36827;&#34892;&#23567;&#25209;&#37327;&#22788;&#29702;&#26102;&#12290;&#25105;&#20204;&#25152;&#30740;&#31350;&#30340;&#31639;&#27861;&#21487;&#20197;&#34987;&#35299;&#37322;&#20026;&#24102;&#23567;&#25209;&#37327;&#22788;&#29702;&#21644;&#37325;&#29699;&#21160;&#37327;&#30340;&#21152;&#36895;&#38543;&#26426;Kaczmarz&#31639;&#27861;&#12290;&#35813;&#20998;&#26512;&#20381;&#36182;&#20110;&#20180;&#32454;&#20998;&#35299;&#21160;&#37327;&#36716;&#31227;&#30697;&#38453;&#65292;&#24182;&#20351;&#29992;&#26032;&#30340;&#29420;&#31435;&#38543;&#26426;&#30697;&#38453;&#20056;&#31215;&#30340;&#35889;&#33539;&#22260;&#38598;&#20013;&#30028;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#25968;&#20540;&#28436;&#31034;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#30028;&#38480;&#30456;&#24403;&#23574;&#38160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simple stochastic momentum methods are widely used in machine learning optimization, but their good practical performance is at odds with an absence of theoretical guarantees of acceleration in the literature. In this work, we aim to close the gap between theory and practice by showing that stochastic heavy ball momentum retains the fast linear rate of (deterministic) heavy ball momentum on quadratic optimization problems, at least when minibatching with a sufficiently large batch size. The algorithm we study can be interpreted as an accelerated randomized Kaczmarz algorithm with minibatching and heavy ball momentum. The analysis relies on carefully decomposing the momentum transition matrix, and using new spectral norm concentration bounds for products of independent random matrices. We provide numerical illustrations demonstrating that our bounds are reasonably sharp.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#24230;&#37327;&#23618;&#27425;&#32858;&#31867;&#30340;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#31283;&#23450;&#19968;&#33268;&#30340;&#23494;&#24230;-based&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#20013;&#25552;&#21462;&#21333;&#20010;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2005.09048</link><description>&lt;p&gt;
&#31283;&#23450;&#19968;&#33268;&#30340;&#23494;&#24230;-based&#32858;&#31867;&#31639;&#27861;&#36890;&#36807;&#22810;&#21442;&#25968;&#25345;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
Stable and consistent density-based clustering via multiparameter persistence. (arXiv:2005.09048v3 [math.ST] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2005.09048
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#31181;&#24230;&#37327;&#23618;&#27425;&#32858;&#31867;&#30340;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#65292;&#30740;&#31350;&#20102;&#19968;&#31181;&#31283;&#23450;&#19968;&#33268;&#30340;&#23494;&#24230;-based&#32858;&#31867;&#31639;&#27861;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20174;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#20013;&#25552;&#21462;&#21333;&#20010;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#20013;&#30340;&#24230;-Rips&#26500;&#36896;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#23494;&#24230;&#25935;&#24863;&#30340;&#22810;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#24341;&#20837;&#30340;&#19968;&#31181;&#24230;&#37327;&#23618;&#27425;&#32858;&#31867;&#30340;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#65292;&#20998;&#26512;&#20102;&#23427;&#23545;&#36755;&#20837;&#25968;&#25454;&#30340;&#25200;&#21160;&#30340;&#31283;&#23450;&#24615;&#12290;&#20174;&#24230;-Rips&#20013;&#21462;&#26576;&#20123;&#19968;&#21442;&#25968;&#20999;&#29255;&#21487;&#20197;&#24674;&#22797;&#20986;&#24050;&#30693;&#30340;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20123;&#26041;&#27861;&#26159;&#19981;&#31283;&#23450;&#30340;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#20316;&#20026;&#22810;&#21442;&#25968;&#23545;&#35937;&#30340;&#24230;-Rips&#26159;&#31283;&#23450;&#30340;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#24230;-Rips&#20013;&#21462;&#20999;&#29255;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20135;&#29983;&#19968;&#20010;&#20855;&#26377;&#26356;&#22909;&#31283;&#23450;&#24615;&#23646;&#24615;&#30340;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20174;&#19968;&#21442;&#25968;&#23618;&#27425;&#32858;&#31867;&#20013;&#25552;&#21462;&#21333;&#20010;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#23545;&#24212;&#20132;&#38169;&#36317;&#31163;&#26041;&#38754;&#26159;&#31283;&#23450;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the degree-Rips construction from topological data analysis, which provides a density-sensitive, multiparameter hierarchical clustering algorithm. We analyze its stability to perturbations of the input data using the correspondence-interleaving distance, a metric for hierarchical clusterings that we introduce. Taking certain one-parameter slices of degree-Rips recovers well-known methods for density-based clustering, but we show that these methods are unstable. However, we prove that degree-Rips, as a multiparameter object, is stable, and we propose an alternative approach for taking slices of degree-Rips, which yields a one-parameter hierarchical clustering algorithm with better stability properties. We prove that this algorithm is consistent, using the correspondence-interleaving distance. We provide an algorithm for extracting a single clustering from one-parameter hierarchical clusterings, which is stable with respect to the correspondence-interleaving distance. And, we
&lt;/p&gt;</description></item></channel></rss>