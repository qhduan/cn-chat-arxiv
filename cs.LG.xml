<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20287</link><description>&lt;p&gt;
&#22522;&#20934;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Counterfactual Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#65292;&#21253;&#21547;&#35780;&#20272;&#23545;&#29031;&#22810;&#20010;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#20197;&#21450;&#35780;&#20272;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#22312;&#29702;&#35299;&#21464;&#37327;&#22240;&#26524;&#20851;&#31995;&#26041;&#38754;&#20855;&#26377;&#20851;&#38190;&#20316;&#29992;&#65292;&#22312;&#35299;&#37322;&#24615;&#21644;&#29983;&#25104;&#26080;&#20559;&#21512;&#25104;&#25968;&#25454;&#26041;&#38754;&#26377;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;&#22270;&#20687;&#29983;&#25104;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#23545;&#20110;&#35780;&#20272;&#23545;&#29031;&#29983;&#25104;&#30340;&#38656;&#27714;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#19968;&#25361;&#25112;&#65292;&#22240;&#20026;&#26681;&#25454;&#23450;&#20041;&#65292;&#23545;&#29031;&#24773;&#26223;&#26159;&#27809;&#26377;&#21487;&#35266;&#27979;&#22522;&#20934;&#20107;&#23454;&#30340;&#20551;&#35774;&#24773;&#20917;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23545;&#29031;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;&#26032;&#39062;&#32508;&#21512;&#26694;&#26550;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#20391;&#37325;&#20110;&#35780;&#20272;&#23545;&#29031;&#30340;&#19981;&#21516;&#26041;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#20363;&#22914;&#32452;&#25104;&#12289;&#26377;&#25928;&#24615;&#12289;&#24178;&#39044;&#30340;&#26368;&#23567;&#24615;&#21644;&#22270;&#20687;&#36924;&#30495;&#24230;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#33539;&#24335;&#30340;&#19977;&#31181;&#19981;&#21516;&#26465;&#20214;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#31867;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36824;&#37197;&#22791;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;Python&#36719;&#20214;&#21253;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20287v1 Announce Type: cross  Abstract: Counterfactual image generation is pivotal for understanding the causal relations of variables, with applications in interpretability and generation of unbiased synthetic data. However, evaluating image generation is a long-standing challenge in itself. The need to evaluate counterfactual generation compounds on this challenge, precisely because counterfactuals, by definition, are hypothetical scenarios without observable ground truths. In this paper, we present a novel comprehensive framework aimed at benchmarking counterfactual image generation methods. We incorporate metrics that focus on evaluating diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We assess the performance of three distinct conditional image generation model types, based on the Structural Causal Model paradigm. Our work is accompanied by a user-friendly Python package which allows to further eval
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07657</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Scalable Spatiotemporal Prediction with Bayesian Neural Fields
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07657
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#22330;&#65288;BayesNF&#65289;&#65292;&#32467;&#21512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#65292;&#29992;&#20110;&#22788;&#29702;&#22823;&#35268;&#27169;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#31354;&#25968;&#25454;&#38598;&#30001;&#31354;&#38388;&#21442;&#32771;&#30340;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#21644;&#21830;&#19994;&#26234;&#33021;&#39046;&#22495;&#65292;&#20363;&#22914;&#31354;&#27668;&#27745;&#26579;&#30417;&#27979;&#65292;&#30142;&#30149;&#36319;&#36394;&#21644;&#20113;&#38656;&#27714;&#39044;&#27979;&#12290;&#38543;&#30528;&#29616;&#20195;&#25968;&#25454;&#38598;&#35268;&#27169;&#21644;&#22797;&#26434;&#24615;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#38656;&#35201;&#26032;&#30340;&#32479;&#35745;&#26041;&#27861;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#26102;&#31354;&#21160;&#24577;&#24182;&#22788;&#29702;&#22823;&#35268;&#27169;&#39044;&#27979;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;Bayesian Neural Field (BayesNF)&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25512;&#26029;&#26102;&#31354;&#22495;&#19978;&#20016;&#23500;&#27010;&#29575;&#20998;&#24067;&#30340;&#36890;&#29992;&#39046;&#22495;&#32479;&#35745;&#27169;&#22411;&#65292;&#21487;&#29992;&#20110;&#21253;&#25324;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#21464;&#24322;&#20998;&#26512;&#22312;&#20869;&#30340;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#12290;BayesNF&#23558;&#29992;&#20110;&#39640;&#23481;&#37327;&#20989;&#25968;&#20272;&#35745;&#30340;&#26032;&#22411;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#19982;&#29992;&#20110;&#40065;&#26834;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#20998;&#23618;&#36125;&#21494;&#26031;&#25512;&#26029;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#22312;&#23450;&#20041;&#20808;&#39564;&#20998;&#24067;&#26041;&#38754;&#36827;&#34892;&#24207;&#21015;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07657v1 Announce Type: cross  Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequenc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#20026;&#24503;&#22269;&#36830;&#32493;&#26085;&#20869;&#24066;&#22330;&#20132;&#26131;&#30340;&#30005;&#21147;&#20215;&#26684;&#36827;&#34892;&#36125;&#21494;&#26031;&#39044;&#27979;&#65292;&#32771;&#34385;&#20102;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;2022&#24180;&#30340;&#30005;&#21147;&#20215;&#26684;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#32479;&#35745;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.05441</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#23618;&#27425;&#27010;&#29575;&#30340;&#26085;&#20869;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Bayesian Hierarchical Probabilistic Forecasting of Intraday Electricity Prices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05441
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#25552;&#20986;&#20102;&#20026;&#24503;&#22269;&#36830;&#32493;&#26085;&#20869;&#24066;&#22330;&#20132;&#26131;&#30340;&#30005;&#21147;&#20215;&#26684;&#36827;&#34892;&#36125;&#21494;&#26031;&#39044;&#27979;&#65292;&#32771;&#34385;&#20102;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#22312;2022&#24180;&#30340;&#30005;&#21147;&#20215;&#26684;&#39564;&#35777;&#20013;&#21462;&#24471;&#20102;&#32479;&#35745;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#23545;&#24503;&#22269;&#36830;&#32493;&#26085;&#20869;&#24066;&#22330;&#20132;&#26131;&#30340;&#30005;&#21147;&#20215;&#26684;&#36827;&#34892;&#36125;&#21494;&#26031;&#39044;&#27979;&#30340;&#30740;&#31350;&#65292;&#20805;&#20998;&#32771;&#34385;&#21442;&#25968;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#21464;&#37327;&#26159;IDFull&#20215;&#26684;&#25351;&#25968;&#65292;&#39044;&#27979;&#20197;&#21518;&#39564;&#39044;&#27979;&#20998;&#24067;&#30340;&#24418;&#24335;&#32473;&#20986;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;2022&#24180;&#26497;&#24230;&#27874;&#21160;&#30340;&#30005;&#21147;&#20215;&#26684;&#36827;&#34892;&#39564;&#35777;&#65292;&#22312;&#20043;&#21069;&#20960;&#20046;&#27809;&#26377;&#25104;&#20026;&#39044;&#27979;&#30740;&#31350;&#23545;&#35937;&#12290;&#20316;&#20026;&#22522;&#20934;&#27169;&#22411;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#39044;&#27979;&#21019;&#24314;&#26102;&#30340;&#25152;&#26377;&#21487;&#29992;&#26085;&#20869;&#20132;&#26131;&#26469;&#35745;&#31639;IDFull&#30340;&#24403;&#21069;&#20540;&#12290;&#26681;&#25454;&#24369;&#24335;&#26377;&#25928;&#20551;&#35774;&#65292;&#20174;&#26368;&#21518;&#20215;&#26684;&#20449;&#24687;&#24314;&#31435;&#30340;&#22522;&#20934;&#26080;&#27861;&#26174;&#33879;&#25913;&#21892;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#28857;&#24230;&#37327;&#21644;&#27010;&#29575;&#35780;&#20998;&#26041;&#38754;&#23384;&#22312;&#30528;&#32479;&#35745;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25361;&#25112;&#20102;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#20351;&#29992;LASSO&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#23459;&#24067;&#30340;&#40644;&#37329;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05441v1 Announce Type: cross  Abstract: We present a first study of Bayesian forecasting of electricity prices traded on the German continuous intraday market which fully incorporates parameter uncertainty. Our target variable is the IDFull price index, forecasts are given in terms of posterior predictive distributions. For validation we use the exceedingly volatile electricity prices of 2022, which have hardly been the subject of forecasting studies before. As a benchmark model, we use all available intraday transactions at the time of forecast creation to compute a current value for the IDFull. According to the weak-form efficiency hypothesis, it would not be possible to significantly improve this benchmark built from last price information. We do, however, observe statistically significant improvement in terms of both point measures and probability scores. Finally, we challenge the declared gold standard of using LASSO for feature selection in electricity price forecastin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT-GNN&#30340;&#26032;&#22411;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14708</link><description>&lt;p&gt;
&#36890;&#36807;&#22240;&#26524;&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
CaT-GNN: Enhancing Credit Card Fraud Detection via Causal Temporal Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14708
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CaT-GNN&#30340;&#26032;&#22411;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#65292;&#24182;&#24341;&#20837;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20449;&#29992;&#21345;&#27450;&#35784;&#23545;&#32463;&#27982;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#23613;&#31649;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#24573;&#35270;&#33410;&#28857;&#30340;&#26412;&#22320;&#32467;&#26500;&#23545;&#39044;&#27979;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#26041;&#27861;&#8212;&#8212;CaT-GNN&#65288;Causal Temporal Graph Neural Networks&#65289;&#65292;&#21033;&#29992;&#22240;&#26524;&#19981;&#21464;&#24615;&#23398;&#20064;&#26469;&#25581;&#31034;&#20132;&#26131;&#25968;&#25454;&#20013;&#30340;&#22266;&#26377;&#30456;&#20851;&#24615;&#12290;&#36890;&#36807;&#23558;&#38382;&#39064;&#20998;&#35299;&#20026;&#21457;&#29616;&#21644;&#24178;&#39044;&#38454;&#27573;&#65292;CaT-GNN&#30830;&#23450;&#20132;&#26131;&#22270;&#20013;&#30340;&#22240;&#26524;&#33410;&#28857;&#65292;&#24182;&#24212;&#29992;&#22240;&#26524;&#28151;&#21512;&#31574;&#30053;&#26469;&#22686;&#24378;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;CaT-GNN&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;Causal-Inspector&#21644;Causal-Intervener&#12290;Causal-Inspector&#21033;&#29992;&#26102;&#38388;&#27880;&#24847;&#21147;&#26426;&#21046;&#20013;&#30340;&#27880;&#24847;&#21147;&#26435;&#37325;&#26469;&#35782;&#21035;&#22240;&#26524;&#21644;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14708v1 Announce Type: cross  Abstract: Credit card fraud poses a significant threat to the economy. While Graph Neural Network (GNN)-based fraud detection methods perform well, they often overlook the causal effect of a node's local structure on predictions. This paper introduces a novel method for credit card fraud detection, the \textbf{\underline{Ca}}usal \textbf{\underline{T}}emporal \textbf{\underline{G}}raph \textbf{\underline{N}}eural \textbf{N}etwork (CaT-GNN), which leverages causal invariant learning to reveal inherent correlations within transaction data. By decomposing the problem into discovery and intervention phases, CaT-GNN identifies causal nodes within the transaction graph and applies a causal mixup strategy to enhance the model's robustness and interpretability. CaT-GNN consists of two key components: Causal-Inspector and Causal-Intervener. The Causal-Inspector utilizes attention weights in the temporal attention mechanism to identify causal and environm
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2402.14244</link><description>&lt;p&gt;
MENTOR&#65306;&#22312;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#24341;&#23548;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
MENTOR: Guiding Hierarchical Reinforcement Learning with Human Feedback and Dynamic Distance Constraint
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14244
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#23545;&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#65292;&#35299;&#20915;&#20102;&#25214;&#21040;&#36866;&#24403;&#23376;&#30446;&#26631;&#30340;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#21452;&#31574;&#30053;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23618;&#27425;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;HRL&#65289;&#20026;&#26234;&#33021;&#20307;&#30340;&#22797;&#26434;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20351;&#29992;&#20102;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#30446;&#26631;&#24182;&#20381;&#27425;&#23436;&#25104;&#30340;&#23618;&#27425;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#38590;&#20197;&#25214;&#21040;&#36866;&#24403;&#30340;&#23376;&#30446;&#26631;&#26469;&#30830;&#20445;&#31283;&#23450;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23618;&#27425;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#21644;&#21160;&#24577;&#36317;&#31163;&#32422;&#26463;&#25972;&#21512;&#21040;&#20854;&#20013;&#65288;MENTOR&#65289;&#12290;MENTOR&#20805;&#24403;&#8220;&#23548;&#24072;&#8221;&#65292;&#23558;&#20154;&#31867;&#21453;&#39304;&#32435;&#20837;&#39640;&#23618;&#31574;&#30053;&#23398;&#20064;&#20013;&#65292;&#20197;&#25214;&#21040;&#26356;&#22909;&#30340;&#23376;&#30446;&#26631;&#12290;&#33267;&#20110;&#20302;&#23618;&#31574;&#30053;&#65292;MENTOR&#35774;&#35745;&#20102;&#19968;&#20010;&#21452;&#31574;&#30053;&#20197;&#20998;&#21035;&#36827;&#34892;&#25506;&#32034;-&#24320;&#21457;&#35299;&#32806;&#65292;&#20197;&#31283;&#23450;&#35757;&#32451;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#20154;&#31867;&#21487;&#20197;&#31616;&#21333;&#22320;&#23558;&#20219;&#21153;&#25286;&#20998;&#25104;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14244v1 Announce Type: new  Abstract: Hierarchical reinforcement learning (HRL) provides a promising solution for complex tasks with sparse rewards of intelligent agents, which uses a hierarchical framework that divides tasks into subgoals and completes them sequentially. However, current methods struggle to find suitable subgoals for ensuring a stable learning process. Without additional guidance, it is impractical to rely solely on exploration or heuristics methods to determine subgoals in a large goal space. To address the issue, We propose a general hierarchical reinforcement learning framework incorporating human feedback and dynamic distance constraints (MENTOR). MENTOR acts as a "mentor", incorporating human feedback into high-level policy learning, to find better subgoals. As for low-level policy, MENTOR designs a dual policy for exploration-exploitation decoupling respectively to stabilize the training. Furthermore, although humans can simply break down tasks into s
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2311.10270</link><description>&lt;p&gt;
&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multiscale Hodge Scattering Networks for Data Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10270
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;&#65288;MHSNs&#65289;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#21644;&#21367;&#31215;&#32467;&#26500;&#65292;&#29983;&#25104;&#23545;&#33410;&#28857;&#25490;&#21015;&#19981;&#21464;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#23556;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#27979;&#37327;&#30340;&#20449;&#21495;&#65292;&#31216;&#20026;\emph{&#22810;&#23610;&#24230;&#38669;&#22855;&#25955;&#23556;&#32593;&#32476;}&#65288;MHSNs&#65289;&#12290;&#25105;&#20204;&#30340;&#26500;&#36896;&#22522;&#20110;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#19978;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#35789;&#20856;&#65292;&#21363;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#65292;&#25105;&#20204;&#26368;&#36817;&#20026;&#32473;&#23450;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#20013;&#30340;&#32500;&#24230;$\kappa \in \mathbb{N}$&#25512;&#24191;&#20102;&#22522;&#20110;&#33410;&#28857;&#30340;&#24191;&#20041;&#21704;-&#27779;&#20160;&#21464;&#25442;&#65288;GHWT&#65289;&#21644;&#20998;&#23618;&#22270;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21464;&#25442;&#65288;HGLET&#65289;&#12290;$\kappa$-GHWT&#21644;$\kappa$-HGLET&#37117;&#24418;&#25104;&#20887;&#20313;&#38598;&#21512;&#65288;&#21363;&#35789;&#20856;&#65289;&#30340;&#22810;&#23610;&#24230;&#22522;&#30784;&#21521;&#37327;&#21644;&#32473;&#23450;&#20449;&#21495;&#30340;&#30456;&#24212;&#25193;&#23637;&#31995;&#25968;&#12290;&#25105;&#20204;&#30340;MHSNs&#20351;&#29992;&#31867;&#20284;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#20998;&#23618;&#32467;&#26500;&#26469;&#32423;&#32852;&#35789;&#20856;&#31995;&#25968;&#27169;&#30340;&#30697;&#12290;&#25152;&#24471;&#29305;&#24449;&#23545;&#21333;&#32431;&#22797;&#21512;&#20223;&#23556;&#30340;&#37325;&#26032;&#25490;&#24207;&#19981;&#21464;&#65288;&#21363;&#33410;&#28857;&#25490;&#21015;&#30340;&#32622;&#25442;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10270v2 Announce Type: replace  Abstract: We propose new scattering networks for signals measured on simplicial complexes, which we call \emph{Multiscale Hodge Scattering Networks} (MHSNs). Our construction is based on multiscale basis dictionaries on simplicial complexes, i.e., the $\kappa$-GHWT and $\kappa$-HGLET, which we recently developed for simplices of dimension $\kappa \in \mathbb{N}$ in a given simplicial complex by generalizing the node-based Generalized Haar-Walsh Transform (GHWT) and Hierarchical Graph Laplacian Eigen Transform (HGLET). The $\kappa$-GHWT and the $\kappa$-HGLET both form redundant sets (i.e., dictionaries) of multiscale basis vectors and the corresponding expansion coefficients of a given signal. Our MHSNs use a layered structure analogous to a convolutional neural network (CNN) to cascade the moments of the modulus of the dictionary coefficients. The resulting features are invariant to reordering of the simplices (i.e., node permutation of the u
&lt;/p&gt;</description></item><item><title>cedar&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;&#23427;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#21644;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#65292;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#12290;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24615;&#33021;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;cedar&#25552;&#39640;&#20102;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#28385;&#36275;&#20102;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.08895</link><description>&lt;p&gt;
cedar&#65306;&#21487;&#32452;&#21512;&#21644;&#20248;&#21270;&#30340;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
cedar: Composable and Optimized Machine Learning Input Data Pipelines. (arXiv:2401.08895v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08895
&lt;/p&gt;
&lt;p&gt;
cedar&#26159;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#21487;&#20197;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;&#23427;&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#21644;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#65292;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#12290;&#36890;&#36807;&#35299;&#20915;&#24403;&#21069;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#24615;&#33021;&#20248;&#21270;&#30340;&#38382;&#39064;&#65292;cedar&#25552;&#39640;&#20102;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#65292;&#28385;&#36275;&#20102;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#39640;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#26159;&#27599;&#20010;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#35757;&#32451;&#20219;&#21153;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#36127;&#36131;&#35835;&#21462;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20351;&#29992;&#22797;&#26434;&#30340;&#21464;&#25442;&#22788;&#29702;&#26679;&#26412;&#25209;&#27425;&#65292;&#24182;&#20197;&#20302;&#24310;&#36831;&#21644;&#39640;&#21534;&#21520;&#37327;&#23558;&#20854;&#21152;&#36733;&#21040;&#35757;&#32451;&#33410;&#28857;&#19978;&#12290;&#39640;&#24615;&#33021;&#30340;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#21464;&#24471;&#36234;&#26469;&#36234;&#20851;&#38190;&#65292;&#21407;&#22240;&#26159;&#25968;&#25454;&#37327;&#24613;&#21095;&#22686;&#21152;&#21644;&#35757;&#32451;&#21534;&#21520;&#37327;&#30340;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#36755;&#20837;&#25968;&#25454;&#31995;&#32479;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#20851;&#38190;&#30340;&#24615;&#33021;&#20248;&#21270;&#65292;&#23548;&#33268;&#36164;&#28304;&#21033;&#29992;&#25928;&#29575;&#26497;&#20302;&#30340;&#22522;&#30784;&#35774;&#26045;&#65292;&#25110;&#32773;&#26356;&#31967;&#31957;&#22320;&#65292;&#28010;&#36153;&#26114;&#36149;&#30340;&#21152;&#36895;&#22120;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#20123;&#38656;&#27714;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;cedar&#65292;&#19968;&#20010;&#32534;&#31243;&#27169;&#22411;&#21644;&#26694;&#26550;&#65292;&#20801;&#35768;&#29992;&#25143;&#36731;&#26494;&#26500;&#24314;&#12289;&#20248;&#21270;&#21644;&#25191;&#34892;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;cedar&#25552;&#20379;&#20102;&#26131;&#20110;&#20351;&#29992;&#30340;&#32534;&#31243;&#25509;&#21475;&#65292;&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#21487;&#32452;&#21512;&#36816;&#31639;&#31526;&#26469;&#23450;&#20041;&#25903;&#25345;&#20219;&#24847;ML&#26694;&#26550;&#21644;&#24211;&#30340;&#36755;&#20837;&#25968;&#25454;&#31649;&#36947;&#12290;
&lt;/p&gt;
&lt;p&gt;
The input data pipeline is an essential component of each machine learning (ML) training job. It is responsible for reading massive amounts of training data, processing batches of samples using complex of transformations, and loading them onto training nodes at low latency and high throughput. Performant input data systems are becoming increasingly critical, driven by skyrocketing data volumes and training throughput demands. Unfortunately, current input data systems cannot fully leverage key performance optimizations, resulting in hugely inefficient infrastructures that require significant resources -- or worse -- underutilize expensive accelerators.  To address these demands, we present cedar, a programming model and framework that allows users to easily build, optimize, and execute input data pipelines. cedar presents an easy-to-use programming interface, allowing users to define input data pipelines using composable operators that support arbitrary ML frameworks and libraries. Mean
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#30340;&#35838;&#31243;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;CSG&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#21270;&#35757;&#32451;&#26041;&#27861;&#21644;&#36731;&#37327;&#32423;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#25353;&#38590;&#26131;&#31243;&#24230;&#20248;&#21270;&#26679;&#26412;&#23637;&#31034;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.11083</link><description>&lt;p&gt;
CSG: &#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#30340;&#35838;&#31243;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CSG: Curriculum Representation Learning for Signed Graph. (arXiv:2310.11083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#30340;&#35838;&#31243;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;CSG&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#35838;&#31243;&#21270;&#35757;&#32451;&#26041;&#27861;&#21644;&#36731;&#37327;&#32423;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#25353;&#38590;&#26131;&#31243;&#24230;&#20248;&#21270;&#26679;&#26412;&#23637;&#31034;&#39034;&#24207;&#65292;&#20174;&#32780;&#25552;&#39640;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;SGNN&#65289;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#31526;&#21495;&#22270;&#23545;&#20110;&#24314;&#27169;&#20855;&#26377;&#27491;&#36127;&#36830;&#25509;&#30340;&#22797;&#26434;&#20851;&#31995;&#38750;&#24120;&#26377;&#20215;&#20540;&#65292;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#20854;&#20998;&#26512;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20043;&#21069;&#65292;&#27809;&#26377;&#38024;&#23545;&#26377;&#31526;&#21495;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#23450;&#35757;&#32451;&#26041;&#26696;&#65292;&#24182;&#19988;&#20256;&#32479;&#30340;&#38543;&#26426;&#25277;&#26679;&#26041;&#27861;&#27809;&#26377;&#35299;&#20915;&#22270;&#32467;&#26500;&#20013;&#19981;&#21516;&#23398;&#20064;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35838;&#31243;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20854;&#20013;&#26679;&#26412;&#20174;&#26131;&#21040;&#38590;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#20154;&#31867;&#23398;&#20064;&#12290;&#20026;&#20102;&#34913;&#37327;&#23398;&#20064;&#22256;&#38590;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;&#29992;&#20110;&#26377;&#31526;&#21495;&#22270;&#30340;&#35838;&#31243;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65288;CSG&#65289;&#12290;&#36890;&#36807;&#23545;&#20845;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#35777;&#39564;&#35777;&#65292;&#25105;&#20204;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#22312;&#38142;&#25509;&#31526;&#21495;&#39044;&#27979;&#65288;AUC&#65289;&#26041;&#38754;&#23558;SGNN&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;&#39640;&#36798;23.7&#65285;&#65292;&#24182;&#19988;&#22312;AUC&#30340;&#26631;&#20934;&#24046;&#26041;&#38754;&#26174;&#33879;&#25552;&#39640;&#20102;&#31283;&#23450;&#24615;&#65292;&#26368;&#22810;&#20943;&#23569;&#20102;8.4&#12290;
&lt;/p&gt;
&lt;p&gt;
Signed graphs are valuable for modeling complex relationships with positive and negative connections, and Signed Graph Neural Networks (SGNNs) have become crucial tools for their analysis. However, prior to our work, no specific training plan existed for SGNNs, and the conventional random sampling approach did not address varying learning difficulties within the graph's structure. We proposed a curriculum-based training approach, where samples progress from easy to complex, inspired by human learning. To measure learning difficulty, we introduced a lightweight mechanism and created the Curriculum representation learning framework for Signed Graphs (CSG). This framework optimizes the order in which samples are presented to the SGNN model. Empirical validation across six real-world datasets showed impressive results, enhancing SGNN model accuracy by up to 23.7% in link sign prediction (AUC) and significantly improving stability with an up to 8.4 reduction in the standard deviation of AUC
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31616;&#21270;GNN&#24615;&#33021;&#30340;&#20302;&#31209;&#20869;&#26680;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#21462;&#20195;&#36807;&#20110;&#22797;&#26434;&#30340;GNN&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.05250</link><description>&lt;p&gt;
&#29992;&#20302;&#31209;&#20869;&#26680;&#27169;&#22411;&#31616;&#21270;GNN&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Simplifying GNN Performance with Low Rank Kernel Models. (arXiv:2310.05250v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31616;&#21270;GNN&#24615;&#33021;&#30340;&#20302;&#31209;&#20869;&#26680;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#20256;&#32479;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#26041;&#27861;&#22312;&#35889;&#22495;&#20013;&#21462;&#20195;&#36807;&#20110;&#22797;&#26434;&#30340;GNN&#26550;&#26500;&#65292;&#24182;&#22312;&#22810;&#20010;&#22270;&#31867;&#22411;&#30340;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#26368;&#36817;&#30340;&#35889;GNN&#26041;&#27861;&#23545;&#21322;&#30417;&#30563;&#33410;&#28857;&#20998;&#31867;&#65288;SSNC&#65289;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#35748;&#20026;&#35768;&#22810;&#24403;&#21069;&#30340;GNN&#26550;&#26500;&#21487;&#33021;&#36807;&#20110;&#31934;&#32454;&#35774;&#35745;&#12290;&#30456;&#21453;&#65292;&#31616;&#21333;&#30340;&#38750;&#21442;&#25968;&#20272;&#35745;&#20256;&#32479;&#26041;&#27861;&#65292;&#22312;&#35889;&#22495;&#20013;&#24212;&#29992;&#65292;&#21487;&#20197;&#21462;&#20195;&#35768;&#22810;&#21463;&#28145;&#24230;&#23398;&#20064;&#21551;&#21457;&#30340;GNN&#35774;&#35745;&#12290;&#36825;&#20123;&#20256;&#32479;&#25216;&#26415;&#20284;&#20046;&#38750;&#24120;&#36866;&#21512;&#21508;&#31181;&#22270;&#31867;&#22411;&#65292;&#22312;&#35768;&#22810;&#24120;&#35265;&#30340;SSNC&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#26368;&#36817;&#22312;GNN&#26041;&#27861;&#26041;&#38754;&#30340;&#24615;&#33021;&#25913;&#36827;&#21487;&#33021;&#37096;&#20998;&#24402;&#22240;&#20110;&#35780;&#20272;&#24815;&#20363;&#30340;&#21464;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23545;&#19982;GNN&#35889;&#36807;&#28388;&#25216;&#26415;&#30456;&#20851;&#30340;&#21508;&#31181;&#36229;&#21442;&#25968;&#36827;&#34892;&#20102;&#28040;&#34701;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We revisit recent spectral GNN approaches to semi-supervised node classification (SSNC). We posit that many of the current GNN architectures may be over-engineered. Instead, simpler, traditional methods from nonparametric estimation, applied in the spectral domain, could replace many deep-learning inspired GNN designs. These conventional techniques appear to be well suited for a variety of graph types reaching state-of-the-art performance on many of the common SSNC benchmarks. Additionally, we show that recent performance improvements in GNN approaches may be partially attributed to shifts in evaluation conventions. Lastly, an ablative study is conducted on the various hyperparameters associated with GNN spectral filtering techniques. Code available at: https://github.com/lucianoAvinas/lowrank-gnn-kernels
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#30452;&#25509;&#20998;&#31867;&#20896;&#24515;&#30149;&#24739;&#32773;&#21644;&#27491;&#24120;&#21463;&#35797;&#32773;&#65292;&#30456;&#36739;&#20110;2D&#27169;&#22411;&#25552;&#39640;&#20102;23.65%&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;Grad-GAM&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;2D&#35821;&#20041;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#30340;&#24322;&#24120;&#23450;&#20301;&#12290;</title><link>http://arxiv.org/abs/2308.00009</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#35780;&#20272;&#20896;&#24515;&#30149;&#30340;3D&#28145;&#24230;&#23398;&#20064;&#20998;&#31867;&#22120;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
A 3D deep learning classifier and its explainability when assessing coronary artery disease. (arXiv:2308.00009v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00009
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#30452;&#25509;&#20998;&#31867;&#20896;&#24515;&#30149;&#24739;&#32773;&#21644;&#27491;&#24120;&#21463;&#35797;&#32773;&#65292;&#30456;&#36739;&#20110;2D&#27169;&#22411;&#25552;&#39640;&#20102;23.65%&#30340;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;Grad-GAM&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#19982;2D&#35821;&#20041;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#30340;&#24322;&#24120;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#21457;&#29616;&#21644;&#35786;&#26029;&#20896;&#24515;&#30149;&#65288;CAD&#65289;&#21487;&#25405;&#25937;&#29983;&#21629;&#24182;&#38477;&#20302;&#21307;&#30103;&#25104;&#26412;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;3D Resnet-50&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#30452;&#25509;&#23545;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#20896;&#29366;&#21160;&#33033;&#36896;&#24433;&#22270;&#20687;&#19978;&#30340;&#27491;&#24120;&#21463;&#35797;&#32773;&#21644;&#20896;&#24515;&#30149;&#24739;&#32773;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;2D Resnet-50&#27169;&#22411;&#25552;&#39640;&#20102;23.65%&#12290;&#36890;&#36807;&#20351;&#29992;Grad-GAM&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;3D&#20896;&#24515;&#30149;&#20998;&#31867;&#19982;2D&#20108;&#31867;&#35821;&#20041;&#20998;&#21106;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#21644;&#20934;&#30830;&#30340;&#24322;&#24120;&#23450;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early detection and diagnosis of coronary artery disease (CAD) could save lives and reduce healthcare costs. In this study, we propose a 3D Resnet-50 deep learning model to directly classify normal subjects and CAD patients on computed tomography coronary angiography images. Our proposed method outperforms a 2D Resnet-50 model by 23.65%. Explainability is also provided by using a Grad-GAM. Furthermore, we link the 3D CAD classification to a 2D two-class semantic segmentation for improved explainability and accurate abnormality localisation.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2307.07439</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Atlas-Based Interpretable Age Prediction. (arXiv:2307.07439v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.07439
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35889;&#30340;&#21487;&#35299;&#37322;&#24180;&#40836;&#39044;&#27979;&#26041;&#27861;&#65292;&#21033;&#29992;&#20840;&#36523;&#22270;&#20687;&#30740;&#31350;&#20102;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;&#35299;&#37322;&#24615;&#26041;&#27861;&#21644;&#37197;&#20934;&#25216;&#26415;&#65292;&#30830;&#23450;&#20102;&#26368;&#33021;&#39044;&#27979;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#65292;&#24182;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#26159;&#26368;&#37325;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24180;&#40836;&#39044;&#27979;&#26159;&#21307;&#23398;&#35780;&#20272;&#21644;&#30740;&#31350;&#30340;&#37325;&#35201;&#37096;&#20998;&#65292;&#21487;&#20197;&#36890;&#36807;&#31361;&#20986;&#23454;&#38469;&#24180;&#40836;&#21644;&#29983;&#29289;&#24180;&#40836;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#24110;&#21161;&#26816;&#27979;&#30142;&#30149;&#21644;&#24322;&#24120;&#34928;&#32769;&#12290;&#20026;&#20102;&#20840;&#38754;&#20102;&#35299;&#21508;&#20010;&#36523;&#20307;&#37096;&#20301;&#30340;&#24180;&#40836;&#30456;&#20851;&#21464;&#21270;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20840;&#36523;&#22270;&#20687;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;Grad-CAM&#35299;&#37322;&#24615;&#26041;&#27861;&#30830;&#23450;&#26368;&#33021;&#39044;&#27979;&#19968;&#20010;&#20154;&#24180;&#40836;&#30340;&#36523;&#20307;&#21306;&#22495;&#12290;&#36890;&#36807;&#20351;&#29992;&#37197;&#20934;&#25216;&#26415;&#29983;&#25104;&#25972;&#20010;&#20154;&#32676;&#30340;&#35299;&#37322;&#24615;&#22270;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#20010;&#20307;&#20043;&#22806;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20197;&#19968;&#20010;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;2.76&#24180;&#30340;&#27169;&#22411;&#65292;&#21019;&#19979;&#20102;&#25972;&#20010;&#36523;&#20307;&#24180;&#40836;&#39044;&#27979;&#30340;&#26368;&#26032;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19977;&#20010;&#20027;&#35201;&#30340;&#20851;&#27880;&#39046;&#22495;&#65306;&#33034;&#26609;&#12289;&#26412;&#21407;&#24615;&#32972;&#37096;&#32908;&#32905;&#21644;&#24515;&#33039;&#21306;&#22495;&#65292;&#20854;&#20013;&#24515;&#33039;&#21306;&#22495;&#20855;&#26377;&#26368;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Age prediction is an important part of medical assessments and research. It can aid in detecting diseases as well as abnormal ageing by highlighting the discrepancy between chronological and biological age. To gain a comprehensive understanding of age-related changes observed in various body parts, we investigate them on a larger scale by using whole-body images. We utilise the Grad-CAM interpretability method to determine the body areas most predictive of a person's age. We expand our analysis beyond individual subjects by employing registration techniques to generate population-wide interpretability maps. Furthermore, we set state-of-the-art whole-body age prediction with a model that achieves a mean absolute error of 2.76 years. Our findings reveal three primary areas of interest: the spine, the autochthonous back muscles, and the cardiac region, which exhibits the highest importance.
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.08913</link><description>&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#25351;&#20195;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Referential communication in heterogeneous communities of pre-trained visual deep networks. (arXiv:2302.08913v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08913
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#34987;&#23884;&#20837;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25110;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22312;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#30456;&#20114;&#20043;&#38388;&#36827;&#34892;&#27807;&#36890;&#20197;&#20102;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#12290;&#20316;&#20026;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#19968;&#32452;&#24322;&#26500;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#32593;&#32476;&#31038;&#21306;&#20013;&#36827;&#34892;"&#25351;&#20195;&#24615;&#27807;&#36890;"&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#21457;&#23637;&#19968;&#31181;&#20849;&#20139;&#21327;&#35758;&#26469;&#25351;&#20195;&#19968;&#32452;&#20505;&#36873;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#12290;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65292;&#36825;&#31181;&#20849;&#20139;&#21327;&#35758;&#20063;&#21487;&#20197;&#29992;&#26469;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#26368;&#21021;&#19981;&#23646;&#20110;&#29616;&#26377;&#31038;&#21306;&#30340;&#35270;&#35273;&#32593;&#32476;&#21487;&#20197;&#36731;&#26494;&#22320;&#23398;&#20064;&#21040;&#31038;&#21306;&#30340;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#20135;&#29983;&#30340;&#21327;&#35758;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of \textit{referential communication} in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evi
&lt;/p&gt;</description></item></channel></rss>