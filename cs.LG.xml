<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2404.01012</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#26597;&#35810;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
Query Performance Prediction using Relevance Judgments Generated by Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01012
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#26694;&#26550;&#65292;&#33021;&#22815;&#35299;&#20915;&#20808;&#21069;&#26041;&#27861;&#20013;&#23545;&#19981;&#21516;IR&#35780;&#20272;&#25351;&#26631;&#20934;&#30830;&#24615;&#21644;&#35299;&#37322;&#24615;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26597;&#35810;&#24615;&#33021;&#39044;&#27979;&#65288;QPP&#65289;&#26088;&#22312;&#20272;&#35745;&#25628;&#32034;&#31995;&#32479;&#23545;&#26597;&#35810;&#30340;&#26816;&#32034;&#36136;&#37327;&#65292;&#32780;&#26080;&#38656;&#20154;&#24037;&#30456;&#20851;&#24615;&#21028;&#26029;&#12290;&#20808;&#21069;&#30340;QPP&#26041;&#27861;&#36890;&#24120;&#36820;&#22238;&#21333;&#20010;&#26631;&#37327;&#20540;&#65292;&#24182;&#19981;&#35201;&#27714;&#39044;&#27979;&#20540;&#25509;&#36817;&#29305;&#23450;&#30340;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#35780;&#20272;&#25351;&#26631;&#65292;&#20174;&#32780;&#23548;&#33268;&#20197;&#19979;&#26576;&#20123;&#32570;&#28857;&#65306;&#65288;i&#65289;&#21333;&#20010;&#26631;&#37327;&#26080;&#27861;&#20934;&#30830;&#34920;&#31034;&#19981;&#21516;&#30340;IR&#35780;&#20272;&#25351;&#26631;&#65292;&#29305;&#21035;&#26159;&#24403;&#24230;&#37327;&#19981;&#39640;&#24230;&#30456;&#20851;&#26102;&#65292;&#65288;ii&#65289;&#21333;&#20010;&#26631;&#37327;&#38480;&#21046;&#20102;QPP&#26041;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#20165;&#20351;&#29992;&#26631;&#37327;&#26080;&#27861;&#35299;&#37322;QPP&#32467;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#33258;&#21160;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#30340;QPP&#26694;&#26550;&#65288;QPP-GenRE&#65289;&#65292;&#23558;QPP&#20998;&#35299;&#20026;&#29420;&#31435;&#30340;&#23376;&#20219;&#21153;&#65292;&#21363;&#23545;&#25490;&#21517;&#21015;&#34920;&#20013;&#27599;&#20010;&#39033;&#30446;&#23545;&#32473;&#23450;&#26597;&#35810;&#30340;&#30456;&#20851;&#24615;&#36827;&#34892;&#21028;&#26029;&#12290;&#36825;&#26679;&#25105;&#20204;&#21487;&#20197;&#20351;&#29992;&#29983;&#25104;&#30340;&#30456;&#20851;&#24615;&#21028;&#26029;&#26469;&#39044;&#27979;&#20219;&#20309;IR&#35780;&#20272;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01012v1 Announce Type: cross  Abstract: Query performance prediction (QPP) aims to estimate the retrieval quality of a search system for a query without human relevance judgments. Previous QPP methods typically return a single scalar value and do not require the predicted values to approximate a specific information retrieval (IR) evaluation measure, leading to certain drawbacks: (i) a single scalar is insufficient to accurately represent different IR evaluation measures, especially when metrics do not highly correlate, and (ii) a single scalar limits the interpretability of QPP methods because solely using a scalar is insufficient to explain QPP results. To address these issues, we propose a QPP framework using automatically generated relevance judgments (QPP-GenRE), which decomposes QPP into independent subtasks of judging the relevance of each item in a ranked list to a given query. This allows us to predict any IR evaluation measure using the generated relevance judgment
&lt;/p&gt;</description></item><item><title>TG-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#65292;&#21033;&#29992;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#65292;&#25351;&#23548;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;</title><link>https://arxiv.org/abs/2404.00271</link><description>&lt;p&gt;
TG-NAS&#65306;&#21033;&#29992;Transformer&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#19982;&#38646;&#25104;&#26412;&#20195;&#29702;&#36827;&#34892;&#39640;&#25928;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
TG-NAS: Leveraging Zero-Cost Proxies with Transformer and Graph Convolution Networks for Efficient Neural Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00271
&lt;/p&gt;
&lt;p&gt;
TG-NAS&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#65292;&#21033;&#29992;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#65292;&#25351;&#23548;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;(NAS)&#26159;&#19968;&#31181;&#21457;&#29616;&#26032;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26550;&#26500;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#32791;&#26102;&#30340;&#35757;&#32451;&#25110;&#23494;&#38598;&#30340;&#37319;&#26679;&#21644;&#35780;&#20272;&#12290;&#38646;&#25104;&#26412;NAS&#26088;&#22312;&#20026;&#26550;&#26500;&#24615;&#33021;&#39044;&#27979;&#21019;&#24314;&#20813;&#35757;&#32451;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#20195;&#29702;&#24615;&#33021;&#20122;&#20248;&#65292;&#24182;&#19988;&#24120;&#24120;&#34987;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#25110;&#28014;&#28857;&#36816;&#31639;&#27425;&#25968;&#31561;&#31616;&#21333;&#25351;&#26631;&#25152;&#36229;&#36234;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#22522;&#20110;&#27169;&#22411;&#30340;&#20195;&#29702;&#26080;&#27861;&#23558;&#27867;&#21270;&#21040;&#26032;&#30340;&#25628;&#32034;&#31354;&#38388;&#65292;&#20854;&#20013;&#20855;&#26377;&#26410;&#35265;&#26032;&#31867;&#22411;&#36816;&#31639;&#31526;&#19988;&#19981;&#24102;&#26377;&#40644;&#37329;&#20934;&#30830;&#24230;&#12290;&#19968;&#20010;&#26222;&#36941;&#26368;&#20248;&#30340;&#20195;&#29702;&#20173;&#28982;&#38590;&#20197;&#25214;&#21040;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;TG-NAS&#65292;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Transformer&#30340;&#36816;&#31639;&#31526;&#23884;&#20837;&#29983;&#25104;&#22120;&#21644;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#26469;&#39044;&#27979;&#26550;&#26500;&#24615;&#33021;&#30340;&#26032;&#22411;&#27169;&#22411;&#36890;&#29992;&#20195;&#29702;&#12290;&#36825;&#31181;&#26041;&#27861;&#25351;&#23548;&#30528;&#22312;&#20219;&#20309;&#32473;&#23450;&#25628;&#32034;&#31354;&#38388;&#20869;&#36827;&#34892;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00271v1 Announce Type: cross  Abstract: Neural architecture search (NAS) is an effective method for discovering new convolutional neural network (CNN) architectures. However, existing approaches often require time-consuming training or intensive sampling and evaluations. Zero-shot NAS aims to create training-free proxies for architecture performance prediction. However, existing proxies have suboptimal performance, and are often outperformed by simple metrics such as model parameter counts or the number of floating-point operations. Besides, existing model-based proxies cannot be generalized to new search spaces with unseen new types of operators without golden accuracy truth. A universally optimal proxy remains elusive. We introduce TG-NAS, a novel model-based universal proxy that leverages a transformer-based operator embedding generator and a graph convolution network (GCN) to predict architecture performance. This approach guides neural architecture search across any giv
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;</title><link>https://arxiv.org/abs/2403.16776</link><description>&lt;p&gt;
Diff-Def: &#36890;&#36807;&#25193;&#25955;&#29983;&#25104;&#30340;&#24418;&#21464;&#22330;&#36827;&#34892;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#21046;&#20316;
&lt;/p&gt;
&lt;p&gt;
Diff-Def: Diffusion-Generated Deformation Fields for Conditional Atlases
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16776
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#33324;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#65292;&#30830;&#20445;&#32467;&#26500;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#21078;&#22270;&#35889;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#21475;&#20998;&#26512;&#12290;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#38024;&#23545;&#36890;&#36807;&#29305;&#23450;&#26465;&#20214;&#65288;&#22914;&#20154;&#21475;&#32479;&#35745;&#23398;&#25110;&#30149;&#29702;&#23398;&#65289;&#23450;&#20041;&#30340;&#29305;&#23450;&#23376;&#20154;&#21475;&#65292;&#24182;&#20801;&#35768;&#30740;&#31350;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#24418;&#24577;&#23398;&#24046;&#24322;&#31561;&#32454;&#31890;&#24230;&#35299;&#21078;&#23398;&#24046;&#24322;&#12290;&#29616;&#26377;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#37197;&#20934;&#30340;&#26041;&#27861;&#25110;&#29983;&#25104;&#27169;&#22411;&#65292;&#21069;&#32773;&#26080;&#27861;&#22788;&#29702;&#22823;&#30340;&#35299;&#21078;&#23398;&#21464;&#24322;&#65292;&#21518;&#32773;&#21487;&#33021;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20986;&#29616;&#19981;&#31283;&#23450;&#21644;&#24187;&#35273;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#20351;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#23558;&#19968;&#20010;&#24120;&#35268;&#20154;&#21475;&#22270;&#35889;&#36716;&#21464;&#20026;&#20195;&#34920;&#29305;&#23450;&#23376;&#20154;&#21475;&#30340;&#22270;&#35889;&#12290;&#36890;&#36807;&#29983;&#25104;&#24418;&#21464;&#22330;&#65292;&#24182;&#23558;&#26377;&#26465;&#20214;&#30340;&#22270;&#35889;&#27880;&#20876;&#21040;&#19968;&#32452;&#22270;&#20687;&#38468;&#36817;&#65292;&#25105;&#20204;&#30830;&#20445;&#32467;&#26500;&#30340;&#21512;&#29702;&#24615;&#65292;&#36991;&#20813;&#30452;&#25509;&#22270;&#20687;&#21512;&#25104;&#26102;&#21487;&#33021;&#20986;&#29616;&#30340;&#24187;&#35273;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16776v1 Announce Type: cross  Abstract: Anatomical atlases are widely used for population analysis. Conditional atlases target a particular sub-population defined via certain conditions (e.g. demographics or pathologies) and allow for the investigation of fine-grained anatomical differences - such as morphological changes correlated with age. Existing approaches use either registration-based methods that are unable to handle large anatomical variations or generative models, which can suffer from training instabilities and hallucinations. To overcome these limitations, we use latent diffusion models to generate deformation fields, which transform a general population atlas into one representing a specific sub-population. By generating a deformation field and registering the conditional atlas to a neighbourhood of images, we ensure structural plausibility and avoid hallucinations, which can occur during direct image synthesis. We compare our method to several state-of-the-art 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16591</link><description>&lt;p&gt;
&#25581;&#31034;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16591
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#38544;&#31169;&#23450;&#20041;&#30340;&#22810;&#26679;&#21270;&#65292;&#30001;&#20110;&#23545;&#38544;&#31169;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#25509;&#21463;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#36825;&#31181;&#20256;&#32479;&#30340;&#38544;&#31169;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20174;&#26080;&#27861;&#38450;&#27490;&#25512;&#26029;&#25259;&#38706;&#21040;&#32570;&#20047;&#23545;&#23545;&#25163;&#32972;&#26223;&#30693;&#35782;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#36125;&#21494;&#26031;&#38544;&#31169;&#24182;&#28145;&#20837;&#25506;&#35752;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#25324;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#31361;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22522;&#20110;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#65288;ABP&#65289;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13522</link><description>&lt;p&gt;
REAL&#65306;&#29992;&#20110;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
REAL: Representation Enhanced Analytic Learning for Exemplar-free Class-incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13522
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;REAL&#26041;&#27861;&#65292;&#36890;&#36807;&#26500;&#24314;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;&#21644;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;&#36807;&#31243;&#26469;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#26080;&#33539;&#20363;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#36951;&#24536;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#33539;&#20363;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;(EFCIL)&#26088;&#22312;&#20943;&#36731;&#31867;&#22686;&#37327;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#32780;&#27809;&#26377;&#21487;&#29992;&#30340;&#21382;&#21490;&#25968;&#25454;&#12290;&#19982;&#23384;&#20648;&#21382;&#21490;&#26679;&#26412;&#30340;&#22238;&#25918;&#24335;CIL&#30456;&#27604;&#65292;EFCIL&#22312;&#26080;&#33539;&#20363;&#32422;&#26463;&#19979;&#26356;&#23481;&#26131;&#36951;&#24536;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#21463;&#26368;&#36817;&#21457;&#23637;&#30340;&#22522;&#20110;&#20998;&#26512;&#23398;&#20064;(AL)&#30340;CIL&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;EFCIL&#30340;&#34920;&#31034;&#22686;&#24378;&#20998;&#26512;&#23398;&#20064;(REAL)&#12290;REAL&#26500;&#24314;&#20102;&#19968;&#20010;&#21452;&#27969;&#22522;&#30784;&#39044;&#35757;&#32451;(DS-BPT)&#21644;&#19968;&#20010;&#34920;&#31034;&#22686;&#24378;&#33976;&#39311;(RED)&#36807;&#31243;&#65292;&#20197;&#22686;&#24378;&#25552;&#21462;&#22120;&#30340;&#34920;&#31034;&#12290;DS-BPT&#22312;&#30417;&#30563;&#23398;&#20064;&#21644;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;(SSCL)&#20004;&#20010;&#27969;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29992;&#20110;&#22522;&#30784;&#30693;&#35782;&#25552;&#21462;&#12290;RED&#36807;&#31243;&#23558;&#30417;&#30563;&#30693;&#35782;&#25552;&#28860;&#21040;SSCL&#39044;&#35757;&#32451;&#39592;&#24178;&#37096;&#20998;&#65292;&#20419;&#36827;&#21518;&#32493;&#30340;&#22522;&#20110;AL&#30340;CIL&#65292;&#23558;CIL&#36716;&#25442;&#20026;&#36882;&#24402;&#26368;&#23567;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13522v1 Announce Type: new  Abstract: Exemplar-free class-incremental learning (EFCIL) aims to mitigate catastrophic forgetting in class-incremental learning without available historical data. Compared with its counterpart (replay-based CIL) that stores historical samples, the EFCIL suffers more from forgetting issues under the exemplar-free constraint. In this paper, inspired by the recently developed analytic learning (AL) based CIL, we propose a representation enhanced analytic learning (REAL) for EFCIL. The REAL constructs a dual-stream base pretraining (DS-BPT) and a representation enhancing distillation (RED) process to enhance the representation of the extractor. The DS-BPT pretrains model in streams of both supervised learning and self-supervised contrastive learning (SSCL) for base knowledge extraction. The RED process distills the supervised knowledge to the SSCL pretrained backbone and facilitates a subsequent AL-basd CIL that converts the CIL to a recursive least
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07953</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#20998;&#35299;&#23545;&#31232;&#30095;DNN&#21152;&#36895;&#36827;&#34892;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#24050;&#25104;&#20026;&#28385;&#36275;&#29616;&#20195;DNN&#26085;&#30410;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#19968;&#31181;&#20855;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31232;&#30095;DNN&#21152;&#36895;&#20173;&#28982;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#31232;&#30095;&#21152;&#36895;&#30340;&#24320;&#38144;&#65292;&#30828;&#20214;&#35774;&#35745;&#24072;&#26368;&#36817;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#25903;&#25345;&#65292;&#36825;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#26576;&#20123;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#24494;&#35843;&#30340;&#20219;&#20309;&#31232;&#30095;&#27169;&#22411;&#26080;&#27861;&#34987;&#20854;&#20182;&#32467;&#26500;&#21270;&#30828;&#20214;&#21152;&#36895;&#12290;&#20026;&#20102;&#24357;&#21512;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#20998;&#35299;&#30340;&#24352;&#37327;&#36817;&#20284;&#65288;TASD&#65289;&#65292;&#21033;&#29992;&#20102;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#20998;&#37197;&#24615;&#36136;&#23558;&#20219;&#20309;&#31232;&#30095;&#24352;&#37327;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;TASDER&#65292;&#36890;&#36807;&#25628;&#32034;&#36880;&#23618;&#39640;&#36136;&#37327;&#30340;&#32467;&#26500;&#21270;&#20998;&#35299;&#26469;&#21152;&#36895;DNNs&#30340;&#26435;&#37325;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07953v1 Announce Type: cross  Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#8220;&#36830;&#25509;&#24615;&#8221;&#35270;&#35282;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#26412;&#22320;&#27169;&#22411;&#38388;&#30340;&#36830;&#25509;&#24615;&#20197;&#29983;&#25104;&#26356;&#20855;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.18949</link><description>&lt;p&gt;
&#25552;&#39640;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#30340;&#32676;&#32452;&#36830;&#25509;&#24615;&#20197;&#23454;&#29616;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Group Connectivity for Generalization of Federated Deep Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18949
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#21644;&#25913;&#36827;&#32852;&#37030;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#8220;&#36830;&#25509;&#24615;&#8221;&#35270;&#35282;&#25506;&#35752;&#20102;&#22914;&#20309;&#25913;&#21892;&#26412;&#22320;&#27169;&#22411;&#38388;&#30340;&#36830;&#25509;&#24615;&#20197;&#29983;&#25104;&#26356;&#20855;&#27867;&#21270;&#33021;&#21147;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#28041;&#21450;&#22810;&#20010;&#24322;&#26500;&#23458;&#25143;&#31471;&#36890;&#36807;&#36845;&#20195;&#26412;&#22320;&#26356;&#26032;&#21644;&#27169;&#22411;&#34701;&#21512;&#20849;&#21516;&#35757;&#32451;&#20840;&#23616;&#27169;&#22411;&#12290;&#19982;&#38598;&#20013;&#24335;&#35757;&#32451;&#30456;&#27604;&#65292;FL&#30340;&#20840;&#23616;&#27169;&#22411;&#30340;&#27867;&#21270;&#23384;&#22312;&#24456;&#22823;&#24046;&#36317;&#65292;&#36825;&#26159;&#20854;&#22312;&#26356;&#24191;&#27867;&#24212;&#29992;&#20013;&#30340;&#29942;&#39048;&#12290;&#26412;&#25991;&#36890;&#36807;&#22522;&#26412;&#30340;&#8220;&#36830;&#25509;&#24615;&#8221;&#35270;&#35282;&#30740;&#31350;&#21644;&#25913;&#36827;FL&#30340;&#27867;&#21270;&#65292;&#21363;&#26412;&#22320;&#27169;&#22411;&#22312;&#21442;&#25968;&#21306;&#22495;&#20013;&#22914;&#20309;&#36830;&#25509;&#24182;&#34701;&#21512;&#20026;&#27867;&#21270;&#30340;&#20840;&#23616;&#27169;&#22411;&#12290;&#26415;&#35821;&#8220;&#36830;&#25509;&#24615;&#8221;&#28304;&#33258;&#32447;&#24615;&#27169;&#24335;&#36830;&#25509;&#65288;LMC&#65289;&#65292;&#30740;&#31350;&#31070;&#32463;&#32593;&#32476;&#30340;&#20004;&#31181;&#19981;&#21516;&#35299;&#20915;&#26041;&#26696;&#65288;&#20363;&#22914;&#27169;&#24335;&#65289;&#30340;&#20869;&#25554;&#25439;&#22833;&#26223;&#35266;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#22266;&#23450;&#30340;&#38170;&#23450;&#27169;&#22411;&#26469;&#30740;&#31350;&#36830;&#25509;&#24615;&#30340;&#20256;&#36882;&#24615;&#36136;&#65292;&#20174;&#20004;&#20010;&#27169;&#22411;&#65288;LMC&#65289;&#21040;&#19968;&#32452;&#27169;&#22411;&#65288;FL&#20013;&#30340;&#27169;&#22411;&#34701;&#21512;&#65289;&#12290;&#26681;&#25454;&#25152;&#21457;&#29616;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18949v1 Announce Type: new  Abstract: Federated learning (FL) involves multiple heterogeneous clients collaboratively training a global model via iterative local updates and model fusion. The generalization of FL's global model has a large gap compared with centralized training, which is its bottleneck for broader applications. In this paper, we study and improve FL's generalization through a fundamental ``connectivity'' perspective, which means how the local models are connected in the parameter region and fused into a generalized global model. The term ``connectivity'' is derived from linear mode connectivity (LMC), studying the interpolated loss landscape of two different solutions (e.g., modes) of neural networks. Bridging the gap between LMC and FL, in this paper, we leverage fixed anchor models to empirically and theoretically study the transitivity property of connectivity from two models (LMC) to a group of models (model fusion in FL). Based on the findings, we propo
&lt;/p&gt;</description></item><item><title>QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.17516</link><description>&lt;p&gt;
QUCE: &#20943;&#23569;&#21644;&#37327;&#21270;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#20197;&#29983;&#25104;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
QUCE: The Minimisation and Quantification of Path-Based Uncertainty for Generative Counterfactual Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17516
&lt;/p&gt;
&lt;p&gt;
QUCE&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#26469;&#37327;&#21270;&#21644;&#32531;&#35299;&#22522;&#20110;&#36335;&#24452;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#23545;&#25239;&#24615;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#26368;&#31361;&#20986;&#30340;&#26041;&#27861;&#20043;&#19968;&#12290;DNNs&#30340;&#26377;&#25928;&#24615;&#38543;&#30528;&#26368;&#36817;&#35745;&#31639;&#33021;&#21147;&#30340;&#22686;&#21152;&#32780;&#28608;&#22686;&#65292;&#20351;&#24471;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25193;&#23637;&#21040;&#22788;&#29702;&#22823;&#25968;&#25454;&#20013;&#30340;&#37325;&#35201;&#22797;&#26434;&#24615;&#20197;&#24212;&#23545;&#39044;&#27979;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;DNN&#27169;&#22411;&#22797;&#26434;&#24615;&#30340;&#25552;&#39640;&#65292;&#21487;&#35299;&#37322;&#24615;&#38477;&#20302;&#12290;&#38024;&#23545;&#36825;&#19968;&#25361;&#25112;&#65292;&#35832;&#22914;&#23545;&#25239;&#26799;&#24230;&#25972;&#21512;&#65288;AGI&#65289;&#36825;&#26679;&#30340;&#21487;&#35299;&#37322;&#27169;&#22411;&#21033;&#29992;DNN&#25552;&#20379;&#30340;&#22522;&#20110;&#36335;&#24452;&#30340;&#26799;&#24230;&#26469;&#38416;&#26126;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#24403;&#26799;&#24230;&#22312;&#36234;&#30028;&#36335;&#24452;&#36941;&#21382;&#26399;&#38388;&#34920;&#29616;&#20986;&#19981;&#35268;&#21017;&#24615;&#26102;&#65292;&#22522;&#20110;&#36335;&#24452;&#30340;&#35299;&#37322;&#22120;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#25439;&#23475;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Quantified Uncertainty Counterfactual Explanations&#65288;QUCE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#20943;&#23569;&#36335;&#24452;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#32531;&#35299;&#36234;&#30028;&#36941;&#21382;&#12290; QUCE&#19981;&#20165;&#22312;&#25552;&#20986;&#35299;&#37322;&#26102;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17516v1 Announce Type: cross  Abstract: Deep Neural Networks (DNNs) stand out as one of the most prominent approaches within the Machine Learning (ML) domain. The efficacy of DNNs has surged alongside recent increases in computational capacity, allowing these approaches to scale to significant complexities for addressing predictive challenges in big data. However, as the complexity of DNN models rises, interpretability diminishes. In response to this challenge, explainable models such as Adversarial Gradient Integration (AGI) leverage path-based gradients provided by DNNs to elucidate their decisions. Yet the performance of path-based explainers can be compromised when gradients exhibit irregularities during out-of-distribution path traversal. In this context, we introduce Quantified Uncertainty Counterfactual Explanations (QUCE), a method designed to mitigate out-of-distribution traversal by minimizing path uncertainty. QUCE not only quantifies uncertainty when presenting e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;</title><link>https://arxiv.org/abs/2402.11858</link><description>&lt;p&gt;
&#22312;&#26446;&#32676;&#19978;&#30340;&#38543;&#26426;Hessian&#25311;&#21512;
&lt;/p&gt;
&lt;p&gt;
Stochastic Hessian Fitting on Lie Group
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#65292;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#29305;&#23450;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#26159;&#24378;&#20984;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#38543;&#26426;Hessian-&#21521;&#37327;&#20056;&#31215;&#19978;&#25311;&#21512;Hessian&#25110;&#20854;&#36870;&#12290;&#20351;&#29992;&#20102;&#19968;&#20010;Hessian&#25311;&#21512;&#20934;&#21017;&#65292;&#21487;&#29992;&#20110;&#25512;&#23548;&#22823;&#37096;&#20998;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;BFGS&#12289;&#39640;&#26031;&#29275;&#39039;&#12289;AdaGrad&#31561;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#19981;&#21516;Hessian&#25311;&#21512;&#26041;&#27861;&#30340;&#19981;&#21516;&#25910;&#25947;&#36895;&#29575;&#65292;&#20363;&#22914;&#65292;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#27425;&#32447;&#24615;&#36895;&#29575;&#21644;&#23545;&#31216;&#27491;&#23450;&#65288;SPL&#65289;&#30697;&#38453;&#21644;&#26576;&#20123;&#26446;&#32676;&#19978;&#30340;&#26799;&#24230;&#19979;&#38477;&#30340;&#32447;&#24615;&#36895;&#29575;&#12290;&#22312;&#29305;&#23450;&#19988;&#36275;&#22815;&#19968;&#33324;&#30340;&#26446;&#32676;&#19978;&#30340;Hessian&#25311;&#21512;&#38382;&#39064;&#22312;&#36731;&#24494;&#26465;&#20214;&#19979;&#34987;&#35777;&#26126;&#26159;&#24378;&#20984;&#30340;&#12290;&#20026;&#20102;&#30830;&#35748;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#19981;&#21516;&#35774;&#32622;&#19979;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#22914;&#26377;&#22122;&#22768;&#30340;Hessian-&#21521;&#37327;&#20056;&#31215;&#12289;&#26102;&#21464;&#30340;Hessians&#21644;&#20302;&#31934;&#24230;&#31639;&#26415;&#12290;&#36825;&#20123;&#21457;&#29616;&#23545;&#20381;&#36182;&#20110;&#38543;&#26426;&#20108;&#38454;&#20248;&#21270;&#30340;&#26041;&#27861;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11858v1 Announce Type: cross  Abstract: This paper studies the fitting of Hessian or its inverse with stochastic Hessian-vector products. A Hessian fitting criterion, which can be used to derive most of the commonly used methods, e.g., BFGS, Gaussian-Newton, AdaGrad, etc., is used for the analysis. Our studies reveal different convergence rates for different Hessian fitting methods, e.g., sublinear rates for gradient descent in the Euclidean space and a commonly used closed-form solution, linear rates for gradient descent on the manifold of symmetric positive definite (SPL) matrices and certain Lie groups. The Hessian fitting problem is further shown to be strongly convex under mild conditions on a specific yet general enough Lie group. To confirm our analysis, these methods are tested under different settings like noisy Hessian-vector products, time varying Hessians, and low precision arithmetic. These findings are useful for stochastic second order optimizations that rely 
&lt;/p&gt;</description></item><item><title>URLBERT&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;URL&#20998;&#31867;&#25110;&#26816;&#27979;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#20004;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#23545;URL&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#25552;&#39640;&#20174;URL&#20013;&#25552;&#21462;&#35821;&#20041;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11495</link><description>&lt;p&gt;
URLBERT&#65306;&#19968;&#31181;&#29992;&#20110;URL&#20998;&#31867;&#30340;&#23545;&#27604;&#21644;&#23545;&#25239;&#39044;&#35757;&#32451;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
URLBERT:A Contrastive and Adversarial Pre-trained Model for URL Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11495
&lt;/p&gt;
&lt;p&gt;
URLBERT&#26159;&#31532;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;URL&#20998;&#31867;&#25110;&#26816;&#27979;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#21644;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#20004;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#20197;&#21152;&#24378;&#27169;&#22411;&#23545;URL&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#25552;&#39640;&#20174;URL&#20013;&#25552;&#21462;&#35821;&#20041;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2402.11495v1 &#21457;&#34920;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495;&#25688;&#35201;&#65306;URL&#22312;&#29702;&#35299;&#21644;&#20998;&#31867;&#32593;&#32476;&#20869;&#23481;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#23433;&#20840;&#25511;&#21046;&#21644;&#22312;&#32447;&#25512;&#33616;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#12290;&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#30446;&#21069;&#22312;&#21508;&#20010;&#39046;&#22495;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#20294;URL&#20998;&#26512;&#39046;&#22495;&#20173;&#32570;&#20047;&#19987;&#38376;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;URLBERT&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#24212;&#29992;&#20110;&#21508;&#31181;URL&#20998;&#31867;&#25110;&#26816;&#27979;&#20219;&#21153;&#30340;&#39044;&#35757;&#32451;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#25968;&#21313;&#20159;&#20010;URL&#30340;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#20102;&#19968;&#20010;URL&#26631;&#35760;&#22120;&#65292;&#20197;&#35299;&#20915;URL&#25968;&#25454;&#30340;&#26631;&#35760;&#21270;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65306;&#65288;1&#65289;&#33258;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#20219;&#21153;&#65292;&#36890;&#36807;&#21306;&#20998;&#30456;&#21516;URL&#30340;&#19981;&#21516;&#21464;&#20307;&#26469;&#22686;&#24378;&#27169;&#22411;&#23545;URL&#32467;&#26500;&#30340;&#29702;&#35299;&#21644;&#23545;&#31867;&#21035;&#24046;&#24322;&#30340;&#25429;&#25417;&#65307;&#65288;2&#65289;&#34394;&#25311;&#23545;&#25239;&#35757;&#32451;&#65292;&#26088;&#22312;&#25552;&#39640;&#27169;&#22411;&#20174;URL&#20013;&#25552;&#21462;&#35821;&#20041;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11495v1 Announce Type: cross  Abstract: URLs play a crucial role in understanding and categorizing web content, particularly in tasks related to security control and online recommendations. While pre-trained models are currently dominating various fields, the domain of URL analysis still lacks specialized pre-trained models. To address this gap, this paper introduces URLBERT, the first pre-trained representation learning model applied to a variety of URL classification or detection tasks. We first train a URL tokenizer on a corpus of billions of URLs to address URL data tokenization. Additionally, we propose two novel pre-training tasks: (1) self-supervised contrastive learning tasks, which strengthen the model's understanding of URL structure and the capture of category differences by distinguishing different variants of the same URL; (2) virtual adversarial training, aimed at improving the model's robustness in extracting semantic features from URLs. Finally, our proposed 
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.05668</link><description>&lt;p&gt;
&#23545;LLMs&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Comprehensive Assessment of Jailbreak Attacks Against LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05668
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#32469;&#36807;&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#12290;&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#23545;&#22810;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#36827;&#34892;&#22823;&#35268;&#27169;&#27979;&#37327;&#65292;&#23454;&#39564;&#35777;&#26126;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#33021;&#22815;&#25345;&#32493;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#28389;&#29992;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24050;&#32463;&#37319;&#21462;&#20102;&#23433;&#20840;&#25514;&#26045;&#20197;&#30830;&#20445;LLMs&#31526;&#21512;&#31038;&#20250;&#20262;&#29702;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19968;&#31181;&#32469;&#36807;LLMs&#23433;&#20840;&#25514;&#26045;&#30340;&#19981;&#31283;&#23450;&#28431;&#27934;&#65292;&#34987;&#31216;&#20026;&#36234;&#29425;&#25915;&#20987;&#12290;&#36890;&#36807;&#24212;&#29992;&#25216;&#26415;&#65292;&#22914;&#35282;&#33394;&#25198;&#28436;&#22330;&#26223;&#12289;&#23545;&#25239;&#24615;&#26679;&#26412;&#25110;&#23545;&#23433;&#20840;&#30446;&#26631;&#30340;&#24494;&#22937;&#30772;&#22351;&#20316;&#20026;&#25552;&#31034;&#65292;LLMs&#21487;&#20197;&#20135;&#29983;&#19981;&#36866;&#24403;&#29978;&#33267;&#26377;&#23475;&#30340;&#22238;&#24212;&#12290;&#34429;&#28982;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#20960;&#31181;&#36234;&#29425;&#25915;&#20987;&#30340;&#31867;&#21035;&#65292;&#20294;&#20182;&#20204;&#37117;&#26159;&#23396;&#31435;&#22320;&#36827;&#34892;&#30340;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#21508;&#31181;&#36234;&#29425;&#25915;&#20987;&#26041;&#27861;&#30340;&#39318;&#27425;&#22823;&#35268;&#27169;&#27979;&#37327;&#12290;&#25105;&#20204;&#38598;&#20013;&#22312;&#26469;&#33258;&#22235;&#20010;&#31867;&#21035;&#30340;13&#31181;&#23574;&#31471;&#36234;&#29425;&#26041;&#27861;&#12289;16&#31181;&#36829;&#35268;&#31867;&#21035;&#30340;160&#20010;&#38382;&#39064;&#20197;&#21450;&#20845;&#31181;&#27969;&#34892;&#30340;LLMs&#19978;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20248;&#21270;&#30340;&#36234;&#29425;&#25552;&#31034;&#22987;&#32456;&#33021;&#22815;&#36798;&#21040;&#26368;&#39640;&#30340;&#25915;&#20987;&#25104;&#21151;&#29575;&#65292;&#24182;&#34920;&#29616;&#20986;...
&lt;/p&gt;
&lt;p&gt;
Misuse of the Large Language Models (LLMs) has raised widespread concern. To address this issue, safeguards have been taken to ensure that LLMs align with social ethics. However, recent findings have revealed an unsettling vulnerability bypassing the safeguards of LLMs, known as jailbreak attacks. By applying techniques, such as employing role-playing scenarios, adversarial examples, or subtle subversion of safety objectives as a prompt, LLMs can produce an inappropriate or even harmful response. While researchers have studied several categories of jailbreak attacks, they have done so in isolation. To fill this gap, we present the first large-scale measurement of various jailbreak attack methods. We concentrate on 13 cutting-edge jailbreak methods from four categories, 160 questions from 16 violation categories, and six popular LLMs. Our extensive experimental results demonstrate that the optimized jailbreak prompts consistently achieve the highest attack success rates, as well as exhi
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.03299</link><description>&lt;p&gt;
GUARD: &#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#26469;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36981;&#24490;&#25351;&#21335;&#30340;&#21512;&#35268;&#24615;
&lt;/p&gt;
&lt;p&gt;
GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03299
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#35282;&#33394;&#25198;&#28436;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#36234;&#29425;&#65292;&#29992;&#20110;&#27979;&#35797;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#36981;&#24490;&#24773;&#20917;&#12290;&#31995;&#32479;&#36890;&#36807;&#25910;&#38598;&#29616;&#26377;&#36234;&#29425;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#39640;&#25928;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#32469;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#36807;&#28388;&#21644;&#26377;&#23475;&#22238;&#24212;&#30340;"&#36234;&#29425;"&#24050;&#32463;&#40723;&#21169;&#31038;&#21306;&#37319;&#21462;&#23433;&#20840;&#25514;&#26045;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#30340;&#23433;&#20840;&#25514;&#26045;&#26159;&#22312;&#21457;&#24067;&#20043;&#21069;&#29992;&#36234;&#29425;&#20027;&#21160;&#27979;&#35797;LLM&#12290;&#22240;&#27492;&#65292;&#36825;&#26679;&#30340;&#27979;&#35797;&#23558;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22823;&#35268;&#27169;&#19988;&#39640;&#25928;&#22320;&#29983;&#25104;&#36234;&#29425;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#22312;&#36861;&#38543;&#19968;&#31181;&#26032;&#39062;&#32780;&#30452;&#35266;&#30340;&#31574;&#30053;&#19979;&#65292;&#20197;&#20154;&#31867;&#29983;&#25104;&#30340;&#26041;&#24335;&#26469;&#29983;&#25104;&#36234;&#29425;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35282;&#33394;&#25198;&#28436;&#31995;&#32479;&#65292;&#23558;&#22235;&#31181;&#19981;&#21516;&#35282;&#33394;&#20998;&#37197;&#32473;&#29992;&#25143;LLM&#65292;&#20197;&#20415;&#21327;&#20316;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25910;&#38598;&#29616;&#26377;&#30340;&#36234;&#29425;&#65292;&#24182;&#36890;&#36807;&#21477;&#23376;&#36880;&#21477;&#36827;&#34892;&#32858;&#31867;&#39057;&#29575;&#21644;&#35821;&#20041;&#27169;&#24335;&#30340;&#21010;&#20998;&#65292;&#23558;&#23427;&#20204;&#20998;&#25104;&#19981;&#21516;&#30340;&#29420;&#31435;&#29305;&#24449;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#29305;&#24449;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#65292;&#20351;&#20854;&#26356;&#26131;&#20110;&#35775;&#38382;&#21644;&#26816;&#32034;&#12290;&#25105;&#20204;&#30340;&#35282;&#33394;&#31995;&#32479;&#23558;&#21033;&#29992;&#36825;&#20010;&#30693;&#35782;&#22270;&#26469;&#29983;&#25104;&#26032;&#30340;&#36234;&#29425;&#65292;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The discovery of "jailbreaks" to bypass safety filters of Large Language Models (LLMs) and harmful responses have encouraged the community to implement safety measures. One major safety measure is to proactively test the LLMs with jailbreaks prior to the release. Therefore, such testing will require a method that can generate jailbreaks massively and efficiently. In this paper, we follow a novel yet intuitive strategy to generate jailbreaks in the style of the human generation. We propose a role-playing system that assigns four different roles to the user LLMs to collaborate on new jailbreaks. Furthermore, we collect existing jailbreaks and split them into different independent characteristics using clustering frequency and semantic patterns sentence by sentence. We organize these characteristics into a knowledge graph, making them more accessible and easier to retrieve. Our system of different roles will leverage this knowledge graph to generate new jailbreaks, which have proved effec
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;</title><link>https://arxiv.org/abs/2312.03187</link><description>&lt;p&gt;
FERGI&#65306;&#26469;&#33258;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#29992;&#25143;&#20559;&#22909;&#30340;&#33258;&#21160;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03187
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#20174;&#29992;&#25143;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#20559;&#22909;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#20010;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#65292;&#21487;&#29992;&#20110;&#36890;&#36807;&#36825;&#20123;&#38754;&#37096;&#21160;&#20316;&#21333;&#20803;&#21306;&#20998;&#22270;&#20687;&#23545;&#24182;&#33258;&#21160;&#26631;&#27880;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20351;&#29992;&#20154;&#31867;&#20559;&#22909;&#21453;&#39304;&#25968;&#25454;&#26469;&#24494;&#35843;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#20381;&#36182;&#20110;&#25163;&#21160;&#27880;&#37322;&#65292;&#20154;&#31867;&#21453;&#39304;&#25910;&#38598;&#30340;&#21487;&#25193;&#23637;&#24615;&#21463;&#21040;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#24182;&#27979;&#35797;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#30340;&#33258;&#21457;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#20013;&#33258;&#21160;&#27880;&#37322;&#20854;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#38754;&#37096;&#34920;&#24773;&#21453;&#24212;&#21040;&#29983;&#25104;&#22270;&#20687;&#65288;FERGI&#65289;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23637;&#31034;&#20102;&#22810;&#20010;&#38754;&#37096;&#36816;&#21160;&#21333;&#20803;&#65288;AUs&#65289;&#30340;&#28608;&#27963;&#19982;&#29992;&#25143;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#35780;&#20272;&#39640;&#24230;&#30456;&#20851;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;AU4&#65288;&#30473;&#27611;&#19979;&#22402;&#32773;&#65289;&#21453;&#26144;&#20102;&#23545;&#29983;&#25104;&#22270;&#20687;&#30340;&#36127;&#38754;&#35780;&#20215;&#65292;&#32780;AU12&#65288;&#22068;&#35282;&#25289;&#21160;&#32773;&#65289;&#21453;&#26144;&#20102;&#27491;&#38754;&#35780;&#20215;&#12290;&#36825;&#20004;&#32773;&#22312;&#20004;&#20010;&#26041;&#38754;&#37117;&#24456;&#26377;&#29992;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21487;&#20197;&#20934;&#30830;&#22320;&#20351;&#29992;&#36825;&#20123;AU&#21709;&#24212;&#23384;&#22312;&#23454;&#36136;&#24046;&#24322;&#30340;&#22270;&#20687;&#23545;&#20043;&#38388;&#33258;&#21160;&#27880;&#37322;&#29992;&#25143;&#20559;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03187v2 Announce Type: replace-cross  Abstract: Researchers have proposed to use data of human preference feedback to fine-tune text-to-image generative models. However, the scalability of human feedback collection has been limited by its reliance on manual annotation. Therefore, we develop and test a method to automatically annotate user preferences from their spontaneous facial expression reaction to the generated images. We collect a dataset of Facial Expression Reaction to Generated Images (FERGI) and show that the activations of multiple facial action units (AUs) are highly correlated with user evaluations of the generated images. Specifically, AU4 (brow lowerer) is reflective of negative evaluations of the generated image whereas AU12 (lip corner puller) is reflective of positive evaluations. These can be useful in two ways. Firstly, we can automatically annotate user preferences between image pairs with substantial difference in these AU responses with an accuracy sig
&lt;/p&gt;</description></item><item><title>RefinedFields&#26159;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#21892;&#26080;&#32422;&#26463;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#21644;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#30340;&#20808;&#39564;&#26465;&#20214;&#20013;&#25552;&#21462;&#26356;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.00639</link><description>&lt;p&gt;
RefinedFields: &#23545;&#26080;&#32422;&#26463;&#22330;&#26223;&#30340;&#36752;&#23556;&#22330;&#32454;&#21270;
&lt;/p&gt;
&lt;p&gt;
RefinedFields: Radiance Fields Refinement for Unconstrained Scenes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00639
&lt;/p&gt;
&lt;p&gt;
RefinedFields&#26159;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#25913;&#21892;&#26080;&#32422;&#26463;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#21644;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#30340;&#20808;&#39564;&#26465;&#20214;&#20013;&#25552;&#21462;&#26356;&#20016;&#23500;&#30340;&#32454;&#33410;&#65292;&#24182;&#22312;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#20013;&#20248;&#20110;&#20197;&#24448;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#26080;&#32422;&#26463;&#30340;&#22270;&#20687;&#20013;&#24314;&#27169;&#22823;&#22330;&#26223;&#34987;&#35777;&#26126;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#22788;&#29702;&#37326;&#22806;&#22330;&#26223;&#24314;&#27169;&#26159;&#22312;&#23553;&#38381;&#30340;&#29615;&#22659;&#20013;&#65292;&#27809;&#26377;&#23545;&#20174;&#30495;&#23454;&#19990;&#30028;&#22270;&#20687;&#33719;&#24471;&#30340;&#20808;&#39564;&#26465;&#20214;&#36827;&#34892;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;RefinedFields&#65292;&#36825;&#26159;&#25105;&#20204;&#25152;&#30693;&#30340;&#31532;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#26469;&#25913;&#21892;&#37326;&#22806;&#22330;&#26223;&#24314;&#27169;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#32593;&#32476;&#36890;&#36807;&#20248;&#21270;&#25351;&#23548;&#20351;&#29992;&#20132;&#26367;&#35757;&#32451;&#36807;&#31243;&#26469;&#32454;&#21270;K-Planes&#34920;&#31034;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#25105;&#20204;&#26041;&#27861;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#26053;&#28216;&#29031;&#29255;&#38598;&#19978;&#30340;&#20248;&#28857;&#12290;RefinedFields&#22686;&#24378;&#20102;&#28210;&#26579;&#22330;&#26223;&#30340;&#32454;&#33410;&#65292;&#20248;&#20110;&#20197;&#24448;&#22312;&#37326;&#22806;&#36827;&#34892;&#26032;&#35270;&#35282;&#21512;&#25104;&#20219;&#21153;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#39029;&#38754;&#21487;&#20197;&#22312;https://refinedfields.github.io&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling large scenes from unconstrained images has proven to be a major challenge in computer vision. Existing methods tackling in-the-wild scene modeling operate in closed-world settings, where no conditioning on priors acquired from real-world images is present. We propose RefinedFields, which is, to the best of our knowledge, the first method leveraging pre-trained models to improve in-the-wild scene modeling. We employ pre-trained networks to refine K-Planes representations via optimization guidance using an alternating training procedure. We carry out extensive experiments and verify the merit of our method on synthetic data and real tourism photo collections. RefinedFields enhances rendered scenes with richer details and outperforms previous work on the task of novel view synthesis in the wild. Our project page can be found at https://refinedfields.github.io .
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;</title><link>https://arxiv.org/abs/2305.16877</link><description>&lt;p&gt;
&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributional Reinforcement Learning with Dual Expectile-Quantile Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.16877
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#21452;&#26399;&#26395;&#20998;&#20301;&#22238;&#24402;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#32463;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#36817;&#20284;&#25972;&#20010;&#22238;&#25253;&#20998;&#24067;&#65292;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#29615;&#22659;&#26679;&#26412;&#12290;&#24120;&#29992;&#30340;&#22522;&#20110;&#19981;&#23545;&#31216;$L_1$&#25439;&#22833;&#30340;&#20998;&#24067;&#24335;RL&#30340;&#20998;&#20301;&#22238;&#24402;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#32780;&#26377;&#25928;&#30340;&#23398;&#20064;&#20219;&#24847;&#22238;&#25253;&#20998;&#24067;&#30340;&#26041;&#24335;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#36890;&#36807;&#20351;&#29992;&#26356;&#39640;&#25928;&#30340;&#28151;&#21512;&#19981;&#23545;&#31216;$L_1$-$L_2$ Huber&#25439;&#22833;&#26469;&#25913;&#36827;&#24448;&#24448;&#20250;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#20998;&#24067;&#20272;&#35745;&#20445;&#35777;&#28040;&#22833;&#20102;&#65292;&#25105;&#20204;&#23454;&#35777;&#35266;&#23519;&#21040;&#20272;&#35745;&#30340;&#20998;&#24067;&#20250;&#36805;&#36895;&#25910;&#25947;&#21040;&#20854;&#22343;&#20540;&#12290;&#20107;&#23454;&#19978;&#65292;&#19982;&#26399;&#26395;&#22238;&#24402;&#30456;&#23545;&#24212;&#30340;&#19981;&#23545;&#31216;$L_2$&#25439;&#22833;&#19981;&#33021;&#30452;&#25509;&#29992;&#20110;&#20998;&#24067;&#24335;&#26102;&#24207;&#24046;&#24322;&#23398;&#20064;&#12290;&#21463;&#21040;$L_2$&#20026;&#22522;&#30784;&#23398;&#20064;&#25928;&#29575;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#21512;&#23398;&#20064;&#22238;&#25253;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#21644;&#20998;&#20301;&#25968;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.16877v2 Announce Type: replace-cross  Abstract: Distributional reinforcement learning (RL) has proven useful in multiple benchmarks as it enables approximating the full distribution of returns and makes a better use of environment samples. The commonly used quantile regression approach to distributional RL -- based on asymmetric $L_1$ losses -- provides a flexible and effective way of learning arbitrary return distributions. In practice, it is often improved by using a more efficient, hybrid asymmetric $L_1$-$L_2$ Huber loss for quantile regression. However, by doing so, distributional estimation guarantees vanish, and we empirically observe that the estimated distribution rapidly collapses to its mean. Indeed, asymmetric $L_2$ losses, corresponding to expectile regression, cannot be readily used for distributional temporal difference learning. Motivated by the efficiency of $L_2$-based learning, we propose to jointly learn expectiles and quantiles of the return distribution
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#39640;&#20102;Transformer&#22312;PPG&#20449;&#21495;&#20266;&#36857;&#26816;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#21147;&#65292;&#24182;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#26159;&#26368;&#31283;&#23450;&#19988;&#34920;&#29616;&#26368;&#20248;&#30340;SSL&#25216;&#26415;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23545;&#20110;&#23545;&#27604;SSL&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2401.01013</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;PPG&#20449;&#21495;&#20266;&#36857;&#26816;&#27979;&#20013;&#25552;&#39640;Transformer&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#21147;
&lt;/p&gt;
&lt;p&gt;
Boosting Transformer's Robustness and Efficacy in PPG Signal Artifact Detection with Self-Supervised Learning. (arXiv:2401.01013v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#25552;&#39640;&#20102;Transformer&#22312;PPG&#20449;&#21495;&#20266;&#36857;&#26816;&#27979;&#20013;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#21147;&#65292;&#24182;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#26159;&#26368;&#31283;&#23450;&#19988;&#34920;&#29616;&#26368;&#20248;&#30340;SSL&#25216;&#26415;&#12290;&#36827;&#19968;&#27493;&#20248;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#23545;&#20110;&#23545;&#27604;SSL&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22312;&#22307;&#26480;&#26031;&#22374;&#21307;&#23398;&#38498;&#30340;&#20799;&#31185;&#37325;&#30151;&#30417;&#25252;&#23460;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#65292;&#22914;&#21322;&#30417;&#30563;&#26631;&#31614;&#20256;&#25773;&#21644;K&#26368;&#36817;&#37051;&#65292;&#22312;&#20174;PPG&#20449;&#21495;&#20013;&#26816;&#27979;&#21040;&#20266;&#36857;&#26041;&#38754;&#27604;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#37319;&#29992;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#22823;&#37327;&#26080;&#26631;&#31614;&#25968;&#25454;&#20013;&#25552;&#21462;&#28508;&#22312;&#29305;&#24449;&#65292;&#28982;&#21518;&#23545;&#26377;&#26631;&#31614;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#35299;&#20915;&#23545;&#20016;&#23500;&#26080;&#26631;&#31614;&#25968;&#25454;&#30340;&#20302;&#25928;&#21033;&#29992;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SSL&#33021;&#22815;&#26174;&#33879;&#22686;&#24378;Transformer&#27169;&#22411;&#23398;&#20064;&#34920;&#31034;&#30340;&#33021;&#21147;&#65292;&#25552;&#39640;&#20854;&#22312;&#20266;&#36857;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#21508;&#31181;SSL&#25216;&#26415;&#20013;&#65292;&#21253;&#25324;&#25513;&#30721;&#12289;&#23545;&#27604;&#23398;&#20064;&#21644;&#26080;&#26631;&#31614;&#33258;&#33976;&#39311;&#65288;DINO&#65289;-&#23545;&#27604;&#23398;&#20064;&#22312;&#23567;&#22411;PPG&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#31283;&#23450;&#12289;&#24615;&#33021;&#26368;&#20248;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#20248;&#21270;&#23545;&#27604;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#23545;&#20110;&#23545;&#27604;SSL&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#12290;&#21463;&#21040;...&#30340;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Recent research at CHU Sainte Justine's Pediatric Critical Care Unit (PICU) has revealed that traditional machine learning methods, such as semi-supervised label propagation and K-nearest neighbors, outperform Transformer-based models in artifact detection from PPG signals, mainly when data is limited. This study addresses the underutilization of abundant unlabeled data by employing self-supervised learning (SSL) to extract latent features from these data, followed by fine-tuning on labeled data. Our experiments demonstrate that SSL significantly enhances the Transformer model's ability to learn representations, improving its robustness in artifact classification tasks. Among various SSL techniques, including masking, contrastive learning, and DINO (self-distillation with no labels)-contrastive learning exhibited the most stable and superior performance in small PPG datasets. Further, we delve into optimizing contrastive loss functions, which are crucial for contrastive SSL. Inspired b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#12290;&#24182;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#22235;&#31181;&#23450;&#21046;&#30340;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.01758</link><description>&lt;p&gt;
ReLU&#28608;&#27963;&#20989;&#25968;&#22312;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20248;&#21270;&#20013;&#30340;&#32447;&#24615;&#21270;&#65306;&#26368;&#20339;&#26085;&#21069;&#33021;&#37327;&#35843;&#24230;
&lt;/p&gt;
&lt;p&gt;
Linearization of ReLU Activation Function for Neural Network-Embedded Optimization:Optimal Day-Ahead Energy Scheduling. (arXiv:2310.01758v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#32447;&#24615;&#21270;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#12290;&#24182;&#38024;&#23545;&#31070;&#32463;&#32593;&#32476;&#23884;&#20837;&#20248;&#21270;&#38382;&#39064;&#25552;&#20986;&#24182;&#27604;&#36739;&#20102;&#22235;&#31181;&#23450;&#21046;&#30340;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22312;&#30005;&#21147;&#31995;&#32479;&#39046;&#22495;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#26356;&#22909;&#22320;&#39044;&#27979;&#36755;&#20837;&#20449;&#24687;&#65292;&#24182;&#20197;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#23545;&#31995;&#32479;&#24615;&#33021;&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#65292;&#22914;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#24494;&#30005;&#32593;&#26085;&#21069;&#33021;&#37327;&#35843;&#24230;&#20013;&#65292;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20837;&#29305;&#24449;&#26159;&#22312;&#24378;&#21046;&#38480;&#21046;&#21516;&#19968;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#30340;&#20248;&#21270;&#27169;&#22411;&#20013;&#35299;&#20915;&#30340;&#21464;&#37327;&#12290;&#36825;&#23558;&#20250;&#20135;&#29983;&#19968;&#20010;&#23884;&#20837;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#38382;&#39064;&#65307;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#23558;&#20351;&#36825;&#31867;&#38382;&#39064;&#24322;&#24120;&#22256;&#38590;&#65292;&#29978;&#33267;&#26080;&#27861;&#35299;&#20915;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#26032;&#20852;&#25361;&#25112;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#32447;&#24615;&#21270;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#24191;&#27867;&#20351;&#29992;&#30340;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#20989;&#25968;&#12290;&#26412;&#25991;&#24320;&#21457;&#12289;&#20998;&#26512;&#21644;&#27604;&#36739;&#20102;&#22235;&#31181;&#36866;&#29992;&#20110;ReLU&#28608;&#27963;&#20989;&#25968;&#30340;&#32447;&#24615;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks have been widely applied in the power system area. They can be used for better predicting input information and modeling system performance with increased accuracy. In some applications such as battery degradation neural network-based microgrid day-ahead energy scheduling, the input features of the trained learning model are variables to be solved in optimization models that enforce limits on the output of the same learning model. This will create a neural network-embedded optimization problem; the use of nonlinear activation functions in the neural network will make such problems extremely hard to solve if not unsolvable. To address this emerging challenge, this paper investigated different methods for linearizing the nonlinear activation functions with a particular focus on the widely used rectified linear unit (ReLU) function. Four linearization methods tailored for the ReLU activation function are developed, analyzed and compared in this paper. Each method employs a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#30340;&#36830;&#32493;&#24615;&#65292;&#25552;&#20986;&#20102;&#24403;&#20551;&#35774;&#36830;&#32493;&#26102;&#65292;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2309.17048</link><description>&lt;p&gt;
&#20851;&#20110;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#36830;&#32493;&#24615;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On Continuity of Robust and Accurate Classifiers. (arXiv:2309.17048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17048
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31283;&#20581;&#21644;&#20934;&#30830;&#20998;&#31867;&#22120;&#30340;&#36830;&#32493;&#24615;&#65292;&#25552;&#20986;&#20102;&#24403;&#20551;&#35774;&#36830;&#32493;&#26102;&#65292;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#26159;&#25104;&#21151;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#20110;&#21508;&#31181;&#39046;&#22495;&#30340;&#20851;&#38190;&#12290;&#21019;&#24314;&#19968;&#20010;&#31283;&#20581;&#30340;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#19981;&#21463;&#23545;&#25239;&#25915;&#20987;&#24433;&#21709;&#30340;&#27169;&#22411;&#65292;&#38656;&#35201;&#20840;&#38754;&#29702;&#35299;&#23545;&#25239;&#26679;&#26412;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24456;&#38590;&#25551;&#36848;&#36825;&#19968;&#29616;&#35937;&#12290;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#35777;&#26126;&#20102;&#23545;&#25239;&#24615;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#20551;&#35774;&#30340;&#31283;&#20581;&#24615;&#65292;&#20294;&#36825;&#31181;&#25913;&#36827;&#26159;&#20197;&#33258;&#28982;&#26679;&#26412;&#24615;&#33021;&#19979;&#38477;&#20026;&#20195;&#20215;&#30340;&#12290;&#22240;&#27492;&#65292;&#26377;&#20154;&#25552;&#20986;&#20551;&#35774;&#30340;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#30456;&#20114;&#30683;&#30462;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21478;&#19968;&#31181;&#35266;&#28857;&#65292;&#20551;&#35774;&#30340;&#36830;&#32493;&#24615;&#19982;&#20854;&#31283;&#20581;&#24615;&#21644;&#20934;&#30830;&#24615;&#26159;&#19981;&#20860;&#23481;&#30340;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#36830;&#32493;&#20989;&#25968;&#19981;&#33021;&#26377;&#25928;&#22320;&#23398;&#20064;&#26368;&#20339;&#31283;&#20581;&#20551;&#35774;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#24341;&#20837;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#35856;&#27874;&#21644;ho&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reliability of a learning model is key to the successful deployment of machine learning in various applications. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difficult to describe the phenomenon due to the complicated nature of the problems in machine learning. It has been shown that adversarial training can improve the robustness of the hypothesis. However, this improvement comes at the cost of decreased performance on natural samples. Hence, it has been suggested that robustness and accuracy of a hypothesis are at odds with each other. In this paper, we put forth the alternative proposal that it is the continuity of a hypothesis that is incompatible with its robustness and accuracy. In other words, a continuous function cannot effectively learn the optimal robust hypothesis. To this end, we will introduce a framework for a rigorous study of harmonic and ho
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14587</link><description>&lt;p&gt;
&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#20855;&#26377;&#30072;&#21464;&#29575;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Joint Communication and Computation Framework for Goal-Oriented Semantic Communication with Distortion Rate Resilience. (arXiv:2309.14587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#32852;&#21512;&#36890;&#20449;&#21644;&#35745;&#31639;&#26694;&#26550;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#20013;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20851;&#20110;&#35821;&#20041;&#36890;&#20449;&#30340;&#30740;&#31350;&#20027;&#35201;&#32771;&#34385;&#20934;&#30830;&#24615;&#20316;&#20026;&#20248;&#21270;&#30446;&#26631;&#23548;&#21521;&#36890;&#20449;&#31995;&#32479;&#30340;&#20027;&#35201;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#24726;&#35770;&#65306;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24212;&#35813;&#36890;&#36807;&#35757;&#32451;&#33258;&#28982;&#22320;&#20986;&#29616;&#65292;&#32780;&#19981;&#26159;&#30001;&#32593;&#32476;&#32422;&#26463;&#25152;&#20915;&#23450;&#12290;&#37492;&#20110;&#36825;&#20010;&#22256;&#22659;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29575;&#30072;&#21464;&#29702;&#35770;&#26469;&#20998;&#26512;&#30001;&#36890;&#20449;&#21644;&#35821;&#20041;&#21387;&#32553;&#24341;&#36215;&#30340;&#30072;&#21464;&#65292;&#24182;&#20998;&#26512;&#23398;&#20064;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21407;&#22987;&#25968;&#25454;&#21644;&#30072;&#21464;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#20174;&#32780;&#35780;&#20272;&#20854;&#23545;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20010;&#20998;&#26512;&#65292;&#25105;&#20204;&#21487;&#20197;&#39044;&#20808;&#20272;&#35745;&#20154;&#24037;&#26234;&#33021;&#20219;&#21153;&#30340;&#23454;&#38469;&#20934;&#30830;&#24615;&#65292;&#20351;&#30446;&#26631;&#23548;&#21521;&#35821;&#20041;&#36890;&#20449;&#38382;&#39064;&#21464;&#24471;&#21487;&#34892;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#24182;&#36827;&#34892;&#20102;&#27169;&#25311;&#21644;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research efforts on semantic communication have mostly considered accuracy as a main problem for optimizing goal-oriented communication systems. However, these approaches introduce a paradox: the accuracy of artificial intelligence (AI) tasks should naturally emerge through training rather than being dictated by network constraints. Acknowledging this dilemma, this work introduces an innovative approach that leverages the rate-distortion theory to analyze distortions induced by communication and semantic compression, thereby analyzing the learning process. Specifically, we examine the distribution shift between the original data and the distorted data, thus assessing its impact on the AI model's performance. Founding upon this analysis, we can preemptively estimate the empirical accuracy of AI tasks, making the goal-oriented semantic communication problem feasible. To achieve this objective, we present the theoretical foundation of our approach, accompanied by simulations and ex
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;&#36924;&#36817;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#32463;&#20856;&#30340;k-Means&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#26041;&#26696;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#20855;&#26377;&#22810;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#39640;&#27010;&#29575;&#19979;&#36755;&#20986;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#22810;&#23545;&#25968;&#36816;&#34892;&#26102;&#38388;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2308.08167</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;k-Means&#30340;&#37327;&#23376;&#36924;&#36817;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A Quantum Approximation Scheme for k-Means. (arXiv:2308.08167v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08167
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;&#36924;&#36817;&#26041;&#26696;&#65292;&#29992;&#20110;&#35299;&#20915;&#32463;&#20856;&#30340;k-Means&#32858;&#31867;&#38382;&#39064;&#65292;&#35813;&#26041;&#26696;&#30340;&#36816;&#34892;&#26102;&#38388;&#19982;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#20855;&#26377;&#22810;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#39640;&#27010;&#29575;&#19979;&#36755;&#20986;&#19968;&#20010;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#22810;&#23545;&#25968;&#36816;&#34892;&#26102;&#38388;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;QRAM&#27169;&#22411;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#37327;&#23376;&#36924;&#36817;&#26041;&#26696;&#65288;&#21363;&#23545;&#20110;&#20219;&#24847;&#949; &gt; 0, &#37117;&#26159; (1 + &#949;)-&#36924;&#36817;&#65289;&#65292;&#29992;&#20110;&#32463;&#20856;&#30340;k-Means&#32858;&#31867;&#38382;&#39064;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#20165;&#19982;&#25968;&#25454;&#28857;&#30340;&#25968;&#37327;&#20855;&#26377;&#22810;&#23545;&#25968;&#20381;&#36182;&#20851;&#31995;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#20010;&#22312;QRAM&#25968;&#25454;&#32467;&#26500;&#20013;&#23384;&#20648;&#30340;&#20855;&#26377;N&#20010;&#28857;&#30340;&#25968;&#25454;&#38598;V&#65292;&#36825;&#20010;&#37327;&#23376;&#31639;&#27861;&#30340;&#36816;&#34892;&#26102;&#38388;&#20026;O&#771;(2^(O&#771;(k/&#949;))&#951;^2d)&#65292;&#24182;&#19988;&#20197;&#39640;&#27010;&#29575;&#36755;&#20986;&#19968;&#20010;&#21253;&#21547;k&#20010;&#20013;&#24515;&#30340;&#38598;&#21512;C&#65292;&#28385;&#36275;cost(V, C) &#8804; (1+&#949;) &#183; cost(V, C_OPT)&#12290;&#36825;&#37324;C_OPT&#34920;&#31034;&#26368;&#20248;&#30340;k&#20010;&#20013;&#24515;&#65292;cost(.)&#34920;&#31034;&#26631;&#20934;&#30340;k-Means&#20195;&#20215;&#20989;&#25968;&#65288;&#21363;&#28857;&#21040;&#26368;&#36817;&#20013;&#24515;&#30340;&#24179;&#26041;&#36317;&#31163;&#20043;&#21644;&#65289;&#65292;&#32780;&#951;&#26159;&#32437;&#27178;&#27604;&#65288;&#21363;&#26368;&#36828;&#36317;&#31163;&#19982;&#26368;&#36817;&#36317;&#31163;&#30340;&#27604;&#20540;&#65289;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#20855;&#26377;&#22810;&#23545;&#25968;&#36816;&#34892;&#26102;&#38388;&#30340;&#37327;&#23376;&#31639;&#27861;&#65292;&#24182;&#19988;&#33021;&#22815;&#25552;&#20379;&#19968;&#20010;&#21487;&#35777;&#26126;&#30340;(1+&#949;)&#36924;&#36817;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We give a quantum approximation scheme (i.e., $(1 + \varepsilon)$-approximation for every $\varepsilon &gt; 0$) for the classical $k$-means clustering problem in the QRAM model with a running time that has only polylogarithmic dependence on the number of data points. More specifically, given a dataset $V$ with $N$ points in $\mathbb{R}^d$ stored in QRAM data structure, our quantum algorithm runs in time $\tilde{O} \left( 2^{\tilde{O}(\frac{k}{\varepsilon})} \eta^2 d\right)$ and with high probability outputs a set $C$ of $k$ centers such that $cost(V, C) \leq (1+\varepsilon) \cdot cost(V, C_{OPT})$. Here $C_{OPT}$ denotes the optimal $k$-centers, $cost(.)$ denotes the standard $k$-means cost function (i.e., the sum of the squared distance of points to the closest center), and $\eta$ is the aspect ratio (i.e., the ratio of maximum distance to minimum distance). This is the first quantum algorithm with a polylogarithmic running time that gives a provable approximation guarantee of $(1+\varep
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#24102;&#26377;&#22024;&#26434;&#36125;&#21494;&#26031;&#21453;&#39304;&#30340;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#36951;&#25022;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2306.13233</link><description>&lt;p&gt;
&#22522;&#20110;&#24102;&#26377;&#22024;&#26434;&#36125;&#21494;&#26031;&#21453;&#39304;&#30340;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#30340;&#23545;&#25968;&#36951;&#25022;&#23545;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Logarithmic Regret for Matrix Games against an Adversary with Noisy Bandit Feedback. (arXiv:2306.13233v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#22312;&#24102;&#26377;&#22024;&#26434;&#36125;&#21494;&#26031;&#21453;&#39304;&#30340;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#25968;&#36951;&#25022;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#21644;&#30697;&#38453;&#21338;&#24328;&#30340;&#21464;&#31181;&#65292;&#20854;&#20013;&#27599;&#27493;&#34892;&#36873;&#25163;&#36873;&#25321;&#19968;&#34892;$i$&#65292;&#21015;&#36873;&#25163;&#36873;&#25321;&#19968;&#21015;$j$&#65292;&#34892;&#36873;&#25163;&#25910;&#21040;&#24179;&#22343;&#20540;&#20026;$A_{i,j}$&#30340;&#22024;&#26434;&#22870;&#21169;&#12290;&#34892;&#36873;&#25163;&#30340;&#30446;&#26631;&#26159;&#23613;&#21487;&#33021;&#22320;&#32047;&#31215;&#22870;&#21169;&#65292;&#21363;&#20351;&#23545;&#25163;&#26159;&#19968;&#20010;&#23545;&#25163;&#24615;&#21015;&#36873;&#25163;&#12290;&#35813;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#35777;&#26126;&#22312;$m \times n$&#30697;&#38453;&#21338;&#24328;&#20013;&#65292;&#23454;&#29616;&#20102;$O(\sqrt{mnT})$&#23545;&#25968;&#36951;&#25022;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;UCB&#39118;&#26684;&#31639;&#27861;&#25152;&#33719;&#24471;&#30340;$O(m\sqrt{nT})$&#23545;&#25968;&#36951;&#25022;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers a variant of zero-sum matrix games where at each timestep the row player chooses row $i$, the column player chooses column $j$, and the row player receives a noisy reward with mean $A_{i,j}$. The objective of the row player is to accumulate as much reward as possible, even against an adversarial column player. If the row player uses the EXP3 strategy, an algorithm known for obtaining $\sqrt{T}$ regret against an arbitrary sequence of rewards, it is immediate that the row player also achieves $\sqrt{T}$ regret relative to the Nash equilibrium in this game setting. However, partly motivated by the fact that the EXP3 strategy is myopic to the structure of the game, O'Donoghue et al. (2021) proposed a UCB-style algorithm that leverages the game structure and demonstrated that this algorithm greatly outperforms EXP3 empirically. While they showed that this UCB-style algorithm achieved $\sqrt{T}$ regret, in this paper we ask if there exists an algorithm that provably ach
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#27668;&#20505;&#21464;&#21270;&#24212;&#29992;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#21487;&#34892;&#24615;&#21644;&#24212;&#29992;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#21644;&#30456;&#24212;&#30340;&#20844;&#20849;&#22522;&#20934;&#25110;&#25968;&#25454;&#38598;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2306.04343</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#27668;&#20505;&#21464;&#21270;&#23545;&#25239;&#65306;&#24212;&#29992;&#21644;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimisation Against Climate Change: Applications and Benchmarks. (arXiv:2306.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04343
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#22312;&#27668;&#20505;&#21464;&#21270;&#24212;&#29992;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#21487;&#34892;&#24615;&#21644;&#24212;&#29992;&#26696;&#20363;&#65292;&#25552;&#20986;&#20102;&#22235;&#20010;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#21644;&#30456;&#24212;&#30340;&#20844;&#20849;&#22522;&#20934;&#25110;&#25968;&#25454;&#38598;&#65292;&#24182;&#40723;&#21169;&#24320;&#21457;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31181;&#20248;&#21270;&#40657;&#30418;&#20989;&#25968;&#30340;&#24378;&#26377;&#21147;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#20989;&#25968;&#38590;&#20197;&#35780;&#20272;&#19988;&#27809;&#26377;&#28176;&#36827;&#20449;&#24687;&#30340;&#35774;&#32622;&#20013;&#24191;&#21463;&#27426;&#36814;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#21487;&#20197;&#20248;&#21270;&#35768;&#22810;&#27668;&#20505;&#21464;&#21270;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#30340;&#27169;&#25311;&#22120;&#27169;&#22411;&#19981;&#21487;&#29992;&#25110;&#38590;&#20197;&#20174;&#20013;&#25277;&#21462;&#26679;&#26412;&#12290;&#34429;&#28982;&#22312;&#27668;&#20505;&#30456;&#20851;&#24212;&#29992;&#20013;&#24050;&#32463;&#26377;&#20102;&#20960;&#20010;&#21487;&#34892;&#24615;&#28436;&#31034;&#36125;&#21494;&#26031;&#20248;&#21270;&#30340;&#26696;&#20363;&#65292;&#20294;&#36824;&#27809;&#26377;&#32479;&#19968;&#30340;&#24212;&#29992;&#21644;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#25552;&#20379;&#36825;&#26679;&#30340;&#32508;&#36848;&#65292;&#20197;&#40723;&#21169;&#22312;&#37325;&#35201;&#21644;&#36866;&#23452;&#30340;&#24212;&#29992;&#39046;&#22495;&#20351;&#29992;&#36125;&#21494;&#26031;&#20248;&#21270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#22235;&#20010;&#20027;&#35201;&#24212;&#29992;&#39046;&#22495;&#65306;&#26448;&#26009;&#21457;&#29616;&#12289;&#39118;&#30005;&#22330;&#24067;&#23616;&#12289;&#26368;&#20248;&#21487;&#20877;&#29983;&#33021;&#28304;&#25511;&#21046;&#21644;&#29615;&#22659;&#30417;&#27979;&#12290;&#23545;&#20110;&#27599;&#20010;&#39046;&#22495;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#26131;&#20110;&#20351;&#29992;&#21644;&#35780;&#20272;&#31995;&#32479;&#30340;&#20844;&#20849;&#22522;&#20934;&#25110;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#20195;&#34920;&#30528;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#12290;&#30001;&#20110;&#32570;&#20047;&#36866;&#24403;&#30340;&#22522;&#20934;&#65292;&#25105;&#20204;&#24314;&#35758;&#26410;&#26469;&#24320;&#21457;&#26356;&#20840;&#38754;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian optimisation is a powerful method for optimising black-box functions, popular in settings where the true function is expensive to evaluate and no gradient information is available. Bayesian optimisation can improve responses to many optimisation problems within climate change for which simulator models are unavailable or expensive to sample from. While there have been several feasibility demonstrations of Bayesian optimisation in climate-related applications, there has been no unifying review of applications and benchmarks. We provide such a review here, to encourage the use of Bayesian optimisation in important and well-suited application domains. We identify four main application domains: material discovery, wind farm layout, optimal renewable control and environmental monitoring. For each domain we identify a public benchmark or data set that is easy to use and evaluate systems against, while being representative of real-world problems. Due to the lack of a suitable benchma
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#35745;&#21010;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38271;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06361</link><description>&lt;p&gt;
&#22810;&#33218;&#36172;&#21338;&#26426;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#39640;&#25928;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Training of Multi-task Neural Solver with Multi-armed Bandits. (arXiv:2305.06361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#29992;&#20110;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#30340;&#35757;&#32451;&#65292;&#36890;&#36807;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#65292;&#30456;&#27604;&#20110;&#26631;&#20934;&#35745;&#21010;&#65292;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38271;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22914;&#20309;&#39640;&#25928;&#22320;&#20026;&#21508;&#31181;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064; (COP) &#35757;&#32451;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#33218;&#36172;&#21338;&#26426;&#30340;&#36890;&#29992;&#39640;&#25928;&#35757;&#32451;&#33539;&#24335;&#65292;&#20197;&#25552;&#20379;&#19968;&#20010;&#32479;&#19968;&#30340;&#22810;&#20219;&#21153;&#31070;&#32463;&#27714;&#35299;&#22120;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#19979;&#30340;&#22810;&#20219;&#21153;&#29702;&#35770;&#25439;&#22833;&#20998;&#35299;&#65292;&#36890;&#36807;&#19968;&#20010;&#20219;&#21153;&#24433;&#21709;&#30697;&#38453;&#36890;&#36807;&#27491;&#30830;&#30340;&#36172;&#21338;&#31639;&#27861;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#35757;&#32451;&#12290;&#30456;&#27604;&#26631;&#20934;&#30340;&#35757;&#32451;&#35745;&#21010;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26377;&#38480;&#30340;&#35757;&#32451;&#39044;&#31639;&#25110;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#27573;&#20869;&#23454;&#29616;&#20102;&#26356;&#39640;&#30340;&#25972;&#20307;&#24615;&#33021;&#65292;&#36825;&#21487;&#20197;&#20026;&#20854;&#20182;&#22810;&#20219;&#21153;&#22823;&#27169;&#22411;&#30340;&#39640;&#25928;&#35757;&#32451;&#25552;&#20379;&#25351;&#23548;&#65292;&#27492;&#22806;&#65292;&#24433;&#21709;&#30697;&#38453;&#21487;&#20197;&#25552;&#20379;&#23398;&#20064;&#20248;&#21270;&#39046;&#22495;&#20013;&#24120;&#35265;&#23454;&#36341;&#30340;&#32463;&#39564;&#35777;&#25454;&#65292;&#20174;&#32780;&#25903;&#25345;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently training a multi-task neural solver for various combinatorial optimization problems (COPs) has been less studied so far. In this paper, we propose a general and efficient training paradigm based on multi-armed bandits to deliver a unified multi-task neural solver. To this end, we resort to the theoretical loss decomposition for multiple tasks under an encoder-decoder framework, which enables more efficient training via proper bandit task-sampling algorithms through an intra-task influence matrix. Our method achieves much higher overall performance with either limited training budgets or the same training epochs, compared to standard training schedules, which can be promising for advising efficient training of other multi-task large models. Additionally, the influence matrix can provide empirical evidence of some common practices in the area of learning to optimize, which in turn supports the validity of our approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#21644;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#22823;&#32423;&#21035;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#21457;&#23637;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478;&#12290;</title><link>http://arxiv.org/abs/2305.02251</link><description>&lt;p&gt;
&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65306;&#20174;&#26041;&#31243;&#24335;&#25506;&#32034;&#21040;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated Scientific Discovery: From Equation Discovery to Autonomous Discovery Systems. (arXiv:2305.02251v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20171;&#32461;&#20102;&#21508;&#31181;&#26041;&#27861;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#24182;&#27010;&#36848;&#20102;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#21644;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#22823;&#32423;&#21035;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#35813;&#30740;&#31350;&#26088;&#22312;&#21457;&#23637;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#30740;&#20102;&#33258;&#21160;&#21270;&#31185;&#23398;&#21457;&#29616;&#65292;&#20174;&#26041;&#31243;&#24335;&#25506;&#32034;&#21644;&#31526;&#21495;&#22238;&#24402;&#21040;&#33258;&#20027;&#21457;&#29616;&#31995;&#32479;&#21644;&#20195;&#29702;&#12290;&#20174;&#8220;&#23439;&#35266;&#8221;&#21644;&#19978;&#19979;&#25991;&#35282;&#24230;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20294;&#20063;&#35752;&#35770;&#20102;&#24320;&#25918;&#38382;&#39064;&#21644;&#26368;&#36817;&#30340;&#35805;&#39064;&#65292;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#21508;&#31181;&#35282;&#33394;&#65292;&#24110;&#21161;&#21457;&#29616;&#20154;&#31867;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#20171;&#32461;&#38381;&#29615;&#31185;&#23398;&#21457;&#29616;&#31995;&#32479;&#65292;&#20174;Adam&#31995;&#32479;&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#21040;&#24403;&#21069;&#22312;&#26448;&#26009;&#31185;&#23398;&#21644;&#22825;&#25991;&#23398;&#31561;&#39046;&#22495;&#30340;&#21162;&#21147;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#20174;&#26426;&#22120;&#23398;&#20064;&#30340;&#35282;&#24230;&#35814;&#32454;&#38416;&#36848;&#33258;&#20027;&#24615;&#65292;&#24182;&#20197;&#33258;&#21160;&#39550;&#39542;&#30340;&#33258;&#20027;&#32423;&#21035;&#20026;&#31867;&#27604;&#12290;&#26368;&#22823;&#32423;&#21035;&#65292;&#31532;&#20116;&#32423;&#65292;&#23450;&#20041;&#20026;&#22312;&#29983;&#20135;&#31185;&#23398;&#30693;&#35782;&#26102;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#31867;&#24178;&#39044;&#12290;&#23454;&#29616;&#36825;&#19968;&#28857;&#26159;&#36808;&#21521;&#35299;&#20915;Nobel Turing Grand Challenge&#30340;&#19968;&#27493;&#65306;&#24320;&#21457;&#33021;&#22815;&#20135;&#29983;&#35834;&#36125;&#23572;&#32423;&#31185;&#23398;&#25104;&#26524;&#30340;AI&#31185;&#23398;&#23478; - &#33021;&#21147;&#30340;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper surveys automated scientific discovery, from equation discovery and symbolic regression to autonomous discovery systems and agents. It discusses the individual approaches from a "big picture" perspective and in context, but also discusses open issues and recent topics like the various roles of deep neural networks in this area, aiding in the discovery of human-interpretable knowledge. Further, we will present closed-loop scientific discovery systems, starting with the pioneering work on the Adam system up to current efforts in fields from material science to astronomy. Finally, we will elaborate on autonomy from a machine learning perspective, but also in analogy to the autonomy levels in autonomous driving. The maximal level, level five, is defined to require no human intervention at all in the production of scientific knowledge. Achieving this is one step towards solving the Nobel Turing Grand Challenge to develop AI Scientists: AI systems capable of making Nobel-quality sc
&lt;/p&gt;</description></item><item><title>AtteSTNet&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#30340;&#26816;&#27979;&#28151;&#21512;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#19982;&#22797;&#26434;&#32593;&#32476;&#30456;&#24403;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#65292;&#20854;&#26497;&#22823;&#30340;&#31616;&#21333;&#24615;&#21644;&#26131;&#20110;&#32500;&#25252;&#24615;&#26159;&#20854;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2112.11479</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#30340;&#28151;&#21512;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
AtteSTNet -- An attention and subword tokenization based approach for code-switched text hate speech detection. (arXiv:2112.11479v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.11479
&lt;/p&gt;
&lt;p&gt;
AtteSTNet&#26159;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#30340;&#26816;&#27979;&#28151;&#21512;&#35821;&#35328;&#20167;&#24680;&#35328;&#35770;&#30340;&#26041;&#27861;&#65292;&#23427;&#19981;&#20165;&#19982;&#22797;&#26434;&#32593;&#32476;&#30456;&#24403;&#65292;&#32780;&#19988;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#24615;&#33021;&#26356;&#22909;&#65292;&#20854;&#26497;&#22823;&#30340;&#31616;&#21333;&#24615;&#21644;&#26131;&#20110;&#32500;&#25252;&#24615;&#26159;&#20854;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#30340;&#26368;&#26032;&#36827;&#23637;&#23548;&#33268;&#31038;&#20132;&#23186;&#20307;&#30340;&#20351;&#29992;&#37327;&#22686;&#21152;&#65292;&#20063;&#23548;&#33268;&#22823;&#37327;&#29992;&#25143;&#29983;&#25104;&#30340;&#25968;&#25454;&#65292;&#20854;&#20013;&#21253;&#25324;&#20196;&#20154;&#35752;&#21388;&#21644;&#20882;&#29359;&#30340;&#35328;&#35770;&#12290;&#31038;&#20132;&#23186;&#20307;&#19978;&#20351;&#29992;&#30340;&#35821;&#35328;&#36890;&#24120;&#26159;&#33521;&#35821;&#21644;&#32946;&#22320;&#26041;&#35821;&#35328;&#30340;&#32452;&#21512;&#12290;&#22312;&#21360;&#24230;&#65292;&#21360;&#22320;&#35821;&#26159;&#20027;&#35201;&#20351;&#29992;&#30340;&#35821;&#35328;&#65292;&#24182;&#32463;&#24120;&#19982;&#33521;&#35821;&#20999;&#25442;&#65292;&#24418;&#25104;&#21360;&#22320;&#33521;&#35821;&#65288;Hinglish&#65289;&#35821;&#35328;&#12290;&#36807;&#21435;&#24050;&#32463;&#37319;&#29992;&#20102;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#26469;&#23545;&#28151;&#21512;&#26102;&#30340;&#21360;&#22320;&#33521;&#35821;&#20167;&#24680;&#35328;&#35770;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#20351;&#29992;&#30340;&#24490;&#29615;&#25110;&#21367;&#31215;&#26426;&#21046;&#35745;&#31639;&#25104;&#26412;&#39640;&#65292;&#20869;&#23384;&#38656;&#27714;&#22823;&#12290;&#36807;&#21435;&#30340;&#25216;&#26415;&#36824;&#20351;&#29992;&#22797;&#26434;&#30340;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#65292;&#20351;&#29616;&#26377;&#25216;&#26415;&#38750;&#24120;&#22797;&#26434;&#19988;&#38590;&#20197;&#25913;&#21464;&#25968;&#25454;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#19981;&#20165;&#19982;&#36825;&#20123;&#22797;&#26434;&#32593;&#32476;&#19968;&#26679;&#65292;&#24182;&#19988;&#22312;&#22914;HASOC&#65288;&#21360;&#27431;&#35821;&#35328;&#30340;&#20167;&#24680;&#35328;&#35770;&#21644;&#20882;&#29359;&#20869;&#23481;&#35782;&#21035;&#65289;&#27492;&#31867;&#28151;&#21512;&#21360;&#22320;&#33521;&#35821;&#25991;&#26412;&#30340;&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#24615;&#33021;&#22522;&#20934;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21517;&#20026;AtteSTNet&#65292;&#23427;&#21033;&#29992;&#27880;&#24847;&#21147;&#26426;&#21046;&#21644;&#23376;&#35789;&#20998;&#21106;&#26469;&#35782;&#21035;&#28151;&#21512;&#35821;&#35328;&#20013;&#30340;&#20167;&#24680;&#35328;&#35770;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#20197;&#21069;&#30340;&#25216;&#26415;&#34920;&#29616;&#26356;&#22909;&#65292;&#26356;&#31616;&#21333;&#26131;&#20110;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in technology have led to a boost in social media usage which has ultimately led to large amounts of user-generated data which also includes hateful and offensive speech. The language used in social media is often a combination of English and the native language in the region. In India, Hindi is used predominantly and is often code-switched with English, giving rise to the Hinglish (Hindi+English) language. Various approaches have been made in the past to classify the code-mixed Hinglish hate speech using different machine learning and deep learning-based techniques. However, these techniques make use of recurrence on convolution mechanisms which are computationally expensive and have high memory requirements. Past techniques also make use of complex data processing making the existing techniques very complex and non-sustainable to change in data. Proposed work gives a much simpler approach which is not only at par with these complex networks but also exceeds perfor
&lt;/p&gt;</description></item></channel></rss>