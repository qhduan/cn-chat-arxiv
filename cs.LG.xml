<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.06534</link><description>&lt;p&gt;
SARDet-100K: &#38754;&#21521;&#22823;&#35268;&#27169;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#22522;&#20934;&#21644;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06534
&lt;/p&gt;
&lt;p&gt;
SARDet-100K&#26159;&#31532;&#19968;&#20010;COCO&#32423;&#21035;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035;SAR&#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#65292;&#25581;&#31034;&#20102;SAR&#29289;&#20307;&#26816;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#26174;&#33879;&#24046;&#24322;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#21521;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#29289;&#20307;&#26816;&#27979;&#36817;&#26469;&#22791;&#21463;&#20851;&#27880;&#65292;&#22240;&#20854;&#19981;&#21487;&#26367;&#20195;&#30340;&#20840;&#22825;&#20505;&#25104;&#20687;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#30740;&#31350;&#39046;&#22495;&#38754;&#20020;&#30528;&#26377;&#38480;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#20027;&#35201;&#21253;&#21547; &lt;2K &#24352;&#22270;&#20687;&#65292;&#19988;&#20165;&#21253;&#21547;&#21333;&#31867;&#21035;&#29289;&#20307;&#65289;&#21644;&#28304;&#20195;&#30721;&#19981;&#21487;&#35775;&#38382;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#19968;&#20010;&#38024;&#23545;&#22823;&#35268;&#27169; SAR &#29289;&#20307;&#26816;&#27979;&#30340;&#24320;&#28304;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598; SARDet-100K &#32467;&#26524;&#26159;&#23545; 10 &#20010;&#29616;&#26377; SAR &#26816;&#27979;&#25968;&#25454;&#38598;&#36827;&#34892;&#28145;&#20837;&#35843;&#30740;&#12289;&#25910;&#38598;&#21644;&#26631;&#20934;&#21270;&#30340;&#20135;&#29289;&#65292;&#20026;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#19988;&#22810;&#26679;&#21270;&#30340;&#25968;&#25454;&#38598;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;SARDet-100K &#26159;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#20010;&#36798;&#21040; COCO &#27700;&#24179;&#30340;&#22823;&#35268;&#27169;&#22810;&#31867;&#21035; SAR &#29289;&#20307;&#26816;&#27979;&#25968;&#25454;&#38598;&#12290;&#20973;&#20511;&#36825;&#19968;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#24182;&#25581;&#31034;&#20102; SAR &#29289;&#20307;&#26816;&#27979;&#20013;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06534v1 Announce Type: cross  Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#25903;&#25345;&#36235;&#20110;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#38598;&#20013;&#19981;&#31561;&#24335;&#19978;&#20248;&#20110;i.i.d.&#33945;&#29305;&#21345;&#32599;&#12290;</title><link>https://arxiv.org/abs/2402.11736</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;Gibbs&#27979;&#24230;&#30340;&#33945;&#29305;&#21345;&#32599;&#65306;&#27010;&#29575;&#38543;&#26426;&#25918;&#29287;&#30340;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Monte Carlo with kernel-based Gibbs measures: Guarantees for probabilistic herding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11736
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#25903;&#25345;&#36235;&#20110;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#38598;&#20013;&#19981;&#31561;&#24335;&#19978;&#20248;&#20110;i.i.d.&#33945;&#29305;&#21345;&#32599;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Kernel herding&#23646;&#20110;&#19968;&#31867;&#30830;&#23450;&#24615;&#30340;&#22235;&#20301;&#25968;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#65288;RKHS&#65289;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#12290;&#23613;&#31649;&#26377;&#24456;&#24378;&#30340;&#23454;&#39564;&#25903;&#25345;&#65292;&#20294;&#22312;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#21363;RKHS&#26159;&#26080;&#38480;&#32500;&#26102;&#65292;&#35777;&#26126;&#36825;&#31181;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#20197;&#27604;&#26631;&#20934;&#31215;&#20998;&#33410;&#28857;&#25968;&#37327;&#30340;&#24179;&#26041;&#26681;&#26356;&#24555;&#30340;&#36895;&#29575;&#20943;&#23569;&#26159;&#22256;&#38590;&#30340;&#12290;&#22312;&#36825;&#31687;&#29702;&#35770;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20851;&#20110;&#31215;&#20998;&#33410;&#28857;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#20854;&#25903;&#25345;&#36235;&#20110;&#26368;&#23567;&#21270;&#19982;&#26680;&#25918;&#29287;&#30456;&#21516;&#30340;&#26368;&#22351;&#24773;&#20917;&#35823;&#24046;&#12290;&#25105;&#20204;&#35777;&#26126;&#23427;&#20248;&#20110;i.i.d.&#33945;&#29305;&#21345;&#32599;&#65292;&#24847;&#21619;&#30528;&#22312;&#26368;&#22351;&#24773;&#20917;&#31215;&#20998;&#35823;&#24046;&#19978;&#20855;&#26377;&#26356;&#32039;&#30340;&#38598;&#20013;&#19981;&#31561;&#24335;&#12290;&#23613;&#31649;&#23578;&#26410;&#25552;&#39640;&#36895;&#29575;&#65292;&#20294;&#36825;&#34920;&#26126;&#20102;&#30740;&#31350;Gibbs&#27979;&#24230;&#30340;&#25968;&#23398;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#26680;&#25918;&#29287;&#21450;&#20854;&#21464;&#20307;&#22312;&#35745;&#31639;&#19978;&#30340;&#25913;&#36827;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11736v1 Announce Type: new  Abstract: Kernel herding belongs to a family of deterministic quadratures that seek to minimize the worst-case integration error over a reproducing kernel Hilbert space (RKHS). In spite of strong experimental support, it has revealed difficult to prove that this worst-case error decreases at a faster rate than the standard square root of the number of quadrature nodes, at least in the usual case where the RKHS is infinite-dimensional. In this theoretical paper, we study a joint probability distribution over quadrature nodes, whose support tends to minimize the same worst-case error as kernel herding. We prove that it does outperform i.i.d. Monte Carlo, in the sense of coming with a tighter concentration inequality on the worst-case integration error. While not improving the rate yet, this demonstrates that the mathematical tools of the study of Gibbs measures can help understand to what extent kernel herding and its variants improve on computation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#65292;&#36890;&#36807;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.06783</link><description>&lt;p&gt;
&#23398;&#20064;&#25945;&#23398;&#65306;&#25913;&#21892;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#23454;&#29616;&#20174;&#27169;&#25311;&#21040;&#29616;&#23454;&#30340;&#36801;&#31227;
&lt;/p&gt;
&lt;p&gt;
Learn to Teach: Improve Sample Efficiency in Teacher-student Learning for Sim-to-Real Transfer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06783
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#65292;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#65292;&#36890;&#36807;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#35299;&#20915;&#20102;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#21040;&#29616;&#23454;&#65288;sim-to-real&#65289;&#30340;&#36801;&#31227;&#26159;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#22495;&#38543;&#26426;&#21270;&#26159;&#19968;&#31181;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#38543;&#26426;&#24615;&#30340;&#24378;&#22823;&#25216;&#26415;&#65292;&#21487;&#20197;&#26377;&#25928;&#35299;&#20915;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#35266;&#27979;&#20013;&#30340;&#22122;&#22768;&#20351;&#24471;&#23398;&#20064;&#21464;&#24471;&#26356;&#21152;&#22256;&#38590;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#33539;&#24335;&#21487;&#20197;&#21152;&#36895;&#38543;&#26426;&#21270;&#29615;&#22659;&#20013;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#29305;&#26435;&#20449;&#24687;&#36827;&#34892;&#23398;&#20064;&#65292;&#25945;&#24072;&#26234;&#33021;&#20307;&#21487;&#20197;&#25351;&#23548;&#23398;&#29983;&#26234;&#33021;&#20307;&#22312;&#22122;&#22768;&#29615;&#22659;&#20013;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#19981;&#26159;&#26679;&#26412;&#25928;&#29575;&#30340;&#65292;&#22240;&#20026;&#22312;&#35757;&#32451;&#23398;&#29983;&#26234;&#33021;&#20307;&#26102;&#23436;&#20840;&#33293;&#24323;&#20102;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#65292;&#28010;&#36153;&#20102;&#29615;&#22659;&#25152;&#36879;&#38706;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#21517;&#20026;&#23398;&#20064;&#25945;&#23398;&#65288;L2T&#65289;&#30340;&#26679;&#26412;&#25928;&#29575;&#23398;&#20064;&#26694;&#26550;&#26469;&#25193;&#23637;&#25945;&#24072;-&#23398;&#29983;&#23398;&#20064;&#33539;&#24335;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#22238;&#25910;&#25945;&#24072;&#26234;&#33021;&#20307;&#25910;&#38598;&#30340;&#32463;&#39564;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23545;&#20110;&#19968;&#23545;&#25945;&#24072;-&#23398;&#29983;&#26234;&#33021;&#20307;&#65292;&#29615;&#22659;&#30340;&#21160;&#24577;&#29305;&#24615;&#23545;&#20004;&#32773;&#37117;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation-to-reality (sim-to-real) transfer is a fundamental problem for robot learning. Domain Randomization, which adds randomization during training, is a powerful technique that effectively addresses the sim-to-real gap. However, the noise in observations makes learning significantly harder. Recently, studies have shown that employing a teacher-student learning paradigm can accelerate training in randomized environments. Learned with privileged information, a teacher agent can instruct the student agent to operate in noisy environments. However, this approach is often not sample efficient as the experience collected by the teacher is discarded completely when training the student, wasting information revealed by the environment. In this work, we extend the teacher-student learning paradigm by proposing a sample efficient learning framework termed Learn to Teach (L2T) that recycles experience collected by the teacher agent. We observe that the dynamics of the environments for both 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;</title><link>http://arxiv.org/abs/2312.05134</link><description>&lt;p&gt;
&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Multi-Distribution Learning. (arXiv:2312.05134v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#20248;&#21270;&#22810;&#20998;&#24067;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#37319;&#26679;&#26469;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;&#23398;&#20064;&#12290;&#38024;&#23545;Vapnik-Chervonenkis (VC)&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#24182;&#19988;&#26679;&#26412;&#22797;&#26434;&#24230;&#19982;&#26368;&#20339;&#19979;&#30028;&#20445;&#25345;&#19968;&#33268;&#12290;&#21516;&#26102;&#65292;&#35813;&#31639;&#27861;&#30340;&#24605;&#24819;&#21644;&#29702;&#35770;&#36824;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#26368;&#32456;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20998;&#24067;&#23398;&#20064;&#65288;MDL&#65289;&#26088;&#22312;&#23398;&#20064;&#19968;&#20010;&#20849;&#20139;&#27169;&#22411;&#65292;&#20351;&#24471;&#22312;k&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#20998;&#24067;&#19979;&#65292;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#39118;&#38505;&#65292;&#24050;&#25104;&#20026;&#36866;&#24212;&#20581;&#22766;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#22810;&#32452;&#21512;&#20316;&#31561;&#38656;&#27714;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#23454;&#29616;&#25968;&#25454;&#39640;&#25928;&#30340;MDL&#38656;&#35201;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#37319;&#26679;&#65292;&#20063;&#31216;&#20026;&#25353;&#38656;&#37319;&#26679;&#12290;&#28982;&#32780;&#65292;&#26368;&#20248;&#26679;&#26412;&#22797;&#26434;&#24230;&#30340;&#19978;&#19979;&#30028;&#20043;&#38388;&#23384;&#22312;&#36739;&#22823;&#24046;&#36317;&#12290;&#38024;&#23545;Vapnik-Chervonenkis&#65288;VC&#65289;&#32500;&#25968;&#20026;d&#30340;&#20551;&#35774;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#21487;&#29983;&#25104;&#19968;&#20010;&#949;-&#26368;&#20248;&#38543;&#26426;&#20551;&#35774;&#65292;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#25509;&#36817;&#20110;&#65288;d+k&#65289;/&#949;^2&#65288;&#22312;&#26576;&#20123;&#23545;&#25968;&#22240;&#23376;&#20013;&#65289;&#65292;&#19982;&#24050;&#30693;&#30340;&#26368;&#20339;&#19979;&#30028;&#21305;&#37197;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#24605;&#24819;&#21644;&#29702;&#35770;&#34987;&#36827;&#19968;&#27493;&#25193;&#23637;&#65292;&#20197;&#36866;&#24212;Rademacher&#31867;&#12290;&#25552;&#20986;&#30340;&#31639;&#27861;&#26159;&#22885;&#25289;&#20811;&#23572;&#39640;&#25928;&#30340;&#65292;&#20165;&#20165;&#35775;&#38382;&#20551;&#35774;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-distribution learning (MDL), which seeks to learn a shared model that minimizes the worst-case risk across $k$ distinct data distributions, has emerged as a unified framework in response to the evolving demand for robustness, fairness, multi-group collaboration, etc. Achieving data-efficient MDL necessitates adaptive sampling, also called on-demand sampling, throughout the learning process. However, there exist substantial gaps between the state-of-the-art upper and lower bounds on the optimal sample complexity. Focusing on a hypothesis class of Vapnik-Chervonenkis (VC) dimension $d$, we propose a novel algorithm that yields an $varepsilon$-optimal randomized hypothesis with a sample complexity on the order of $(d+k)/\varepsilon^2$ (modulo some logarithmic factor), matching the best-known lower bound. Our algorithmic ideas and theory have been further extended to accommodate Rademacher classes. The proposed algorithms are oracle-efficient, which access the hypothesis class solely
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#22235;&#31181;&#36755;&#20837;&#27169;&#24577;&#30340;&#22788;&#29702;&#21644;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#21644;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2311.00721</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#22312;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#25110;&#29983;&#29702;&#20449;&#21495;&#19978;&#36827;&#34892;&#20849;&#24773;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Empathy Detection Using Machine Learning on Text, Audiovisual, Audio or Physiological Signals. (arXiv:2311.00721v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.00721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#20998;&#26512;&#65292;&#21253;&#25324;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#22235;&#31181;&#36755;&#20837;&#27169;&#24577;&#30340;&#22788;&#29702;&#21644;&#32593;&#32476;&#35774;&#35745;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#21644;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#26159;&#19968;&#20010;&#31038;&#20132;&#25216;&#33021;&#65292;&#34920;&#26126;&#19968;&#20010;&#20010;&#20307;&#29702;&#35299;&#20182;&#20154;&#30340;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20849;&#24773;&#24341;&#36215;&#20102;&#21253;&#25324;&#24773;&#24863;&#35745;&#31639;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#24515;&#29702;&#23398;&#22312;&#20869;&#30340;&#21508;&#20010;&#23398;&#31185;&#30340;&#20851;&#27880;&#12290;&#20849;&#24773;&#26159;&#19968;&#20010;&#20381;&#36182;&#20110;&#19978;&#19979;&#25991;&#30340;&#26415;&#35821;&#65292;&#22240;&#27492;&#26816;&#27979;&#25110;&#35782;&#21035;&#20849;&#24773;&#22312;&#31038;&#20250;&#12289;&#21307;&#30103;&#21644;&#25945;&#32946;&#31561;&#39046;&#22495;&#20855;&#26377;&#28508;&#22312;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#20849;&#24773;&#26816;&#27979;&#39046;&#22495;&#28041;&#21450;&#33539;&#22260;&#24191;&#27867;&#19988;&#26377;&#37325;&#21472;&#65292;&#20294;&#20174;&#25972;&#20307;&#25991;&#29486;&#35282;&#24230;&#26469;&#30475;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#20849;&#24773;&#26816;&#27979;&#30740;&#31350;&#20173;&#28982;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#31995;&#32479;&#25910;&#38598;&#21644;&#31579;&#36873;&#20102;&#26469;&#33258;10&#20010;&#30693;&#21517;&#25968;&#25454;&#24211;&#30340;801&#31687;&#35770;&#25991;&#65292;&#24182;&#20998;&#26512;&#20102;&#36873;&#23450;&#30340;54&#31687;&#35770;&#25991;&#12290;&#25105;&#20204;&#26681;&#25454;&#20849;&#24773;&#26816;&#27979;&#31995;&#32479;&#30340;&#36755;&#20837;&#27169;&#24577;&#65292;&#21363;&#25991;&#26412;&#12289;&#35270;&#21548;&#12289;&#38899;&#39057;&#21644;&#29983;&#29702;&#20449;&#21495;&#65292;&#23545;&#35770;&#25991;&#36827;&#34892;&#20998;&#32452;&#12290;&#25105;&#20204;&#20998;&#21035;&#30740;&#31350;&#20102;&#29305;&#23450;&#27169;&#24577;&#30340;&#39044;&#22788;&#29702;&#21644;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#21327;&#35758;&#12289;&#24120;&#35265;&#25968;&#25454;&#38598;&#30340;&#25551;&#36848;&#21644;&#21487;&#29992;&#24615;&#35814;&#24773;&#65292;&#20197;&#21450;&#35780;&#20272;&#21327;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empathy is a social skill that indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science and Psychology. Empathy is a context-dependent term; thus, detecting or recognising empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection studies leveraging Machine Learning remains underexplored from a holistic literature perspective. To this end, we systematically collect and screen 801 papers from 10 well-known databases and analyse the selected 54 papers. We group the papers based on input modalities of empathy detection systems, i.e., text, audiovisual, audio and physiological signals. We examine modality-specific pre-processing and network architecture design protocols, popular dataset descriptions and availability details, and evaluation protocols. We fur
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2310.01737</link><description>&lt;p&gt;
&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#20197;&#23454;&#29616;&#40065;&#26834;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Blending Imitation and Reinforcement Learning for Robust Policy Improvement. (arXiv:2310.01737v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01737
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#21644;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20854;&#26679;&#26412;&#22797;&#26434;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#38556;&#30861;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#27169;&#20223;&#23398;&#20064;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20248;&#21270;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#36890;&#24120;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#19987;&#23478;&#31034;&#33539;&#30340;&#36136;&#37327;&#38480;&#21046;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#34701;&#21512;&#27169;&#20223;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#22312;&#32447;&#35780;&#20272;&#32467;&#26524;&#20132;&#26367;&#20351;&#29992;&#20108;&#32773;&#65292;&#26377;&#25928;&#22320;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#12290;&#36825;&#31181;&#31639;&#27861;&#33021;&#22815;&#20174;&#22810;&#31181;&#40657;&#30418;&#19987;&#23478;&#31034;&#33539;&#20013;&#23398;&#20064;&#21644;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has shown promising performance, its sample complexity continues to be a substantial hurdle, restricting its broader application across a variety of domains. Imitation learning (IL) utilizes oracles to improve sample efficiency, yet it is often constrained by the quality of the oracles deployed. which actively interleaves between IL and RL based on an online estimate of their performance. RPI draws on the strengths of IL, using oracle queries to facilitate exploration, an aspect that is notably challenging in sparse-reward RL, particularly during the early stages of learning. As learning unfolds, RPI gradually transitions to RL, effectively treating the learned policy as an improved oracle. This algorithm is capable of learning from and improving upon a diverse set of black-box oracles. Integral to RPI are Robust Active Policy Selection (RAPS) and Robust Policy Gradient (RPG), both of which reason over whether to perform state-wise imitation from the o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;Thompson&#37319;&#26679;&#19982;&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31574;&#30053;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#19968;&#33324;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.00539</link><description>&lt;p&gt;
&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#19979;&#30340;Thompson&#25506;&#32034;&#22312;&#26368;&#20339;&#33218;&#35782;&#21035;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Thompson Exploration with Best Challenger Rule in Best Arm Identification. (arXiv:2310.00539v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00539
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;Thompson&#37319;&#26679;&#19982;&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#35299;&#20915;&#26368;&#20339;&#33218;&#35782;&#21035;&#38382;&#39064;&#12290;&#35813;&#31574;&#30053;&#22312;&#28176;&#36817;&#24773;&#20917;&#19979;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#22312;&#19968;&#33324;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#20013;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32463;&#20856;&#21333;&#21442;&#25968;&#25351;&#25968;&#27169;&#22411;&#19979;&#65292;&#22266;&#23450;&#32622;&#20449;&#24230;&#19979;&#30340;&#26368;&#20339;&#33218;&#35782;&#21035;&#65288;BAI&#65289;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#30446;&#21069;&#24050;&#26377;&#24456;&#22810;&#31574;&#30053;&#34987;&#25552;&#20986;&#65292;&#20294;&#22823;&#22810;&#25968;&#38656;&#35201;&#22312;&#27599;&#19968;&#36718;&#35299;&#20915;&#19968;&#20010;&#26368;&#20248;&#21270;&#38382;&#39064;&#21644;/&#25110;&#32773;&#38656;&#35201;&#25506;&#32034;&#19968;&#20010;&#33218;&#33267;&#23569;&#19968;&#23450;&#27425;&#25968;&#65292;&#38500;&#38750;&#26159;&#38024;&#23545;&#39640;&#26031;&#27169;&#22411;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31574;&#30053;&#65292;&#23558;Thompson&#37319;&#26679;&#19982;&#19968;&#20010;&#35745;&#31639;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#8212;&#8212;&#26368;&#20339;&#20505;&#36873;&#35268;&#21017;&#30456;&#32467;&#21512;&#12290;&#34429;&#28982;Thompson&#37319;&#26679;&#26368;&#21021;&#34987;&#32771;&#34385;&#29992;&#20110;&#26368;&#22823;&#21270;&#32047;&#31215;&#22870;&#21169;&#65292;&#20294;&#25105;&#20204;&#35777;&#26126;&#23427;&#20063;&#21487;&#20197;&#33258;&#28982;&#22320;&#29992;&#20110;&#22312;BAI&#20013;&#25506;&#32034;&#33218;&#32780;&#19981;&#24378;&#36843;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31574;&#30053;&#22312;&#20219;&#24847;&#20004;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#26159;&#28176;&#36817;&#26368;&#20248;&#30340;&#65292;&#24182;&#19988;&#22312;&#19968;&#33324;&#30340;$K$&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#19978;&#65288;$K\geq 3$&#65289;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#31574;&#30053;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the fixed-confidence best arm identification (BAI) problem in the bandit framework in the canonical single-parameter exponential models. For this problem, many policies have been proposed, but most of them require solving an optimization problem at every round and/or are forced to explore an arm at least a certain number of times except those restricted to the Gaussian model. To address these limitations, we propose a novel policy that combines Thompson sampling with a computationally efficient approach known as the best challenger rule. While Thompson sampling was originally considered for maximizing the cumulative reward, we demonstrate that it can be used to naturally explore arms in BAI without forcing it. We show that our policy is asymptotically optimal for any two-armed bandit problems and achieves near optimality for general $K$-armed bandit problems for $K\geq 3$. Nevertheless, in numerical experiments, our policy shows competitive performance compared to as
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAPS&#21644;MAPS-SE&#20004;&#20010;&#31639;&#27861;&#65292;&#21487;&#22312;&#22810;&#40657;&#30418;&#39044;&#35328;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#24182;&#20027;&#21160;&#36873;&#25321;&#21644;&#25913;&#36827;&#26368;&#20248;&#39044;&#35328;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10259</link><description>&lt;p&gt;
&#22810;&#40657;&#30418;&#39044;&#35328;&#19979;&#30340;&#20027;&#21160;&#31574;&#30053;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Active Policy Improvement from Multiple Black-box Oracles. (arXiv:2306.10259v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAPS&#21644;MAPS-SE&#20004;&#20010;&#31639;&#27861;&#65292;&#21487;&#22312;&#22810;&#40657;&#30418;&#39044;&#35328;&#24773;&#20917;&#19979;&#65292;&#37319;&#29992;&#27169;&#20223;&#23398;&#20064;&#24182;&#20027;&#21160;&#36873;&#25321;&#21644;&#25913;&#36827;&#26368;&#20248;&#39044;&#35328;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#22797;&#26434;&#39046;&#22495;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#26159;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30830;&#23450;&#26377;&#25928;&#31574;&#30053;&#24448;&#24448;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#25506;&#32034;&#65292;&#32780;&#27169;&#20223;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19987;&#23478;&#28436;&#31034;&#26469;&#25351;&#23548;&#25506;&#32034;&#65292;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24773;&#22659;&#19979;&#65292;&#20154;&#20204;&#36890;&#24120;&#21482;&#33021;&#25509;&#35302;&#21040;&#22810;&#20010;&#27425;&#20248;&#30340;&#40657;&#30418;&#39044;&#35328;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#26368;&#20248;&#30340;&#39044;&#35328;&#65292;&#36825;&#20123;&#39044;&#35328;&#19981;&#33021;&#22312;&#25152;&#26377;&#29366;&#24577;&#19979;&#26222;&#36941;&#20248;&#20110;&#24444;&#27492;&#65292;&#36825;&#32473;&#20027;&#21160;&#20915;&#23450;&#22312;&#21738;&#31181;&#29366;&#24577;&#19979;&#20351;&#29992;&#21738;&#31181;&#39044;&#35328;&#20197;&#21450;&#22914;&#20309;&#25913;&#36827;&#21508;&#33258;&#20272;&#35745;&#20540;&#20989;&#25968;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21363;MAPS&#21644;MAPS-SE&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has made significant strides in various complex domains. However, identifying an effective policy via RL often necessitates extensive exploration. Imitation learning aims to mitigate this issue by using expert demonstrations to guide exploration. In real-world scenarios, one often has access to multiple suboptimal black-box experts, rather than a single optimal oracle. These experts do not universally outperform each other across all states, presenting a challenge in actively deciding which oracle to use and in which state. We introduce MAPS and MAPS-SE, a class of policy improvement algorithms that perform imitation learning from multiple suboptimal oracles. In particular, MAPS actively selects which of the oracles to imitate and improve their value function estimates, and MAPS-SE additionally leverages an active state exploration criterion to determine which states one should explore. We provide a comprehensive theoretical analysis and demonstrate that MAP
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2208.10300</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#21442;&#25968;&#20248;&#21270;&#20013;&#30340;&#26377;&#25928;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#19982;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Efficient Utility Function Learning for Multi-Objective Parameter Optimization with Prior Knowledge. (arXiv:2208.10300v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10300
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#30495;&#23454;&#19990;&#30028;&#38382;&#39064;&#20013;&#29992;&#19987;&#23478;&#30693;&#35782;&#23450;&#20041;&#25928;&#29992;&#20989;&#25968;&#22256;&#38590;&#19988;&#19982;&#19987;&#23478;&#21453;&#22797;&#20114;&#21160;&#26114;&#36149;&#30340;&#38382;&#39064;&#12290;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#65292;&#33021;&#22815;&#22312;&#20351;&#29992;&#24456;&#23569;&#32467;&#26524;&#26102;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#24182;&#36890;&#36807;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#20256;&#36882;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#25216;&#26415;&#36890;&#24120;&#20551;&#23450;&#24050;&#26377;&#25928;&#29992;&#20989;&#25968;&#12289;&#36890;&#36807;&#20114;&#21160;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#25110;&#23581;&#35797;&#30830;&#23450;&#23436;&#25972;&#30340;Pareto&#21069;&#27839;&#26469;&#36827;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#20013;&#65292;&#32467;&#26524;&#24448;&#24448;&#22522;&#20110;&#38544;&#21547;&#21644;&#26174;&#24615;&#30340;&#19987;&#23478;&#30693;&#35782;&#65292;&#38590;&#20197;&#23450;&#20041;&#19968;&#20010;&#25928;&#29992;&#20989;&#25968;&#65292;&#32780;&#20114;&#21160;&#23398;&#20064;&#25110;&#21518;&#32493;&#21551;&#21457;&#24335;&#38656;&#35201;&#21453;&#22797;&#24182;&#19988;&#26114;&#36149;&#22320;&#19987;&#23478;&#21442;&#19982;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#20351;&#29992;&#20559;&#22909;&#23398;&#20064;&#31163;&#32447;&#23398;&#20064;&#25928;&#29992;&#20989;&#25968;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#12290;&#19982;&#20854;&#20182;&#24037;&#20316;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#19981;&#20165;&#20351;&#29992;&#65288;&#25104;&#23545;&#30340;&#65289;&#32467;&#26524;&#20559;&#22909;&#65292;&#32780;&#19988;&#20351;&#29992;&#25928;&#29992;&#20989;&#25968;&#31354;&#38388;&#30340;&#31895;&#30053;&#20449;&#24687;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#39640;&#25928;&#29992;&#20989;&#25968;&#20272;&#35745;&#65292;&#29305;&#21035;&#26159;&#22312;&#20351;&#29992;&#24456;&#23569;&#30340;&#32467;&#26524;&#26102;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#25928;&#29992;&#20989;&#25968;&#23398;&#20064;&#20219;&#21153;&#20013;&#20986;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#20854;&#20256;&#36882;&#21040;&#25972;&#20010;&#20248;&#21270;&#38142;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art in multi-objective optimization assumes either a given utility function, learns a utility function interactively or tries to determine the complete Pareto front, requiring a post elicitation of the preferred result. However, result elicitation in real world problems is often based on implicit and explicit expert knowledge, making it difficult to define a utility function, whereas interactive learning or post elicitation requires repeated and expensive expert involvement. To mitigate this, we learn a utility function offline, using expert knowledge by means of preference learning. In contrast to other works, we do not only use (pairwise) result preferences, but also coarse information about the utility function space. This enables us to improve the utility function estimate, especially when using very few results. Additionally, we model the occurring uncertainties in the utility function learning task and propagate them through the whole optimization chain. 
&lt;/p&gt;</description></item></channel></rss>