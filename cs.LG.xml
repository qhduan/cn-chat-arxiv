<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#36890;&#29992;&#34920;&#31034;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.02047</link><description>&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#30340;&#36890;&#29992;&#34920;&#31034;&#65306;&#34701;&#21512;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;
&lt;/p&gt;
&lt;p&gt;
Universal representations for financial transactional data: embracing local, global, and external contexts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02047
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#36890;&#29992;&#34920;&#31034;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#26412;&#22320;&#12289;&#20840;&#23616;&#21644;&#22806;&#37096;&#35821;&#22659;&#65292;&#25552;&#20986;&#20102;&#26032;&#39062;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#36229;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37329;&#34701;&#20132;&#26131;&#30340;&#26377;&#25928;&#22788;&#29702;&#23545;&#38134;&#34892;&#25968;&#25454;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#19987;&#27880;&#20110;&#20026;&#29420;&#31435;&#38382;&#39064;&#25552;&#20379;&#19987;&#38376;&#21270;&#35299;&#20915;&#26041;&#26696;&#65292;&#32780;&#19981;&#26159;&#26500;&#24314;&#36866;&#29992;&#20110;&#35768;&#22810;&#38382;&#39064;&#30340;&#36890;&#29992;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#21508;&#31181;&#20225;&#19994;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#32771;&#34385;&#25968;&#25454;&#29305;&#23450;&#24615;&#30340;&#26032;&#39062;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22806;&#37096;&#20449;&#24687;&#21040;&#23458;&#25143;&#34920;&#31034;&#30340;&#26041;&#24335;&#65292;&#20511;&#37492;&#20854;&#20182;&#23458;&#25143;&#34892;&#21160;&#30340;&#35265;&#35299;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#65292;&#25551;&#36848;&#20102;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#34920;&#31034;&#36136;&#37327;&#65292;&#28041;&#21450;&#25972;&#20010;&#20132;&#26131;&#21382;&#21490;&#65307;&#26412;&#22320;&#33539;&#22260;&#20869;&#65292;&#21453;&#26144;&#23458;&#25143;&#24403;&#21069;&#29366;&#24577;&#65307;&#21160;&#24577;&#33539;&#22260;&#20869;&#65292;&#25429;&#25417;&#34920;&#31034;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;&#26412;&#22320;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#23545;&#20110;&#19979;&#19968;&#20010;MCC&#39044;&#27979;&#20219;&#21153;&#30340;ROC-AUC&#25552;&#21319;&#39640;&#36798;14&#65285;&#65292;&#23545;&#20110;dow...
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02047v1 Announce Type: cross  Abstract: Effective processing of financial transactions is essential for banking data analysis. However, in this domain, most methods focus on specialized solutions to stand-alone problems instead of constructing universal representations suitable for many problems. We present a representation learning framework that addresses diverse business challenges. We also suggest novel generative models that account for data specifics, and a way to integrate external information into a client's representation, leveraging insights from other customers' actions. Finally, we offer a benchmark, describing representation quality globally, concerning the entire transaction history; locally, reflecting the client's current state; and dynamically, capturing representation evolution over time. Our generative approach demonstrates superior performance in local tasks, with an increase in ROC-AUC of up to 14\% for the next MCC prediction task and up to 46\% for dow
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;</title><link>https://arxiv.org/abs/2403.06833</link><description>&lt;p&gt;
LLMs&#33021;&#22815;&#23558;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#21527;&#65311;&#25105;&#20204;&#20855;&#20307;&#25351;&#30340;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Separate Instructions From Data? And What Do We Even Mean By That?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06833
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24418;&#24335;&#21270;&#30340;&#24230;&#37327;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#31181;&#21487;&#20197;&#20174;&#27169;&#22411;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#26032;&#25968;&#25454;&#38598;SEP&#65292;&#29992;&#20110;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35843;&#33410;&#25351;&#20196;&#30340;&#25216;&#26415;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#25104;&#26524;&#65292;&#20026;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25171;&#24320;&#20102;&#26080;&#25968;&#26032;&#21487;&#33021;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#20854;&#20182;&#35745;&#31639;&#26426;&#31185;&#23398;&#39046;&#22495;&#24050;&#24314;&#31435;&#20026;&#35268;&#33539;&#30340;&#22522;&#26412;&#23433;&#20840;&#29305;&#24615;&#65292;&#27604;&#22914;&#25351;&#20196;&#19982;&#25968;&#25454;&#20043;&#38388;&#30340;&#20998;&#31163;&#65292;&#23548;&#33268;&#23427;&#20204;&#21457;&#29983;&#25925;&#38556;&#25110;&#26131;&#21463;&#31532;&#19977;&#26041;&#25805;&#25511;&#21644;&#24178;&#25200;&#65288;&#20363;&#22914;&#36890;&#36807;&#38388;&#25509;&#25552;&#31034;/&#21629;&#20196;&#27880;&#20837;&#65289;&#12290;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#36804;&#20170;&#20026;&#27490;&#65292;&#29978;&#33267;&#27809;&#26377;&#30830;&#20999;&#23450;&#20041;&#36825;&#31181;&#20998;&#31163;&#31350;&#31455;&#24847;&#21619;&#30528;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#27979;&#35797;&#20854;&#36829;&#21453;&#24773;&#20917;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#27491;&#24335;&#30340;&#25351;&#26631;&#26469;&#37327;&#21270;&#25351;&#20196;&#19982;&#25968;&#25454;&#20998;&#31163;&#29616;&#35937;&#65292;&#20197;&#21450;&#19968;&#20010;&#21487;&#20197;&#20174;&#27169;&#22411;&#30340;&#40657;&#30418;&#36755;&#20986;&#35745;&#31639;&#30340;&#32463;&#39564;&#21464;&#37327;&#12290;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;SEP&#65288;&#24212;&#35813;&#25191;&#34892;&#36824;&#26159;&#22788;&#29702;&#65311;&#65289;&#65292;&#35813;&#25968;&#25454;&#38598;&#20801;&#35768;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06833v1 Announce Type: cross  Abstract: Instruction-tuned Large Language Models (LLMs) have achieved breakthrough results, opening countless new possibilities for many practical applications. However, LLMs lack elementary safety features that are established norms in other areas of computer science, such as the separation between instructions and data, causing them to malfunction or rendering them vulnerable to manipulation and interference by third parties e.g., via indirect prompt/command injection. Even worse, so far, there is not even an established definition of what precisely such a separation would mean and how its violation could be tested. In this work, we aim to close this gap. We introduce a formal measure to quantify the phenomenon of instruction-data separation as well as an empirical variant of the measure that can be computed from a model`s black-box outputs. We also introduce a new dataset, SEP (Should it be Executed or Processed?), which allows estimating th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.04960</link><description>&lt;p&gt;
SecGPT&#65306;&#19968;&#31181;&#38754;&#21521;&#22522;&#20110;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
SecGPT: An Execution Isolation Architecture for LLM-Based Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04960
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#25191;&#34892;&#38548;&#31163;&#26550;&#26500;SecGPT&#65292;&#26088;&#22312;&#35299;&#20915;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#25152;&#24341;&#21457;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34987;&#25193;&#23637;&#20026;&#31995;&#32479;&#65292;&#22914;ChatGPT&#65292;&#24050;&#32463;&#24320;&#22987;&#25903;&#25345;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#21033;&#29992;LLMs&#30340;&#20107;&#23454;&#19978;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#33258;&#21160;&#25191;&#34892;&#33539;&#24335;&#65306;&#21363;&#65292;&#24212;&#29992;&#31243;&#24207;&#21450;&#20854;&#20132;&#20114;&#26159;&#29992;&#33258;&#28982;&#35821;&#35328;&#23450;&#20041;&#30340;&#65292;&#25552;&#20379;&#23545;&#29992;&#25143;&#25968;&#25454;&#30340;&#35775;&#38382;&#65292;&#24182;&#34987;&#20801;&#35768;&#33258;&#30001;&#22320;&#30456;&#20114;&#20132;&#20114;&#20197;&#21450;&#19982;&#31995;&#32479;&#20114;&#21160;&#12290;&#36825;&#20123;LLM&#24212;&#29992;&#31243;&#24207;&#29983;&#24577;&#31995;&#32479;&#31867;&#20284;&#20110;&#26089;&#26399;&#35745;&#31639;&#24179;&#21488;&#30340;&#35774;&#32622;&#65292;&#22312;&#37027;&#37324;&#24212;&#29992;&#31243;&#24207;&#21644;&#31995;&#32479;&#20043;&#38388;&#32570;&#20047;&#36275;&#22815;&#30340;&#38548;&#31163;&#12290;&#30001;&#20110;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#21487;&#33021;&#19981;&#21487;&#20449;&#65292;&#24182;&#19988;&#21463;&#33258;&#28982;&#35821;&#35328;&#30028;&#38754;&#30340;&#19981;&#31934;&#30830;&#24615;&#21152;&#21095;&#65292;&#24403;&#21069;&#30340;&#35774;&#35745;&#20250;&#20026;&#29992;&#25143;&#24102;&#26469;&#23433;&#20840;&#21644;&#38544;&#31169;&#39118;&#38505;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SecGPT&#65292;&#19968;&#31181;&#38754;&#21521;LLM&#31995;&#32479;&#30340;&#26550;&#26500;&#65292;&#26088;&#22312;&#32531;&#35299;&#30001;&#31532;&#19977;&#26041;&#24212;&#29992;&#31243;&#24207;&#25191;&#34892;&#24341;&#36215;&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;SecGPT&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#38548;&#31163;&#24212;&#29992;&#31243;&#24207;&#30340;&#25191;&#34892;&#21644;&#26356;&#22810;&#30340;&#39044;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04960v1 Announce Type: cross  Abstract: Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applications. These LLM apps leverage the de facto natural language-based automated execution paradigm of LLMs: that is, apps and their interactions are defined in natural language, provided access to user data, and allowed to freely interact with each other and the system. These LLM app ecosystems resemble the settings of earlier computing platforms, where there was insufficient isolation between apps and the system. Because third-party apps may not be trustworthy, and exacerbated by the imprecision of the natural language interfaces, the current designs pose security and privacy risks for users. In this paper, we propose SecGPT, an architecture for LLM-based systems that aims to mitigate the security and privacy issues that arise with the execution of third-party apps. SecGPT's key idea is to isolate the execution of apps and more pre
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#22312;&#20855;&#26377;&#39640;&#26031;&#25968;&#25454;&#30340;&#19968;&#33324;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23558;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#26144;&#23556;&#21040;&#31561;&#25928;&#30340;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19982;&#20005;&#26684;&#30028;&#38480;&#21644;&#25968;&#20540;&#23454;&#39564;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.10164</link><description>&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#21644;&#22810;&#39033;&#24335;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Random features and polynomial rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20998;&#26512;&#20102;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#22312;&#20855;&#26377;&#39640;&#26031;&#25968;&#25454;&#30340;&#19968;&#33324;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#23558;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#26144;&#23556;&#21040;&#31561;&#25928;&#30340;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#24471;&#21040;&#20102;&#19982;&#20005;&#26684;&#30028;&#38480;&#21644;&#25968;&#20540;&#23454;&#39564;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#22312;&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#36215;&#30528;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25551;&#36848;&#20102;&#31070;&#32463;&#32593;&#32476;&#25509;&#36817;&#26080;&#38480;&#23485;&#24230;&#26497;&#38480;&#26102;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#20855;&#26377;&#39640;&#26031;&#25968;&#25454;&#30340;&#19968;&#33324;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#21033;&#29992;&#26080;&#24207;&#31995;&#32479;&#30340;&#32479;&#35745;&#21147;&#23398;&#24037;&#20855;&#23558;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#26144;&#23556;&#21040;&#31561;&#25928;&#30340;&#22810;&#39033;&#24335;&#27169;&#22411;&#65292;&#24182;&#32472;&#21046;&#20102;&#24179;&#22343;&#27867;&#21270;&#26354;&#32447;&#20316;&#20026;&#38382;&#39064;&#30340;&#20004;&#20010;&#20027;&#35201;&#25511;&#21046;&#21442;&#25968;&#30340;&#20989;&#25968;&#65306;&#38543;&#26426;&#29305;&#24449;&#30340;&#25968;&#37327;N&#21644;&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;P&#65292;&#20551;&#35774;&#23427;&#20204;&#37117;&#25353;&#29031;&#36755;&#20837;&#32500;&#24230;D&#30340;&#24130;&#36827;&#34892;&#32553;&#25918;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25193;&#23637;&#20102;N&#65292;P&#21644;D&#20043;&#38388;&#27604;&#20363;&#32553;&#25918;&#30340;&#24773;&#20917;&#12290;&#23427;&#20204;&#19982;&#29305;&#23450;&#23398;&#20064;&#20219;&#21153;&#24050;&#30693;&#30340;&#20005;&#26684;&#30028;&#38480;&#19968;&#33268;&#65292;&#24182;&#19982;&#25968;&#20540;&#23454;&#39564;&#23450;&#37327;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10164v1 Announce Type: cross  Abstract: Random features models play a distinguished role in the theory of deep learning, describing the behavior of neural networks close to their infinite-width limit. In this work, we present a thorough analysis of the generalization performance of random features models for generic supervised learning problems with Gaussian data. Our approach, built with tools from the statistical mechanics of disordered systems, maps the random features model to an equivalent polynomial model, and allows us to plot average generalization curves as functions of the two main control parameters of the problem: the number of random features $N$ and the size $P$ of the training set, both assumed to scale as powers in the input dimension $D$. Our results extend the case of proportional scaling between $N$, $P$ and $D$. They are in accordance with rigorous bounds known for certain particular learning tasks and are in quantitative agreement with numerical experime
&lt;/p&gt;</description></item><item><title>&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#35299;&#37322;&#38169;&#35823;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#35299;&#37322;&#31639;&#27861;&#38656;&#35201;&#26126;&#30830;&#20854;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#24182;&#28548;&#28165;&#21487;&#20197;&#21644;&#19981;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02870</link><description>&lt;p&gt;
&#27809;&#26377;&#35299;&#37322;&#30340;&#32479;&#35745;&#23398;&#65306;&#23545;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#30340;&#20919;&#38745;&#35266;&#23519;
&lt;/p&gt;
&lt;p&gt;
Statistics without Interpretation: A Sober Look at Explainable Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02870
&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#65292;&#36825;&#23548;&#33268;&#35299;&#37322;&#38169;&#35823;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#35299;&#37322;&#31639;&#27861;&#38656;&#35201;&#26126;&#30830;&#20854;&#36755;&#20986;&#30340;&#35299;&#37322;&#26041;&#24335;&#65292;&#24182;&#28548;&#28165;&#21487;&#20197;&#21644;&#19981;&#33021;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#36825;&#19968;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#20197;&#21450;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20851;&#20110;&#35299;&#37322;&#31639;&#27861;&#30340;&#24555;&#36895;&#21457;&#23637;&#30340;&#25991;&#29486;&#20013;&#65292;&#36825;&#20123;&#31639;&#27861;&#24448;&#24448;&#19981;&#28165;&#26970;&#25152;&#29992;&#20110;&#20309;&#22788;&#21450;&#20854;&#20351;&#29992;&#26041;&#24335;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#22240;&#20026;&#35299;&#37322;&#31639;&#27861;&#24448;&#24448;&#22312;&#25968;&#23398;&#19978;&#22797;&#26434;&#19988;&#38590;&#20197;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#27809;&#26377;&#28165;&#26224;&#35299;&#37322;&#30340;&#22797;&#26434;&#32479;&#35745;&#26041;&#27861;&#24456;&#21487;&#33021;&#23548;&#33268;&#35299;&#37322;&#30340;&#38169;&#35823;&#65292;&#36825;&#19968;&#20107;&#23454;&#22312;&#25991;&#29486;&#20013;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#20026;&#20102;&#21521;&#21069;&#25512;&#36827;&#65292;&#20851;&#20110;&#35299;&#37322;&#31639;&#27861;&#30340;&#35770;&#25991;&#24212;&#26126;&#30830;&#35299;&#37322;&#31639;&#27861;&#30340;&#36755;&#20986;&#22914;&#20309;&#35299;&#37322;&#12290;&#20182;&#20204;&#36824;&#24212;&#28548;&#28165;&#22312;&#32473;&#20986;&#35299;&#37322;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#22238;&#31572;&#21738;&#20123;&#20851;&#20110;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21738;&#20123;&#38382;&#39064;&#26080;&#27861;&#22238;&#31572;&#12290;&#25105;&#20204;&#30340;&#35770;&#28857;&#22522;&#20110;&#32479;&#35745;&#23398;&#21644;&#23427;&#20204;&#30340;&#35299;&#37322;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#23427;&#36824;&#20381;&#36182;&#20110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#21644;&#24212;&#29992;&#32479;&#35745;&#23398;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly growing literature on explanation algorithms, it often remains unclear what precisely these algorithms are for and how they should be used. We argue that this is because explanation algorithms are often mathematically complex but don't admit a clear interpretation. Unfortunately, complex statistical methods that don't have a clear interpretation are bound to lead to errors in interpretation, a fact that has become increasingly apparent in the literature. In order to move forward, papers on explanation algorithms should make clear how precisely the output of the algorithms should be interpreted. They should also clarify what questions about the function can and cannot be answered given the explanations. Our argument is based on the distinction between statistics and their interpretation. It also relies on parallels between explainable machine learning and applied statistics.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.15800</link><description>&lt;p&gt;
&#20351;&#29992;SHAP&#21644;LIME&#36827;&#34892;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#29305;&#24449;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
Provably Stable Feature Rankings with SHAP and LIME. (arXiv:2401.15800v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15800
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#26469;&#35774;&#35745;&#21487;&#38752;&#22320;&#25490;&#21517;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#29305;&#24449;&#24402;&#22240;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;SHAP&#21644;LIME&#31561;&#24120;&#29992;&#26041;&#27861;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#23548;&#33268;&#30340;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#22240;&#26159;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#30340;&#26222;&#36941;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20998;&#36755;&#20837;&#21464;&#37327;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#22914;SHAP&#21644;LIME&#65292;&#30001;&#20110;&#38543;&#26426;&#37319;&#26679;&#32780;&#20855;&#26377;&#39640;&#24230;&#19981;&#31283;&#23450;&#24615;&#12290;&#20511;&#37492;&#22810;&#37325;&#20551;&#35774;&#26816;&#39564;&#30340;&#24605;&#24819;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#33021;&#22815;&#20197;&#39640;&#27010;&#29575;&#27491;&#30830;&#25490;&#21517;&#26368;&#37325;&#35201;&#29305;&#24449;&#30340;&#24402;&#22240;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;RankSHAP&#20445;&#35777;$K$&#20010;&#26368;&#39640;Shapley&#20540;&#20855;&#26377;&#36229;&#36807;$1-\alpha$&#30340;&#27491;&#30830;&#25490;&#24207;&#27010;&#29575;&#12290;&#23454;&#35777;&#32467;&#26524;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#21644;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#36824;&#22312;&#20043;&#21069;&#30340;&#24037;&#20316;&#22522;&#30784;&#19978;&#20026;LIME&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#65292;&#30830;&#20445;&#20197;&#27491;&#30830;&#39034;&#24207;&#36873;&#25321;&#26368;&#37325;&#35201;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature attributions are ubiquitous tools for understanding the predictions of machine learning models. However, popular methods for scoring input variables such as SHAP and LIME suffer from high instability due to random sampling. Leveraging ideas from multiple hypothesis testing, we devise attribution methods that correctly rank the most important features with high probability. Our algorithm RankSHAP guarantees that the $K$ highest Shapley values have the proper ordering with probability exceeding $1-\alpha$. Empirical results demonstrate its validity and impressive computational efficiency. We also build on previous work to yield similar results for LIME, ensuring the most important features are selected in the right order.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.04343</link><description>&lt;p&gt;
&#31169;&#26377;&#38646;&#38454;&#20248;&#21270;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31169;&#26377;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Private Fine-tuning of Large Language Models with Zeroth-order Optimization. (arXiv:2401.04343v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04343
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31169;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#21487;&#33021;&#20250;&#23384;&#22312;&#36829;&#21453;&#38544;&#31169;&#30340;&#39118;&#38505;&#12290;&#24046;&#20998;&#38544;&#31169;&#26159;&#19968;&#31181;&#36890;&#36807;&#24378;&#21046;&#31639;&#27861;&#31283;&#23450;&#24615;&#26469;&#20943;&#36731;&#38544;&#31169;&#39118;&#38505;&#30340;&#26694;&#26550;&#12290;DP-SGD&#21487;&#20197;&#20197;&#20445;&#25252;&#38544;&#31169;&#30340;&#26041;&#24335;&#35757;&#32451;&#20855;&#26377;&#31169;&#26377;&#25968;&#25454;&#30340;&#27169;&#22411;&#65292;&#20294;&#20250;&#24102;&#26469;&#24615;&#33021;&#25439;&#22833;&#21644;&#37325;&#22823;&#24037;&#31243;&#25361;&#25112;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;DP-ZO&#65292;&#19968;&#31181;&#36890;&#36807;&#31169;&#26377;&#21270;&#38646;&#38454;&#20248;&#21270;&#26469;&#20445;&#25252;&#35757;&#32451;&#25968;&#25454;&#38544;&#31169;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#35774;&#35745;&#30340;&#19968;&#20010;&#20851;&#38190;&#35265;&#35299;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#30340;&#38646;&#38454;&#31639;&#27861;SPSA&#20013;&#30340;&#26799;&#24230;&#26041;&#21521;&#22987;&#32456;&#26159;&#38543;&#26426;&#30340;&#65292;&#32780;&#20165;&#20381;&#36182;&#20110;&#31169;&#26377;&#25968;&#25454;&#30340;&#20449;&#24687;&#26159;&#27493;&#38271;&#65292;&#21363;&#19968;&#20010;&#26631;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21482;&#38656;&#35201;&#23545;&#26631;&#37327;&#27493;&#38271;&#36827;&#34892;&#38544;&#31169;&#22788;&#29702;&#65292;&#36825;&#26159;&#23384;&#20648;&#25928;&#29575;&#39640;&#30340;&#26041;&#27861;&#12290;DP-ZO&#21487;&#20197;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#22122;&#22768;&#25110;&#39640;&#26031;&#22122;&#22768;&#26469;&#23454;&#29616;&#65292;&#22312;&#19981;&#21516;&#20219;&#21153;&#20043;&#38388;&#25552;&#20379;&#20102;&#38544;&#31169;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#24378;&#22823;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pretrained models on private datasets may run the risk of violating privacy. Differential privacy is a framework for mitigating privacy risks by enforcing algorithmic stability. DP-SGD enables training models with private data in a privacy-preserving manner, but raises new obstacles in the form of performance loss and significant engineering challenges. We introduce DP-ZO, a new method for fine-tuning large language models that preserves the privacy of training data by privatizing zeroth-order optimization. A key insight into the design of our method is that the direction of the gradient in SPSA, the zeroth-order algorithm we use, is always random and the only information that depends on private data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO, which can be instantiated with either Laplace or Gaussian noise, provides a strong privacy-utility trade-off across different tasks, and model si
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#25991;&#29486;&#20013;&#30340;&#26576;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#26159;&#38169;&#35823;&#30340;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.01879</link><description>&lt;p&gt;
&#20851;&#20110;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#30340;&#29702;&#35770;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Theoretical guarantees on the best-of-n alignment policy. (arXiv:2401.01879v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01879
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#23545;&#40784;&#29983;&#25104;&#27169;&#22411;&#30340;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#24182;&#35777;&#26126;&#20102;&#20043;&#21069;&#25991;&#29486;&#20013;&#30340;&#26576;&#20010;&#20998;&#26512;&#34920;&#36798;&#24335;&#26159;&#38169;&#35823;&#30340;&#12290;&#30740;&#31350;&#32773;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#31616;&#21333;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#23545;&#40784;&#26041;&#27861;&#26159;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#20174;&#19968;&#20010;&#22522;&#26412;&#31574;&#30053;&#20013;&#25277;&#21462;n&#20010;&#26679;&#26412;&#65292;&#24182;&#26681;&#25454;&#22870;&#21169;&#20989;&#25968;&#23545;&#23427;&#20204;&#36827;&#34892;&#25490;&#24207;&#65292;&#36873;&#25321;&#25490;&#21517;&#26368;&#39640;&#30340;&#26679;&#26412;&#12290;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#20998;&#26512;&#34920;&#36798;&#24335;&#22768;&#31216;&#26368;&#20339;n&#23545;&#40784;&#31574;&#30053;&#19982;&#22522;&#26412;&#31574;&#30053;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#31561;&#20110;$\log (n) (n-1)/n$&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#35813;&#35770;&#26029;&#30340;&#19981;&#27491;&#30830;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#21482;&#26159;&#23454;&#38469;KL&#25955;&#24230;&#30340;&#19968;&#20010;&#19978;&#30028;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35813;&#19978;&#30028;&#30340;&#32039;&#33268;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KL&#25955;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20960;&#20010;&#20363;&#23376;&#30340;&#23454;&#39564;&#35777;&#26126;&#23427;&#33021;&#25552;&#20379;&#19968;&#20010;&#32039;&#33268;&#30340;&#36817;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
A simple and effective method for the alignment of generative models is the best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked based on a reward function, and the highest ranking one is selected. A commonly used analytical expression in the literature claims that the KL divergence between the best-of-$n$ policy and the base policy is equal to $\log (n) (n-1)/n.$ We disprove the validity of this claim, and show that it is an upper bound on the actual KL divergence. We also explore the tightness of this upper bound in different regimes. Finally, we propose a new estimator for the KL divergence and empirically show that it provides a tight approximation through a few examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#30340;&#26368;&#22823;&#26435;&#37325;&#29109;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26435;&#37325;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#26631;&#20934;&#26041;&#27861;&#22312;&#36229;&#20986;&#20998;&#24067;&#24773;&#20917;&#19979;&#39044;&#27979;&#22810;&#26679;&#24615;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15704</link><description>&lt;p&gt;
&#26368;&#22823;&#26435;&#37325;&#29109;
&lt;/p&gt;
&lt;p&gt;
Maximum Weight Entropy. (arXiv:2309.15704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#30340;&#26368;&#22823;&#26435;&#37325;&#29109;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26435;&#37325;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#26631;&#20934;&#26041;&#27861;&#22312;&#36229;&#20986;&#20998;&#24067;&#24773;&#20917;&#19979;&#39044;&#27979;&#22810;&#26679;&#24615;&#32570;&#20047;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#20351;&#29992;&#36125;&#21494;&#26031;&#21644;&#38598;&#25104;&#26041;&#27861;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;&#24403;&#22312;&#36229;&#20986;&#20998;&#24067;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#26102;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39044;&#27979;&#22810;&#26679;&#24615;&#30340;&#32570;&#20047;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35748;&#20026;&#26631;&#20934;&#26041;&#27861;&#22312;&#26435;&#37325;&#31354;&#38388;&#20013;&#37319;&#26679;&#30340;&#8220;&#36807;&#24230;&#32422;&#26463;&#8221;&#23548;&#33268;&#20102;&#26435;&#37325;&#22810;&#26679;&#24615;&#30340;&#32570;&#20047;&#12290;&#26412;&#25991;&#24314;&#35758;&#37319;&#29992;&#26368;&#22823;&#29109;&#21407;&#29702;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#26435;&#37325;&#22810;&#26679;&#24615;&#65292;&#25551;&#36848;&#26368;&#22823;&#29109;&#26435;&#37325;&#20998;&#24067;&#26469;&#34920;&#31034;&#35748;&#30693;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#20135;&#29983;&#19982;&#35757;&#32451;&#35266;&#23519;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper deals with uncertainty quantification and out-of-distribution detection in deep learning using Bayesian and ensemble methods. It proposes a practical solution to the lack of prediction diversity observed recently for standard approaches when used out-of-distribution (Ovadia et al., 2019; Liu et al., 2021). Considering that this issue is mainly related to a lack of weight diversity, we claim that standard methods sample in "over-restricted" regions of the weight space due to the use of "over-regularization" processes, such as weight decay and zero-mean centered Gaussian priors. We propose to solve the problem by adopting the maximum entropy principle for the weight distribution, with the underlying idea to maximize the weight diversity. Under this paradigm, the epistemic uncertainty is described by the weight distribution of maximal entropy that produces neural networks "consistent" with the training observations. Considering stochastic neural networks, a practical optimizati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30830;&#31435;&#20102;&#20854;&#39640;&#25928;&#21435;&#37327;&#21270;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;PQC&#26550;&#26500;&#35774;&#35745;&#24314;&#35758;&#21644;&#35782;&#21035;&#20102;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#30340;&#24517;&#35201;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2309.11647</link><description>&lt;p&gt;
&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;
Potential and limitations of random Fourier features for dequantizing quantum machine learning. (arXiv:2309.11647v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11647
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21435;&#37327;&#21270;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#28508;&#21147;&#19982;&#23616;&#38480;&#24615;&#65292;&#24182;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30830;&#31435;&#20102;&#20854;&#39640;&#25928;&#21435;&#37327;&#21270;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#65292;&#24182;&#25552;&#20986;&#20102;PQC&#26550;&#26500;&#35774;&#35745;&#24314;&#35758;&#21644;&#35782;&#21035;&#20102;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#30340;&#24517;&#35201;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#26159;&#36817;&#26399;&#37327;&#23376;&#35774;&#22791;&#26368;&#24191;&#27867;&#25506;&#32034;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#30446;&#21069;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#65292;&#20854;&#20013;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#34987;&#29992;&#20316;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#27169;&#22411;&#20855;&#26377;&#20016;&#23500;&#30340;&#32467;&#26500;&#65292;&#22240;&#27492;&#21487;&#33021;&#36890;&#36807;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#36827;&#34892;&#39640;&#25928;&#30340;&#21435;&#37327;&#21270;&#12290;&#26412;&#25991;&#22312;&#22238;&#24402;&#38382;&#39064;&#19978;&#30830;&#31435;&#20102;&#38543;&#26426;&#20613;&#37324;&#21494;&#29305;&#24449;&#22312;&#21464;&#20998;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#20013;&#25552;&#20379;&#39640;&#25928;&#21435;&#37327;&#21270;&#30340;&#24517;&#35201;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;&#21033;&#29992;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20855;&#20307;&#30340;&#21442;&#25968;&#21270;&#37327;&#23376;&#30005;&#36335;&#26550;&#26500;&#35774;&#35745;&#24314;&#35758;&#65292;&#20197;&#21450;&#35782;&#21035;&#20102;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#21462;&#24471;&#28508;&#22312;&#37327;&#23376;&#20248;&#21183;&#30340;&#24517;&#35201;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantum machine learning is arguably one of the most explored applications of near-term quantum devices. Much focus has been put on notions of variational quantum machine learning where parameterized quantum circuits (PQCs) are used as learning models. These PQC models have a rich structure which suggests that they might be amenable to efficient dequantization via random Fourier features (RFF). In this work, we establish necessary and sufficient conditions under which RFF does indeed provide an efficient dequantization of variational quantum machine learning for regression. We build on these insights to make concrete suggestions for PQC architecture design, and to identify structures which are necessary for a regression problem to admit a potential quantum advantage via PQC based optimization.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;</title><link>http://arxiv.org/abs/2308.03887</link><description>&lt;p&gt;
&#29992;&#19968;&#31181;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Cell Tracking with a Time-Symmetric Deep Learning Approach. (arXiv:2308.03887v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.03887
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#23545;&#31216;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21319;&#32454;&#32990;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#12290;&#35813;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#32780;&#26159;&#22522;&#20110;&#32454;&#32990;&#30340;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#65292;&#20855;&#26377;&#23398;&#20064;&#32454;&#32990;&#36816;&#21160;&#27169;&#24335;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#22788;&#29702;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#22823;&#37327;&#35270;&#39057;&#24103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#35270;&#39057;&#26174;&#24494;&#38236;&#35760;&#24405;&#20934;&#30830;&#36319;&#36394;&#27963;&#32454;&#32990;&#20173;&#28982;&#26159;&#30446;&#21069;&#27969;&#34892;&#30340;&#26368;&#20808;&#36827;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#26041;&#27861;&#30340;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#26377;&#20960;&#20010;&#29616;&#26377;&#21644;&#26032;&#30340;&#24212;&#29992;&#23581;&#35797;&#23558;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26694;&#26550;&#25972;&#21512;&#21040;&#35813;&#20219;&#21153;&#20013;&#65292;&#20294;&#22823;&#37096;&#20998;&#20173;&#28982;&#20005;&#37325;&#20381;&#36182;&#20110;&#23884;&#20837;&#20854;&#26550;&#26500;&#25110;&#20854;&#20182;&#21069;&#25552;&#26465;&#20214;&#20013;&#30340;&#36830;&#32493;&#24103;&#36319;&#36394;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#24191;&#20041;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#36319;&#36394;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20165;&#20381;&#36182;&#20110;&#32454;&#32990;&#21487;&#20197;&#26681;&#25454;&#20854;&#26102;&#31354;&#37051;&#22495;&#36827;&#34892;&#36319;&#36394;&#30340;&#20551;&#35774;&#65292;&#32780;&#38750;&#20165;&#38480;&#20110;&#36830;&#32493;&#24103;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#39069;&#22806;&#20248;&#28857;&#26159;&#32454;&#32990;&#30340;&#36816;&#21160;&#27169;&#24335;&#21487;&#20197;&#23436;&#20840;&#30001;&#39044;&#27979;&#22120;&#22312;&#27809;&#26377;&#20219;&#20309;&#20808;&#39564;&#20551;&#35774;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#65292;&#24182;&#19988;&#20855;&#26377;&#22788;&#29702;&#22823;&#37327;&#20855;&#26377;&#20005;&#37325;&#20266;&#24433;&#30340;&#35270;&#39057;&#24103;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The accurate tracking of live cells using video microscopy recordings remains a challenging task for popular state-of-the-art image processing based object tracking methods. In recent years, several existing and new applications have attempted to integrate deep-learning based frameworks for this task, but most of them still heavily rely on consecutive frame based tracking embedded in their architecture or other premises that hinder generalized learning. To address this issue, we aimed to develop a new deep-learning based tracking method that relies solely on the assumption that cells can be tracked based on their spatio-temporal neighborhood, without restricting it to consecutive frames. The proposed method has the additional benefit that the motion patterns of the cells can be learned completely by the predictor without any prior assumptions, and it has the potential to handle a large number of video frames with heavy artifacts. The efficacy of the proposed method is demonstrated thro
&lt;/p&gt;</description></item><item><title>&#31561;&#21464;&#29699;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#38477;&#20302;&#23545;&#29305;&#23450;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#22312;&#21435;&#22122;&#21644;&#37325;&#24314;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.03298</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#21644;&#39640;&#24615;&#33021;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#30340;&#31561;&#21464;&#29699;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Equivariant Spherical CNN for Data Efficient and High-Performance Medical Image Processing. (arXiv:2307.03298v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03298
&lt;/p&gt;
&lt;p&gt;
&#31561;&#21464;&#29699;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#22788;&#29702;&#30340;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36890;&#36807;&#38477;&#20302;&#23545;&#29305;&#23450;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#65292;&#24182;&#19988;&#22312;&#21435;&#22122;&#21644;&#37325;&#24314;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31361;&#20986;&#20102;&#31561;&#21464;&#32593;&#32476;&#20316;&#20026;&#39640;&#25928;&#21644;&#39640;&#24615;&#33021;&#36884;&#24452;&#22312;&#26029;&#23618;&#25195;&#25551;&#24212;&#29992;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24314;&#31435;&#22312;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#23616;&#38480;&#24615;&#20043;&#19978;&#65292;CNN&#24050;&#32463;&#22312;&#21508;&#31181;&#21307;&#23398;&#24433;&#20687;&#31995;&#32479;&#30340;&#21518;&#22788;&#29702;&#20013;&#26174;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;CNN&#30340;&#25928;&#29575;&#20005;&#37325;&#20381;&#36182;&#20110;&#19968;&#20010;&#19981;&#21464;&#21644;&#36866;&#24403;&#30340;&#35757;&#32451;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31561;&#21464;&#32593;&#32476;&#65292;&#26088;&#22312;&#20943;&#23569;CNN&#23545;&#29305;&#23450;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#31561;&#21464;CNN&#22312;&#29699;&#20449;&#21495;&#19978;&#22312;&#26029;&#23618;&#25195;&#25551;&#21307;&#23398;&#25104;&#20687;&#38382;&#39064;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#29699;&#24418;CNN&#65288;SCNN&#65289;&#22312;&#21435;&#22122;&#21644;&#37325;&#24314;&#22522;&#20934;&#38382;&#39064;&#19978;&#20855;&#26377;&#20248;&#36234;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;SCNN&#20316;&#20026;&#20256;&#32479;&#22270;&#20687;&#37325;&#24314;&#24037;&#20855;&#30340;&#34917;&#20805;&#65292;&#22686;&#24378;&#32467;&#26524;&#21516;&#26102;&#20943;&#23569;&#23545;&#35757;&#32451;&#38598;&#30340;&#20381;&#36182;&#24615;&#12290;&#22312;&#25152;&#26377;&#26696;&#20363;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
This work highlights the significance of equivariant networks as efficient and high-performance approaches for tomography applications. Our study builds upon the limitations of Convolutional Neural Networks (CNNs), which have shown promise in post-processing various medical imaging systems. However, the efficiency of conventional CNNs heavily relies on an undiminished and proper training set. To tackle this issue, in this study, we introduce an equivariant network, aiming to reduce CNN's dependency on specific training sets. We evaluate the efficacy of equivariant CNNs on spherical signals for tomographic medical imaging problems. Our results demonstrate superior quality and computational efficiency of spherical CNNs (SCNNs) in denoising and reconstructing benchmark problems. Furthermore, we propose a novel approach to employ SCNNs as a complement to conventional image reconstruction tools, enhancing the outcomes while reducing reliance on the training set. Across all cases, we observe
&lt;/p&gt;</description></item></channel></rss>