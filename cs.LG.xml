<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19647</link><description>&lt;p&gt;
&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65306;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21363;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#65292;&#21487;&#20197;&#22312;&#35821;&#35328;&#27169;&#22411;&#20013;&#21457;&#29616;&#21644;&#32534;&#36753;&#21487;&#35299;&#37322;&#30340;&#22240;&#26524;&#22270;&#65292;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#21644;&#21253;&#21547;&#20102;&#29992;&#20110;&#25552;&#39640;&#20998;&#31867;&#22120;&#27867;&#21270;&#33021;&#21147;&#30340;SHIFT&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#29992;&#20110;&#21457;&#29616;&#21644;&#24212;&#29992;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#30005;&#36335;&#26159;&#20154;&#31867;&#21487;&#35299;&#37322;&#29305;&#24449;&#30340;&#22240;&#26524;&#30456;&#20851;&#23376;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#37322;&#35821;&#35328;&#27169;&#22411;&#34892;&#20026;&#12290; &#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#30830;&#23450;&#30340;&#30005;&#36335;&#30001;&#22810;&#20041;&#19988;&#38590;&#20197;&#35299;&#37322;&#30340;&#21333;&#20803;&#32452;&#25104;&#65292;&#20363;&#22914;&#27880;&#24847;&#21147;&#22836;&#25110;&#31070;&#32463;&#20803;&#65292;&#20351;&#23427;&#20204;&#19981;&#36866;&#29992;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#12290; &#30456;&#27604;&#20043;&#19979;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23454;&#29616;&#20102;&#23545;&#26410;&#39044;&#26009;&#26426;&#21046;&#30340;&#35814;&#32454;&#29702;&#35299;&#12290; &#30001;&#20110;&#23427;&#20204;&#22522;&#20110;&#32454;&#31890;&#24230;&#21333;&#20803;&#65292;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#23545;&#19979;&#28216;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65306;&#25105;&#20204; introduc&#20102;SHIFT&#65292;&#36890;&#36807;&#20999;&#38500;&#20154;&#31867;&#21028;&#26029;&#20026;&#20219;&#21153;&#19981;&#30456;&#20851;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#25552;&#39640;&#20998;&#31867;&#22120;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290; &#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21457;&#29616;&#25104;&#21315;&#19978;&#19975;&#20010;&#31232;&#30095;&#29305;&#24449;&#30005;&#36335;&#26469;&#23637;&#31034;&#19968;&#20010;&#23436;&#20840;&#26080;&#30417;&#30563;&#19988;&#21487;&#25193;&#23637;&#30340;&#21487;&#35299;&#37322;&#24615;&#31649;&#32447;&#65292;&#29992;&#20110;&#33258;&#21160;&#21457;&#29616;&#30340;&#27169;&#22411;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19647v1 Announce Type: cross  Abstract: We introduce methods for discovering and applying sparse feature circuits. These are causally implicated subnetworks of human-interpretable features for explaining language model behaviors. Circuits identified in prior work consist of polysemantic and difficult-to-interpret units like attention heads or neurons, rendering them unsuitable for many downstream applications. In contrast, sparse feature circuits enable detailed understanding of unanticipated mechanisms. Because they are based on fine-grained units, sparse feature circuits are useful for downstream tasks: We introduce SHIFT, where we improve the generalization of a classifier by ablating features that a human judges to be task-irrelevant. Finally, we demonstrate an entirely unsupervised and scalable interpretability pipeline by discovering thousands of sparse feature circuits for automatically discovered model behaviors.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMA&#30340;&#26032;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#25105;&#25193;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#27169;&#22359;&#21270;&#36866;&#37197;&#65292;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#36951;&#24536;&#65292;&#35299;&#20915;&#20808;&#21069;&#38024;&#23545;&#38745;&#24577;&#27169;&#22411;&#26550;&#26500;&#24773;&#20917;&#19979;&#23384;&#22312;&#30340;&#36807;&#22810;&#21442;&#25968;&#20998;&#37197;&#25110;&#36866;&#24212;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.18886</link><description>&lt;p&gt;
&#20351;&#29992;&#28151;&#21512;&#36866;&#37197;&#22120;&#36827;&#34892;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#25105;&#25193;&#23637;&#20197;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18886
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMA&#30340;&#26032;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#33258;&#25105;&#25193;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#27169;&#22359;&#21270;&#36866;&#37197;&#65292;&#23454;&#29616;&#25345;&#32493;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#26368;&#23567;&#36951;&#24536;&#65292;&#35299;&#20915;&#20808;&#21069;&#38024;&#23545;&#38745;&#24577;&#27169;&#22411;&#26550;&#26500;&#24773;&#20917;&#19979;&#23384;&#22312;&#30340;&#36807;&#22810;&#21442;&#25968;&#20998;&#37197;&#25110;&#36866;&#24212;&#24615;&#19981;&#36275;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#20174;&#36830;&#32493;&#21040;&#36798;&#30340;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#20943;&#23569;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#30340;&#36951;&#24536;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEMA&#30340;&#26032;&#22411;&#24494;&#35843;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#25105;&#25193;&#23637;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#27169;&#22359;&#21270;&#36866;&#37197;&#65292;&#33258;&#21160;&#20915;&#23450;...&#65288;&#25688;&#35201;&#26410;&#23436;&#25972;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18886v1 Announce Type: new  Abstract: Continual learning aims to learn from a stream of continuously arriving data with minimum forgetting of previously learned knowledge. While previous works have explored the effectiveness of leveraging the generalizable knowledge from pre-trained models in continual learning, existing parameter-efficient fine-tuning approaches focus on the use of a predetermined or task-wise set of adapters or prompts. However, these approaches still suffer from forgetting due to task interference on jointly used parameters or restricted flexibility. The reliance on a static model architecture may lead to the allocation of excessive parameters that are not essential or, conversely, inadequate adaptation for downstream tasks, given that the scale and distribution of incoming data are unpredictable in continual learning. We propose Self-Expansion of pre-trained models with Modularized Adaptation (SEMA), a novel fine-tuning approach which automatically decid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.13901</link><description>&lt;p&gt;
&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#38750;&#28176;&#36817;&#25910;&#25947;&#65306;&#26032;&#26041;&#27861;&#21644;&#25913;&#36827;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Non-asymptotic Convergence of Discrete-time Diffusion Models: New Approach and Improved Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13901
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#25913;&#36827;&#20102;&#23545;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#20445;&#35777;&#65292;&#24182;&#25552;&#39640;&#20102;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21435;&#22122;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#29983;&#25104;&#25216;&#26415;&#20986;&#29616;&#65292;&#23558;&#22122;&#22768;&#36716;&#21270;&#20026;&#25968;&#25454;&#12290;&#29702;&#35770;&#19978;&#20027;&#35201;&#30740;&#31350;&#20102;&#36830;&#32493;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#19988;&#20165;&#22312;&#25991;&#29486;&#20013;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#20102;&#33719;&#24471;&#12290;&#26412;&#25991;&#20026;&#26356;&#22823;&#31867;&#30340;&#20998;&#24067;&#24314;&#31435;&#20102;&#31163;&#25955;&#26102;&#38388;&#25193;&#25955;&#27169;&#22411;&#30340;&#25910;&#25947;&#24615;&#20445;&#35777;&#65292;&#24182;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#23545;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#30340;&#25910;&#25947;&#36895;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#39318;&#20808;&#20026;&#20855;&#26377;&#26377;&#38480;&#20108;&#38454;&#30697;&#30340;&#24179;&#28369;&#21644;&#19968;&#33324;&#65288;&#21487;&#33021;&#38750;&#20809;&#28369;&#65289;&#20998;&#24067;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#21518;&#23558;&#32467;&#26524;&#19987;&#38376;&#24212;&#29992;&#20110;&#19968;&#20123;&#26377;&#26126;&#30830;&#21442;&#25968;&#20381;&#36182;&#20851;&#31995;&#30340;&#26377;&#36259;&#20998;&#24067;&#31867;&#21035;&#65292;&#21253;&#25324;&#20855;&#26377;Lipschitz&#20998;&#25968;&#12289;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#21644;&#20855;&#26377;&#26377;&#30028;&#25903;&#25745;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13901v1 Announce Type: new  Abstract: The denoising diffusion model emerges recently as a powerful generative technique that converts noise into data. Theoretical convergence guarantee has been mainly studied for continuous-time diffusion models, and has been obtained for discrete-time diffusion models only for distributions with bounded support in the literature. In this paper, we establish the convergence guarantee for substantially larger classes of distributions under discrete-time diffusion models and further improve the convergence rate for distributions with bounded support. In particular, we first establish the convergence rates for both smooth and general (possibly non-smooth) distributions having finite second moment. We then specialize our results to a number of interesting classes of distributions with explicit parameter dependencies, including distributions with Lipschitz scores, Gaussian mixture distributions, and distributions with bounded support. We further 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#38590;&#39064;</title><link>https://arxiv.org/abs/2402.11317</link><description>&lt;p&gt;
&#38024;&#23545;&#38750;&#38745;&#24577;&#21160;&#24577;&#30340;&#24555;&#36895;&#22312;&#32447;&#35843;&#25972;&#30340;&#21435;&#20559;&#32622;&#31163;&#32447;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DORA&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#22312;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36866;&#24212;&#24615;&#31574;&#30053;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#19982;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#20043;&#38388;&#30340;&#38590;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#33021;&#22815;&#36866;&#24212;&#38750;&#38745;&#24577;&#29615;&#22659;&#30340;&#31574;&#30053;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#20165;&#26377;&#19968;&#32452;&#26377;&#38480;&#30340;&#39044;&#20808;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#31163;&#32447;&#35774;&#32622;&#20013;&#23398;&#20064;&#36825;&#31181;&#36866;&#24212;&#24615;&#31574;&#30053;&#23384;&#22312;&#26174;&#33879;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#21435;&#20559;&#32622;&#31163;&#32447;&#34920;&#31034;&#24555;&#36895;&#22312;&#32447;&#35843;&#25972;&#65288;DORA&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;DORA&#34701;&#20837;&#20102;&#20449;&#24687;&#29942;&#39048;&#21407;&#29702;&#65292;&#26368;&#22823;&#21270;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#29615;&#22659;&#25968;&#25454;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#20102;&#21160;&#24577;&#32534;&#30721;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20114;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DORA&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11317v1 Announce Type: cross  Abstract: Developing policies that can adjust to non-stationary environments is essential for real-world reinforcement learning applications. However, learning such adaptable policies in offline settings, with only a limited set of pre-collected trajectories, presents significant challenges. A key difficulty arises because the limited offline data makes it hard for the context encoder to differentiate between changes in the environment dynamics and shifts in the behavior policy, often leading to context misassociations. To address this issue, we introduce a novel approach called Debiased Offline Representation for fast online Adaptation (DORA). DORA incorporates an information bottleneck principle that maximizes mutual information between the dynamics encoding and the environmental data, while minimizing mutual information between the dynamics encoding and the actions of the behavior policy. We present a practical implementation of DORA, leverag
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24773;&#20917;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#22810;&#26102;&#24207;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#22810;&#31354;&#38388;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;&#36825;&#31687;&#35770;&#25991;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.06289</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#35780;&#20272;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#21644;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Evaluating Membership Inference Attacks and Defenses in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06289
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#35780;&#20272;&#20102;&#32852;&#37030;&#23398;&#20064;&#20013;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#21644;&#38450;&#24481;&#30340;&#24773;&#20917;&#12290;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#21457;&#29616;&#65306;&#22810;&#26102;&#24207;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#26377;&#25928;&#24615;&#65292;&#22810;&#31354;&#38388;&#30340;&#27169;&#22411;&#20449;&#24687;&#26377;&#21161;&#20110;&#25552;&#39640;&#25915;&#20987;&#30340;&#25928;&#26524;&#12290;&#36825;&#31687;&#35770;&#25991;&#36824;&#35780;&#20272;&#20102;&#20004;&#31181;&#38450;&#24481;&#26426;&#21046;&#30340;&#25928;&#29992;&#21644;&#38544;&#31169;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;(MIAs)&#23545;&#20110;&#38544;&#31169;&#20445;&#25252;&#30340;&#23041;&#32961;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#26085;&#30410;&#22686;&#38271;&#12290;&#21322;&#35802;&#23454;&#30340;&#25915;&#20987;&#32773;&#65292;&#20363;&#22914;&#26381;&#21153;&#22120;&#65292;&#21487;&#20197;&#26681;&#25454;&#35266;&#23519;&#21040;&#30340;&#27169;&#22411;&#20449;&#24687;&#30830;&#23450;&#19968;&#20010;&#29305;&#23450;&#26679;&#26412;&#26159;&#21542;&#23646;&#20110;&#30446;&#26631;&#23458;&#25143;&#31471;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;MIAs&#21644;&#30456;&#24212;&#30340;&#38450;&#24481;&#31574;&#30053;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#25105;&#20204;&#23545;MIAs&#30340;&#35780;&#20272;&#25581;&#31034;&#20102;&#20004;&#20010;&#37325;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#32467;&#21512;&#22810;&#20010;&#36890;&#20449;&#36718;&#27425;&#30340;&#27169;&#22411;&#20449;&#24687;(&#22810;&#26102;&#24207;)&#30456;&#27604;&#20110;&#21033;&#29992;&#21333;&#20010;&#26102;&#26399;&#30340;&#27169;&#22411;&#20449;&#24687;&#25552;&#39640;&#20102;MIAs&#30340;&#25972;&#20307;&#26377;&#25928;&#24615;&#12290;&#20854;&#27425;&#65292;&#22312;&#38750;&#30446;&#26631;&#23458;&#25143;&#31471;(Multi-spatial)&#20013;&#34701;&#20837;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;MIAs&#30340;&#25928;&#26524;&#65292;&#29305;&#21035;&#26159;&#24403;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#26159;&#21516;&#36136;&#30340;&#26102;&#20505;&#12290;&#36825;&#20984;&#26174;&#20102;&#22312;MIAs&#20013;&#32771;&#34385;&#26102;&#24207;&#21644;&#31354;&#38388;&#27169;&#22411;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#38544;&#31169;-&#25928;&#29992;&#26435;&#34913;&#35780;&#20272;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#38450;&#24481;&#26426;&#21046;&#23545;MIAs&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Membership Inference Attacks (MIAs) pose a growing threat to privacy preservation in federated learning. The semi-honest attacker, e.g., the server, may determine whether a particular sample belongs to a target client according to the observed model information. This paper conducts an evaluation of existing MIAs and corresponding defense strategies. Our evaluation on MIAs reveals two important findings about the trend of MIAs. Firstly, combining model information from multiple communication rounds (Multi-temporal) enhances the overall effectiveness of MIAs compared to utilizing model information from a single epoch. Secondly, incorporating models from non-target clients (Multi-spatial) significantly improves the effectiveness of MIAs, particularly when the clients' data is homogeneous. This highlights the importance of considering the temporal and spatial model information in MIAs. Next, we assess the effectiveness via privacy-utility tradeoff for two type defense mechanisms against MI
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;</title><link>https://arxiv.org/abs/2402.03664</link><description>&lt;p&gt;
&#39640;&#25928;&#27714;&#35299;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Efficient Solvers for Partial Gromov-Wasserstein
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#30340;&#26032;&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;&#26469;&#35299;&#20915;&#20559;&#24046;Gromov-Wasserstein&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#24046;Gromov-Wasserstein&#65288;PGW&#65289;&#38382;&#39064;&#21487;&#20197;&#27604;&#36739;&#20855;&#26377;&#19981;&#22343;&#21248;&#36136;&#37327;&#30340;&#24230;&#37327;&#31354;&#38388;&#20013;&#30340;&#27979;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#36825;&#20123;&#31354;&#38388;&#20043;&#38388;&#30340;&#19981;&#24179;&#34913;&#21644;&#37096;&#20998;&#21305;&#37197;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#21487;&#20197;&#36716;&#21270;&#20026;Gromov-Wasserstein&#38382;&#39064;&#30340;&#19968;&#20010;&#21464;&#31181;&#65292;&#31867;&#20284;&#20110;&#25226;&#20559;&#24046;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#36716;&#21270;&#20026;&#26368;&#20248;&#36816;&#36755;&#38382;&#39064;&#12290;&#36825;&#20010;&#36716;&#21270;&#23548;&#33268;&#20102;&#20004;&#20010;&#26032;&#30340;&#27714;&#35299;&#22120;&#65292;&#22522;&#20110;Frank-Wolfe&#31639;&#27861;&#65292;&#25968;&#23398;&#21644;&#35745;&#31639;&#19978;&#31561;&#20215;&#65292;&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;PGW&#38382;&#39064;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;PGW&#38382;&#39064;&#26500;&#25104;&#20102;&#24230;&#37327;&#27979;&#24230;&#31354;&#38388;&#30340;&#24230;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#19982;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#22312;&#24418;&#29366;&#21305;&#37197;&#21644;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#35745;&#31639;&#26102;&#38388;&#21644;&#24615;&#33021;&#27604;&#36739;&#65292;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#27714;&#35299;&#22120;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The partial Gromov-Wasserstein (PGW) problem facilitates the comparison of measures with unequal masses residing in potentially distinct metric spaces, thereby enabling unbalanced and partial matching across these spaces. In this paper, we demonstrate that the PGW problem can be transformed into a variant of the Gromov-Wasserstein problem, akin to the conversion of the partial optimal transport problem into an optimal transport problem. This transformation leads to two new solvers, mathematically and computationally equivalent, based on the Frank-Wolfe algorithm, that provide efficient solutions to the PGW problem. We further establish that the PGW problem constitutes a metric for metric measure spaces. Finally, we validate the effectiveness of our proposed solvers in terms of computation time and performance on shape-matching and positive-unlabeled learning problems, comparing them against existing baselines.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#26102;&#38388;&#22359;&#65292;&#20197; $\sqrt{\log T/T}$ &#30340;&#36895;&#29575;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2312.15574</link><description>&lt;p&gt;
&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster Rates for Switchback Experiments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.15574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#24555;&#36895;&#30340;Switchback&#23454;&#39564;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25972;&#20010;&#26102;&#38388;&#22359;&#65292;&#20197; $\sqrt{\log T/T}$ &#30340;&#36895;&#29575;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Switchback&#23454;&#39564;&#35774;&#35745;&#20013;&#65292;&#19968;&#20010;&#21333;&#29420;&#30340;&#21333;&#20803;&#65288;&#20363;&#22914;&#25972;&#20010;&#31995;&#32479;&#65289;&#22312;&#20132;&#26367;&#30340;&#26102;&#38388;&#22359;&#20013;&#26292;&#38706;&#20110;&#19968;&#20010;&#38543;&#26426;&#22788;&#29702;&#65292;&#22788;&#29702;&#24182;&#34892;&#22788;&#29702;&#20102;&#36328;&#21333;&#20803;&#21644;&#26102;&#38388;&#24178;&#25200;&#38382;&#39064;&#12290;Hu&#21644;Wager&#65288;2022&#65289;&#26368;&#36817;&#25552;&#20986;&#20102;&#19968;&#31181;&#25130;&#26029;&#22359;&#36215;&#22987;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#65292;&#24182;&#22312;Markov&#26465;&#20214;&#19979;&#35777;&#26126;&#20102;&#29992;&#20110;&#20272;&#35745;&#20840;&#23616;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;GATE&#65289;&#30340;$T^{-1/3}$&#36895;&#29575;&#65292;&#20182;&#20204;&#22768;&#31216;&#36825;&#20010;&#36895;&#29575;&#26159;&#26368;&#20248;&#30340;&#65292;&#24182;&#24314;&#35758;&#23558;&#27880;&#24847;&#21147;&#36716;&#21521;&#19981;&#21516;&#65288;&#19988;&#20381;&#36182;&#35774;&#35745;&#65289;&#30340;&#20272;&#35745;&#37327;&#65292;&#20197;&#33719;&#24471;&#26356;&#24555;&#30340;&#36895;&#29575;&#12290;&#23545;&#20110;&#30456;&#21516;&#30340;&#35774;&#35745;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#20272;&#35745;&#22120;&#65292;&#20351;&#29992;&#25972;&#20010;&#22359;&#65292;&#24182;&#24778;&#20154;&#22320;&#35777;&#26126;&#65292;&#22312;&#30456;&#21516;&#30340;&#20551;&#35774;&#19979;&#65292;&#23427;&#23454;&#38469;&#19978;&#36798;&#21040;&#20102;&#21407;&#22987;&#30340;&#35774;&#35745;&#29420;&#31435;GATE&#20272;&#35745;&#37327;&#30340;$\sqrt{\log T/T}$&#30340;&#20272;&#35745;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Switchback experimental design, wherein a single unit (e.g., a whole system) is exposed to a single random treatment for interspersed blocks of time, tackles both cross-unit and temporal interference. Hu and Wager (2022) recently proposed a treatment-effect estimator that truncates the beginnings of blocks and established a $T^{-1/3}$ rate for estimating the global average treatment effect (GATE) in a Markov setting with rapid mixing. They claim this rate is optimal and suggest focusing instead on a different (and design-dependent) estimand so as to enjoy a faster rate. For the same design we propose an alternative estimator that uses the whole block and surprisingly show that it in fact achieves an estimation rate of $\sqrt{\log T/T}$ for the original design-independent GATE estimand under the same assumptions.
&lt;/p&gt;</description></item><item><title>&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20351;&#29992;&#22810;&#20851;&#31995;&#25968;&#25454;&#30340;&#24067;&#25289;&#26684;&#25463;&#20811;&#25216;&#26415;&#22823;&#23398;&#20851;&#31995;&#23398;&#20064;&#36164;&#28304;&#24211;&#65292;&#21253;&#21547;&#22823;&#37327;SQL&#25968;&#25454;&#24211;&#65292;&#24182;&#30001;getML&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/1511.03086</link><description>&lt;p&gt;
&#24067;&#25289;&#26684;&#25463;&#20811;&#25216;&#26415;&#22823;&#23398;&#20851;&#31995;&#23398;&#20064;&#36164;&#28304;&#24211;
&lt;/p&gt;
&lt;p&gt;
The CTU Prague Relational Learning Repository
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1511.03086
&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20351;&#29992;&#22810;&#20851;&#31995;&#25968;&#25454;&#30340;&#24067;&#25289;&#26684;&#25463;&#20811;&#25216;&#26415;&#22823;&#23398;&#20851;&#31995;&#23398;&#20064;&#36164;&#28304;&#24211;&#65292;&#21253;&#21547;&#22823;&#37327;SQL&#25968;&#25454;&#24211;&#65292;&#24182;&#30001;getML&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24067;&#25289;&#26684;&#20851;&#31995;&#23398;&#20064;&#36164;&#28304;&#24211;&#30340;&#30446;&#30340;&#26159;&#25903;&#25345;&#20855;&#26377;&#22810;&#20851;&#31995;&#25968;&#25454;&#30340;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#12290;&#27492;&#36164;&#28304;&#24211;&#30446;&#21069;&#21253;&#21547;148&#20010;SQL&#25968;&#25454;&#24211;&#65292;&#25176;&#31649;&#22312;&#20301;&#20110;\url{https://relational-data.org}&#30340;&#20844;&#20849;MySQL&#26381;&#21153;&#22120;&#19978;&#12290;&#26381;&#21153;&#22120;&#30001;getML&#25552;&#20379;&#65292;&#20197;&#25903;&#25345;&#20851;&#31995;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#65288;\url{www.getml.com}&#65289;&#12290;&#21487;&#25628;&#32034;&#30340;&#20803;&#25968;&#25454;&#24211;&#25552;&#20379;&#20803;&#25968;&#25454;&#65288;&#20363;&#22914;&#25968;&#25454;&#24211;&#20013;&#30340;&#34920;&#25968;&#37327;&#12289;&#34920;&#20013;&#30340;&#34892;&#21015;&#25968;&#12289;&#33258;&#20851;&#32852;&#25968;&#37327;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:1511.03086v2 Announce Type: replace  Abstract: The aim of the Prague Relational Learning Repository is to support machine learning research with multi-relational data. The repository currently contains 148 SQL databases hosted on a public MySQL server located at \url{https://relational-data.org}. The server is provided by getML to support the relational machine learning community (\url{www.getml.com}). A searchable meta-database provides metadata (e.g., the number of tables in the database, the number of rows and columns in the tables, the number of self-relationships).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#21327;&#21464;&#37327;&#19979;&#30340;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20986;&#30340;&#26032;&#19979;&#30028;&#20855;&#26377;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#30340;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2306.13255</link><description>&lt;p&gt;
&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#19979;&#22810;&#31867;&#20998;&#31867;&#30340;&#28176;&#36827;&#27867;&#21270;&#31934;&#24230;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Precise Asymptotic Generalization for Multiclass Classification with Overparameterized Linear Models. (arXiv:2306.13255v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39640;&#26031;&#21327;&#21464;&#37327;&#19979;&#30340;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#20043;&#21069;&#30340;&#29468;&#24819;&#65292;&#24182;&#25552;&#20986;&#30340;&#26032;&#19979;&#30028;&#20855;&#26377;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#30340;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#20855;&#26377;&#39640;&#26031;&#21327;&#21464;&#37327;&#21452;&#23618;&#27169;&#22411;&#19979;&#65292;&#36807;&#21442;&#25968;&#21270;&#32447;&#24615;&#27169;&#22411;&#22312;&#22810;&#31867;&#20998;&#31867;&#20013;&#30340;&#28176;&#36827;&#27867;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#25968;&#25454;&#28857;&#25968;&#12289;&#29305;&#24449;&#21644;&#31867;&#21035;&#25968;&#37117;&#21516;&#26102;&#22686;&#38271;&#12290;&#25105;&#20204;&#23436;&#20840;&#35299;&#20915;&#20102;Subramanian&#31561;&#20154;&#22312;'22&#24180;&#25152;&#25552;&#20986;&#30340;&#29468;&#24819;&#65292;&#19982;&#39044;&#27979;&#30340;&#27867;&#21270;&#21306;&#38388;&#30456;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26032;&#30340;&#19979;&#30028;&#31867;&#20284;&#20110;&#20449;&#24687;&#35770;&#20013;&#30340;&#24378;&#23545;&#20598;&#23450;&#29702;&#65306;&#23427;&#20204;&#33021;&#22815;&#30830;&#31435;&#35823;&#20998;&#31867;&#29575;&#36880;&#28176;&#36235;&#36817;&#20110;0&#25110;1.&#25105;&#20204;&#32039;&#23494;&#30340;&#32467;&#26524;&#30340;&#19968;&#20010;&#20196;&#20154;&#24778;&#35766;&#30340;&#32467;&#26524;&#26159;&#65292;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#20998;&#31867;&#22120;&#22312;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#22238;&#24402;&#22120;&#26368;&#20248;&#30340;&#33539;&#22260;&#20869;&#65292;&#21487;&#20197;&#22312;&#28176;&#36827;&#19978;&#27425;&#20248;&#12290;&#25105;&#20204;&#20998;&#26512;&#30340;&#20851;&#38190;&#22312;&#20110;&#19968;&#31181;&#26032;&#30340;Hanson-Wright&#19981;&#31561;&#24335;&#21464;&#20307;&#65292;&#35813;&#21464;&#20307;&#22312;&#20855;&#26377;&#31232;&#30095;&#26631;&#31614;&#30340;&#22810;&#31867;&#38382;&#39064;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30456;&#21516;&#31867;&#22411;&#20998;&#26512;&#22312;&#20960;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the asymptotic generalization of an overparameterized linear model for multiclass classification under the Gaussian covariates bi-level model introduced in Subramanian et al.~'22, where the number of data points, features, and classes all grow together. We fully resolve the conjecture posed in Subramanian et al.~'22, matching the predicted regimes for generalization. Furthermore, our new lower bounds are akin to an information-theoretic strong converse: they establish that the misclassification rate goes to 0 or 1 asymptotically. One surprising consequence of our tight results is that the min-norm interpolating classifier can be asymptotically suboptimal relative to noninterpolating classifiers in the regime where the min-norm interpolating regressor is known to be optimal.  The key to our tight analysis is a new variant of the Hanson-Wright inequality which is broadly useful for multiclass problems with sparse labels. As an application, we show that the same type of analysis 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#20027;&#35201;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#21644;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2302.00058</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#32508;&#36848;(arXiv&#65306;2302.00058v2 [cs.LG]&#26356;&#26032;)
&lt;/p&gt;
&lt;p&gt;
Graph-based Time-Series Anomaly Detection: A Survey. (arXiv:2302.00058v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65292;&#20027;&#35201;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#30340;&#28508;&#21147;&#21644;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25216;&#26415;&#30340;&#21457;&#23637;&#65292;&#35768;&#22810;&#31995;&#32479;&#25345;&#32493;&#25910;&#38598;&#22823;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#65292;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#32593;&#32476;&#23433;&#20840;&#12289;&#36710;&#36742;&#32500;&#25252;&#21644;&#21307;&#30103;&#30417;&#27979;&#31561;&#39046;&#22495;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24050;&#25104;&#20026;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#20294;&#30001;&#20110;&#38656;&#35201;&#21516;&#26102;&#32771;&#34385;&#21464;&#37327;&#20869;&#37096;&#21644;&#21464;&#37327;&#38388;&#30340;&#20381;&#36182;&#24615;&#65292;&#36825;&#19968;&#20219;&#21153;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#38590;&#39064;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#26412;&#32508;&#36848;&#20840;&#38754;&#32780;&#26368;&#26032;&#22320;&#22238;&#39038;&#20102;&#22522;&#20110;&#22270;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;(G-TSAD)&#12290;&#39318;&#20808;&#25506;&#35752;&#20102;&#22270;&#34920;&#31034;&#23398;&#20064;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#28982;&#21518;&#22312;&#26102;&#38388;&#24207;&#21015;&#32972;&#26223;&#19979;&#22238;&#39038;&#20102;&#26368;&#20808;&#36827;&#30340;&#22270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#35752;&#35770;&#20102;&#36825;&#20123;&#25216;&#26415;&#22914;&#20309;&#24212;&#29992;&#20110;&#23454;&#38469;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the recent advances in technology, a wide range of systems continue to collect a large amount of data over time and thus generate time series. Time-Series Anomaly Detection (TSAD) is an important task in various time-series applications such as e-commerce, cybersecurity, vehicle maintenance, and healthcare monitoring. However, this task is very challenging as it requires considering both the intra-variable dependency and the inter-variable dependency, where a variable can be defined as an observation in time series data. Recent graph-based approaches have made impressive progress in tackling the challenges of this field. In this survey, we conduct a comprehensive and up-to-date review of Graph-based TSAD (G-TSAD). First, we explore the significant potential of graph representation learning for time-series data. Then, we review state-of-the-art graph anomaly detection techniques in the context of time series and discuss their strengths and drawbacks. Finally, we discuss the technic
&lt;/p&gt;</description></item></channel></rss>