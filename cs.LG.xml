<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#32467;&#26500;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#21704;&#23494;&#39039;&#20989;&#25968;&#65292;&#25299;&#23637;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#20540;&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.10070</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#20445;&#32467;&#26500;&#26680;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Structure-Preserving Kernel Method for Learning Hamiltonian Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10070
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#32467;&#26500;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#20013;&#24674;&#22797;&#21704;&#23494;&#39039;&#20989;&#25968;&#65292;&#25299;&#23637;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#20855;&#26377;&#20986;&#33394;&#30340;&#25968;&#20540;&#24615;&#33021;&#21644;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20445;&#32467;&#26500;&#30340;&#26680;&#23725;&#22238;&#24402;&#26041;&#27861;&#65292;&#20801;&#35768;&#20174;&#21253;&#21547;&#21704;&#23494;&#39039;&#21521;&#37327;&#22330;&#30340;&#22122;&#22768;&#35266;&#27979;&#25968;&#25454;&#38598;&#20013;&#24674;&#22797;&#28508;&#22312;&#30340;&#39640;&#32500;&#38750;&#32447;&#24615;&#21704;&#23494;&#39039;&#20989;&#25968;&#12290;&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20010;&#38381;&#24335;&#35299;&#65292;&#22312;&#36825;&#19968;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#25968;&#20540;&#24615;&#33021;&#65292;&#36229;&#36234;&#20102;&#25991;&#29486;&#20013;&#25552;&#20986;&#30340;&#20854;&#20182;&#25216;&#26415;&#12290;&#20174;&#26041;&#27861;&#35770;&#30340;&#35282;&#24230;&#30475;&#65292;&#35813;&#35770;&#25991;&#25193;&#23637;&#20102;&#26680;&#22238;&#24402;&#26041;&#27861;&#65292;&#35299;&#20915;&#38656;&#35201;&#21253;&#21547;&#26799;&#24230;&#32447;&#24615;&#20989;&#25968;&#30340;&#25439;&#22833;&#20989;&#25968;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#22320;&#65292;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#35777;&#26126;&#20102;&#24494;&#20998;&#20877;&#29616;&#23646;&#24615;&#21644;&#34920;&#31034;&#23450;&#29702;&#12290;&#20998;&#26512;&#20102;&#20445;&#32467;&#26500;&#26680;&#20272;&#35745;&#22120;&#21644;&#39640;&#26031;&#21518;&#39564;&#22343;&#20540;&#20272;&#35745;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36827;&#34892;&#20102;&#23436;&#25972;&#30340;&#35823;&#24046;&#20998;&#26512;&#65292;&#25552;&#20379;&#20351;&#29992;&#22266;&#23450;&#21644;&#33258;&#36866;&#24212;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20248;&#33391;&#24615;&#33021;&#24471;&#21040;&#20102;&#30830;&#35748;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10070v1 Announce Type: cross  Abstract: A structure-preserving kernel ridge regression method is presented that allows the recovery of potentially high-dimensional and nonlinear Hamiltonian functions out of datasets made of noisy observations of Hamiltonian vector fields. The method proposes a closed-form solution that yields excellent numerical performances that surpass other techniques proposed in the literature in this setup. From the methodological point of view, the paper extends kernel regression methods to problems in which loss functions involving linear functions of gradients are required and, in particular, a differential reproducing property and a Representer Theorem are proved in this context. The relation between the structure-preserving kernel estimator and the Gaussian posterior mean estimator is analyzed. A full error analysis is conducted that provides convergence rates using fixed and adaptive regularization parameters. The good performance of the proposed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#36827;&#34892;&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#30340;&#32039;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#35770;&#36848;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#29305;&#28857;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.02520</link><description>&lt;p&gt;
&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#30340;&#32467;&#26500;&#21270;&#30697;&#38453;&#23398;&#20064;&#19982;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Structured Matrix Learning under Arbitrary Entrywise Dependence and Estimation of Markov Transition Kernel. (arXiv:2401.02520v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;&#20219;&#24847;&#20803;&#32032;&#38388;&#20381;&#36182;&#19979;&#36827;&#34892;&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#35777;&#26126;&#20102;&#25552;&#20986;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#30340;&#32039;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#35770;&#36848;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#29305;&#28857;&#12290;&#26368;&#21518;&#65292;&#35770;&#25991;&#36824;&#23637;&#31034;&#20102;&#35813;&#26694;&#26550;&#22312;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#20272;&#35745;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#30697;&#38453;&#20272;&#35745;&#38382;&#39064;&#36890;&#24120;&#22312;&#24378;&#22122;&#22768;&#20381;&#36182;&#20551;&#35774;&#19979;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#25991;&#32771;&#34385;&#22122;&#22768;&#20302;&#31209;&#21152;&#31232;&#30095;&#30697;&#38453;&#24674;&#22797;&#30340;&#19968;&#33324;&#26694;&#26550;&#65292;&#20854;&#20013;&#22122;&#22768;&#30697;&#38453;&#21487;&#20197;&#26469;&#33258;&#20219;&#24847;&#20855;&#26377;&#20803;&#32032;&#38388;&#20219;&#24847;&#20381;&#36182;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#20851;&#30456;&#20301;&#32422;&#26463;&#30340;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#22120;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#23427;&#22312;&#21508;&#31181;&#22122;&#22768;&#20998;&#24067;&#19979;&#37117;&#26159;&#32039;&#33268;&#30340;&#65292;&#26082;&#28385;&#36275;&#30830;&#23450;&#24615;&#19979;&#30028;&#21448;&#21305;&#37197;&#26368;&#23567;&#21270;&#39118;&#38505;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32467;&#26524;&#65292;&#26029;&#35328;&#20004;&#20010;&#20219;&#24847;&#30340;&#20302;&#31209;&#26080;&#20851;&#30697;&#38453;&#20043;&#38388;&#30340;&#24046;&#24322;&#24517;&#39035;&#22312;&#20854;&#20803;&#32032;&#19978;&#25193;&#25955;&#33021;&#37327;&#65292;&#25442;&#21477;&#35805;&#35828;&#19981;&#33021;&#22826;&#31232;&#30095;&#65292;&#36825;&#25581;&#31034;&#20102;&#26080;&#20851;&#20302;&#31209;&#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#21487;&#33021;&#24341;&#36215;&#29420;&#31435;&#20852;&#36259;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#26694;&#26550;&#22312;&#20960;&#20010;&#37325;&#35201;&#30340;&#32479;&#35745;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#20272;&#35745;&#32467;&#26500;&#21270;&#39532;&#23572;&#21487;&#22827;&#36716;&#31227;&#26680;&#30340;&#38382;&#39064;&#20013;&#65292;&#37319;&#29992;&#20102;&#36825;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of structured matrix estimation has been studied mostly under strong noise dependence assumptions. This paper considers a general framework of noisy low-rank-plus-sparse matrix recovery, where the noise matrix may come from any joint distribution with arbitrary dependence across entries. We propose an incoherent-constrained least-square estimator and prove its tightness both in the sense of deterministic lower bound and matching minimax risks under various noise distributions. To attain this, we establish a novel result asserting that the difference between two arbitrary low-rank incoherent matrices must spread energy out across its entries, in other words cannot be too sparse, which sheds light on the structure of incoherent low-rank matrices and may be of independent interest. We then showcase the applications of our framework to several important statistical machine learning problems. In the problem of estimating a structured Markov transition kernel, the proposed method
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38543;&#26426;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.09454</link><description>&lt;p&gt;
&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#26465;&#20214;&#19979;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Asymptotically Efficient Online Learning for Censored Regression Models Under Non-I.I.D Data. (arXiv:2309.09454v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.09454
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#38543;&#26426;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#65292;&#24182;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#36798;&#21040;&#20102;&#26368;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28176;&#36817;&#39640;&#25928;&#30340;&#22312;&#32447;&#23398;&#20064;&#26041;&#27861;&#22312;&#38543;&#26426;&#34987;&#23457;&#26597;&#22238;&#24402;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#27169;&#22411;&#28041;&#21450;&#21040;&#23398;&#20064;&#21644;&#32479;&#35745;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#20173;&#32570;&#20047;&#20851;&#20110;&#23398;&#20064;&#31639;&#27861;&#25928;&#29575;&#30340;&#20840;&#38754;&#29702;&#35770;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20004;&#27493;&#22312;&#32447;&#31639;&#27861;&#65292;&#31532;&#19968;&#27493;&#19987;&#27880;&#20110;&#23454;&#29616;&#31639;&#27861;&#25910;&#25947;&#24615;&#65292;&#31532;&#20108;&#27493;&#29992;&#20110;&#25913;&#21892;&#20272;&#35745;&#24615;&#33021;&#12290;&#22312;&#25968;&#25454;&#30340;&#19968;&#33324;&#28608;&#21169;&#26465;&#20214;&#19979;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#38543;&#26426;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#26041;&#27861;&#21644;&#23545;&#38789;&#30340;&#26497;&#38480;&#29702;&#35770;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#26159;&#24378;&#19968;&#33268;&#30340;&#21644;&#28176;&#36817;&#27491;&#24577;&#30340;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#20272;&#35745;&#20540;&#30340;&#21327;&#26041;&#24046;&#22312;&#28176;&#36817;&#19978;&#21487;&#20197;&#36798;&#21040;&#20811;&#25289;&#32654;&#27931;&#30028;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#30340;&#24615;&#33021;&#22312;&#19968;&#33324;&#24773;&#20917;&#19979;&#26159;&#21487;&#20197;&#26399;&#26395;&#30340;&#26368;&#22909;&#30340;&#12290;&#19982;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#26159;&#19981;&#20381;&#36182;&#20256;&#32479;&#26041;&#27861;&#32780;&#24471;&#20986;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The asymptotically efficient online learning problem is investigated for stochastic censored regression models, which arise from various fields of learning and statistics but up to now still lacks comprehensive theoretical studies on the efficiency of the learning algorithms. For this, we propose a two-step online algorithm, where the first step focuses on achieving algorithm convergence, and the second step is dedicated to improving the estimation performance. Under a general excitation condition on the data, we show that our algorithm is strongly consistent and asymptotically normal by employing the stochastic Lyapunov function method and limit theories for martingales. Moreover, we show that the covariances of the estimates can achieve the Cramer-Rao (C-R) bound asymptotically, indicating that the performance of the proposed algorithm is the best possible that one can expect in general. Unlike most of the existing works, our results are obtained without resorting to the traditionall
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#24314;&#31435;&#20102;&#27969;&#24335;&#29615;&#22659;&#19979;&#30340;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21462;&#24471;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.09907</link><description>&lt;p&gt;
&#21033;&#29992;&#27969;&#24335;&#20998;&#26512;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#20013;&#36827;&#34892;&#22686;&#37327;&#24322;&#24120;&#26816;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Incremental Outlier Detection Modelling Using Streaming Analytics in Finance &amp; Health Care. (arXiv:2305.09907v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#24314;&#31435;&#20102;&#27969;&#24335;&#29615;&#22659;&#19979;&#30340;&#22686;&#37327;&#23398;&#20064;&#27169;&#22411;&#65292;&#22312;&#37329;&#34701;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#21462;&#24471;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26500;&#24314;&#20102;&#22312;&#32447;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#20351;&#29992;&#27969;&#29615;&#22659;&#19979;&#30340;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36827;&#34892;&#22686;&#37327;&#26500;&#24314;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#24212;&#24403;&#20351;&#29992;&#27969;&#24335;&#27169;&#22411;&#26469;&#22788;&#29702;&#27969;&#24335;&#25968;&#25454;&#30340;&#39640;&#24230;&#24517;&#35201;&#24615;&#12290;&#26412;&#39033;&#30446;&#30340;&#30446;&#26631;&#26159;&#30740;&#31350;&#21644;&#20998;&#26512;&#36866;&#29992;&#20110;&#29616;&#23454;&#29615;&#22659;&#30340;&#27969;&#24335;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#23454;&#29616;&#20102;&#21508;&#31181;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#65292;&#22914;One class&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;OC-SVM&#65289;&#12289;&#23396;&#31435;&#26862;&#26519;&#33258;&#36866;&#24212;&#28369;&#21160;&#31383;&#21475;&#26041;&#27861;&#65288;IForest ASD&#65289;&#12289;Exact Storm&#12289;&#22522;&#20110;&#35282;&#24230;&#30340;&#24322;&#24120;&#26816;&#27979;&#65288;ABOD&#65289;&#12289;&#23616;&#37096;&#24322;&#24120;&#22240;&#23376;&#65288;LOF&#65289;&#12289;KitNet&#12289;KNN ASD&#26041;&#27861;&#12290;&#24182;&#39564;&#35777;&#20102;&#19978;&#36848;&#26500;&#24314;&#27169;&#22411;&#22312;&#21508;&#31181;&#37329;&#34701;&#38382;&#39064;&#19978;&#30340;&#26377;&#25928;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#20363;&#22914;&#20449;&#29992;&#21345;&#27450;&#35784;&#26816;&#27979;&#12289;&#27969;&#22833;&#39044;&#27979;&#12289;&#20197;&#22826;&#22346;&#27450;&#35784;&#39044;&#27979;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#27169;&#22411;&#22312;&#20581;&#24247;&#39044;&#27979;&#38382;&#39064;&#19978;&#30340;&#34920;&#29616;&#65292;&#22914;&#24515;&#33039;&#20013;&#39118;&#39044;&#27979;&#12289;&#31958;&#23615;&#30149;&#39044;&#27979;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we had built the online model which are built incrementally by using online outlier detection algorithms under the streaming environment. We identified that there is highly necessity to have the streaming models to tackle the streaming data. The objective of this project is to study and analyze the importance of streaming models which is applicable in the real-world environment. In this work, we built various Outlier Detection (OD) algorithms viz., One class Support Vector Machine (OC-SVM), Isolation Forest Adaptive Sliding window approach (IForest ASD), Exact Storm, Angle based outlier detection (ABOD), Local outlier factor (LOF), KitNet, KNN ASD methods. The effectiveness and validity of the above-built models on various finance problems such as credit card fraud detection, churn prediction, ethereum fraud prediction. Further, we also analyzed the performance of the models on the health care prediction problems such as heart stroke prediction, diabetes prediction and h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20445;&#35777;&#20102;&#30697;&#38453;&#30340;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#20197;&#21450;&#38750;&#36127;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#36817;&#20284;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#36895;&#24230;&#21644;&#20248;&#20110;&#25991;&#29486;&#20013;&#31867;&#20284;&#26041;&#27861;&#30340;&#31232;&#30095;&#24615;&#12289;&#27491;&#20132;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.02672</link><description>&lt;p&gt;
&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;:&#26368;&#22823;&#29109;&#21407;&#21017;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Orthogonal Non-negative Matrix Factorization: a Maximum-Entropy-Principle Approach. (arXiv:2210.02672v2 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#21017;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20445;&#35777;&#20102;&#30697;&#38453;&#30340;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#20197;&#21450;&#38750;&#36127;&#24615;&#12290;&#35813;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#36817;&#20284;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#36895;&#24230;&#21644;&#20248;&#20110;&#25991;&#29486;&#20013;&#31867;&#20284;&#26041;&#27861;&#30340;&#31232;&#30095;&#24615;&#12289;&#27491;&#20132;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#27491;&#20132;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#65288;ONMF&#65289;&#38382;&#39064;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#38382;&#39064;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20004;&#20010;&#38750;&#36127;&#30697;&#38453;&#65288;&#29305;&#24449;&#30697;&#38453;&#21644;&#28151;&#21512;&#30697;&#38453;&#65289;&#30340;&#20056;&#31215;&#26469;&#36817;&#20284;&#36755;&#20837;&#25968;&#25454;&#30697;&#38453;&#65292;&#20854;&#20013;&#19968;&#20010;&#30697;&#38453;&#26159;&#27491;&#20132;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;ONMF&#35299;&#37322;&#20026;&#29305;&#23450;&#30340;&#35774;&#26045;&#23450;&#20301;&#38382;&#39064;&#65292;&#24182;&#38024;&#23545;ONMF&#38382;&#39064;&#37319;&#29992;&#22522;&#20110;&#26368;&#22823;&#29109;&#21407;&#21017;&#30340;FLP&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20445;&#35777;&#20102;&#29305;&#24449;&#30697;&#38453;&#25110;&#28151;&#21512;&#30697;&#38453;&#30340;&#27491;&#20132;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#20004;&#32773;&#30340;&#38750;&#36127;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#30340;&#8220;&#30495;&#23454;&#8221;&#28508;&#22312;&#29305;&#24449;&#25968;&#37327;&#30340;&#29305;&#24449;-&#36229;&#21442;&#25968;&#29992;&#20110;ONMF&#12290;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#20197;&#21450;&#26631;&#20934;&#30340;&#22522;&#22240;&#33455;&#29255;&#25968;&#32452;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#24433;&#21709;&#36817;&#20284;&#36136;&#37327;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#36739;&#22909;&#30340;&#31232;&#30095;&#24615;&#12289;&#27491;&#20132;&#24615;&#21644;&#24615;&#33021;&#36895;&#24230;&#65292;&#30456;&#23545;&#20110;&#25991;&#29486;&#20013;&#31867;&#20284;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a new methodology to solve the orthogonal nonnegative matrix factorization (ONMF) problem, where the objective is to approximate an input data matrix by a product of two nonnegative matrices, the features matrix and the mixing matrix, where one of them is orthogonal. We show how the ONMF can be interpreted as a specific facility-location problem (FLP), and adapt a maximum-entropy-principle based solution for FLP to the ONMF problem. The proposed approach guarantees orthogonality and sparsity of the features or the mixing matrix, while ensuring nonnegativity of both. Additionally, our methodology develops a quantitative characterization of ``true" number of underlying features - a hyperparameter required for the ONMF. An evaluation of the proposed method conducted on synthetic datasets, as well as a standard genetic microarray dataset indicates significantly better sparsity, orthogonality, and performance speed compared to similar methods in the literature, w
&lt;/p&gt;</description></item></channel></rss>