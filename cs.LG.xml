<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.03216</link><description>&lt;p&gt;
BGE M3-&#23884;&#20837;&#65306;&#36890;&#36807;&#33258;&#30693;&#35782;&#33976;&#39311;&#23454;&#29616;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03216
&lt;/p&gt;
&lt;p&gt;
BGE M3-&#23884;&#20837;&#26159;&#19968;&#31181;&#26032;&#30340;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#30340;&#25991;&#26412;&#23884;&#20837;&#27169;&#22411;&#65292;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#24182;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23427;&#33021;&#22815;&#21516;&#26102;&#25191;&#34892;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#24182;&#33021;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#12290;&#20854;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20102;&#19968;&#31181;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#21644;&#20248;&#21270;&#30340;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23884;&#20837;&#27169;&#22411;&#65292;&#31216;&#20026;M3-&#23884;&#20837;&#65292;&#20197;&#20854;&#22312;&#22810;&#35821;&#35328;&#12289;&#22810;&#21151;&#33021;&#21644;&#22810;&#31890;&#24230;&#26041;&#38754;&#30340;&#22810;&#26679;&#24615;&#32780;&#33879;&#31216;&#12290;&#23427;&#21487;&#20197;&#25903;&#25345;&#36229;&#36807;100&#31181;&#24037;&#20316;&#35821;&#35328;&#65292;&#22312;&#22810;&#35821;&#35328;&#21644;&#36328;&#35821;&#35328;&#26816;&#32034;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26032;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#25191;&#34892;&#23884;&#20837;&#27169;&#22411;&#30340;&#19977;&#31181;&#24120;&#35265;&#26816;&#32034;&#21151;&#33021;&#65306;&#23494;&#38598;&#26816;&#32034;&#12289;&#22810;&#21521;&#37327;&#26816;&#32034;&#21644;&#31232;&#30095;&#26816;&#32034;&#65292;&#20026;&#29616;&#23454;&#19990;&#30028;&#30340;IR&#24212;&#29992;&#25552;&#20379;&#20102;&#32479;&#19968;&#30340;&#27169;&#22411;&#22522;&#30784;&#12290;&#23427;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#31890;&#24230;&#30340;&#36755;&#20837;&#65292;&#20174;&#30701;&#21477;&#21040;&#38271;&#36798;8192&#20010;&#26631;&#35760;&#30340;&#25991;&#26723;&#12290;M3-&#23884;&#20837;&#30340;&#26377;&#25928;&#35757;&#32451;&#21253;&#25324;&#20197;&#19979;&#25216;&#26415;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#26469;&#33258;&#19981;&#21516;&#26816;&#32034;&#21151;&#33021;&#30340;&#30456;&#20851;&#24615;&#20998;&#25968;&#25972;&#21512;&#20026;&#25945;&#24072;&#20449;&#21495;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#20248;&#21270;&#20102;&#25209;&#22788;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a new embedding model, called M3-Embedding, which is distinguished for its versatility in Multi-Linguality, Multi-Functionality, and Multi-Granularity. It can support more than 100 working languages, leading to new state-of-the-art performances on multi-lingual and cross-lingual retrieval tasks. It can simultaneously perform the three common retrieval functionalities of embedding model: dense retrieval, multi-vector retrieval, and sparse retrieval, which provides a unified model foundation for real-world IR applications. It is able to process inputs of different granularities, spanning from short sentences to long documents of up to 8192 tokens. The effective training of M3-Embedding involves the following technical contributions. We propose a novel self-knowledge distillation approach, where the relevance scores from different retrieval functionalities can be integrated as the teacher signal to enhance the training quality. We also optimize the batching strat
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#25237;&#31080;&#30340;&#23454;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#34920;&#24615;&#25237;&#31080;&#35268;&#21017;&#36873;&#25321;&#33719;&#32988;&#32773;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20943;&#23569;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;</title><link>http://arxiv.org/abs/2304.09995</link><description>&lt;p&gt;
&#22522;&#20110;&#25237;&#31080;&#30340;&#23454;&#20363;&#31579;&#36873;&#26041;&#27861;&#65306;&#20351;&#29992;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#25237;&#31080;
&lt;/p&gt;
&lt;p&gt;
Data as voters: instance selection using approval-based multi-winner voting. (arXiv:2304.09995v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09995
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#25237;&#31080;&#30340;&#23454;&#20363;&#36873;&#25321;&#26041;&#27861;&#65292;&#36890;&#36807;&#20195;&#34920;&#24615;&#25237;&#31080;&#35268;&#21017;&#36873;&#25321;&#33719;&#32988;&#32773;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20943;&#23569;&#35757;&#32451;&#38598;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;&#25110;&#25968;&#25454;&#25366;&#25496;&#65289;&#20013;&#30340;&#23454;&#20363;&#31579;&#36873;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110;&#22522;&#20110;&#36190;&#25104;&#31080;&#30340;&#22810;&#33719;&#32988;&#32773;&#36873;&#20030;&#20013;&#20195;&#34920;&#24615;&#34920;&#24449;&#30340;&#32467;&#26524;&#12290;&#22312;&#25105;&#20204;&#30340;&#27169;&#22411;&#20013;&#65292;&#23454;&#20363;&#25198;&#28436;&#36873;&#27665;&#21644;&#20505;&#36873;&#20154;&#30340;&#21452;&#37325;&#35282;&#33394;&#12290;&#27599;&#20010;&#35757;&#32451;&#38598;&#20013;&#30340;&#23454;&#20363;&#65288;&#20316;&#20026;&#36873;&#27665;&#65289;&#36190;&#25104;&#20854;&#26412;&#22320;&#38598;&#21512;&#20013;&#30340;&#23454;&#20363;&#65288;&#25198;&#28436;&#20505;&#36873;&#20154;&#30340;&#35282;&#33394;&#65289;&#65288;&#38500;&#33258;&#36523;&#20197;&#22806;&#30340;&#23454;&#20363;&#65289;&#65292;&#36825;&#20010;&#27010;&#24565;&#22312;&#25991;&#29486;&#20013;&#24050;&#32463;&#23384;&#22312;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#20195;&#34920;&#24615;&#25237;&#31080;&#35268;&#21017;&#36873;&#25321;&#36873;&#20030;&#33719;&#32988;&#32773;&#65292;&#24182;&#20316;&#20026;&#20943;&#23569;&#35757;&#32451;&#38598;&#20013;&#30340;&#25968;&#25454;&#23454;&#20363;&#20445;&#30041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel approach to the instance selection problem in machine learning (or data mining). Our approach is based on recent results on (proportional) representation in approval-based multi-winner elections. In our model, instances play a double role as voters and candidates. Each instance in the training set (acting as a voter) approves of the instances (playing the role of candidates) belonging to its local set (except itself), a concept already existing in the literature. We then select the election winners using a representative voting rule, and such winners are the data instances kept in the reduced training set.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20998;&#35010;&#24182;&#34892;&#65292;&#24212;&#29992;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#19978;&#65292;&#33021;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#24182;&#34892;&#26041;&#27861;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#21516;&#26102;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2303.13775</link><description>&lt;p&gt;
GSplit: &#36890;&#36807;&#20998;&#35010;&#24182;&#34892;&#23454;&#29616;&#22823;&#35268;&#27169;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#30340;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
GSplit: Scaling Graph Neural Network Training on Large Graphs via Split-Parallelism. (arXiv:2303.13775v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#26041;&#27861;&#65292;&#21363;&#20998;&#35010;&#24182;&#34892;&#65292;&#24212;&#29992;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#19978;&#65292;&#33021;&#26377;&#25928;&#32531;&#35299;&#25968;&#25454;&#24182;&#34892;&#26041;&#27861;&#30340;&#24615;&#33021;&#29942;&#39048;&#65292;&#21516;&#26102;&#22312;&#22823;&#35268;&#27169;&#22270;&#19978;&#30340;&#24615;&#33021;&#34920;&#29616;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#34892;&#19994;&#12289;&#31185;&#23398;&#21644;&#24037;&#31243;&#39046;&#22495;&#65288;&#22914;&#25512;&#33616;&#31995;&#32479;&#12289;&#31038;&#20132;&#22270;&#20998;&#26512;&#12289;&#30693;&#35782;&#24211;&#12289;&#26448;&#26009;&#31185;&#23398;&#21644;&#29983;&#29289;&#23398;&#65289;&#20013;&#65292;&#25317;&#26377;&#25968;&#21313;&#20159;&#20010;&#36793;&#30340;&#22823;&#35268;&#27169;&#22270;&#24418;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#30001;&#20110;&#22312;&#21508;&#31181;&#22270;&#20998;&#26512;&#20219;&#21153;&#20013;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#37319;&#29992;&#26469;&#23398;&#20064;&#36825;&#20123;&#22270;&#24418;&#12290;&#22312;&#22823;&#22411;&#22270;&#24418;&#19978;&#35757;&#32451;&#36890;&#24120;&#37319;&#29992;&#23567;&#25209;&#37327;&#35757;&#32451;&#65292;&#24182;&#19988;&#25968;&#25454;&#24182;&#34892;&#26159;&#23558;&#23567;&#25209;&#37327;&#35757;&#32451;&#25193;&#23637;&#21040;&#22810;&#20010; GPU &#30340;&#26631;&#20934;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;GNN &#35757;&#32451;&#31995;&#32479;&#30340;&#20960;&#20010;&#22522;&#26412;&#24615;&#33021;&#29942;&#39048;&#19982;&#25968;&#25454;&#24182;&#34892;&#26041;&#27861;&#30340;&#22266;&#26377;&#38480;&#21046;&#26377;&#20851;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24182;&#34892;&#23567;&#25209;&#37327;&#35757;&#32451;&#33539;&#24335;- &#20998;&#35010;&#24182;&#34892;&#65292;&#24182;&#23558;&#20854;&#23454;&#29616;&#22312;&#19968;&#20010;&#21517;&#20026;gsplit&#30340;&#26032;&#31995;&#32479;&#20013;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;gsplit &#30340;&#24615;&#33021;&#20248;&#20110;DGL&#12289;Quiver&#21644;PaGraph&#31561;&#29616;&#26377;&#30340;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale graphs with billions of edges are ubiquitous in many industries, science, and engineering fields such as recommendation systems, social graph analysis, knowledge base, material science, and biology. Graph neural networks (GNN), an emerging class of machine learning models, are increasingly adopted to learn on these graphs due to their superior performance in various graph analytics tasks. Mini-batch training is commonly adopted to train on large graphs, and data parallelism is the standard approach to scale mini-batch training to multiple GPUs. In this paper, we argue that several fundamental performance bottlenecks of GNN training systems have to do with inherent limitations of the data parallel approach. We then propose split parallelism, a novel parallel mini-batch training paradigm. We implement split parallelism in a novel system called gsplit and show that it outperforms state-of-the-art systems such as DGL, Quiver, and PaGraph.
&lt;/p&gt;</description></item></channel></rss>