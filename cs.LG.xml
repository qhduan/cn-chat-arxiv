<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#36731;&#37327;&#32423;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#65288;LVFL&#65289;&#30340;&#27010;&#24565;&#65292;&#38024;&#23545;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#37319;&#29992;&#20998;&#31163;&#30340;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;</title><link>https://arxiv.org/abs/2404.00466</link><description>&lt;p&gt;
&#35745;&#31639;&#21644;&#36890;&#20449;&#39640;&#25928;&#30340;&#36731;&#37327;&#32423;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Computation and Communication Efficient Lightweighting Vertical Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00466
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#36731;&#37327;&#32423;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#65288;LVFL&#65289;&#30340;&#27010;&#24565;&#65292;&#38024;&#23545;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#37319;&#29992;&#20998;&#31163;&#30340;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#24314;&#31435;&#20102;&#25910;&#25947;&#30028;&#38480;&#65292;&#24182;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#25506;&#32034;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#24050;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#21644;&#20851;&#38190;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21162;&#21147;&#37117;&#38598;&#20013;&#22312;&#25552;&#39640;&#36825;&#20123;&#25928;&#29575;&#65292;&#20294;&#30001;&#20110;&#22402;&#30452;FL&#30340;&#19981;&#21516;&#36807;&#31243;&#21644;&#27169;&#22411;&#32467;&#26500;&#65292;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#22522;&#20110;&#27700;&#24179;FL&#30340;&#25216;&#26415;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#36731;&#37327;&#32423;&#32437;&#21521;&#32852;&#37030;&#23398;&#20064;&#65288;LVFL&#65289;&#30340;&#27010;&#24565;&#65292;&#26088;&#22312;&#25552;&#39640;&#35745;&#31639;&#21644;&#36890;&#20449;&#25928;&#29575;&#12290;&#36825;&#31181;&#26041;&#27861;&#28041;&#21450;&#38024;&#23545;&#29305;&#24449;&#27169;&#22411;&#30340;&#21333;&#29420;&#36731;&#37327;&#21270;&#31574;&#30053;&#65292;&#20197;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;&#65292;&#24182;&#38024;&#23545;&#29305;&#24449;&#23884;&#20837;&#36827;&#34892;&#36731;&#37327;&#21270;&#65292;&#20197;&#22686;&#24378;&#36890;&#20449;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;LVFL&#31639;&#27861;&#24314;&#31435;&#20102;&#25910;&#25947;&#30028;&#38480;&#65292;&#32771;&#34385;&#20102;&#36890;&#20449;&#21644;&#35745;&#31639;&#36731;&#37327;&#21270;&#27604;&#29575;&#12290;&#25105;&#20204;&#22312;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#23545;&#35813;&#31639;&#27861;&#36827;&#34892;&#35780;&#20272;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LVFL&#26174;&#33879;&#20943;&#36731;&#20102;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00466v1 Announce Type: new  Abstract: The exploration of computational and communication efficiency within Federated Learning (FL) has emerged as a prominent and crucial field of study. While most existing efforts to enhance these efficiencies have focused on Horizontal FL, the distinct processes and model structures of Vertical FL preclude the direct application of Horizontal FL-based techniques. In response, we introduce the concept of Lightweight Vertical Federated Learning (LVFL), targeting both computational and communication efficiencies. This approach involves separate lightweighting strategies for the feature model, to improve computational efficiency, and for feature embedding, to enhance communication efficiency. Moreover, we establish a convergence bound for our LVFL algorithm, which accounts for both communication and computational lightweighting ratios. Our evaluation of the algorithm on a image classification dataset reveals that LVFL significantly alleviates c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#32467;&#21512;transformer&#35780;&#20272;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36741;&#21161;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13843</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35270;&#35273;Transformer&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Vision Transformers for Thyroid Carcinoma Diagnosis: A review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#32467;&#21512;transformer&#35780;&#20272;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36741;&#21161;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21457;&#23637;&#26234;&#33021;&#35786;&#26029;&#31995;&#32479;&#20197;&#24110;&#21161;&#21307;&#23398;&#19987;&#23478;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#27835;&#30103;&#19981;&#21487;&#27835;&#24840;&#30142;&#30149;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35782;&#21035;&#30002;&#29366;&#33146;&#30284;&#65288;TC&#65289;&#30340;&#25361;&#25112;&#26041;&#38754;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#32467;&#21512;transformer&#35780;&#20272;TC&#39044;&#21518;&#65292;&#24182;&#30830;&#23450;&#20010;&#20307;&#30340;&#24694;&#24615;&#39118;&#38505;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#24635;&#32467;&#20102;&#21508;&#31181;&#20851;&#20110;&#20197;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#37319;&#29992;transformer&#36827;&#34892;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#30340;&#26041;&#27861;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;AI&#31639;&#27861;&#12289;&#26694;&#26550;&#30446;&#26631;&#21644;&#20351;&#29992;&#30340;&#35745;&#31639;&#29615;&#22659;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#20854;&#29305;&#24449;&#23457;&#26597;&#21644;&#23545;&#27604;&#20102;&#21487;&#29992;&#30340;TC&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;AI&#24037;&#20855;&#22312;&#36890;&#36807;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#25110;&#28151;&#21512;&#26041;&#24335;&#21327;&#21161;&#35786;&#26029;&#21644;&#27835;&#30103;TC&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13843v1 Announce Type: cross  Abstract: The growing interest in developing smart diagnostic systems to help medical experts process extensive data for treating incurable diseases has been notable. In particular, the challenge of identifying thyroid cancer (TC) has seen progress with the use of machine learning (ML) and big data analysis, incorporating transformers to evaluate TC prognosis and determine the risk of malignancy in individuals. This review article presents a summary of various studies on AIbased approaches, especially those employing transformers, for diagnosing TC. It introduces a new categorization system for these methods based on artifcial intelligence (AI) algorithms, the goals of the framework, and the computing environments used. Additionally, it scrutinizes and contrasts the available TC datasets by their features. The paper highlights the importance of AI instruments in aiding the diagnosis and treatment of TC through supervised, unsupervised, or mixed 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#65288;NDAEs&#65289;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#31995;&#32479;&#29702;&#35770;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#34920;&#26126;&#20102;&#20854;&#22312;&#22122;&#22768;&#21644;&#22806;&#37096;&#24178;&#25200;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12938</link><description>&lt;p&gt;
&#31070;&#32463;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Neural Differential Algebraic Equations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12938
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#65288;NDAEs&#65289;&#29992;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#31995;&#32479;&#29702;&#35770;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#36890;&#36807;&#20855;&#20307;&#31034;&#20363;&#34920;&#26126;&#20102;&#20854;&#22312;&#22122;&#22768;&#21644;&#22806;&#37096;&#24178;&#25200;&#19979;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#65288;DAEs&#65289;&#25551;&#36848;&#20102;&#31526;&#21512;&#24494;&#20998;&#21644;&#20195;&#25968;&#32422;&#26463;&#30340;&#31995;&#32479;&#30340;&#26102;&#38388;&#28436;&#21270;&#12290;&#29305;&#21035;&#24863;&#20852;&#36259;&#30340;&#26159;&#21253;&#21547;&#20854;&#32452;&#20214;&#20043;&#38388;&#38544;&#24615;&#20851;&#31995;&#65288;&#22914;&#23432;&#24658;&#20851;&#31995;&#65289;&#30340;&#31995;&#32479;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#36866;&#29992;&#20110;&#22522;&#20110;&#25968;&#25454;&#30340;DAE&#24314;&#27169;&#30340;&#31070;&#32463;&#24494;&#20998;&#20195;&#25968;&#26041;&#31243;&#65288;NDAEs&#65289;&#12290;&#36825;&#19968;&#26041;&#27861;&#24314;&#31435;&#22312;&#36890;&#29992;&#24494;&#20998;&#26041;&#31243;&#30340;&#27010;&#24565;&#20043;&#19978;&#65307;&#21363;&#65292;&#26500;&#24314;&#20026;&#21463;&#29305;&#23450;&#31185;&#23398;&#39046;&#22495;&#29702;&#35770;&#25903;&#25345;&#30340;&#19968;&#32452;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;NDAEs&#25277;&#35937;&#36866;&#29992;&#20110;&#30456;&#20851;&#31995;&#32479;&#29702;&#35770;&#25968;&#25454;&#39537;&#21160;&#30340;&#24314;&#27169;&#20219;&#21153;&#12290;&#25152;&#31034;&#31034;&#20363;&#21253;&#25324;&#65288;i&#65289;&#27833;&#31665;&#27969;&#24418;&#21160;&#24577;&#30340;&#36870;&#38382;&#39064;&#21644;&#65288;ii&#65289;&#27893;&#12289;&#27833;&#31665;&#21644;&#31649;&#36947;&#32593;&#32476;&#30340;&#24046;&#24322;&#24314;&#27169;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#23545;&#22122;&#22768;&#21644;&#22806;&#37096;&#24178;&#25200;&#30340;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12938v1 Announce Type: new  Abstract: Differential-Algebraic Equations (DAEs) describe the temporal evolution of systems that obey both differential and algebraic constraints. Of particular interest are systems that contain implicit relationships between their components, such as conservation relationships. Here, we present Neural Differential-Algebraic Equations (NDAEs) suitable for data-driven modeling of DAEs. This methodology is built upon the concept of the Universal Differential Equation; that is, a model constructed as a system of Neural Ordinary Differential Equations informed by theory from particular science domains. In this work, we show that the proposed NDAEs abstraction is suitable for relevant system-theoretic data-driven modeling tasks. Presented examples include (i) the inverse problem of tank-manifold dynamics and (ii) discrepancy modeling of a network of pumps, tanks, and pipes. Our experiments demonstrate the proposed method's robustness to noise and extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.14701</link><description>&lt;p&gt;
COMPASS&#65306;&#21033;&#29992;&#35821;&#35328;&#24314;&#27169;&#23545;&#24739;&#32773;-&#27835;&#30103;&#24072;&#32852;&#30431;&#31574;&#30053;&#36827;&#34892;&#35745;&#31639;&#26144;&#23556;
&lt;/p&gt;
&lt;p&gt;
COMPASS: Computational Mapping of Patient-Therapist Alliance Strategies with Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14701
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;COMPASS&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#65292;&#30452;&#25509;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#65292;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#26159;&#39044;&#27979;&#24515;&#29702;&#27835;&#30103;&#27835;&#30103;&#25104;&#21151;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#20256;&#32479;&#19978;&#65292;&#24037;&#20316;&#32852;&#30431;&#35780;&#20272;&#20381;&#36182;&#20110;&#27835;&#30103;&#24072;&#21644;&#24739;&#32773;&#22635;&#20889;&#30340;&#38382;&#21367;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;COMPASS&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21487;&#30452;&#25509;&#20174;&#24515;&#29702;&#27835;&#30103;&#35838;&#31243;&#20013;&#20351;&#29992;&#30340;&#33258;&#28982;&#35821;&#35328;&#20013;&#25512;&#26029;&#27835;&#30103;&#24037;&#20316;&#32852;&#30431;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24515;&#29702;&#27835;&#30103;&#20250;&#35805;&#30340;&#36716;&#24405;&#65292;&#24182;&#23558;&#20854;&#19982;&#24037;&#20316;&#32852;&#30431;&#28165;&#21333;&#20013;&#38472;&#36848;&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#36827;&#34892;&#27604;&#36739;&#12290;&#36890;&#36807;&#20998;&#26512;&#28085;&#30422;&#22810;&#31181;&#31934;&#31070;&#30142;&#30149;&#30340;&#36229;&#36807;950&#20010;&#20250;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26174;&#24494;&#22320;&#26144;&#23556;&#24739;&#32773;-&#27835;&#30103;&#24072;&#23545;&#40784;&#36712;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20026;&#20020;&#24202;&#31934;&#31070;&#30149;&#23398;&#25552;&#20379;&#35299;&#37322;&#24615;&#65292;&#24182;&#22312;&#35782;&#21035;&#19982;&#27491;&#22312;&#27835;&#30103;&#30340;&#30142;&#30149;&#30456;&#20851;&#30340;&#26032;&#20852;&#27169;&#24335;&#26041;&#38754;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;&#31070;&#32463;&#20027;&#39064;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14701v1 Announce Type: cross  Abstract: The therapeutic working alliance is a critical factor in predicting the success of psychotherapy treatment. Traditionally, working alliance assessment relies on questionnaires completed by both therapists and patients. In this paper, we present COMPASS, a novel framework to directly infer the therapeutic working alliance from the natural language used in psychotherapy sessions. Our approach utilizes advanced large language models to analyze transcripts of psychotherapy sessions and compare them with distributed representations of statements in the working alliance inventory. Analyzing a dataset of over 950 sessions covering diverse psychiatric conditions, we demonstrate the effectiveness of our method in microscopically mapping patient-therapist alignment trajectories and providing interpretability for clinical psychiatry and in identifying emerging patterns related to the condition being treated. By employing various neural topic mode
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20174;&#25968;&#25454;&#12289;&#26234;&#33021;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#23454;&#29616;6G&#21407;&#29983;AI&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;6G&#21407;&#29983;AI&#26694;&#26550;&#65292;&#21253;&#25324;&#33258;&#23450;&#20041;&#26041;&#27861;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26032;&#30340;&#20113;&#36793;&#32536;&#21327;&#21516;&#21512;&#20316;&#33539;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.17471</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;6G&#21407;&#29983;AI&#26694;&#26550;&#19982;&#20113;&#36793;&#32536;&#21327;&#21516;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
Foundation Model Based Native AI Framework in 6G with Cloud-Edge-End Collaboration. (arXiv:2310.17471v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17471
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20998;&#26512;&#20102;&#20174;&#25968;&#25454;&#12289;&#26234;&#33021;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#26469;&#23454;&#29616;6G&#21407;&#29983;AI&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;6G&#21407;&#29983;AI&#26694;&#26550;&#65292;&#21253;&#25324;&#33258;&#23450;&#20041;&#26041;&#27861;&#21644;&#20219;&#21153;&#23548;&#21521;&#30340;AI&#24037;&#20855;&#21253;&#65292;&#20197;&#21450;&#26032;&#30340;&#20113;&#36793;&#32536;&#21327;&#21516;&#21512;&#20316;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26410;&#26469;&#30340;&#26080;&#32447;&#36890;&#20449;&#32593;&#32476;&#26377;&#26395;&#36229;&#36234;&#20197;&#25968;&#25454;&#20026;&#20013;&#24515;&#12289;&#20197;&#35774;&#22791;&#20026;&#23548;&#21521;&#30340;&#36830;&#25509;&#26041;&#24335;&#65292;&#25552;&#20379;&#22522;&#20110;&#20219;&#21153;&#23548;&#21521;&#36830;&#25509;&#30340;&#26234;&#33021;&#27785;&#28024;&#24335;&#20307;&#39564;&#65292;&#29305;&#21035;&#26159;&#22312;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;&#65288;PFM&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;6G&#21407;&#29983;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#21457;&#23637;&#24895;&#26223;&#19979;&#12290;&#22240;&#27492;&#65292;&#22312;6G&#20013;&#65292;&#37325;&#26032;&#23450;&#20041;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#27169;&#24335;&#65292;&#26500;&#24314;&#21407;&#29983;&#26234;&#33021;&#24211;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#25454;&#12289;&#26234;&#33021;&#21644;&#32593;&#32476;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#23454;&#29616;6G&#21407;&#29983;AI&#30340;&#25361;&#25112;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#25552;&#20986;&#20102;&#19968;&#20010;6G&#21407;&#29983;AI&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#24847;&#22270;&#24863;&#30693;PFM&#30340;&#23450;&#21046;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#38754;&#21521;&#20219;&#21153;&#30340;AI&#24037;&#20855;&#21253;&#30340;&#26500;&#24314;&#65292;&#24182;&#27010;&#36848;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#20113;&#36793;&#32536;&#21327;&#21516;&#21512;&#20316;&#33539;&#24335;&#12290;&#20316;&#20026;&#19968;&#20010;&#23454;&#38469;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#23558;&#35813;&#26694;&#26550;&#24212;&#29992;&#20110;&#32534;&#25490;&#65292;&#23454;&#29616;&#20102;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#26368;&#22823;&#36895;&#29575;&#20043;&#21644;&#12290;
&lt;/p&gt;
&lt;p&gt;
Future wireless communication networks are in a position to move beyond data-centric, device-oriented connectivity and offer intelligent, immersive experiences based on task-oriented connections, especially in the context of the thriving development of pre-trained foundation models (PFM) and the evolving vision of 6G native artificial intelligence (AI). Therefore, redefining modes of collaboration between devices and servers and constructing native intelligence libraries become critically important in 6G. In this paper, we analyze the challenges of achieving 6G native AI from the perspectives of data, intelligence, and networks. Then, we propose a 6G native AI framework based on foundation models, provide a customization approach for intent-aware PFM, present a construction of a task-oriented AI toolkit, and outline a novel cloud-edge-end collaboration paradigm. As a practical use case, we apply this framework for orchestration, achieving the maximum sum rate within a wireless communic
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.13019</link><description>&lt;p&gt;
&#36890;&#36807;DeepFool&#31639;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#26377;&#38024;&#23545;&#24615;&#30340;&#31867;&#21035;&#25805;&#32437;&#30340;&#23545;&#25239;&#25915;&#20987;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#29256;DeepFool&#31639;&#27861;&#65292;&#21517;&#20026;Targeted DeepFool&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#65292;&#24182;&#24341;&#20837;&#20102;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#25552;&#39640;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23545;&#25239;&#25915;&#20987;&#30340;&#26131;&#21463;&#25915;&#20987;&#24615;&#24341;&#36215;&#20102;&#20005;&#37325;&#20851;&#27880;&#12290;&#20102;&#35299;&#36825;&#20123;&#26131;&#21463;&#25915;&#20987;&#24615;&#24182;&#24320;&#21457;&#26377;&#25928;&#30340;&#38450;&#24481;&#26426;&#21046;&#33267;&#20851;&#37325;&#35201;&#12290;DeepFool&#26159;Moosavi-Dezfooli&#31561;&#20154;&#65288;2016&#24180;&#65289;&#25552;&#20986;&#30340;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#23558;&#36755;&#20837;&#22270;&#20687;&#38169;&#35823;&#20998;&#31867;&#30340;&#26368;&#23567;&#25200;&#21160;&#12290;&#28982;&#32780;&#65292;DeepFool&#32570;&#20047;&#26377;&#38024;&#23545;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#20854;&#22312;&#29305;&#23450;&#25915;&#20987;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#36739;&#20302;&#12290;&#27492;&#22806;&#65292;&#22312;&#20808;&#21069;&#30340;&#30456;&#20851;&#24037;&#20316;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#20027;&#35201;&#20851;&#27880;&#30340;&#26159;&#25104;&#21151;&#29575;&#65292;&#32780;&#27809;&#26377;&#32771;&#34385;&#22270;&#20687;&#34987;&#25197;&#26354;&#30340;&#31243;&#24230;&#12289;&#22270;&#20687;&#36136;&#37327;&#30340;&#23436;&#25972;&#24615;&#20197;&#21450;&#38169;&#35823;&#20998;&#31867;&#30340;&#32622;&#20449;&#24230;&#27700;&#24179;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Targeted DeepFool&#65292;&#36825;&#26159;DeepFool&#30340;&#22686;&#24378;&#29256;&#65292;&#21487;&#20197;&#38024;&#23545;&#29305;&#23450;&#31867;&#21035;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26368;&#23567;&#32622;&#20449;&#24230;&#20998;&#25968;&#35201;&#27714;&#36229;&#21442;&#25968;&#26469;&#22686;&#24378;&#28789;&#27963;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have significantly advanced various domains, but their vulnerability to adversarial attacks poses serious concerns. Understanding these vulnerabilities and developing effective defense mechanisms is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016), finds minimal perturbations to misclassify input images. However, DeepFool lacks a targeted approach, making it less effective in specific attack scenarios. Also, in previous related works, researchers primarily focus on success, not considering how much an image is getting distorted; the integrity of the image quality, and the confidence level to misclassifying. So, in this paper, we propose Targeted DeepFool, an augmented version of DeepFool that allows targeting specific classes for misclassification. We also introduce a minimum confidence score requirement hyperparameter to enhance flexibility. Our experiments demonstrate the effectiveness and efficiency of the proposed method across 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;&#23558;&#26435;&#37325;&#25353;&#36755;&#20837;&#36890;&#36947;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#65292;&#21487;&#20197;&#35299;&#20915;&#28608;&#27963;&#24322;&#24120;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#20351;&#24471;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.15531</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight Quantization of Large Language Models. (arXiv:2309.15531v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15531
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37325;&#26032;&#24605;&#32771;&#39057;&#36947;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#38548;&#31163;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#12290;&#36890;&#36807;&#23558;&#26435;&#37325;&#25353;&#36755;&#20837;&#36890;&#36947;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#65292;&#21487;&#20197;&#35299;&#20915;&#28608;&#27963;&#24322;&#24120;&#20540;&#30340;&#38382;&#39064;&#65292;&#24182;&#25104;&#21151;&#22320;&#20351;&#24471;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#36817;&#26399;&#23637;&#31034;&#20102;&#26174;&#33879;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22312;&#26377;&#25928;&#22320;&#20026;LLMs&#25552;&#20379;&#26381;&#21153;&#26041;&#38754;&#19968;&#30452;&#26159;&#20010;&#25361;&#25112;&#65292;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20854;&#22823;&#20869;&#23384;&#29942;&#39048;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#25209;&#37327;&#25512;&#29702;&#35774;&#32622;&#65288;&#22914;&#31227;&#21160;&#35774;&#22791;&#65289;&#20013;&#12290;&#20165;&#23545;&#26435;&#37325;&#36827;&#34892;&#37327;&#21270;&#21487;&#33021;&#26159;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#23384;&#22312;&#22823;&#24133;&#24230;&#28608;&#27963;&#24322;&#24120;&#20540;&#65292;&#20302;&#20110;4&#20301;&#30340;&#37327;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21487;&#21462;&#30340;&#24322;&#24120;&#25928;&#26524;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#27599;&#20010;&#36755;&#20837;&#36890;&#36947;&#65288;IC&#65289;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#30340;per-IC&#37327;&#21270;&#26041;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#30340;&#27599;&#20010;&#36755;&#20986;&#36890;&#36947;&#65288;OC&#65289;&#20869;&#36827;&#34892;&#37327;&#21270;&#20998;&#32452;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#21160;&#26426;&#26159;&#35266;&#23519;&#21040;&#28608;&#27963;&#24322;&#24120;&#20540;&#24433;&#21709;&#26435;&#37325;&#30697;&#38453;&#30340;&#36755;&#20837;&#32500;&#24230;&#65292;&#22240;&#27492;&#22312;IC&#26041;&#21521;&#19978;&#23545;&#26435;&#37325;&#36827;&#34892;&#31867;&#20284;&#20998;&#32452;&#21487;&#20197;&#23558;&#24322;&#24120;&#20540;&#38548;&#31163;&#21040;&#19968;&#20010;&#20998;&#32452;&#20869;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#28608;&#27963;&#30340;&#24322;&#24120;&#20540;&#24182;&#19981;&#20915;&#23450;&#37327;&#21270;&#30340;&#38590;&#24230;&#65292;&#20854;&#22266;&#26377;&#30340;&#26435;&#37325;&#25935;&#24863;&#24615;&#20063;&#23384;&#22312;&#12290;&#36890;&#36807;per-IC&#37327;&#21270;&#20316;&#20026;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20302;&#20301;&#26435;&#37325;&#37327;&#21270;&#20013;&#30340;&#24322;&#24120;&#20540;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have recently demonstrated a remarkable success across various tasks. However, efficiently serving LLMs has been a challenge due to its large memory bottleneck, specifically in small batch inference settings (e.g. mobile devices). Weight-only quantization can be a promising approach, but sub-4 bit quantization remains a challenge due to large-magnitude activation outliers. To mitigate the undesirable outlier effect, we first propose per-IC quantization, a simple yet effective method that creates quantization groups within each input channel (IC) rather than the conventional per-output channel (OC). Our method is motivated by the observation that activation outliers affect the input dimension of the weight matrix, so similarly grouping the weights in the IC direction can isolate outliers to be within a group. We also find that activation outliers do not dictate quantization difficulty, and inherent weight sensitivities also exist. With per-IC quantization as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.02694</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24230;&#37327;&#26041;&#27861;&#65306;&#19968;&#39033;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Loss Functions and Metrics in Deep Learning. A Review. (arXiv:2307.02694v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02694
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#65292;&#26088;&#22312;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#19968;&#20010;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#26159;&#36873;&#25321;&#29992;&#20110;&#35757;&#32451;&#21644;&#35780;&#20272;&#27169;&#22411;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#24230;&#37327;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#28145;&#24230;&#23398;&#20064;&#20013;&#26368;&#24120;&#35265;&#30340;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#27979;&#37327;&#26041;&#27861;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#27599;&#31181;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#23427;&#20204;&#22312;&#21508;&#31181;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#30340;&#35780;&#35770;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;&#26368;&#24120;&#35265;&#30340;&#28145;&#24230;&#23398;&#20064;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#25439;&#22833;&#20989;&#25968;&#21644;&#24615;&#33021;&#25351;&#26631;&#65292;&#24182;&#24110;&#21161;&#20174;&#19994;&#32773;&#36873;&#25321;&#26368;&#36866;&#21512;&#20854;&#29305;&#23450;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the essential components of deep learning is the choice of the loss function and performance metrics used to train and evaluate models. This paper reviews the most prevalent loss functions and performance measurements in deep learning. We examine the benefits and limits of each technique and illustrate their application to various deep-learning problems. Our review aims to give a comprehensive picture of the different loss functions and performance indicators used in the most common deep learning tasks and help practitioners choose the best method for their specific task.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#35780;&#20272;AI&#31995;&#32479;&#26102;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#32858;&#21512;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.02191</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#30340;&#22522;&#20934;&#20107;&#23454;&#19979;&#35780;&#20272;AI&#31995;&#32479;&#65306;&#30382;&#32932;&#30149;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Evaluating AI systems under uncertain ground truth: a case study in dermatology. (arXiv:2307.02191v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02191
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24635;&#32467;&#20102;&#22312;&#20581;&#24247;&#39046;&#22495;&#20013;&#35780;&#20272;AI&#31995;&#32479;&#26102;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65306;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#24573;&#35270;&#20102;&#36825;&#19968;&#28857;&#65292;&#32780;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#32858;&#21512;&#27880;&#37322;&#30340;&#26694;&#26550;&#65292;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;AI&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23433;&#20840;&#36215;&#35265;&#65292;&#22312;&#37096;&#32626;&#20043;&#21069;&#65292;&#21355;&#29983;&#39046;&#22495;&#30340;AI&#31995;&#32479;&#38656;&#35201;&#32463;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#23558;&#20854;&#39044;&#27979;&#32467;&#26524;&#19982;&#20551;&#23450;&#20026;&#30830;&#23450;&#30340;&#22522;&#20934;&#20107;&#23454;&#36827;&#34892;&#39564;&#35777;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#24182;&#38750;&#22914;&#27492;&#65292;&#22522;&#20934;&#20107;&#23454;&#21487;&#33021;&#26159;&#19981;&#30830;&#23450;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#26631;&#20934;&#30340;AI&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36825;&#19968;&#28857;&#34987;&#22823;&#37096;&#20998;&#24573;&#35270;&#20102;&#65292;&#20294;&#26159;&#23427;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#65292;&#22914;&#39640;&#20272;&#26410;&#26469;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#27979;&#37327;&#20102;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#25105;&#20204;&#20551;&#35774;&#23427;&#21487;&#20197;&#20998;&#35299;&#20026;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#65306;&#27880;&#37322;&#19981;&#30830;&#23450;&#24615;&#26159;&#30001;&#20110;&#32570;&#20047;&#21487;&#38752;&#27880;&#37322;&#65292;&#20197;&#21450;&#30001;&#20110;&#26377;&#38480;&#30340;&#35266;&#27979;&#20449;&#24687;&#32780;&#23548;&#33268;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#30830;&#23450;&#22320;&#32858;&#21512;&#27880;&#37322;&#26102;&#65292;&#36890;&#24120;&#20250;&#24573;&#35270;&#36825;&#31181;&#22522;&#20934;&#20107;&#23454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20363;&#22914;&#36890;&#36807;&#22810;&#25968;&#25237;&#31080;&#25110;&#24179;&#22343;&#20540;&#26469;&#32858;&#21512;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#22312;&#35813;&#26694;&#26550;&#20013;&#20351;&#29992;&#32479;&#35745;&#27169;&#22411;&#36827;&#34892;&#27880;&#37322;&#30340;&#32858;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#27880;&#37322;&#30340;&#32858;&#21512;&#26694;&#26550;&#35299;&#37322;&#20026;&#25152;&#35859;&#21487;&#33021;&#24615;&#30340;&#21518;&#39564;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
For safety, AI systems in health undergo thorough evaluations before deployment, validating their predictions against a ground truth that is assumed certain. However, this is actually not the case and the ground truth may be uncertain. Unfortunately, this is largely ignored in standard evaluation of AI models but can have severe consequences such as overestimating the future performance. To avoid this, we measure the effects of ground truth uncertainty, which we assume decomposes into two main components: annotation uncertainty which stems from the lack of reliable annotations, and inherent uncertainty due to limited observational information. This ground truth uncertainty is ignored when estimating the ground truth by deterministically aggregating annotations, e.g., by majority voting or averaging. In contrast, we propose a framework where aggregation is done using a statistical model. Specifically, we frame aggregation of annotations as posterior inference of so-called plausibilities
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#23616;&#37096;&#26368;&#20248;&#25511;&#21046;&#20989;&#25968;&#30340;&#27010;&#24565;&#26469;&#34920;&#24449;&#36845;&#20195;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.05816</link><description>&lt;p&gt;
&#24102;&#26377;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Policy Gradient Framework for Stochastic Optimal Control Problems with Global Convergence Guarantee. (arXiv:2302.05816v2 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05816
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#20840;&#23616;&#25910;&#25947;&#20445;&#35777;&#30340;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#23616;&#37096;&#26368;&#20248;&#25511;&#21046;&#20989;&#25968;&#30340;&#27010;&#24565;&#26469;&#34920;&#24449;&#36845;&#20195;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#36830;&#32493;&#26102;&#38388;&#20013;&#38543;&#26426;&#26368;&#20248;&#25511;&#21046;&#38382;&#39064;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#25511;&#21046;&#30340;&#26799;&#24230;&#27969;&#65292;&#23558;&#20854;&#35270;&#20026;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#36830;&#32493;&#26102;&#38388;&#26497;&#38480;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26799;&#24230;&#27969;&#30340;&#20840;&#23616;&#25910;&#25947;&#24615;&#65292;&#24182;&#22312;&#19968;&#20123;&#27491;&#21017;&#24615;&#20551;&#35774;&#19979;&#24314;&#31435;&#20102;&#25910;&#25947;&#36895;&#24230;&#12290;&#20998;&#26512;&#20013;&#30340;&#20027;&#35201;&#21019;&#26032;&#26159;&#24341;&#20837;&#20102;&#23616;&#37096;&#26368;&#20248;&#25511;&#21046;&#20989;&#25968;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#34920;&#24449;&#36845;&#20195;&#30340;&#23616;&#37096;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider policy gradient methods for stochastic optimal control problem in continuous time. In particular, we analyze the gradient flow for the control, viewed as a continuous time limit of the policy gradient method. We prove the global convergence of the gradient flow and establish a convergence rate under some regularity assumptions. The main novelty in the analysis is the notion of local optimal control function, which is introduced to characterize the local optimality of the iterate.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.04979</link><description>&lt;p&gt;
&#26080;&#26631;&#31614;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Label-free segmentation from cardiac ultrasound using self-supervised learning. (arXiv:2210.04979v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04979
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#22312;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20998;&#21106;&#20013;&#21462;&#24471;&#20102;&#21487;&#38752;&#30340;&#32467;&#26524;&#65292;&#19982;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#30456;&#20284;&#30340;&#27979;&#37327;&#20934;&#30830;&#24230;&#65292;&#24182;&#19988;&#33021;&#22815;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#30340;&#20998;&#21106;&#21644;&#27979;&#37327;&#23545;&#20110;&#24515;&#33039;&#36229;&#22768;&#26469;&#35828;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#36825;&#20123;&#20219;&#21153;&#32791;&#26102;&#19988;&#38590;&#20197;&#37325;&#29616;&#12290;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#25552;&#20379;&#36741;&#21161;&#65292;&#20294;&#26159;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#36827;&#34892;&#25163;&#21160;&#26631;&#27880;&#12290;&#26412;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#26080;&#38656;&#25163;&#21160;&#26631;&#27880;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#27969;&#31243;&#65292;&#32467;&#21512;&#20102;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#20020;&#24202;&#39046;&#22495;&#30693;&#35782;&#21644;&#28145;&#24230;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;450&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;93000&#24352;&#22270;&#29255;&#65289;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#24182;&#22312;8393&#20010;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65288;4476266&#24352;&#22270;&#29255;&#65292;&#24179;&#22343;&#24180;&#40836;61&#23681;&#65292;&#22899;&#24615;&#21344;51%&#65289;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#21033;&#29992;&#20998;&#21106;&#32467;&#26524;&#36827;&#34892;&#29983;&#29289;&#27979;&#37327;&#12290;&#25105;&#20204;&#36824;&#23545;&#26469;&#33258;&#39069;&#22806;10030&#21517;&#24739;&#32773;&#30340;&#22806;&#37096;&#22270;&#20687;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#25163;&#21160;&#25551;&#36857;&#30340;&#24038;&#23460;&#20449;&#24687;&#12290;&#22312;&#20960;&#31181;&#19981;&#21516;&#30340;&#27979;&#37327;&#25351;&#26631;&#65288;r2 0.56-0.84&#65289;&#19978;&#65292;&#20020;&#24202;&#27979;&#37327;&#21644;&#25105;&#20204;&#30340;&#27969;&#31243;&#39044;&#27979;&#20043;&#38388;&#30340;r2&#20540;&#19982;&#24050;&#25253;&#36947;&#30340;&#20020;&#24202;&#21307;&#29983;&#20043;&#38388;&#30340;&#21464;&#24322;&#31243;&#24230;&#30456;&#20284;&#65292;&#24182;&#19988;&#19982;&#30417;&#30563;&#23398;&#20064;&#30340;&#32467;&#26524;&#30456;&#24403;&#12290;&#26816;&#27979;&#24322;&#24120;&#24515;&#33108;&#22823;&#23567;&#21644;&#21151;&#33021;&#30340;&#24179;&#22343;&#20934;&#30830;&#24230;&#20026;0.85&#65288;&#33539;&#22260;0.71-0.97&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Segmentation and measurement of cardiac chambers is critical in cardiac ultrasound but is laborious and poorly reproducible. Neural networks can assist, but supervised approaches require the same laborious manual annotations. We built a pipeline for self-supervised (no manual labels) segmentation combining computer vision, clinical domain knowledge, and deep learning. We trained on 450 echocardiograms (93,000 images) and tested on 8,393 echocardiograms (4,476,266 images; mean 61 years, 51% female), using the resulting segmentations to calculate biometrics. We also tested against external images from an additional 10,030 patients with available manual tracings of the left ventricle. r2 between clinically measured and pipeline-predicted measurements were similar to reported inter-clinician variation and comparable to supervised learning across several different measurements (r2 0.56-0.84). Average accuracy for detecting abnormal chamber size and function was 0.85 (range 0.71-0.97) compar
&lt;/p&gt;</description></item></channel></rss>