<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>CLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#26041;&#27861;&#65292;&#29992;&#20110;&#32452;&#21512;&#22810;&#20010;LoRA&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#23558;&#19981;&#21516;&#27010;&#24565;LoRA&#27169;&#22411;&#26080;&#32541;&#28151;&#21512;&#21040;&#19968;&#20010;&#22270;&#20687;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19776</link><description>&lt;p&gt;
CLoRA: &#19968;&#31181;&#23545;&#27604;&#26041;&#27861;&#26469;&#32452;&#21512;&#22810;&#20010; LoRA &#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CLoRA: A Contrastive Approach to Compose Multiple LoRA Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19776
&lt;/p&gt;
&lt;p&gt;
CLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#23545;&#27604;&#26041;&#27861;&#65292;&#29992;&#20110;&#32452;&#21512;&#22810;&#20010;LoRA&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#23558;&#19981;&#21516;&#27010;&#24565;LoRA&#27169;&#22411;&#26080;&#32541;&#28151;&#21512;&#21040;&#19968;&#20010;&#22270;&#20687;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#35843;&#25972;&#65288;LoRA&#65289;&#24050;&#32463;&#25104;&#20026;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#20013;&#19968;&#31181;&#24378;&#22823;&#19988;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#24335;&#26469;&#35843;&#25972;&#21644;&#25913;&#36827;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#26080;&#38656;&#20840;&#38754;&#22320;&#37325;&#26032;&#35757;&#32451;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340; LoRA &#27169;&#22411;&#65292;&#20363;&#22914;&#20195;&#34920;&#29305;&#23450;&#29483;&#21644;&#29305;&#23450;&#29399;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#29983;&#25104;&#19968;&#20010;&#22270;&#20687;&#65292;&#35813;&#22270;&#20687;&#30495;&#23454;&#22320;&#20307;&#29616;&#20102; LoRA &#25152;&#23450;&#20041;&#30340;&#20004;&#31181;&#21160;&#29289;&#12290;&#28982;&#32780;&#65292;&#26080;&#32541;&#22320;&#28151;&#21512;&#22810;&#20010;&#27010;&#24565; LoRA &#27169;&#22411;&#20197;&#25429;&#33719;&#19968;&#20010;&#22270;&#20687;&#20013;&#30340;&#21508;&#31181;&#27010;&#24565;&#30340;&#20219;&#21153;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#24120;&#35265;&#26041;&#27861;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#65292;&#20027;&#35201;&#26159;&#22240;&#20026;&#19981;&#21516; LoRA &#27169;&#22411;&#20869;&#30340;&#27880;&#24847;&#26426;&#21046;&#37325;&#21472;&#65292;&#23548;&#33268;&#19968;&#20010;&#27010;&#24565;&#21487;&#33021;&#34987;&#23436;&#20840;&#24573;&#30053;&#65288;&#20363;&#22914;&#28431;&#25481;&#20102;&#29399;&#65289;&#65292;&#25110;&#32773;&#27010;&#24565;&#34987;&#38169;&#35823;&#22320;&#32452;&#21512;&#22312;&#19968;&#36215;&#65288;&#20363;&#22914;&#29983;&#25104;&#20004;&#21482;&#29483;&#30340;&#22270;&#20687;&#32780;&#19981;&#26159;&#19968;&#21482;&#29483;&#21644;&#19968;&#21482;&#29399;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19776v1 Announce Type: cross  Abstract: Low-Rank Adaptations (LoRAs) have emerged as a powerful and popular technique in the field of image generation, offering a highly effective way to adapt and refine pre-trained deep learning models for specific tasks without the need for comprehensive retraining. By employing pre-trained LoRA models, such as those representing a specific cat and a particular dog, the objective is to generate an image that faithfully embodies both animals as defined by the LoRAs. However, the task of seamlessly blending multiple concept LoRAs to capture a variety of concepts in one image proves to be a significant challenge. Common approaches often fall short, primarily because the attention mechanisms within different LoRA models overlap, leading to scenarios where one concept may be completely ignored (e.g., omitting the dog) or where concepts are incorrectly combined (e.g., producing an image of two cats instead of one cat and one dog). To overcome th
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16591</link><description>&lt;p&gt;
&#25581;&#31034;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16591
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#38544;&#31169;&#23450;&#20041;&#30340;&#22810;&#26679;&#21270;&#65292;&#30001;&#20110;&#23545;&#38544;&#31169;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#25509;&#21463;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#36825;&#31181;&#20256;&#32479;&#30340;&#38544;&#31169;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20174;&#26080;&#27861;&#38450;&#27490;&#25512;&#26029;&#25259;&#38706;&#21040;&#32570;&#20047;&#23545;&#23545;&#25163;&#32972;&#26223;&#30693;&#35782;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#36125;&#21494;&#26031;&#38544;&#31169;&#24182;&#28145;&#20837;&#25506;&#35752;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#25324;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#31361;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22522;&#20110;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#65288;ABP&#65289;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
&lt;/p&gt;</description></item><item><title>FAST&#26694;&#26550;&#36890;&#36807;&#24555;&#36895;&#20998;&#27573;&#24418;&#29366;&#20989;&#25968;&#30340;&#20248;&#21270;&#21644;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#20351;&#24471;&#36879;&#26126;&#30340;&#38468;&#21152;&#27169;&#22411;&#30340;&#25311;&#21512;&#36895;&#24230;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;2&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>https://arxiv.org/abs/2402.12630</link><description>&lt;p&gt;
FAST: &#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#36879;&#26126;&#26426;&#22120;&#23398;&#20064;&#20013;&#24555;&#36895;&#38468;&#21152;&#20998;&#21106;&#30340;&#20248;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12630
&lt;/p&gt;
&lt;p&gt;
FAST&#26694;&#26550;&#36890;&#36807;&#24555;&#36895;&#20998;&#27573;&#24418;&#29366;&#20989;&#25968;&#30340;&#20248;&#21270;&#21644;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#20351;&#24471;&#36879;&#26126;&#30340;&#38468;&#21152;&#27169;&#22411;&#30340;&#25311;&#21512;&#36895;&#24230;&#27604;&#29616;&#26377;&#26041;&#27861;&#24555;2&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FAST&#65292;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#38468;&#21152;&#20998;&#21106;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;FAST&#20026;&#25968;&#25454;&#38598;&#20013;&#30340;&#27599;&#20010;&#29305;&#24449;&#20998;&#27573;&#24120;&#25968;&#24418;&#29366;&#20989;&#25968;&#65292;&#20197;&#20135;&#29983;&#36879;&#26126;&#30340;&#38468;&#21152;&#27169;&#22411;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#20248;&#21270;&#36807;&#31243;&#36866;&#37197;&#36825;&#20123;&#27169;&#22411;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#22914;&#21487;&#35299;&#37322;&#24615;&#22686;&#24378;&#26426;&#22120; \citep{nori2019interpretml}&#65292;&#24555;&#32422;2&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#36824;&#22312;FAST&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#26032;&#30340;&#29305;&#24449;&#36873;&#25321;&#31639;&#27861;&#65292;&#20197;&#36866;&#37197;&#24615;&#33021;&#33391;&#22909;&#30340;&#31616;&#32422;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;FAST&#25552;&#39640;&#20102;&#38468;&#21152;&#27169;&#22411;&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12630v1 Announce Type: cross  Abstract: We present FAST, an optimization framework for fast additive segmentation. FAST segments piecewise constant shape functions for each feature in a dataset to produce transparent additive models. The framework leverages a novel optimization procedure to fit these models $\sim$2 orders of magnitude faster than existing state-of-the-art methods, such as explainable boosting machines \citep{nori2019interpretml}. We also develop new feature selection algorithms in the FAST framework to fit parsimonious models that perform well. Through experiments and case studies, we show that FAST improves the computational efficiency and interpretability of additive models.
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.05346</link><description>&lt;p&gt;
KIX: &#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
KIX: A Metacognitive Generalization Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05346
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#32570;&#20047;&#36890;&#29992;&#34892;&#20026;&#65292;&#38656;&#35201;&#21033;&#29992;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;KIX&#65292;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#20419;&#36827;&#20102;&#30693;&#35782;&#19982;&#24378;&#21270;&#23398;&#20064;&#30340;&#34701;&#21512;&#65292;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#25552;&#20379;&#20102;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21644;&#20854;&#20182;&#21160;&#29289;&#33021;&#22815;&#28789;&#27963;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#65292;&#24182;&#19988;&#33021;&#22815;&#36890;&#36807;&#37325;&#22797;&#20351;&#29992;&#21644;&#24212;&#29992;&#38271;&#26399;&#31215;&#32047;&#30340;&#39640;&#32423;&#30693;&#35782;&#26469;&#36866;&#24212;&#26032;&#39062;&#24773;&#22659;&#65292;&#36825;&#34920;&#29616;&#20102;&#19968;&#31181;&#27867;&#21270;&#26234;&#33021;&#34892;&#20026;&#12290;&#20294;&#26159;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26356;&#22810;&#22320;&#26159;&#19987;&#23478;&#65292;&#32570;&#20047;&#36825;&#31181;&#36890;&#29992;&#34892;&#20026;&#12290;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#38656;&#35201;&#29702;&#35299;&#21644;&#21033;&#29992;&#20851;&#38190;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#35748;&#30693;&#27867;&#21270;&#26694;&#26550;&#65292;&#31216;&#20026;Knowledge-Interaction-eXecution (KIX)&#65292;&#24182;&#19988;&#35748;&#20026;&#36890;&#36807;&#19982;&#23545;&#35937;&#30340;&#20132;&#20114;&#26469;&#21033;&#29992;&#31867;&#22411;&#31354;&#38388;&#21487;&#20197;&#20419;&#36827;&#23398;&#20064;&#21487;&#36801;&#31227;&#30340;&#20132;&#20114;&#27010;&#24565;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#36825;&#26159;&#23558;&#30693;&#35782;&#34701;&#20837;&#21040;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#24182;&#26377;&#26395;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#20013;&#23454;&#29616;&#33258;&#20027;&#21644;&#36890;&#29992;&#34892;&#20026;&#30340;&#25512;&#24191;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans and other animals aptly exhibit general intelligence behaviors in solving a variety of tasks with flexibility and ability to adapt to novel situations by reusing and applying high level knowledge acquired over time. But artificial agents are more of a specialist, lacking such generalist behaviors. Artificial agents will require understanding and exploiting critical structured knowledge representations. We present a metacognitive generalization framework, Knowledge-Interaction-eXecution (KIX), and argue that interactions with objects leveraging type space facilitate the learning of transferable interaction concepts and generalization. It is a natural way of integrating knowledge into reinforcement learning and promising to act as an enabler for autonomous and generalist behaviors in artificial intelligence systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASPIRE&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#32422;&#26463;&#30340;D-&#33539;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20197;&#28789;&#27963;&#25511;&#21046;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.14364</link><description>&lt;p&gt;
&#20855;&#26377;&#38750;&#20984;&#30446;&#26631;&#20989;&#25968;&#30340;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#65306;&#31639;&#27861;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis. (arXiv:2307.14364v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;ASPIRE&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#32422;&#26463;&#30340;D-&#33539;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20197;&#28789;&#27963;&#25511;&#21046;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (DRO) &#26088;&#22312;&#25214;&#21040;&#19968;&#20010;&#26368;&#20248;&#20915;&#31574;&#65292;&#20197;&#22312;&#27010;&#29575;&#20998;&#24067;&#30340;&#27169;&#31946;&#38598;&#21512;&#20013;&#26368;&#23567;&#21270;&#26368;&#22351;&#24773;&#20917;&#25104;&#26412;&#65292;&#24050;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#20363;&#22914;&#32593;&#32476;&#34892;&#20026;&#20998;&#26512;&#12289;&#39118;&#38505;&#31649;&#29702;&#31561;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;DRO&#25216;&#26415;&#38754;&#20020;&#19977;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;1&#65289;&#22914;&#20309;&#22788;&#29702;&#20998;&#24067;&#29615;&#22659;&#20013;&#30340;&#24322;&#27493;&#26356;&#26032;&#65307;2&#65289;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#65307;3&#65289;&#22914;&#20309;&#26681;&#25454;&#19981;&#21516;&#22330;&#26223;&#36866;&#24403;&#35843;&#25972;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE)&#31639;&#27861;&#30340;&#24322;&#27493;&#20998;&#24067;&#24335;&#31639;&#27861;&#65292;&#20197;&#22788;&#29702;&#32852;&#37030;&#20998;&#24067;&#40065;&#26834;&#20248;&#21270; (FDRO) &#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#21363;&#32422;&#26463;&#30340;D-&#33539;&#25968;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#20197;&#26377;&#25928;&#21033;&#29992;&#20808;&#39564;&#20998;&#24067;&#24182;&#28789;&#27963;&#25511;&#21046;&#40065;&#26834;&#24615;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributionally Robust Optimization (DRO), which aims to find an optimal decision that minimizes the worst case cost over the ambiguity set of probability distribution, has been widely applied in diverse applications, e.g., network behavior analysis, risk management, etc. However, existing DRO techniques face three key challenges: 1) how to deal with the asynchronous updating in a distributed environment; 2) how to leverage the prior distribution effectively; 3) how to properly adjust the degree of robustness according to different scenarios. To this end, we propose an asynchronous distributed algorithm, named Asynchronous Single-looP alternatIve gRadient projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to tackle the federated distributionally robust optimization (FDRO) problem. Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set, is developed to effectively leverage the prior distribution and flexibly control the degree of robustness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#36229;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#24635;&#32467;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#21464;&#20307;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#21508;&#31181;&#19982;HGNN&#30456;&#20851;&#30340;&#24212;&#29992;&#21644;&#24403;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2202.13852</link><description>&lt;p&gt;
&#36229;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#65306;&#26041;&#27861;&#21644;&#24212;&#29992;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Hyperbolic Graph Neural Networks: A Review of Methods and Applications. (arXiv:2202.13852v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13852
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#24403;&#21069;&#36229;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#24635;&#32467;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#21464;&#20307;&#12290;&#27492;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#21508;&#31181;&#19982;HGNN&#30456;&#20851;&#30340;&#24212;&#29992;&#21644;&#24403;&#21069;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#23558;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#24191;&#21040;&#20102;&#22270;&#32467;&#26500;&#25968;&#25454;&#65292;&#24182;&#22240;&#20854;&#20986;&#33394;&#30340;&#34920;&#24449;&#33021;&#21147;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#27431;&#20960;&#37324;&#24471;&#27169;&#22411;&#22312;&#19982;&#22270;&#30456;&#20851;&#30340;&#23398;&#20064;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#21463;&#21040;&#27431;&#20960;&#37324;&#24471;&#20960;&#20309;&#30340;&#34920;&#24449;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20855;&#26377;&#39640;&#24230;&#38750;&#27431;&#20960;&#37324;&#24471;&#28508;&#22312;&#35299;&#21078;&#30340;&#25968;&#25454;&#38598;&#12290;&#26368;&#36817;&#65292;&#36229;&#36793;&#30028;&#31354;&#38388;&#22312;&#22788;&#29702;&#20855;&#26377;&#26641;&#29366;&#32467;&#26500;&#21644;&#24130;&#24459;&#20998;&#24067;&#30340;&#22270;&#25968;&#25454;&#26041;&#38754;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#36825;&#24402;&#21151;&#20110;&#20854;&#25351;&#25968;&#32423;&#30340;&#22686;&#38271;&#29305;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#24403;&#21069;&#36229;&#36793;&#30028;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25216;&#26415;&#32454;&#33410;&#65292;&#23558;&#23427;&#20204;&#32479;&#19968;&#20026;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#24635;&#32467;&#20102;&#27599;&#20010;&#32452;&#20214;&#30340;&#21464;&#20307;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21508;&#31181;&#19982;HGNN&#30456;&#20851;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#36824;&#30830;&#23450;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#36825;&#20123;&#25361;&#25112;&#21487;&#33021;&#25104;&#20026;&#36827;&#19968;&#27493;&#21457;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#25104;&#23601;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks generalize conventional neural networks to graph-structured data and have received widespread attention due to their impressive representation ability. In spite of the remarkable achievements, the performance of Euclidean models in graph-related learning is still bounded and limited by the representation ability of Euclidean geometry, especially for datasets with highly non-Euclidean latent anatomy. Recently, hyperbolic space has gained increasing popularity in processing graph data with tree-like structure and power-law distribution, owing to its exponential growth property. In this survey, we comprehensively revisit the technical details of the current hyperbolic graph neural networks, unifying them into a general framework and summarizing the variants of each component. More importantly, we present various HGNN-related applications. Last, we also identify several challenges, which potentially serve as guidelines for further flourishing the achievements of graph
&lt;/p&gt;</description></item></channel></rss>