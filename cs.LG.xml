<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#21487;&#20197;&#33258;&#30001;&#25554;&#20540;&#24182;&#20135;&#29983;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21516;&#26102;&#20855;&#26377;&#26368;&#20248;&#36755;&#36816;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.19895</link><description>&lt;p&gt;
&#19968;&#31181;&#20449;&#24687;&#35770;&#26694;&#26550;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
An Information-Theoretic Framework for Out-of-Distribution Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19895
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#26694;&#26550;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#65292;&#21487;&#20197;&#33258;&#30001;&#25554;&#20540;&#24182;&#20135;&#29983;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#21516;&#26102;&#20855;&#26377;&#26368;&#20248;&#36755;&#36816;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#20449;&#24687;&#35770;&#27867;&#21270;&#30028;&#38480;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;Integral Probability Metric&#65288;IPM&#65289;&#21644;$f$-divergence&#20043;&#38388;&#33258;&#30001;&#25554;&#20540;&#65292;&#33258;&#28982;&#22320;&#24674;&#22797;&#20102;&#19968;&#20123;&#24050;&#30693;&#32467;&#26524;&#65288;&#21253;&#25324;Wasserstein&#21644;KL-bound&#65289;&#65292;&#24182;&#20135;&#29983;&#20102;&#26032;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#26368;&#20248;&#36755;&#36816;&#35299;&#37322;&#12290;&#22312;&#20004;&#20010;&#20855;&#20307;&#31034;&#20363;&#20013;&#35780;&#20272;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#30028;&#38480;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#20005;&#26684;&#25913;&#36827;&#20102;&#29616;&#26377;&#30028;&#38480;&#65292;&#25110;&#32773;&#24674;&#22797;&#20102;&#29616;&#26377;OOD&#27867;&#21270;&#30028;&#38480;&#20013;&#30340;&#26368;&#20339;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19895v1 Announce Type: cross  Abstract: We study the Out-of-Distribution (OOD) generalization in machine learning and propose a general framework that provides information-theoretic generalization bounds. Our framework interpolates freely between Integral Probability Metric (IPM) and $f$-divergence, which naturally recovers some known results (including Wasserstein- and KL-bounds), as well as yields new generalization bounds. Moreover, we show that our framework admits an optimal transport interpretation. When evaluated in two concrete examples, the proposed bounds either strictly improve upon existing bounds in some cases or recover the best among existing OOD generalization bounds.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;</title><link>https://arxiv.org/abs/2403.03551</link><description>&lt;p&gt;
&#36890;&#36807;&#24494;&#35843;&#39044;&#20808;&#20026;&#39640;&#26031;&#38477;&#22122;&#32780;&#35757;&#32451;&#30340;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#65292;&#29992;&#20110;&#22270;&#20687;&#22686;&#24378;&#30340;&#19979;&#28216;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Low-Dose CT Image Reconstruction by Fine-Tuning a UNet Pretrained for Gaussian Denoising for the Downstream Task of Image Enhancement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03551
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31934;&#35843;UNet&#36827;&#34892;&#20302;&#21058;&#37327;CT&#22270;&#20687;&#37325;&#24314;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#20026;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#65288;CT&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#21307;&#23398;&#25104;&#20687;&#27169;&#24577;&#65292;&#30001;&#20110;&#20854;&#22522;&#20110;&#30005;&#31163;&#36752;&#23556;&#65292;&#22240;&#27492;&#24076;&#26395;&#23613;&#37327;&#20943;&#23569;&#36752;&#23556;&#21058;&#37327;&#12290;&#28982;&#32780;&#65292;&#38477;&#20302;&#36752;&#23556;&#21058;&#37327;&#20250;&#23548;&#33268;&#22270;&#20687;&#36136;&#37327;&#19979;&#38477;&#65292;&#20174;&#20302;&#21058;&#37327;CT&#65288;LDCT&#65289;&#25968;&#25454;&#37325;&#24314;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#20540;&#24471;&#36827;&#34892;&#30740;&#31350;&#12290;&#26681;&#25454;LoDoPaB-CT&#22522;&#20934;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#20351;&#29992;&#28041;&#21450;UNet&#22411;&#26550;&#26500;&#30340;&#27969;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25490;&#21517;&#31532;&#19968;&#30340;&#26041;&#27861;ItNet&#20351;&#29992;&#21253;&#25324;&#28388;&#27874;&#21453;&#25237;&#24433;&#65288;FBP&#65289;&#12289;&#22312;CT&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;UNet&#21644;&#36845;&#20195;&#32454;&#21270;&#27493;&#39588;&#30340;&#19977;&#38454;&#27573;&#27969;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#31616;&#21333;&#30340;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#31532;&#19968;&#38454;&#27573;&#20063;&#20351;&#29992;&#20102;FBP&#65292;&#32780;&#26032;&#39062;&#20043;&#22788;&#22312;&#20110;&#31532;&#20108;&#38454;&#27573;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#29305;&#28857;&#26159;CT&#22270;&#20687;&#22686;&#24378;&#38454;&#27573;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#28857;&#22312;&#20110;&#31070;&#32463;&#32593;&#32476;&#26159;&#39044;&#35757;&#32451;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03551v1 Announce Type: cross  Abstract: Computed Tomography (CT) is a widely used medical imaging modality, and as it is based on ionizing radiation, it is desirable to minimize the radiation dose. However, a reduced radiation dose comes with reduced image quality, and reconstruction from low-dose CT (LDCT) data is still a challenging task which is subject to research. According to the LoDoPaB-CT benchmark, a benchmark for LDCT reconstruction, many state-of-the-art methods use pipelines involving UNet-type architectures. Specifically the top ranking method, ItNet, employs a three-stage process involving filtered backprojection (FBP), a UNet trained on CT data, and an iterative refinement step. In this paper, we propose a less complex two-stage method. The first stage also employs FBP, while the novelty lies in the training strategy for the second stage, characterized as the CT image enhancement stage. The crucial point of our approach is that the neural network is pretrained
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;</title><link>https://arxiv.org/abs/2402.12498</link><description>&lt;p&gt;
&#23553;&#24314;&#32593;&#32476;&#29992;&#20110;&#35270;&#35273;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Feudal Networks for Visual Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12498
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#65292;&#36890;&#36807;&#39640;&#32423;&#31649;&#29702;&#32773;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#24037;&#20316;&#20195;&#29702;&#30340;&#20998;&#23618;&#32467;&#26500;&#65292;&#22312;&#19981;&#21516;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#25805;&#20316;&#65292;&#20855;&#26377;&#29420;&#29305;&#27169;&#22359;&#26469;&#23454;&#29616;&#33258;&#30417;&#30563;&#23398;&#20064;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#23548;&#33322;&#36981;&#24490;&#20154;&#31867;&#21487;&#20197;&#22312;&#27809;&#26377;&#35814;&#32454;&#22320;&#22270;&#30340;&#24773;&#20917;&#19979;&#23548;&#33322;&#30340;&#30452;&#35273;&#12290;&#19968;&#31181;&#24120;&#35265;&#26041;&#27861;&#26159;&#22312;&#24314;&#31435;&#21253;&#21547;&#21487;&#29992;&#20110;&#35268;&#21010;&#30340;&#22270;&#20687;&#33410;&#28857;&#30340;&#25299;&#25169;&#22270;&#30340;&#21516;&#26102;&#36827;&#34892;&#20132;&#20114;&#24335;&#25506;&#32034;&#12290;&#26368;&#36817;&#30340;&#21464;&#20307;&#20174;&#34987;&#21160;&#35270;&#39057;&#20013;&#23398;&#20064;&#65292;&#24182;&#21487;&#20197;&#21033;&#29992;&#22797;&#26434;&#30340;&#31038;&#20132;&#21644;&#35821;&#20041;&#32447;&#32034;&#36827;&#34892;&#23548;&#33322;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#35270;&#39057;&#65292;&#21033;&#29992;&#22823;&#22411;&#22270;&#24182;&#19988;&#30001;&#20110;&#20351;&#29992;&#20102;&#37324;&#31243;&#35745;&#65292;&#22330;&#26223;&#19981;&#26159;&#26410;&#30693;&#30340;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#20351;&#29992;&#23553;&#24314;&#23398;&#20064;&#30340;&#35270;&#35273;&#23548;&#33322;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#30001;&#24037;&#20316;&#20195;&#29702;&#12289;&#20013;&#32423;&#31649;&#29702;&#32773;&#21644;&#39640;&#32423;&#31649;&#29702;&#32773;&#32452;&#25104;&#30340;&#20998;&#23618;&#32467;&#26500;&#12290;&#23553;&#24314;&#23398;&#20064;&#33539;&#24335;&#30340;&#20851;&#38190;&#22312;&#20110;&#65292;&#27599;&#20010;&#32423;&#21035;&#30340;&#20195;&#29702;&#30475;&#21040;&#20219;&#21153;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#23610;&#24230;&#19978;&#36816;&#20316;&#12290;&#22312;&#27492;&#26694;&#26550;&#20013;&#24320;&#21457;&#20102;&#20004;&#20010;&#29420;&#29305;&#30340;&#27169;&#22359;&#12290;&#23545;&#20110;&#39640;&#32423;&#31649;&#29702;&#32773;&#65292;&#25105;&#20204;&#33258;&#30417;&#30563;&#22320;&#23398;&#20064;&#19968;&#20010;&#35760;&#24518;&#20195;&#29702;&#22320;&#22270;&#20197;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12498v1 Announce Type: cross  Abstract: Visual navigation follows the intuition that humans can navigate without detailed maps. A common approach is interactive exploration while building a topological graph with images at nodes that can be used for planning. Recent variations learn from passive videos and can navigate using complex social and semantic cues. However, a significant number of training videos are needed, large graphs are utilized, and scenes are not unseen since odometry is utilized. We introduce a new approach to visual navigation using feudal learning, which employs a hierarchical structure consisting of a worker agent, a mid-level manager, and a high-level manager. Key to the feudal learning paradigm, agents at each level see a different aspect of the task and operate at different spatial and temporal scales. Two unique modules are developed in this framework. For the high- level manager, we learn a memory proxy map in a self supervised manner to record prio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2402.09066</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#22266;&#20307;&#24223;&#29289;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Solid Waste Detection in Remote Sensing Images: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09066
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22266;&#20307;&#24223;&#29289;&#22312;&#36965;&#24863;&#22270;&#20687;&#20013;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#30740;&#31350;&#32773;&#21033;&#29992;&#22320;&#29699;&#35266;&#27979;&#21355;&#26143;&#25552;&#20379;&#30340;&#39640;&#20998;&#36776;&#29575;&#25968;&#25454;&#65292;&#36890;&#36807;&#36965;&#24863;&#22270;&#20687;&#23454;&#29616;&#20102;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#35782;&#21035;&#12289;&#30417;&#27979;&#21644;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#21644;&#34920;&#24449;&#38750;&#27861;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#23545;&#29615;&#22659;&#20445;&#25252;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24212;&#23545;&#27745;&#26579;&#21644;&#20581;&#24247;&#21361;&#23475;&#12290;&#19981;&#24403;&#31649;&#29702;&#30340;&#22403;&#22334;&#22635;&#22475;&#22330;&#36890;&#36807;&#38632;&#27700;&#28183;&#36879;&#27745;&#26579;&#22303;&#22756;&#21644;&#22320;&#19979;&#27700;&#65292;&#23545;&#21160;&#29289;&#21644;&#20154;&#31867;&#26500;&#25104;&#23041;&#32961;&#12290;&#20256;&#32479;&#30340;&#22635;&#22475;&#22330;&#36776;&#35782;&#26041;&#27861;&#65292;&#22914;&#29616;&#22330;&#26816;&#26597;&#65292;&#32791;&#26102;&#19988;&#26114;&#36149;&#12290;&#36965;&#24863;&#25216;&#26415;&#26159;&#29992;&#20110;&#35782;&#21035;&#21644;&#30417;&#27979;&#22266;&#20307;&#24223;&#29289;&#22788;&#32622;&#22330;&#22320;&#30340;&#19968;&#31181;&#32463;&#27982;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#23454;&#29616;&#24191;&#27867;&#35206;&#30422;&#21644;&#22810;&#27425;&#33719;&#21462;&#12290;&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#21355;&#26143;&#37197;&#22791;&#20102;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25104;&#20687;&#33021;&#21147;&#65292;&#20960;&#21313;&#24180;&#26469;&#19968;&#30452;&#25552;&#20379;&#39640;&#20998;&#36776;&#29575;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#19987;&#38376;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#36965;&#24863;&#22270;&#20687;&#25191;&#34892;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#22914;&#24223;&#29289;&#22330;&#22320;&#26816;&#27979;&#12289;&#20542;&#20498;&#22330;&#30417;&#27979;&#21644;&#36866;&#23452;&#20301;&#32622;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09066v1 Announce Type: cross Abstract: The detection and characterization of illegal solid waste disposal sites are essential for environmental protection, particularly for mitigating pollution and health hazards. Improperly managed landfills contaminate soil and groundwater via rainwater infiltration, posing threats to both animals and humans. Traditional landfill identification approaches, such as on-site inspections, are time-consuming and expensive. Remote sensing is a cost-effective solution for the identification and monitoring of solid waste disposal sites that enables broad coverage and repeated acquisitions over time. Earth Observation (EO) satellites, equipped with an array of sensors and imaging capabilities, have been providing high-resolution data for several decades. Researchers proposed specialized techniques that leverage remote sensing imagery to perform a range of tasks such as waste site detection, dumping site monitoring, and assessment of suitable locati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;IRLEED&#65292;&#23427;&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#26469;&#35299;&#20915;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#21644;&#24322;&#36136;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;IRLEED&#36890;&#36807;&#32467;&#21512;&#28436;&#31034;&#32773;&#27425;&#20248;&#24615;&#30340;&#26222;&#36866;&#27169;&#22411;&#21644;&#26368;&#22823;&#29109;IRL&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20174;&#22810;&#26679;&#30340;&#27425;&#20248;&#28436;&#31034;&#20013;&#24471;&#20986;&#26368;&#20339;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2402.01886</link><description>&lt;p&gt;
&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Inverse Reinforcement Learning by Estimating Expertise of Demonstrators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01886
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;IRLEED&#65292;&#23427;&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#26469;&#35299;&#20915;&#27169;&#20223;&#23398;&#20064;&#20013;&#30340;&#27425;&#20248;&#21644;&#24322;&#36136;&#28436;&#31034;&#30340;&#38382;&#39064;&#12290;IRLEED&#36890;&#36807;&#32467;&#21512;&#28436;&#31034;&#32773;&#27425;&#20248;&#24615;&#30340;&#26222;&#36866;&#27169;&#22411;&#21644;&#26368;&#22823;&#29109;IRL&#26694;&#26550;&#65292;&#26377;&#25928;&#22320;&#20174;&#22810;&#26679;&#30340;&#27425;&#20248;&#28436;&#31034;&#20013;&#24471;&#20986;&#26368;&#20339;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27169;&#20223;&#23398;&#20064;&#20013;&#65292;&#21033;&#29992;&#27425;&#20248;&#21644;&#24322;&#36136;&#30340;&#28436;&#31034;&#25552;&#20986;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#24615;&#36136;&#21508;&#19981;&#30456;&#21516;&#12290;&#28982;&#32780;&#65292;&#26631;&#20934;&#30340;&#27169;&#20223;&#23398;&#20064;&#31639;&#27861;&#23558;&#36825;&#20123;&#25968;&#25454;&#38598;&#35270;&#20026;&#21516;&#36136;&#30340;&#65292;&#20174;&#32780;&#32487;&#25215;&#20102;&#27425;&#20248;&#28436;&#31034;&#30340;&#32570;&#38519;&#12290;&#20808;&#21069;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#22914;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#23376;&#38598;&#12289;&#32622;&#20449;&#24230;&#25490;&#21517;&#25110;&#26126;&#30830;&#30340;&#29615;&#22659;&#30693;&#35782;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IRLEED&#65288;&#36890;&#36807;&#20272;&#35745;&#28436;&#31034;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#32780;&#19981;&#38656;&#35201;&#20808;&#21069;&#23545;&#28436;&#31034;&#32773;&#19987;&#19994;&#30693;&#35782;&#36827;&#34892;&#20102;&#35299;&#12290;IRLEED&#36890;&#36807;&#23558;&#28436;&#31034;&#32773;&#27425;&#20248;&#24615;&#30340;&#26222;&#36866;&#27169;&#22411;&#19982;&#26368;&#22823;&#29109;IRL&#26694;&#26550;&#30456;&#32467;&#21512;&#65292;&#26469;&#22788;&#29702;&#22870;&#21169;&#20559;&#24046;&#21644;&#34892;&#21160;&#26041;&#24046;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#20174;&#22810;&#26679;&#30340;&#27425;&#20248;&#28436;&#31034;&#20013;&#24471;&#20986;&#26368;&#20248;&#31574;&#30053;&#12290;&#22312;&#22312;&#32447;&#21644;&#31163;&#32447;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Imitation Learning (IL), utilizing suboptimal and heterogeneous demonstrations presents a substantial challenge due to the varied nature of real-world data. However, standard IL algorithms consider these datasets as homogeneous, thereby inheriting the deficiencies of suboptimal demonstrators. Previous approaches to this issue typically rely on impractical assumptions like high-quality data subsets, confidence rankings, or explicit environmental knowledge. This paper introduces IRLEED, Inverse Reinforcement Learning by Estimating Expertise of Demonstrators, a novel framework that overcomes these hurdles without prior knowledge of demonstrator expertise. IRLEED enhances existing Inverse Reinforcement Learning (IRL) algorithms by combining a general model for demonstrator suboptimality to address reward bias and action variance, with a Maximum Entropy IRL framework to efficiently derive the optimal policy from diverse, suboptimal demonstrations. Experiments in both online and offline I
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#28909;&#39044;&#35686;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#27668;&#20505;&#21644;&#20581;&#24247;&#29615;&#22659;&#20013;&#30340;&#20302;&#20449;&#21495;&#25928;&#24212;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.14196</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#28909;&#39044;&#35686;&#30340;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Optimizing Heat Alert Issuance with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#28909;&#39044;&#35686;&#31995;&#32479;&#65292;&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#21644;&#32508;&#21512;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#27668;&#20505;&#21644;&#20581;&#24247;&#29615;&#22659;&#20013;&#30340;&#20302;&#20449;&#21495;&#25928;&#24212;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#36866;&#24212;&#27668;&#20505;&#21464;&#21270;&#30340;&#20851;&#38190;&#25112;&#30053;&#20043;&#19968;&#26159;&#21033;&#29992;&#39044;&#35686;&#31995;&#32479;&#20943;&#23569;&#26497;&#31471;&#39640;&#28201;&#20107;&#20214;&#30340;&#19981;&#21033;&#20581;&#24247;&#24433;&#21709;&#65292;&#20197;&#20419;&#20351;&#39044;&#38450;&#24615;&#34892;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#20248;&#21270;&#27492;&#31867;&#31995;&#32479;&#25928;&#26524;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#36129;&#29486;&#26377;&#19977;&#20010;&#26041;&#38754;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24378;&#21270;&#23398;&#20064;&#29615;&#22659;&#65292;&#35780;&#20272;&#28909;&#39044;&#35686;&#25919;&#31574;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#20943;&#23569;&#19982;&#39640;&#28201;&#26377;&#20851;&#30340;&#20303;&#38498;&#20154;&#25968;&#12290;&#22870;&#21169;&#27169;&#22411;&#22522;&#20110;&#21382;&#21490;&#22825;&#27668;&#12289;&#21307;&#30103;&#20445;&#38505;&#20581;&#24247;&#35760;&#24405;&#20197;&#21450;&#31038;&#20250;&#32463;&#27982;/&#22320;&#29702;&#29305;&#24449;&#30340;&#20840;&#38754;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#25105;&#20204;&#20351;&#29992;&#21464;&#20998;&#36125;&#21494;&#26031;&#25216;&#26415;&#35299;&#20915;&#20102;&#22312;&#27668;&#20505;&#21644;&#20581;&#24247;&#29615;&#22659;&#20013;&#24120;&#35265;&#30340;&#20302;&#20449;&#21495;&#25928;&#24212;&#21644;&#31354;&#38388;&#24322;&#36136;&#24615;&#12290;&#36716;&#25442;&#27169;&#22411;&#32467;&#21512;&#20102;&#30495;&#23454;&#30340;&#21382;&#21490;&#22825;&#27668;&#27169;&#24335;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;&#27668;&#20505;&#21306;&#22495;&#30456;&#20284;&#24615;&#30340;&#25968;&#25454;&#22686;&#24378;&#26426;&#21046;&#36827;&#34892;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14196v2 Announce Type: replace  Abstract: A key strategy in societal adaptation to climate change is the use of alert systems to reduce the adverse health impacts of extreme heat events by prompting preventative action. In this work, we investigate reinforcement learning (RL) as a tool to optimize the effectiveness of such systems. Our contributions are threefold. First, we introduce a novel RL environment enabling the evaluation of the effectiveness of heat alert policies to reduce heat-related hospitalizations. The rewards model is trained from a comprehensive dataset of historical weather, Medicare health records, and socioeconomic/geographic features. We use variational Bayesian techniques to address low-signal effects and spatial heterogeneity, which are commonly encountered in climate &amp; health settings. The transition model incorporates real historical weather patterns enriched by a data augmentation mechanism based on climate region similarity. Second, we use this env
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#22312;&#25345;&#32493;&#22270;&#21644;&#26223;&#35266;&#31354;&#38388;&#19978;&#24341;&#20837;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#19982;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#26679;&#24066;&#22330;&#24773;&#26223;&#19979;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#30340;&#32489;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.16920</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#30340;&#32858;&#31867;&#23454;&#29616;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Sparse Portfolio Selection via Topological Data Analysis based Clustering. (arXiv:2401.16920v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16920
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#36873;&#25321;&#31574;&#30053;&#65292;&#36890;&#36807;&#21033;&#29992;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#22312;&#25345;&#32493;&#22270;&#21644;&#26223;&#35266;&#31354;&#38388;&#19978;&#24341;&#20837;&#26032;&#30340;&#36317;&#31163;&#24230;&#37327;&#65292;&#24182;&#19982;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#26679;&#24066;&#22330;&#24773;&#26223;&#19979;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#30340;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#26500;&#24314;&#30340;&#25968;&#25454;&#39537;&#21160;&#32858;&#31867;&#22411;&#32929;&#31080;&#36873;&#25321;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#36164;&#20135;&#36873;&#25321;&#31574;&#30053;&#21033;&#29992;&#32929;&#31080;&#20215;&#26684;&#27874;&#21160;&#30340;&#25299;&#25169;&#29305;&#24449;&#65292;&#36873;&#25321;&#19968;&#32452;&#25299;&#25169;&#31867;&#20284;&#65288;&#19981;&#21516;&#65289;&#30340;&#36164;&#20135;&#29992;&#20110;&#31232;&#30095;&#25351;&#25968;&#36861;&#36394;&#65288;&#39532;&#31185;&#32500;&#33576;&#65289;&#25237;&#36164;&#32452;&#21512;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22312;&#25345;&#32493;&#22270;&#21644;&#26223;&#35266;&#31354;&#38388;&#19978;&#32771;&#34385;&#26102;&#38388;&#25104;&#20998;&#30340;&#26032;&#36317;&#31163;&#24230;&#37327;&#65292;&#20316;&#20026;&#32858;&#31867;&#31639;&#27861;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#23545;2009&#24180;&#33267;2020&#24180;&#30340;S\&amp;P&#25351;&#25968;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#21253;&#25324;&#23545;COVID-19&#25968;&#25454;&#30340;&#30740;&#31350;&#65292;&#20197;&#39564;&#35777;&#25105;&#20204;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#23558;&#25299;&#25169;&#25968;&#25454;&#20998;&#26512;&#19982;&#32858;&#31867;&#31639;&#27861;&#30456;&#32467;&#21512;&#30340;&#31574;&#30053;&#26174;&#33879;&#25552;&#21319;&#20102;&#19981;&#21516;&#24066;&#22330;&#24773;&#26223;&#19979;&#31232;&#30095;&#25237;&#36164;&#32452;&#21512;&#30340;&#32508;&#21512;&#32489;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper uses topological data analysis (TDA) tools and introduces a data-driven clustering-based stock selection strategy tailored for sparse portfolio construction. Our asset selection strategy exploits the topological features of stock price movements to select a subset of topologically similar (different) assets for a sparse index tracking (Markowitz) portfolio. We introduce new distance measures, which serve as an input to the clustering algorithm, on the space of persistence diagrams and landscapes that consider the time component of a time series. We conduct an empirical analysis on the S\&amp;P index from 2009 to 2020, including a study on the COVID-19 data to validate the robustness of our methodology. Our strategy to integrate TDA with the clustering algorithm significantly enhanced the performance of sparse portfolios across various performance measures in diverse market scenarios.
&lt;/p&gt;</description></item><item><title>&#20010;&#20307;&#21270;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#25104;&#27169;&#22411;IGNITE&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#30340;&#21160;&#24577;&#29305;&#24449;&#65292;&#32467;&#21512;&#20154;&#21475;&#29305;&#24449;&#21644;&#27835;&#30103;&#20449;&#24687;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30495;&#23454;&#20540;&#65292;&#20026;&#20010;&#20307;&#21270;&#21307;&#30103;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2401.04402</link><description>&lt;p&gt;
IGNITE: &#20010;&#20307;&#21270;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
IGNITE: Individualized GeNeration of Imputations in Time-series Electronic health records. (arXiv:2401.04402v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04402
&lt;/p&gt;
&lt;p&gt;
&#20010;&#20307;&#21270;&#26102;&#38388;&#24207;&#21015;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#30340;&#29983;&#25104;&#27169;&#22411;IGNITE&#36890;&#36807;&#23398;&#20064;&#20010;&#20307;&#30340;&#21160;&#24577;&#29305;&#24449;&#65292;&#32467;&#21512;&#20154;&#21475;&#29305;&#24449;&#21644;&#27835;&#30103;&#20449;&#24687;&#65292;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30495;&#23454;&#20540;&#65292;&#20026;&#20010;&#20307;&#21270;&#21307;&#30103;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#20026;&#25512;&#21160;&#20010;&#20307;&#21270;&#21307;&#30103;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#26681;&#25454;&#20010;&#20307;&#24046;&#24322;&#37327;&#36523;&#23450;&#21046;&#27835;&#30103;&#26041;&#26696;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#35768;&#22810;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#21644;&#32479;&#35745;&#27169;&#22411;&#20511;&#21161;&#20016;&#23500;&#30340;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#26469;&#30740;&#31350;&#24739;&#32773;&#30340;&#29983;&#29702;&#21644;&#27835;&#30103;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#32437;&#21521;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#24448;&#24448;&#31232;&#30095;&#19988;&#23384;&#22312;&#22823;&#37327;&#32570;&#22833;&#65292;&#20854;&#20013;&#32570;&#22833;&#30340;&#20449;&#24687;&#20063;&#21487;&#33021;&#21453;&#26144;&#24739;&#32773;&#30340;&#20581;&#24247;&#29366;&#20917;&#12290;&#22240;&#27492;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#20010;&#20307;&#21270;&#21307;&#30103;&#20013;&#30340;&#25104;&#21151;&#20005;&#37325;&#20381;&#36182;&#20110;&#22914;&#20309;&#20174;&#29983;&#29702;&#25968;&#25454;&#12289;&#27835;&#30103;&#20197;&#21450;&#25968;&#25454;&#20013;&#30340;&#32570;&#22833;&#20540;&#26469;&#34920;&#31034;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#20010;&#20307;&#30340;&#20154;&#21475;&#29305;&#24449;&#21644;&#27835;&#30103;&#30340;&#26465;&#20214;&#19979;&#65292;&#23398;&#20064;&#22810;&#21464;&#37327;&#25968;&#25454;&#30340;&#24739;&#32773;&#21160;&#24577;&#65292;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#30495;&#23454;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic Health Records present a valuable modality for driving personalized medicine, where treatment is tailored to fit individual-level differences. For this purpose, many data-driven machine learning and statistical models rely on the wealth of longitudinal EHRs to study patients' physiological and treatment effects. However, longitudinal EHRs tend to be sparse and highly missing, where missingness could also be informative and reflect the underlying patient's health status. Therefore, the success of data-driven models for personalized medicine highly depends on how the EHR data is represented from physiological data, treatments, and the missing values in the data. To this end, we propose a novel deep-learning model that learns the underlying patient dynamics over time across multivariate data to generate personalized realistic values conditioning on an individual's demographic characteristics and treatments. Our proposed model, IGNITE (Individualized GeNeration of Imputations in
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;G-$\Delta$UQ&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#38170;&#23450;&#31574;&#30053;&#23558;&#38543;&#26426;&#25968;&#25454;&#20013;&#24515;&#21270;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#25903;&#25345;&#37096;&#20998;&#38543;&#26426;&#30340;GNN&#12290;</title><link>http://arxiv.org/abs/2401.03350</link><description>&lt;p&gt;
&#20934;&#30830;&#21487;&#25193;&#23637;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks. (arXiv:2401.03350v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.03350
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;G-$\Delta$UQ&#65292;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#20869;&#22312;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#22270;&#38170;&#23450;&#31574;&#30053;&#23558;&#38543;&#26426;&#25968;&#25454;&#20013;&#24515;&#21270;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#65292;&#24182;&#19988;&#33021;&#22815;&#25903;&#25345;&#37096;&#20998;&#38543;&#26426;&#30340;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24191;&#27867;&#29992;&#20110;&#33410;&#28857;&#21644;&#22270;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#65292;&#20294;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;GNN&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21487;&#38752;&#24615;&#20173;&#30456;&#23545;&#36739;&#23569;&#25506;&#32034;&#12290;&#20107;&#23454;&#19978;&#65292;&#34429;&#28982;&#20107;&#21518;&#26657;&#20934;&#31574;&#30053;&#21487;&#20197;&#29992;&#20110;&#25913;&#21892;&#20869;&#37096;&#20998;&#24067;&#26657;&#20934;&#65292;&#20294;&#23427;&#20204;&#19981;&#19968;&#23450;&#20063;&#33021;&#25913;&#36827;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#26657;&#20934;&#12290;&#28982;&#32780;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#20869;&#37096;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#25216;&#26415;&#23588;&#20854;&#26377;&#20215;&#20540;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38543;&#21518;&#19982;&#20107;&#21518;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;G-$\Delta$UQ&#30340;&#26032;&#22411;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#25913;&#21892;&#20869;&#22312;&#30340;GNN&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36890;&#36807;&#26032;&#39062;&#30340;&#22270;&#38170;&#23450;&#31574;&#30053;&#23558;&#38543;&#26426;&#25968;&#25454;&#20013;&#24515;&#21270;&#21407;&#21017;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#65292;&#24182;&#33021;&#22815;&#25903;&#25345;&#37096;&#20998;&#38543;&#26426;&#30340;GNN&#12290;&#34429;&#28982;&#20027;&#27969;&#35266;&#28857;&#26159;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#20272;&#35745;&#65292;&#38656;&#35201;&#23436;&#20840;&#38543;&#26426;&#32593;&#32476;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36890;&#36807;&#21151;&#33021;&#22810;&#26679;&#24615;&#24341;&#20837;&#30340;&#20013;&#35266;&#38170;&#23450;&#21487;&#20197;&#22312;&#20445;&#35777;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
While graph neural networks (GNNs) are widely used for node and graph representation learning tasks, the reliability of GNN uncertainty estimates under distribution shifts remains relatively under-explored. Indeed, while post-hoc calibration strategies can be used to improve in-distribution calibration, they need not also improve calibration under distribution shift. However, techniques which produce GNNs with better intrinsic uncertainty estimates are particularly valuable, as they can always be combined with post-hoc strategies later. Therefore, in this work, we propose G-$\Delta$UQ, a novel training framework designed to improve intrinsic GNN uncertainty estimates. Our framework adapts the principle of stochastic data centering to graph data through novel graph anchoring strategies, and is able to support partially stochastic GNNs. While, the prevalent wisdom is that fully stochastic networks are necessary to obtain reliable estimates, we find that the functional diversity induced b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#28151;&#21512;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#32852;&#37030;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#28151;&#21512;&#24322;&#26500;&#24615;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.02702</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#26469;&#35299;&#20915;&#32852;&#37030;&#20248;&#21270;&#20013;&#30340;&#28151;&#21512;&#24322;&#26500;&#24615;
&lt;/p&gt;
&lt;p&gt;
Tackling Hybrid Heterogeneity on Federated Optimization via Gradient Diversity Maximization. (arXiv:2310.02702v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28151;&#21512;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#32852;&#37030;&#20248;&#21270;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#26469;&#20943;&#36731;&#28151;&#21512;&#24322;&#26500;&#24615;&#36127;&#38754;&#24433;&#21709;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#25968;&#25454;&#26679;&#26412;&#34987;&#20998;&#25955;&#21644;&#20998;&#24067;&#22312;&#22810;&#20010;&#23458;&#25143;&#31471;&#20043;&#38388;&#12290;&#36825;&#20123;&#26679;&#26412;&#21487;&#33021;&#34920;&#29616;&#20986;&#32479;&#35745;&#24322;&#36136;&#24615;&#65292;&#21363;&#25968;&#25454;&#20998;&#24067;&#22312;&#23458;&#25143;&#31471;&#20043;&#38388;&#19981;&#26159;&#29420;&#31435;&#21644;&#30456;&#21516;&#30340;&#12290;&#27492;&#22806;&#65292;&#31995;&#32479;&#24322;&#36136;&#24615;&#65292;&#21363;&#23458;&#25143;&#31471;&#35745;&#31639;&#33021;&#21147;&#30340;&#21464;&#21270;&#65292;&#20250;&#32473;&#32852;&#37030;&#23398;&#20064;&#24102;&#26469;&#20559;&#24046;&#12290;&#32479;&#35745;&#21644;&#31995;&#32479;&#24322;&#36136;&#24615;&#30340;&#32508;&#21512;&#25928;&#24212;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#32852;&#37030;&#20248;&#21270;&#30340;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#28151;&#21512;&#24322;&#26500;&#24615;&#30340;&#24433;&#21709;&#24182;&#27809;&#26377;&#24471;&#21040;&#20005;&#35880;&#30340;&#35752;&#35770;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#26381;&#21153;&#22120;&#31471;&#20248;&#21270;&#65292;&#25506;&#35752;&#20102;&#28151;&#21512;&#24322;&#26500;&#24615;&#22914;&#20309;&#24433;&#21709;&#32852;&#37030;&#20248;&#21270;&#12290;&#29702;&#35770;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#26381;&#21153;&#22120;&#26356;&#26032;&#26041;&#21521;&#19978;&#33258;&#36866;&#24212;&#22320;&#26368;&#22823;&#21270;&#26799;&#24230;&#22810;&#26679;&#24615;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#28151;&#21512;&#24322;&#26500;&#24615;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26381;&#21153;&#22120;&#31471;&#26799;&#24230;&#30340;&#20248;&#21270;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning refers to a distributed machine learning paradigm in which data samples are decentralized and distributed among multiple clients. These samples may exhibit statistical heterogeneity, which refers to data distributions are not independent and identical across clients. Additionally, system heterogeneity, or variations in the computational power of the clients, introduces biases into federated learning. The combined effects of statistical and system heterogeneity can significantly reduce the efficiency of federated optimization. However, the impact of hybrid heterogeneity is not rigorously discussed. This paper explores how hybrid heterogeneity affects federated optimization by investigating server-side optimization. The theoretical results indicate that adaptively maximizing gradient diversity in server update direction can help mitigate the potential negative consequences of hybrid heterogeneity. To this end, we introduce a novel server-side gradient-based optimizer \
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;MOLAR&#65292;&#23427;&#21033;&#29992;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24322;&#36136;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#29420;&#31435;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2306.06291</link><description>&lt;p&gt;
&#26368;&#20248;&#24322;&#26500;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal Heterogeneous Collaborative Linear Regression and Contextual Bandits. (arXiv:2306.06291v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#22120;MOLAR&#65292;&#23427;&#21033;&#29992;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#20013;&#30340;&#31232;&#30095;&#24322;&#36136;&#24615;&#26469;&#25552;&#39640;&#20272;&#35745;&#31934;&#24230;&#65292;&#24182;&#19988;&#30456;&#27604;&#29420;&#31435;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#21644;&#22797;&#26434;&#30340;&#25968;&#25454;&#38598;&#24448;&#24448;&#26469;&#33258;&#20110;&#20960;&#20010;&#21487;&#33021;&#26159;&#24322;&#26500;&#30340;&#26469;&#28304;&#12290;&#21327;&#21516;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#20849;&#24615;&#25552;&#39640;&#25928;&#29575;&#65292;&#21516;&#26102;&#32771;&#34385;&#21487;&#33021;&#20986;&#29616;&#30340;&#24046;&#24322;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#21327;&#21516;&#32447;&#24615;&#22238;&#24402;&#21644;&#19978;&#19979;&#25991;&#33218;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#20010;&#23454;&#20363;&#30340;&#30456;&#20851;&#21442;&#25968;&#31561;&#20110;&#20840;&#23616;&#21442;&#25968;&#21152;&#19978;&#19968;&#20010;&#31232;&#30095;&#30340;&#23454;&#20363;&#29305;&#23450;&#26415;&#35821;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MOLAR&#30340;&#26032;&#22411;&#20108;&#38454;&#27573;&#20272;&#35745;&#22120;&#65292;&#23427;&#36890;&#36807;&#39318;&#20808;&#26500;&#24314;&#23454;&#20363;&#32447;&#24615;&#22238;&#24402;&#20272;&#35745;&#30340;&#36880;&#39033;&#20013;&#20301;&#25968;&#65292;&#28982;&#21518;&#23558;&#23454;&#20363;&#29305;&#23450;&#20272;&#35745;&#20540;&#25910;&#32553;&#21040;&#20013;&#20301;&#25968;&#38468;&#36817;&#26469;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#12290;&#19982;&#29420;&#31435;&#26368;&#23567;&#20108;&#20056;&#20272;&#35745;&#30456;&#27604;&#65292;MOLAR&#25552;&#39640;&#20102;&#20272;&#35745;&#35823;&#24046;&#23545;&#25968;&#25454;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;MOLAR&#24212;&#29992;&#20110;&#24320;&#21457;&#29992;&#20110;&#31232;&#30095;&#24322;&#26500;&#21327;&#21516;&#19978;&#19979;&#25991;&#33218;&#30340;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#30456;&#27604;&#29420;&#31435;&#33218;&#27169;&#22411;&#20855;&#26377;&#26356;&#22909;&#30340;&#36951;&#25022;&#20445;&#35777;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#36129;&#29486;&#20248;&#20110;&#20808;&#21069;&#22312;&#25991;&#29486;&#20013;&#25253;&#36947;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large and complex datasets are often collected from several, possibly heterogeneous sources. Collaborative learning methods improve efficiency by leveraging commonalities across datasets while accounting for possible differences among them. Here we study collaborative linear regression and contextual bandits, where each instance's associated parameters are equal to a global parameter plus a sparse instance-specific term. We propose a novel two-stage estimator called MOLAR that leverages this structure by first constructing an entry-wise median of the instances' linear regression estimates, and then shrinking the instance-specific estimates towards the median. MOLAR improves the dependence of the estimation error on the data dimension, compared to independent least squares estimates. We then apply MOLAR to develop methods for sparsely heterogeneous collaborative contextual bandits, which lead to improved regret guarantees compared to independent bandit methods. We further show that our 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10769</link><description>&lt;p&gt;
&#36861;&#36214;&#33976;&#39311;&#65306;&#21152;&#36895;&#37319;&#26679;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Catch-Up Distillation: You Only Need to Train Once for Accelerating Sampling. (arXiv:2305.10769v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#36861;&#36214;&#33976;&#39311;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#20256;&#32479;&#37319;&#26679;&#31639;&#27861;&#65292;&#35753;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#21644;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#23545;&#40784;&#65292;&#20174;&#32780;&#23454;&#29616;&#21482;&#38656;&#19968;&#27425;&#35757;&#32451;&#20415;&#33021;&#21152;&#36895;&#37319;&#26679;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#26679;&#26412;&#65292;&#36890;&#24120;&#38656;&#35201;&#25191;&#34892;&#22823;&#37327;&#30340;&#37319;&#26679;&#27493;&#39588;&#65292;&#36825;&#38459;&#30861;&#20102;&#23454;&#26102;&#26679;&#26412;&#21512;&#25104;&#30340;&#21487;&#33021;&#24615;&#12290;&#20256;&#32479;&#30340;&#36890;&#36807;&#30693;&#35782;&#33976;&#39311;&#21152;&#36895;&#37319;&#26679;&#30340;&#31639;&#27861;&#20381;&#36182;&#20110;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#26435;&#37325;&#21644;&#31163;&#25955;&#26102;&#38388;&#27493;&#39588;&#22330;&#26223;&#65292;&#38656;&#35201;&#39069;&#22806;&#30340;&#22521;&#35757;&#35838;&#31243;&#25165;&#33021;&#23454;&#29616;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36861;&#36214;&#33976;&#39311;&#65288;CUD&#65289;&#65292;&#23427;&#40723;&#21169;&#36895;&#24230;&#20272;&#35745;&#27169;&#22411;&#30340;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#8220;&#36861;&#36214;&#8221;&#20854;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CUD&#35843;&#25972;&#20102;&#21407;&#22987;&#30340;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#35757;&#32451;&#30446;&#26631;&#65292;&#20197;&#20351;&#24403;&#21069;&#26102;&#21051;&#36755;&#20986;&#19982;&#22320;&#38754;&#30495;&#23454;&#26631;&#31614;&#21644;&#20808;&#21069;&#26102;&#21051;&#36755;&#20986;&#23545;&#40784;&#65292;&#21033;&#29992;&#22522;&#20110;&#40857;&#26684;-&#24211;&#22612;&#30340;&#22810;&#27493;&#23545;&#40784;&#33976;&#39311;&#36827;&#34892;&#31934;&#30830;&#30340;ODE&#20272;&#35745;&#65292;&#21516;&#26102;&#38450;&#27490;&#24322;&#27493;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion Probability Models (DPMs) have made impressive advancements in various machine learning domains. However, achieving high-quality synthetic samples typically involves performing a large number of sampling steps, which impedes the possibility of real-time sample synthesis. Traditional accelerated sampling algorithms via knowledge distillation rely on pre-trained model weights and discrete time step scenarios, necessitating additional training sessions to achieve their goals. To address these issues, we propose the Catch-Up Distillation (CUD), which encourages the current moment output of the velocity estimation model ``catch up'' with its previous moment output. Specifically, CUD adjusts the original Ordinary Differential Equation (ODE) training objective to align the current moment output with both the ground truth label and the previous moment output, utilizing Runge-Kutta-based multi-step alignment distillation for precise ODE estimation while preventing asynchronous updates
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#33014;&#22218;&#32593;&#32476;&#26550;&#26500;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#27861;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#21457;&#29616;&#20854;&#32534;&#30721;&#30340;&#34920;&#31034;&#21487;&#33021;&#19982;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#24182;&#19981;&#20005;&#26684;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2305.05349</link><description>&lt;p&gt;
&#26088;&#22312;&#34920;&#24449;&#22522;&#20110;&#33014;&#22218;&#32593;&#32476;&#26550;&#26500;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards the Characterization of Representations Learned via Capsule-based Network Architectures. (arXiv:2305.05349v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#33014;&#22218;&#32593;&#32476;&#26550;&#26500;&#23398;&#20064;&#30340;&#34920;&#31034;&#26041;&#27861;&#21450;&#20854;&#21487;&#35299;&#37322;&#24615;&#65292;&#21457;&#29616;&#20854;&#32534;&#30721;&#30340;&#34920;&#31034;&#21487;&#33021;&#19982;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#24182;&#19981;&#20005;&#26684;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33014;&#22218;&#32593;&#32476;&#20316;&#20026;&#26631;&#20934;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#19968;&#31181;&#26356;&#20026;&#32039;&#20945;&#21644;&#21487;&#35299;&#37322;&#30340;&#26367;&#20195;&#26041;&#27861;&#32780;&#37325;&#26032;&#24341;&#20837;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#30740;&#31350;&#35777;&#26126;&#20102;&#20854;&#21387;&#32553;&#33021;&#21147;&#65292;&#20294;&#33267;&#20170;&#23578;&#26410;&#23436;&#20840;&#35780;&#20272;&#20854;&#21487;&#35299;&#37322;&#24615;&#36136;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#31995;&#32479;&#32780;&#21407;&#21017;&#24615;&#30340;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;&#36825;&#31181;&#31867;&#22411;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29305;&#21035;&#27880;&#24847;&#20998;&#26512;&#25152;&#23398;&#21040;&#30340;&#34920;&#31034;&#20013;&#26159;&#21542;&#30830;&#23454;&#32534;&#30721;&#20102;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#30340;&#27700;&#24179;&#12290;&#22312;MNIST&#12289;SVHN&#12289;PASCAL-part&#21644;CelebA&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;CapsNets&#20013;&#32534;&#30721;&#30340;&#34920;&#31034;&#21487;&#33021;&#26082;&#19981;&#20687;&#25991;&#29486;&#20013;&#36890;&#24120;&#25152;&#36848;&#30340;&#37027;&#26679;&#20998;&#31163;&#65292;&#20063;&#19981;&#26159;&#20005;&#26684;&#19982;&#37096;&#20998;-&#25972;&#20307;&#20851;&#31995;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Capsule Networks (CapsNets) have been re-introduced as a more compact and interpretable alternative to standard deep neural networks. While recent efforts have proved their compression capabilities, to date, their interpretability properties have not been fully assessed. Here, we conduct a systematic and principled study towards assessing the interpretability of these types of networks. Moreover, we pay special attention towards analyzing the level to which part-whole relationships are indeed encoded within the learned representation. Our analysis in the MNIST, SVHN, PASCAL-part and CelebA datasets suggest that the representations encoded in CapsNets might not be as disentangled nor strictly related to parts-whole relationships as is commonly stated in the literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#21435;&#37327;&#23376;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#36866;&#24212;&#21040;&#36825;&#31181;&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#35777;&#26126;&#20102;&#26368;&#36817;&#30340;&#20302;&#31209;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#21644;spa&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2304.04932</link><description>&lt;p&gt;
&#37327;&#23376;&#22855;&#24322;&#20540;&#21464;&#25442;&#21450;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#21435;&#37327;&#23376;&#21270;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Robust Dequantization of the Quantum Singular value Transformation and Quantum Machine Learning Algorithms. (arXiv:2304.04932v1 [quant-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04932
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#40065;&#26834;&#21435;&#37327;&#23376;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36817;&#20284;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#36866;&#24212;&#21040;&#36825;&#31181;&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#35777;&#26126;&#20102;&#26368;&#36817;&#30340;&#20302;&#31209;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#21644;spa&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#24050;&#32463;&#26377;&#20960;&#31181;&#29992;&#20110;&#35299;&#20915;&#32447;&#24615;&#20195;&#25968;&#38382;&#39064;&#21644;&#29305;&#21035;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#37327;&#23376;&#31639;&#27861;&#34987;&#8220;&#21435;&#37327;&#23376;&#21270;&#8221;&#12290;&#36825;&#20123;&#21435;&#37327;&#23376;&#21270;&#32467;&#26524;&#36890;&#24120;&#22312;&#32463;&#20856;&#31639;&#27861;&#36890;&#36807;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#26041;&#27861;&#35775;&#38382;&#25968;&#25454;&#26102;&#25104;&#31435;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#21435;&#37327;&#23376;&#21270;&#32467;&#26524;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#36817;&#20284;&#38271;&#24230;&#24179;&#26041;&#37319;&#26679;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#32463;&#20856;&#31639;&#27861;&#21482;&#33021;&#20174;&#25509;&#36817;&#29702;&#24819;&#20998;&#24067;&#30340;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#34429;&#28982;&#37327;&#23376;&#31639;&#27861;&#22312;&#38754;&#23545;&#23567;&#25200;&#21160;&#26102;&#26412;&#36136;&#19978;&#26159;&#40065;&#26834;&#30340;&#65292;&#20294;&#24403;&#21069;&#30340;&#21435;&#37327;&#23376;&#21270;&#25216;&#26415;&#24182;&#19981;&#26159;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#25216;&#26415;&#36129;&#29486;&#22312;&#20110;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#35768;&#22810;&#38543;&#26426;&#32447;&#24615;&#20195;&#25968;&#25216;&#26415;&#36866;&#24212;&#21040;&#36825;&#31181;&#26356;&#24369;&#30340;&#20551;&#35774;&#19979;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#25216;&#26415;&#35777;&#26126;&#20102;&#26368;&#36817;&#30001;Chia&#12289;Gily\'en&#12289;Li&#12289;Lin&#12289;Tang&#21644;Wang&#65288;JACM 2022&#65289;&#25552;&#20986;&#30340;&#20302;&#31209;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#21644;&#29992;&#20110;spa&#30340;&#21435;&#37327;&#23376;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several quantum algorithms for linear algebra problems, and in particular quantum machine learning problems, have been "dequantized" in the past few years. These dequantization results typically hold when classical algorithms can access the data via length-squared sampling. In this work we investigate how robust these dequantization results are. We introduce the notion of approximate length-squared sampling, where classical algorithms are only able to sample from a distribution close to the ideal distribution in total variation distance. While quantum algorithms are natively robust against small perturbations, current techniques in dequantization are not. Our main technical contribution is showing how many techniques from randomized linear algebra can be adapted to work under this weaker assumption as well. We then use these techniques to show that the recent low-rank dequantization framework by Chia, Gily\'en, Li, Lin, Tang and Wang (JACM 2022) and the dequantization framework for spa
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#32435;&#20837;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#21453;&#21521;&#24037;&#31243;&#21644;&#35745;&#31639;&#26426;&#23433;&#20840;&#20219;&#21153;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2210.05102</link><description>&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Pre-Training Representations of Binary Code Using Contrastive Learning. (arXiv:2210.05102v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05102
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#39044;&#35757;&#32451;&#20108;&#36827;&#21046;&#20195;&#30721;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#32435;&#20837;&#20108;&#36827;&#21046;&#20195;&#30721;&#30340;&#34920;&#31034;&#23398;&#20064;&#20013;&#65292;&#23545;&#20110;&#21453;&#21521;&#24037;&#31243;&#21644;&#35745;&#31639;&#26426;&#23433;&#20840;&#20219;&#21153;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32534;&#35793;&#21518;&#30340;&#36719;&#20214;&#20197;&#21487;&#25191;&#34892;&#30340;&#20108;&#36827;&#21046;&#20195;&#30721;&#24418;&#24335;&#20132;&#20184;&#12290;&#24320;&#21457;&#20154;&#21592;&#32534;&#20889;&#28304;&#20195;&#30721;&#26469;&#34920;&#36798;&#36719;&#20214;&#30340;&#35821;&#20041;&#65292;&#20294;&#32534;&#35793;&#22120;&#23558;&#20854;&#36716;&#25442;&#20026;CPU&#21487;&#20197;&#30452;&#25509;&#25191;&#34892;&#30340;&#20108;&#36827;&#21046;&#26684;&#24335;&#12290;&#22240;&#27492;&#65292;&#20108;&#36827;&#21046;&#20195;&#30721;&#20998;&#26512;&#23545;&#20110;&#21453;&#21521;&#24037;&#31243;&#21644;&#35745;&#31639;&#26426;&#23433;&#20840;&#20219;&#21153;&#31561;&#27809;&#26377;&#28304;&#20195;&#30721;&#30340;&#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#19982;&#21253;&#21547;&#20016;&#23500;&#35821;&#20041;&#20449;&#24687;&#30340;&#28304;&#20195;&#30721;&#21644;&#33258;&#28982;&#35821;&#35328;&#19981;&#21516;&#65292;&#20108;&#36827;&#21046;&#20195;&#30721;&#36890;&#24120;&#38590;&#20197;&#29702;&#35299;&#21644;&#20998;&#26512;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#24037;&#20316;&#20351;&#29992;AI&#27169;&#22411;&#36741;&#21161;&#28304;&#20195;&#30721;&#20998;&#26512;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#32771;&#34385;&#20108;&#36827;&#21046;&#20195;&#30721;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#32435;&#20837;&#20108;&#36827;&#21046;&#20195;&#30721;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#31216;&#20026;COMBO&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#22312;COMBO&#20013;&#25552;&#20986;&#20102;&#19977;&#20010;&#32452;&#20214;&#65306;&#65288;1&#65289;&#29992;&#20110;&#20919;&#21551;&#21160;&#39044;&#35757;&#32451;&#30340;&#20027;&#35201;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#65288;2&#65289;&#29992;&#20110;&#23558;&#28304;&#20195;&#30721;&#21644;&#27880;&#37322;&#20449;&#24687;&#25554;&#20837;&#21040;&#20108;&#36827;&#21046;&#20195;&#30721;&#20013;&#30340;&#21333;&#32431;&#25554;&#20540;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compiled software is delivered as executable binary code. Developers write source code to express the software semantics, but the compiler converts it to a binary format that the CPU can directly execute. Therefore, binary code analysis is critical to applications in reverse engineering and computer security tasks where source code is not available. However, unlike source code and natural language that contain rich semantic information, binary code is typically difficult for human engineers to understand and analyze. While existing work uses AI models to assist source code analysis, few studies have considered binary code. In this paper, we propose a COntrastive learning Model for Binary cOde Analysis, or COMBO, that incorporates source code and comment information into binary code during representation learning. Specifically, we present three components in COMBO: (1) a primary contrastive learning method for cold-start pre-training, (2) a simplex interpolation method to incorporate so
&lt;/p&gt;</description></item></channel></rss>