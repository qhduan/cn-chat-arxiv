<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;</title><link>https://arxiv.org/abs/2402.16200</link><description>&lt;p&gt;
IR2&#65306;&#20449;&#24687;&#27491;&#21017;&#21270;&#29992;&#20110;&#20449;&#24687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
IR2: Information Regularization for Information Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16200
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;IR2&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#22312;&#22797;&#26434;&#26597;&#35810;&#30340;&#20449;&#24687;&#26816;&#32034;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#21516;&#26102;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#22320;&#22312;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20449;&#24687;&#26816;&#32034;&#65288;IR&#65289;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#22797;&#26434;&#26597;&#35810;&#65292;&#20173;&#28982;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;IR2&#65292;&#21363;&#20449;&#24687;&#26816;&#32034;&#30340;&#20449;&#24687;&#27491;&#21017;&#21270;&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20013;&#20943;&#23569;&#36807;&#25311;&#21512;&#30340;&#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#22312;&#20855;&#26377;&#22797;&#26434;&#26597;&#35810;&#29305;&#24449;&#30340;&#19977;&#20010;&#26368;&#36817;&#30340;IR&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65306;DORIS-MAE&#12289;ArguAna&#21644;WhatsThatBook&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#19981;&#20165;&#22312;&#25152;&#32771;&#34385;&#30340;&#20219;&#21153;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#21512;&#25104;&#26597;&#35810;&#29983;&#25104;&#26041;&#27861;&#65292;&#32780;&#19988;&#36824;&#33021;&#23558;&#25104;&#26412;&#38477;&#20302;&#39640;&#36798;50&#65285;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#23558;&#19981;&#21516;&#38454;&#27573;&#30340;&#19977;&#31181;&#27491;&#21017;&#21270;&#26041;&#27861;&#8212;&#8212;&#36755;&#20837;&#12289;&#25552;&#31034;&#21644;&#36755;&#20986;&#36827;&#34892;&#20102;&#20998;&#31867;&#21644;&#25506;&#32034;&#65292;&#27599;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#27809;&#26377;&#27491;&#21017;&#21270;&#30340;&#27169;&#22411;&#22343;&#25552;&#20379;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16200v1 Announce Type: cross  Abstract: Effective information retrieval (IR) in settings with limited training data, particularly for complex queries, remains a challenging task. This paper introduces IR2, Information Regularization for Information Retrieval, a technique for reducing overfitting during synthetic data generation. This approach, representing a novel application of regularization techniques in synthetic data creation for IR, is tested on three recent IR tasks characterized by complex queries: DORIS-MAE, ArguAna, and WhatsThatBook. Experimental results indicate that our regularization techniques not only outperform previous synthetic query generation methods on the tasks considered but also reduce cost by up to 50%. Furthermore, this paper categorizes and explores three regularization methods at different stages of the query synthesis pipeline-input, prompt, and output-each offering varying degrees of performance improvement compared to models where no regulariz
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#21547;&#21327;&#21464;&#37327;&#36716;&#31227;&#65288;LCS&#65289;&#33539;&#24335;&#65292;&#22686;&#21152;&#20102;&#39046;&#22495;&#38388;&#30340;&#21487;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24674;&#22797;&#26631;&#31614;&#21464;&#37327;&#28508;&#22312;&#21407;&#22240;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2208.14161</link><description>&lt;p&gt;
&#21487;&#35782;&#21035;&#30340;&#28508;&#22312;&#22240;&#26524;&#20869;&#23481;&#29992;&#20110;&#38544;&#21547;&#21327;&#21464;&#37327;&#36716;&#31227;&#19979;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Identifiable Latent Causal Content for Domain Adaptation under Latent Covariate Shift
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2208.14161
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38544;&#21547;&#21327;&#21464;&#37327;&#36716;&#31227;&#65288;LCS&#65289;&#33539;&#24335;&#65292;&#22686;&#21152;&#20102;&#39046;&#22495;&#38388;&#30340;&#21487;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#24674;&#22797;&#26631;&#31614;&#21464;&#37327;&#28508;&#22312;&#21407;&#22240;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#28304;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;MSDA&#65289;&#35299;&#20915;&#20102;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#28304;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#21644;&#26469;&#33258;&#30446;&#26631;&#22495;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#23398;&#20064;&#38024;&#23545;&#26410;&#26631;&#35760;&#30446;&#26631;&#39046;&#22495;&#30340;&#26631;&#31614;&#39044;&#27979;&#20989;&#25968;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#28508;&#22312;&#21327;&#21464;&#37327;&#36716;&#31227;&#65288;LCS&#65289;&#30340;&#26032;&#33539;&#24335;&#65292;&#23427;&#24341;&#20837;&#20102;&#26356;&#22823;&#30340;&#39046;&#22495;&#38388;&#21487;&#21464;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23427;&#20026;&#24674;&#22797;&#26631;&#31614;&#21464;&#37327;&#30340;&#28508;&#22312;&#21407;&#22240;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2208.14161v3 Announce Type: replace  Abstract: Multi-source domain adaptation (MSDA) addresses the challenge of learning a label prediction function for an unlabeled target domain by leveraging both the labeled data from multiple source domains and the unlabeled data from the target domain. Conventional MSDA approaches often rely on covariate shift or conditional shift paradigms, which assume a consistent label distribution across domains. However, this assumption proves limiting in practical scenarios where label distributions do vary across domains, diminishing its applicability in real-world settings. For example, animals from different regions exhibit diverse characteristics due to varying diets and genetics.   Motivated by this, we propose a novel paradigm called latent covariate shift (LCS), which introduces significantly greater variability and adaptability across domains. Notably, it provides a theoretical assurance for recovering the latent cause of the label variable, w
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#27169;&#22411;&#20197;&#35299;&#20915;&#22810;&#25945;&#24072;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#39046;&#22495;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#38388;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.15288</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#22522;&#20110;&#20154;&#31867;&#21453;&#39304;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Active teacher selection for reinforcement learning from human feedback. (arXiv:2310.15288v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15288
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#27169;&#22411;&#20197;&#35299;&#20915;&#22810;&#25945;&#24072;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#30740;&#31350;&#34920;&#26126;&#35813;&#27169;&#22411;&#22312;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#39046;&#22495;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#65292;&#24182;&#25581;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#38388;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#20351;&#24471;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#33021;&#22815;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#30446;&#26631;&#12290;&#36825;&#20123;&#31995;&#32479;&#30340;&#19968;&#20010;&#26680;&#24515;&#38480;&#21046;&#26159;&#23427;&#20204;&#20551;&#35774;&#25152;&#26377;&#21453;&#39304;&#37117;&#26469;&#33258;&#19968;&#20010;&#21333;&#19968;&#30340;&#20154;&#31867;&#25945;&#24072;&#65292;&#23613;&#31649;&#38656;&#35201;&#35810;&#38382;&#19981;&#21516;&#25945;&#24072;&#30340;&#24847;&#35265;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;"Hidden Utility Bandit"&#65288;HUB&#65289;&#26694;&#26550;&#26469;&#24314;&#27169;&#25945;&#24072;&#22312;&#29702;&#24615;&#12289;&#19987;&#19994;&#30693;&#35782;&#21644;&#25104;&#26412;&#26041;&#38754;&#30340;&#24046;&#24322;&#65292;&#20174;&#32780;&#24418;&#24335;&#21270;&#20102;&#20174;&#22810;&#20010;&#25945;&#24072;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#35299;&#20915;&#31639;&#27861;&#65292;&#24182;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#20004;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#39046;&#22495;&#65306;&#35770;&#25991;&#25512;&#33616;&#31995;&#32479;&#21644;COVID-19&#30123;&#33495;&#27979;&#35797;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;"Active Teacher Selection"&#65288;ATS&#65289;&#31639;&#27861;&#36890;&#36807;&#20027;&#21160;&#36873;&#25321;&#20309;&#26102;&#20197;&#21450;&#36873;&#25321;&#21738;&#20010;&#25945;&#24072;&#26469;&#26597;&#35810;&#65292;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;HUB&#26694;&#26550;&#21644;ATS&#31639;&#27861;&#23637;&#31034;&#20102;&#21033;&#29992;&#25945;&#24072;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#23398;&#20064;&#20934;&#30830;&#30340;&#22870;&#21169;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#65292;&#20026;&#40065;&#26834;&#22870;&#21169;&#24314;&#27169;&#30340;&#20027;&#21160;&#25945;&#24072;&#36873;&#25321;&#30340;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning from human feedback (RLHF) enables machine learning systems to learn objectives from human feedback. A core limitation of these systems is their assumption that all feedback comes from a single human teacher, despite querying a range of distinct teachers. We propose the Hidden Utility Bandit (HUB) framework to model differences in teacher rationality, expertise, and costliness, formalizing the problem of learning from multiple teachers. We develop a variety of solution algorithms and apply them to two real-world domains: paper recommendation systems and COVID-19 vaccine testing. We find that the Active Teacher Selection (ATS) algorithm outperforms baseline algorithms by actively selecting when and which teacher to query. The HUB framework and ATS algorithm demonstrate the importance of leveraging differences between teachers to learn accurate reward models, facilitating future research on active teacher selection for robust reward modeling.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.14474</link><description>&lt;p&gt;
&#27827;&#24029;&#23398;&#20064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Limits to Reservoir Learning. (arXiv:2307.14474v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.14474
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#38480;&#21046;&#20102;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#22522;&#20110;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#12290;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#22312;&#22122;&#22768;&#19979;&#30340;&#24615;&#33021;&#19979;&#38477;&#24847;&#21619;&#30528;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#23398;&#20064;&#20989;&#25968;&#26063;&#65292;&#24182;&#35752;&#35770;&#20102;&#27809;&#26377;&#22122;&#22768;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26681;&#25454;&#29289;&#29702;&#23398;&#25152;&#26263;&#31034;&#30340;&#35745;&#31639;&#38480;&#21046;&#26469;&#38480;&#21046;&#26426;&#22120;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20449;&#24687;&#22788;&#29702;&#33021;&#21147;&#65288;IPC&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#23545;&#20449;&#21495;&#38598;&#21512;&#21040;&#23436;&#25972;&#20989;&#25968;&#22522;&#30340;&#26399;&#26395;&#24179;&#26041;&#35823;&#24046;&#36827;&#34892;&#24402;&#19968;&#21270;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#20351;&#29992;IPC&#26469;&#34913;&#37327;&#22122;&#22768;&#19979;&#20648;&#27700;&#24211;&#35745;&#31639;&#26426;&#65288;&#19968;&#31181;&#29305;&#27530;&#30340;&#24490;&#29615;&#32593;&#32476;&#65289;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;IPC&#22312;&#31995;&#32479;&#23610;&#23544;n&#19978;&#26159;&#19968;&#20010;&#22810;&#39033;&#24335;&#65292;&#21363;&#20351;&#32771;&#34385;&#21040;n&#20010;&#36755;&#20986;&#20449;&#21495;&#30340;$2^n$&#20010;&#21487;&#33021;&#30340;&#36880;&#28857;&#20056;&#31215;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#31181;&#36864;&#21270;&#24847;&#21619;&#30528;&#22312;&#20648;&#27700;&#24211;&#22122;&#22768;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#20648;&#27700;&#24211;&#25152;&#34920;&#31034;&#30340;&#20989;&#25968;&#26063;&#38656;&#35201;&#25351;&#25968;&#25968;&#37327;&#30340;&#26679;&#26412;&#26469;&#36827;&#34892;&#23398;&#20064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#22312;&#27809;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#21516;&#19968;&#38598;&#21512;&#30340;$2^n$&#20010;&#20989;&#25968;&#22312;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we bound a machine's ability to learn based on computational limitations implied by physicality. We start by considering the information processing capacity (IPC), a normalized measure of the expected squared error of a collection of signals to a complete basis of functions. We use the IPC to measure the degradation under noise of the performance of reservoir computers, a particular kind of recurrent network, when constrained by physical considerations. First, we show that the IPC is at most a polynomial in the system size $n$, even when considering the collection of $2^n$ possible pointwise products of the $n$ output signals. Next, we argue that this degradation implies that the family of functions represented by the reservoir requires an exponential number of samples to learn in the presence of the reservoir's noise. Finally, we conclude with a discussion of the performance of the same collection of $2^n$ functions without noise when being used for binary classification
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#20855;&#26377;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#21487;&#35777;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#38480;&#21046;&#20102;&#30446;&#26631;&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.06986</link><description>&lt;p&gt;
&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#20013;&#38750;&#32447;&#24615;&#29305;&#24449;&#23398;&#20064;&#30340;&#21487;&#35777;&#20445;&#35777;
&lt;/p&gt;
&lt;p&gt;
Provable Guarantees for Nonlinear Feature Learning in Three-Layer Neural Networks. (arXiv:2305.06986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#23427;&#20855;&#26377;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#21487;&#35777;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#38480;&#21046;&#20102;&#30446;&#26631;&#32467;&#26500;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#20302;&#27979;&#35797;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#29702;&#35770;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#26159;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#23398;&#20064;&#20998;&#23618;&#29305;&#24449;&#12290;&#28145;&#24230;&#32593;&#32476;&#25552;&#21462;&#26174;&#33879;&#29305;&#24449;&#30340;&#33021;&#21147;&#23545;&#20854;&#21331;&#36234;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#33539;&#24335;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20173;&#28982;&#19981;&#22815;&#28165;&#26224;&#65292;&#29616;&#26377;&#30340;&#20998;&#26512;&#20027;&#35201;&#23616;&#38480;&#20110;&#20004;&#23618;&#32593;&#32476;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19977;&#23618;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#35777;&#26126;&#30340;&#27604;&#20004;&#23618;&#32593;&#32476;&#26356;&#20016;&#23500;&#30340;&#29305;&#24449;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36890;&#36807;&#36880;&#23618;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#19977;&#23618;&#32593;&#32476;&#23398;&#20064;&#30340;&#29305;&#24449;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#23450;&#29702;&#65292;&#23427;&#19978;&#30028;&#20102;&#30446;&#26631;&#20855;&#26377;&#29305;&#23450;&#23618;&#27425;&#32467;&#26500;&#26102;&#23454;&#29616;&#20302;&#27979;&#35797;&#38169;&#35823;&#25152;&#38656;&#30340;&#26679;&#26412;&#22797;&#26434;&#24230;&#21644;&#23485;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#20363;&#21270;&#21040;&#29305;&#23450;&#30340;&#32479;&#35745;&#23398;&#23398;&#20064;&#35774;&#32622;&#20013;&#8212;&#8212;&#21333;&#25351;&#25968;&#27169;&#22411;&#21644;&#20108;&#27425;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the central questions in the theory of deep learning is to understand how neural networks learn hierarchical features. The ability of deep networks to extract salient features is crucial to both their outstanding generalization ability and the modern deep learning paradigm of pretraining and finetuneing. However, this feature learning process remains poorly understood from a theoretical perspective, with existing analyses largely restricted to two-layer networks. In this work we show that three-layer neural networks have provably richer feature learning capabilities than two-layer networks. We analyze the features learned by a three-layer network trained with layer-wise gradient descent, and present a general purpose theorem which upper bounds the sample complexity and width needed to achieve low test error when the target has specific hierarchical structure. We instantiate our framework in specific statistical learning settings -- single-index models and functions of quadratic 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2210.13455</link><description>&lt;p&gt;
E-MCTS&#65306;&#36890;&#36807;&#35268;&#21010;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#28145;&#24230;&#25506;&#32034;&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
E-MCTS: Deep Exploration in Model-Based Reinforcement Learning by Planning with Epistemic Uncertainty. (arXiv:2210.13455v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;E-MCTS&#65292;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#24212;&#29992;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28145;&#24230;&#25506;&#32034;&#65292;&#20197;&#21450;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#36825;&#31181;&#26041;&#27861;&#22312;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#21644;&#28145;&#24230;&#25506;&#32034;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#25311;&#36864;&#28779;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#26159;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#20013;&#24212;&#29992;&#26368;&#24191;&#27867;&#12289;&#24615;&#33021;&#26368;&#20248;&#31168;&#30340;&#35268;&#21010;&#26041;&#27861;&#20043;&#19968;&#12290;MCTS&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#28145;&#24230;&#25506;&#32034;&#21644;&#38754;&#23545;&#26410;&#30693;&#26102;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#20004;&#20010;&#25361;&#25112;&#21487;&#20197;&#36890;&#36807;&#22312;MCTS&#39044;&#27979;&#20013;&#20351;&#29992;&#21407;&#21017;&#24615;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26469;&#32531;&#35299;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22312;MCTS&#20013;&#20256;&#25773;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#20272;&#35745;&#20854;&#39044;&#27979;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21033;&#29992;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#28145;&#24230;&#25506;&#32034;&#31639;&#27861;&#65292;&#36890;&#36807;&#26126;&#30830;&#35268;&#21010;&#25506;&#32034;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#24212;&#29992;&#20110;&#22522;&#20110;MCTS&#30340;&#27169;&#22411;&#22522;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#21253;&#25324;&#20351;&#29992;&#23398;&#20064;&#21644;&#25552;&#20379;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#25104;&#21151;&#30340;&#34920;&#35266;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24182;&#36827;&#34892;&#20102;&#28145;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#23558;&#20854;&#19982;&#22522;&#20110;&#38750;&#35268;&#21010;&#30340;&#28145;&#24230;&#25506;&#32034;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
One of the most well-studied and highly performing planning approaches used in Model-Based Reinforcement Learning (MBRL) is Monte-Carlo Tree Search (MCTS). Key challenges of MCTS-based MBRL methods remain dedicated deep exploration and reliability in the face of the unknown, and both challenges can be alleviated through principled epistemic uncertainty estimation in the predictions of MCTS. We present two main contributions: First, we develop methodology to propagate epistemic uncertainty in MCTS, enabling agents to estimate the epistemic uncertainty in their predictions. Second, we utilize the propagated uncertainty for a novel deep exploration algorithm by explicitly planning to explore. We incorporate our approach into variations of MCTS-based MBRL approaches with learned and provided models, and empirically show deep exploration through successful epistemic uncertainty estimation achieved by our approach. We compare to a non-planning-based deep-exploration baseline, and demonstrate
&lt;/p&gt;</description></item></channel></rss>