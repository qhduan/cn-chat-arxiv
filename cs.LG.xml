<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#30740;&#31350;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#38024;&#23545;&#19981;&#21516;&#24773;&#24418;&#25552;&#20986;&#20102;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#24322;&#36136;&#20195;&#29702;&#12289;&#21516;&#36136;&#20195;&#29702;&#21644;&#38750;&#21333;&#32431;&#35270;&#35282;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.07143</link><description>&lt;p&gt;
&#22312;&#32447;&#21512;&#21516;&#35774;&#35745;&#30340;&#26032;&#35270;&#35282;&#65306;&#24322;&#36136;&#12289;&#21516;&#36136;&#12289;&#38750;&#21333;&#32431;&#35270;&#35282;&#20195;&#29702;&#21644;&#22242;&#38431;&#29983;&#20135;
&lt;/p&gt;
&lt;p&gt;
New Perspectives in Online Contract Design: Heterogeneous, Homogeneous, Non-myopic Agents and Team Production
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#65292;&#38024;&#23545;&#19981;&#21516;&#24773;&#24418;&#25552;&#20986;&#20102;&#35774;&#35745;&#23398;&#20064;&#31639;&#27861;&#30340;&#19981;&#21516;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#24322;&#36136;&#20195;&#29702;&#12289;&#21516;&#36136;&#20195;&#29702;&#21644;&#38750;&#21333;&#32431;&#35270;&#35282;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20174;&#22312;&#32447;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#37325;&#22797;&#30340;&#22996;&#25176;-&#20195;&#29702;&#38382;&#39064;&#12290; &#22996;&#25176;&#26041;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#37325;&#22797;&#20114;&#21160;&#23398;&#20064;&#26368;&#22823;&#21270;&#20854;&#25928;&#29992;&#30340;&#26368;&#20339;&#21512;&#21516;&#65292;&#32780;&#27809;&#26377;&#20851;&#20110;&#20195;&#29702;&#26041;&#31867;&#22411;&#65288;&#21363;&#20195;&#29702;&#26041;&#30340;&#25104;&#26412;&#21644;&#29983;&#20135;&#20989;&#25968;&#65289;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290; &#25105;&#30740;&#31350;&#20102;&#19977;&#31181;&#19981;&#21516;&#30340;&#24773;&#22659;&#65292;&#22996;&#25176;&#26041;&#22312;&#27599;&#19968;&#36718;&#19982;$\textit{&#21333;&#20010;}$&#20195;&#29702;&#26041;&#31614;&#35746;&#21512;&#21516;&#26102;&#65306;1. &#20195;&#29702;&#26041;&#26159;&#24322;&#36136;&#30340;&#65307;2. &#20195;&#29702;&#26041;&#26159;&#21516;&#36136;&#30340;&#65307;3. &#22996;&#25176;&#26041;&#19982;&#30456;&#21516;&#30340;&#20195;&#29702;&#26041;&#20114;&#21160;&#19988;&#35813;&#20195;&#29702;&#26041;&#26159;&#38750;&#21333;&#32431;&#30340;&#12290; &#25105;&#25552;&#20986;&#19981;&#21516;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#26469;&#35774;&#35745;&#27599;&#31181;&#24773;&#20917;&#19979;&#30340;&#23398;&#20064;&#31639;&#27861;&#12290; &#23545;&#20110;&#24322;&#36136;&#20195;&#29702;&#31867;&#22411;&#65292;&#25105;&#30830;&#23450;&#20102;&#19968;&#20010;&#26465;&#20214;&#65292;&#20801;&#35768;&#23558;&#38382;&#39064;&#30452;&#25509;&#31616;&#21270;&#20026;Lipschitz&#32769;&#34382;&#26426;&#38382;&#39064;&#12290; &#23545;&#20110;&#30456;&#21516;&#20195;&#29702;&#26041;&#65292;&#25105;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#36870;&#21338;&#24328;&#35770;&#30340;&#22810;&#39033;&#24335;&#26679;&#26412;&#22797;&#26434;&#24230;&#26041;&#26696;&#26469;&#23398;&#20064;&#26368;&#20339;&#21512;&#21516;&#12290; &#23545;&#20110;&#25112;&#30053;&#24615;&#38750;&#21333;&#32431;&#20195;&#29702;&#65292;&#25105;&#35774;&#35745;&#20102;&#19968;&#20010;&#20302;&#25112;&#30053;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07143v1 Announce Type: cross  Abstract: This work studies the repeated principal-agent problem from an online learning perspective. The principal's goal is to learn the optimal contract that maximizes her utility through repeated interactions, without prior knowledge of the agent's type (i.e., the agent's cost and production functions).   I study three different settings when the principal contracts with a $\textit{single}$ agent each round: 1. The agents are heterogeneous; 2. the agents are homogenous; 3. the principal interacts with the same agent and the agent is non-myopic. I present different approaches and techniques for designing learning algorithms in each setting. For heterogeneous agent types, I identify a condition that allows the problem to be reduced to Lipschitz bandits directly. For identical agents, I give a polynomial sample complexity scheme to learn the optimal contract based on inverse game theory. For strategic non-myopic agents, I design a low strategic
&lt;/p&gt;</description></item><item><title>&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#39057;&#35889;&#30340;&#26497;&#22823;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#26497;&#22823;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23384;&#22312;&#20445;&#25345;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#65292;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#23545;&#31216;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14515</link><description>&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#21644;&#26497;&#22823;&#24615;&#36136;
&lt;/p&gt;
&lt;p&gt;
Spectral invariance and maximality properties of the frequency spectrum of quantum neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14515
&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20102;&#39057;&#35889;&#30340;&#26497;&#22823;&#24615;&#36136;&#65292;&#35777;&#26126;&#20102;&#22312;&#19968;&#31867;&#27169;&#22411;&#20013;&#23384;&#22312;&#26497;&#22823;&#32467;&#26524;&#65292;&#20197;&#21450;&#22312;&#19968;&#20123;&#26465;&#20214;&#19979;&#23384;&#22312;&#20445;&#25345;&#39057;&#35889;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#65292;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#35266;&#23519;&#21040;&#30340;&#32467;&#26524;&#23545;&#31216;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#65288;QNNs&#65289;&#26159;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#28909;&#38376;&#26041;&#27861;&#65292;&#30001;&#20110;&#20854;&#19982;&#21464;&#20998;&#37327;&#23376;&#30005;&#36335;&#30340;&#23494;&#20999;&#32852;&#31995;&#65292;&#20351;&#20854;&#25104;&#20026;&#22312;&#22122;&#22768;&#20013;&#38388;&#23610;&#24230;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#19978;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#30340;&#26377;&#21069;&#36884;&#30340;&#20505;&#36873;&#26041;&#27861;&#12290;QNN&#21487;&#20197;&#34920;&#31034;&#20026;&#26377;&#38480;&#20613;&#37324;&#21494;&#32423;&#25968;&#65292;&#20854;&#20013;&#39057;&#29575;&#38598;&#34987;&#31216;&#20026;&#39057;&#35889;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#39057;&#35889;&#24182;&#35777;&#26126;&#65292;&#23545;&#20110;&#19968;&#22823;&#31867;&#27169;&#22411;&#65292;&#23384;&#22312;&#21508;&#31181;&#26497;&#22823;&#24615;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#22312;&#19968;&#20123;&#28201;&#21644;&#26465;&#20214;&#19979;&#65292;&#23384;&#22312;&#19968;&#20010;&#20445;&#25345;&#39057;&#35889;&#30340;&#20855;&#26377;&#30456;&#21516;&#38754;&#31215;$A = RL$&#30340;&#27169;&#22411;&#31867;&#20043;&#38388;&#30340;&#21452;&#23556;&#65292;&#20854;&#20013;$R$&#34920;&#31034;&#37327;&#23376;&#27604;&#29305;&#25968;&#37327;&#65292;$L$&#34920;&#31034;&#23618;&#25968;&#65292;&#25105;&#20204;&#22240;&#27492;&#31216;&#20043;&#20026;&#38754;&#31215;&#20445;&#25345;&#21464;&#25442;&#19979;&#30340;&#20809;&#35889;&#19981;&#21464;&#24615;&#12290;&#36890;&#36807;&#36825;&#20010;&#65292;&#25105;&#20204;&#35299;&#37322;&#20102;&#25991;&#29486;&#20013;&#32463;&#24120;&#35266;&#23519;&#21040;&#30340;&#22312;&#32467;&#26524;&#20013;$R$&#21644;$L$&#30340;&#23545;&#31216;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#26368;&#22823;&#39057;&#35889;&#30340;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14515v1 Announce Type: cross  Abstract: Quantum Neural Networks (QNNs) are a popular approach in Quantum Machine Learning due to their close connection to Variational Quantum Circuits, making them a promising candidate for practical applications on Noisy Intermediate-Scale Quantum (NISQ) devices. A QNN can be expressed as a finite Fourier series, where the set of frequencies is called the frequency spectrum. We analyse this frequency spectrum and prove, for a large class of models, various maximality results. Furthermore, we prove that under some mild conditions there exists a bijection between classes of models with the same area $A = RL$ that preserves the frequency spectrum, where $R$ denotes the number of qubits and $L$ the number of layers, which we consequently call spectral invariance under area-preserving transformations. With this we explain the symmetry in $R$ and $L$ in the results often observed in the literature and show that the maximal frequency spectrum depen
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#22312;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20016;&#23500;&#30340;&#28304;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.04416</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Data Centric Approach for Unsupervised Domain Generalization via Retrieval from Web Scale Multimodal Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04416
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22522;&#20110;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#25968;&#25454;&#26816;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#30340;&#25968;&#25454;&#20013;&#24515;&#26041;&#27861;&#12290;&#22312;&#22810;&#27169;&#24577;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;&#38382;&#39064;&#20013;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20016;&#23500;&#30340;&#28304;&#25968;&#25454;&#65292;&#26469;&#35299;&#20915;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#25968;&#25454;&#33719;&#21462;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;(DG)&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#25110;&#22810;&#20010;&#28304;&#39046;&#22495;&#22312;&#20849;&#20139;&#26631;&#31614;&#31354;&#38388;&#30340;&#20551;&#35774;&#19979;&#23398;&#20064;&#19968;&#20010;&#33021;&#22815;&#25512;&#24191;&#21040;&#26410;&#35265;&#27979;&#35797;&#39046;&#22495;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;DG&#26041;&#27861;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#20016;&#23500;&#30340;&#30446;&#26631;&#26631;&#31614;&#31354;&#38388;&#20013;&#30340;&#28304;&#25968;&#25454;&#65292;&#36825;&#20010;&#35201;&#27714;&#22312;&#35768;&#22810;&#29616;&#23454;&#24212;&#29992;&#20013;&#22826;&#36807;&#20005;&#26684;&#65292;&#22240;&#20026;&#33719;&#21462;&#19982;&#30446;&#26631;&#20219;&#21153;&#30456;&#21516;&#30340;&#26631;&#31614;&#31354;&#38388;&#36153;&#29992;&#39640;&#26114;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22788;&#29702;&#20102;&#26080;&#30417;&#30563;&#39046;&#22495;&#27867;&#21270;(UDG)&#38382;&#39064;&#30340;&#22810;&#27169;&#24577;&#29256;&#26412;&#65292;&#35813;&#38382;&#39064;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#26410;&#26631;&#35760;&#30340;&#28304;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;LAION-2B&#22312;&#24494;&#35843;&#26399;&#38388;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#19981;&#26174;&#24335;&#22320;&#20551;&#35774;&#28304;&#25968;&#25454;&#38598;&#19982;&#30446;&#26631;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20219;&#20309;&#20851;&#31995;&#12290;&#30456;&#21453;&#65292;&#23427;&#21482;&#20381;&#36182;&#20110;&#28304;&#25968;&#25454;&#38598;&#21487;&#20197;&#22312;&#32852;&#21512;&#35270;&#35273;-&#35821;&#35328;&#31354;&#38388;&#20013;&#39640;&#25928;&#25628;&#32034;&#30340;&#21069;&#25552;&#12290;&#38024;&#23545;&#36825;&#31181;&#22810;&#27169;&#24577;UDG&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#19968;&#20010;&#23567;&#22411;&#65288;&#23567;&#20110;100K&#65289;&#30340;&#28304;&#25968;&#25454;&#23376;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain generalization (DG) is an important problem that learns a model that can generalize to unseen test domains leveraging one or more source domains, under the assumption of shared label spaces. However, most DG methods assume access to abundant source data in the target label space, a requirement that proves overly stringent for numerous real-world applications, where acquiring the same label space as the target task is prohibitively expensive. For this setting, we tackle the multimodal version of the unsupervised domain generalization (UDG) problem, which uses a large task-agnostic unlabeled source dataset, such as LAION-2B during finetuning. Our framework does not explicitly assume any relationship between the source dataset and target task. Instead, it relies only on the premise that the source dataset can be efficiently searched in a joint vision-language space. For this multimodal UDG setting, we propose a novel method to build a small ($&lt;$100K) subset of the source data in th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;</title><link>https://arxiv.org/abs/2401.17177</link><description>&lt;p&gt;
&#25968;&#25454;&#39537;&#21160;&#30340;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#21457;&#29616;&#20559;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Data-Driven Discovery of PDEs via the Adjoint Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17177
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#30340;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#65292;&#20294;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20934;&#30830;&#24615;&#19982;PDE-FIND&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20276;&#38543;&#26041;&#27861;&#26469;&#21457;&#29616;&#32473;&#23450;&#25968;&#25454;&#30340;&#28508;&#22312;&#20559;&#24494;&#20998;&#26041;&#31243;&#65288;PDEs&#65289;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#24605;&#36335;&#26159;&#20197;&#19968;&#33324;&#24418;&#24335;&#32771;&#34385;&#21442;&#25968;&#21270;&#30340;PDE&#65292;&#24182;&#21046;&#23450;&#26368;&#23567;&#21270;PDE&#35299;&#19982;&#25968;&#25454;&#35823;&#24046;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;&#21033;&#29992;&#21464;&#20998;&#35745;&#31639;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#25289;&#26684;&#26391;&#26085;&#20056;&#23376;&#65288;&#20276;&#38543;&#26041;&#31243;&#65289;&#30340;&#28436;&#21270;&#26041;&#31243;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#30452;&#25509;&#35745;&#31639;&#20986;&#19982;PDE&#21442;&#25968;&#30456;&#20851;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#26799;&#24230;&#12290;&#29305;&#21035;&#26159;&#23545;&#20110;&#19968;&#26063;&#21442;&#25968;&#21270;&#21644;&#38750;&#32447;&#24615;PDEs&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#25512;&#23548;&#20986;&#30456;&#24212;&#30340;&#20276;&#38543;&#26041;&#31243;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#65292;&#22312;&#32473;&#23450;&#20809;&#28369;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25152;&#25552;&#20986;&#30340;&#20276;&#38543;&#26041;&#27861;&#21487;&#20197;&#20197;&#26426;&#22120;&#31934;&#24230;&#24674;&#22797;&#30495;&#23454;&#30340;PDE&#12290;&#28982;&#32780;&#65292;&#22312;&#23384;&#22312;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#65292;&#20276;&#38543;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#19982;&#33879;&#21517;&#30340;PDE-FIND&#65288;Rudy et al., 2017&#65289;&#26041;&#27861;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form, and formulate the optimization problem that minimizes the error of PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, for a family of parameterized and nonlinear PDEs, we show how the corresponding adjoint equations can be derived. Here, we show that given smooth data set, the proposed adjoint method can recover the true PDE up to machine accuracy. However, in the presence of noise, the accuracy of the adjoint method becomes comparable to the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017). Even th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#30340;&#24615;&#33021;&#21487;&#20197;&#24471;&#20197;&#25552;&#21319;&#65292;&#32780;&#19981;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2401.11576</link><description>&lt;p&gt;
&#21033;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Quantum Architecture Search with Unsupervised Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11576
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#30340;&#24615;&#33021;&#21487;&#20197;&#24471;&#20197;&#25552;&#21319;&#65292;&#32780;&#19981;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#36827;&#34892;&#37327;&#23376;&#26550;&#26500;&#25628;&#32034;&#65288;QAS&#65289;&#20195;&#34920;&#20102;&#19968;&#31181;&#21069;&#27839;&#26041;&#27861;&#65292;&#26377;&#26395;&#22312;&#22024;&#26434;&#30340;&#20013;&#38388;&#35268;&#27169;&#37327;&#23376;&#65288;NISQ&#65289;&#35774;&#22791;&#19978;&#23454;&#29616;&#28508;&#22312;&#30340;&#37327;&#23376;&#20248;&#21183;&#12290;&#22823;&#22810;&#25968;QAS&#31639;&#27861;&#23558;&#23427;&#20204;&#30340;&#25628;&#32034;&#31354;&#38388;&#21644;&#25628;&#32034;&#31639;&#27861;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#22240;&#27492;&#36890;&#24120;&#38656;&#35201;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#35780;&#20272;&#22823;&#37327;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#22522;&#20110;&#39044;&#27979;&#30340;QAS&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#26681;&#25454;&#30005;&#36335;&#32467;&#26500;&#20272;&#35745;&#30005;&#36335;&#30340;&#24615;&#33021;&#26469;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#39640;&#24615;&#33021;&#30340;&#39044;&#27979;&#22120;&#36890;&#24120;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#26631;&#35760;&#65292;&#20197;&#33719;&#24471;&#22823;&#37327;&#24102;&#26631;&#31614;&#30340;&#37327;&#23376;&#30005;&#36335;&#12290;&#26368;&#36817;&#65292;&#19968;&#20010;&#32463;&#20856;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;Arch2vec&#21551;&#21457;&#25105;&#20204;&#65292;&#34920;&#26126;&#26550;&#26500;&#25628;&#32034;&#21487;&#20197;&#20174;&#23558;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#19982;&#25628;&#32034;&#36807;&#31243;&#20998;&#31163;&#20013;&#33719;&#30410;&#12290;&#26080;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#26159;&#21542;&#33021;&#24110;&#21161;QAS
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11576v2 Announce Type: replace-cross  Abstract: Utilizing unsupervised representation learning for quantum architecture search (QAS) represents a cutting-edge approach poised to realize potential quantum advantage on Noisy Intermediate-Scale Quantum (NISQ) devices. Most QAS algorithms combine their search space and search algorithms together and thus generally require evaluating a large number of quantum circuits during the search process. Predictor-based QAS algorithms can alleviate this problem by directly estimating the performance of circuits according to their structures. However, a high-performance predictor generally requires very time-consuming labeling to obtain a large number of labeled quantum circuits. Recently, a classical neural architecture search algorithm Arch2vec inspires us by showing that architecture search can benefit from decoupling unsupervised representation learning from the search process. Whether unsupervised representation learning can help QAS w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2001.01095</link><description>&lt;p&gt;
&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;: &#36890;&#36807;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
High-Dimensional Independence Testing via Maximum and Average Distance Correlations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2001.01095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#39640;&#32500;&#24230;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#31243;&#24207;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#21644;&#24191;&#27867;&#30340;&#24212;&#29992;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#30740;&#31350;&#20102;&#21033;&#29992;&#26368;&#22823;&#21644;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#36827;&#34892;&#22810;&#20803;&#29420;&#31435;&#24615;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#39640;&#32500;&#29615;&#22659;&#20013;&#34920;&#24449;&#20102;&#23427;&#20204;&#30456;&#23545;&#20110;&#36793;&#38469;&#30456;&#20851;&#32500;&#24230;&#25968;&#37327;&#30340;&#19968;&#33268;&#24615;&#29305;&#24615;&#65292;&#35780;&#20272;&#20102;&#27599;&#20010;&#26816;&#39564;&#32479;&#35745;&#37327;&#30340;&#20248;&#21183;&#65292;&#26816;&#26597;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#38646;&#20998;&#24067;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24555;&#36895;&#21345;&#26041;&#26816;&#39564;&#30340;&#26816;&#27979;&#31243;&#24207;&#12290;&#24471;&#20986;&#30340;&#26816;&#39564;&#26159;&#38750;&#21442;&#25968;&#30340;&#65292;&#24182;&#36866;&#29992;&#20110;&#27431;&#27663;&#36317;&#31163;&#21644;&#39640;&#26031;&#26680;&#20316;&#20026;&#24213;&#23618;&#24230;&#37327;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#25152;&#25552;&#20986;&#30340;&#27979;&#35797;&#30340;&#23454;&#38469;&#20351;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22810;&#20803;&#30456;&#20851;&#22330;&#26223;&#20013;&#35780;&#20272;&#20102;&#26368;&#22823;&#36317;&#31163;&#30456;&#20851;&#24615;&#12289;&#24179;&#22343;&#36317;&#31163;&#30456;&#20851;&#24615;&#21644;&#21407;&#22987;&#36317;&#31163;&#30456;&#20851;&#24615;&#30340;&#23454;&#35777;&#34920;&#29616;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#19968;&#20010;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#20197;&#26816;&#27979;&#20154;&#31867;&#34880;&#27974;&#20013;&#19981;&#21516;&#30284;&#30151;&#31867;&#22411;&#21644;&#32957;&#27700;&#24179;&#30340;&#23384;&#22312;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces and investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, assess the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;</title><link>http://arxiv.org/abs/2401.05572</link><description>&lt;p&gt;
&#29992;&#20110;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Innate-Values-driven Reinforcement Learning for Cooperative Multi-Agent Systems. (arXiv:2401.05572v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05572
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65288;IVRL&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#22312;&#21512;&#20316;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#20854;&#21512;&#20316;&#20249;&#20276;&#30340;&#38656;&#27714;&#65292;&#25903;&#25345;&#20854;&#31038;&#21306;&#24182;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#22825;&#20215;&#20540;&#25551;&#36848;&#20102;&#26234;&#33021;&#20307;&#30340;&#20869;&#22312;&#21160;&#26426;&#65292;&#21453;&#26144;&#20102;&#20182;&#20204;&#36861;&#27714;&#30446;&#26631;&#21644;&#21457;&#23637;&#22810;&#26679;&#25216;&#33021;&#20197;&#28385;&#36275;&#21508;&#31181;&#38656;&#27714;&#30340;&#22266;&#26377;&#20852;&#36259;&#21644;&#20559;&#22909;&#12290;&#24378;&#21270;&#23398;&#20064;&#30340;&#26412;&#36136;&#26159;&#22522;&#20110;&#22870;&#21169;&#39537;&#21160;&#65288;&#22914;&#25928;&#29992;&#65289;&#30340;&#34892;&#20026;&#20114;&#21160;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#33258;&#28982;&#26234;&#33021;&#20307;&#12290;&#29305;&#21035;&#26159;&#22312;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#65292;&#24314;&#31435;&#26234;&#33021;&#20307;&#23545;&#24179;&#34913;&#32676;&#20307;&#25928;&#29992;&#21644;&#31995;&#32479;&#25104;&#26412;&#30340;&#35748;&#30693;&#65292;&#28385;&#36275;&#32676;&#20307;&#25104;&#21592;&#22312;&#21512;&#20316;&#20013;&#30340;&#38656;&#27714;&#65292;&#26159;&#20010;&#20307;&#20026;&#25903;&#25345;&#20854;&#31038;&#21306;&#21644;&#34701;&#20837;&#20154;&#31867;&#31038;&#20250;&#32780;&#23398;&#20064;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#22797;&#21512;&#20869;&#22312;&#20215;&#20540;&#22686;&#24378;&#23398;&#20064;&#27169;&#22411; - &#20808;&#22825;&#20215;&#20540;&#39537;&#21160;&#22686;&#24378;&#23398;&#20064;&#65292;&#29992;&#20110;&#25551;&#36848;&#22810;&#26234;&#33021;&#20307;&#21512;&#20316;&#20013;&#22797;&#26434;&#30340;&#20114;&#21160;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Innate values describe agents' intrinsic motivations, which reflect their inherent interests and preferences to pursue goals and drive them to develop diverse skills satisfying their various needs. The essence of reinforcement learning (RL) is learning from interaction based on reward-driven (such as utilities) behaviors, much like natural agents. It is an excellent model to describe the innate-values-driven (IV) behaviors of AI agents. Especially in multi-agent systems (MAS), building the awareness of AI agents to balance the group utilities and system costs and satisfy group members' needs in their cooperation is a crucial problem for individuals learning to support their community and integrate human society in the long term. This paper proposes a hierarchical compound intrinsic value reinforcement learning model -innate-values-driven reinforcement learning termed IVRL to describe the complex behaviors of multi-agent interaction in their cooperation. We implement the IVRL architec
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26694;&#26550;&#65292;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.08732</link><description>&lt;p&gt;
&#21487;&#35777;&#20445;&#20581;&#24247;&#21830;&#21153;&#23383;&#20307;&#23609;&#21253;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;(&#35793;&#27880;)&#27700;&#12290;
&lt;/p&gt;
&lt;p&gt;
Provably Robust Cost-Sensitive Learning via Randomized Smoothing. (arXiv:2310.08732v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08732
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#26694;&#26550;&#65292;&#20026;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#20998;&#31867;&#22120;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#20248;&#21270;&#26041;&#26696;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#20102;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#65292;&#21462;&#24471;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20851;&#27880;&#20110;&#22312;&#25104;&#26412;&#25935;&#24863;&#30340;&#24773;&#26223;&#19979;&#23398;&#20064;&#23545;&#25239;&#24615;&#31283;&#20581;&#20998;&#31867;&#22120;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#19981;&#21516;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#21464;&#25442;&#30340;&#28508;&#22312;&#21361;&#23475;&#34987;&#32534;&#30721;&#22312;&#19968;&#20010;&#20108;&#36827;&#21046;&#25104;&#26412;&#30697;&#38453;&#20013;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#35201;&#20040;&#26159;&#32463;&#39564;&#24615;&#30340;&#65292;&#26080;&#27861;&#35777;&#26126;&#31283;&#20581;&#24615;&#65292;&#35201;&#20040;&#23384;&#22312;&#22266;&#26377;&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38543;&#26426;&#24179;&#28369;&#65292;&#19968;&#31181;&#26356;&#21487;&#25193;&#23637;&#30340;&#31283;&#20581;&#24615;&#35748;&#35777;&#26694;&#26550;&#65292;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#35777;&#26126;&#25104;&#26412;&#25935;&#24863;&#30340;&#31283;&#20581;&#24615;&#12290;&#24314;&#31435;&#22312;&#19968;&#31181;&#25104;&#26412;&#25935;&#24863;&#35748;&#35777;&#21322;&#24452;&#30340;&#27010;&#24565;&#20043;&#19978;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#35843;&#25972;&#26631;&#20934;&#30340;&#38543;&#26426;&#24179;&#28369;&#35748;&#35777;&#27969;&#31243;&#65292;&#20026;&#20219;&#20309;&#25104;&#26412;&#30697;&#38453;&#20135;&#29983;&#20005;&#26684;&#30340;&#31283;&#20581;&#24615;&#20445;&#35777;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#38024;&#23545;&#19981;&#21516;&#25968;&#25454;&#23376;&#32452;&#35774;&#35745;&#30340;&#32454;&#31890;&#24230;&#35748;&#35777;&#21322;&#24452;&#20248;&#21270;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#38024;&#23545;&#25104;&#26412;&#25935;&#24863;&#31283;&#20581;&#24615;&#20248;&#21270;&#30340;&#24179;&#28369;&#20998;&#31867;&#22120;&#12290;&#22312;&#22270;&#20687;&#22522;&#20934;&#27979;&#35797;&#21644;&#30495;&#23454;&#30340;&#21307;&#23398;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We focus on learning adversarially robust classifiers under a cost-sensitive scenario, where the potential harm of different classwise adversarial transformations is encoded in a binary cost matrix. Existing methods are either empirical that cannot certify robustness or suffer from inherent scalability issues. In this work, we study whether randomized smoothing, a more scalable robustness certification framework, can be leveraged to certify cost-sensitive robustness. Built upon a notion of cost-sensitive certified radius, we show how to adapt the standard randomized smoothing certification pipeline to produce tight robustness guarantees for any cost matrix. In addition, with fine-grained certified radius optimization schemes specifically designed for different data subgroups, we propose an algorithm to train smoothed classifiers that are optimized for cost-sensitive robustness. Extensive experiments on image benchmarks and a real-world medical dataset demonstrate the superiority of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03346</link><description>&lt;p&gt;
&#31283;&#23450;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;: &#31163;&#32447;&#30446;&#26631;&#36798;&#25104;&#30340;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Stabilizing Contrastive RL: Techniques for Offline Goal Reaching. (arXiv:2306.03346v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03346
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#30340;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#31561;&#23454;&#39564;&#26041;&#27861;&#65292;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#28436;&#31034;&#20102;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#24050;&#32463;&#24320;&#21457;&#20102;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#24378;&#21270;&#23398;&#20064;&#20063;&#21487;&#20197;&#34987;&#35270;&#20026;&#33258;&#30417;&#30563;&#38382;&#39064;&#65306;&#23398;&#20064;&#36798;&#21040;&#20219;&#20309;&#30446;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#20154;&#31867;&#25351;&#23450;&#30340;&#22870;&#21169;&#25110;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#20026;&#24378;&#21270;&#23398;&#20064;&#24314;&#31435;&#33258;&#30417;&#30563;&#22522;&#30784;&#23454;&#38469;&#19978;&#38754;&#20020;&#30528;&#19968;&#20123;&#37325;&#35201;&#30340;&#25361;&#25112;&#12290;&#22522;&#20110;&#27492;&#21069;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#21078;&#26512;&#23454;&#39564;&#65292;&#24182;&#21457;&#29616;&#19968;&#20010;&#27973;&#32780;&#23485;&#30340;&#32467;&#26500;&#65292;&#32467;&#21512;&#35880;&#24910;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#21644;&#25968;&#25454;&#22686;&#24378;&#65292;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#19982;&#23545;&#27604;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#28436;&#31034;&#20102;&#36890;&#36807;&#36825;&#20123;&#35774;&#35745;&#20915;&#31574;&#65292;&#23545;&#27604;&#26041;&#27861;&#21487;&#20197;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#65292;&#20854;&#20013;&#20219;&#21153;&#30001;&#35757;&#32451;&#21518;&#25552;&#20379;&#30340;&#21333;&#20010;&#30446;&#26631;&#22270;&#20687;&#25351;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the same way that the computer vision (CV) and natural language processing (NLP) communities have developed self-supervised methods, reinforcement learning (RL) can be cast as a self-supervised problem: learning to reach any goal, without requiring human-specified rewards or labels. However, actually building a self-supervised foundation for RL faces some important challenges. Building on prior contrastive approaches to this RL problem, we conduct careful ablation experiments and discover that a shallow and wide architecture, combined with careful weight initialization and data augmentation, can significantly boost the performance of these contrastive RL approaches on challenging simulated benchmarks. Additionally, we demonstrate that, with these design decisions, contrastive approaches can solve real-world robotic manipulation tasks, with tasks being specified by a single goal image provided after training.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2304.09914</link><description>&lt;p&gt;
&#26623;&#23376;&#25919;&#27835;&#30340;&#38754;&#23380;&#65306;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27604;&#36739;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
The Face of Populism: Examining Differences in Facial Emotional Expressions of Political Leaders Using Machine Learning. (arXiv:2304.09914v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#20998;&#26512;&#20102;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;220&#20010;&#25919;&#27835;&#39046;&#34966;&#30340;YouTube&#35270;&#39057;&#65292;&#24635;&#32467;&#20102;&#25919;&#27835;&#39046;&#34966;&#38754;&#37096;&#24773;&#24863;&#34920;&#36798;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#23186;&#20307;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#25919;&#27835;&#20449;&#24687;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#20256;&#25773;&#21644;&#28040;&#36153;&#26041;&#24335;&#65292;&#36825;&#31181;&#36716;&#21464;&#20419;&#20351;&#25919;&#27835;&#20154;&#29289;&#37319;&#21462;&#26032;&#30340;&#31574;&#30053;&#26469;&#25429;&#25417;&#21644;&#20445;&#25345;&#36873;&#27665;&#30340;&#27880;&#24847;&#21147;&#12290;&#36825;&#20123;&#31574;&#30053;&#24448;&#24448;&#20381;&#36182;&#20110;&#24773;&#24863;&#35828;&#26381;&#21644;&#21560;&#24341;&#12290;&#38543;&#30528;&#34394;&#25311;&#31354;&#38388;&#20013;&#35270;&#35273;&#20869;&#23481;&#36234;&#26469;&#36234;&#26222;&#36941;&#65292;&#24456;&#22810;&#25919;&#27835;&#27807;&#36890;&#20063;&#34987;&#26631;&#24535;&#30528;&#21796;&#36215;&#24773;&#24863;&#30340;&#35270;&#39057;&#20869;&#23481;&#21644;&#22270;&#20687;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#22522;&#20110;&#29616;&#26377;&#35757;&#32451;&#22909;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25552;&#20379;&#30340;Python&#24211;fer&#65292;&#24212;&#29992;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#31639;&#27861;&#65292;&#23545;&#25551;&#32472;&#26469;&#33258;15&#20010;&#19981;&#21516;&#22269;&#23478;&#30340;&#25919;&#27835;&#39046;&#34966;&#30340;220&#20010;YouTube&#35270;&#39057;&#26679;&#26412;&#36827;&#34892;&#20998;&#26512;&#12290;&#35813;&#31639;&#27861;&#36820;&#22238;&#24773;&#32490;&#20998;&#25968;&#65292;&#27599;&#19968;&#24103;&#37117;&#20195;&#34920;6&#31181;&#24773;&#32490;&#29366;&#24577;&#65288;&#24868;&#24594;&#65292;&#21388;&#24694;&#65292;&#24656;&#24807;&#65292;&#24555;&#20048;&#65292;&#24754;&#20260;&#21644;&#24778;&#35766;&#65289;&#21644;&#19968;&#20010;&#20013;&#24615;&#34920;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Online media has revolutionized the way political information is disseminated and consumed on a global scale, and this shift has compelled political figures to adopt new strategies of capturing and retaining voter attention. These strategies often rely on emotional persuasion and appeal, and as visual content becomes increasingly prevalent in virtual space, much of political communication too has come to be marked by evocative video content and imagery. The present paper offers a novel approach to analyzing material of this kind. We apply a deep-learning-based computer-vision algorithm to a sample of 220 YouTube videos depicting political leaders from 15 different countries, which is based on an existing trained convolutional neural network architecture provided by the Python library fer. The algorithm returns emotion scores representing the relative presence of 6 emotional states (anger, disgust, fear, happiness, sadness, and surprise) and a neutral expression for each frame of the pr
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#21270;&#38454;&#27573;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#36817;&#20284;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#26159;&#36817;&#20284;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#12290;</title><link>http://arxiv.org/abs/2303.07475</link><description>&lt;p&gt;
&#19968;&#33324;&#25439;&#22833;&#20989;&#25968;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#23548;&#33268;&#65288;&#36817;&#20284;&#65289;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
General Loss Functions Lead to (Approximate) Interpolation in High Dimensions. (arXiv:2303.07475v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07475
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#21270;&#38454;&#27573;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#30340;&#36817;&#20284;&#34920;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#26159;&#36817;&#20284;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#19968;&#33324;&#20984;&#25439;&#22833;&#20989;&#25968;&#21644;&#36229;&#21442;&#25968;&#21270;&#38454;&#27573;&#30340;&#20108;&#20803;&#21644;&#22810;&#20803;&#20998;&#31867;&#35774;&#32622;&#65292;&#20197;&#36817;&#20284;&#22320;&#34920;&#24449;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#26799;&#24230;&#19979;&#38477;&#30340;&#38544;&#21547;&#20559;&#24046;&#36817;&#20284;&#20110;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#65292;&#26368;&#23567;&#33539;&#25968;&#25554;&#20540;&#26469;&#33258;&#20110;&#23545;&#24179;&#26041;&#25439;&#22833;&#30340;&#35757;&#32451;&#12290;&#19982;&#20043;&#21069;&#19987;&#38376;&#38024;&#23545;&#25351;&#25968;&#23614;&#25439;&#22833;&#24182;&#20351;&#29992;&#20013;&#38388;&#25903;&#25345;&#21521;&#37327;&#26426;&#20844;&#24335;&#30340;&#30740;&#31350;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#30452;&#25509;&#22522;&#20110;Ji&#21644;Telgarsky&#65288;2021&#65289;&#30340;&#21407;&#22987;-&#23545;&#20598;&#20998;&#26512;&#26041;&#27861;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#25935;&#24863;&#24230;&#20998;&#26512;&#25552;&#20379;&#20102;&#19968;&#33324;&#20984;&#25439;&#22833;&#30340;&#26032;&#36817;&#20284;&#31561;&#25928;&#24615;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36824;&#24674;&#22797;&#20102;&#20108;&#20803;&#21644;&#22810;&#20803;&#20998;&#31867;&#35774;&#32622;&#19979;&#25351;&#25968;&#23614;&#25439;&#22833;&#30340;&#29616;&#26377;&#31934;&#30830;&#31561;&#25928;&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#32039;&#23494;&#24615;&#30340;&#35777;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#35777;&#25454;&#26469;&#28436;&#31034;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide a unified framework, applicable to a general family of convex losses and across binary and multiclass settings in the overparameterized regime, to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques, which we use to demonstrate the ef
&lt;/p&gt;</description></item><item><title>Refiner&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38450;&#24481;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#20302;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20581;&#22766;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#28151;&#28102;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#32773;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.02042</link><description>&lt;p&gt;
Refiner: &#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#30340;&#25968;&#25454;&#31934;&#28860;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Refiner: Data Refining against Gradient Leakage Attacks in Federated Learning. (arXiv:2212.02042v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02042
&lt;/p&gt;
&lt;p&gt;
Refiner&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38450;&#24481;&#33539;&#24335;&#65292;&#36890;&#36807;&#26500;&#24314;&#19982;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#20302;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#20581;&#22766;&#25968;&#25454;&#65292;&#26377;&#25928;&#22320;&#28151;&#28102;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#32773;&#65292;&#20174;&#32780;&#25552;&#39640;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#23545;&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#26131;&#21463;&#26799;&#24230;&#27844;&#28431;&#25915;&#20987;&#30340;&#20851;&#27880;&#12290;&#36825;&#31867;&#25915;&#20987;&#21033;&#29992;&#23458;&#25143;&#31471;&#19978;&#20256;&#30340;&#26799;&#24230;&#26469;&#37325;&#26500;&#20854;&#25935;&#24863;&#25968;&#25454;&#65292;&#20174;&#32780;&#30772;&#22351;&#20102;&#32852;&#37030;&#23398;&#20064;&#30340;&#38544;&#31169;&#20445;&#25252;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#23041;&#32961;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#38450;&#24481;&#26426;&#21046;&#26469;&#20943;&#36731;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#26426;&#21046;&#36890;&#36807;&#25805;&#32437;&#19978;&#20256;&#30340;&#26799;&#24230;&#26469;&#38450;&#27490;&#25915;&#20987;&#12290;&#28982;&#32780;&#65292;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#36825;&#20123;&#38450;&#24481;&#25514;&#26045;&#22312;&#38754;&#23545;&#22797;&#26434;&#25915;&#20987;&#26102;&#20855;&#26377;&#26377;&#38480;&#30340;&#24377;&#24615;&#65292;&#36825;&#34920;&#26126;&#36843;&#20999;&#38656;&#35201;&#26356;&#26377;&#25928;&#30340;&#38450;&#24481;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38450;&#24481;&#33539;&#24335;&#65292;&#19981;&#21516;&#20110;&#20256;&#32479;&#30340;&#26799;&#24230;&#25200;&#21160;&#26041;&#27861;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#26500;&#24314;&#20581;&#22766;&#25968;&#25454;&#12290;&#30452;&#35266;&#22320;&#35828;&#65292;&#22914;&#26524;&#20581;&#22766;&#25968;&#25454;&#19982;&#23458;&#25143;&#31471;&#21407;&#22987;&#25968;&#25454;&#20855;&#26377;&#24456;&#20302;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#65292;&#19982;&#20581;&#22766;&#25968;&#25454;&#30456;&#20851;&#30340;&#26799;&#24230;&#21487;&#20197;&#26377;&#25928;&#22320;&#28151;&#28102;&#25915;&#20987;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;Refiner&#65292;&#23427;&#21516;&#26102;&#20248;&#21270;&#20102;&#20004;&#20010;&#25351;&#26631;&#65292;&#29992;&#20110;&#38544;&#31169;&#20445;&#25252;&#21644;...
&lt;/p&gt;
&lt;p&gt;
Recent works have brought attention to the vulnerability of Federated Learning (FL) systems to gradient leakage attacks. Such attacks exploit clients' uploaded gradients to reconstruct their sensitive data, thereby compromising the privacy protection capability of FL. In response, various defense mechanisms have been proposed to mitigate this threat by manipulating the uploaded gradients. Unfortunately, empirical evaluations have demonstrated limited resilience of these defenses against sophisticated attacks, indicating an urgent need for more effective defenses. In this paper, we explore a novel defensive paradigm that departs from conventional gradient perturbation approaches and instead focuses on the construction of robust data. Intuitively, if robust data exhibits low semantic similarity with clients' raw data, the gradients associated with robust data can effectively obfuscate attackers. To this end, we design Refiner that jointly optimizes two metrics for privacy protection and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#36827;&#34892;&#20102;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2010.11750</link><description>&lt;p&gt;
&#37327;&#21270;&#24322;&#26500;&#36716;&#31227;&#30340;&#31934;&#30830;&#39640;&#32500;&#28176;&#36817;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Precise High-Dimensional Asymptotics for Quantifying Heterogeneous Transfers. (arXiv:2010.11750v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2010.11750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#65292;&#23545;&#20110;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#39640;&#32500;&#24773;&#20917;&#19979;&#30340;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#36827;&#34892;&#20102;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26102;&#20351;&#29992;&#26469;&#33258;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#26679;&#26412;&#30340;&#38382;&#39064;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#20160;&#20040;&#26102;&#20505;&#23558;&#26469;&#33258;&#20004;&#20010;&#20219;&#21153;&#30340;&#25968;&#25454;&#21512;&#24182;&#27604;&#21333;&#29420;&#23398;&#20064;&#19968;&#20010;&#20219;&#21153;&#26356;&#22909;&#65311;&#30452;&#35266;&#19978;&#65292;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#36716;&#31227;&#25928;&#24212;&#21462;&#20915;&#20110;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#65292;&#22914;&#26679;&#26412;&#22823;&#23567;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#12290;&#28982;&#32780;&#65292;&#37327;&#21270;&#36825;&#31181;&#36716;&#31227;&#25928;&#24212;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#25105;&#20204;&#38656;&#35201;&#27604;&#36739;&#32852;&#21512;&#23398;&#20064;&#21644;&#21333;&#20219;&#21153;&#23398;&#20064;&#20043;&#38388;&#30340;&#39118;&#38505;&#65292;&#24182;&#19988;&#19968;&#20010;&#20219;&#21153;&#26159;&#21542;&#27604;&#21478;&#19968;&#20010;&#20219;&#21153;&#20855;&#26377;&#27604;&#36739;&#20248;&#21183;&#21462;&#20915;&#20110;&#20004;&#20010;&#20219;&#21153;&#20043;&#38388;&#30830;&#20999;&#30340;&#25968;&#25454;&#38598;&#36716;&#31227;&#31867;&#22411;&#12290;&#26412;&#25991;&#21033;&#29992;&#38543;&#26426;&#30697;&#38453;&#29702;&#35770;&#22312;&#20855;&#26377;&#20004;&#20010;&#20219;&#21153;&#30340;&#32447;&#24615;&#22238;&#24402;&#35774;&#32622;&#20013;&#35299;&#20915;&#20102;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#32473;&#20986;&#20102;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#19968;&#20123;&#24120;&#29992;&#20272;&#35745;&#37327;&#30340;&#36229;&#39069;&#39118;&#38505;&#30340;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#65292;&#24403;&#26679;&#26412;&#22823;&#23567;&#19982;&#29305;&#24449;&#32500;&#24230;&#25104;&#27604;&#20363;&#22686;&#21152;&#26102;&#65292;&#22266;&#23450;&#27604;&#20363;&#12290;&#31934;&#30830;&#28176;&#36817;&#20998;&#26512;&#20197;&#26679;&#26412;&#22823;&#23567;&#30340;&#20989;&#25968;&#24418;&#24335;&#32473;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of learning one task with samples from another task has received much interest recently. In this paper, we ask a fundamental question: when is combining data from two tasks better than learning one task alone? Intuitively, the transfer effect from one task to another task depends on dataset shifts such as sample sizes and covariance matrices. However, quantifying such a transfer effect is challenging since we need to compare the risks between joint learning and single-task learning, and the comparative advantage of one over the other depends on the exact kind of dataset shift between both tasks. This paper uses random matrix theory to tackle this challenge in a linear regression setting with two tasks. We give precise asymptotics about the excess risks of some commonly used estimators in the high-dimensional regime, when the sample sizes increase proportionally with the feature dimension at fixed ratios. The precise asymptotics is provided as a function of the sample sizes 
&lt;/p&gt;</description></item></channel></rss>