<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#20219;&#21153;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#65292;&#22312;eNMPC&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#23454;&#29616;&#26356;&#20855;&#33021;&#21147;&#30340;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.14425</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#24494;&#20998;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#20219;&#21153;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#29992;&#20110;eNMPC
&lt;/p&gt;
&lt;p&gt;
Task-optimal data-driven surrogate models for eNMPC via differentiable simulation and optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14425
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#24494;&#20998;&#27169;&#25311;&#21644;&#20248;&#21270;&#30340;&#20219;&#21153;&#26368;&#20248;&#25968;&#25454;&#39537;&#21160;&#26367;&#20195;&#27169;&#22411;&#26041;&#27861;&#65292;&#22312;eNMPC&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#65292;&#20026;&#23454;&#29616;&#26356;&#20855;&#33021;&#21147;&#30340;&#25511;&#21046;&#22120;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25511;&#21046;&#20013;&#20248;&#21270;&#24615;&#33021;&#30340;Koopman&#26367;&#20195;&#27169;&#22411;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#12290;&#19982;&#20043;&#21069;&#37319;&#29992;&#26631;&#20934;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#30340;&#36129;&#29486;&#30456;&#21453;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#35757;&#32451;&#31639;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;&#26426;&#26800;&#27169;&#25311;&#27169;&#22411;&#30340;&#29615;&#22659;&#30340;&#28508;&#22312;&#21487;&#24494;&#24615;&#12290;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#25991;&#29486;&#24050;&#30693;&#30340;eNMPC&#26696;&#20363;&#30740;&#31350;&#20013;&#20854;&#20182;&#25511;&#21046;&#22120;&#31867;&#22411;&#21644;&#35757;&#32451;&#31639;&#27861;&#32452;&#21512;&#30340;&#24615;&#33021;&#36827;&#34892;&#27604;&#36739;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36825;&#20010;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#22240;&#27492;&#22312;&#20351;&#29992;&#21160;&#24577;&#26367;&#20195;&#27169;&#22411;&#30340;&#26356;&#26377;&#33021;&#21147;&#30340;&#25511;&#21046;&#22120;&#26041;&#38754;&#26500;&#25104;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14425v1 Announce Type: new  Abstract: We present a method for end-to-end learning of Koopman surrogate models for optimal performance in control. In contrast to previous contributions that employ standard reinforcement learning (RL) algorithms, we use a training algorithm that exploits the potential differentiability of environments based on mechanistic simulation models. We evaluate the performance of our method by comparing it to that of other controller type and training algorithm combinations on a literature known eNMPC case study. Our method exhibits superior performance on this problem, thereby constituting a promising avenue towards more capable controllers that employ dynamic surrogate models.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#22312;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20013;&#37325;&#29992;&#21382;&#21490;&#36712;&#36857;&#21487;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;</title><link>https://arxiv.org/abs/2403.00675</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#22312;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20013;&#37325;&#29992;&#21382;&#21490;&#36712;&#36857;&#65306;&#25910;&#25947;&#24615;&#21644;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;
Reusing Historical Trajectories in Natural Policy Gradient via Importance Sampling: Convergence and Convergence Rate
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00675
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#22312;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#20013;&#37325;&#29992;&#21382;&#21490;&#36712;&#36857;&#21487;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#23398;&#20064;&#25511;&#21046;&#30340;&#25968;&#23398;&#26694;&#26550;&#65292;&#20854;&#25104;&#21151;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#23427;&#21487;&#20197;&#21033;&#29992;&#30340;&#25968;&#25454;&#37327;&#12290;&#26377;&#25928;&#21033;&#29992;&#20808;&#21069;&#31574;&#30053;&#24471;&#21040;&#30340;&#21382;&#21490;&#36712;&#36857;&#23545;&#20110;&#21152;&#24555;&#31574;&#30053;&#20248;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#23454;&#35777;&#35777;&#25454;&#34920;&#26126;&#22522;&#20110;&#37325;&#35201;&#24615;&#25277;&#26679;&#30340;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#25928;&#26524;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#25991;&#29486;&#24448;&#24448;&#24573;&#35270;&#20102;&#19981;&#21516;&#36845;&#20195;&#20043;&#38388;&#36712;&#36857;&#30340;&#30456;&#20114;&#20381;&#36182;&#24615;&#65292;&#19988;&#33391;&#22909;&#30340;&#23454;&#35777;&#34920;&#29616;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#35777;&#26126;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36890;&#36807;&#37325;&#35201;&#24615;&#25277;&#26679;&#37325;&#26032;&#21033;&#29992;&#21382;&#21490;&#36712;&#36857;&#30340;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#34920;&#26126;&#20102;&#25152;&#25552;&#26799;&#24230;&#20272;&#35745;&#22120;&#30340;&#20559;&#24046;&#28176;&#36817;&#21487;&#24573;&#30053;&#65292;&#24471;&#21040;&#30340;&#31639;&#27861;&#26159;&#25910;&#25947;&#30340;&#65292;&#24182;&#19988;&#37325;&#29992;&#36807;&#21435;&#30340;&#36712;&#36857;&#26377;&#21161;&#20110;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25152;&#25552;&#20272;&#35745;&#22120;&#24212;&#29992;&#20110;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00675v1 Announce Type: new  Abstract: Reinforcement learning provides a mathematical framework for learning-based control, whose success largely depends on the amount of data it can utilize. The efficient utilization of historical trajectories obtained from previous policies is essential for expediting policy optimization. Empirical evidence has shown that policy gradient methods based on importance sampling work well. However, existing literature often neglect the interdependence between trajectories from different iterations, and the good empirical performance lacks a rigorous theoretical justification. In this paper, we study a variant of the natural policy gradient method with reusing historical trajectories via importance sampling. We show that the bias of the proposed estimator of the gradient is asymptotically negligible, the resultant algorithm is convergent, and reusing past trajectories helps improve the convergence rate. We further apply the proposed estimator to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#29983;&#36873;&#25321;&#65292;&#25913;&#36827;&#20102;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#12290;&#20351;&#29992;ZPDES&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#22320;&#30740;&#31350;&#20013;&#25552;&#39640;&#20102;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#23398;&#20064;&#25104;&#32489;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23398;&#29983;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.01669</link><description>&lt;p&gt;
&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#20013;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#30340;&#25913;&#36827;&#65306;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#20064;&#32773;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Improved Performances and Motivation in Intelligent Tutoring Systems: Combining Machine Learning and Learner Choice
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01669
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#21644;&#23398;&#29983;&#36873;&#25321;&#65292;&#25913;&#36827;&#20102;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24615;&#33021;&#21644;&#21160;&#26426;&#12290;&#20351;&#29992;ZPDES&#31639;&#27861;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65292;&#24182;&#22312;&#23454;&#22320;&#30740;&#31350;&#20013;&#25552;&#39640;&#20102;&#19981;&#21516;&#23398;&#29983;&#32676;&#20307;&#30340;&#23398;&#20064;&#25104;&#32489;&#12290;&#30740;&#31350;&#36824;&#25506;&#35752;&#20102;&#23398;&#29983;&#36873;&#25321;&#23545;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23398;&#26657;&#20013;&#65292;&#22823;&#35268;&#27169;&#30340;&#35838;&#22530;&#35268;&#27169;&#32473;&#20010;&#24615;&#21270;&#23398;&#20064;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#25945;&#32946;&#25216;&#26415;&#65292;&#23588;&#20854;&#26159;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65288;ITS&#65289;&#35797;&#22270;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#22522;&#20110;&#23398;&#20064;&#36827;&#23637;&#20551;&#35774;&#65288;LPH&#65289;&#21644;&#22810;&#33218;&#36172;&#21338;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;ZPDES&#31639;&#27861;&#23545;&#26368;&#22823;&#21270;&#23398;&#20064;&#36827;&#23637;&#65288;LP&#65289;&#30340;&#32451;&#20064;&#36827;&#34892;&#25490;&#24207;&#12290;&#35813;&#31639;&#27861;&#22312;&#20043;&#21069;&#30340;&#23454;&#22320;&#30740;&#31350;&#20013;&#24050;&#32463;&#26174;&#31034;&#20986;&#23558;&#23398;&#20064;&#34920;&#29616;&#25552;&#21319;&#21040;&#26356;&#24191;&#27867;&#30340;&#23398;&#29983;&#32676;&#20307;&#20013;&#65292;&#19982;&#25163;&#24037;&#35774;&#35745;&#30340;&#35838;&#31243;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;&#20854;&#21160;&#26426;&#24433;&#21709;&#23578;&#26410;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;ZPDES&#19981;&#20801;&#35768;&#23398;&#29983;&#21457;&#34920;&#36873;&#25321;&#24847;&#35265;&#12290;&#36825;&#31181;&#32570;&#20047;&#26426;&#26500;&#30340;&#38480;&#21046;&#19982;&#20851;&#27880;&#24314;&#27169;&#22909;&#22855;&#39537;&#21160;&#23398;&#20064;&#30340;LPH&#29702;&#35770;&#19981;&#19968;&#33268;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#30740;&#31350;&#20102;&#36825;&#31181;&#36873;&#25321;&#21487;&#33021;&#24615;&#30340;&#24341;&#20837;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#25928;&#29575;&#21644;&#21160;&#26426;&#12290;&#32473;&#23450;&#30340;&#36873;&#25321;&#19982;&#32451;&#20064;&#38590;&#24230;&#27491;&#20132;&#30340;&#32500;&#24230;&#26377;&#20851;&#65292;&#20316;&#20026;&#19968;&#31181;&#26377;&#36259;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large class sizes pose challenges to personalized learning in schools, which educational technologies, especially intelligent tutoring systems (ITS), aim to address. In this context, the ZPDES algorithm, based on the Learning Progress Hypothesis (LPH) and multi-armed bandit machine learning techniques, sequences exercises that maximize learning progress (LP). This algorithm was previously shown in field studies to boost learning performances for a wider diversity of students compared to a hand-designed curriculum. However, its motivational impact was not assessed. Also, ZPDES did not allow students to express choices. This limitation in agency is at odds with the LPH theory concerned with modeling curiosity-driven learning. We here study how the introduction of such choice possibilities impact both learning efficiency and motivation. The given choice concerns dimensions that are orthogonal to exercise difficulty, acting as a playful feature.   In an extensive field study (265 7-8 years
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;UNITE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#21644;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.02914</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#35270;&#39057;&#22495;&#33258;&#36866;&#24212;&#65306;&#37319;&#29992;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Video Domain Adaptation with Masked Pre-Training and Collaborative Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02914
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#25552;&#20986;&#20102;UNITE&#26694;&#26550;&#65292;&#21033;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#21644;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#36827;&#34892;&#36974;&#34109;&#39044;&#35757;&#32451;&#21644;&#21327;&#20316;&#33258;&#35757;&#32451;&#65292;&#22312;&#22810;&#20010;&#35270;&#39057;&#39046;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#31216;&#20026;UNITE&#65292;&#20351;&#29992;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#26469;&#35843;&#25972;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#21040;&#30446;&#26631;&#22495;&#12290;UNITE&#39318;&#20808;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#36890;&#36807;&#25945;&#24072;&#24341;&#23548;&#30340;&#36974;&#34109;&#33976;&#39311;&#30446;&#26631;&#24471;&#21040;&#20855;&#26377;&#21306;&#20998;&#24615;&#30340;&#29305;&#24449;&#23398;&#20064;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#30446;&#26631;&#25968;&#25454;&#36827;&#34892;&#36974;&#34109;&#33258;&#35757;&#32451;&#65292;&#21033;&#29992;&#35270;&#39057;&#23398;&#29983;&#27169;&#22411;&#21644;&#22270;&#20687;&#25945;&#24072;&#27169;&#22411;&#19968;&#36215;&#20026;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#35270;&#39057;&#29983;&#25104;&#25913;&#36827;&#30340;&#20266;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#33258;&#35757;&#32451;&#36807;&#31243;&#25104;&#21151;&#21033;&#29992;&#20102;&#20004;&#20010;&#27169;&#22411;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#36328;&#22495;&#24378;&#22823;&#30340;&#36716;&#31227;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#39057;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#30456;&#27604;&#20808;&#21069;&#25253;&#36947;&#30340;&#32467;&#26524;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02914v3 Announce Type: replace-cross  Abstract: In this work, we tackle the problem of unsupervised domain adaptation (UDA) for video action recognition. Our approach, which we call UNITE, uses an image teacher model to adapt a video student model to the target domain. UNITE first employs self-supervised pre-training to promote discriminative feature learning on target domain videos using a teacher-guided masked distillation objective. We then perform self-training on masked target data, using the video student model and image teacher model together to generate improved pseudolabels for unlabeled target videos. Our self-training process successfully leverages the strengths of both models to achieve strong transfer performance across domains. We evaluate our approach on multiple video domain adaptation benchmarks and observe significant improvements upon previously reported results.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#12289;&#28201;&#21644;&#21464;&#20998;&#21518;&#39564;&#30340;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#26469;&#38477;&#20302;&#21442;&#25968;&#22823;&#23567;&#65292;&#24182;&#19988;&#20855;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#36739;&#20302;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2311.01409</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#12289;&#28201;&#21644;&#21464;&#20998;&#21518;&#39564;&#30340;&#31934;&#30830;&#21644;&#21487;&#25193;&#23637;&#38543;&#26426;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Coreset-based, Tempered Variational Posterior for Accurate and Scalable Stochastic Gaussian Process Inference. (arXiv:2311.01409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01409
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#12289;&#28201;&#21644;&#21464;&#20998;&#21518;&#39564;&#30340;&#39640;&#26031;&#36807;&#31243;&#25512;&#29702;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#30340;&#12289;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#26469;&#38477;&#20302;&#21442;&#25968;&#22823;&#23567;&#65292;&#24182;&#19988;&#20855;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#21644;&#36739;&#20302;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21464;&#20998;&#39640;&#26031;&#36807;&#31243;($\mathcal{GP}$)&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21487;&#23398;&#20064;&#30340;&#26435;&#37325;&#20266;&#36755;&#20837;&#36755;&#20986;&#28857;&#30340;&#21518;&#39564;&#65288;&#26680;&#24515;&#38598;&#65289;&#12290;&#19982;&#33258;&#30001;&#24418;&#24335;&#30340;&#21464;&#20998;&#26063;&#19981;&#21516;&#65292;&#25552;&#20986;&#30340;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#12289;&#28201;&#21644;&#21464;&#20998;&#30340;$\mathcal{GP}$&#65288;CVTGP&#65289;&#26159;&#22522;&#20110;$\mathcal{GP}$&#20808;&#39564;&#21644;&#25968;&#25454;&#20284;&#28982;&#20989;&#25968;&#26469;&#23450;&#20041;&#30340;&#65292;&#22240;&#27492;&#36866;&#24212;&#20102;&#24314;&#27169;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#25552;&#20986;&#30340;&#21518;&#39564;&#36827;&#34892;&#28508;&#22312;&#30340;$\mathcal{GP}$&#26680;&#24515;&#38598;&#21464;&#37327;&#30340;&#36793;&#32536;&#21270;&#65292;&#25512;&#23548;&#20986;CVTGP&#30340;&#23545;&#25968;&#36793;&#38469;&#20284;&#28982;&#19979;&#30028;&#65292;&#24182;&#19988;&#35777;&#26126;&#20854;&#36866;&#29992;&#20110;&#38543;&#26426;&#20248;&#21270;&#12290;CVTGP&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#26680;&#24515;&#38598;&#30340;&#28201;&#21644;&#21518;&#39564;&#26469;&#20943;&#23567;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#22823;&#23567;&#21040;$\mathcal{O}(M)$&#65292;&#20855;&#26377;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#24182;&#19988;&#36890;&#36807;&#25552;&#20379;&#31232;&#30095;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#25454;&#34920;&#31034;&#26469;&#20445;&#25345;$\mathcal{O}(M^3)$&#26102;&#38388;&#22797;&#26434;&#24230;&#21644;$\mathcal{O}(M^2)$&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#22238;&#24402;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20102;CVTGP&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel stochastic variational Gaussian process ($\mathcal{GP}$) inference method, based on a posterior over a learnable set of weighted pseudo input-output points (coresets). Instead of a free-form variational family, the proposed coreset-based, variational tempered family for $\mathcal{GP}$s (CVTGP) is defined in terms of the $\mathcal{GP}$ prior and the data-likelihood; hence, accommodating the modeling inductive biases. We derive CVTGP's lower bound for the log-marginal likelihood via marginalization of the proposed posterior over latent $\mathcal{GP}$ coreset variables, and show it is amenable to stochastic optimization. CVTGP reduces the learnable parameter size to $\mathcal{O}(M)$, enjoys numerical stability, and maintains $\mathcal{O}(M^3)$ time- and $\mathcal{O}(M^2)$ space-complexity, by leveraging a coreset-based tempered posterior that, in turn, provides sparse and explainable representations of the data. Results on simulated and real-world regression problems wi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#36817;&#30340;&#20004;&#31181;&#19982;&#21160;&#37327;&#27010;&#24565;&#30456;&#20851;&#30340;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#23545;&#23398;&#20064;&#29575;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#29575;&#22343;&#20026;&#26368;&#20248;&#12290;</title><link>http://arxiv.org/abs/2304.04172</link><description>&lt;p&gt;
$\mu^2$-SGD: &#36890;&#36807;&#21452;&#21160;&#37327;&#26426;&#21046;&#23454;&#29616;&#31283;&#23450;&#30340;&#38543;&#26426;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
$\mu^2$-SGD: Stable Stochastic Optimization via a Double Momentum Mechanism. (arXiv:2304.04172v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04172
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#36817;&#30340;&#20004;&#31181;&#19982;&#21160;&#37327;&#27010;&#24565;&#30456;&#20851;&#30340;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#31283;&#23450;&#30340;&#38543;&#26426;&#20248;&#21270;&#65292;&#23545;&#23398;&#20064;&#29575;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#36895;&#29575;&#22343;&#20026;&#26368;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#30446;&#26631;&#20989;&#25968;&#20026;&#24179;&#28369;&#20989;&#25968;&#26399;&#26395;&#30340;&#38543;&#26426;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#31181;&#24773;&#20917;&#65292;&#25105;&#20204;&#24314;&#35758;&#19968;&#31181;&#26032;&#30340;&#26799;&#24230;&#20272;&#35745;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#26368;&#36817;&#30340;&#20004;&#31181;&#19982;&#21160;&#37327;&#27010;&#24565;&#30456;&#20851;&#30340;&#26426;&#21046;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;SGD&#26679;&#24335;&#30340;&#31639;&#27861;&#21644;&#19968;&#20010;&#21152;&#36895;&#29256;&#65292;&#21033;&#29992;&#36825;&#20010;&#26032;&#30340;&#20272;&#35745;&#22120;&#65292;&#24182;&#35777;&#26126;&#20102;&#36825;&#20123;&#26032;&#26041;&#27861;&#23545;&#23398;&#20064;&#29575;&#30340;&#36873;&#25321;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#30456;&#21516;&#30340;&#22266;&#23450;&#23398;&#20064;&#29575;&#36873;&#25321;&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#26377;&#22122;&#22768;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#34920;&#26126;&#36825;&#20123;&#26041;&#27861;&#22312;&#38750;&#24120;&#24191;&#27867;&#30340;&#23398;&#20064;&#29575;&#33539;&#22260;&#20869;&#23454;&#29616;&#20102;&#30456;&#21516;&#30340;&#26368;&#20248;&#35823;&#24046;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider stochastic convex optimization problems where the objective is an expectation over smooth functions. For this setting we suggest a novel gradient estimate that combines two recent mechanism that are related to notion of momentum. Then, we design an SGD-style algorithm as well as an accelerated version that make use of this new estimator, and demonstrate the robustness of these new approaches to the choice of the learning rate. Concretely, we show that these approaches obtain the optimal convergence rates for both noiseless and noisy case with the same choice of fixed learning rate. Moreover, for the noisy case we show that these approaches achieve the same optimal bound for a very wide range of learning rates.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#23457;&#35745;&#36825;&#20010;&#36807;&#31243;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#24471;&#21040;&#20445;&#25252;&#12290;</title><link>http://arxiv.org/abs/2210.09126</link><description>&lt;p&gt;
&#21487;&#39564;&#35777;&#19988;&#20855;&#26377;&#35777;&#26126;&#23433;&#20840;&#24615;&#30340;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Verifiable and Provably Secure Machine Unlearning. (arXiv:2210.09126v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09126
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#65292;&#21487;&#20197;&#35753;&#29992;&#25143;&#23457;&#35745;&#36825;&#20010;&#36807;&#31243;&#65292;&#20197;&#30830;&#20445;&#35757;&#32451;&#25968;&#25454;&#30340;&#38544;&#31169;&#24471;&#21040;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#26088;&#22312;&#22312;&#35757;&#32451;&#21518;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31227;&#38500;&#26576;&#20123;&#28857;&#65307;&#20363;&#22914;&#24403;&#29992;&#25143;&#35831;&#27714;&#21024;&#38500;&#25968;&#25454;&#26102;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#65292;&#20294;&#26159;&#27809;&#26377;&#19968;&#31181;&#31639;&#27861;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#23457;&#35745;&#36825;&#20010;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#25143;&#26080;&#27861;&#36890;&#36807;&#26816;&#26597;&#27169;&#22411;&#26412;&#36523;&#26469;&#39564;&#35777;&#20854;&#25968;&#25454;&#26159;&#21542;&#24050;&#34987;&#21024;&#38500;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#19981;&#26159;&#32771;&#34385;&#27169;&#22411;&#21442;&#25968;&#65292;&#32780;&#26159;&#23558;&#21487;&#39564;&#35777;&#30340;&#31639;&#27861;&#35270;&#20026;&#19968;&#31181;&#23433;&#20840;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#39564;&#35777;&#21435;&#38500;&#31639;&#27861;&#30340;&#31532;&#19968;&#20010;&#21152;&#23494;&#23450;&#20041;&#65292;&#20197;&#27491;&#24335;&#25429;&#25417;&#26426;&#22120;&#23398;&#20064;&#21435;&#38500;&#31639;&#27861;&#31995;&#32479;&#30340;&#20445;&#35777;&#12290;&#22312;&#27492;&#26694;&#26550;&#19979;&#65292;&#26381;&#21153;&#22120;&#39318;&#20808;&#35745;&#31639;&#19968;&#20010;&#35777;&#26126;&#65292;&#35777;&#26126;&#35813;&#27169;&#22411;&#22312;&#25968;&#25454;&#38598; $D$ &#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#32473;&#23450;&#19968;&#20010;&#35201;&#21024;&#38500;&#30340;&#29992;&#25143;&#25968;&#25454;&#28857; $d$&#65292;&#26381;&#21153;&#22120;&#20351;&#29992;&#21435;&#38500;&#31639;&#27861;&#26356;&#26032;&#27169;&#22411;&#12290;&#28982;&#21518;&#23427;&#25552;&#20379;&#27491;&#30830;&#25191;&#34892;&#21435;&#38500;&#31639;&#27861;&#24182;&#19988; $d \notin D'$ &#30340;&#35777;&#26126;&#65292;&#20854;&#20013; $D'$ &#26159;&#26032;&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning aims to remove points from the training dataset of a machine learning model after training; for example when a user requests their data to be deleted. While many machine unlearning methods have been proposed, none of them enable users to audit the procedure. Furthermore, recent work shows a user is unable to verify if their data was unlearnt from an inspection of the model alone. Rather than reasoning about model parameters, we propose to view verifiable unlearning as a security problem. To this end, we present the first cryptographic definition of verifiable unlearning to formally capture the guarantees of a machine unlearning system. In this framework, the server first computes a proof that the model was trained on a dataset $D$. Given a user data point $d$ requested to be deleted, the server updates the model using an unlearning algorithm. It then provides a proof of the correct execution of unlearning and that $d \notin D'$, where $D'$ is the new training dataset
&lt;/p&gt;</description></item></channel></rss>