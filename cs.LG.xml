<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIPS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;&#65292;&#22635;&#34917;&#20102;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2402.10758</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
Stochastic Localization via Iterative Posterior Sampling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10758
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SLIPS&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;&#65292;&#22635;&#34917;&#20102;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22312;&#22522;&#20110;&#24471;&#20998;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#36817;&#26399;&#23545;&#38543;&#26426;&#23450;&#20301;&#25216;&#26415;&#20135;&#29983;&#20102;&#26032;&#30340;&#20852;&#36259;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#20154;&#20204;&#36890;&#36807;&#38543;&#26426;&#36807;&#31243;&#65288;&#31216;&#20026;&#35266;&#27979;&#36807;&#31243;&#65289;&#20026;&#25968;&#25454;&#20998;&#24067;&#20013;&#30340;&#26679;&#26412;&#24341;&#20837;&#22122;&#22768;&#65292;&#24182;&#36880;&#28176;&#23398;&#20064;&#19982;&#35813;&#21160;&#21147;&#23398;&#20851;&#32852;&#30340;&#21435;&#22122;&#22120;&#12290;&#38500;&#20102;&#29305;&#23450;&#24212;&#29992;&#20043;&#22806;&#65292;&#23545;&#20110;&#20174;&#38750;&#26631;&#20934;&#21270;&#30446;&#26631;&#23494;&#24230;&#20013;&#25277;&#26679;&#30340;&#38382;&#39064;&#65292;&#23545;&#38543;&#26426;&#23450;&#20301;&#30340;&#20351;&#29992;&#23578;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#35752;&#12290;&#26412;&#39033;&#24037;&#20316;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#38543;&#26426;&#23450;&#20301;&#26694;&#26550;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31867;&#26126;&#30830;&#30340;&#35266;&#27979;&#36807;&#31243;&#65292;&#19982;&#28789;&#27963;&#30340;&#21435;&#22122;&#26102;&#38388;&#34920;&#30456;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#23436;&#25972;&#30340;&#26041;&#27861;&#35770;&#65292;&#21363;&#8220;&#36890;&#36807;&#36845;&#20195;&#21518;&#39564;&#25277;&#26679;&#23454;&#29616;&#38543;&#26426;&#23450;&#20301;&#8221;&#65288;SLIPS&#65289;&#65292;&#20197;&#33719;&#24471;&#35813;&#21160;&#21147;&#23398;&#30340;&#36817;&#20284;&#26679;&#26412;&#65292;&#24182;&#20316;&#20026;&#21103;&#20135;&#21697;&#65292;&#26679;&#26412;&#26469;&#33258;&#30446;&#26631;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#26696;&#22522;&#20110;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10758v1 Announce Type: cross  Abstract: Building upon score-based learning, new interest in stochastic localization techniques has recently emerged. In these models, one seeks to noise a sample from the data distribution through a stochastic process, called observation process, and progressively learns a denoiser associated to this dynamics. Apart from specific applications, the use of stochastic localization for the problem of sampling from an unnormalized target density has not been explored extensively. This work contributes to fill this gap. We consider a general stochastic localization framework and introduce an explicit class of observation processes, associated with flexible denoising schedules. We provide a complete methodology, $\textit{Stochastic Localization via Iterative Posterior Sampling}$ (SLIPS), to obtain approximate samples of this dynamics, and as a by-product, samples from the target distribution. Our scheme is based on a Markov chain Monte Carlo estimati
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.01770</link><description>&lt;p&gt;
&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#32593;&#32476;&#20013;&#21387;&#32553;&#34920;&#31034;&#30340;&#31616;&#21333;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
A simple connection from loss flatness to compressed representations in neural networks. (arXiv:2310.01770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#25439;&#22833;&#24179;&#22374;&#24615;&#21644;&#31070;&#32463;&#34920;&#31034;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#65292;&#35777;&#26126;&#20102;&#25439;&#22833;&#24179;&#22374;&#24615;&#19982;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#36827;&#34892;&#30740;&#31350;&#30340;&#26041;&#27861;&#26377;&#24456;&#22810;&#31181;&#65292;&#21253;&#25324;&#33267;&#23569;&#20004;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#31354;&#38388;&#20013;&#25439;&#22833;&#26223;&#35266;&#30340;&#24418;&#29366;&#65292;&#21478;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#31354;&#38388;&#20013;&#34920;&#31034;&#27969;&#24418;&#30340;&#32467;&#26500;&#65288;&#21363;&#21333;&#20301;&#27963;&#21160;&#30340;&#31354;&#38388;&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#30456;&#20851;&#20294;&#24456;&#23569;&#21516;&#26102;&#36827;&#34892;&#30740;&#31350;&#21644;&#26126;&#30830;&#20851;&#32852;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#26512;&#26041;&#27861;&#26469;&#24314;&#31435;&#36825;&#31181;&#32852;&#31995;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#30340;&#26368;&#21518;&#38454;&#27573;&#65292;&#31070;&#32463;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#19982;&#27491;&#22312;&#36827;&#34892;&#30340;&#21442;&#25968;&#20248;&#21270;&#25152;&#25506;&#32034;&#30340;&#26368;&#23567;&#20540;&#21608;&#22260;&#30340;&#25439;&#22833;&#24179;&#22374;&#24615;&#30456;&#20851;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#21487;&#20197;&#30001;&#19968;&#20010;&#30456;&#23545;&#31616;&#21333;&#30340;&#25968;&#23398;&#20851;&#31995;&#26469;&#39044;&#27979;&#65306;&#25439;&#22833;&#24179;&#22374;&#24615;&#24847;&#21619;&#30528;&#31070;&#32463;&#34920;&#31034;&#30340;&#21387;&#32553;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;\citet{ma_linear_2021}&#30340;&#20808;&#21069;&#30740;&#31350;&#23494;&#20999;&#30456;&#20851;&#65292;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;&#24179;&#22374;&#24615;&#65288;&#21363;&#23567;&#29305;&#24449;&#20540;&#65289;&#19982;&#34920;&#31034;&#27969;&#24418;&#30340;&#20307;&#31215;&#21387;&#32553;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks' generalization capacity has been studied in a variety of ways, including at least two distinct categories of approach: one based on the shape of the loss landscape in parameter space, and the other based on the structure of the representation manifold in feature space (that is, in the space of unit activities). These two approaches are related, but they are rarely studied together and explicitly connected. Here, we present a simple analysis that makes such a connection. We show that, in the last phase of learning of deep neural networks, compression of the volume of the manifold of neural representations correlates with the flatness of the loss around the minima explored by ongoing parameter optimization. We show that this is predicted by a relatively simple mathematical relationship: loss flatness implies compression of neural representations. Our results build closely on prior work of \citet{ma_linear_2021}, which shows how flatness (i.e., small eigenvalues of t
&lt;/p&gt;</description></item></channel></rss>