<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2402.17767</link><description>&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#20351;&#29992;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;
&lt;/p&gt;
&lt;p&gt;
Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17767
&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#25104;&#21151;&#22312;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25171;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#20351;&#21830;&#21697;&#31227;&#21160;&#25805;&#20316;&#22120;&#65288;Stretch RE2&#65289;&#33021;&#22815;&#22312;&#22810;&#26679;&#30340;&#20197;&#21069;&#26410;&#35265;&#30340;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#25289;&#24320;&#27249;&#26588;&#21644;&#25277;&#23625;&#12290;&#25105;&#20204;&#22312;31&#20010;&#19981;&#21516;&#30340;&#29289;&#20307;&#21644;13&#20010;&#19981;&#21516;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;4&#22825;&#30340;&#23454;&#38469;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#38646;&#20987;&#25171;&#19979;&#65292;&#23545;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#26032;&#39062;&#30340;&#27249;&#26588;&#21644;&#25277;&#23625;&#30340;&#25171;&#24320;&#29575;&#36798;&#21040;61%&#12290;&#23545;&#22833;&#36133;&#27169;&#24335;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24863;&#30693;&#35823;&#24046;&#26159;&#25105;&#20204;&#31995;&#32479;&#38754;&#20020;&#30340;&#26368;&#37325;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17767v1 Announce Type: cross  Abstract: Pulling open cabinets and drawers presents many difficult technical challenges in perception (inferring articulation parameters for objects from onboard sensors), planning (producing motion plans that conform to tight task constraints), and control (making and maintaining contact while applying forces on the environment). In this work, we build an end-to-end system that enables a commodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in diverse previously unseen real world environments. We conduct 4 days of real world testing of this system spanning 31 different objects from across 13 different real world environments. Our system achieves a success rate of 61% on opening novel cabinets and drawers in unseen environments zero-shot. An analysis of the failure modes suggests that errors in perception are the most significant challenge for our system. We will open source code and models for others to replicate and bui
&lt;/p&gt;</description></item><item><title>TransAxx&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#24211;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#23545;Vision Transformer&#27169;&#22411;&#36827;&#34892;&#36817;&#20284;&#24863;&#30693;&#24494;&#35843;&#65292;&#26469;&#25552;&#39640;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.07545</link><description>&lt;p&gt;
TransAxx&#65306;&#20855;&#26377;&#36817;&#20284;&#35745;&#31639;&#33021;&#21147;&#30340;&#39640;&#25928;Transformer&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TransAxx: Efficient Transformers with Approximate Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07545
&lt;/p&gt;
&lt;p&gt;
TransAxx&#26159;&#19968;&#20010;&#22522;&#20110;PyTorch&#24211;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#25903;&#25345;&#36817;&#20284;&#35745;&#31639;&#65292;&#24182;&#36890;&#36807;&#23545;Vision Transformer&#27169;&#22411;&#36827;&#34892;&#36817;&#20284;&#24863;&#30693;&#24494;&#35843;&#65292;&#26469;&#25552;&#39640;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;Transformer&#26550;&#26500;&#24341;&#20837;&#30340;Vision Transformer (ViT)&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#24456;&#22823;&#30340;&#31454;&#20105;&#21147;&#65292;&#24182;&#19988;&#24448;&#24448;&#25104;&#20026;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#30340;&#19968;&#31181;&#27969;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#39640;&#35745;&#31639;&#38656;&#27714;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#20302;&#21151;&#32791;&#35774;&#22791;&#19978;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#37319;&#29992;&#36817;&#20284;&#20056;&#27861;&#22120;&#26469;&#35299;&#20915;DNN&#21152;&#36895;&#22120;&#39640;&#35745;&#31639;&#38656;&#27714;&#30340;&#38382;&#39064;&#65292;&#20294;&#20043;&#21069;&#30340;&#30740;&#31350;&#24182;&#27809;&#26377;&#25506;&#32034;&#20854;&#22312;ViT&#27169;&#22411;&#19978;&#30340;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransAxx&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#27969;&#34892;&#30340;PyTorch&#24211;&#30340;&#26694;&#26550;&#65292;&#23427;&#33021;&#22815;&#24555;&#36895;&#25903;&#25345;&#36817;&#20284;&#31639;&#26415;&#65292;&#20197;&#26080;&#32541;&#22320;&#35780;&#20272;&#36817;&#20284;&#35745;&#31639;&#23545;&#20110;DNN (&#22914;ViT&#27169;&#22411;)&#30340;&#24433;&#21709;&#12290;&#20351;&#29992;TransAxx&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;Transformer&#27169;&#22411;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#23545;&#36817;&#20284;&#20056;&#27861;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#36817;&#20284;&#24863;&#30693;&#30340;&#24494;&#35843;&#20197;&#24674;&#22797;&#20934;&#30830;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#36817;&#20284;&#21152;&#27861;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) models which were recently introduced by the transformer architecture have shown to be very competitive and often become a popular alternative to Convolutional Neural Networks (CNNs). However, the high computational requirements of these models limit their practical applicability especially on low-power devices. Current state-of-the-art employs approximate multipliers to address the highly increased compute demands of DNN accelerators but no prior research has explored their use on ViT models. In this work we propose TransAxx, a framework based on the popular PyTorch library that enables fast inherent support for approximate arithmetic to seamlessly evaluate the impact of approximate computing on DNNs such as ViT models. Using TransAxx we analyze the sensitivity of transformer models on the ImageNet dataset to approximate multiplications and perform approximate-aware finetuning to regain accuracy. Furthermore, we propose a methodology to generate approximate ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#22312;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2202.03482</link><description>&lt;p&gt;
&#39046;&#33322;&#31070;&#32463;&#31354;&#38388;&#65306;&#37325;&#26032;&#23457;&#35270;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#20197;&#20811;&#26381;&#26041;&#21521;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;
Navigating Neural Space: Revisiting Concept Activation Vectors to Overcome Directional Divergence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.03482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#22312;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#26469;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#20110;&#29702;&#35299;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#31574;&#30053;&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#24050;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#24314;&#27169;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#36890;&#24120;&#65292;CAVs&#26159;&#36890;&#36807;&#21033;&#29992;&#32447;&#24615;&#20998;&#31867;&#22120;&#26469;&#35745;&#31639;&#30340;&#65292;&#35813;&#20998;&#31867;&#22120;&#20248;&#21270;&#20855;&#26377;&#32473;&#23450;&#27010;&#24565;&#21644;&#26080;&#32473;&#23450;&#27010;&#24565;&#30340;&#26679;&#26412;&#30340;&#28508;&#22312;&#34920;&#31034;&#30340;&#21487;&#20998;&#31163;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#26412;&#25991;&#20013;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#20197;&#21487;&#20998;&#31163;&#24615;&#20026;&#23548;&#21521;&#30340;&#35745;&#31639;&#26041;&#27861;&#20250;&#23548;&#33268;&#19982;&#31934;&#30830;&#24314;&#27169;&#27010;&#24565;&#26041;&#21521;&#30340;&#23454;&#38469;&#30446;&#26631;&#21457;&#25955;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#31181;&#24046;&#24322;&#21487;&#20197;&#24402;&#22240;&#20110;&#20998;&#25955;&#26041;&#21521;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#21363;&#19982;&#27010;&#24565;&#26080;&#20851;&#30340;&#20449;&#21495;&#65292;&#36825;&#20123;&#20449;&#21495;&#34987;&#32447;&#24615;&#27169;&#22411;&#30340;&#28388;&#27874;&#22120;&#65288;&#21363;&#26435;&#37325;&#65289;&#25429;&#33719;&#20197;&#20248;&#21270;&#31867;&#21035;&#21487;&#20998;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;&#27169;&#24335;&#30340;CAVs&#65292;&#20165;&#20851;&#27880;&#27010;&#24565;&#20449;&#21495;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#20934;&#30830;&#30340;&#27010;&#24565;&#26041;&#21521;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;CAV&#26041;&#27861;&#19982;&#30495;&#23454;&#27010;&#24565;&#26041;&#21521;&#30340;&#23545;&#40784;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
With a growing interest in understanding neural network prediction strategies, Concept Activation Vectors (CAVs) have emerged as a popular tool for modeling human-understandable concepts in the latent space. Commonly, CAVs are computed by leveraging linear classifiers optimizing the separability of latent representations of samples with and without a given concept. However, in this paper we show that such a separability-oriented computation leads to solutions, which may diverge from the actual goal of precisely modeling the concept direction. This discrepancy can be attributed to the significant influence of distractor directions, i.e., signals unrelated to the concept, which are picked up by filters (i.e., weights) of linear models to optimize class-separability. To address this, we introduce pattern-based CAVs, solely focussing on concept signals, thereby providing more accurate concept directions. We evaluate various CAV methods in terms of their alignment with the true concept dire
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.10745</link><description>&lt;p&gt;
Mori-Zwanzig&#28508;&#21464;&#31354;&#38388;Koopman&#38381;&#21253;&#29992;&#20110;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
Mori-Zwanzig latent space Koopman closure for nonlinear autoencoder. (arXiv:2310.10745v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10745
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#21644;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#30340;&#38598;&#25104;&#23454;&#29616;&#23545;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#30340;&#36924;&#36817;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#31934;&#30830;&#24615;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Koopman&#31639;&#23376;&#25552;&#20379;&#20102;&#19968;&#31181;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#38750;&#32447;&#24615;&#31995;&#32479;&#30340;&#20840;&#23616;&#32447;&#24615;&#21270;&#65292;&#20351;&#20854;&#25104;&#20026;&#31616;&#21270;&#22797;&#26434;&#21160;&#21147;&#23398;&#29702;&#35299;&#30340;&#23453;&#36149;&#26041;&#27861;&#12290;&#34429;&#28982;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22312;&#36924;&#36817;&#26377;&#38480;Koopman&#31639;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#20363;&#22914;&#36873;&#25321;&#21512;&#36866;&#30340;&#21487;&#35266;&#23519;&#37327;&#12289;&#38477;&#32500;&#21644;&#20934;&#30830;&#39044;&#27979;&#22797;&#26434;&#31995;&#32479;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Mori-Zwanzig&#33258;&#32534;&#30721;&#22120;&#65288;MZ-AE&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#31283;&#20581;&#22320;&#36924;&#36817;Koopman&#31639;&#23376;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#25552;&#21462;&#20851;&#38190;&#21487;&#35266;&#23519;&#37327;&#26469;&#36924;&#36817;&#26377;&#38480;&#19981;&#21464;Koopman&#23376;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;Mori-Zwanzig&#24418;&#24335;&#20027;&#20041;&#38598;&#25104;&#38750;&#39532;&#23572;&#21487;&#22827;&#26657;&#27491;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#35813;&#26041;&#27861;&#22312;&#38750;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#30340;&#28508;&#21464;&#27969;&#24418;&#20013;&#20135;&#29983;&#20102;&#21160;&#21147;&#23398;&#30340;&#23553;&#38381;&#34920;&#31034;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31934;&#30830;&#24615;&#21644;...
&lt;/p&gt;
&lt;p&gt;
The Koopman operator presents an attractive approach to achieve global linearization of nonlinear systems, making it a valuable method for simplifying the understanding of complex dynamics. While data-driven methodologies have exhibited promise in approximating finite Koopman operators, they grapple with various challenges, such as the judicious selection of observables, dimensionality reduction, and the ability to predict complex system behaviours accurately. This study presents a novel approach termed Mori-Zwanzig autoencoder (MZ-AE) to robustly approximate the Koopman operator in low-dimensional spaces. The proposed method leverages a nonlinear autoencoder to extract key observables for approximating a finite invariant Koopman subspace and integrates a non-Markovian correction mechanism using the Mori-Zwanzig formalism. Consequently, this approach yields a closed representation of dynamics within the latent manifold of the nonlinear autoencoder, thereby enhancing the precision and s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22312;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21253;&#25324;&#39046;&#22495;&#29305;&#23450;&#30340;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#31070;&#32463;&#34920;&#31034;&#65292;&#20197;&#21450;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#20026;&#23398;&#20064;&#31283;&#20581;&#12289;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08751</link><description>&lt;p&gt;
&#22810;&#26679;&#30340;&#31070;&#32463;&#38899;&#39057;&#23884;&#20837; - &#24674;&#22797;&#29305;&#24449;&#65281;
&lt;/p&gt;
&lt;p&gt;
Diverse Neural Audio Embeddings -- Bringing Features back !. (arXiv:2309.08751v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22312;&#38899;&#39057;&#20998;&#31867;&#20219;&#21153;&#20013;&#23398;&#20064;&#22810;&#26679;&#21270;&#30340;&#29305;&#24449;&#34920;&#31034;&#65292;&#21253;&#25324;&#39046;&#22495;&#29305;&#23450;&#30340;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#31070;&#32463;&#34920;&#31034;&#65292;&#20197;&#21450;&#31471;&#21040;&#31471;&#26550;&#26500;&#65292;&#20026;&#23398;&#20064;&#31283;&#20581;&#12289;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#26550;&#26500;&#30340;&#20986;&#29616;&#65292;&#20174;&#31471;&#21040;&#31471;&#30340;&#26550;&#26500;&#24320;&#22987;&#27969;&#34892;&#12290;&#36825;&#31181;&#36716;&#21464;&#23548;&#33268;&#20102;&#31070;&#32463;&#26550;&#26500;&#22312;&#27809;&#26377;&#39046;&#22495;&#29305;&#23450;&#20559;&#35265;/&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#35757;&#32451;&#65292;&#26681;&#25454;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#22810;&#26679;&#30340;&#29305;&#24449;&#34920;&#31034;&#65288;&#22312;&#26412;&#20363;&#20013;&#26159;&#39046;&#22495;&#29305;&#23450;&#30340;&#65289;&#23398;&#20064;&#38899;&#39057;&#23884;&#20837;&#12290;&#23545;&#20110;&#28041;&#21450;&#25968;&#30334;&#31181;&#22768;&#38899;&#20998;&#31867;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#23398;&#20064;&#20998;&#21035;&#38024;&#23545;&#38899;&#39640;&#12289;&#38899;&#33394;&#21644;&#31070;&#32463;&#34920;&#31034;&#31561;&#22810;&#26679;&#30340;&#38899;&#39057;&#23646;&#24615;&#24314;&#31435;&#31283;&#20581;&#30340;&#23884;&#20837;&#65292;&#21516;&#26102;&#20063;&#36890;&#36807;&#31471;&#21040;&#31471;&#26550;&#26500;&#36827;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25163;&#24037;&#21046;&#20316;&#30340;&#23884;&#20837;&#65292;&#20363;&#22914;&#22522;&#20110;&#38899;&#39640;&#21644;&#38899;&#33394;&#30340;&#23884;&#20837;&#65292;&#34429;&#28982;&#21333;&#29420;&#20351;&#29992;&#26102;&#26080;&#27861;&#20987;&#36133;&#23436;&#20840;&#31471;&#21040;&#31471;&#30340;&#34920;&#31034;&#65292;&#20294;&#23558;&#36825;&#20123;&#23884;&#20837;&#19982;&#31471;&#21040;&#31471;&#23884;&#20837;&#30456;&#32467;&#21512;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#36825;&#39033;&#24037;&#20316;&#23558;&#20026;&#22312;&#31471;&#21040;&#31471;&#27169;&#22411;&#20013;&#24341;&#20837;&#19968;&#20123;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#26469;&#23398;&#20064;&#31283;&#20581;&#12289;&#22810;&#26679;&#21270;&#30340;&#34920;&#31034;&#38138;&#24179;&#36947;&#36335;&#65292;&#24182;&#36229;&#36234;&#20165;&#35757;&#32451;&#31471;&#21040;&#31471;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of modern AI architectures, a shift has happened towards end-to-end architectures. This pivot has led to neural architectures being trained without domain-specific biases/knowledge, optimized according to the task. We in this paper, learn audio embeddings via diverse feature representations, in this case, domain-specific. For the case of audio classification over hundreds of categories of sound, we learn robust separate embeddings for diverse audio properties such as pitch, timbre, and neural representation, along with also learning it via an end-to-end architecture. We observe handcrafted embeddings, e.g., pitch and timbre-based, although on their own, are not able to beat a fully end-to-end representation, yet adding these together with end-to-end embedding helps us, significantly improve performance. This work would pave the way to bring some domain expertise with end-to-end models to learn robust, diverse representations, surpassing the performance of just training 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2308.12563</link><description>&lt;p&gt;
&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65306;&#23545;&#29983;&#29702;&#20449;&#21495;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multivariate Time-Series Anomaly Detection with Contaminated Data: Application to Physiological Signals. (arXiv:2308.12563v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12563
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#38024;&#23545;&#24102;&#26377;&#27745;&#26579;&#25968;&#25454;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21435;&#27745;&#21644;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#23454;&#29616;&#20102;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#65292;&#23545;&#20110;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#24322;&#24120;&#26816;&#27979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20027;&#27969;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#36890;&#24120;&#22312;&#23398;&#26415;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#21463;&#21040;&#25511;&#21046;&#23454;&#39564;&#26465;&#20214;&#19979;&#30340;&#28165;&#27905;&#35757;&#32451;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#22330;&#26223;&#19979;&#30340;&#24615;&#33021;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#38469;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#21253;&#21547;&#22122;&#22768;&#30340;&#25361;&#25112;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#30740;&#31350;&#22312;&#24863;&#30693;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#65288;TSAD&#65289;&#20013;&#28145;&#20837;&#30740;&#31350;&#20102;&#26631;&#31614;&#32423;&#22122;&#22768;&#30340;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#23454;&#29992;&#30340;&#31471;&#21040;&#31471;&#26080;&#30417;&#30563;TSAD&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20013;&#21253;&#21547;&#24322;&#24120;&#30340;&#24773;&#20917;&#19979;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;TSAD-C&#65292;&#20854;&#22312;&#35757;&#32451;&#38454;&#27573;&#19981;&#38656;&#35201;&#35775;&#38382;&#24322;&#24120;&#26631;&#31614;&#12290;TSAD-C&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21435;&#27745;&#22120;&#29992;&#20110;&#32416;&#27491;&#35757;&#32451;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#24322;&#24120;&#65288;&#20063;&#31216;&#20026;&#22122;&#22768;&#65289;&#65292;&#19968;&#20010;&#21464;&#37327;&#20381;&#36182;&#24314;&#27169;&#27169;&#22359;&#29992;&#20110;&#25429;&#25417;&#21435;&#27745;&#21518;&#25968;&#25454;&#20013;&#30340;&#38271;&#26399;&#20869;&#37096;&#21644;&#36328;&#21464;&#37327;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#35270;&#20026;&#26367;&#20195;&#24615;&#30340;&#24322;&#24120;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data are contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three modules: a Decontaminator to rectify the abnormalities (aka noise) present in the training data, a Variable Dependency Modeling module to capture both long-term intra- and inter-variable dependencies within the decontaminated data that can be considered as a surrogate o
&lt;/p&gt;</description></item><item><title>JEN-1&#26159;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#21644;&#24310;&#32493;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2308.04729</link><description>&lt;p&gt;
JEN-1&#65306;&#20855;&#26377;&#20840;&#21521;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#24341;&#23548;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
JEN-1: Text-Guided Universal Music Generation with Omnidirectional Diffusion Models. (arXiv:2308.04729v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04729
&lt;/p&gt;
&lt;p&gt;
JEN-1&#26159;&#19968;&#20010;&#39640;&#20445;&#30495;&#24230;&#36890;&#29992;&#38899;&#20048;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#65292;&#23454;&#29616;&#20102;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#21644;&#24310;&#32493;&#31561;&#29983;&#25104;&#20219;&#21153;&#65292;&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#36827;&#27493;&#65292;&#38899;&#20048;&#29983;&#25104;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#38899;&#20048;&#65288;&#21363;&#25991;&#26412;&#21040;&#38899;&#20048;&#65289;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#38899;&#20048;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#21644;&#39640;&#37319;&#26679;&#29575;&#30340;&#35201;&#27714;&#12290;&#23613;&#31649;&#20219;&#21153;&#30340;&#37325;&#35201;&#24615;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#38899;&#20048;&#36136;&#37327;&#12289;&#35745;&#31639;&#25928;&#29575;&#21644;&#27867;&#21270;&#33021;&#21147;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;JEN-1&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#25991;&#26412;&#21040;&#38899;&#20048;&#29983;&#25104;&#30340;&#36890;&#29992;&#39640;&#20445;&#30495;&#27169;&#22411;&#12290;JEN-1&#26159;&#19968;&#20010;&#32467;&#21512;&#20102;&#33258;&#22238;&#24402;&#21644;&#38750;&#33258;&#22238;&#24402;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#12290;&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;JEN-1&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#21253;&#25324;&#25991;&#26412;&#24341;&#23548;&#30340;&#38899;&#20048;&#29983;&#25104;&#12289;&#38899;&#20048;&#20462;&#34917;&#20197;&#21450;&#24310;&#32493;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;JEN-1&#22312;&#25991;&#26412;&#38899;&#20048;&#23545;&#40784;&#21644;&#38899;&#20048;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#28436;&#31034;&#21487;&#22312;&#27492;&#32593;&#22336;&#33719;&#21462;&#65306;http://URL
&lt;/p&gt;
&lt;p&gt;
Music generation has attracted growing interest with the advancement of deep generative models. However, generating music conditioned on textual descriptions, known as text-to-music, remains challenging due to the complexity of musical structures and high sampling rate requirements. Despite the task's significance, prevailing generative models exhibit limitations in music quality, computational efficiency, and generalization. This paper introduces JEN-1, a universal high-fidelity model for text-to-music generation. JEN-1 is a diffusion model incorporating both autoregressive and non-autoregressive training. Through in-context learning, JEN-1 performs various generation tasks including text-guided music generation, music inpainting, and continuation. Evaluations demonstrate JEN-1's superior performance over state-of-the-art methods in text-music alignment and music quality while maintaining computational efficiency. Our demos are available at this http URL
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#38382;&#39064;&#20026;&#25237;&#24433;&#30697;&#38453;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#30830;&#23450;&#26368;&#20248;&#35299;&#30340;&#20998;&#31163;&#20998;&#25903;&#23450;&#30028;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#26032;&#39062;&#21644;&#32039;&#23494;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#20351;&#24471;&#26368;&#20248;&#24615;&#24046;&#36317;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20943;&#23569;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2305.12292</link><description>&lt;p&gt;
&#26368;&#20248;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#65306;&#21322;&#23450;&#26494;&#24347;&#21644;&#29305;&#24449;&#21521;&#37327;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Optimal Low-Rank Matrix Completion: Semidefinite Relaxations and Eigenvector Disjunctions. (arXiv:2305.12292v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#38382;&#39064;&#20026;&#25237;&#24433;&#30697;&#38453;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#30830;&#23450;&#26368;&#20248;&#35299;&#30340;&#20998;&#31163;&#20998;&#25903;&#23450;&#30028;&#26041;&#26696;&#65292;&#24182;&#19988;&#36890;&#36807;&#26032;&#39062;&#21644;&#32039;&#23494;&#30340;&#20984;&#26494;&#24347;&#26041;&#27861;&#65292;&#20351;&#24471;&#26368;&#20248;&#24615;&#24046;&#36317;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#20943;&#23569;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#30340;&#30446;&#30340;&#26159;&#35745;&#31639;&#19968;&#20010;&#22797;&#26434;&#24230;&#26368;&#23567;&#30340;&#30697;&#38453;&#65292;&#20197;&#23613;&#21487;&#33021;&#20934;&#30830;&#22320;&#24674;&#22797;&#32473;&#23450;&#30340;&#19968;&#32452;&#35266;&#27979;&#25968;&#25454;&#65292;&#24182;&#19988;&#20855;&#26377;&#20247;&#22810;&#24212;&#29992;&#65292;&#22914;&#20135;&#21697;&#25512;&#33616;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#20302;&#31209;&#30697;&#38453;&#22635;&#34917;&#30340;&#26041;&#27861;&#26159;&#21551;&#21457;&#24335;&#30340;&#65292;&#34429;&#28982;&#39640;&#24230;&#21487;&#25193;&#23637;&#24182;&#19988;&#36890;&#24120;&#33021;&#22815;&#30830;&#23450;&#39640;&#36136;&#37327;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#19981;&#20855;&#22791;&#20219;&#20309;&#26368;&#20248;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20302;&#31209;&#38382;&#39064;&#37325;&#26032;&#34920;&#36848;&#20026;&#25237;&#24433;&#30697;&#38453;&#30340;&#38750;&#20984;&#38382;&#39064;&#65292;&#24182;&#23454;&#29616;&#19968;&#31181;&#20998;&#31163;&#20998;&#25903;&#23450;&#30028;&#26041;&#26696;&#26469;&#37325;&#26032;&#23457;&#35270;&#30697;&#38453;&#22635;&#34917;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20248;&#24615;&#23548;&#21521;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20302;&#31209;&#30697;&#38453;&#20998;&#35299;&#20026;&#19968;&#32452;&#31209;&#19968;&#30697;&#38453;&#30340;&#21644;&#65292;&#24182;&#36890;&#36807; Shor &#26494;&#24347;&#26469;&#28608;&#21169;&#27599;&#20010;&#31209;&#19968;&#30697;&#38453;&#20013;&#30340;&#27599;&#20010; 2*2 &#23567;&#30697;&#38453;&#30340;&#34892;&#21015;&#24335;&#20026;&#38646;&#65292;&#20174;&#32780;&#25512;&#23548;&#20986;&#19968;&#31181;&#26032;&#39062;&#19988;&#36890;&#24120;&#24456;&#32039;&#30340;&#20984;&#26494;&#24347;&#31867;&#12290;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#65292;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26032;&#20984;&#26494;&#24347;&#26041;&#27861;&#23558;&#26368;&#20248;&#24615;&#24046;&#36317;&#20943;&#23569;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-rank matrix completion consists of computing a matrix of minimal complexity that recovers a given set of observations as accurately as possible, and has numerous applications such as product recommendation. Unfortunately, existing methods for solving low-rank matrix completion are heuristics that, while highly scalable and often identifying high-quality solutions, do not possess any optimality guarantees. We reexamine matrix completion with an optimality-oriented eye, by reformulating low-rank problems as convex problems over the non-convex set of projection matrices and implementing a disjunctive branch-and-bound scheme that solves them to certifiable optimality. Further, we derive a novel and often tight class of convex relaxations by decomposing a low-rank matrix as a sum of rank-one matrices and incentivizing, via a Shor relaxation, that each two-by-two minor in each rank-one matrix has determinant zero. In numerical experiments, our new convex relaxations decrease the optimali
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#26641;&#27169;&#22411;&#26041;&#27861;&#39044;&#27979;&#24847;&#22823;&#21033;&#34562;&#31665;&#30340;&#34588;&#34562;&#29983;&#20135;&#37327;&#21464;&#21270;&#65292;&#24110;&#21161;&#34588;&#34562;&#20859;&#27542;&#32773;&#35780;&#20272;&#39118;&#38505;&#65292;&#20445;&#25252;&#34588;&#34562;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.01215</link><description>&lt;p&gt;
&#22522;&#20110;&#26641;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#34588;&#34562;&#29983;&#20135;&#37327;
&lt;/p&gt;
&lt;p&gt;
A Machine Learning Approach to Forecasting Honey Production with Tree-Based Methods. (arXiv:2304.01215v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01215
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#26641;&#27169;&#22411;&#26041;&#27861;&#39044;&#27979;&#24847;&#22823;&#21033;&#34562;&#31665;&#30340;&#34588;&#34562;&#29983;&#20135;&#37327;&#21464;&#21270;&#65292;&#24110;&#21161;&#34588;&#34562;&#20859;&#27542;&#32773;&#35780;&#20272;&#39118;&#38505;&#65292;&#20445;&#25252;&#34588;&#34562;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34588;&#34562;&#20859;&#27542;&#19994;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#32463;&#21382;&#20102;&#21487;&#35266;&#30340;&#29983;&#20135;&#21464;&#21270;&#65292;&#36825;&#26159;&#30001;&#20110;&#36880;&#28176;&#24694;&#21270;&#30340;&#27668;&#20505;&#26465;&#20214;&#36896;&#25104;&#30340;&#12290;&#36825;&#20123;&#29616;&#35937;&#21487;&#33021;&#20250;&#23545;&#34588;&#34562;&#27963;&#21160;&#19981;&#21033;&#12290;&#25105;&#20204;&#20351;&#29992;&#26641;&#27169;&#22411;&#26041;&#27861;&#21306;&#20998;&#34588;&#34562;&#29983;&#20135;&#37327;&#30340;&#24433;&#21709;&#22240;&#32032;&#65292;&#24182;&#39044;&#27979;&#24847;&#22823;&#21033;&#34562;&#31665;&#30340;&#34588;&#34562;&#29983;&#20135;&#37327;&#21464;&#21270;&#65292;&#24847;&#22823;&#21033;&#26159;&#27431;&#27954;&#26368;&#22823;&#30340;&#34562;&#34588;&#29983;&#20135;&#22269;&#20043;&#19968;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547;&#20102;&#20174;2019&#24180;&#21040;2022&#24180;&#25910;&#38598;&#30340;&#25968;&#30334;&#20010;&#34562;&#31665;&#30340;&#25968;&#25454;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#31934;&#24230;&#34588;&#34562;&#20859;&#27542;&#25216;&#26415;&#12290;&#25105;&#20204;&#35757;&#32451;&#21644;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#20854;&#20855;&#26377;&#25351;&#23548;&#24615;&#32780;&#19981;&#20165;&#20165;&#26159;&#39044;&#27979;&#24615;&#12290;&#19982;&#26631;&#20934;&#32447;&#24615;&#25216;&#26415;&#30456;&#27604;&#65292;&#26641;&#27169;&#22411;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#20445;&#25252;&#34588;&#34562;&#27963;&#21160;&#24182;&#35780;&#20272;&#39118;&#38505;&#31649;&#29702;&#20013;&#34588;&#34562;&#20859;&#27542;&#32773;&#30340;&#28508;&#22312;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
The beekeeping sector has undergone considerable production variations over the past years due to adverse weather conditions, occurring more frequently as climate change progresses. These phenomena can be high-impact and cause the environment to be unfavorable to the bees' activity. We disentangle the honey production drivers with tree-based methods and predict honey production variations for hives in Italy, one of the largest honey producers in Europe. The database covers hundreds of beehive data from 2019-2022 gathered with advanced precision beekeeping techniques. We train and interpret the machine learning models making them prescriptive other than just predictive. Superior predictive performances of tree-based methods compared to standard linear techniques allow for better protection of bees' activity and assess potential losses for beekeepers for risk management.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;Auto.gov&#8221;&#26694;&#26550;&#65292;&#21487;&#22686;&#24378;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#30340;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#21463;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;Q-&#32593;&#32476;&#65288;DQN&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21322;&#33258;&#21160;&#30340;&#12289;&#30452;&#35266;&#30340;&#27835;&#29702;&#25552;&#26696;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#29702;&#30001;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#24847;&#22806;&#30340;&#24066;&#22330;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2302.09551</link><description>&lt;p&gt;
Auto.gov&#65306;&#38754;&#21521;DeFi&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#38142;&#19978;&#27835;&#29702;
&lt;/p&gt;
&lt;p&gt;
Auto.gov: Learning-based On-chain Governance for Decentralized Finance (DeFi). (arXiv:2302.09551v2 [q-fin.RM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#8220;Auto.gov&#8221;&#26694;&#26550;&#65292;&#21487;&#22686;&#24378;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#30340;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#21463;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#28145;&#24230;Q-&#32593;&#32476;&#65288;DQN&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21322;&#33258;&#21160;&#30340;&#12289;&#30452;&#35266;&#30340;&#27835;&#29702;&#25552;&#26696;&#65292;&#24182;&#37327;&#21270;&#20102;&#20854;&#29702;&#30001;&#65292;&#20351;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#24212;&#23545;&#24694;&#24847;&#34892;&#20026;&#21644;&#24847;&#22806;&#30340;&#24066;&#22330;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21435;&#20013;&#24515;&#21270;&#37329;&#34701;&#65288;DeFi&#65289;&#32463;&#21382;&#20102;&#26174;&#33879;&#22686;&#38271;&#65292;&#28044;&#29616;&#20986;&#20102;&#21508;&#31181;&#21327;&#35758;&#65292;&#20363;&#22914;&#20511;&#36151;&#21327;&#35758;&#21644;&#33258;&#21160;&#21270;&#20570;&#24066;&#21830;&#65288;AMM&#65289;&#12290;&#20256;&#32479;&#19978;&#65292;&#36825;&#20123;&#21327;&#35758;&#37319;&#29992;&#38142;&#19979;&#27835;&#29702;&#65292;&#20854;&#20013;&#20195;&#24065;&#25345;&#26377;&#32773;&#25237;&#31080;&#20462;&#25913;&#21442;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#21327;&#35758;&#26680;&#24515;&#22242;&#38431;&#36827;&#34892;&#30340;&#25163;&#21160;&#21442;&#25968;&#35843;&#25972;&#23481;&#26131;&#36973;&#21463;&#21246;&#32467;&#25915;&#20987;&#65292;&#21361;&#21450;&#31995;&#32479;&#30340;&#23436;&#25972;&#24615;&#21644;&#23433;&#20840;&#24615;&#12290;&#27492;&#22806;&#65292;&#32431;&#31929;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#26041;&#27861;&#21487;&#33021;&#20250;&#20351;&#21327;&#35758;&#21463;&#21040;&#26032;&#30340;&#21033;&#29992;&#21644;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Auto.gov&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#38754;&#21521;DeFi&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#38142;&#19978;&#27835;&#29702;&#26694;&#26550;&#65292;&#21487;&#22686;&#24378;&#23433;&#20840;&#24615;&#24182;&#38477;&#20302;&#21463;&#25915;&#20987;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#20102;&#28145;&#24230;Q-&#32593;&#32476;&#65288;DQN&#65289;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#21322;&#33258;&#21160;&#21270;&#30340;&#12289;&#30452;&#35266;&#30340;&#27835;&#29702;&#25552;&#26696;&#19982;&#37327;&#21270;&#30340;&#29702;&#30001;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#22320;&#36866;&#24212;&#21644;&#32531;&#35299;&#24694;&#24847;&#34892;&#20026;&#21644;&#24847;&#22806;&#30340;&#24066;&#22330;&#24773;&#20917;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, decentralized finance (DeFi) has experienced remarkable growth, with various protocols such as lending protocols and automated market makers (AMMs) emerging. Traditionally, these protocols employ off-chain governance, where token holders vote to modify parameters. However, manual parameter adjustment, often conducted by the protocol's core team, is vulnerable to collusion, compromising the integrity and security of the system. Furthermore, purely deterministic, algorithm-based approaches may expose the protocol to novel exploits and attacks.  In this paper, we present "Auto.gov", a learning-based on-chain governance framework for DeFi that enhances security and reduces susceptibility to attacks. Our model leverages a deep Q- network (DQN) reinforcement learning approach to propose semi-automated, intuitive governance proposals with quantitative justifications. This methodology enables the system to efficiently adapt to and mitigate the negative impact of malicious beha
&lt;/p&gt;</description></item></channel></rss>