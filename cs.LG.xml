<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13257</link><description>&lt;p&gt;
Arcee&#30340;MergeKit&#65306;&#29992;&#20110;&#21512;&#24182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Arcee's MergeKit: A Toolkit for Merging Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13257
&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#24555;&#36895;&#25193;&#24352;&#20026;&#36890;&#36807;&#21512;&#24182;&#20854;&#21442;&#25968;&#26469;&#32467;&#21512;&#36825;&#20123;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#37327;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19987;&#38376;&#38024;&#23545;&#20010;&#21035;&#20219;&#21153;&#36827;&#34892;&#19987;&#38376;&#21270;&#65292;&#26080;&#27861;&#21033;&#29992;&#24444;&#27492;&#30340;&#20248;&#21183;&#12290;&#27169;&#22411;&#21512;&#24182;&#20419;&#36827;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20026;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#27169;&#22411;&#21512;&#24182;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#21253;&#25324;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#19981;&#26029;&#25193;&#22823;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MergeKit&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#24320;&#28304;&#30340;&#24211;&#65292;&#26088;&#22312;&#20419;&#36827;&#27169;&#22411;&#21512;&#24182;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13257v1 Announce Type: new  Abstract: The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pre-trained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multi-task learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of mo
&lt;/p&gt;</description></item><item><title>&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.01919</link><description>&lt;p&gt;
&#20351;&#29992;&#20984;&#20248;&#21270;&#21644;&#21015;&#23376;&#38598;&#36873;&#25321;&#30340;&#30697;&#38453;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
Matrix Completion with Convex Optimization and Column Subset Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01919
&lt;/p&gt;
&lt;p&gt;
&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#25552;&#20986;&#20351;&#29992;&#20984;&#20248;&#21270;&#35299;&#20915;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#20110;&#30697;&#38453;&#24674;&#22797;&#38382;&#39064;&#30340;&#20004;&#27493;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#21015;&#23376;&#38598;&#36873;&#25321;&#21644;&#20302;&#31209;&#30697;&#38453;&#23436;&#25104;&#38382;&#39064;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27599;&#19968;&#27493;&#20013;&#35299;&#20915;&#19968;&#20010;&#20984;&#20248;&#21270;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#23454;&#29616;&#25105;&#20204;&#30340;&#21015;&#36873;&#25321;&#30697;&#38453;&#23436;&#25104;&#65288;CSMC&#65289;&#26041;&#27861;&#30340;&#31639;&#27861;&#65292;&#27599;&#31181;&#31639;&#27861;&#38024;&#23545;&#19981;&#21516;&#35268;&#27169;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23545;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27491;&#24335;&#20998;&#26512;&#65292;&#22312;&#20998;&#26512;&#20013;&#25105;&#20204;&#38416;&#26126;&#20102;&#24517;&#35201;&#30340;&#20551;&#35774;&#21644;&#25214;&#21040;&#27491;&#30830;&#35299;&#30340;&#27010;&#29575;&#12290;&#22312;&#35770;&#25991;&#30340;&#31532;&#20108;&#37096;&#20998;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23454;&#39564;&#24037;&#20316;&#30340;&#32467;&#26524;&#12290;&#25968;&#20540;&#23454;&#39564;&#39564;&#35777;&#20102;&#31639;&#27861;&#30340;&#27491;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;&#20026;&#20102;&#30740;&#31350;&#30697;&#38453;&#22823;&#23567;&#12289;&#31209;&#21644;&#32570;&#22833;&#20803;&#32032;&#27604;&#20363;&#23545;&#35299;&#30340;&#36136;&#37327;&#21644;&#35745;&#31639;&#26102;&#38388;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#34987;&#24212;&#29992;&#20110;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;&#20363;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01919v1 Announce Type: new  Abstract: We introduce a two-step method for the matrix recovery problem. Our approach combines the theoretical foundations of the Column Subset Selection and Low-rank Matrix Completion problems. The proposed method, in each step, solves a convex optimization task. We present two algorithms that implement our Columns Selected Matrix Completion (CSMC) method, each dedicated to a different size problem. We performed a formal analysis of the presented method, in which we formulated the necessary assumptions and the probability of finding a correct solution. In the second part of the paper, we present the results of the experimental work. Numerical experiments verified the correctness and performance of the algorithms. To study the influence of the matrix size, rank, and the proportion of missing elements on the quality of the solution and the computation time, we performed experiments on synthetic data. The presented method was applied to two real-li
&lt;/p&gt;</description></item><item><title>&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;</title><link>https://arxiv.org/abs/2402.13572</link><description>&lt;p&gt;
&#35770;&#19968;&#31181;&#21464;&#31181;Looped Transformer&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
On the Expressive Power of a Variant of the Looped Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13572
&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#22411;Transformer&#22359;AlgoFormer&#65292;&#30456;&#27604;&#26631;&#20934;Transformer&#21644;Looped Transformer&#65292;AlgoFormer&#22312;&#30456;&#21516;&#21442;&#25968;&#25968;&#37327;&#19979;&#33021;&#22815;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38500;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#22312;&#35299;&#20915;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;&#31185;&#23398;&#35745;&#31639;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#65289;&#26041;&#38754;&#65292;Transformer&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35797;&#22270;&#20174;&#34920;&#36798;&#33021;&#21147;&#21644;&#21151;&#33021;&#24615;&#35282;&#24230;&#35299;&#37322;&#65292;&#26631;&#20934;&#30340;Transformer&#33021;&#22815;&#25191;&#34892;&#19968;&#20123;&#31639;&#27861;&#12290;&#20026;&#20102;&#36171;&#20104;Transformer&#31639;&#27861;&#33021;&#21147;&#65292;&#24182;&#21463;&#21040;&#26368;&#36817;&#25552;&#20986;&#30340;Looped Transformer&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Transformer&#22359;&#65292;&#21517;&#20026;Algorithm Transformer&#65288;&#31616;&#31216;AlgoFormer&#65289;&#12290;&#19982;&#26631;&#20934;Transformer&#21644;&#32431;&#31929;&#30340;Looped Transformer&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;AlgoFormer&#22312;&#20351;&#29992;&#30456;&#21516;&#25968;&#37327;&#30340;&#21442;&#25968;&#26102;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#30340;&#31639;&#27861;&#34920;&#31034;&#34920;&#36798;&#33021;&#21147;&#12290;&#29305;&#21035;&#26159;&#65292;&#21463;&#20154;&#31867;&#35774;&#35745;&#30340;&#23398;&#20064;&#31639;&#27861;&#32467;&#26500;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30340;Transformer&#22359;&#21253;&#25324;&#19968;&#20010;&#36127;&#36131;&#36827;&#34892;ta
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13572v1 Announce Type: cross  Abstract: Besides natural language processing, transformers exhibit extraordinary performance in solving broader applications, including scientific computing and computer vision. Previous works try to explain this from the expressive power and capability perspectives that standard transformers are capable of performing some algorithms. To empower transformers with algorithmic capabilities and motivated by the recently proposed looped transformer (Yang et al., 2024; Giannou et al., 2023), we design a novel transformer block, dubbed Algorithm Transformer (abbreviated as AlgoFormer). Compared with the standard transformer and vanilla looped transformer, the proposed AlgoFormer can achieve significantly higher expressiveness in algorithm representation when using the same number of parameters. In particular, inspired by the structure of human-designed learning algorithms, our transformer block consists of a pre-transformer that is responsible for ta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#32473;&#20986;&#20102;&#23545;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#20851;&#38190;&#38382;&#39064;&#30340;&#21021;&#27493;&#22238;&#31572;</title><link>https://arxiv.org/abs/2402.11650</link><description>&lt;p&gt;
&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
Theoretical foundations for programmatic reinforcement learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#22522;&#30784;&#65292;&#32473;&#20986;&#20102;&#23545;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#20013;&#20851;&#38190;&#38382;&#39064;&#30340;&#21021;&#27493;&#22238;&#31572;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#33268;&#21147;&#20110;&#22312;&#26410;&#30693;&#30340;&#38543;&#26426;&#29615;&#22659;&#20013;&#23398;&#20064;&#26368;&#20248;&#31574;&#30053;&#30340;&#31639;&#27861;&#12290;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#23558;&#31574;&#30053;&#34920;&#31034;&#20026;&#31243;&#24207;&#65292;&#21363;&#28041;&#21450;&#25511;&#21046;&#24490;&#29615;&#31561;&#39640;&#38454;&#26500;&#36896;&#12290;&#23613;&#31649;&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#24418;&#24335;&#26041;&#27861;&#20132;&#21449;&#39046;&#22495;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#65292;&#20294;&#22312;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#26041;&#38754;&#30693;&#20043;&#29978;&#23569;&#65306;&#20160;&#20040;&#26679;&#30340;&#31243;&#24207;&#21270;&#31574;&#30053;&#26159;&#22909;&#30340;&#65311;&#26368;&#20248;&#31243;&#24207;&#21270;&#31574;&#30053;&#26377;&#22810;&#22823;&#65311;&#25105;&#20204;&#22914;&#20309;&#23398;&#20064;&#23427;&#20204;&#65311;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#39318;&#27425;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#21551;&#21160;&#23545;&#31243;&#24207;&#21270;&#24378;&#21270;&#23398;&#20064;&#30340;&#29702;&#35770;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11650v1 Announce Type: new  Abstract: The field of Reinforcement Learning (RL) is concerned with algorithms for learning optimal policies in unknown stochastic environments. Programmatic RL studies representations of policies as programs, meaning involving higher order constructs such as control loops. Despite attracting a lot of attention at the intersection of the machine learning and formal methods communities, very little is known on the theoretical front about programmatic RL: what are good classes of programmatic policies? How large are optimal programmatic policies? How can we learn them? The goal of this paper is to give first answers to these questions, initiating a theoretical study of programmatic RL.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;&#38382;&#39064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22823;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#23567;&#30340;&#22240;&#23376;&#24352;&#37327;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#20102;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.11940</link><description>&lt;p&gt;
&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#23454;&#29616;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Low-Tubal-Rank Tensor Recovery via Factorized Gradient Descent. (arXiv:2401.11940v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#35299;&#20915;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;&#38382;&#39064;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#23558;&#22823;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#23567;&#30340;&#22240;&#23376;&#24352;&#37327;&#65292;&#22312;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#38656;&#27714;&#30340;&#21516;&#26102;&#65292;&#30830;&#20445;&#20102;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#23569;&#37327;&#34987;&#30772;&#22351;&#30340;&#32447;&#24615;&#27979;&#37327;&#20013;&#24674;&#22797;&#20855;&#26377;&#20302;&#32990;&#29366;&#31209;&#32467;&#26500;&#30340;&#24352;&#37327;&#30340;&#38382;&#39064;&#12290;&#20256;&#32479;&#26041;&#27861;&#38656;&#35201;&#35745;&#31639;&#24352;&#37327;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;t-SVD&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35745;&#31639;&#23494;&#38598;&#30340;&#36807;&#31243;&#65292;&#20351;&#23427;&#20204;&#38590;&#20197;&#22788;&#29702;&#22823;&#35268;&#27169;&#24352;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31867;&#20284;&#20110;Burer-Monteiro&#65288;BM&#65289;&#26041;&#27861;&#30340;&#20998;&#35299;&#36807;&#31243;&#30340;&#39640;&#25928;&#20302;&#32990;&#29366;&#31209;&#24352;&#37327;&#24674;&#22797;&#26041;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#22522;&#26412;&#26041;&#27861;&#28041;&#21450;&#23558;&#19968;&#20010;&#22823;&#24352;&#37327;&#20998;&#35299;&#20026;&#20004;&#20010;&#36739;&#23567;&#30340;&#22240;&#23376;&#24352;&#37327;&#65292;&#28982;&#21518;&#36890;&#36807;&#20998;&#35299;&#26799;&#24230;&#19979;&#38477;&#65288;FGD&#65289;&#26469;&#35299;&#20915;&#38382;&#39064;&#12290;&#35813;&#31574;&#30053;&#28040;&#38500;&#20102;t-SVD&#35745;&#31639;&#30340;&#38656;&#35201;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#35745;&#31639;&#25104;&#26412;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#20197;&#20445;&#35777;FGD&#22312;&#26080;&#22122;&#22768;&#21644;&#26377;&#22122;&#22768;&#24773;&#20917;&#19979;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper considers the problem of recovering a tensor with an underlying low-tubal-rank structure from a small number of corrupted linear measurements. Traditional approaches tackling such a problem require the computation of tensor Singular Value Decomposition (t-SVD), that is a computationally intensive process, rendering them impractical for dealing with large-scale tensors. Aim to address this challenge, we propose an efficient and effective low-tubal-rank tensor recovery method based on a factorization procedure akin to the Burer-Monteiro (BM) method. Precisely, our fundamental approach involves decomposing a large tensor into two smaller factor tensors, followed by solving the problem through factorized gradient descent (FGD). This strategy eliminates the need for t-SVD computation, thereby reducing computational costs and storage requirements. We provide rigorous theoretical analysis to ensure the convergence of FGD under both noise-free and noisy situations. Additionally, it 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#21644;&#35780;&#20272;HVAC&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#20026;&#32858;&#21512;&#22120;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#19979;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.10726</link><description>&lt;p&gt;
&#29992;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#23454;&#29992;&#24037;&#20855;&#22686;&#24378;&#32858;&#21512;&#22120;&#33021;&#21147;&#65306;&#21033;&#29992;&#32858;&#21512;&#19982;&#20998;&#25955;&#30340;&#28789;&#27963;&#24615;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
Empowering Aggregators with Practical Data-Driven Tools: Harnessing Aggregated and Disaggregated Flexibility for Demand Response. (arXiv:2401.10726v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#21644;&#35780;&#20272;HVAC&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#20026;&#32858;&#21512;&#22120;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#19981;&#30830;&#23450;&#24615;&#19979;&#23454;&#29616;&#38656;&#27714;&#21709;&#24212;&#25552;&#20379;&#20102;&#23454;&#29992;&#24037;&#20855;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#21487;&#20877;&#29983;&#33021;&#28304;&#24102;&#26469;&#19981;&#30830;&#23450;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#32858;&#21512;&#22120;&#21644;&#24314;&#31569;&#29289;&#23621;&#20303;&#32773;&#36890;&#36807;&#38656;&#27714;&#21709;&#24212;&#65288;DR&#65289;&#26041;&#26696;&#28608;&#27963;&#28789;&#27963;&#24615;&#30340;&#20851;&#38190;&#30456;&#20114;&#20316;&#29992;&#65292;&#30528;&#37325;&#20110;&#23454;&#29616;&#31283;&#20581;&#30340;&#33073;&#30899;&#21644;&#22686;&#24378;&#33021;&#28304;&#31995;&#32479;&#30340;&#38887;&#24615;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#25968;&#25454;&#26377;&#38480;&#30340;&#29615;&#22659;&#20013;&#20248;&#21270;&#32858;&#21512;&#28789;&#27963;&#24615;&#25552;&#20379;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#25955;&#20613;&#37324;&#21494;&#21464;&#25442;&#65288;DFT&#65289;&#21644;&#32858;&#31867;&#25216;&#26415;&#35782;&#21035;&#24314;&#31569;&#29289;&#23621;&#27665;&#30340;&#27963;&#21160;&#27169;&#24335;&#12290;&#20854;&#27425;&#65292;&#30740;&#31350;&#35780;&#20272;&#20102;DR&#20107;&#20214;&#26399;&#38388;&#20379;&#28909;&#36890;&#39118;&#31354;&#35843;&#65288;HVAC&#65289;&#31995;&#32479;&#30340;&#20998;&#25955;&#28789;&#27963;&#24615;&#25552;&#20379;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#20248;&#21270;&#25216;&#26415;&#36827;&#34892;&#31934;&#30830;&#30340;&#35774;&#22791;&#32423;&#20998;&#26512;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#20026;&#32858;&#21512;&#22120;&#22312;&#25972;&#20010;&#24314;&#31569;&#29289;&#28040;&#36153;&#20165;&#26377;&#19968;&#20010;&#26234;&#33021;&#30005;&#34920;&#30340;&#29615;&#22659;&#20013;&#25552;&#20379;&#28789;&#27963;&#24615;&#26381;&#21153;&#25552;&#20379;&#20102;&#19968;&#26465;&#38750;&#20405;&#20837;&#24615;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores the crucial interplay between aggregators and building occupants in activating flexibility through Demand Response (DR) programs, with a keen focus on achieving robust decarbonization and fortifying the resilience of the energy system amidst the uncertainties presented by Renewable Energy Sources (RES). Firstly, it introduces a methodology of optimizing aggregated flexibility provision strategies in environments with limited data, utilizing Discrete Fourier Transformation (DFT) and clustering techniques to identify building occupant's activity patterns. Secondly, the study assesses the disaggregated flexibility provision of Heating Ventilation and Air Conditioning (HVAC) systems during DR events, employing machine learning and optimization techniques for precise, device-level analysis. The first approach offers a non-intrusive pathway for aggregators to provide flexibility services in environments of a single smart meter for the whole building's consumption, while t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;Tabula&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#30740;&#31350;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2310.12746</link><description>&lt;p&gt;
TabuLa: &#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
TabuLa: Harnessing Language Models for Tabular Data Synthesis. (arXiv:2310.12746v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.12746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;Tabula&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#24037;&#20855;&#65292;&#36890;&#36807;&#30740;&#31350;&#25105;&#20204;&#21457;&#29616;&#65292;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#26041;&#38754;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#34920;&#26684;&#25968;&#25454;&#22312;&#21508;&#34892;&#19994;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#20197;&#21450;&#23545;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#26085;&#30410;&#20851;&#27880;&#65292;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#34920;&#26126;&#65292;&#21487;&#20197;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#29983;&#25104;&#36924;&#30495;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#30001;&#20110;LLMs&#23558;&#34920;&#26684;&#25968;&#25454;&#39044;&#22788;&#29702;&#20026;&#20840;&#25991;&#65292;&#23427;&#20204;&#20855;&#26377;&#36991;&#20813;&#39640;&#32500;&#24230;&#25968;&#25454;&#30340;&#29420;&#28909;&#32534;&#30721;&#25152;&#24102;&#26469;&#30340;&#32500;&#24230;&#28798;&#38590;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#35757;&#32451;&#26102;&#38388;&#38271;&#12289;&#22312;&#26032;&#20219;&#21153;&#19978;&#30340;&#21487;&#37325;&#29992;&#24615;&#26377;&#38480;&#65292;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#21462;&#20195;&#29616;&#26377;&#30340;&#34920;&#26684;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#32467;&#26500;&#30340;Tabula&#65292;&#19968;&#31181;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#22120;&#12290;&#36890;&#36807;Tabula&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#34920;&#26684;&#25968;&#25454;&#21512;&#25104;&#30340;&#32972;&#26223;&#19979;&#65292;&#37319;&#29992;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#35774;&#35745;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#38480;&#21046;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#19987;&#38376;&#38024;&#23545;&#34920;&#26684;&#25968;&#25454;&#23450;&#21046;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given the ubiquitous use of tabular data in industries and the growing concerns in data privacy and security, tabular data synthesis emerges as a critical research area. The recent state-of-the-art methods show that large language models (LLMs) can be adopted to generate realistic tabular data. As LLMs pre-process tabular data as full text, they have the advantage of avoiding the curse of dimensionality associated with one-hot encoding high-dimensional data. However, their long training time and limited re-usability on new tasks prevent them from replacing exiting tabular generative models. In this paper, we propose Tabula, a tabular data synthesizer based on the language model structure. Through Tabula, we demonstrate the inherent limitation of employing pre-trained language models designed for natural language processing (NLP) in the context of tabular data synthesis. Our investigation delves into the development of a dedicated foundational model tailored specifically for tabular dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25968;&#27010;&#24565;&#20462;&#25913;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.11875</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20998;&#25968;&#27010;&#24565;&#65306;&#22686;&#36827;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Fractional Concepts in Neural Networks: Enhancing Activation and Loss Functions. (arXiv:2310.11875v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#20998;&#25968;&#27010;&#24565;&#20462;&#25913;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#31070;&#32463;&#20803;&#30340;&#28608;&#27963;&#20989;&#25968;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#24182;&#25552;&#39640;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#20998;&#25968;&#27010;&#24565;&#20462;&#25913;&#28608;&#27963;&#21644;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#20801;&#35768;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#30830;&#23450;&#35757;&#32451;&#36807;&#31243;&#30340;&#20998;&#25968;&#23548;&#25968;&#38454;&#25968;&#20316;&#20026;&#39069;&#22806;&#30340;&#36229;&#21442;&#25968;&#26469;&#23450;&#20041;&#21644;&#20248;&#21270;&#20854;&#28608;&#27963;&#20989;&#25968;&#12290;&#36825;&#23558;&#20351;&#24471;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#20803;&#33021;&#22815;&#35843;&#25972;&#20854;&#28608;&#27963;&#20989;&#25968;&#20197;&#26356;&#22909;&#22320;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#24182;&#20943;&#23569;&#36755;&#20986;&#38169;&#35823;&#65292;&#26377;&#21487;&#33021;&#25552;&#39640;&#32593;&#32476;&#30340;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The paper presents a method for using fractional concepts in a neural network to modify the activation and loss functions. The methodology allows the neural network to define and optimize its activation functions by determining the fractional derivative order of the training process as an additional hyperparameter. This will enable neurons in the network to adjust their activation functions to match input data better and reduce output errors, potentially improving the network's overall performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;Banach&#31354;&#38388;&#30340;&#20248;&#21270;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2310.03696</link><description>&lt;p&gt;
&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;Banach&#31354;&#38388;&#20248;&#21270;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Banach Space Optimality of Neural Architectures With Multivariate Nonlinearities. (arXiv:2310.03696v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22312;Banach&#31354;&#38388;&#30340;&#20248;&#21270;&#24615;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#22823;&#31867;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;/&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#21464;&#20998;&#20248;&#21270;&#24615;&#65288;&#20855;&#20307;&#32780;&#35328;&#65292;&#26159;Banach&#31354;&#38388;&#20248;&#21270;&#24615;&#65289;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#27491;&#21017;&#21270;&#31639;&#23376;&#21644;k-&#24179;&#38754;&#21464;&#25442;&#26500;&#24314;&#20102;&#19968;&#31867;&#26032;&#30340;Banach&#31354;&#38388;&#23478;&#26063;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19968;&#20010;&#34920;&#31034;&#23450;&#29702;&#65292;&#35813;&#23450;&#29702;&#35828;&#26126;&#22312;&#36825;&#20123;Banach&#31354;&#38388;&#19978;&#25552;&#20986;&#30340;&#23398;&#20064;&#38382;&#39064;&#30340;&#35299;&#38598;&#23436;&#20840;&#30001;&#20855;&#26377;&#22810;&#21464;&#37327;&#38750;&#32447;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#25551;&#36848;&#12290;&#36825;&#20123;&#26368;&#20248;&#30340;&#26550;&#26500;&#20855;&#26377;&#36339;&#36291;&#36830;&#25509;&#65292;&#24182;&#19982;&#27491;&#20132;&#26435;&#37325;&#24402;&#19968;&#21270;&#21644;&#22810;&#32034;&#24341;&#27169;&#22411;&#24687;&#24687;&#30456;&#20851;&#65292;&#36825;&#20004;&#20010;&#27169;&#22411;&#22312;&#31070;&#32463;&#32593;&#32476;&#30028;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20852;&#36259;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#36866;&#29992;&#20110;&#21253;&#25324;&#20462;&#27491;&#32447;&#24615;&#21333;&#20803;&#65288;ReLU&#65289;&#28608;&#27963;&#20989;&#25968;&#12289;&#33539;&#25968;&#28608;&#27963;&#20989;&#25968;&#20197;&#21450;&#22312;&#34180;&#26495;/&#22810;&#27425;&#35856;&#27874;&#26679;&#26465;&#29702;&#35770;&#20013;&#25214;&#21040;&#30340;&#24452;&#21521;&#22522;&#20989;&#25968;&#22312;&#20869;&#30340;&#22810;&#31181;&#32463;&#20856;&#38750;&#32447;&#24615;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the variational optimality (specifically, the Banach space optimality) of a large class of neural architectures with multivariate nonlinearities/activation functions. To that end, we construct a new family of Banach spaces defined via a regularization operator and the $k$-plane transform. We prove a representer theorem that states that the solution sets to learning problems posed over these Banach spaces are completely characterized by neural architectures with multivariate nonlinearities. These optimal architectures have skip connections and are tightly connected to orthogonal weight normalization and multi-index models, both of which have received considerable interest in the neural network community. Our framework is compatible with a number of classical nonlinearities including the rectified linear unit (ReLU) activation function, the norm activation function, and the radial basis functions found in the theory of thin-plate/polyharmonic splines. We also show that the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#32622;&#25442;&#32676;&#20316;&#29992;&#19979;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#65292;&#24182;&#36890;&#36807;&#34892;&#21015;&#24335;&#21464;&#37327;&#30340;&#30452;&#31215;&#25551;&#36848;&#20102;&#31561;&#21464;&#25110;&#19981;&#21464;&#23376;&#21464;&#37327;&#30340;&#29305;&#24615;&#21644;&#22855;&#24322;&#24615;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#26435;&#20540;&#20849;&#20139;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#31561;&#21464;&#21644;&#19981;&#21464;&#32447;&#24615;&#32593;&#32476;&#21442;&#25968;&#21270;&#21644;&#35774;&#35745;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2309.13736</link><description>&lt;p&gt;
&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#24615;&#36136;&#65306;&#32622;&#25442;&#32676;&#19979;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
Geometry of Linear Neural Networks: Equivariance and Invariance under Permutation Groups. (arXiv:2309.13736v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.13736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#32447;&#24615;&#31070;&#32463;&#32593;&#32476;&#22312;&#32622;&#25442;&#32676;&#20316;&#29992;&#19979;&#30340;&#31561;&#21464;&#24615;&#21644;&#19981;&#21464;&#24615;&#65292;&#24182;&#36890;&#36807;&#34892;&#21015;&#24335;&#21464;&#37327;&#30340;&#30452;&#31215;&#25551;&#36848;&#20102;&#31561;&#21464;&#25110;&#19981;&#21464;&#23376;&#21464;&#37327;&#30340;&#29305;&#24615;&#21644;&#22855;&#24322;&#24615;&#12290;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#26435;&#20540;&#20849;&#20139;&#23646;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#20110;&#31561;&#21464;&#21644;&#19981;&#21464;&#32447;&#24615;&#32593;&#32476;&#21442;&#25968;&#21270;&#21644;&#35774;&#35745;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#32447;&#24615;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#21442;&#25968;&#21270;&#30340;&#20989;&#25968;&#38598;&#21512;&#26159;&#19968;&#20010;&#34892;&#21015;&#24335;&#21464;&#37327;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32622;&#25442;&#32676;&#20316;&#29992;&#19979;&#31561;&#21464;&#25110;&#19981;&#21464;&#30340;&#20989;&#25968;&#23376;&#21464;&#37327;&#12290;&#36825;&#26679;&#30340;&#32676;&#20316;&#29992;&#31034;&#20363;&#21253;&#25324;&#23545;&#22270;&#20687;&#30340;&#24179;&#31227;&#25110;90&#24230;&#26059;&#36716;&#12290;&#25105;&#20204;&#23558;&#36825;&#26679;&#30340;&#31561;&#21464;&#25110;&#19981;&#21464;&#23376;&#21464;&#37327;&#25551;&#36848;&#20026;&#34892;&#21015;&#24335;&#21464;&#37327;&#30340;&#30452;&#31215;&#65292;&#20174;&#20013;&#25512;&#23548;&#20986;&#20854;&#32500;&#24230;&#12289;&#27425;&#25968;&#12289;&#27431;&#20960;&#37324;&#24471;&#36317;&#31163;&#12289;&#20197;&#21450;&#22855;&#24322;&#24615;&#12290;&#25105;&#20204;&#23436;&#20840;&#21051;&#30011;&#20102;&#20219;&#24847;&#32622;&#25442;&#32676;&#30340;&#19981;&#21464;&#24615;&#65292;&#20197;&#21450;&#24490;&#29615;&#32676;&#30340;&#31561;&#21464;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#31232;&#30095;&#24615;&#21644;&#26435;&#20540;&#20849;&#20139;&#23646;&#24615;&#65292;&#23545;&#31561;&#21464;&#21644;&#19981;&#21464;&#32447;&#24615;&#32593;&#32476;&#30340;&#21442;&#25968;&#21270;&#21644;&#35774;&#35745;&#24471;&#20986;&#32467;&#35770;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#19981;&#21464;&#30340;&#32447;&#24615;&#20989;&#25968;&#37117;&#21487;&#20197;&#30001;&#19968;&#20010;&#20855;&#26377;&#26435;&#20540;&#20849;&#20139;&#23646;&#24615;&#30340;&#32447;&#24615;&#33258;&#32534;&#30721;&#22120;&#26469;&#21442;&#25968;&#21270;&#65292;&#35813;&#23646;&#24615;&#26159;&#30001;&#25152;&#32771;&#34385;&#32622;&#25442;&#30340;&#24490;&#29615;&#20998;&#35299;&#25152;&#24378;&#21152;&#30340;&#12290;&#31561;&#21464;&#20989;&#25968;&#30340;&#31209;&#21463;&#38480;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
The set of functions parameterized by a linear fully-connected neural network is a determinantal variety. We investigate the subvariety of functions that are equivariant or invariant under the action of a permutation group. Examples of such group actions are translations or $90^\circ$ rotations on images. We describe such equivariant or invariant subvarieties as direct products of determinantal varieties, from which we deduce their dimension, degree, Euclidean distance degree, and their singularities. We fully characterize invariance for arbitrary permutation groups, and equivariance for cyclic groups. We draw conclusions for the parameterization and the design of equivariant and invariant linear networks in terms of sparsity and weight-sharing properties. We prove that all invariant linear functions can be parameterized by a single linear autoencoder with a weight-sharing property imposed by the cycle decomposition of the considered permutation. The space of rank-bounded equivariant f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#35299;&#20915;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#22312;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15720</link><description>&lt;p&gt;
&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#22312;&#22238;&#24402;&#38382;&#39064;&#20013;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Surrogate-based Autotuning for Randomized Sketching Algorithms in Regression Problems. (arXiv:2308.15720v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#35299;&#20915;&#38543;&#26426;&#21270;&#33609;&#22270;&#31639;&#27861;&#20013;&#30340;&#21442;&#25968;&#36873;&#25321;&#38382;&#39064;&#65292;&#22312;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;&#20013;&#21462;&#24471;&#20102;&#25509;&#36817;&#26368;&#20248;&#24615;&#33021;&#30340;&#23454;&#35777;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#38543;&#26426;&#25968;&#20540;&#32447;&#24615;&#20195;&#25968;(RandNLA)&#20013;&#25552;&#20986;&#30340;&#31639;&#27861;&#22312;&#22788;&#29702;&#39640;&#32500;&#35745;&#31639;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#22909;&#30340;&#25928;&#26524;&#65292;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#32463;&#39564;&#24615;&#33021;&#20197;&#21450;&#24378;&#22823;&#30340;&#27010;&#29575;&#20445;&#35777;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#19968;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#25152;&#38480;&#21046;&#65292;&#21363;&#29992;&#25143;&#38656;&#35201;&#35774;&#32622;&#21508;&#31181;&#19981;&#21516;&#20110;&#20256;&#32479;NLA&#20013;&#20351;&#29992;&#30340;&#31639;&#27861;&#29305;&#23450;&#35843;&#21442;&#21442;&#25968;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#26469;&#35299;&#20915;RandNLA&#31639;&#27861;&#20013;&#21442;&#25968;&#36873;&#25321;&#30340;&#22522;&#30784;&#24615;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#22522;&#20110;&#33609;&#22270;&#21644;&#39044;&#22788;&#29702;(SAP)&#30340;&#38543;&#26426;&#21270;&#26368;&#23567;&#20108;&#20056;&#26041;&#27861;&#36827;&#34892;&#20102;&#26367;&#20195;&#27169;&#22411;&#33258;&#21160;&#35843;&#20248;&#30340;&#35814;&#32454;&#30740;&#31350;&#65292;&#36825;&#22312;&#29616;&#20195;RandNLA&#20013;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25104;&#21151;&#26696;&#20363;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;&#33258;&#21160;&#35843;&#20248;&#26041;&#27861;&#21487;&#20197;&#20197;&#27604;&#38543;&#26426;&#25628;&#32034;&#23569;&#32422;4&#20493;&#30340;&#35797;&#39564;&#25104;&#26412;&#23454;&#29616;&#25509;&#36817;&#26368;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithms from Randomized Numerical Linear Algebra (RandNLA) are known to be effective in handling high-dimensional computational problems, providing high-quality empirical performance as well as strong probabilistic guarantees. However, their practical application is complicated by the fact that the user needs to set various algorithm-specific tuning parameters which are different than those used in traditional NLA. This paper demonstrates how a surrogate-based autotuning approach can be used to address fundamental problems of parameter selection in RandNLA algorithms. In particular, we provide a detailed investigation of surrogate-based autotuning for sketch-and-precondition (SAP) based randomized least squares methods, which have been one of the great success stories in modern RandNLA. Empirical results show that our surrogate-based autotuning approach can achieve near-optimal performance with much less tuning cost than a random search (up to about 4x fewer trials of different para
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13969</link><description>&lt;p&gt;
&#27880;&#37325;&#27880;&#24847;&#21147;&#65306;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Fixating on Attention: Integrating Human Eye Tracking into Vision Transformers. (arXiv:2308.13969v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#30524;&#36861;&#36394;&#38598;&#25104;&#21040;&#35270;&#35273;Transformer&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22522;&#20110;Transformer&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#22312;&#22810;&#31181;&#35270;&#35273;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20851;&#38190;&#20219;&#21153;&#65292;&#22914;&#21307;&#23398;&#22270;&#20687;&#35299;&#37322;&#21644;&#33258;&#21160;&#39550;&#39542;&#65292;&#20173;&#28982;&#38656;&#35201;&#20381;&#36182;&#20154;&#31867;&#21028;&#26029;&#12290;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#20154;&#31867;&#35270;&#35273;&#36755;&#20837;&#65292;&#29305;&#21035;&#26159;&#36890;&#36807;&#30524;&#21160;&#20202;&#25910;&#38598;&#21040;&#30340;&#27880;&#35270;&#28857;&#65292;&#38598;&#25104;&#21040;Transformer&#27169;&#22411;&#20013;&#65292;&#20197;&#25552;&#39640;&#22312;&#22810;&#31181;&#39550;&#39542;&#24773;&#20917;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#20934;&#30830;&#24615;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#20154;&#31867;&#23454;&#39564;&#23545;&#35937;&#21644;Vision Transformer&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#65292;&#27880;&#35270;&#21306;&#22495;&#22312;&#24038;&#21491;&#39550;&#39542;&#20915;&#31574;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;&#36890;&#36807;&#27604;&#36739;&#20154;&#31867;&#27880;&#35270;&#22270;&#21644;ViT&#27880;&#24847;&#21147;&#26435;&#37325;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#21333;&#20010;&#22836;&#37096;&#21644;&#23618;&#20043;&#38388;&#30340;&#37325;&#21472;&#21160;&#24577;&#12290;&#36890;&#36807;&#21033;&#29992;&#36825;&#31181;&#37325;&#21472;&#21160;&#24577;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#21098;&#32780;&#19981;&#25439;&#22833;&#20934;&#30830;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#39550;&#39542;&#22330;&#26223;&#20449;&#24687;&#19982;&#27880;&#35270;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#37319;&#29992;&#8220;&#32852;&#21512;&#31354;&#38388;-&#27880;&#35270;&#8221;&#65288;JSF&#65289;&#30340;&#27880;&#24847;&#21147;&#35774;&#32622;&#12290;&#26368;&#21518;
&lt;/p&gt;
&lt;p&gt;
Modern transformer-based models designed for computer vision have outperformed humans across a spectrum of visual tasks. However, critical tasks, such as medical image interpretation or autonomous driving, still require reliance on human judgments. This work demonstrates how human visual input, specifically fixations collected from an eye-tracking device, can be integrated into transformer models to improve accuracy across multiple driving situations and datasets. First, we establish the significance of fixation regions in left-right driving decisions, as observed in both human subjects and a Vision Transformer (ViT). By comparing the similarity between human fixation maps and ViT attention weights, we reveal the dynamics of overlap across individual heads and layers. This overlap is exploited for model pruning without compromising accuracy. Thereafter, we incorporate information from the driving scene with fixation data, employing a "joint space-fixation" (JSF) attention setup. Lastly
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26032;&#30340;&#20943;&#23567;&#27169;&#22411;&#65288;ROM&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;Sinkhorn&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#23398;&#20064;&#20943;&#23569;&#30340;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#12290;</title><link>http://arxiv.org/abs/2308.13840</link><description>&lt;p&gt;
&#21463;&#26368;&#20248;&#20256;&#36755;&#21551;&#21457;&#30340;&#24930;&#34928;&#20943;&#38382;&#39064;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65306;&#21033;&#29992;Sinkhorn&#25439;&#22833;&#21644;Wasserstein&#26680;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport-inspired Deep Learning Framework for Slow-Decaying Problems: Exploiting Sinkhorn Loss and Wasserstein Kernel. (arXiv:2308.13840v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#26368;&#20248;&#20256;&#36755;&#29702;&#35770;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26032;&#30340;&#20943;&#23567;&#27169;&#22411;&#65288;ROM&#65289;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;Sinkhorn&#31639;&#27861;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#20174;&#32780;&#23454;&#29616;&#31934;&#30830;&#23398;&#20064;&#20943;&#23569;&#30340;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23567;&#27169;&#22411;&#65288;ROMs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#31185;&#23398;&#35745;&#31639;&#20013;&#20197;&#22788;&#29702;&#39640;&#32500;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ROM&#26041;&#27861;&#21487;&#33021;&#21482;&#33021;&#37096;&#20998;&#25429;&#25417;&#21040;&#25968;&#25454;&#30340;&#22266;&#26377;&#20960;&#20309;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#21253;&#25324;&#24213;&#23618;&#32467;&#26500;&#12289;&#20851;&#31995;&#21644;&#23545;&#31934;&#30830;&#24314;&#27169;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#26412;&#29305;&#24449;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#26368;&#20248;&#20256;&#36755;&#65288;OT&#65289;&#29702;&#35770;&#21644;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;ROM&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;Wasserstein&#36317;&#31163;&#20026;&#33258;&#23450;&#20041;&#26680;&#30340;&#26680;Proper&#27491;&#20132;&#20998;&#35299;&#65288;kPOD&#65289;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;Sinkhorn&#31639;&#27861;&#39640;&#25928;&#22320;&#35757;&#32451;&#24471;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;OT&#30340;&#38750;&#32447;&#24615;&#38477;&#32500;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#33021;&#22815;&#25429;&#25417;&#25968;&#25454;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#36825;&#23545;&#20110;&#20934;&#30830;&#23398;&#20064;&#20943;&#23569;&#30340;&#35299;&#20915;&#26041;&#26696;&#27969;&#24418;&#33267;&#20851;&#37325;&#35201;&#12290;&#19982;&#20256;&#32479;&#30340;&#22343;&#26041;&#35823;&#24046;&#25110;&#20132;&#21449;&#29109;&#31561;&#24230;&#37327;&#26631;&#20934;&#30456;&#27604;&#65292;
&lt;/p&gt;
&lt;p&gt;
Reduced order models (ROMs) are widely used in scientific computing to tackle high-dimensional systems. However, traditional ROM methods may only partially capture the intrinsic geometric characteristics of the data. These characteristics encompass the underlying structure, relationships, and essential features crucial for accurate modeling.  To overcome this limitation, we propose a novel ROM framework that integrates optimal transport (OT) theory and neural network-based methods. Specifically, we investigate the Kernel Proper Orthogonal Decomposition (kPOD) method exploiting the Wasserstein distance as the custom kernel, and we efficiently train the resulting neural network (NN) employing the Sinkhorn algorithm. By leveraging an OT-based nonlinear reduction, the presented framework can capture the geometric structure of the data, which is crucial for accurate learning of the reduced solution manifold. When compared with traditional metrics such as mean squared error or cross-entropy,
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#24418;&#24335;&#23450;&#20041;&#19979;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;&#22270;&#29305;&#24449;&#22686;&#24378;&#12289;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;GNNs&#26550;&#26500;&#22686;&#24378;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2308.08235</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#36798;&#33021;&#21147;&#65306;&#19968;&#39033;&#32508;&#36848;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Graph Neural Networks: A Survey. (arXiv:2308.08235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#19981;&#21516;&#24418;&#24335;&#23450;&#20041;&#19979;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#65292;&#24182;&#23545;&#22270;&#29305;&#24449;&#22686;&#24378;&#12289;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;GNNs&#26550;&#26500;&#22686;&#24378;&#36827;&#34892;&#20102;&#32508;&#36848;&#21644;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26159;&#35768;&#22810;&#19982;&#22270;&#30456;&#20851;&#30340;&#24212;&#29992;&#20013;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#23613;&#31649;&#22312;&#23454;&#36341;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#35768;&#22810;&#30740;&#31350;&#24037;&#20316;&#38598;&#20013;&#22312;GNNs&#30340;&#29702;&#35770;&#38480;&#21046;&#65292;&#21363;&#20854;&#34920;&#36798;&#33021;&#21147;&#12290;&#26089;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;GNNs&#30340;&#22270;&#21516;&#26500;&#35782;&#21035;&#33021;&#21147;&#65292;&#32780;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#21033;&#29992;&#23376;&#22270;&#35745;&#25968;&#21644;&#36830;&#25509;&#23398;&#20064;&#31561;&#23646;&#24615;&#26469;&#25551;&#36848;GNNs&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#36825;&#26356;&#21152;&#23454;&#38469;&#19988;&#26356;&#25509;&#36817;&#23454;&#38469;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20851;&#20110;&#27492;&#26041;&#21521;&#30340;&#32508;&#36848;&#35770;&#25991;&#21644;&#24320;&#28304;&#20179;&#24211;&#32508;&#21512;&#24635;&#32467;&#21644;&#35752;&#35770;&#36825;&#20123;&#27169;&#22411;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#19981;&#21516;&#24418;&#24335;&#23450;&#20041;&#19979;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22522;&#20110;&#19977;&#20010;&#31867;&#21035;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35780;&#36848;&#65292;&#21363;&#22270;&#29305;&#24449;&#22686;&#24378;&#12289;&#22270;&#25299;&#25169;&#22686;&#24378;&#21644;GNNs&#26550;&#26500;&#22686;&#24378;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) are effective machine learning models for many graph-related applications. Despite their empirical success, many research efforts focus on the theoretical limitations of GNNs, i.e., the GNNs expressive power. Early works in this domain mainly focus on studying the graph isomorphism recognition ability of GNNs, and recent works try to leverage the properties such as subgraph counting and connectivity learning to characterize the expressive power of GNNs, which are more practical and closer to real-world. However, no survey papers and open-source repositories comprehensively summarize and discuss models in this important direction. To fill the gap, we conduct a first survey for models for enhancing expressive power under different forms of definition. Concretely, the models are reviewed based on three categories, i.e., Graph feature enhancement, Graph topology enhancement, and GNNs architecture enhancement.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2308.05239</link><description>&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS).
&lt;/p&gt;
&lt;p&gt;
AI-Enabled Software and System Architecture Frameworks: Focusing on smart Cyber-Physical Systems (CPS). (arXiv:2308.05239v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05239
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#36866;&#24212;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#35201;&#27714;&#30340;AI-enabled&#36719;&#20214;&#21644;&#31995;&#32479;&#26550;&#26500;&#26694;&#26550;&#65292;&#37325;&#28857;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;(CPS)&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;ML-enabled CPS&#30340;&#20248;&#28857;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#29486;&#20013;&#25552;&#20986;&#20102;&#20960;&#31181;&#36719;&#20214;&#12289;&#31995;&#32479;&#21644;&#20225;&#19994;&#30340;&#26550;&#26500;&#26694;&#26550;&#12290;&#23427;&#20204;&#35782;&#21035;&#20102;&#21508;&#31181;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#24182;&#23450;&#20041;&#20102;&#26550;&#26500;&#30340;&#35266;&#28857;&#21644;&#35270;&#22270;&#65292;&#20197;&#26694;&#26550;&#21644;&#35299;&#20915;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20851;&#27880;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#30340;&#26550;&#26500;&#26694;&#26550;&#20013;&#65292;&#23578;&#26410;&#21253;&#25324;&#19982;&#25968;&#25454;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30456;&#20851;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#22914;&#25968;&#25454;&#31185;&#23398;&#23478;&#21644;&#25968;&#25454;&#24037;&#31243;&#24072;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#26410;&#33021;&#35299;&#20915;&#21709;&#24212;&#25968;&#25454;&#31185;&#23398;&#31038;&#21306;&#20851;&#27880;&#30340;&#26550;&#26500;&#35270;&#28857;&#21644;&#35270;&#22270;&#12290;&#26412;&#25991;&#36890;&#36807;&#24314;&#31435;&#36866;&#29992;&#20110;&#29616;&#20195;&#24212;&#29992;&#21644;&#32452;&#32455;&#30340;&#26550;&#26500;&#26694;&#26550;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#20854;&#20013;&#26426;&#22120;&#23398;&#20064;&#24037;&#20214;&#26222;&#36941;&#23384;&#22312;&#19988;&#33267;&#20851;&#37325;&#35201;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#26234;&#33021;&#29289;&#32852;&#32593;&#31995;&#32479;&#65288;CPS&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#32452;&#36866;&#24212;CPS&#39640;&#25928;&#24320;&#21457;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#20248;&#28857;&#26631;&#20934;&#65292;&#21363;&#29992;&#20110;&#35780;&#20272;&#21644;&#22522;&#20934;&#21270;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;CPS&#30340;&#26631;&#20934;&#65292;
&lt;/p&gt;
&lt;p&gt;
Several architecture frameworks for software, systems, and enterprises have been proposed in the literature. They identified various stakeholders and defined architecture viewpoints and views to frame and address stakeholder concerns. However, the stakeholders with data science and Machine Learning (ML) related concerns, such as data scientists and data engineers, are yet to be included in existing architecture frameworks. Therefore, they failed to address the architecture viewpoints and views responsive to the concerns of the data science community. In this paper, we address this gap by establishing the architecture frameworks adapted to meet the requirements of modern applications and organizations where ML artifacts are both prevalent and crucial. In particular, we focus on ML-enabled Cyber-Physical Systems (CPSs) and propose two sets of merit criteria for their efficient development and performance assessment, namely the criteria for evaluating and benchmarking ML-enabled CPSs, and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.00721</link><description>&lt;p&gt;
&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21435;&#37325;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Pre-trained Data Deduplication Model based on Active Learning. (arXiv:2308.00721v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00721
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#39318;&#27425;&#35299;&#20915;&#20102;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#65292;&#21516;&#26102;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#12290;&#36890;&#36807;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#19981;&#20165;&#38477;&#20302;&#20102;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#36824;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#25968;&#25454;&#36136;&#37327;&#38382;&#39064;&#26085;&#30410;&#31361;&#20986;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#25361;&#25112;&#26159;&#37325;&#22797;&#25968;&#25454;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#26159;&#30001;&#20110;&#25968;&#25454;&#30340;&#37325;&#22797;&#36755;&#20837;&#25110;&#22810;&#20010;&#25968;&#25454;&#28304;&#30340;&#21512;&#24182;&#23548;&#33268;&#30340;&#12290;&#36825;&#20123;"&#33039;&#25968;&#25454;"&#38382;&#39064;&#20005;&#37325;&#38480;&#21046;&#20102;&#22823;&#25968;&#25454;&#30340;&#26377;&#25928;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#25968;&#25454;&#21435;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#39044;&#35757;&#32451;&#21435;&#37325;&#27169;&#22411;&#65292;&#36825;&#26159;&#39318;&#27425;&#21033;&#29992;&#20027;&#21160;&#23398;&#20064;&#35299;&#20915;&#35821;&#20041;&#32423;&#21035;&#30340;&#21435;&#37325;&#38382;&#39064;&#30340;&#24037;&#20316;&#12290;&#35813;&#27169;&#22411;&#26500;&#24314;&#22312;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;Transformer&#19978;&#65292;&#24182;&#36890;&#36807;&#32454;&#35843;&#23558;&#20854;&#24212;&#29992;&#20110;&#24207;&#21015;&#20998;&#31867;&#20219;&#21153;&#65292;&#39318;&#27425;&#23558;Transformer&#21644;&#20027;&#21160;&#23398;&#20064;&#38598;&#25104;&#21040;&#31471;&#21040;&#31471;&#26550;&#26500;&#20013;&#65292;&#20197;&#36873;&#25321;&#26368;&#26377;&#20215;&#20540;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#37325;&#27169;&#22411;&#35757;&#32451;&#65292;&#21516;&#26102;&#39318;&#27425;&#37319;&#29992;R-Drop&#26041;&#27861;&#23545;&#27599;&#19968;&#36718;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#26082;&#33021;&#38477;&#20302;&#25163;&#21160;&#26631;&#35760;&#30340;&#25104;&#26412;&#65292;&#20063;&#33021;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the era of big data, the issue of data quality has become increasingly prominent. One of the main challenges is the problem of duplicate data, which can arise from repeated entry or the merging of multiple data sources. These "dirty data" problems can significantly limit the effective application of big data. To address the issue of data deduplication, we propose a pre-trained deduplication model based on active learning, which is the first work that utilizes active learning to address the problem of deduplication at the semantic level. The model is built on a pre-trained Transformer and fine-tuned to solve the deduplication problem as a sequence to classification task, which firstly integrate the transformer with active learning into an end-to-end architecture to select the most valuable data for deduplication model training, and also firstly employ the R-Drop method to perform data augmentation on each round of labeled data, which can reduce the cost of manual labeling and improve
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.10798</link><description>&lt;p&gt;
&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#20197;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;: &#19968;&#31181;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lightweight Parameter Pruning for Energy-Efficient Deep Learning: A Binarized Gating Module Approach. (arXiv:2302.10798v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#21270;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#26469;&#23454;&#29616;&#33410;&#33021;&#28145;&#24230;&#23398;&#20064;&#65292;&#21487;&#20197;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#65292;&#21253;&#21547;&#20108;&#20540;&#38376;&#25511;&#27169;&#22359;&#21644;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31070;&#32463;&#32593;&#32476;&#36234;&#26469;&#36234;&#22823;&#19988;&#26356;&#21152;&#22797;&#26434;&#65292;&#32511;&#33394;AI&#24050;&#32463;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#31038;&#21306;&#30340;&#20851;&#27880;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#37319;&#29992;&#23545;&#32593;&#32476;&#21442;&#25968;&#36827;&#34892;&#35009;&#21098;&#65292;&#20197;&#20943;&#23569;&#35757;&#32451;&#25512;&#26029;&#26102;&#30340;&#35745;&#31639;&#36127;&#33655;&#12290;&#28982;&#32780;&#65292;&#35009;&#21098;&#26041;&#26696;&#36890;&#24120;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#24320;&#38144;&#65292;&#22240;&#20026;&#38656;&#35201;&#36827;&#34892;&#36845;&#20195;&#35757;&#32451;&#21644;&#24494;&#35843;&#25110;&#37325;&#22797;&#35745;&#31639;&#21160;&#24577;&#35009;&#21098;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#35009;&#21098;&#31574;&#30053;&#65292;&#20197;&#23398;&#20064;&#36731;&#37327;&#32423;&#23376;&#32593;&#32476;&#65292;&#26082;&#33021;&#26368;&#23567;&#21270;&#33021;&#32791;&#65292;&#21448;&#33021;&#22312;&#32473;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#19982;&#23436;&#20840;&#21442;&#25968;&#21270;&#30340;&#32593;&#32476;&#20445;&#25345;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#35009;&#21098;&#26041;&#26696;&#20197;&#32511;&#33394;&#20026;&#23548;&#21521;&#65292;&#22240;&#20026;&#23427;&#20165;&#38656;&#35201;&#21160;&#24577;&#35009;&#21098;&#26041;&#27861;&#36827;&#34892;&#19968;&#27425;&#35757;&#32451;&#26469;&#21457;&#29616;&#26368;&#20339;&#30340;&#38745;&#24577;&#23376;&#32593;&#32476;&#12290;&#35813;&#26041;&#26696;&#30001;&#19968;&#20010;&#20108;&#36827;&#21046;&#38376;&#25511;&#27169;&#22359;&#21644;&#19968;&#20010;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#32452;&#25104;&#65292;&#20197;&#21457;&#29616;&#20855;&#26377;&#29992;&#25143;&#23450;&#20041;&#31232;&#30095;&#24230;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#31561;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35009;&#21098;&#21644;&#36716;&#25442;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#33021;&#37327;&#28040;&#32791;&#65292;&#21516;&#26102;&#22312;&#24050;&#26377;&#30340;&#22522;&#20934;&#27979;&#35797;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The subject of green AI has been gaining attention within the deep learning community given the recent trend of ever larger and more complex neural network models. Existing solutions for reducing the computational load of training at inference time usually involve pruning the network parameters. Pruning schemes often create extra overhead either by iterative training and fine-tuning for static pruning or repeated computation of a dynamic pruning graph. We propose a new parameter pruning strategy for learning a lighter-weight sub-network that minimizes the energy cost while maintaining comparable performance to the fully parameterised network on given downstream tasks. Our proposed pruning scheme is green-oriented, as it only requires a one-off training to discover the optimal static sub-networks by dynamic pruning methods. The pruning scheme consists of a binary gating module and a novel loss function to uncover sub-networks with user-defined sparsity. Our method enables pruning and tr
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;</title><link>http://arxiv.org/abs/2211.10227</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#30340;&#23545;&#25239;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Detection by Approximation of Ensemble Boundary. (arXiv:2211.10227v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.10227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Walsh&#31995;&#25968;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#29616;&#20102;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;&#26041;&#27861;&#65292;&#38024;&#23545;&#35299;&#20915;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#38598;&#25104;&#12290;&#35813;&#38598;&#25104;&#20351;&#29992;Walsh&#31995;&#25968;&#36827;&#34892;&#32452;&#21512;&#65292;&#33021;&#22815;&#36924;&#36817;&#24067;&#23572;&#20989;&#25968;&#24182;&#25511;&#21046;&#38598;&#25104;&#20915;&#31574;&#36793;&#30028;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#30340;&#20551;&#35774;&#26159;&#39640;&#26354;&#29575;&#30340;&#20915;&#31574;&#36793;&#30028;&#20801;&#35768;&#25214;&#21040;&#23545;&#25239;&#25200;&#21160;&#65292;&#20294;&#20250;&#25913;&#21464;&#20915;&#31574;&#36793;&#30028;&#30340;&#26354;&#29575;&#65292;&#32780;&#19982;&#28165;&#26224;&#22270;&#20687;&#30456;&#27604;&#65292;&#20351;&#29992;Walsh&#31995;&#25968;&#23545;&#20854;&#36827;&#34892;&#36924;&#36817;&#30340;&#26041;&#24335;&#20063;&#26377;&#25152;&#19981;&#21516;&#12290;&#36890;&#36807;&#35266;&#23519;&#28165;&#26224;&#22270;&#20687;&#21644;&#23545;&#25239;&#22270;&#20687;&#20043;&#38388;&#30340;Walsh&#31995;&#25968;&#36924;&#36817;&#24046;&#24322;&#65292;&#23454;&#39564;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#36801;&#31227;&#24615;&#21487;&#29992;&#20110;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#36924;&#36817;&#20915;&#31574;&#36793;&#30028;&#21487;&#33021;&#26377;&#21161;&#20110;&#29702;&#35299;DNN&#30340;&#23398;&#20064;&#21644;&#21487;&#36801;&#31227;&#24615;&#29305;&#24615;&#12290;&#23613;&#31649;&#26412;&#25991;&#30340;&#23454;&#39564;&#20351;&#29992;&#22270;&#20687;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#24314;&#27169;&#20004;&#31867;&#27169;&#24335;&#35782;&#21035;&#38382;&#39064;&#30340;&#38598;&#25104;&#36793;&#30028;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
A new method of detecting adversarial attacks is proposed for an ensemble of Deep Neural Networks (DNNs) solving two-class pattern recognition problems. The ensemble is combined using Walsh coefficients which are capable of approximating Boolean functions and thereby controlling the complexity of the ensemble decision boundary. The hypothesis in this paper is that decision boundaries with high curvature allow adversarial perturbations to be found, but change the curvature of the decision boundary, which is then approximated in a different way by Walsh coefficients compared to the clean images. By observing the difference in Walsh coefficient approximation between clean and adversarial images, it is shown experimentally that transferability of attack may be used for detection. Furthermore, approximating the decision boundary may aid in understanding the learning and transferability properties of DNNs. While the experiments here use images, the proposed approach of modelling two-class en
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.06433</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#20135;&#29983;&#19982;&#20154;&#31867;&#23545;&#40784;&#30340;&#35270;&#35273;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised video pretraining yields human-aligned visual representations. (arXiv:2210.06433v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.06433
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#33258;&#30417;&#30563;&#35757;&#32451;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;VITO&#24471;&#21040;&#20102;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65292;&#35813;&#26041;&#27861;&#22312;&#22270;&#20687;&#29702;&#35299;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36890;&#36807;&#35266;&#23519;&#23545;&#35937;&#21644;&#22330;&#26223;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26041;&#24335;&#23398;&#20064;&#21040;&#20102;&#24378;&#22823;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#38656;&#35201;&#26126;&#30830;&#30340;&#26102;&#38388;&#29702;&#35299;&#30340;&#29305;&#23450;&#20219;&#21153;&#20043;&#22806;&#65292;&#38745;&#24577;&#22270;&#20687;&#39044;&#35757;&#32451;&#20173;&#28982;&#26159;&#23398;&#20064;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#20027;&#27969;&#33539;&#24335;&#12290;&#25105;&#20204;&#23545;&#36825;&#31181;&#19981;&#21305;&#37197;&#25552;&#20986;&#20102;&#36136;&#30097;&#65292;&#24182;&#19988;&#38382;&#26159;&#21542;&#35270;&#39057;&#39044;&#35757;&#32451;&#21487;&#20197;&#20135;&#29983;&#20855;&#26377;&#20154;&#31867;&#24863;&#30693;&#29305;&#24449;&#30340;&#35270;&#35273;&#34920;&#31034;&#65306;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#27867;&#21270;&#24615;&#12289;&#23545;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#21644;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#31579;&#36873;&#35270;&#39057;&#30340;&#26032;&#39062;&#31243;&#24207;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#23545;&#27604;&#24615;&#26694;&#26550;&#65292;&#20174;&#20854;&#20013;&#30340;&#22797;&#26434;&#36716;&#25442;&#20013;&#23398;&#20064;&#12290;&#36825;&#31181;&#20174;&#35270;&#39057;&#20013;&#25552;&#28860;&#30693;&#35782;&#30340;&#31616;&#21333;&#33539;&#24335;&#34987;&#31216;&#20026;VITO&#65292;&#23427;&#20135;&#29983;&#30340;&#19968;&#33324;&#34920;&#31034;&#22312;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#19978;&#36828;&#36828;&#20248;&#20110;&#20808;&#21069;&#30340;&#35270;&#39057;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#19978;&#20248;&#20110;&#22270;&#20687;&#39044;&#35757;&#32451;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;VITO&#34920;&#31034;&#23545;&#33258;&#28982;&#21644;&#21512;&#25104;&#24418;&#21464;&#30340;&#40065;&#26834;&#24615;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans learn powerful representations of objects and scenes by observing how they evolve over time. Yet, outside of specific tasks that require explicit temporal understanding, static image pretraining remains the dominant paradigm for learning visual foundation models. We question this mismatch, and ask whether video pretraining can yield visual representations that bear the hallmarks of human perception: generalisation across tasks, robustness to perturbations, and consistency with human judgements. To that end we propose a novel procedure for curating videos, and develop a contrastive framework which learns from the complex transformations therein. This simple paradigm for distilling knowledge from videos, called VITO, yields general representations that far outperform prior video pretraining methods on image understanding tasks, and image pretraining methods on video understanding tasks. Moreover, VITO representations are significantly more robust to natural and synthetic deformati
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#26469;&#35299;&#20915;&#22810;&#23792;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#36817;&#20284;&#35823;&#24046;&#12290;</title><link>http://arxiv.org/abs/2202.13059</link><description>&lt;p&gt;
&#29992;&#29109;&#36817;&#20284;&#30340;&#39640;&#26031;&#28151;&#21512;&#21464;&#20998;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Variational Inference with Gaussian Mixture by Entropy Approximation. (arXiv:2202.13059v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.13059
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#30340;&#21464;&#20998;&#25512;&#26029;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#26469;&#35299;&#20915;&#22810;&#23792;&#24615;&#30340;&#38382;&#39064;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#36817;&#20284;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#20998;&#25512;&#26029;&#26159;&#19968;&#31181;&#29992;&#20110;&#36817;&#20284;&#26080;&#27861;&#22788;&#29702;&#30340;&#21518;&#39564;&#20998;&#24067;&#20197;&#37327;&#21270;&#26426;&#22120;&#23398;&#20064;&#19981;&#30830;&#23450;&#24615;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#21333;&#23792;&#30340;&#39640;&#26031;&#20998;&#24067;&#36890;&#24120;&#34987;&#36873;&#25321;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#65292;&#24456;&#38590;&#36924;&#36817;&#22810;&#23792;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#28151;&#21512;&#20998;&#24067;&#20316;&#20026;&#21442;&#25968;&#20998;&#24067;&#12290;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#36827;&#34892;&#21464;&#20998;&#25512;&#26029;&#30340;&#19968;&#20010;&#20027;&#35201;&#38590;&#28857;&#26159;&#22914;&#20309;&#36817;&#20284;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#12290;&#25105;&#20204;&#23558;&#39640;&#26031;&#28151;&#21512;&#30340;&#29109;&#36817;&#20284;&#20026;&#21333;&#23792;&#39640;&#26031;&#30340;&#29109;&#20043;&#21644;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#26512;&#35745;&#31639;&#24471;&#21040;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#30495;&#23454;&#29109;&#19982;&#36817;&#20284;&#29109;&#20043;&#38388;&#30340;&#36817;&#20284;&#35823;&#24046;&#65292;&#20197;&#20415;&#25581;&#31034;&#25105;&#20204;&#30340;&#36817;&#20284;&#20309;&#26102;&#36215;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36817;&#20284;&#35823;&#24046;&#30001;&#39640;&#26031;&#28151;&#21512;&#22343;&#20540;&#20043;&#38388;&#36317;&#31163;&#19982;&#26041;&#24046;&#20043;&#21644;&#30340;&#27604;&#29575;&#25511;&#21046;&#12290;&#27492;&#22806;&#65292;&#24403;&#39640;&#26031;&#28151;&#21512;&#32452;&#20214;&#30340;&#25968;&#37327;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#26102;&#65292;&#36817;&#20284;&#35823;&#24046;&#36235;&#36817;&#20110;&#38646;&#12290;
&lt;/p&gt;
&lt;p&gt;
Variational inference is a technique for approximating intractable posterior distributions in order to quantify the uncertainty of machine learning. Although the unimodal Gaussian distribution is usually chosen as a parametric distribution, it hardly approximates the multimodality. In this paper, we employ the Gaussian mixture distribution as a parametric distribution. A main difficulty of variational inference with the Gaussian mixture is how to approximate the entropy of the Gaussian mixture. We approximate the entropy of the Gaussian mixture as the sum of the entropy of the unimodal Gaussian, which can be analytically calculated. In addition, we theoretically analyze the approximation error between the true entropy and approximated one in order to reveal when our approximation works well. Specifically, the approximation error is controlled by the ratios of the distances between the means to the sum of the variances of the Gaussian mixture. Furthermore, it converges to zero when the 
&lt;/p&gt;</description></item><item><title>DS^3M&#26159;&#19968;&#31181;&#28145;&#24230;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#21644;&#19981;&#35268;&#21017;&#20999;&#25442;&#26426;&#21046;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#32463;&#27982;&#25928;&#30410;&#21644;&#26356;&#28145;&#20837;&#30340;&#29616;&#35937;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2106.02329</link><description>&lt;p&gt;
&#28145;&#24230;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;DS^3M&#65289;&#29992;&#20110;&#20855;&#26377;&#20999;&#25442;&#26426;&#21046;&#30340;&#38750;&#32447;&#24615;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series Forecasting with Regime Switching. (arXiv:2106.02329v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02329
&lt;/p&gt;
&lt;p&gt;
DS^3M&#26159;&#19968;&#31181;&#28145;&#24230;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#29992;&#20110;&#20934;&#30830;&#39044;&#27979;&#20855;&#26377;&#22797;&#26434;&#38750;&#32447;&#24615;&#20381;&#36182;&#21644;&#19981;&#35268;&#21017;&#20999;&#25442;&#26426;&#21046;&#30340;&#26102;&#38388;&#24207;&#21015;&#65292;&#20197;&#23454;&#29616;&#32463;&#27982;&#25928;&#30410;&#21644;&#26356;&#28145;&#20837;&#30340;&#29616;&#35937;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#32463;&#24120;&#23637;&#31034;&#22797;&#26434;&#30340;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#20197;&#21450;&#19981;&#35268;&#21017;&#30340;&#20999;&#25442;&#26426;&#21046;&#34892;&#20026;&#12290;&#36825;&#20123;&#29305;&#24449;&#22312;&#24314;&#27169;&#12289;&#25512;&#29702;&#21644;&#23545;&#28508;&#22312;&#38543;&#26426;&#29616;&#35937;&#25552;&#20379;&#28145;&#20837;&#29702;&#35299;&#26041;&#38754;&#37117;&#23384;&#22312;&#25216;&#26415;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#24314;&#27169;&#26694;&#26550;&#65292;&#21363;&#28145;&#24230;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;DS^3M&#65289;&#12290;&#35813;&#26694;&#26550;&#26088;&#22312;&#20934;&#30830;&#39044;&#27979;&#27492;&#31867;&#26102;&#38388;&#24207;&#21015;&#65292;&#21516;&#26102;&#29087;&#32451;&#22320;&#35782;&#21035;&#21160;&#24577;&#20013;&#38544;&#34255;&#30340;&#19981;&#35268;&#21017;&#20999;&#25442;&#26426;&#21046;&#12290;&#36825;&#31181;&#35782;&#21035;&#19981;&#20165;&#22312;&#32463;&#27982;&#19978;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#65292;&#36824;&#26377;&#21161;&#20110;&#26356;&#28145;&#20837;&#22320;&#29702;&#35299;&#28508;&#22312;&#29616;&#35937;&#12290;&#22312;DS^3M&#20013;&#65292;&#35813;&#26550;&#26500;&#20351;&#29992;&#31163;&#25955;&#28508;&#22312;&#21464;&#37327;&#26469;&#34920;&#31034;&#20999;&#25442;&#26426;&#21046;&#65292;&#20351;&#29992;&#36830;&#32493;&#28508;&#22312;&#21464;&#37327;&#26469;&#32771;&#34385;&#38543;&#26426;&#39537;&#21160;&#22240;&#32032;&#12290;&#36890;&#36807;&#23558;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65288;RNN&#65289;&#19982;&#38750;&#32447;&#24615;&#20999;&#25442;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSSM&#65289;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#25104;&#21151;&#25429;&#25417;&#20102;&#38750;&#32447;&#24615;&#20381;&#36182;&#20851;&#31995;&#21644;&#19981;&#35268;&#21017;&#20999;&#25442;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern time series data often display complex nonlinear dependencies along with irregular regime-switching behaviors. These features present technical challenges in modeling, inference, and in offering insightful understanding into the underlying stochastic phenomena. To tackle these challenges, we introduce a novel modeling framework known as the Deep Switching State Space Model (DS$^3$M). This framework is engineered to make accurate forecasts for such time series while adeptly identifying the irregular regimes hidden within the dynamics. These identifications not only have significant economic ramifications but also contribute to a deeper understanding of the underlying phenomena. In DS$^3$M, the architecture employs discrete latent variables to represent regimes and continuous latent variables to account for random driving factors. By melding a Recurrent Neural Network (RNN) with a nonlinear Switching State Space Model (SSSM), we manage to capture the nonlinear dependencies and irr
&lt;/p&gt;</description></item></channel></rss>