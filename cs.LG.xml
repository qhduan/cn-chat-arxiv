<rss version="2.0"><channel><title>Chat Arxiv cs.LG</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.LG</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10568</link><description>&lt;p&gt;
MoPE&#65306;&#36890;&#36807;Prompt&#19987;&#23478;&#28151;&#21512;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
MoPE: Parameter-Efficient and Scalable Multimodal Fusion via Mixture of Prompt Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MoPE&#25216;&#26415;&#65292;&#36890;&#36807;&#35299;&#24320;&#25552;&#31034;&#20197;&#33258;&#36866;&#24212;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#65292;&#24341;&#20837;&#20102;&#28151;&#21512;Prompt&#19987;&#23478;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#20013;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt&#35843;&#25972;&#24050;&#32463;&#35777;&#26126;&#22312;&#34701;&#21512;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#21333;&#27169;&#22522;&#30784;&#27169;&#22411;&#26102;&#20855;&#26377;&#21442;&#25968;&#25928;&#29575;&#24615;&#12290;&#28982;&#32780;&#65292;&#20854;&#26377;&#38480;&#30340;&#36866;&#24212;&#24615;&#21644;&#34920;&#36798;&#33021;&#21147;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#19982;&#20854;&#20182;&#35843;&#25972;&#26041;&#27861;&#30456;&#27604;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#31616;&#21333;&#25552;&#31034;&#35299;&#24320;&#20197;&#33258;&#36866;&#24212;&#22320;&#25429;&#33719;&#25968;&#25454;&#38598;&#32423;&#21644;&#23454;&#20363;&#32423;&#29305;&#24449;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#24314;&#31435;&#22312;&#36825;&#31181;&#35299;&#24320;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Prompt&#19987;&#23478;&#30340;&#28151;&#21512;&#65288;MoPE&#65289;&#25216;&#26415;&#26469;&#22686;&#24378;&#34920;&#36798;&#33021;&#21147;&#12290;MoPE&#21033;&#29992;&#22810;&#27169;&#24577;&#37197;&#23545;&#20808;&#39564;&#22312;&#27599;&#20010;&#23454;&#20363;&#22522;&#30784;&#19978;&#36335;&#30001;&#26368;&#26377;&#25928;&#30340;&#25552;&#31034;&#12290;&#19982;&#31616;&#21333;&#25552;&#31034;&#30456;&#27604;&#65292;&#25105;&#20204;&#22522;&#20110;MoPE&#30340;&#26465;&#20214;&#25552;&#31034;&#23545;&#22810;&#27169;&#24577;&#34701;&#21512;&#20855;&#26377;&#26356;&#22823;&#30340;&#34920;&#36798;&#33021;&#21147;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#21644;&#21487;&#35757;&#32451;&#21442;&#25968;&#24635;&#25968;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#19968;&#20010;&#19987;&#23478;&#36335;&#30001;&#30340;&#27491;&#21017;&#21270;&#39033;&#65292;&#23548;&#33268;&#19987;&#23478;&#30340;&#19981;&#26029;&#21457;&#23637;&#19987;&#38271;&#65292;&#19981;&#21516;&#19987;&#23478;&#19987;&#27880;&#20110;&#19981;&#21516;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10568v1 Announce Type: cross  Abstract: Prompt-tuning has demonstrated parameter-efficiency in fusing unimodal foundation models for multimodal tasks. However, its limited adaptivity and expressiveness lead to suboptimal performance when compared with other tuning methods. In this paper, we address this issue by disentangling the vanilla prompts to adaptively capture dataset-level and instance-level features. Building upon this disentanglement, we introduce the mixture of prompt experts (MoPE) technique to enhance expressiveness. MoPE leverages multimodal pairing priors to route the most effective prompt on a per-instance basis. Compared to vanilla prompting, our MoPE-based conditional prompting exhibits greater expressiveness for multimodal fusion, scaling better with the training data and the overall number of trainable parameters. We also study a regularization term for expert routing, leading to emergent expert specialization, where different experts focus on different c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#38797;&#28857;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#36895;&#29575;&#20043;&#21069;&#21482;&#38024;&#23545;&#21452;&#32447;&#24615;&#30446;&#26631;&#24050;&#30693;&#12290;</title><link>https://arxiv.org/abs/2403.02912</link><description>&lt;p&gt;
&#20855;&#26377;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#25910;&#25947;&#36895;&#29575;&#30340;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#29992;&#20110;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#38797;&#28857;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Mirror Descent Algorithms with Nearly Dimension-Independent Rates for Differentially-Private Stochastic Saddle-Point Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02912
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#30340;&#38543;&#26426;&#38797;&#28857;&#38382;&#39064;&#30340;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;&#65292;&#23454;&#29616;&#20102;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#36895;&#29575;&#20043;&#21069;&#21482;&#38024;&#23545;&#21452;&#32447;&#24615;&#30446;&#26631;&#24050;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22810;&#38754;&#20307;&#35774;&#32622;&#20013;&#20855;&#26377;&#24046;&#20998;&#38544;&#31169;&#65288;DP&#65289;&#30340;&#38543;&#26426;&#65288;&#20984;&#20985;&#65289;&#38797;&#28857;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#38543;&#26426;&#38236;&#20687;&#19979;&#38477;&#30340;&#65288;&#1013;&#65292;&#948;&#65289;-DP&#31639;&#27861;&#65292;&#20854;&#23454;&#29616;&#20102;&#39044;&#26399;&#23545;&#20598;&#38388;&#38553;&#30340;&#20960;&#20046;&#19982;&#32500;&#24230;&#26080;&#20851;&#30340;&#25910;&#25947;&#36895;&#29575;&#65292;&#36825;&#31181;&#20445;&#35777;&#22312;&#20197;&#21069;&#21482;&#38024;&#23545;&#21452;&#32447;&#24615;&#30446;&#26631;&#24050;&#30693;&#12290;&#23545;&#20110;&#20984;&#20985;&#21644;&#19968;&#38454;&#24179;&#28369;&#38543;&#26426;&#30446;&#26631;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23454;&#29616;&#20102;&#19968;&#20010;&#29575;&#65292;&#21363;sqrt(log(d)/n) + (log(d)^{3/2}/[n&#1013;])^{1/3}&#65292;&#20854;&#20013;d&#26159;&#38382;&#39064;&#30340;&#32500;&#24230;&#65292;n&#26159;&#25968;&#25454;&#38598;&#22823;&#23567;&#12290;&#22312;&#39069;&#22806;&#30340;&#20108;&#38454;&#24179;&#28369;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#23558;&#39044;&#26399;&#38388;&#38553;&#30340;&#36895;&#29575;&#25913;&#36827;&#20026;sqrt(log(d)/n) + (log(d)^{3/2}/[n&#1013;])^{2/5}&#12290;&#22312;&#36825;&#31181;&#39069;&#22806;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#20351;&#29992;&#20559;&#24046;&#20943;&#23569;&#30340;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#35777;&#26126;&#20102;&#23545;&#20598;&#38388;&#38553;&#21463;&#24120;&#25968;&#25104;&#21151;&#27010;&#29575;&#30340;&#30028;&#20026;log(d)/sqrt(n) + log(d)/[n&#1013;]^{1/2}&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02912v1 Announce Type: cross  Abstract: We study the problem of differentially-private (DP) stochastic (convex-concave) saddle-points in the polyhedral setting. We propose $(\varepsilon, \delta)$-DP algorithms based on stochastic mirror descent that attain nearly dimension-independent convergence rates for the expected duality gap, a type of guarantee that was known before only for bilinear objectives. For convex-concave and first-order-smooth stochastic objectives, our algorithms attain a rate of $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{1/3}$, where $d$ is the dimension of the problem and $n$ the dataset size. Under an additional second-order-smoothness assumption, we improve the rate on the expected gap to $\sqrt{\log(d)/n} + (\log(d)^{3/2}/[n\varepsilon])^{2/5}$. Under this additional assumption, we also show, by using bias-reduced gradient estimators, that the duality gap is bounded by $\log(d)/\sqrt{n} + \log(d)/[n\varepsilon]^{1/2}$ with constant success pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2311.02960</link><description>&lt;p&gt;
&#36890;&#36807;&#23618;&#38388;&#29305;&#24449;&#21387;&#32553;&#21644;&#24046;&#21035;&#24615;&#23398;&#20064;&#29702;&#35299;&#28145;&#24230;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination. (arXiv:2311.02960v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.02960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#65292;&#25581;&#31034;&#20102;&#28145;&#24230;&#32593;&#32476;&#22312;&#23618;&#32423;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#28436;&#21270;&#27169;&#24335;&#12290;&#30740;&#31350;&#21457;&#29616;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#36215;&#21040;&#20102;&#19982;&#28145;&#23618;&#38750;&#32447;&#24615;&#32593;&#32476;&#31867;&#20284;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#24050;&#32463;&#35777;&#26126;&#26159;&#20174;&#21407;&#22987;&#25968;&#25454;&#20013;&#23398;&#20064;&#26377;&#24847;&#20041;&#29305;&#24449;&#30340;&#19968;&#31181;&#39640;&#25928;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#32593;&#32476;&#22914;&#20309;&#22312;&#19981;&#21516;&#23618;&#32423;&#19978;&#36827;&#34892;&#31561;&#32423;&#29305;&#24449;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#30740;&#31350;&#20013;&#38388;&#29305;&#24449;&#30340;&#32467;&#26500;&#25581;&#31034;&#36825;&#20010;&#35868;&#22242;&#12290;&#21463;&#21040;&#25105;&#20204;&#23454;&#35777;&#21457;&#29616;&#30340;&#32447;&#24615;&#23618;&#22312;&#29305;&#24449;&#23398;&#20064;&#20013;&#27169;&#20223;&#38750;&#32447;&#24615;&#32593;&#32476;&#20013;&#28145;&#23618;&#30340;&#35282;&#33394;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#28145;&#24230;&#32447;&#24615;&#32593;&#32476;&#22914;&#20309;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#21270;&#20026;&#36755;&#20986;&#65292;&#36890;&#36807;&#30740;&#31350;&#35757;&#32451;&#21518;&#30340;&#27599;&#20010;&#23618;&#30340;&#36755;&#20986;&#65288;&#21363;&#29305;&#24449;&#65289;&#22312;&#22810;&#31867;&#20998;&#31867;&#38382;&#39064;&#30340;&#32972;&#26223;&#19979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102;&#34913;&#37327;&#20013;&#38388;&#29305;&#24449;&#30340;&#31867;&#20869;&#21387;&#32553;&#21644;&#31867;&#38388;&#24046;&#21035;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#36890;&#36807;&#23545;&#36825;&#20004;&#20010;&#24230;&#37327;&#26631;&#20934;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#29305;&#24449;&#20174;&#27973;&#23618;&#21040;&#28145;&#23618;&#30340;&#28436;&#21464;&#36981;&#24490;&#30528;&#19968;&#31181;&#31616;&#21333;&#32780;&#37327;&#21270;&#30340;&#27169;&#24335;&#65292;&#21069;&#25552;&#26159;&#36755;&#20837;&#25968;&#25454;&#26159;
&lt;/p&gt;
&lt;p&gt;
Over the past decade, deep learning has proven to be a highly effective tool for learning meaningful features from raw data. However, it remains an open question how deep networks perform hierarchical feature learning across layers. In this work, we attempt to unveil this mystery by investigating the structures of intermediate features. Motivated by our empirical findings that linear layers mimic the roles of deep layers in nonlinear networks for feature learning, we explore how deep linear networks transform input data into output by investigating the output (i.e., features) of each layer after training in the context of multi-class classification problems. Toward this goal, we first define metrics to measure within-class compression and between-class discrimination of intermediate features, respectively. Through theoretical analysis of these two metrics, we show that the evolution of features follows a simple and quantitative pattern from shallow to deep layers when the input data is
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.10468</link><description>&lt;p&gt;
&#36830;&#25509;&#38544;&#34255;&#31070;&#32463;&#20803;&#65288;CHNNet&#65289;&#65306;&#19968;&#31181;&#24555;&#36895;&#25910;&#25947;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Connected Hidden Neurons (CHNNet): An Artificial Neural Network for Rapid Convergence. (arXiv:2305.10468v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10468
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#30456;&#20114;&#36830;&#25509;&#65292;&#21487;&#20197;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26680;&#24515;&#30446;&#30340;&#26159;&#27169;&#20223;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#30340;&#21151;&#33021;&#12290;&#28982;&#32780;&#65292;&#19982;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#19981;&#21516;&#65292;&#20256;&#32479;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#26159;&#25353;&#23618;&#27425;&#32467;&#26500;&#21270;&#30340;&#65292;&#36825;&#21487;&#33021;&#20250;&#22952;&#30861;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#65292;&#22240;&#20026;&#21516;&#19968;&#23618;&#20013;&#30340;&#31070;&#32463;&#20803;&#20043;&#38388;&#27809;&#26377;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#20026;&#24378;&#22823;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20854;&#20013;&#21516;&#19968;&#38544;&#34255;&#23618;&#20013;&#30340;&#38544;&#34255;&#31070;&#32463;&#20803;&#26159;&#20114;&#30456;&#36830;&#25509;&#30340;&#65292;&#20351;&#24471;&#31070;&#32463;&#20803;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#27169;&#24335;&#24182;&#21152;&#36895;&#25910;&#25947;&#36895;&#24230;&#12290;&#36890;&#36807;&#22312;&#27973;&#23618;&#21644;&#28145;&#23618;&#32593;&#32476;&#20013;&#23558;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#20316;&#20026;&#23436;&#20840;&#36830;&#25509;&#30340;&#23618;&#36827;&#34892;&#23454;&#39564;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#20010;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25910;&#25947;&#36895;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The core purpose of developing artificial neural networks was to mimic the functionalities of biological neural networks. However, unlike biological neural networks, traditional artificial neural networks are often structured hierarchically, which can impede the flow of information between neurons as the neurons in the same layer have no connections between them. Hence, we propose a more robust model of artificial neural networks where the hidden neurons, residing in the same hidden layer, are interconnected, enabling the neurons to learn complex patterns and speeding up the convergence rate. With the experimental study of our proposed model as fully connected layers in shallow and deep networks, we demonstrate that the model results in a significant increase in convergence rate.
&lt;/p&gt;</description></item><item><title>&#21333;&#32431;&#22797;&#24418;&#27744;&#21270;&#23618;NervePool&#22312;&#27744;&#21270;&#22270;&#32467;&#26500;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#39030;&#28857;&#20998;&#21306;&#29983;&#25104;&#21333;&#32431;&#22797;&#24418;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#24314;&#27169;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#32553;&#23567;&#39640;&#32500;&#21333;&#32431;&#24418;&#65292;&#23454;&#29616;&#38477;&#37319;&#26679;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2305.06315</link><description>&lt;p&gt;
NervePool: &#19968;&#20010;&#21333;&#32431;&#22797;&#24418;&#27744;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
NervePool: A Simplicial Pooling Layer. (arXiv:2305.06315v1 [cs.CG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06315
&lt;/p&gt;
&lt;p&gt;
&#21333;&#32431;&#22797;&#24418;&#27744;&#21270;&#23618;NervePool&#22312;&#27744;&#21270;&#22270;&#32467;&#26500;&#25968;&#25454;&#26102;&#65292;&#22522;&#20110;&#39030;&#28857;&#20998;&#21306;&#29983;&#25104;&#21333;&#32431;&#22797;&#24418;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#21487;&#20197;&#28789;&#27963;&#22320;&#24314;&#27169;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#65292;&#21516;&#26102;&#32553;&#23567;&#39640;&#32500;&#21333;&#32431;&#24418;&#65292;&#23454;&#29616;&#38477;&#37319;&#26679;&#65292;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#28145;&#24230;&#23398;&#20064;&#38382;&#39064;&#65292;&#27744;&#21270;&#23618;&#23545;&#20110;&#38477;&#37319;&#26679;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#20943;&#23569;&#36807;&#25311;&#21512;&#37117;&#24456;&#37325;&#35201;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#27744;&#21270;&#23618;&#65292;NervePool&#65292;&#36866;&#29992;&#20110;&#21333;&#32431;&#22797;&#24418;&#32467;&#26500;&#30340;&#25968;&#25454;&#65292;&#36825;&#31181;&#32467;&#26500;&#26159;&#22270;&#30340;&#25512;&#24191;&#65292;&#21253;&#25324;&#27604;&#39030;&#28857;&#21644;&#36793;&#26356;&#39640;&#32500;&#24230;&#30340;&#21333;&#32431;&#24418;&#65307;&#36825;&#31181;&#32467;&#26500;&#21487;&#20197;&#26356;&#28789;&#27963;&#22320;&#24314;&#27169;&#26356;&#39640;&#38454;&#30340;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;&#21333;&#32431;&#22797;&#21512;&#32553;&#23567;&#26041;&#26696;&#22522;&#20110;&#39030;&#28857;&#30340;&#20998;&#21306;&#26500;&#24314;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#21487;&#20197;&#29983;&#25104;&#21333;&#32431;&#22797;&#24418;&#30340;&#20998;&#23618;&#34920;&#31034;&#65292;&#20197;&#19968;&#31181;&#23398;&#20064;&#30340;&#26041;&#24335;&#25240;&#21472;&#20449;&#24687;&#12290;NervePool&#24314;&#31435;&#22312;&#23398;&#20064;&#30340;&#39030;&#28857;&#32676;&#38598;&#20998;&#37197;&#30340;&#22522;&#30784;&#19978;&#65292;&#24182;&#20197;&#19968;&#31181;&#30830;&#23450;&#24615;&#30340;&#26041;&#24335;&#25193;&#23637;&#21040;&#39640;&#32500;&#21333;&#32431;&#24418;&#30340;&#32553;&#23567;&#12290;&#34429;&#28982;&#22312;&#23454;&#36341;&#20013;&#65292;&#27744;&#21270;&#25805;&#20316;&#26159;&#36890;&#36807;&#19968;&#31995;&#21015;&#30697;&#38453;&#36816;&#31639;&#26469;&#35745;&#31639;&#30340;&#65292;&#20294;&#26159;&#20854;&#25299;&#25169;&#21160;&#26426;&#26159;&#19968;&#20010;&#22522;&#20110;&#21333;&#32431;&#24418;&#26143;&#26143;&#30340;&#24182;&#38598;&#21644;&#31070;&#32463;&#22797;&#21512;&#20307;&#30340;&#38598;&#21512;&#26500;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;
For deep learning problems on graph-structured data, pooling layers are important for down sampling, reducing computational cost, and to minimize overfitting. We define a pooling layer, NervePool, for data structured as simplicial complexes, which are generalizations of graphs that include higher-dimensional simplices beyond vertices and edges; this structure allows for greater flexibility in modeling higher-order relationships. The proposed simplicial coarsening scheme is built upon partitions of vertices, which allow us to generate hierarchical representations of simplicial complexes, collapsing information in a learned fashion. NervePool builds on the learned vertex cluster assignments and extends to coarsening of higher dimensional simplices in a deterministic fashion. While in practice, the pooling operations are computed via a series of matrix operations, the topological motivation is a set-theoretic construction based on unions of stars of simplices and the nerve complex
&lt;/p&gt;</description></item></channel></rss>