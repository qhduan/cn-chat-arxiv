# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Label Differential Privacy via Aggregation.](http://arxiv.org/abs/2310.10092) | ä»¥å‰ç ”ç©¶è¡¨æ˜Žæœ´ç´ çš„LBAå’ŒLLPä¸èƒ½æä¾›æ ‡ç­¾å·®åˆ†éšç§ã€‚ä½†æœ¬ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨å…·æœ‰éšæœºæŠ½æ ·çš„åŠ æƒLBAå¯ä»¥æä¾›æ ‡ç­¾å·®åˆ†éšç§ã€‚ |
| [^2] | [Deep Backtracking Counterfactuals for Causally Compliant Explanations.](http://arxiv.org/abs/2310.07665) | æœ¬ç ”ç©¶æä¾›äº†ä¸€ç§å®žç”¨æ–¹æ³•ï¼Œç”¨äºŽåœ¨æ·±åº¦ç”Ÿæˆç»„ä»¶çš„ç»“æž„å› æžœæ¨¡åž‹ä¸­è®¡ç®—å›žæº¯åäº‹å®žã€‚é€šè¿‡åœ¨å› æžœæ¨¡åž‹çš„ç»“æž„åŒ–æ½œåœ¨ç©ºé—´ä¸­è§£å†³ä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆåäº‹å®žï¼Œå¹¶ä¸”ä¸Žå…¶ä»–æ–¹æ³•ç›¸æ¯”å…·å¤‡äº†å¤šåŠŸèƒ½ã€æ¨¡å—åŒ–å’Œç¬¦åˆå› æžœå…³ç³»çš„ç‰¹ç‚¹ã€‚ |
| [^3] | [On the near-optimality of betting confidence sets for bounded means.](http://arxiv.org/abs/2310.01547) | æœ¬æ–‡ä¸ºæŠ•æ³¨ç½®ä¿¡åŒºé—´çš„æ”¹è¿›æ€§èƒ½æä¾›äº†ç†è®ºè§£é‡Šï¼Œå¹¶æ¯”è¾ƒäº†ç»å…¸æ–¹æ³•ä¸­çš„å®½åº¦ï¼Œå‘çŽ°æŠ•æ³¨ç½®ä¿¡åŒºé—´å…·æœ‰è¾ƒå°çš„æžé™å®½åº¦ã€‚ |
| [^4] | [The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing.](http://arxiv.org/abs/2309.16883) | æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¢žå¼ºéšæœºå¹³æ»‘çš„æ–¹æ³•ï¼Œé€šè¿‡ç ”ç©¶éšæœºå¹³æ»‘å¼•å…¥çš„æ–¹å·®ä¸Žåˆ†ç±»å™¨çš„Lipschitzå¸¸æ•°å’Œè¾¹ç•Œä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠé‡‡ç”¨å•çº¯å½¢æŠ•å½±æŠ€æœ¯æ¥å¢žåŠ è®¤è¯é²æ£’åŠå¾„ã€‚ |
| [^5] | [Decoding trust: A reinforcement learning perspective.](http://arxiv.org/abs/2309.14598) | è¿™é¡¹ç ”ç©¶é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ­ç¤ºäº†åœ¨æˆå¯¹åœºæ™¯ä¸­é«˜æ°´å¹³çš„ä¿¡ä»»å’Œå¯ä¿¡åº¦æ˜¯é€šè¿‡åŒæ—¶é‡è§†åŽ†å²ç»éªŒå’Œæœªæ¥å›žæŠ¥æ¥å½¢æˆçš„ã€‚ |
| [^6] | [Fantastic Generalization Measures are Nowhere to be Found.](http://arxiv.org/abs/2309.13658) | æœ¬è®ºæ–‡ç ”ç©¶äº†è¿‡å‚æ•°åŒ–æƒ…å†µä¸‹çš„æ³›åŒ–ç•Œé™é—®é¢˜ï¼Œé€šè¿‡åˆ†æžå¤šä¸ªç•Œé™å‘çŽ°åœ¨è¿™ç§æƒ…å†µä¸‹æ— æ³•æ‰¾åˆ°ç´§è‡´çš„ç•Œé™æ¥è§£é‡Šç¥žç»ç½‘ç»œçš„å‡ºè‰²æ€§èƒ½ã€‚ |
| [^7] | [Reducing sequential change detection to sequential estimation.](http://arxiv.org/abs/2309.09111) | è¿™ä¸ªè®ºæ–‡å°†é¡ºåºå˜åŒ–æ£€æµ‹ç®€åŒ–ä¸ºé¡ºåºä¼°è®¡ï¼Œé€šè¿‡ä½¿ç”¨ç½®ä¿¡åºåˆ—æ¥æ£€æµ‹æ•°æ®æµä¸­çš„å˜åŒ–ï¼Œå¹¶è¯æ˜Žäº†è¯¥æ–¹æ³•å…·æœ‰å¼ºå¤§çš„ä¿è¯ã€‚ |
| [^8] | [Optimal and Fair Encouragement Policy Evaluation and Learning.](http://arxiv.org/abs/2309.07176) | æœ¬ç ”ç©¶æŽ¢è®¨äº†åœ¨å…³é”®é¢†åŸŸä¸­é’ˆå¯¹é¼“åŠ±æ”¿ç­–çš„æœ€ä¼˜å’Œå…¬å¹³è¯„ä¼°ä»¥åŠå­¦ä¹ çš„é—®é¢˜ï¼Œç ”ç©¶å‘çŽ°åœ¨äººç±»ä¸éµå¾ªæ²»ç–—å»ºè®®çš„æƒ…å†µä¸‹ï¼Œæœ€ä¼˜ç­–ç•¥è§„åˆ™åªæ˜¯å»ºè®®ã€‚åŒæ—¶ï¼Œé’ˆå¯¹æ²»ç–—çš„å¼‚è´¨æ€§å’Œå…¬å¹³è€ƒè™‘å› ç´ ï¼Œå†³ç­–è€…çš„æƒè¡¡å’Œå†³ç­–è§„åˆ™ä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ã€‚åœ¨ç¤¾ä¼šæœåŠ¡é¢†åŸŸï¼Œç ”ç©¶æ˜¾ç¤ºå­˜åœ¨ä¸€ä¸ªä½¿ç”¨å·®è·é—®é¢˜ï¼Œé‚£äº›æœ€æœ‰å¯èƒ½å—ç›Šçš„äººå´æ— æ³•èŽ·å¾—è¿™äº›ç›ŠæœåŠ¡ã€‚ |
| [^9] | [Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to Data Valuation.](http://arxiv.org/abs/2308.15709) | æœ¬è®ºæ–‡ç ”ç©¶äº†æ•°æ®ä»·å€¼è¯„ä¼°é¢ä¸´çš„éšç§æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§éšç§å‹å¥½çš„æ”¹è¿›æ–¹æ³•TKNN-Shapleyï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŠ¤éšç§çš„å‰æä¸‹èƒ½å¤Ÿè¯„ä¼°æ•°æ®è´¨é‡ï¼Œå…·æœ‰è¾ƒå¥½çš„éšç§-å®žç”¨æ€§æƒè¡¡ã€‚ |
| [^10] | [Will More Expressive Graph Neural Networks do Better on Generative Tasks?.](http://arxiv.org/abs/2308.11978) | æœ¬è®ºæ–‡è°ƒæŸ¥äº†æ›´å…·è¡¨çŽ°åŠ›çš„å›¾ç¥žç»ç½‘ç»œåœ¨åˆ†å­å›¾ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨çŽ°èƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ›¿æ¢å›¾ç”Ÿæˆæ¨¡åž‹çš„åŸºç¡€GNNæ¥è¿›è¡Œå®žéªŒã€‚ç ”ç©¶å‘çŽ°ï¼Œä½¿ç”¨æ›´å…·è¡¨çŽ°åŠ›çš„GNNå¯ä»¥æ”¹å–„ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚ |
| [^11] | [Energy Discrepancies: A Score-Independent Loss for Energy-Based Models.](http://arxiv.org/abs/2307.06431) | æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èƒ½é‡æ¨¡åž‹æŸå¤±å‡½æ•°ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–åˆ†æ•°è®¡ç®—æˆ–æ˜‚è´µçš„è’™ç‰¹å¡ç½—æ–¹æ³•çš„æƒ…å†µä¸‹ï¼Œè¿‘ä¼¼å®žçŽ°æ˜¾å¼åˆ†æ•°åŒ¹é…å’Œè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œå¹¶åœ¨å­¦ä¹ ä½Žç»´æ•°æ®åˆ†å¸ƒæ—¶å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚ |
| [^12] | [A stochastic optimization approach to minimize robust density power-based divergences for general parametric density models.](http://arxiv.org/abs/2307.05251) | æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§éšæœºä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºŽè§£å†³ç¨³å¥å¯†åº¦åŠŸçŽ‡åˆ†æ­§ï¼ˆDPDï¼‰åœ¨ä¸€èˆ¬å‚æ•°å¯†åº¦æ¨¡åž‹ä¸­çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ï¼Œå¹¶é€šè¿‡åº”ç”¨ä¼ ç»Ÿçš„éšæœºä¼˜åŒ–ç†è®ºæ¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚ |
| [^13] | [On the Linear Convergence of Policy Gradient under Hadamard Parameterization.](http://arxiv.org/abs/2305.19575) | æœ¬æ–‡ç ”ç©¶äº†Hadamardå‚æ•°åŒ–ä¸‹ç­–ç•¥æ¢¯åº¦çš„æ”¶æ•›æ€§ï¼Œè¯æ˜Žäº†ç®—æ³•å…·æœ‰å…¨å±€çº¿æ€§æ”¶æ•›æ€§å’Œå±€éƒ¨çº¿æ€§æ”¶æ•›é€Ÿåº¦æ›´å¿«çš„æ€§è´¨ã€‚ |
| [^14] | [SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise.](http://arxiv.org/abs/2305.16491) | è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ—¶é—´åºåˆ—åˆ†æžæ–¹æ³•ï¼Œå³SAMoSSAã€‚è¯¥æ–¹æ³•ç»¼åˆäº†å¤šå…ƒå¥‡å¼‚è°±åˆ†æžå’Œè‡ªå›žå½’åˆ†æžï¼Œåœ¨å­¦ä¹ æ—¶é—´åºåˆ—ä¸­çš„ç¡®å®šæ€§å’Œéšæœºæ€§æˆåˆ†æ–¹é¢å…·æœ‰è‰¯å¥½çš„ç†è®ºä¿è¯ã€‚ |
| [^15] | [DIVA: A Dirichlet Process Based Incremental Deep Clustering Algorithm via Variational Auto-Encoder.](http://arxiv.org/abs/2305.14067) | æœ¬æ–‡æå‡ºäº†DIVAç®—æ³•ï¼Œä¸€ä¸ªåŸºäºŽç‹„åˆ©å…‹é›·è¿‡ç¨‹çš„å¢žé‡æ·±åº¦èšç±»æ¡†æž¶ï¼Œåˆ©ç”¨æ— é™æ··åˆé«˜æ–¯ä½œä¸ºå…ˆéªŒï¼Œå¹¶åˆ©ç”¨ä¸€ç§è®°å¿†åŒ–çš„åœ¨çº¿å˜åˆ†æŽ¨ç†æ–¹æ³•å®žçŽ°ç°‡çš„åŠ¨æ€é€‚åº”ç§»åŠ¨ï¼Œè€Œä¸éœ€è¦å…ˆçŸ¥é“ç‰¹å¾çš„æ•°é‡ã€‚è¯¥ç®—æ³•è¡¨çŽ°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨å¢žé‡ç‰¹å¾çš„æƒ…å†µä¸‹ã€‚ |
| [^16] | [Moment Matching Denoising Gibbs Sampling.](http://arxiv.org/abs/2305.11650) | æœ¬æ–‡æå‡ºäº†åŠ¨é‡åŒ¹é…åŽ»å™ªGibbsé‡‡æ ·æ–¹æ³•ï¼Œå¯ä»¥åœ¨ç»™å®šâ€˜å˜ˆæ‚â€™çš„æ¨¡åž‹çš„æƒ…å†µä¸‹ï¼Œä»Žå¹²å‡€çš„æ¨¡åž‹ä¸­æœ‰æ•ˆåœ°è¿›è¡Œé‡‡æ ·ã€‚ |
| [^17] | [A Note on "Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms".](http://arxiv.org/abs/2304.04258) | æœ¬æ–‡æå‡ºäº†ä¸€ç§æ›´è‡ªç„¶å’Œå¯è§£é‡Šçš„æ•ˆç”¨å‡½æ•°ï¼Œæ›´å¥½åœ°åæ˜ äº†KNNæ¨¡åž‹çš„æ€§èƒ½ï¼Œæä¾›äº†ç›¸åº”è®¡ç®—è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•è¢«ç§°ä¸ºè½¯æ ‡ç­¾KNN-SVï¼Œä¸ŽåŽŸå§‹æ–¹æ³•å…·æœ‰ç›¸åŒçš„æ—¶é—´å¤æ‚åº¦ã€‚ |
| [^18] | [A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation.](http://arxiv.org/abs/2304.02858) | æœ¬æ–‡ç ”ç©¶äº†é›†æˆå­¦ä¹ å’Œæ•°æ®å¢žå¼ºæ–¹æ³•çš„åº”ç”¨ï¼Œé’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡è®¡ç®—è¯„ä¼°ï¼Œæ‰¾åˆ°äº†æœ€æœ‰æ•ˆçš„ç»„åˆã€‚ |
| [^19] | [Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces.](http://arxiv.org/abs/2211.14400) | è¯¥è®ºæ–‡ç ”ç©¶äº†åœ¨Sobolevå’ŒBesovç©ºé—´ä¸­ï¼Œä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„æ·±åº¦ç¥žç»ç½‘ç»œèƒ½å¤Ÿä»¥æ€Žæ ·çš„å‚æ•°æ•ˆçŽ‡é€¼è¿‘å‡½æ•°ï¼ŒåŒ…æ‹¬$L_p(\Omega)$èŒƒæ•°ä¸‹çš„è¯¯å·®åº¦é‡ã€‚æˆ‘ä»¬æä¾›äº†æ‰€æœ‰$1\leq p,q \leq \infty$å’Œ$s>0$çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„ä½æå–æŠ€æœ¯æ¥èŽ·å¾—å°–é”çš„ä¸Šç•Œã€‚ |
| [^20] | [$k$-Means Clustering for Persistent Homology.](http://arxiv.org/abs/2210.10003) | æœ¬æ–‡è¯æ˜Žäº†$k$-å‡å€¼èšç±»ç®—æ³•åœ¨æŒä¹…å›¾ç©ºé—´ä¸Šçš„æ”¶æ•›æ€§ï¼Œè§£å†³äº†ä»£æ•°æž„é€ å¯¼è‡´çš„å¤æ‚åº¦é—®é¢˜ï¼Œé€šè¿‡å®žéªŒè¯æ˜Žç›´æŽ¥åœ¨æŒä¹…å›¾å’ŒæŒä¹…åº¦é‡ä¸Šè¿›è¡Œèšç±»ä¼˜äºŽå‘é‡è¡¨ç¤ºã€‚ |
| [^21] | [Dimensionality Reduction and Wasserstein Stability for Kernel Regression.](http://arxiv.org/abs/2203.09347) | æœ¬æ–‡ç ”ç©¶äº†åœ¨é«˜ç»´å›žå½’æ¡†æž¶ä¸­çš„é™ç»´ä¸ŽWassersteinç¨³å®šæ€§åº”ç”¨ï¼Œé’ˆå¯¹åœ¨æ‰°åŠ¨è¾“å…¥æ•°æ®ç”¨äºŽæ‹Ÿåˆå›žå½’å‡½æ•°æ—¶å‡ºçŽ°çš„è¯¯å·®æŽ¨å¯¼äº†ç¨³å®šæ€§ç»“æžœï¼Œå¹¶åˆ©ç”¨ä¸»æˆåˆ†åˆ†æžå’Œæ ¸å›žå½’æ–‡çŒ®ä¸­çš„ä¼°è®¡ï¼ŒæŽ¨å¯¼äº†ä¸¤æ­¥æ³•çš„æ”¶æ•›é€Ÿåº¦ã€‚ |
| [^22] | [Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation.](http://arxiv.org/abs/2203.05400) | è¯¥è®ºæ–‡ç ”ç©¶äº†é«˜æ–¯è¿‡ç¨‹æ’å€¼ä¸­å…‰æ»‘å‚æ•°ä¼°è®¡çš„æ¸è¿‘ç•Œé™ã€‚ç»“æžœè¡¨æ˜Žï¼Œå…‰æ»‘å‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸èƒ½åœ¨æ¸è¿‘æ„ä¹‰ä¸‹æ¬ å¹³æ»‘çœŸå€¼ï¼Œå¹¶ä¸”æœ€å¤§ä¼¼ç„¶ä¼°è®¡èƒ½æ¢å¤ä¸€ç±»åˆ†æ®µæ”¯æŒè‡ªç›¸ä¼¼å‡½æ•°çš„çœŸå®žå…‰æ»‘åº¦ã€‚ |
| [^23] | [Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects.](http://arxiv.org/abs/2110.08693) | æœ¬æ–‡æå‡ºäº†ä¸€ç§æ‰©å±•å¹³æ–¹æ ¹é€Ÿåº¦å‡½æ•°çš„æ–°è¡¨ç¤ºæ–¹æ³•å’Œåº¦é‡æ–¹æ³•ï¼Œç”¨äºŽåˆ†æžå’Œæ¯”è¾ƒæ ‘çŠ¶ä¸‰ç»´ç‰©ä½“ï¼Œä»Žè€Œæé«˜ç‰©ä½“å½¢çŠ¶å·®å¼‚è®¡ç®—çš„ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚ |

# è¯¦ç»†

[^1]: é€šè¿‡èšåˆå®žçŽ°æ ‡ç­¾å·®åˆ†éšç§

    Label Differential Privacy via Aggregation. (arXiv:2310.10092v1 [cs.LG])

    [http://arxiv.org/abs/2310.10092](http://arxiv.org/abs/2310.10092)

    ä»¥å‰ç ”ç©¶è¡¨æ˜Žæœ´ç´ çš„LBAå’ŒLLPä¸èƒ½æä¾›æ ‡ç­¾å·®åˆ†éšç§ã€‚ä½†æœ¬ç ”ç©¶æ˜¾ç¤ºï¼Œä½¿ç”¨å…·æœ‰éšæœºæŠ½æ ·çš„åŠ æƒLBAå¯ä»¥æä¾›æ ‡ç­¾å·®åˆ†éšç§ã€‚

    

    åœ¨è®¸å¤šçŽ°å®žåº”ç”¨ä¸­ï¼Œç‰¹åˆ«æ˜¯ç”±äºŽéšç§é¢†åŸŸçš„æœ€æ–°å‘å±•ï¼Œè®­ç»ƒæ•°æ®å¯ä»¥è¿›è¡Œèšåˆï¼Œä»¥ä¿æŠ¤æ•æ„Ÿè®­ç»ƒæ ‡ç­¾çš„éšç§ã€‚åœ¨æ ‡ç­¾æ¯”ä¾‹å­¦ä¹ (LLP)æ¡†æž¶ä¸­ï¼Œæ•°æ®é›†è¢«åˆ’åˆ†ä¸ºç‰¹å¾å‘é‡çš„åŒ…ï¼Œåªèƒ½èŽ·å¾—æ¯ä¸ªåŒ…ä¸­æ ‡ç­¾çš„æ€»å’Œã€‚è¿›ä¸€æ­¥é™åˆ¶çš„é™åˆ¶å­¦ä¹ (LBA)æ˜¯åªèƒ½èŽ·å¾—åŒ…çš„ç‰¹å¾å‘é‡çš„æ€»å’Œï¼ˆå¯èƒ½æ˜¯åŠ æƒçš„ï¼‰ã€‚æˆ‘ä»¬ç ”ç©¶è¿™ç§èšåˆæŠ€æœ¯æ˜¯å¦èƒ½å¤Ÿåœ¨æ ‡ç­¾å·®åˆ†éšç§(label-DP)çš„æ¦‚å¿µä¸‹æä¾›éšç§ä¿è¯ï¼Œè¯¥æ¦‚å¿µä¹‹å‰åœ¨[Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22]ä¸­è¿›è¡Œäº†ç ”ç©¶ã€‚å¾ˆå®¹æ˜“çœ‹å‡ºï¼Œæœ´ç´ çš„LBAå’ŒLLPä¸èƒ½æä¾›æ ‡ç­¾å·®åˆ†éšç§ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬çš„ä¸»è¦ç»“æžœè¡¨æ˜Žï¼Œä½¿ç”¨å…·æœ‰$m$ä¸ªéšæœºæŠ½æ ·çš„ä¸ç›¸äº¤$k$-å¤§å°åŒ…çš„åŠ æƒLBAå®žé™…ä¸Šæ˜¯$(\varepsilon,

    In many real-world applications, in particular due to recent developments in the privacy landscape, training data may be aggregated to preserve the privacy of sensitive training labels. In the learning from label proportions (LLP) framework, the dataset is partitioned into bags of feature-vectors which are available only with the sum of the labels per bag. A further restriction, which we call learning from bag aggregates (LBA) is where instead of individual feature-vectors, only the (possibly weighted) sum of the feature-vectors per bag is available. We study whether such aggregation techniques can provide privacy guarantees under the notion of label differential privacy (label-DP) previously studied in for e.g. [Chaudhuri-Hsu'11, Ghazi et al.'21, Esfandiari et al.'22].  It is easily seen that naive LBA and LLP do not provide label-DP. Our main result however, shows that weighted LBA using iid Gaussian weights with $m$ randomly sampled disjoint $k$-sized bags is in fact $(\varepsilon, 
    
[^2]: æ·±åº¦å›žæº¯å¯¹å› æžœä¸€è‡´è§£é‡Šçš„åäº‹å®žæŽ¨ç†

    Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v1 [cs.AI])

    [http://arxiv.org/abs/2310.07665](http://arxiv.org/abs/2310.07665)

    æœ¬ç ”ç©¶æä¾›äº†ä¸€ç§å®žç”¨æ–¹æ³•ï¼Œç”¨äºŽåœ¨æ·±åº¦ç”Ÿæˆç»„ä»¶çš„ç»“æž„å› æžœæ¨¡åž‹ä¸­è®¡ç®—å›žæº¯åäº‹å®žã€‚é€šè¿‡åœ¨å› æžœæ¨¡åž‹çš„ç»“æž„åŒ–æ½œåœ¨ç©ºé—´ä¸­è§£å†³ä¼˜åŒ–é—®é¢˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿç”Ÿæˆåäº‹å®žï¼Œå¹¶ä¸”ä¸Žå…¶ä»–æ–¹æ³•ç›¸æ¯”å…·å¤‡äº†å¤šåŠŸèƒ½ã€æ¨¡å—åŒ–å’Œç¬¦åˆå› æžœå…³ç³»çš„ç‰¹ç‚¹ã€‚

    

    åäº‹å®žæŽ¨ç†å¯ä»¥é€šè¿‡å›žç­”åœ¨æ”¹å˜æƒ…å†µä¸‹ä¼šè§‚å¯Ÿåˆ°ä»€ä¹ˆæ¥æä¾›æœ‰ä»·å€¼çš„è§è§£ï¼Œæ¡ä»¶æ˜¯æ ¹æ®å®žé™…è§‚å¯Ÿã€‚è™½ç„¶ç»å…¸çš„ä»‹å…¥å¼è§£é‡Šå·²ç»å¾—åˆ°äº†å¹¿æ³›ç ”ç©¶ï¼Œå›žæº¯åŽŸåˆ™è¢«æå‡ºä½œä¸ºä¸€ç§ä¿æŒæ‰€æœ‰å› æžœå®šå¾‹å®Œæ•´æ€§çš„æ›¿ä»£å“²å­¦ï¼Œä½†å…¶ç ”ç©¶è¾ƒå°‘ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†åœ¨ç”±æ·±åº¦ç”Ÿæˆç»„ä»¶ç»„æˆçš„ç»“æž„å› æžœæ¨¡åž‹ä¸­è®¡ç®—å›žæº¯åäº‹å®žçš„å®žç”¨æ–¹æ³•ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯¹ç»“æž„åˆ†é…æ–½åŠ äº†æ¡ä»¶ï¼Œé€šè¿‡åœ¨å› æžœæ¨¡åž‹çš„ç»“æž„åŒ–æ½œåœ¨ç©ºé—´ä¸­è§£å†³ä¸€ä¸ªå¯è¡Œçš„çº¦æŸä¼˜åŒ–é—®é¢˜æ¥ç”Ÿæˆåäº‹å®žã€‚æˆ‘ä»¬çš„æ–¹æ³•è¿˜å¯ä»¥ä¸Žåäº‹å®žè§£é‡Šé¢†åŸŸçš„æ–¹æ³•è¿›è¡Œæ¯”è¾ƒã€‚ä¸Žè¿™äº›æ–¹æ³•ç›¸æ¯”ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä»£è¡¨äº†ä¸€ç§å¤šåŠŸèƒ½ã€æ¨¡å—åŒ–å’Œéµå®ˆå› æžœçš„æ›¿ä»£æ–¹æ¡ˆã€‚

    Counterfactuals can offer valuable insights by answering what would have been observed under altered circumstances, conditional on a factual observation. Whereas the classical interventional interpretation of counterfactuals has been studied extensively, backtracking constitutes a less studied alternative the backtracking principle has emerged as an alternative philosophy where all causal laws are kept intact. In the present work, we introduce a practical method for computing backtracking counterfactuals in structural causal models that consist of deep generative components. To this end, we impose conditions on the structural assignments that enable the generation of counterfactuals by solving a tractable constrained optimization problem in the structured latent space of a causal model. Our formulation also facilitates a comparison with methods in the field of counterfactual explanations. Compared to these, our method represents a versatile, modular and causally compliant alternative. 
    
[^3]: å…³äºŽæœ‰ç•Œå‡å€¼çš„æŠ•æ³¨ç½®ä¿¡åŒºé—´çš„è¿‘ä¼˜æ€§

    On the near-optimality of betting confidence sets for bounded means. (arXiv:2310.01547v1 [math.ST])

    [http://arxiv.org/abs/2310.01547](http://arxiv.org/abs/2310.01547)

    æœ¬æ–‡ä¸ºæŠ•æ³¨ç½®ä¿¡åŒºé—´çš„æ”¹è¿›æ€§èƒ½æä¾›äº†ç†è®ºè§£é‡Šï¼Œå¹¶æ¯”è¾ƒäº†ç»å…¸æ–¹æ³•ä¸­çš„å®½åº¦ï¼Œå‘çŽ°æŠ•æ³¨ç½®ä¿¡åŒºé—´å…·æœ‰è¾ƒå°çš„æžé™å®½åº¦ã€‚

    

    åœ¨ç»Ÿè®¡å­¦ä¸­ï¼Œä»Žç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆi.i.d.ï¼‰è§‚æµ‹ä¸­æž„å»ºä¸€å…ƒåˆ†å¸ƒçš„éžæ¸è¿‘ç½®ä¿¡åŒºé—´ï¼ˆCIï¼‰æ˜¯ä¸€é¡¹åŸºæœ¬ä»»åŠ¡ã€‚å¯¹äºŽæœ‰ç•Œè§‚æµ‹å€¼ï¼Œç»å…¸çš„éžå‚æ•°æ–¹æ³•é€šè¿‡åè½¬æ ‡å‡†æµ“åº¦ç•Œé™ï¼ˆå¦‚Hoeffdingæˆ–Bernsteinä¸ç­‰å¼ï¼‰æ¥è¿›è¡Œã€‚æœ€è¿‘ï¼Œä¸€ç§æ›¿ä»£çš„åŸºäºŽæŠ•æ³¨çš„æ–¹æ³•è¢«ç”¨äºŽå®šä¹‰CIå’Œå…¶æ—¶é—´ä¸€è‡´å˜ä½“ï¼Œç§°ä¸ºç½®ä¿¡åºåˆ—ï¼ˆCSï¼‰ï¼Œå·²è¢«è¯æ˜Žåœ¨å®žè¯ä¸Šä¼˜äºŽç»å…¸æ–¹æ³•ã€‚æœ¬æ–‡ä¸ºè¿™ç§æŠ•æ³¨CIå’ŒCSçš„æ”¹è¿›ç»éªŒæ€§æ€§èƒ½æä¾›äº†ç†è®ºä¸Šçš„è§£é‡Šã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®å¦‚ä¸‹ï¼šï¼ˆiï¼‰æˆ‘ä»¬é¦–å…ˆæ¯”è¾ƒCIï¼Œä½¿ç”¨å®ƒä»¬çš„ä¸€é˜¶æ¸è¿‘å®½åº¦çš„å€¼ï¼ˆç»è¿‡$\sqrt{n}$ç¼©æ”¾ï¼‰ï¼Œå¹¶ä¸”è¡¨æ˜ŽWaudby-Smithå’ŒRamdasï¼ˆ2023ï¼‰çš„æŠ•æ³¨CIæ¯”çŽ°æœ‰çš„ç»éªŒBernsteinï¼ˆEBï¼‰CIçš„æžé™å®½åº¦æ›´å°ã€‚ï¼ˆiiï¼‰æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸¤ä¸ªä¸‹ç•Œã€‚

    Constructing nonasymptotic confidence intervals (CIs) for the mean of a univariate distribution from independent and identically distributed (i.i.d.) observations is a fundamental task in statistics. For bounded observations, a classical nonparametric approach proceeds by inverting standard concentration bounds, such as Hoeffding's or Bernstein's inequalities. Recently, an alternative betting-based approach for defining CIs and their time-uniform variants called confidence sequences (CSs), has been shown to be empirically superior to the classical methods. In this paper, we provide theoretical justification for this improved empirical performance of betting CIs and CSs.  Our main contributions are as follows: (i) We first compare CIs using the values of their first-order asymptotic widths (scaled by $\sqrt{n}$), and show that the betting CI of Waudby-Smith and Ramdas (2023) has a smaller limiting width than existing empirical Bernstein (EB)-CIs. (ii) Next, we establish two lower bounds
    
[^4]: å¢žå¼ºéšæœºå¹³æ»‘çš„Lipschitz-æ–¹å·®-è¾¹ç•Œæƒè¡¡

    The Lipschitz-Variance-Margin Tradeoff for Enhanced Randomized Smoothing. (arXiv:2309.16883v1 [cs.LG])

    [http://arxiv.org/abs/2309.16883](http://arxiv.org/abs/2309.16883)

    æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªå¢žå¼ºéšæœºå¹³æ»‘çš„æ–¹æ³•ï¼Œé€šè¿‡ç ”ç©¶éšæœºå¹³æ»‘å¼•å…¥çš„æ–¹å·®ä¸Žåˆ†ç±»å™¨çš„Lipschitzå¸¸æ•°å’Œè¾¹ç•Œä¹‹é—´çš„å…³ç³»ï¼Œä»¥åŠé‡‡ç”¨å•çº¯å½¢æŠ•å½±æŠ€æœ¯æ¥å¢žåŠ è®¤è¯é²æ£’åŠå¾„ã€‚

    

    é¢å¯¹å™ªå£°è¾“å…¥å’Œå¯¹æŠ—æ€§æ”»å‡»æ—¶ï¼Œæ·±åº¦ç¥žç»ç½‘ç»œçš„å®žé™…åº”ç”¨å—åˆ°å…¶ä¸ç¨³å®šçš„é¢„æµ‹çš„é˜»ç¢ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®¤è¯åŠå¾„æ˜¯æ¨¡åž‹é²æ£’æ€§çš„å…³é”®æŒ‡æ ‡ã€‚ç„¶è€Œï¼Œå¦‚ä½•è®¾è®¡ä¸€ä¸ªå…·æœ‰è¶³å¤Ÿè®¤è¯åŠå¾„çš„é«˜æ•ˆåˆ†ç±»å™¨å‘¢ï¼Ÿéšæœºå¹³æ»‘é€šè¿‡åœ¨è¾“å…¥ä¸­æ³¨å…¥å™ªå£°æ¥èŽ·å¾—å¹³æ»‘ä¸”æ›´é²æ£’çš„åˆ†ç±»å™¨çš„æ¡†æž¶æä¾›äº†æœ‰å¸Œæœ›çš„è§£å†³æ–¹æ¡ˆã€‚æœ¬æ–‡é¦–å…ˆå±•ç¤ºäº†éšæœºå¹³æ»‘å¼•å…¥çš„æ–¹å·®ä¸Žåˆ†ç±»å™¨çš„å¦å¤–ä¸¤ä¸ªé‡è¦å±žæ€§ï¼Œå³å…¶Lipschitzå¸¸æ•°å’Œè¾¹ç•Œä¹‹é—´çš„å¯†åˆ‡å…³ç³»ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œæˆ‘ä»¬çš„å·¥ä½œå¼ºè°ƒäº†åŸºåˆ†ç±»å™¨çš„Lipschitzå¸¸æ•°å¯¹å¹³æ»‘åˆ†ç±»å™¨å’Œç»éªŒæ–¹å·®çš„åŒé‡å½±å“ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¢žåŠ è®¤è¯é²æ£’åŠå¾„ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§ä¸åŒçš„å•çº¯å½¢æŠ•å½±æŠ€æœ¯ï¼Œä»¥ä¾¿é€šè¿‡Bernstçš„æ–¹å·®-è¾¹ç•Œæƒè¡¡æ¥åˆ©ç”¨åŸºåˆ†ç±»å™¨ã€‚

    Real-life applications of deep neural networks are hindered by their unsteady predictions when faced with noisy inputs and adversarial attacks. The certified radius is in this context a crucial indicator of the robustness of models. However how to design an efficient classifier with a sufficient certified radius? Randomized smoothing provides a promising framework by relying on noise injection in inputs to obtain a smoothed and more robust classifier. In this paper, we first show that the variance introduced by randomized smoothing closely interacts with two other important properties of the classifier, i.e. its Lipschitz constant and margin. More precisely, our work emphasizes the dual impact of the Lipschitz constant of the base classifier, on both the smoothed classifier and the empirical variance. Moreover, to increase the certified robust radius, we introduce a different simplex projection technique for the base classifier to leverage the variance-margin trade-off thanks to Bernst
    
[^5]: è§£è¯»ä¿¡ä»»:å¼ºåŒ–å­¦ä¹ è§†è§’

    Decoding trust: A reinforcement learning perspective. (arXiv:2309.14598v1 [q-bio.PE])

    [http://arxiv.org/abs/2309.14598](http://arxiv.org/abs/2309.14598)

    è¿™é¡¹ç ”ç©¶é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ­ç¤ºäº†åœ¨æˆå¯¹åœºæ™¯ä¸­é«˜æ°´å¹³çš„ä¿¡ä»»å’Œå¯ä¿¡åº¦æ˜¯é€šè¿‡åŒæ—¶é‡è§†åŽ†å²ç»éªŒå’Œæœªæ¥å›žæŠ¥æ¥å½¢æˆçš„ã€‚

    

    å¯¹ä¿¡ä»»æ¸¸æˆçš„è¡Œä¸ºå®žéªŒè¡¨æ˜Žï¼Œä¿¡ä»»å’Œå¯ä¿¡åº¦åœ¨äººç±»ä¸­æ™®éå­˜åœ¨ï¼Œè¿™ä¸Žæ­£ç»Ÿç»æµŽå­¦ä¸­å‡è®¾çš„ç»æµŽäººçš„é¢„æµ‹ç›¸çŸ›ç›¾ã€‚è¿™æ„å‘³ç€ä¸€å®šå­˜åœ¨æŸç§æœºåˆ¶ä¿ƒä½¿ä»–ä»¬çš„å‡ºçŽ°ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„è§£é‡Šéƒ½éœ€è¦ä¾èµ–äºŽä¸€äº›åŸºäºŽæ¨¡ä»¿å­¦ä¹ çš„å› ç´ ï¼Œå³ç®€å•ç‰ˆæœ¬çš„ç¤¾ä¼šå­¦ä¹ ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è½¬å‘å¼ºåŒ–å­¦ä¹ çš„èŒƒå¼ï¼Œä¸ªä½“é€šè¿‡ç´¯ç§¯ç»éªŒè¯„ä¼°é•¿æœŸå›žæŠ¥æ¥æ›´æ–°ä»–ä»¬çš„ç­–ç•¥ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ä½¿ç”¨Q-learningç®—æ³•ç ”ç©¶ä¿¡ä»»æ¸¸æˆï¼Œæ¯ä¸ªå‚ä¸Žè€…åˆ†åˆ«ä¸Žä¸¤ä¸ªä¸æ–­æ¼”åŒ–çš„Qè¡¨å…³è”ï¼ŒæŒ‡å¯¼ä»–ä»¬ä½œä¸ºä¿¡ä»»è€…å’Œæ‰˜ç®¡æ–¹çš„å†³ç­–ã€‚åœ¨æˆå¯¹çš„åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å‘çŽ°å½“ä¸ªä½“åŒæ—¶é‡è§†åŽ†å²ç»éªŒå’Œæœªæ¥å›žæŠ¥æ—¶ï¼Œä¿¡ä»»å’Œå¯ä¿¡åº¦æ°´å¹³è¾ƒé«˜ã€‚ä»Žæœºåˆ¶ä¸Šçœ‹ï¼ŒQçš„æ¼”åŒ–...

    Behavioral experiments on the trust game have shown that trust and trustworthiness are universal among human beings, contradicting the prediction by assuming \emph{Homo economicus} in orthodox Economics. This means some mechanism must be at work that favors their emergence. Most previous explanations however need to resort to some factors based upon imitative learning, a simple version of social learning. Here, we turn to the paradigm of reinforcement learning, where individuals update their strategies by evaluating the long-term return through accumulated experience. Specifically, we investigate the trust game with the Q-learning algorithm, where each participant is associated with two evolving Q-tables that guide one's decision making as trustor and trustee respectively. In the pairwise scenario, we reveal that high levels of trust and trustworthiness emerge when individuals appreciate both their historical experience and returns in the future. Mechanistically, the evolution of the Q
    
[^6]: æ— æ³•æ‰¾åˆ°å‡ºè‰²çš„æ³›åŒ–åº¦é‡æ–¹æ³•

    Fantastic Generalization Measures are Nowhere to be Found. (arXiv:2309.13658v1 [cs.LG])

    [http://arxiv.org/abs/2309.13658](http://arxiv.org/abs/2309.13658)

    æœ¬è®ºæ–‡ç ”ç©¶äº†è¿‡å‚æ•°åŒ–æƒ…å†µä¸‹çš„æ³›åŒ–ç•Œé™é—®é¢˜ï¼Œé€šè¿‡åˆ†æžå¤šä¸ªç•Œé™å‘çŽ°åœ¨è¿™ç§æƒ…å†µä¸‹æ— æ³•æ‰¾åˆ°ç´§è‡´çš„ç•Œé™æ¥è§£é‡Šç¥žç»ç½‘ç»œçš„å‡ºè‰²æ€§èƒ½ã€‚

    

    è¿‡åŽ»çš„æ–‡çŒ®ä¸­æå‡ºäº†è®¸å¤šæ³›åŒ–ç•Œé™ä½œä¸ºè§£é‡Šç¥žç»ç½‘ç»œåœ¨è¿‡å‚æ•°åŒ–æƒ…å†µä¸‹æ³›åŒ–èƒ½åŠ›çš„æ½œåœ¨æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›ç•Œé™éƒ½ä¸æ˜¯ç´§è‡´çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ä»–ä»¬çš„è®ºæ–‡â€œFantastic Generalization Measures and Where to Find Themâ€ä¸­ï¼ŒJiangç­‰äººï¼ˆ2020ï¼‰æ£€æŸ¥äº†åå‡ ä¸ªæ³›åŒ–ç•Œé™ï¼Œå¹¶é€šè¿‡å®žéªŒè¯æ˜Žæ²¡æœ‰ä¸€ä¸ªèƒ½å¤Ÿè§£é‡Šç¥žç»ç½‘ç»œå“è¶Šçš„æ€§èƒ½ã€‚è¿™å¼•å‡ºäº†ä¸€ä¸ªé—®é¢˜ï¼Œå³æ˜¯å¦æœ‰å¯èƒ½æ‰¾åˆ°ç´§è‡´çš„æ³›åŒ–ç•Œé™ã€‚æˆ‘ä»¬è€ƒè™‘äº†æ–‡çŒ®ä¸­å¸¸è§çš„ä¸¤ç§æ³›åŒ–ç•Œé™ï¼šï¼ˆ1ï¼‰ä¾èµ–äºŽè®­ç»ƒé›†å’Œå­¦ä¹ ç®—æ³•è¾“å‡ºçš„ç•Œé™ã€‚æ–‡çŒ®ä¸­æœ‰å¤šä¸ªè¿™ç§ç±»åž‹çš„ç•Œé™ï¼ˆä¾‹å¦‚åŸºäºŽèŒƒæ•°å’ŒåŸºäºŽé—´éš”çš„ç•Œé™ï¼‰ï¼Œä½†æˆ‘ä»¬è¯æ˜Žåœ¨è¿‡å‚æ•°åŒ–çš„æƒ…å†µä¸‹ï¼Œæ²¡æœ‰è¿™æ ·çš„ç•Œé™èƒ½å¤Ÿä¸€è‡´åœ°ç´§è‡´ï¼›ï¼ˆ2ï¼‰ä¾èµ–äºŽè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ç•Œé™ã€‚

    Numerous generalization bounds have been proposed in the literature as potential explanations for the ability of neural networks to generalize in the overparameterized setting. However, none of these bounds are tight. For instance, in their paper ``Fantastic Generalization Measures and Where to Find Them'', Jiang et al. (2020) examine more than a dozen generalization bounds, and show empirically that none of them imply guarantees that can explain the remarkable performance of neural networks. This raises the question of whether tight generalization bounds are at all possible. We consider two types of generalization bounds common in the literature: (1) bounds that depend on the training set and the output of the learning algorithm. There are multiple bounds of this type in the literature (e.g., norm-based and margin-based bounds), but we prove mathematically that no such bound can be uniformly tight in the overparameterized setting; (2) bounds that depend on the training set and on the 
    
[^7]: å°†é¡ºåºå˜åŒ–æ£€æµ‹ç®€åŒ–ä¸ºé¡ºåºä¼°è®¡

    Reducing sequential change detection to sequential estimation. (arXiv:2309.09111v1 [math.ST])

    [http://arxiv.org/abs/2309.09111](http://arxiv.org/abs/2309.09111)

    è¿™ä¸ªè®ºæ–‡å°†é¡ºåºå˜åŒ–æ£€æµ‹ç®€åŒ–ä¸ºé¡ºåºä¼°è®¡ï¼Œé€šè¿‡ä½¿ç”¨ç½®ä¿¡åºåˆ—æ¥æ£€æµ‹æ•°æ®æµä¸­çš„å˜åŒ–ï¼Œå¹¶è¯æ˜Žäº†è¯¥æ–¹æ³•å…·æœ‰å¼ºå¤§çš„ä¿è¯ã€‚

    

    æœ¬æ–‡è€ƒè™‘äº†é¡ºåºå˜åŒ–æ£€æµ‹çš„é—®é¢˜ï¼Œç›®æ ‡æ˜¯è®¾è®¡ä¸€ä¸ªèƒ½å¤Ÿæ£€æµ‹æ•°æ®æµåˆ†å¸ƒä¸­å‚æ•°æˆ–å‡½æ•°ðœƒçš„ä»»ä½•å˜åŒ–çš„æ–¹æ¡ˆï¼Œè¯¥æ–¹æ¡ˆå…·æœ‰è¾ƒå°çš„æ£€æµ‹å»¶è¿Ÿï¼Œä½†åœ¨æ²¡æœ‰å˜åŒ–çš„æƒ…å†µä¸‹èƒ½å¤Ÿä¿è¯å‡è­¦æŠ¥çš„é¢‘çŽ‡å—æŽ§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç½®ä¿¡åºåˆ—æè¿°äº†ä¸€ç§ä»Žé¡ºåºå˜åŒ–æ£€æµ‹åˆ°é¡ºåºä¼°è®¡çš„ç®€å•çº¦åŒ–æ–¹æ³•ï¼šæˆ‘ä»¬åœ¨æ¯ä¸ªæ—¶é—´æ­¥å¼€å§‹ä¸€ä¸ªæ–°çš„$(1-\alpha)$ç½®ä¿¡åºåˆ—ï¼Œå¹¶åœ¨æ‰€æœ‰æ´»åŠ¨ç½®ä¿¡åºåˆ—çš„äº¤é›†ä¸ºç©ºæ—¶å®£å¸ƒå˜åŒ–ã€‚æˆ‘ä»¬è¯æ˜Žäº†å¹³å‡æŒç»­æ—¶é—´è‡³å°‘ä¸º$1/\alpha$ï¼Œä»Žè€Œå¾—åˆ°äº†å…·æœ‰æœ€å°ç»“æž„å‡è®¾çš„å˜åŒ–æ£€æµ‹æ–¹æ¡ˆï¼ˆå› æ­¤å…è®¸å¯èƒ½ç›¸å…³çš„è§‚æµ‹å’Œéžå‚æ•°åˆ†å¸ƒç±»ï¼‰ï¼Œä½†å´å…·æœ‰å¼ºå¤§çš„ä¿è¯ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¸ŽLordenï¼ˆ1971ï¼‰çš„å˜åŒ–æ£€æµ‹åˆ°é¡ºåºæµ‹è¯•çš„ç®€åŒ–å’ŒShinç­‰äººçš„e-detectoræœ‰ç€æœ‰è¶£çš„ç›¸ä¼¼ä¹‹å¤„ã€‚

    We consider the problem of sequential change detection, where the goal is to design a scheme for detecting any changes in a parameter or functional $\theta$ of the data stream distribution that has small detection delay, but guarantees control on the frequency of false alarms in the absence of changes. In this paper, we describe a simple reduction from sequential change detection to sequential estimation using confidence sequences: we begin a new $(1-\alpha)$-confidence sequence at each time step, and proclaim a change when the intersection of all active confidence sequences becomes empty. We prove that the average run length is at least $1/\alpha$, resulting in a change detection scheme with minimal structural assumptions~(thus allowing for possibly dependent observations, and nonparametric distribution classes), but strong guarantees. Our approach bears an interesting parallel with the reduction from change detection to sequential testing of Lorden (1971) and the e-detector of Shin e
    
[^8]: æœ€ä¼˜å’Œå…¬å¹³çš„é¼“åŠ±æ”¿ç­–è¯„ä¼°ä¸Žå­¦ä¹ 

    Optimal and Fair Encouragement Policy Evaluation and Learning. (arXiv:2309.07176v1 [cs.LG])

    [http://arxiv.org/abs/2309.07176](http://arxiv.org/abs/2309.07176)

    æœ¬ç ”ç©¶æŽ¢è®¨äº†åœ¨å…³é”®é¢†åŸŸä¸­é’ˆå¯¹é¼“åŠ±æ”¿ç­–çš„æœ€ä¼˜å’Œå…¬å¹³è¯„ä¼°ä»¥åŠå­¦ä¹ çš„é—®é¢˜ï¼Œç ”ç©¶å‘çŽ°åœ¨äººç±»ä¸éµå¾ªæ²»ç–—å»ºè®®çš„æƒ…å†µä¸‹ï¼Œæœ€ä¼˜ç­–ç•¥è§„åˆ™åªæ˜¯å»ºè®®ã€‚åŒæ—¶ï¼Œé’ˆå¯¹æ²»ç–—çš„å¼‚è´¨æ€§å’Œå…¬å¹³è€ƒè™‘å› ç´ ï¼Œå†³ç­–è€…çš„æƒè¡¡å’Œå†³ç­–è§„åˆ™ä¹Ÿä¼šå‘ç”Ÿå˜åŒ–ã€‚åœ¨ç¤¾ä¼šæœåŠ¡é¢†åŸŸï¼Œç ”ç©¶æ˜¾ç¤ºå­˜åœ¨ä¸€ä¸ªä½¿ç”¨å·®è·é—®é¢˜ï¼Œé‚£äº›æœ€æœ‰å¯èƒ½å—ç›Šçš„äººå´æ— æ³•èŽ·å¾—è¿™äº›ç›ŠæœåŠ¡ã€‚

    

    åœ¨å…³é”®é¢†åŸŸä¸­ï¼Œå¼ºåˆ¶ä¸ªä½“æŽ¥å—æ²»ç–—é€šå¸¸æ˜¯ä¸å¯èƒ½çš„ï¼Œå› æ­¤åœ¨äººç±»ä¸éµå¾ªæ²»ç–—å»ºè®®çš„æƒ…å†µä¸‹ï¼Œæœ€ä¼˜ç­–ç•¥è§„åˆ™åªæ˜¯å»ºè®®ã€‚åœ¨è¿™äº›é¢†åŸŸä¸­ï¼ŒæŽ¥å—æ²»ç–—çš„ä¸ªä½“å¯èƒ½å­˜åœ¨å¼‚è´¨æ€§ï¼Œæ²»ç–—æ•ˆæžœä¹Ÿå¯èƒ½å­˜åœ¨å¼‚è´¨æ€§ã€‚è™½ç„¶æœ€ä¼˜æ²»ç–—è§„åˆ™å¯ä»¥æœ€å¤§åŒ–æ•´ä¸ªäººç¾¤çš„å› æžœç»“æžœï¼Œä½†åœ¨é¼“åŠ±çš„æƒ…å†µä¸‹ï¼Œå¯¹äºŽè®¿é—®å¹³ç­‰é™åˆ¶æˆ–å…¶ä»–å…¬å¹³è€ƒè™‘å› ç´ å¯èƒ½æ˜¯ç›¸å…³çš„ã€‚ä¾‹å¦‚ï¼Œåœ¨ç¤¾ä¼šæœåŠ¡é¢†åŸŸï¼Œä¸€ä¸ªæŒä¹…çš„éš¾é¢˜æ˜¯é‚£äº›æœ€æœ‰å¯èƒ½ä»Žä¸­å—ç›Šçš„äººä¸­é‚£äº›èŽ·ç›ŠæœåŠ¡çš„ä½¿ç”¨å·®è·ã€‚å½“å†³ç­–è€…å¯¹è®¿é—®å’Œå¹³å‡ç»“æžœéƒ½æœ‰åˆ†é…åå¥½æ—¶ï¼Œæœ€ä¼˜å†³ç­–è§„åˆ™ä¼šå‘ç”Ÿå˜åŒ–ã€‚æˆ‘ä»¬ç ”ç©¶äº†å› æžœè¯†åˆ«ã€ç»Ÿè®¡æ–¹å·®å‡å°‘ä¼°è®¡å’Œç¨³å¥ä¼°è®¡çš„æœ€ä¼˜æ²»ç–—è§„åˆ™ï¼ŒåŒ…æ‹¬åœ¨è¿åé˜³æ€§æ¡ä»¶çš„æƒ…å†µä¸‹ã€‚

    In consequential domains, it is often impossible to compel individuals to take treatment, so that optimal policy rules are merely suggestions in the presence of human non-adherence to treatment recommendations. In these same domains, there may be heterogeneity both in who responds in taking-up treatment, and heterogeneity in treatment efficacy. While optimal treatment rules can maximize causal outcomes across the population, access parity constraints or other fairness considerations can be relevant in the case of encouragement. For example, in social services, a persistent puzzle is the gap in take-up of beneficial services among those who may benefit from them the most. When in addition the decision-maker has distributional preferences over both access and average outcomes, the optimal decision rule changes. We study causal identification, statistical variance-reduced estimation, and robust estimation of optimal treatment rules, including under potential violations of positivity. We c
    
[^9]: é˜ˆå€¼KNN-Shapleyï¼šä¸€ç§çº¿æ€§æ—¶é—´å’Œéšç§å‹å¥½çš„æ•°æ®ä»·å€¼è¯„ä¼°æ–¹æ³•

    Threshold KNN-Shapley: A Linear-Time and Privacy-Friendly Approach to Data Valuation. (arXiv:2308.15709v1 [cs.LG])

    [http://arxiv.org/abs/2308.15709](http://arxiv.org/abs/2308.15709)

    æœ¬è®ºæ–‡ç ”ç©¶äº†æ•°æ®ä»·å€¼è¯„ä¼°é¢ä¸´çš„éšç§æŒ‘æˆ˜ï¼Œå¹¶æå‡ºäº†ä¸€ç§éšç§å‹å¥½çš„æ”¹è¿›æ–¹æ³•TKNN-Shapleyï¼Œè¯¥æ–¹æ³•åœ¨ä¿æŠ¤éšç§çš„å‰æä¸‹èƒ½å¤Ÿè¯„ä¼°æ•°æ®è´¨é‡ï¼Œå…·æœ‰è¾ƒå¥½çš„éšç§-å®žç”¨æ€§æƒè¡¡ã€‚

    

    æ•°æ®ä»·å€¼è¯„ä¼°æ˜¯æ•°æ®ä¸­å¿ƒåŒ–æœºå™¨å­¦ä¹ ç ”ç©¶ä¸­çš„å…³é”®é—®é¢˜ï¼Œæ—¨åœ¨é‡åŒ–å•ä¸ªæ•°æ®æºåœ¨è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡åž‹ä¸­çš„æœ‰ç”¨æ€§ã€‚ç„¶è€Œï¼Œå°½ç®¡å…¶é‡è¦æ€§ï¼Œæ•°æ®ä»·å€¼è¯„ä¼°é¢ä¸´ç€å¾ˆå¤šé‡è¦ä½†ç»å¸¸è¢«å¿½è§†çš„éšç§æŒ‘æˆ˜ã€‚æœ¬æ–‡é’ˆå¯¹ç›®å‰æœ€å®žç”¨çš„æ•°æ®ä»·å€¼è¯„ä¼°æ–¹æ³•ä¹‹ä¸€KNN-Shapleyï¼Œç ”ç©¶äº†è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬é¦–å…ˆå¼ºè°ƒäº†KNN-Shapleyå›ºæœ‰çš„éšç§é£Žé™©ï¼Œå¹¶å±•ç¤ºäº†å°†KNN-Shapleyæ”¹è¿›ä»¥æ»¡è¶³å·®åˆ†éšç§(DP)çš„æ˜¾è‘—æŠ€æœ¯å›°éš¾ã€‚ä¸ºäº†å…‹æœè¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬å¼•å…¥äº†TKNN-Shapleyï¼ŒKNN-Shapleyçš„ä¸€ç§æ”¹è¿›å˜ä½“ï¼Œå…·æœ‰éšç§å‹å¥½æ€§ï¼Œå¯ä»¥è¿›è¡Œç®€å•çš„ä¿®æ­£ä»¥åŒ…å«DPä¿è¯ï¼ˆDP-TKNN-Shapleyï¼‰ã€‚æˆ‘ä»¬è¯æ˜Žï¼ŒDP-TKNN-Shapleyåœ¨è¾¨åˆ«æ•°æ®è´¨é‡æ–¹é¢å…·æœ‰ä¸€äº›ä¼˜åŠ¿ï¼Œå¹¶åœ¨éšç§-å®žç”¨æ€§æƒè¡¡æ–¹é¢ä¼˜äºŽæœ´ç´ åŒ–çš„KNN-Shapleyã€‚æ­¤å¤–ï¼Œå³ä½¿æ˜¯éžéšç§çš„TKNN-Shapleyä¹Ÿèƒ½ä»¥çº¿æ€§æ—¶é—´è¿è¡Œã€‚

    Data valuation, a critical aspect of data-centric ML research, aims to quantify the usefulness of individual data sources in training machine learning (ML) models. However, data valuation faces significant yet frequently overlooked privacy challenges despite its importance. This paper studies these challenges with a focus on KNN-Shapley, one of the most practical data valuation methods nowadays. We first emphasize the inherent privacy risks of KNN-Shapley, and demonstrate the significant technical difficulties in adapting KNN-Shapley to accommodate differential privacy (DP). To overcome these challenges, we introduce TKNN-Shapley, a refined variant of KNN-Shapley that is privacy-friendly, allowing for straightforward modifications to incorporate DP guarantee (DP-TKNN-Shapley). We show that DP-TKNN-Shapley has several advantages and offers a superior privacy-utility tradeoff compared to naively privatized KNN-Shapley in discerning data quality. Moreover, even non-private TKNN-Shapley ac
    
[^10]: æ›´å…·è¡¨çŽ°åŠ›çš„å›¾ç¥žç»ç½‘ç»œåœ¨ç”Ÿæˆä»»åŠ¡ä¸­æ˜¯å¦æ›´å¥½ï¼Ÿ

    Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v1 [cs.LG])

    [http://arxiv.org/abs/2308.11978](http://arxiv.org/abs/2308.11978)

    æœ¬è®ºæ–‡è°ƒæŸ¥äº†æ›´å…·è¡¨çŽ°åŠ›çš„å›¾ç¥žç»ç½‘ç»œåœ¨åˆ†å­å›¾ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨çŽ°èƒ½åŠ›ï¼Œå¹¶é€šè¿‡æ›¿æ¢å›¾ç”Ÿæˆæ¨¡åž‹çš„åŸºç¡€GNNæ¥è¿›è¡Œå®žéªŒã€‚ç ”ç©¶å‘çŽ°ï¼Œä½¿ç”¨æ›´å…·è¡¨çŽ°åŠ›çš„GNNå¯ä»¥æ”¹å–„ç”Ÿæˆä»»åŠ¡çš„æ€§èƒ½ã€‚

    

    å›¾ç”Ÿæˆæ˜¯ä¸€ä¸ªé‡è¦çš„æŒ‘æˆ˜ï¼Œå®ƒæ¶‰åŠæ ¹æ®ç»™å®šçš„æ ‡ç­¾é¢„æµ‹ä¸€ä¸ªå®Œæ•´çš„å…·æœ‰å¤šä¸ªèŠ‚ç‚¹å’Œè¾¹çš„å›¾ã€‚è¿™ä¸ªä»»åŠ¡å¯¹è®¸å¤šå®žé™…åº”ç”¨éžå¸¸é‡è¦ï¼ŒåŒ…æ‹¬è¯ç‰©å’Œåˆ†å­è®¾è®¡ã€‚è¿‘å¹´æ¥ï¼Œåœ¨å›¾ç”Ÿæˆé¢†åŸŸå‡ºçŽ°äº†å‡ ç§æˆåŠŸçš„æ–¹æ³•ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•å­˜åœ¨ä¸¤ä¸ªé‡å¤§é—®é¢˜ï¼š(1) è¿™äº›æ–¹æ³•ä¸­ä½¿ç”¨çš„åŸºç¡€å›¾ç¥žç»ç½‘ç»œï¼ˆGNNï¼‰æž¶æž„å¾€å¾€æœªç»æ·±å…¥æŽ¢ç´¢ï¼›(2) è¿™äº›æ–¹æ³•å¾€å¾€åªåœ¨æœ‰é™çš„æŒ‡æ ‡ä¸Šè¿›è¡Œè¯„ä¼°ã€‚ä¸ºå¡«è¡¥è¿™ä¸ªç©ºç™½ï¼Œæˆ‘ä»¬é€šè¿‡å°†å›¾ç”Ÿæˆæ¨¡åž‹çš„åŸºç¡€GNNæ›¿æ¢ä¸ºæ›´å…·è¡¨çŽ°åŠ›çš„GNNï¼Œç ”ç©¶äº†GNNåœ¨åˆ†å­å›¾ç”Ÿæˆä»»åŠ¡ä¸­çš„è¡¨çŽ°èƒ½åŠ›ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åˆ†æžäº†ä¸¤ç§ä¸åŒç”Ÿæˆæ¡†æž¶ï¼ˆGCPNå’ŒGraphAFï¼‰ä¸­å…­ç§GNNåœ¨å…­ä¸ªä¸åŒçš„åˆ†å­ç”Ÿæˆç›®æ ‡ä¸Šçš„æ€§èƒ½ã€‚

    Graph generation poses a significant challenge as it involves predicting a complete graph with multiple nodes and edges based on simply a given label. This task also carries fundamental importance to numerous real-world applications, including de-novo drug and molecular design. In recent years, several successful methods have emerged in the field of graph generation. However, these approaches suffer from two significant shortcomings: (1) the underlying Graph Neural Network (GNN) architectures used in these methods are often underexplored; and (2) these methods are often evaluated on only a limited number of metrics. To fill this gap, we investigate the expressiveness of GNNs under the context of the molecular graph generation task, by replacing the underlying GNNs of graph generative models with more expressive GNNs. Specifically, we analyse the performance of six GNNs in two different generative frameworks (GCPN and GraphAF), on six different molecular generative objectives on the ZIN
    
[^11]: èƒ½é‡å·®å¼‚ï¼šä¸€ç§é€‚ç”¨äºŽèƒ½é‡æ¨¡åž‹çš„ç‹¬ç«‹äºŽè¯„åˆ†çš„æŸå¤±å‡½æ•°

    Energy Discrepancies: A Score-Independent Loss for Energy-Based Models. (arXiv:2307.06431v1 [stat.ML])

    [http://arxiv.org/abs/2307.06431](http://arxiv.org/abs/2307.06431)

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„èƒ½é‡æ¨¡åž‹æŸå¤±å‡½æ•°ï¼Œèƒ½å¤Ÿåœ¨ä¸ä¾èµ–åˆ†æ•°è®¡ç®—æˆ–æ˜‚è´µçš„è’™ç‰¹å¡ç½—æ–¹æ³•çš„æƒ…å†µä¸‹ï¼Œè¿‘ä¼¼å®žçŽ°æ˜¾å¼åˆ†æ•°åŒ¹é…å’Œè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œå¹¶åœ¨å­¦ä¹ ä½Žç»´æ•°æ®åˆ†å¸ƒæ—¶å…·æœ‰æ›´å¥½çš„æ€§èƒ½ã€‚

    

    èƒ½é‡æ¨¡åž‹æ˜¯ä¸€ç§ç®€å•è€Œå¼ºå¤§çš„æ¦‚çŽ‡æ¨¡åž‹ï¼Œä½†å®ƒä»¬çš„æ™®åŠå—åˆ°äº†è®­ç»ƒçš„è®¡ç®—è´Ÿæ‹…çš„é™åˆ¶ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æŸå¤±å‡½æ•°ç§°ä¸ºèƒ½é‡å·®å¼‚ï¼ˆEDï¼‰ï¼Œå®ƒä¸ä¾èµ–äºŽåˆ†æ•°çš„è®¡ç®—æˆ–æ˜‚è´µçš„é©¬å°”å¯å¤«é“¾è’™ç‰¹å¡ç½—ã€‚æˆ‘ä»¬è¯æ˜Žäº†åœ¨ä¸åŒçš„æžé™ä¸‹ï¼ŒEDæŽ¥è¿‘äºŽæ˜¾å¼åˆ†æ•°åŒ¹é…å’Œè´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±ï¼Œæœ‰æ•ˆåœ°åœ¨ä¸¤è€…ä¹‹é—´æ’å€¼ã€‚å› æ­¤ï¼Œæœ€å°åŒ–EDä¼°è®¡å…‹æœäº†åœ¨åŸºäºŽåˆ†æ•°çš„ä¼°è®¡æ–¹æ³•ä¸­é‡åˆ°çš„è¿‘è§†é—®é¢˜ï¼ŒåŒæ—¶è¿˜äº«æœ‰ç†è®ºä¿è¯ã€‚é€šè¿‡æ•°å€¼å®žéªŒè¯æ˜Žï¼Œä¸Žæ˜¾å¼åˆ†æ•°åŒ¹é…æˆ–å¯¹æ¯”æ•£åº¦ç›¸æ¯”ï¼ŒEDèƒ½å¤Ÿæ›´å¿«é€Ÿã€æ›´å‡†ç¡®åœ°å­¦ä¹ ä½Žç»´æ•°æ®åˆ†å¸ƒã€‚å¯¹äºŽé«˜ç»´å›¾åƒæ•°æ®ï¼Œæˆ‘ä»¬æè¿°äº†æµå½¢å‡è®¾å¯¹æˆ‘ä»¬æ–¹æ³•çš„é™åˆ¶ï¼Œå¹¶é€šè¿‡å¯¹eæ¨¡åž‹çš„è®­ç»ƒï¼Œè¯æ˜Žäº†èƒ½é‡å·®å¼‚çš„æœ‰æ•ˆæ€§ã€‚

    Energy-based models are a simple yet powerful class of probabilistic models, but their widespread adoption has been limited by the computational burden of training them. We propose a novel loss function called Energy Discrepancy (ED) which does not rely on the computation of scores or expensive Markov chain Monte Carlo. We show that ED approaches the explicit score matching and negative log-likelihood loss under different limits, effectively interpolating between both. Consequently, minimum ED estimation overcomes the problem of nearsightedness encountered in score-based estimation methods, while also enjoying theoretical guarantees. Through numerical experiments, we demonstrate that ED learns low-dimensional data distributions faster and more accurately than explicit score matching or contrastive divergence. For high-dimensional image data, we describe how the manifold hypothesis puts limitations on our approach and demonstrate the effectiveness of energy discrepancy by training the e
    
[^12]: ç”¨äºŽä¸€èˆ¬å‚æ•°å¯†åº¦æ¨¡åž‹çš„æœ€å°åŒ–ç¨³å¥å¯†åº¦åŠŸçŽ‡åˆ†æ­§çš„éšæœºä¼˜åŒ–æ–¹æ³•

    A stochastic optimization approach to minimize robust density power-based divergences for general parametric density models. (arXiv:2307.05251v1 [stat.ME])

    [http://arxiv.org/abs/2307.05251](http://arxiv.org/abs/2307.05251)

    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§éšæœºä¼˜åŒ–æ–¹æ³•ï¼Œç”¨äºŽè§£å†³ç¨³å¥å¯†åº¦åŠŸçŽ‡åˆ†æ­§ï¼ˆDPDï¼‰åœ¨ä¸€èˆ¬å‚æ•°å¯†åº¦æ¨¡åž‹ä¸­çš„è®¡ç®—å¤æ‚æ€§é—®é¢˜ï¼Œå¹¶é€šè¿‡åº”ç”¨ä¼ ç»Ÿçš„éšæœºä¼˜åŒ–ç†è®ºæ¥éªŒè¯å…¶æœ‰æ•ˆæ€§ã€‚

    

    å¯†åº¦åŠŸçŽ‡åˆ†æ­§ï¼ˆDPDï¼‰æ˜¯ä¸€ç§ç”¨äºŽç¨³å¥åœ°ä¼°è®¡è§‚æµ‹æ•°æ®æ½œåœ¨åˆ†å¸ƒçš„æ–¹æ³•ï¼Œå®ƒåŒ…æ‹¬ä¸€ä¸ªè¦ä¼°è®¡çš„å‚æ•°å¯†åº¦æ¨¡åž‹çš„å¹‚çš„ç§¯åˆ†é¡¹ã€‚è™½ç„¶å¯¹äºŽä¸€äº›ç‰¹å®šçš„å¯†åº¦ï¼ˆå¦‚æ­£æ€å¯†åº¦å’ŒæŒ‡æ•°å¯†åº¦ï¼‰å¯ä»¥å¾—åˆ°ç§¯åˆ†é¡¹çš„æ˜¾å¼å½¢å¼ï¼Œä½†DPDçš„è®¡ç®—å¤æ‚æ€§ä½¿å¾—å…¶æ— æ³•åº”ç”¨äºŽæ›´ä¸€èˆ¬çš„å‚æ•°å¯†åº¦æ¨¡åž‹ï¼Œè¿™å·²ç»è¶…è¿‡äº†DPDæå‡ºçš„25å¹´ã€‚æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§ç”¨äºŽä¸€èˆ¬å‚æ•°å¯†åº¦æ¨¡åž‹æœ€å°åŒ–DPDçš„éšæœºä¼˜åŒ–æ–¹æ³•ï¼Œå¹¶é€šè¿‡å‚è€ƒéšæœºä¼˜åŒ–çš„ä¼ ç»Ÿç†è®ºè¯´æ˜Žäº†å…¶é€‚ç”¨æ€§ã€‚æ‰€æå‡ºçš„æ–¹æ³•è¿˜å¯ä»¥é€šè¿‡ä½¿ç”¨æœªå½’ä¸€åŒ–æ¨¡åž‹æ¥æœ€å°åŒ–å¦ä¸€ä¸ªåŸºäºŽå¯†åº¦åŠŸçŽ‡çš„Î³-ç¦»å·®[Kanamoriå’ŒFujisawaï¼ˆ2015ï¼‰ï¼ŒBiometrika]ã€‚

    Density power divergence (DPD) [Basu et al. (1998), Biometrika], designed to estimate the underlying distribution of the observations robustly, comprises an integral term of the power of the parametric density models to be estimated. While the explicit form of the integral term can be obtained for some specific densities (such as normal density and exponential density), its computational intractability has prohibited the application of DPD-based estimation to more general parametric densities, over a quarter of a century since the proposal of DPD. This study proposes a stochastic optimization approach to minimize DPD for general parametric density models and explains its adequacy by referring to conventional theories on stochastic optimization. The proposed approach also can be applied to the minimization of another density power-based $\gamma$-divergence with the aid of unnormalized models [Kanamori and Fujisawa (2015), Biometrika].
    
[^13]: å…³äºŽHadamardå‚æ•°åŒ–ä¸‹ç­–ç•¥æ¢¯åº¦çš„çº¿æ€§æ”¶æ•›æ€§.

    On the Linear Convergence of Policy Gradient under Hadamard Parameterization. (arXiv:2305.19575v1 [math.OC])

    [http://arxiv.org/abs/2305.19575](http://arxiv.org/abs/2305.19575)

    æœ¬æ–‡ç ”ç©¶äº†Hadamardå‚æ•°åŒ–ä¸‹ç­–ç•¥æ¢¯åº¦çš„æ”¶æ•›æ€§ï¼Œè¯æ˜Žäº†ç®—æ³•å…·æœ‰å…¨å±€çº¿æ€§æ”¶æ•›æ€§å’Œå±€éƒ¨çº¿æ€§æ”¶æ•›é€Ÿåº¦æ›´å¿«çš„æ€§è´¨ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†åœ¨è¡¨æ ¼å¼è®¾ç½®ä¸‹Hadamardå‚æ•°åŒ–ä¸‹ç¡®å®šæ€§ç­–ç•¥æ¢¯åº¦çš„æ”¶æ•›æ€§ï¼Œå¹¶å»ºç«‹äº†ç®—æ³•çš„å…¨å±€çº¿æ€§æ”¶æ•›æ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆè¯æ˜Žäº†é”™è¯¯åœ¨æ‰€æœ‰è¿­ä»£ä¸­ä»¥$O(\frac{1}{k})$çš„é€ŸçŽ‡ä¸‹é™ã€‚åŸºäºŽè¿™ä¸ªç»“æžœï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜Žäº†è¯¥ç®—æ³•åœ¨$k_0$æ¬¡è¿­ä»£ä¹‹åŽå…·æœ‰æ›´å¿«çš„å±€éƒ¨çº¿æ€§æ”¶æ•›é€Ÿåº¦ï¼Œå…¶ä¸­$k_0$æ˜¯ä»…ä¾èµ–äºŽMDPé—®é¢˜å’Œæ­¥é•¿çš„å¸¸æ•°ã€‚æ€»ä½“è€Œè¨€ï¼Œè¯¥ç®—æ³•æ˜¾ç¤ºäº†ä¸€ä¸ªè¾ƒå¼±å¸¸æ•°çš„çº¿æ€§æ”¶æ•›çŽ‡ï¼Œè€Œä¸ä»…ä»…æ˜¯å±€éƒ¨çº¿æ€§æ”¶æ•›çŽ‡ã€‚

    The convergence of deterministic policy gradient under the Hadamard parametrization is studied in the tabular setting and the global linear convergence of the algorithm is established. To this end, we first show that the error decreases at an $O(\frac{1}{k})$ rate for all the iterations. Based on this result, we further show that the algorithm has a faster local linear convergence rate after $k_0$ iterations, where $k_0$ is a constant that only depends on the MDP problem and the step size. Overall, the algorithm displays a linear convergence rate for all the iterations with a loose constant than that for the local linear convergence rate.
    
[^14]: SAMoSSAï¼šå¸¦éšæœºè‡ªå›žå½’å™ªå£°çš„å¤šå…ƒå¥‡å¼‚è°±åˆ†æž

    SAMoSSA: Multivariate Singular Spectrum Analysis with Stochastic Autoregressive Noise. (arXiv:2305.16491v1 [cs.LG])

    [http://arxiv.org/abs/2305.16491](http://arxiv.org/abs/2305.16491)

    è¯¥è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ—¶é—´åºåˆ—åˆ†æžæ–¹æ³•ï¼Œå³SAMoSSAã€‚è¯¥æ–¹æ³•ç»¼åˆäº†å¤šå…ƒå¥‡å¼‚è°±åˆ†æžå’Œè‡ªå›žå½’åˆ†æžï¼Œåœ¨å­¦ä¹ æ—¶é—´åºåˆ—ä¸­çš„ç¡®å®šæ€§å’Œéšæœºæ€§æˆåˆ†æ–¹é¢å…·æœ‰è‰¯å¥½çš„ç†è®ºä¿è¯ã€‚

    

    æ—¶é—´åºåˆ—åˆ†æžçš„æƒ¯ä¾‹æ˜¯å…ˆä¼°è®¡ç¡®å®šæ€§ã€éžå¹³ç¨³è¶‹åŠ¿å’Œå­£èŠ‚æˆåˆ†ï¼Œç„¶åŽå­¦ä¹ æ®‹å·®éšæœºã€å¹³ç¨³æˆåˆ†ã€‚æœ€è¿‘å·²ç»è¡¨æ˜Žï¼Œåœ¨æ²¡æœ‰ç›¸å…³å¹³ç¨³æˆåˆ†çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥ä½¿ç”¨å¤šå…ƒå¥‡å¼‚è°±åˆ†æžï¼ˆmSSAï¼‰å‡†ç¡®åœ°å­¦ä¹ ç¡®å®šæ€§éžå¹³ç¨³æˆåˆ†ï¼›åŒæ—¶ï¼Œåœ¨æ²¡æœ‰ç¡®å®šæ€§éžå¹³ç¨³æˆåˆ†çš„æƒ…å†µä¸‹ï¼Œè‡ªå›žå½’ï¼ˆARï¼‰å¹³ç¨³æˆåˆ†ä¹Ÿå¯ä»¥è½»æ¾å­¦ä¹ ï¼Œä¾‹å¦‚é€šè¿‡æ™®é€šæœ€å°äºŒä¹˜ï¼ˆOLSï¼‰ã€‚ç„¶è€Œï¼Œå°½ç®¡è¿™ç§ä¸¤ä¸ªæ­¥éª¤çš„å­¦ä¹ ç®—æ³•å·²ç»æ™®éå­˜åœ¨ï¼Œä½†å…³äºŽåŒæ—¶æ¶‰åŠç¡®å®šæ€§å’Œå¹³ç¨³æˆåˆ†çš„å¤šé˜¶æ®µå­¦ä¹ ç®—æ³•çš„ç†è®ºæ”¯æ’‘åœ¨æ–‡çŒ®ä¸­è¿˜æ²¡æœ‰è§£å†³ã€‚æˆ‘ä»¬é€šè¿‡ä¸ºä¸€ç§è‡ªç„¶çš„ä¸¤é˜¶æ®µç®—æ³•å»ºç«‹ç†è®ºä¿è¯æ¥è§£å†³è¿™ä¸ªå¼€æ”¾æ€§é—®é¢˜ï¼Œå…¶ä¸­é¦–å…ˆåº”ç”¨mSSAæ¥ä¼°è®¡éžå¹³ç¨³æˆåˆ†ï¼Œå°½ç®¡å­˜åœ¨ç›¸å…³æ€§å¹³ç¨³æˆåˆ†ã€‚

    The well-established practice of time series analysis involves estimating deterministic, non-stationary trend and seasonality components followed by learning the residual stochastic, stationary components. Recently, it has been shown that one can learn the deterministic non-stationary components accurately using multivariate Singular Spectrum Analysis (mSSA) in the absence of a correlated stationary component; meanwhile, in the absence of deterministic non-stationary components, the Autoregressive (AR) stationary component can also be learnt readily, e.g. via Ordinary Least Squares (OLS). However, a theoretical underpinning of multi-stage learning algorithms involving both deterministic and stationary components has been absent in the literature despite its pervasiveness. We resolve this open question by establishing desirable theoretical guarantees for a natural two-stage algorithm, where mSSA is first applied to estimate the non-stationary components despite the presence of a correla
    
[^15]: DIVAï¼šåŸºäºŽç‹„åˆ©å…‹é›·è¿‡ç¨‹çš„å˜åˆ†è‡ªç¼–ç å™¨çš„å¢žé‡æ·±åº¦èšç±»ç®—æ³•

    DIVA: A Dirichlet Process Based Incremental Deep Clustering Algorithm via Variational Auto-Encoder. (arXiv:2305.14067v1 [cs.LG])

    [http://arxiv.org/abs/2305.14067](http://arxiv.org/abs/2305.14067)

    æœ¬æ–‡æå‡ºäº†DIVAç®—æ³•ï¼Œä¸€ä¸ªåŸºäºŽç‹„åˆ©å…‹é›·è¿‡ç¨‹çš„å¢žé‡æ·±åº¦èšç±»æ¡†æž¶ï¼Œåˆ©ç”¨æ— é™æ··åˆé«˜æ–¯ä½œä¸ºå…ˆéªŒï¼Œå¹¶åˆ©ç”¨ä¸€ç§è®°å¿†åŒ–çš„åœ¨çº¿å˜åˆ†æŽ¨ç†æ–¹æ³•å®žçŽ°ç°‡çš„åŠ¨æ€é€‚åº”ç§»åŠ¨ï¼Œè€Œä¸éœ€è¦å…ˆçŸ¥é“ç‰¹å¾çš„æ•°é‡ã€‚è¯¥ç®—æ³•è¡¨çŽ°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨å¢žé‡ç‰¹å¾çš„æƒ…å†µä¸‹ã€‚

    

    åŸºäºŽç”Ÿæˆæ¨¡åž‹çš„æ·±åº¦èšç±»æ¡†æž¶åœ¨åˆ†ç±»å¤æ‚æ•°æ®æ–¹é¢è¡¨çŽ°å‡ºè‰²ï¼Œä½†åœ¨å¤„ç†åŠ¨æ€å’Œå¤æ‚ç‰¹å¾æ–¹é¢å—åˆ°é™åˆ¶ï¼Œå› ä¸ºå®ƒä»¬éœ€è¦å…ˆçŸ¥é“ç°‡çš„æ•°é‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªéžå‚æ•°æ·±åº¦èšç±»æ¡†æž¶ï¼Œé‡‡ç”¨æ— é™æ··åˆé«˜æ–¯ä½œä¸ºå…ˆéªŒã€‚æˆ‘ä»¬çš„æ¡†æž¶åˆ©ç”¨ä¸€ç§è®°å¿†åŒ–çš„åœ¨çº¿å˜åˆ†æŽ¨ç†æ–¹æ³•ï¼Œå®žçŽ°äº†ç°‡çš„â€œå‡ºç”Ÿâ€å’Œâ€œåˆå¹¶â€ç§»åŠ¨ï¼Œä½¿æˆ‘ä»¬çš„æ¡†æž¶èƒ½å¤Ÿä»¥â€œåŠ¨æ€é€‚åº”â€çš„æ–¹å¼èšç±»æ•°æ®ï¼Œè€Œä¸éœ€è¦å…ˆçŸ¥é“ç‰¹å¾çš„æ•°é‡ã€‚æˆ‘ä»¬æŠŠè¯¥æ¡†æž¶å‘½åä¸ºDIVAï¼Œå³åŸºäºŽç‹„åˆ©å…‹é›·è¿‡ç¨‹çš„å¢žé‡æ·±åº¦èšç±»æ¡†æž¶çš„å˜åˆ†è‡ªç¼–ç å™¨ã€‚æˆ‘ä»¬çš„æ¡†æž¶åœ¨åˆ†ç±»å…·æœ‰åŠ¨æ€å˜åŒ–ç‰¹å¾çš„å¤æ‚æ•°æ®æ–¹é¢è¡¨çŽ°ä¼˜è¶Šï¼Œç‰¹åˆ«æ˜¯åœ¨å¢žé‡ç‰¹å¾çš„æƒ…å†µä¸‹ï¼Œè¶…è¿‡äº†æœ€å…ˆè¿›çš„åŸºå‡†ã€‚

    Generative model-based deep clustering frameworks excel in classifying complex data, but are limited in handling dynamic and complex features because they require prior knowledge of the number of clusters. In this paper, we propose a nonparametric deep clustering framework that employs an infinite mixture of Gaussians as a prior. Our framework utilizes a memoized online variational inference method that enables the "birth" and "merge" moves of clusters, allowing our framework to cluster data in a "dynamic-adaptive" manner, without requiring prior knowledge of the number of features. We name the framework as DIVA, a Dirichlet Process-based Incremental deep clustering framework via Variational Auto-Encoder. Our framework, which outperforms state-of-the-art baselines, exhibits superior performance in classifying complex data with dynamically changing features, particularly in the case of incremental features.
    
[^16]: åŠ¨é‡åŒ¹é…åŽ»å™ªGibbsé‡‡æ ·

    Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])

    [http://arxiv.org/abs/2305.11650](http://arxiv.org/abs/2305.11650)

    æœ¬æ–‡æå‡ºäº†åŠ¨é‡åŒ¹é…åŽ»å™ªGibbsé‡‡æ ·æ–¹æ³•ï¼Œå¯ä»¥åœ¨ç»™å®šâ€˜å˜ˆæ‚â€™çš„æ¨¡åž‹çš„æƒ…å†µä¸‹ï¼Œä»Žå¹²å‡€çš„æ¨¡åž‹ä¸­æœ‰æ•ˆåœ°è¿›è¡Œé‡‡æ ·ã€‚

    

    èƒ½é‡åŸºæ¨¡åž‹ï¼ˆEBMsï¼‰ä¸ºå»ºæ¨¡å¤æ‚æ•°æ®åˆ†å¸ƒæä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æž¶ã€‚ç„¶è€Œï¼ŒEBMs çš„è®­ç»ƒå’Œé‡‡æ ·ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç”¨äºŽå¯æ‰©å±• EBM è®­ç»ƒçš„å¹¿æ³›ä½¿ç”¨çš„åŽ»å™ªåˆ†æ•°åŒ¹é…ï¼ˆDSMï¼‰æ–¹æ³•å­˜åœ¨ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œå¯¼è‡´èƒ½é‡æ¨¡åž‹å­¦ä¹ åˆ°â€œå˜ˆæ‚â€çš„æ•°æ®åˆ†å¸ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„é‡‡æ ·æ¡†æž¶ï¼šï¼ˆä¼ªï¼‰Gibbsé‡‡æ ·ä¸ŽåŠ¨é‡åŒ¹é…ï¼Œå¯ä»¥åœ¨ç»™å®šç»è¿‡DSMè®­ç»ƒè‰¯å¥½çš„â€œå˜ˆæ‚â€æ¨¡åž‹çš„æƒ…å†µä¸‹ï¼Œä»ŽåŸºç¡€â€œå¹²å‡€â€æ¨¡åž‹ä¸­æœ‰æ•ˆåœ°è¿›è¡Œé‡‡æ ·ã€‚æˆ‘ä»¬æŽ¢è®¨äº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºŽç›¸å…³æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•å°†è¯¥æ–¹æ³•æ‰©å±•åˆ°é«˜ç»´æ•°æ®é›†ã€‚

    Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
    
[^17]: å…³äºŽâ€œæœ€è¿‘é‚»ç®—æ³•çš„ä»»åŠ¡ç‰¹å®šæ•°æ®æœ‰æ•ˆæ€§â€çš„æ³¨è®°ï¼ˆarXivï¼š2304.04258v1 [stat.ML]ï¼‰

    A Note on "Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms". (arXiv:2304.04258v1 [stat.ML])

    [http://arxiv.org/abs/2304.04258](http://arxiv.org/abs/2304.04258)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ›´è‡ªç„¶å’Œå¯è§£é‡Šçš„æ•ˆç”¨å‡½æ•°ï¼Œæ›´å¥½åœ°åæ˜ äº†KNNæ¨¡åž‹çš„æ€§èƒ½ï¼Œæä¾›äº†ç›¸åº”è®¡ç®—è¿‡ç¨‹ï¼Œè¯¥æ–¹æ³•è¢«ç§°ä¸ºè½¯æ ‡ç­¾KNN-SVï¼Œä¸ŽåŽŸå§‹æ–¹æ³•å…·æœ‰ç›¸åŒçš„æ—¶é—´å¤æ‚åº¦ã€‚

    

    æ•°æ®æœ‰æ•ˆæ€§æ˜¯ä¸€ä¸ªç ”ç©¶å•ä¸ªæ•°æ®ç‚¹å¯¹æœºå™¨å­¦ä¹ ï¼ˆMLï¼‰æ¨¡åž‹å½±å“çš„æ—¥ç›Šå¢žé•¿çš„ç ”ç©¶é¢†åŸŸã€‚åŸºäºŽåˆä½œåšå¼ˆè®ºå’Œç»æµŽå­¦ï¼Œæ•°æ® Shapley æ˜¯ä¸€ç§æœ‰æ•ˆçš„æ•°æ®æœ‰æ•ˆæ€§è®¡ç®—æ–¹æ³•ã€‚ç„¶è€Œï¼Œäººä»¬éƒ½çŸ¥é“ Shapley å€¼ï¼ˆSVï¼‰çš„è®¡ç®—å¯èƒ½éžå¸¸æ˜‚è´µã€‚å¹¸è¿çš„æ˜¯ï¼ŒJia ç­‰äººï¼ˆ2019ï¼‰è¡¨æ˜Žï¼Œå¯¹äºŽ K æœ€è¿‘é‚»ï¼ˆKNNï¼‰æ¨¡åž‹ï¼Œè®¡ç®— Data Shapley ç«Ÿç„¶éžå¸¸ç®€å•å’Œé«˜æ•ˆã€‚åœ¨æœ¬ç¬”è®°ä¸­ï¼Œæˆ‘ä»¬é‡å®¡äº† Jia ç­‰äººï¼ˆ2019ï¼‰çš„å·¥ä½œï¼Œå¹¶æå‡ºäº†ä¸€ç§æ›´è‡ªç„¶å’Œå¯è§£é‡Šçš„æ•ˆç”¨å‡½æ•°ï¼Œæ›´å¥½åœ°åæ˜ äº† KNN æ¨¡åž‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬æŽ¨å¯¼äº†å…·æœ‰æ–°æ•ˆç”¨å‡½æ•°çš„ KNN åˆ†ç±»å™¨/å›žå½’å™¨çš„ Data Shapley çš„ç›¸åº”è®¡ç®—è¿‡ç¨‹ã€‚æˆ‘ä»¬çš„æ–°æ–¹æ³•è¢«ç§°ä¸ºè½¯æ ‡ç­¾ KNN-SVï¼Œä¸ŽåŽŸå§‹æ–¹æ³•å…·æœ‰ç›¸åŒçš„æ—¶é—´å¤æ‚åº¦ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æä¾›äº†ä¸€ç§åŸºäºŽå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼ˆLSHï¼‰çš„è½¯æ ‡ç­¾ KNN-SV çš„é«˜æ•ˆè¿‘ä¼¼ç®—æ³•ã€‚

    Data valuation is a growing research field that studies the influence of individual data points for machine learning (ML) models. Data Shapley, inspired by cooperative game theory and economics, is an effective method for data valuation. However, it is well-known that the Shapley value (SV) can be computationally expensive. Fortunately, Jia et al. (2019) showed that for K-Nearest Neighbors (KNN) models, the computation of Data Shapley is surprisingly simple and efficient.  In this note, we revisit the work of Jia et al. (2019) and propose a more natural and interpretable utility function that better reflects the performance of KNN models. We derive the corresponding calculation procedure for the Data Shapley of KNN classifiers/regressors with the new utility functions. Our new approach, dubbed soft-label KNN-SV, achieves the same time complexity as the original method. We further provide an efficient approximation algorithm for soft-label KNN-SV based on locality sensitive hashing (LSH
    
[^18]: é¢å‘ç±»åˆ«ä¸å‡é—®é¢˜çš„é›†æˆå­¦ä¹ å’Œæ•°æ®å¢žå¼ºæ¨¡åž‹ç»¼è¿°ï¼šç»„åˆã€å®žçŽ°å’Œè¯„ä¼°

    A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v1 [cs.LG])

    [http://arxiv.org/abs/2304.02858](http://arxiv.org/abs/2304.02858)

    æœ¬æ–‡ç ”ç©¶äº†é›†æˆå­¦ä¹ å’Œæ•°æ®å¢žå¼ºæ–¹æ³•çš„åº”ç”¨ï¼Œé’ˆå¯¹ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ï¼Œé€šè¿‡è®¡ç®—è¯„ä¼°ï¼Œæ‰¾åˆ°äº†æœ€æœ‰æ•ˆçš„ç»„åˆã€‚

    

    åˆ†ç±»é—®é¢˜ä¸­çš„ç±»åˆ«ä¸å¹³è¡¡ï¼ˆCIï¼‰æ˜¯æŒ‡å±žäºŽä¸€ä¸ªç±»çš„è§‚æµ‹å€¼æ•°é‡ä½ŽäºŽå…¶ä»–ç±»çš„æ•°é‡ã€‚é›†æˆå­¦ä¹ ç»“åˆæ•°æ®å¢žå¼ºæ–¹æ³•å·²è¢«å¹¿æ³›åº”ç”¨äºŽè§£å†³ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜ã€‚åœ¨è¿‡åŽ»çš„åå¹´é‡Œï¼Œä¸€äº›ç­–ç•¥å·²ç»è¢«åº”ç”¨äºŽå¢žå¼ºé›†æˆå­¦ä¹ å’Œæ•°æ®å¢žå¼ºæ–¹æ³•ï¼ŒåŒæ—¶è¿˜å¼€å‘äº†ä¸€äº›æ–°æ–¹æ³•ï¼Œå¦‚ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰ã€‚æœ¬æ–‡å¯¹ç”¨äºŽè§£å†³åŸºå‡†CIé—®é¢˜çš„æ•°æ®å¢žå¼ºå’Œé›†æˆå­¦ä¹ æ–¹æ³•è¿›è¡Œè®¡ç®—è¯„ä¼°ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªè¯„ä¼°CIé—®é¢˜çš„10ä¸ªæ•°æ®å¢žå¼ºæ–¹æ³•å’Œ10ä¸ªé›†æˆå­¦ä¹ æ–¹æ³•çš„é€šç”¨æ¡†æž¶ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯è¯†åˆ«æé«˜åˆ†ç±»æ•ˆæžœæœ€æœ‰æ•ˆçš„ç»„åˆã€‚

    Class imbalance (CI) in classification problems arises when the number of observations belonging to one class is lower than the other classes. Ensemble learning that combines multiple models to obtain a robust model has been prominently used with data augmentation methods to address class imbalance problems. In the last decade, a number of strategies have been added to enhance ensemble learning and data augmentation methods, along with new methods such as generative adversarial networks (GANs). A combination of these has been applied in many studies, but the true rank of different combinations would require a computational review. In this paper, we present a computational review to evaluate data augmentation and ensemble learning methods used to address prominent benchmark CI problems. We propose a general framework that evaluates 10 data augmentation and 10 ensemble learning methods for CI problems. Our objective was to identify the most effective combination for improving classificat
    
[^19]: åœ¨Sobolevå’ŒBesovç©ºé—´ä¸Šï¼Œå…³äºŽæ·±åº¦ReLUç¥žç»ç½‘ç»œçš„æœ€ä½³é€¼è¿‘é€ŸçŽ‡ç ”ç©¶

    Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14400](http://arxiv.org/abs/2211.14400)

    è¯¥è®ºæ–‡ç ”ç©¶äº†åœ¨Sobolevå’ŒBesovç©ºé—´ä¸­ï¼Œä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„æ·±åº¦ç¥žç»ç½‘ç»œèƒ½å¤Ÿä»¥æ€Žæ ·çš„å‚æ•°æ•ˆçŽ‡é€¼è¿‘å‡½æ•°ï¼ŒåŒ…æ‹¬$L_p(\Omega)$èŒƒæ•°ä¸‹çš„è¯¯å·®åº¦é‡ã€‚æˆ‘ä»¬æä¾›äº†æ‰€æœ‰$1\leq p,q \leq \infty$å’Œ$s>0$çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œå¹¶å¼•å…¥äº†ä¸€ç§æ–°çš„ä½æå–æŠ€æœ¯æ¥èŽ·å¾—å°–é”çš„ä¸Šç•Œã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°çš„æ·±åº¦ç¥žç»ç½‘ç»œåœ¨Sobolevç©ºé—´$W^s(L_q(\Omega))$å’ŒBesovç©ºé—´$B^s_r(L_q(\Omega))$ä¸­ä»¥$L_p(\Omega)$èŒƒæ•°åº¦é‡è¯¯å·®çš„å‚æ•°æ•ˆçŽ‡é—®é¢˜ã€‚æˆ‘ä»¬çš„ç ”ç©¶å¯¹äºŽåœ¨ç§‘å­¦è®¡ç®—å’Œä¿¡å·å¤„ç†ç­‰é¢†åŸŸä¸­åº”ç”¨ç¥žç»ç½‘ç»œéžå¸¸é‡è¦ï¼Œåœ¨è¿‡åŽ»åªæœ‰å½“$p=q=\infty$æ—¶æ‰å®Œå…¨è§£å†³ã€‚æˆ‘ä»¬çš„è´¡çŒ®æ˜¯æä¾›äº†æ‰€æœ‰$1\leq p,q\leq \infty$å’Œ$s>0$çš„å®Œæ•´è§£å†³æ–¹æ¡ˆï¼ŒåŒ…æ‹¬æ¸è¿‘åŒ¹é…çš„ä¸Šä¸‹ç•Œã€‚å…³é”®çš„æŠ€æœ¯å·¥å…·æ˜¯ä¸€ç§æ–°çš„ä½æå–æŠ€æœ¯ï¼Œå®ƒæä¾›äº†ç¨€ç–å‘é‡çš„æœ€ä½³ç¼–ç ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨$p>q$çš„éžçº¿æ€§åŒºåŸŸèŽ·å¾—å°–é”çš„ä¸Šç•Œã€‚æˆ‘ä»¬è¿˜æä¾›äº†ä¸€ç§åŸºäºŽçš„$L_p$é€¼è¿‘ä¸‹ç•ŒæŽ¨å¯¼çš„æ–°æ–¹æ³•ã€‚

    Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the problem of how efficiently, in terms of the number of parameters, deep neural networks with the ReLU activation function can approximate functions in the Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with error measured in the $L_p(\Omega)$ norm. This problem is important when studying the application of neural networks in a variety of fields, including scientific computing and signal processing, and has previously been completely solved only when $p=q=\infty$. Our contribution is to provide a complete solution for all $1\leq p,q\leq \infty$ and $s > 0$, including asymptotically matching upper and lower bounds. The key technical tool is a novel bit-extraction technique which gives an optimal encoding of sparse vectors. This enables us to obtain sharp upper bounds in the non-linear regime where $p > q$. We also provide a novel method for deriving $L_p$-approximation lower bounds based upon
    
[^20]: $k$-å‡å€¼èšç±»ç”¨äºŽæŒä¹…åŒè°ƒ

    $k$-Means Clustering for Persistent Homology. (arXiv:2210.10003v3 [stat.AP] UPDATED)

    [http://arxiv.org/abs/2210.10003](http://arxiv.org/abs/2210.10003)

    æœ¬æ–‡è¯æ˜Žäº†$k$-å‡å€¼èšç±»ç®—æ³•åœ¨æŒä¹…å›¾ç©ºé—´ä¸Šçš„æ”¶æ•›æ€§ï¼Œè§£å†³äº†ä»£æ•°æž„é€ å¯¼è‡´çš„å¤æ‚åº¦é—®é¢˜ï¼Œé€šè¿‡å®žéªŒè¯æ˜Žç›´æŽ¥åœ¨æŒä¹…å›¾å’ŒæŒä¹…åº¦é‡ä¸Šè¿›è¡Œèšç±»ä¼˜äºŽå‘é‡è¡¨ç¤ºã€‚

    

    æŒä¹…åŒè°ƒæ˜¯æ‹“æ‰‘æ•°æ®åˆ†æžä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œç”¨äºŽæå–å’Œæ€»ç»“æ•°æ®é›†ä¸­çš„æ‹“æ‰‘ç‰¹å¾ï¼Œå¹¶ä»¥æŒä¹…å›¾çš„å½¢å¼è¡¨ç¤ºã€‚è¿‘å¹´æ¥ï¼Œåœ¨è®¸å¤šé¢†åŸŸä¸­å¹¿æ³›åº”ç”¨çš„æŒä¹…åŒè°ƒæ–¹æ³•å—åˆ°äº†å¾ˆå¤§çš„å…³æ³¨ã€‚ç„¶è€Œï¼Œå®ƒçš„ä»£æ•°æž„é€ å¯¼è‡´äº†ä¸€ä¸ªå…·æœ‰é«˜åº¦å¤æ‚å‡ ä½•çš„æŒç»­å›¾ç©ºé—´çš„åº¦é‡ç©ºé—´ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¯æ˜Žäº†$k$-å‡å€¼èšç±»ç®—æ³•åœ¨æŒä¹…å›¾ç©ºé—´ä¸Šçš„æ”¶æ•›æ€§ï¼Œå¹¶åœ¨Karush-Kuhn-Tuckeræ¡†æž¶ä¸‹å»ºç«‹äº†ä¼˜åŒ–é—®é¢˜çš„ç†è®ºæ€§è´¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¯¹æŒä¹…åŒè°ƒçš„å„ç§è¡¨ç¤ºè¿›è¡Œäº†æ•°å€¼å®žéªŒï¼ŒåŒ…æ‹¬æŒä¹…å›¾çš„åµŒå…¥ä»¥åŠå›¾å’Œå®ƒä»¬çš„æŽ¨å¹¿ä½œä¸ºæŒä¹…åº¦é‡ï¼›æˆ‘ä»¬å‘çŽ°ï¼Œç›´æŽ¥åœ¨æŒä¹…å›¾å’ŒæŒä¹…åº¦é‡ä¸Šè¿›è¡Œèšç±»çš„æ€§èƒ½ä¼˜äºŽå®ƒä»¬çš„å‘é‡è¡¨ç¤ºã€‚

    Persistent homology is a methodology central to topological data analysis that extracts and summarizes the topological features within a dataset as a persistence diagram; it has recently gained much popularity from its myriad successful applications to many domains. However, its algebraic construction induces a metric space of persistence diagrams with a highly complex geometry. In this paper, we prove convergence of the $k$-means clustering algorithm on persistence diagram space and establish theoretical properties of the solution to the optimization problem in the Karush--Kuhn--Tucker framework. Additionally, we perform numerical experiments on various representations of persistent homology, including embeddings of persistence diagrams as well as diagrams themselves and their generalizations as persistence measures; we find that clustering performance directly on persistence diagrams and measures outperform their vectorized representations.
    
[^21]: é™ç»´ä¸ŽWassersteinç¨³å®šæ€§åœ¨æ ¸å›žå½’ä¸­çš„åº”ç”¨

    Dimensionality Reduction and Wasserstein Stability for Kernel Regression. (arXiv:2203.09347v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2203.09347](http://arxiv.org/abs/2203.09347)

    æœ¬æ–‡ç ”ç©¶äº†åœ¨é«˜ç»´å›žå½’æ¡†æž¶ä¸­çš„é™ç»´ä¸ŽWassersteinç¨³å®šæ€§åº”ç”¨ï¼Œé’ˆå¯¹åœ¨æ‰°åŠ¨è¾“å…¥æ•°æ®ç”¨äºŽæ‹Ÿåˆå›žå½’å‡½æ•°æ—¶å‡ºçŽ°çš„è¯¯å·®æŽ¨å¯¼äº†ç¨³å®šæ€§ç»“æžœï¼Œå¹¶åˆ©ç”¨ä¸»æˆåˆ†åˆ†æžå’Œæ ¸å›žå½’æ–‡çŒ®ä¸­çš„ä¼°è®¡ï¼ŒæŽ¨å¯¼äº†ä¸¤æ­¥æ³•çš„æ”¶æ•›é€Ÿåº¦ã€‚

    

    åœ¨é«˜ç»´å›žå½’æ¡†æž¶ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªæœ´ç´ çš„ä¸¤æ­¥æ³•ï¼Œé¦–å…ˆé™ä½Žè¾“å…¥å˜é‡çš„ç»´æ•°ï¼Œå†ä½¿ç”¨æ ¸å›žå½’æ¥é¢„æµ‹è¾“å‡ºå˜é‡ã€‚ä¸ºäº†åˆ†æžç”±æ­¤äº§ç”Ÿçš„å›žå½’è¯¯å·®ï¼Œæˆ‘ä»¬æŽ¨å¯¼äº†ä¸€ä¸ªé’ˆå¯¹Wassersteinè·ç¦»çš„æ–°çš„æ ¸å›žå½’ç¨³å®šæ€§ç»“æžœã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿé™åˆ¶å½“æ‰°åŠ¨è¾“å…¥æ•°æ®ç”¨äºŽæ‹Ÿåˆå›žå½’å‡½æ•°æ—¶å‡ºçŽ°çš„è¯¯å·®ã€‚æˆ‘ä»¬å°†é€šç”¨çš„ç¨³å®šæ€§ç»“æžœåº”ç”¨äºŽä¸»æˆåˆ†åˆ†æž(PCA)ï¼Œåˆ©ç”¨å·²çŸ¥çš„ä¸»æˆåˆ†åˆ†æžå’Œæ ¸å›žå½’æ–‡çŒ®ä¸­çš„ä¼°è®¡ï¼ŒæŽ¨å¯¼å‡ºäº†ä¸¤æ­¥æ³•çš„æ”¶æ•›é€Ÿåº¦ã€‚åŽè€…åœ¨åŠç›‘ç£è®¾ç½®ä¸­ç‰¹åˆ«æœ‰ç”¨ã€‚

    In a high-dimensional regression framework, we study consequences of the naive two-step procedure where first the dimension of the input variables is reduced and second, the reduced input variables are used to predict the output variable with kernel regression. In order to analyze the resulting regression errors, a novel stability result for kernel regression with respect to the Wasserstein distance is derived. This allows us to bound errors that occur when perturbed input data is used to fit the regression function. We apply the general stability result to principal component analysis (PCA). Exploiting known estimates from the literature on both principal component analysis and kernel regression, we deduce convergence rates for the two-step procedure. The latter turns out to be particularly useful in a semi-supervised setting.
    
[^22]: é«˜æ–¯è¿‡ç¨‹æ’å€¼ä¸­å…‰æ»‘å‚æ•°ä¼°è®¡çš„æ¸è¿‘ç•Œé™

    Asymptotic Bounds for Smoothness Parameter Estimates in Gaussian Process Interpolation. (arXiv:2203.05400v4 [math.ST] UPDATED)

    [http://arxiv.org/abs/2203.05400](http://arxiv.org/abs/2203.05400)

    è¯¥è®ºæ–‡ç ”ç©¶äº†é«˜æ–¯è¿‡ç¨‹æ’å€¼ä¸­å…‰æ»‘å‚æ•°ä¼°è®¡çš„æ¸è¿‘ç•Œé™ã€‚ç»“æžœè¡¨æ˜Žï¼Œå…‰æ»‘å‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸èƒ½åœ¨æ¸è¿‘æ„ä¹‰ä¸‹æ¬ å¹³æ»‘çœŸå€¼ï¼Œå¹¶ä¸”æœ€å¤§ä¼¼ç„¶ä¼°è®¡èƒ½æ¢å¤ä¸€ç±»åˆ†æ®µæ”¯æŒè‡ªç›¸ä¼¼å‡½æ•°çš„çœŸå®žå…‰æ»‘åº¦ã€‚

    

    å¸¸è§çš„æ–¹æ³•æ˜¯ç”¨Maternåæ–¹å·®æ ¸å°†ç¡®å®šæ€§å“åº”å‡½æ•°ï¼ˆå¦‚è®¡ç®—æœºå®žéªŒçš„è¾“å‡ºï¼‰å»ºæ¨¡ä¸ºé«˜æ–¯è¿‡ç¨‹ã€‚Maternæ ¸çš„å…‰æ»‘å‚æ•°å†³å®šäº†æ¨¡åž‹åœ¨å¤§æ•°æ®æžé™ä¸‹çš„è®¸å¤šé‡è¦å±žæ€§ï¼ŒåŒ…æ‹¬æ¡ä»¶å‡å€¼æ”¶æ•›åˆ°å“åº”å‡½æ•°çš„é€ŸçŽ‡ã€‚æˆ‘ä»¬è¯æ˜Žï¼Œå½“æ•°æ®åœ¨å›ºå®šæœ‰ç•Œå­é›†$\mathbb{R}^d$ä¸ŠèŽ·å¾—æ—¶ï¼Œå…‰æ»‘å‚æ•°çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ä¸èƒ½åœ¨æ¸è¿‘æ„ä¹‰ä¸‹æ¬ å¹³æ»‘çœŸå€¼ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æžœæ•°æ®ç”Ÿæˆçš„å“åº”å‡½æ•°å…·æœ‰Sobolevå…‰æ»‘åº¦$\nu_0 > d/2$ï¼Œé‚£ä¹ˆå…‰æ»‘å‚æ•°ä¼°è®¡ä¸èƒ½åœ¨æ¸è¿‘æ„ä¹‰ä¸‹å°äºŽ$\nu_0$ã€‚è¿™ä¸€ä¸‹ç•Œæ˜¯ç²¾å‡†çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å±•ç¤ºäº†æœ€å¤§ä¼¼ç„¶ä¼°è®¡åœ¨ä¸€ç±»åˆ†æ®µæ”¯æŒè‡ªç›¸ä¼¼å‡½æ•°ä¸­èƒ½æ¢å¤çœŸå®žçš„å…‰æ»‘åº¦ã€‚å¯¹äºŽäº¤å‰éªŒè¯ï¼Œæˆ‘ä»¬è¯æ˜Žäº†ä¸€ä¸ªæ¸è¿‘ä¸‹ç•Œ$\nu_0-d/2$ï¼Œä½†è¿™å¾ˆä¸å¯èƒ½æˆç«‹ã€‚

    It is common to model a deterministic response function, such as the output of a computer experiment, as a Gaussian process with a Mat\'ern covariance kernel. The smoothness parameter of a Mat\'ern kernel determines many important properties of the model in the large data limit, including the rate of convergence of the conditional mean to the response function. We prove that the maximum likelihood estimate of the smoothness parameter cannot asymptotically undersmooth the truth when the data are obtained on a fixed bounded subset of $\mathbb{R}^d$. That is, if the data-generating response function has Sobolev smoothness $\nu_0 > d/2$, then the smoothness parameter estimate cannot be asymptotically less than $\nu_0$. The lower bound is sharp. Additionally, we show that maximum likelihood estimation recovers the true smoothness for a class of compactly supported self-similar functions. For cross-validation we prove an asymptotic lower bound $\nu_0 - d/2$, which however is unlikely to be s
    
[^23]: å­¦ä¹ æ ‘çŠ¶ä¸‰ç»´ç‰©ä½“çš„å‡ ä½•å’Œæ‹“æ‰‘çš„ç”Ÿæˆæ¨¡åž‹

    Learning Generative Models of the Geometry and Topology of Tree-like 3D Objects. (arXiv:2110.08693v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.08693](http://arxiv.org/abs/2110.08693)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ‰©å±•å¹³æ–¹æ ¹é€Ÿåº¦å‡½æ•°çš„æ–°è¡¨ç¤ºæ–¹æ³•å’Œåº¦é‡æ–¹æ³•ï¼Œç”¨äºŽåˆ†æžå’Œæ¯”è¾ƒæ ‘çŠ¶ä¸‰ç»´ç‰©ä½“ï¼Œä»Žè€Œæé«˜ç‰©ä½“å½¢çŠ¶å·®å¼‚è®¡ç®—çš„ç²¾åº¦å’Œæ•ˆçŽ‡ã€‚

    

    å¦‚ä½•åˆ†æžå±•çŽ°å‡ºå¤æ‚å‡ ä½•å’Œæ‹“æ‰‘å˜åŒ–çš„è¯¦ç»†ä¸‰ç»´ç”Ÿç‰©ç‰©ä½“ï¼Œä¾‹å¦‚ç¥žç»å…ƒå’Œæ¤ç‰©æ ‘ï¼Ÿæœ¬æ–‡æå‡ºäº†ä¸€ä¸ªæ–°çš„æ•°å­¦æ¡†æž¶ï¼Œç”¨äºŽè¡¨ç¤ºã€æ¯”è¾ƒå’Œè®¡ç®—è¿™äº›æ ‘çŠ¶ä¸‰ç»´å¯¹è±¡çš„å½¢çŠ¶å·®å¼‚ï¼Œå¹¶å®šä¹‰äº†ä¸€ç§æ–°çš„åº¦é‡æ–¹æ³•æ¥é‡åŒ–å°†ä¸€ä¸ªæ ‘çŠ¶ç‰©ä½“å˜å½¢ä¸ºå¦ä¸€ä¸ªç‰©ä½“æ‰€éœ€çš„å¼¯æ›²ã€æ‹‰ä¼¸å’Œåˆ†æ”¯æ»‘åŠ¨ã€‚

    How can one analyze detailed 3D biological objects, such as neurons and botanical trees, that exhibit complex geometrical and topological variation? In this paper, we develop a novel mathematical framework for representing, comparing, and computing geodesic deformations between the shapes of such tree-like 3D objects. A hierarchical organization of subtrees characterizes these objects -- each subtree has the main branch with some side branches attached -- and one needs to match these structures across objects for meaningful comparisons. We propose a novel representation that extends the Square-Root Velocity Function (SRVF), initially developed for Euclidean curves, to tree-shaped 3D objects. We then define a new metric that quantifies the bending, stretching, and branch sliding needed to deform one tree-shaped object into the other. Compared to the current metrics, such as the Quotient Euclidean Distance (QED) and the Tree Edit Distance (TED), the proposed representation and metric cap
    

