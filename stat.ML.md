# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs.](http://arxiv.org/abs/2310.20145) | æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„robust Bayesian Optimizationç®—æ³•ï¼ŒAIRBOï¼Œå®ƒèƒ½å¤Ÿåœ¨ä»»æ„è¾“å…¥ä¸ç¡®å®šæ€§ä¸‹æœ‰æ•ˆè¯†åˆ«å‡ºè¡¨ç°ä¸€è‡´è‰¯å¥½çš„é²æ£’æœ€ä¼˜è§£ã€‚ |
| [^2] | [Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation.](http://arxiv.org/abs/2310.18919) | æœ¬ç ”ç©¶è§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸­å»¶è¿Ÿåé¦ˆå¯¹çº¿æ€§å‡½æ•°é€¼è¿‘çš„æŒ‘æˆ˜ï¼Œé€šè¿‡åéªŒé‡‡æ ·ç®—æ³•å®ç°äº†åœ¨ä¸åŒæƒ…å†µä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚ |
| [^3] | [On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers.](http://arxiv.org/abs/2310.14421) | æœ¬æ–‡ç ”ç©¶äº†é’ˆå¯¹AIåˆ†ç±»å™¨çš„å¯¹æŠ—é²æ£’æ€§åº¦é‡çš„å­˜åœ¨æ€§ã€å”¯ä¸€æ€§å’Œå¯æ‰©å±•æ€§ï¼Œæå‡ºäº†å¯ä»¥éªŒè¯çš„æ•°å­¦æ¡ä»¶ï¼Œå¹¶åœ¨åˆæˆåŸºå‡†æµ‹è¯•å’Œç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­è¿›è¡Œäº†å®é™…è®¡ç®—å’Œè§£é‡Šã€‚ |
| [^4] | [Almost Equivariance via Lie Algebra Convolutions.](http://arxiv.org/abs/2310.13164) | æœ¬æ–‡ç ”ç©¶äº†å‡ ä¹ç­‰å˜æ€§çš„ä¸»é¢˜ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªä¸åŒäºç°æœ‰å®šä¹‰çš„å‡ ä¹ç­‰å˜æ€§å®šä¹‰ï¼Œå¹¶é€šè¿‡åˆ©ç”¨æç¾¤çš„æä»£æ•°ç»™å‡ºäº†åœ¨æ¨¡å‹ä¸­ç¼–ç å‡ ä¹ç­‰å˜æ€§çš„å®ç”¨æ–¹æ³•ã€‚ |
| [^5] | [Sampling via Gradient Flows in the Space of Probability Measures.](http://arxiv.org/abs/2310.03597) | é€šè¿‡æ¢¯åº¦æµæŠ½æ ·æ–¹æ³•çš„ç ”ç©¶æ–¹å‘åœ¨è®¡ç®—ç§‘å­¦å’Œå·¥ç¨‹ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ¬æ–‡é€šè¿‡ç ”ç©¶æ¦‚ç‡æµ‹åº¦ç©ºé—´ä¸­çš„æ¢¯åº¦æµçš„è®¾è®¡ç»„æˆéƒ¨åˆ†ï¼Œæå‡ºäº†ä¸‰ä¸ªè´¡çŒ®ï¼šKullback-Leibleræ•£åº¦ä½œä¸ºèƒ½é‡æ³›å‡½çš„ç‹¬ç‰¹å±æ€§ã€åº¦é‡çš„é€‰æ‹©ä¸ä¸å˜æ€§çš„å…³ç³»ã€‚ |
| [^6] | [Uncertainty Quantification via Neural Posterior Principal Components.](http://arxiv.org/abs/2309.15533) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ç¥ç»ç½‘ç»œåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­é¢„æµ‹ä»»æ„è¾“å…¥å›¾åƒåéªŒåˆ†å¸ƒçš„ä¸»æˆåˆ†çš„æ–¹æ³•ï¼Œä»¥å®ç°ä¸ç¡®å®šæ€§é‡åŒ–ã€‚ |
| [^7] | [PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference.](http://arxiv.org/abs/2309.02334) | æå‡ºäº†ä¸€ç§åä¸ºPolyLUTçš„æ–°æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œåœ¨FPGAä¸Šè¿›è¡Œéƒ¨ç½²ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šå˜é‡å¤šé¡¹å¼ä½œä¸ºåŸºæœ¬æ¨¡å—ï¼Œå¹¶åˆ©ç”¨è½¯é€»è¾‘å°†å¤šé¡¹å¼è¯„ä¼°éšè—åœ¨FPGAçš„æŸ¥æ‰¾è¡¨ä¸­ï¼Œä»è€Œå®ç°è¶…ä½å»¶è¿Ÿæ¨ç†ï¼Œå¹¶å‡å°‘äº†è½¯ä»¶é€»è¾‘çš„å±‚æ•°ã€‚ |
| [^8] | [Low Tensor Rank Learning of Neural Dynamics.](http://arxiv.org/abs/2308.11567) | ç ”ç©¶å‘ç°é€šè¿‡å­¦ä¹ è¿‡ç¨‹ä¸­çš„å¼ é‡ç§©æ¼”åŒ–æ¥ç†è§£ç¥ç»å…ƒè¿æ¥åœ¨å­¦ä¹ ä¸­çš„åè°ƒå˜åŒ–ã€‚ç ”ç©¶è¡¨æ˜è®­ç»ƒè¿‡çš„é€’å½’ç¥ç»ç½‘ç»œçš„æƒé‡çŸ©é˜µé€šå¸¸å…·æœ‰ä½ç§©ç»“æ„ï¼Œè€Œè¿™ç§ç»“æ„åœ¨æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸­ä¿æŒåœ¨ä¸€ä¸ªå›ºå®šçš„ä½ç»´å­ç©ºé—´ä¸­ã€‚å¯¹çœŸå®æƒé‡è¿›è¡Œä½ç§©åˆ†è§£éªŒè¯äº†è¿™ä¸€è§‚å¯Ÿç»“æœã€‚ |
| [^9] | [Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language.](http://arxiv.org/abs/2308.05061) | æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ä¼ æ„Ÿå™¨æ•°æ®ã€æ–¹ç¨‹å’Œè‡ªç„¶è¯­è¨€æç¤ºä¸Šä¸‹æ–‡ä¸­è¿ç®—ç¬¦å­¦ä¹ çš„æ–¹æ³•ã€‚é€šè¿‡æ•´åˆäººç±»çŸ¥è¯†å’Œè¯­è¨€æè¿°ï¼Œè¯¥æ–¹æ³•ä¸ä»…æ‰©å±•äº†ç‰©ç†ä¿¡æ¯å­¦ä¹ çš„çµæ´»æ€§å’Œæ™®é€‚æ€§ï¼Œè€Œä¸”æ˜¾è‘—æé«˜äº†å­¦ä¹ æ€§èƒ½å’Œå‡å°‘äº†æ•°æ®éœ€æ±‚ã€‚ |
| [^10] | [Online covariance estimation for stochastic gradient descent under Markovian sampling.](http://arxiv.org/abs/2308.01481) | æœ¬æ–‡ç ”ç©¶äº†åœ¨é©¬å°”å¯å¤«é‡‡æ ·ä¸‹çš„éšæœºæ¢¯åº¦ä¸‹é™ä¸­çš„åœ¨çº¿é‡å æ‰¹æ¬¡å‡å€¼åæ–¹å·®ä¼°è®¡å™¨ï¼Œå¹¶è¯æ˜äº†å…¶æ”¶æ•›é€Ÿç‡ä¸º$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$å’Œ$O\big(\sqrt{d}\,n^{-1/8}\big)$ï¼Œåˆ†åˆ«å¯¹åº”äºçŠ¶æ€ç›¸å…³å’ŒçŠ¶æ€æ— å…³çš„é©¬å°”å¯å¤«é‡‡æ ·ã€‚è¿™äº›é€Ÿç‡ä¸ç‹¬ç«‹åŒåˆ†å¸ƒæƒ…å†µä¸‹çš„æœ€ä½³æ”¶æ•›é€Ÿç‡ç›¸åŒ¹é…ï¼Œå¹¶ä¸”å…‹æœäº†ç”±äºé©¬å°”å¯å¤«é‡‡æ ·è€Œå¼•èµ·çš„æŒ‘æˆ˜ã€‚ |
| [^11] | [A Theory for Emergence of Complex Skills in Language Models.](http://arxiv.org/abs/2307.15936) | æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿè®¡æ¡†æ¶ï¼Œé€šè¿‡åˆ†æè¯­è¨€æ¨¡å‹çš„äº¤å‰ç†µæŸå¤±ä¸åŸºæœ¬è¯­è¨€ä»»åŠ¡çš„èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ï¼Œæ­ç¤ºäº†è¯­è¨€æ¨¡å‹ä¸­å¤æ‚æŠ€èƒ½äº§ç”Ÿçš„æœºåˆ¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ‰©å±•å®šå¾‹ï¼Œé¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆå­¦ä¹ ï¼Œå¹¶è¡¨ç°å‡ºè¿åé€šå¸¸æ³›åŒ–ç†è®ºçš„èƒ½åŠ›ã€‚ |
| [^12] | [Adaptive Linear Estimating Equations.](http://arxiv.org/abs/2307.07320) | æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£å†³è‡ªé€‚åº”çº¿æ€§å›å½’æ¨¡å‹ä¸­éæ­£æ€æ¸è¿‘è¡Œä¸ºçš„æ–¹æ³•ï¼Œä½¿ç”¨è‡ªé€‚åº”çº¿æ€§ä¼°è®¡æ–¹ç¨‹æ„å»ºå»åä¼°è®¡é‡ï¼Œå¹¶åœ¨å¤šè‡‚è€è™æœºçš„èƒŒæ™¯ä¸‹ä¿æŒäº†æœ€å°äºŒä¹˜ä¼°è®¡é‡çš„éæ¸è¿‘æ€§èƒ½ã€‚ |
| [^13] | [Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges.](http://arxiv.org/abs/2307.01050) | æœ¬æ–‡ç ”ç©¶äº†æœ€ä¼˜è¿è¾“å’Œå˜åˆ†æ¨æ–­ä¹‹é—´çš„è”ç³»ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè·¯å¾„ç©ºé—´æ•£åº¦çš„é‡‡æ ·å’Œç”Ÿæˆå»ºæ¨¡æ¡†æ¶ã€‚é€šè¿‡å¼€å‘æ–°é¢–çš„åŸºäºå¾—åˆ†çš„å›ç«æµæŠ€æœ¯å’Œæ­£åˆ™åŒ–çš„è¿­ä»£æ¯”ä¾‹æ‹Ÿåˆç›®æ ‡ï¼Œæœ¬æ–‡å±•ç¤ºäº†è¿™äº›æ–¹æ³•çš„æ½œåŠ›ã€‚ |
| [^14] | [Is RLHF More Difficult than Standard RL?.](http://arxiv.org/abs/2306.14111) | æœ¬æ–‡è¯æ˜äº†å¯¹äºå¹¿æ³›çš„åå¥½æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç°æœ‰çš„ç®—æ³•å’ŒæŠ€æœ¯ç›´æ¥è§£å†³åŸºäºåå¥½çš„RLé—®é¢˜ï¼Œè€Œå‡ ä¹ä¸éœ€è¦é¢å¤–çš„æˆæœ¬ã€‚ |
| [^15] | [Practical Equivariances via Relational Conditional Neural Processes.](http://arxiv.org/abs/2306.10915) | æœ¬æ–‡æå‡ºçš„å…³ç³»æ¡ä»¶ç¥ç»è¿‡ç¨‹ï¼ˆRCNPsï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆå°†ç­‰å˜æ€§çº³å…¥ä»»ä½•ç¥ç»è¿‡ç¨‹æ¨¡å‹çš„æ–¹æ³•ï¼Œå¹¶æ‰©å±•äº†ç­‰å˜ç¥ç»è¿‡ç¨‹çš„é€‚ç”¨æ€§å’Œå½±å“åŠ›åˆ°æ›´é«˜çš„ç»´åº¦ã€‚ |
| [^16] | [For SALE: State-Action Representation Learning for Deep Reinforcement Learning.](http://arxiv.org/abs/2306.02451) | SALEæ˜¯ä¸€ç§åŸºäºçŠ¶æ€-åŠ¨ä½œè¡¨ç¤ºå­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»ä½çº§çŠ¶æ€ä¸­å®ç°è¡¨ç¤ºå­¦ä¹ ï¼ŒTD7ç®—æ³•å¼•å…¥äº†è¯¥æ–¹æ³•å¹¶åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚ |
| [^17] | [DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model.](http://arxiv.org/abs/2306.01001) | æœ¬æ–‡æå‡ºäº†ä¸€ç§æ‰©æ•£æ¨¡å‹ä¸­çš„è´Ÿè·é¢„æµ‹ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œé‡‡ç”¨Seq2Seqç½‘ç»œç»“æ„æ¥åˆ†ç¦»ä¸¤ç§ç±»å‹çš„ä¸ç¡®å®šæ€§å¹¶å¤„ç†å¼‚å¸¸æƒ…å†µï¼Œä¸ä»…ç€çœ¼äºé¢„æµ‹æ¡ä»¶æœŸæœ›å€¼ã€‚ |
| [^18] | [Exact Generalization Guarantees for (Regularized) Wasserstein Distributionally Robust Models.](http://arxiv.org/abs/2305.17076) | æœ¬æ–‡è¡¨æ˜Wassersteinåˆ†å¸ƒå¼å¼ºé²æ£’ä¼°è®¡å™¨çš„æ³›åŒ–ä¿è¯é€‚ç”¨äºä¸€èˆ¬æ¨¡å‹ç±»åˆ«ï¼Œä¸å—ç»´æ•°ç¾éš¾æ‰€å›°æ‰°ï¼Œç”šè‡³å¯ä»¥æ¶µç›–æµ‹è¯•æ—¶çš„åˆ†å¸ƒå˜åŒ–ã€‚ |
| [^19] | [Differentiable Clustering with Perturbed Spanning Forests.](http://arxiv.org/abs/2305.16358) | ä»‹ç»äº†ä¸€ç§åŸºäºæ‰°åŠ¨ç”Ÿæˆæ ‘çš„å¯å¾®èšç±»æ–¹æ³•ï¼Œä¾èµ–äºçº¿æ€§è§„åˆ’è§£çš„éšæœºæ‰°åŠ¨ï¼Œå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚ |
| [^20] | [An $\varepsilon$-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond.](http://arxiv.org/abs/2305.16041) | æå‡ºä¸€ç§æ–°é¢–çš„é‡‡æ ·è§„åˆ™EB-TC $\varepsilon$ï¼Œç”¨äºéšæœºèµŒåšæœºä¸­çš„$\varepsilon$-æœ€ä½³è‡‚çš„è¾¨è¯†ã€‚è¯¥è§„åˆ™å¯ç”¨äºç¡®å®šå›ºå®šç½®ä¿¡åº¦æˆ–å›ºå®šé¢„ç®—æ ‡è¯†ä¸”å…·å¤‡è‡ªé€‚åº”è°ƒæ•´å‹˜æ¢å‚æ•°çš„æ¸è¿‘æœ€ä¼˜æ€§ã€‚åœ¨ä»¿çœŸå®éªŒä¸­è¡¨ç°è‰¯å¥½ï¼Œé€‚ç”¨äºä¸åŒé—®é¢˜é¢†åŸŸã€‚ |
| [^21] | [Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective.](http://arxiv.org/abs/2305.15408) | æœ¬æ–‡ä»ç†è®ºå±‚é¢æ¢ç©¶äº†å¸¦æœ‰â€œæ€ç»´é“¾â€æç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³åŸºæœ¬æ•°å­¦å’Œå†³ç­–é—®é¢˜ä¸­çš„èƒ½åŠ›ï¼Œå‘ç°è‡ªå›å½’Transformerå¤§å°æ’å®šå³å¯è§£å†³ä»»åŠ¡ï¼Œæ­ç¤ºäº†â€œæ€ç»´é“¾â€æç¤ºçš„èƒŒåæœºåˆ¶ã€‚ |
| [^22] | [Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness.](http://arxiv.org/abs/2305.13946) | æœ¬æ–‡æå‡ºäº†åœ¨çº¿æŠ•èµ„ç»„åˆé€‰æ‹©çš„ç¬¬ä¸€ä¸ªæ•°æ®ç›¸å…³ä¸Šç•Œï¼Œç®—æ³•æ˜¾ç¤ºäºšçº¿æ€§é—æ†¾ç‡ï¼Œå¹¶åœ¨æ•°æ®â€œå®¹æ˜“â€æ—¶å®ç°å¯¹æ•°é—æ†¾ã€‚ |
| [^23] | [Deep Learning with Kernels through RKHM and the Perron-Frobenius Operator.](http://arxiv.org/abs/2305.13588) | è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ ¸æ–¹æ³•çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šæ·±åº¦RKHMï¼Œé€šè¿‡ä½¿ç”¨$C^*$ä»£æ•°è·å¾—æ›´æ¸©å’Œçš„ç•Œé™ï¼Œå¹¶æä¾›äº†è‰¯æ€§è¿‡æ‹Ÿåˆçš„ç†è®ºè§£é‡Šã€‚ |
| [^24] | [Moment Matching Denoising Gibbs Sampling.](http://arxiv.org/abs/2305.11650) | æœ¬æ–‡æå‡ºäº†åŠ¨é‡åŒ¹é…å»å™ªGibbsé‡‡æ ·æ–¹æ³•ï¼Œå¯ä»¥åœ¨ç»™å®šâ€˜å˜ˆæ‚â€™çš„æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä»å¹²å‡€çš„æ¨¡å‹ä¸­æœ‰æ•ˆåœ°è¿›è¡Œé‡‡æ ·ã€‚ |
| [^25] | [Bridging RL Theory and Practice with the Effective Horizon.](http://arxiv.org/abs/2304.09853) | æœ¬è®ºæ–‡é€šè¿‡å¯¹å¸¸è§æ·±åº¦å¼ºåŒ–å­¦ä¹ æµ‹è¯•åŸºå‡†ä¸­155ä¸ªMDPçš„æ•°æ®é›†è¿›è¡Œåˆ†æï¼Œå‘ç°å½“æœ€é«˜Qå€¼çš„åŠ¨ä½œåœ¨éšæœºç­–ç•¥ä¸‹Qå€¼æœ€é«˜æ—¶ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ å¾€å¾€ä¼šæˆåŠŸï¼›åä¹‹ï¼Œåˆ™å¤±è´¥çš„å¯èƒ½æ€§è¾ƒé«˜ã€‚ |
| [^26] | [Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks.](http://arxiv.org/abs/2303.05445) | è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå¸¦å¸æ”¶çš„æ³›æ´ªï¼ˆFwAï¼‰çš„æ–°åè®®ï¼Œç”¨äºè§£å†³å¤æ‚ç½‘ç»œä¸Šçš„å¼‚æ„èµŒåšæœºé—®é¢˜ã€‚é€šè¿‡ä¸¥æ ¼çš„é—æ†¾åˆ†æï¼Œè¯æ˜äº†è¯¥åè®®çš„æœ‰æ•ˆæ€§ã€‚ |
| [^27] | [Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation.](http://arxiv.org/abs/2303.04772) | æœ¬æ–‡ä»‹ç»äº†æ— é™ç»´åº¦å¾—åˆ†æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªåˆ†è¾¨ç‡æ°´å¹³ä¸Šçš„ç¦»æ•£åŒ–æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨å¤šçº§æ‰©æ•£ç®—æ³•åœ¨å¤šä¸ªåˆ†è¾¨ç‡ä¸Šé«˜æ•ˆåœ°å­¦ä¹ ã€‚å®è¯è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç›¸åŒæˆ–æ›´é«˜åˆ†è¾¨ç‡ä¸‹äº§ç”Ÿæ¯”ä¼ ç»ŸåŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹æ›´é«˜è´¨é‡çš„æ ·æœ¬ï¼Œå¹¶å¯ä»¥ç”Ÿæˆä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒå¹¶å¤„ç†çŸ©å½¢åŸŸã€‚ |
| [^28] | [Data pruning and neural scaling laws: fundamental limitations of score-based algorithms.](http://arxiv.org/abs/2302.06960) | è¯„åˆ†æ•°æ®ä¿®å‰ªç®—æ³•åœ¨é«˜å‹ç¼©åŒºåŸŸå¤±è´¥ï¼Œé€šè¿‡éšæœºåŒ–çš„æ ¡å‡†åè®®å¯ä»¥æé«˜ç°æœ‰ä¿®å‰ªç®—æ³•åœ¨è¯¥åŒºåŸŸçš„æ€§èƒ½ã€‚ |
| [^29] | [A unified recipe for deriving (time-uniform) PAC-Bayes bounds.](http://arxiv.org/abs/2302.03421) | è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ¨å¯¼PAC-Bayesianæ³›åŒ–ç•Œé™çš„ç»Ÿä¸€æ¡†æ¶ï¼Œä¸åŒäºä¼ ç»Ÿçš„å›ºå®šæ ·æœ¬å¤§å°æ–¹å¼ï¼Œè¯¥æ¡†æ¶é€‚ç”¨äºæ‰€æœ‰åœæ­¢æ—¶é—´ã€‚åŒæ—¶ï¼Œè¯¥è®ºæ–‡è¿˜æå‡ºäº†æ–°çš„è¾¹ç•Œæ–¹æ³•ï¼Œä¹Ÿå¯ä»¥åº”ç”¨äºéå¹³ç¨³æŸå¤±å‡½æ•°å’Œéç‹¬ç«‹åŒåˆ†å¸ƒçš„æ•°æ®ã€‚ |
| [^30] | [The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing.](http://arxiv.org/abs/2302.01186) | è¯¥ç ”ç©¶æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç§©çŸ©é˜µæ„ŸçŸ¥é—®é¢˜æ—¶å…·æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚ |
| [^31] | [Are you using test log-likelihood correctly?.](http://arxiv.org/abs/2212.00219) | ä½¿ç”¨æµ‹è¯•å¯¹æ•°ä¼¼ç„¶è¿›è¡Œæ¯”è¾ƒå¯èƒ½ä¸å…¶ä»–æŒ‡æ ‡ç›¸çŸ›ç›¾ï¼Œå¹¶ä¸”é«˜æµ‹è¯•å¯¹æ•°ä¼¼ç„¶ä¸æ„å‘³ç€æ›´å‡†ç¡®çš„åéªŒè¿‘ä¼¼ã€‚ |
| [^32] | [Statistical inference for transfer learning with high-dimensional quantile regression.](http://arxiv.org/abs/2211.14578) | æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜ç»´åˆ†ä½æ•°å›å½’æ¨¡å‹ä¸­çš„è½¬ç§»å­¦ä¹ æ–¹æ³•ï¼Œä»¥é€‚åº”æºåŸŸå’Œç›®æ ‡åŸŸä¸­çš„å¼‚è´¨æ€§å’Œé‡å°¾åˆ†å¸ƒã€‚æ ¹æ®ç²¾å¿ƒé€‰æ‹©çš„å¯è½¬ç§»æºåŸŸå»ºç«‹äº†è½¬ç§»å­¦ä¹ ä¼°è®¡é‡çš„è¯¯å·®ç•Œé™ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„ç½®ä¿¡åŒºé—´å’Œå‡è®¾æ£€éªŒç¨‹åºï¼Œä»¥å®ç°ä¸€æ­¥å®Œæˆã€‚ |
| [^33] | [Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees.](http://arxiv.org/abs/2210.07893) | æœ¬æ–‡é’ˆå¯¹é«˜æ–¯è¿‡ç¨‹æ¨¡å‹çš„æ•°å€¼ç¨³å®šæ€§è¿›è¡Œäº†ç ”ç©¶ï¼Œé€šè¿‡æ„Ÿå…´è¶£ç‚¹çš„é€‰æ‹©å’Œè®¡ç®—ï¼Œæä¾›äº†ç¨³å®šå¯é çš„ç¨€ç–é€¼è¿‘æ–¹æ³•ã€‚ |
| [^34] | [Detecting hidden confounding in observational data using multiple environments.](http://arxiv.org/abs/2205.13935) | ä½¿ç”¨ç‹¬ç«‹æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸‹çš„å¤šç¯å¢ƒæ–¹æ³•ï¼Œå¯ä»¥æ£€æµ‹è§‚æµ‹æ•°æ®ä¸­çš„æœªè§‚å¯Ÿåˆ°çš„æ··æ·†å› ç´ ï¼Œå¹¶æå‡ºäº†æµ‹è¯•ç‹¬ç«‹æ€§çš„ç¨‹åºã€‚ |
| [^35] | [Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case.](http://arxiv.org/abs/2202.05069) | æœ¬æ–‡æå‡ºäº†ä¸€ç§é€‚ç”¨äºçº¿æ€§å›å½’æƒ…å†µçš„è¿ç§»å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿå°†æ–°æ•°æ®ä¸å†å²æ•°æ®ç›¸ç»“åˆï¼Œç‰¹åˆ«åœ¨æ–°æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å…·æœ‰ç›Šå¤„ï¼Œå¹¶ä¸”åœ¨å®éªŒéªŒè¯ä¸­è¡¨ç°å‡ºå¯¹è´Ÿè¿ç§»å­¦ä¹ çš„é²æ£’æ€§ã€‚ |
| [^36] | [Interpretable Sequence Classification Via Prototype Trajectory.](http://arxiv.org/abs/2007.01777) | ProtoryNetæ˜¯ä¸€ç§åŸºäºåŸå‹è½¨è¿¹çš„å¯è§£é‡Šæ·±åº¦ç¥ç»ç½‘ç»œï¼Œå®ƒé€šè¿‡æ•æ‰æ—¶é—´æ¨¡å¼å’ŒåŸå‹çš„è¿‘ä¼¼ç¨‹åº¦æ¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼Œå¹¶å®ç°äº†ç›´è§‚å’Œç»†è‡´çš„æ¨ç†è¿‡ç¨‹è§£é‡Šã€‚ |

# è¯¦ç»†

[^1]: é«˜æ•ˆrobust Bayesian Optimizationå¯¹äºä»»æ„ä¸ç¡®å®šè¾“å…¥çš„åº”ç”¨

    Efficient Robust Bayesian Optimization for Arbitrary Uncertain inputs. (arXiv:2310.20145v1 [cs.LG])

    [http://arxiv.org/abs/2310.20145](http://arxiv.org/abs/2310.20145)

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„robust Bayesian Optimizationç®—æ³•ï¼ŒAIRBOï¼Œå®ƒèƒ½å¤Ÿåœ¨ä»»æ„è¾“å…¥ä¸ç¡®å®šæ€§ä¸‹æœ‰æ•ˆè¯†åˆ«å‡ºè¡¨ç°ä¸€è‡´è‰¯å¥½çš„é²æ£’æœ€ä¼˜è§£ã€‚

    

    Bayesian Optimization (BO) æ˜¯ä¸€ç§å¹¿æ³›åº”ç”¨äºå„ç§åº”ç”¨ä¸­çš„é«˜æ•ˆä¼˜åŒ–ç®—æ³•ã€‚åœ¨ä¸€äº›å…·æœ‰æŒ‘æˆ˜æ€§çš„BOä»»åŠ¡ä¸­ï¼Œç”±äºä¼˜åŒ–è¿‡ç¨‹ä¸­çš„ä¸å¯é¿å…çš„éšæœºæ€§ï¼Œå¦‚åŠ å·¥è¯¯å·®ã€æ‰§è¡Œå™ªå£°æˆ–ä¸Šä¸‹æ–‡å˜å¼‚ï¼Œè¾“å…¥ä¸ç¡®å®šæ€§ä¼šå‡ºç°ã€‚è¿™ç§ä¸ç¡®å®šæ€§ä¼šä½¿è¾“å…¥åœ¨è¯„ä¼°ä¹‹å‰åç¦»é¢„æœŸå€¼ï¼Œå¯¼è‡´æœ€ç»ˆç»“æœçš„æ€§èƒ½æ³¢åŠ¨è¾ƒå¤§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„robust Bayesian Optimizationç®—æ³•ï¼ŒAIRBOï¼Œå®ƒèƒ½æœ‰æ•ˆåœ°è¯†åˆ«åœ¨ä»»æ„è¾“å…¥ä¸ç¡®å®šæ€§ä¸‹è¡¨ç°ä¸€è‡´è‰¯å¥½çš„é²æ£’æœ€ä¼˜è§£ã€‚æˆ‘ä»¬çš„æ–¹æ³•é€šè¿‡ä½¿ç”¨æœ€å¤§å‡å€¼å·®(MMD)èµ‹èƒ½é«˜æ–¯è¿‡ç¨‹ï¼Œç›´æ¥å»ºæ¨¡ä»»æ„åˆ†å¸ƒçš„ä¸ç¡®å®šè¾“å…¥ï¼Œå¹¶é€šè¿‡Nystromé€¼è¿‘åŠ é€ŸåéªŒæ¨æ–­ã€‚æˆ‘ä»¬åœ¨MMDä¼°è®¡è¯¯å·®ä¸‹å»ºç«‹äº†ä¸¥æ ¼çš„ç†è®ºé—æ†¾ç•Œï¼Œå¹¶åœ¨åˆæˆå‡½æ•°ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒã€‚

    Bayesian Optimization (BO) is a sample-efficient optimization algorithm widely employed across various applications. In some challenging BO tasks, input uncertainty arises due to the inevitable randomness in the optimization process, such as machining errors, execution noise, or contextual variability. This uncertainty deviates the input from the intended value before evaluation, resulting in significant performance fluctuations in the final result. In this paper, we introduce a novel robust Bayesian Optimization algorithm, AIRBO, which can effectively identify a robust optimum that performs consistently well under arbitrary input uncertainty. Our method directly models the uncertain inputs of arbitrary distributions by empowering the Gaussian Process with the Maximum Mean Discrepancy (MMD) and further accelerates the posterior inference via Nystrom approximation. Rigorous theoretical regret bound is established under MMD estimation error and extensive experiments on synthetic function
    
[^2]: å»¶è¿Ÿåé¦ˆçš„çº¿æ€§å‡½æ•°é€¼è¿‘å¼ºåŒ–å­¦ä¹ ä¸­çš„åéªŒé‡‡æ ·

    Posterior Sampling with Delayed Feedback for Reinforcement Learning with Linear Function Approximation. (arXiv:2310.18919v1 [cs.LG])

    [http://arxiv.org/abs/2310.18919](http://arxiv.org/abs/2310.18919)

    æœ¬ç ”ç©¶è§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸­å»¶è¿Ÿåé¦ˆå¯¹çº¿æ€§å‡½æ•°é€¼è¿‘çš„æŒ‘æˆ˜ï¼Œé€šè¿‡åéªŒé‡‡æ ·ç®—æ³•å®ç°äº†åœ¨ä¸åŒæƒ…å†µä¸‹çš„ä¼˜è¶Šæ€§èƒ½ã€‚

    

    è¿ç”¨å‡½æ•°é€¼è¿‘åœ¨å¼ºåŒ–å­¦ä¹ ä¸­å–å¾—äº†æ˜¾è‘—è¿›å±•ï¼Œä½†ç°æœ‰çš„é«˜æ•ˆç®—æ³•é€šå¸¸ä¾èµ–äºå³æ—¶åé¦ˆã€‚æœ¬æ–‡é€šè¿‡é‡‡ç”¨åéªŒé‡‡æ ·æ¥è§£å†³å»¶è¿Ÿåé¦ˆå¯¹å¼ºåŒ–å­¦ä¹ ä¸­çº¿æ€§å‡½æ•°é€¼è¿‘çš„æŒ‘æˆ˜ï¼Œé¦–å…ˆä»‹ç»äº†Delayed-PSVIç®—æ³•ï¼Œé€šè¿‡åéªŒé‡‡æ ·ä¸­çš„å™ªå£°æ‰°åŠ¨æœ‰æ•ˆåœ°æ¢ç´¢ä»·å€¼å‡½æ•°ç©ºé—´ã€‚æˆ‘ä»¬æä¾›äº†å»¶è¿Ÿåé¦ˆå¼ºåŒ–å­¦ä¹ ä¸­åéªŒé‡‡æ ·ç®—æ³•çš„é¦–æ¬¡åˆ†æï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬çš„ç®—æ³•åœ¨ä¸€ç³»åˆ—æƒ…å†µä¸‹çš„ä¼˜è¶Šæ€§ã€‚

    Recent studies in reinforcement learning (RL) have made significant progress by leveraging function approximation to alleviate the sample complexity hurdle for better performance. Despite the success, existing provably efficient algorithms typically rely on the accessibility of immediate feedback upon taking actions. The failure to account for the impact of delay in observations can significantly degrade the performance of real-world systems due to the regret blow-up. In this work, we tackle the challenge of delayed feedback in RL with linear function approximation by employing posterior sampling, which has been shown to empirically outperform the popular UCB algorithms in a wide range of regimes. We first introduce Delayed-PSVI, an optimistic value-based algorithm that effectively explores the value function space via noise perturbation with posterior sampling. We provide the first analysis for posterior sampling algorithms with delayed feedback in RL and show our algorithm achieves $
    
[^3]: å¯¹AIåˆ†ç±»å™¨çš„å¯¹æŠ—é²æ£’æ€§åº¦é‡çš„å­˜åœ¨æ€§ï¼Œå”¯ä¸€æ€§å’Œå¯æ‰©å±•æ€§ç ”ç©¶

    On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v1 [stat.ML])

    [http://arxiv.org/abs/2310.14421](http://arxiv.org/abs/2310.14421)

    æœ¬æ–‡ç ”ç©¶äº†é’ˆå¯¹AIåˆ†ç±»å™¨çš„å¯¹æŠ—é²æ£’æ€§åº¦é‡çš„å­˜åœ¨æ€§ã€å”¯ä¸€æ€§å’Œå¯æ‰©å±•æ€§ï¼Œæå‡ºäº†å¯ä»¥éªŒè¯çš„æ•°å­¦æ¡ä»¶ï¼Œå¹¶åœ¨åˆæˆåŸºå‡†æµ‹è¯•å’Œç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­è¿›è¡Œäº†å®é™…è®¡ç®—å’Œè§£é‡Šã€‚

    

    æœ¬æ–‡æå‡ºå¹¶è¯æ˜äº†é’ˆå¯¹ï¼ˆå±€éƒ¨ï¼‰å”¯ä¸€å¯é€†åˆ†ç±»å™¨ã€å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼ˆGLMï¼‰å’Œç†µAIï¼ˆEAIï¼‰å…·æœ‰æœ€å°å¯¹æŠ—è·¯å¾„ï¼ˆMAPï¼‰å’Œæœ€å°å¯¹æŠ—è·ç¦»ï¼ˆMADï¼‰çš„å­˜åœ¨æ€§ã€å”¯ä¸€æ€§å’Œæ˜ç¡®çš„åˆ†æè®¡ç®—çš„ç®€å•å¯éªŒè¯çš„æ•°å­¦æ¡ä»¶ã€‚åœ¨å¸¸è§çš„åˆæˆåŸºå‡†æµ‹è¯•æ•°æ®é›†ä¸Šï¼Œé’ˆå¯¹ç¥ç»ç½‘ç»œã€æå‡éšæœºæ£®æ—ã€GLMå’ŒEAIç­‰å„ç±»AIå·¥å…·è¿›è¡ŒMAPå’ŒMADçš„å®é™…è®¡ç®—ã€æ¯”è¾ƒå’Œè§£é‡Šï¼ŒåŒ…æ‹¬åŒå·çŠ¶èºæ—‹çº¿åŠå…¶æ‰©å±•ä»¥åŠä¸¤ä¸ªç”Ÿç‰©åŒ»å­¦æ•°æ®é—®é¢˜ï¼ˆç”¨äºå¥åº·ä¿é™©ç†èµ”é¢„æµ‹å’Œå¿ƒè„ç—…å‘ä½œè‡´æ­»ç‡åˆ†ç±»ï¼‰ã€‚åœ¨ç”Ÿç‰©åŒ»å­¦åº”ç”¨ä¸­ï¼Œå±•ç¤ºäº†MAPå¦‚ä½•åœ¨é¢„å®šä¹‰çš„å¯è®¿é—®æ§åˆ¶å˜é‡å­é›†ä¸­æä¾›å”¯ä¸€çš„æœ€å°æ‚£è€…ç‰¹å®šé£é™©ç¼“è§£å¹²é¢„æªæ–½ã€‚

    Simply-verifiable mathematical conditions for existence, uniqueness and explicit analytical computation of minimal adversarial paths (MAP) and minimal adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for generalized linear models (GLM), and for entropic AI (EAI) are formulated and proven. Practical computation of MAP and MAD, their comparison and interpretations for various classes of AI tools (for neuronal networks, boosted random forests, GLM and EAI) are demonstrated on the common synthetic benchmarks: on a double Swiss roll spiral and its extensions, as well as on the two biomedical data problems (for the health insurance claim predictions, and for the heart attack lethality classification). On biomedical applications it is demonstrated how MAP provides unique minimal patient-specific risk-mitigating interventions in the predefined subsets of accessible control variables.
    
[^4]: å‡ ä¹ç­‰å˜æ€§é€šè¿‡æä»£æ•°å·ç§¯

    Almost Equivariance via Lie Algebra Convolutions. (arXiv:2310.13164v1 [cs.LG])

    [http://arxiv.org/abs/2310.13164](http://arxiv.org/abs/2310.13164)

    æœ¬æ–‡ç ”ç©¶äº†å‡ ä¹ç­‰å˜æ€§çš„ä¸»é¢˜ï¼Œå¹¶æä¾›äº†ä¸€ä¸ªä¸åŒäºç°æœ‰å®šä¹‰çš„å‡ ä¹ç­‰å˜æ€§å®šä¹‰ï¼Œå¹¶é€šè¿‡åˆ©ç”¨æç¾¤çš„æä»£æ•°ç»™å‡ºäº†åœ¨æ¨¡å‹ä¸­ç¼–ç å‡ ä¹ç­‰å˜æ€§çš„å®ç”¨æ–¹æ³•ã€‚

    

    æœ€è¿‘ï¼Œåœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæ¨¡å‹ç›¸å¯¹äºç¾¤ä½œç”¨çš„ç­‰å˜æ€§å·²æˆä¸ºä¸€ä¸ªé‡è¦çš„ç ”ç©¶è¯¾é¢˜ã€‚ç„¶è€Œï¼Œèµ‹äºˆä¸€ä¸ªæ¶æ„å…·ä½“çš„ç¾¤ç­‰å˜æ€§å¯¹æ¨¡å‹æ‰€æœŸæœ›çœ‹åˆ°çš„æ•°æ®å˜æ¢ç±»å‹æ–½åŠ äº†å¼ºå¤§çš„å…ˆéªŒã€‚ä¸¥æ ¼ç­‰å˜æ¨¡å‹å¼ºåˆ¶æ‰§è¡Œå¯¹ç§°æ€§ï¼Œä½†çœŸå®ä¸–ç•Œçš„æ•°æ®å¹¶ä¸æ€»æ˜¯ç¬¦åˆè¿™æ ·çš„ä¸¥æ ¼ç­‰å˜æ€§ï¼Œå¯èƒ½æ˜¯å› ä¸ºæ•°æ®ä¸­çš„å™ªå£°æˆ–ä»…ç¼–ç äº†è¿‘ä¼¼æˆ–éƒ¨åˆ†å¯¹ç§°æ€§çš„æ½œåœ¨ç‰©ç†å®šå¾‹ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸¥æ ¼ç­‰å˜æ€§çš„å…ˆéªŒå®é™…ä¸Šå¯èƒ½è¿‡äºå¼ºå¤§ï¼Œå¯¼è‡´æ¨¡å‹åœ¨çœŸå®æ•°æ®ä¸Šè¡¨ç°ä¸ä½³ã€‚å› æ­¤ï¼Œåœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªç›¸å…³çš„ä¸»é¢˜ï¼Œå³å‡ ä¹ç­‰å˜æ€§ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªä¸å½“å‰æ–‡çŒ®ä¸­ç°æœ‰å®šä¹‰ä¸åŒçš„å‡ ä¹ç­‰å˜æ€§å®šä¹‰ï¼Œå¹¶é€šè¿‡åˆ©ç”¨æç¾¤çš„æä»£æ•°ç»™å‡ºäº†åœ¨æ¨¡å‹ä¸­ç¼–ç å‡ ä¹ç­‰å˜æ€§çš„å®ç”¨æ–¹æ³•ã€‚

    Recently, the equivariance of models with respect to a group action has become an important topic of research in machine learning. However, imbuing an architecture with a specific group equivariance imposes a strong prior on the types of data transformations that the model expects to see. While strictly-equivariant models enforce symmetries, real-world data does not always conform to such strict equivariances, be it due to noise in the data or underlying physical laws that encode only approximate or partial symmetries. In such cases, the prior of strict equivariance can actually prove too strong and cause models to underperform on real-world data. Therefore, in this work we study a closely related topic, that of almost equivariance. We provide a definition of almost equivariance that differs from those extant in the current literature and give a practical method for encoding almost equivariance in models by appealing to the Lie algebra of a Lie group. Specifically, we define Lie algebr
    
[^5]: åœ¨æ¦‚ç‡æµ‹åº¦ç©ºé—´ä¸­é€šè¿‡æ¢¯åº¦æµè¿›è¡ŒæŠ½æ ·

    Sampling via Gradient Flows in the Space of Probability Measures. (arXiv:2310.03597v1 [stat.ML])

    [http://arxiv.org/abs/2310.03597](http://arxiv.org/abs/2310.03597)

    é€šè¿‡æ¢¯åº¦æµæŠ½æ ·æ–¹æ³•çš„ç ”ç©¶æ–¹å‘åœ¨è®¡ç®—ç§‘å­¦å’Œå·¥ç¨‹ä¸­å…·æœ‰é‡è¦æ„ä¹‰ã€‚æœ¬æ–‡é€šè¿‡ç ”ç©¶æ¦‚ç‡æµ‹åº¦ç©ºé—´ä¸­çš„æ¢¯åº¦æµçš„è®¾è®¡ç»„æˆéƒ¨åˆ†ï¼Œæå‡ºäº†ä¸‰ä¸ªè´¡çŒ®ï¼šKullback-Leibleræ•£åº¦ä½œä¸ºèƒ½é‡æ³›å‡½çš„ç‹¬ç‰¹å±æ€§ã€åº¦é‡çš„é€‰æ‹©ä¸ä¸å˜æ€§çš„å…³ç³»ã€‚

    

    åœ¨è®¡ç®—ç§‘å­¦å’Œå·¥ç¨‹ä¸­ï¼Œä½¿ç”¨æœªçŸ¥å½’ä¸€åŒ–å¸¸æ•°çš„ç›®æ ‡æ¦‚ç‡åˆ†å¸ƒè¿›è¡ŒæŠ½æ ·æ˜¯ä¸€é¡¹åŸºæœ¬çš„æŒ‘æˆ˜ã€‚æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œé€šè¿‡è€ƒè™‘æ¦‚ç‡æµ‹åº¦ç©ºé—´ä¸­çš„æ¢¯åº¦æµæ´¾ç”Ÿçš„ç®—æ³•ä¸ºç®—æ³•å¼€å‘å¼€è¾Ÿäº†æ–°çš„é€”å¾„ã€‚æœ¬æ–‡é€šè¿‡å®¡æŸ¥è¿™ç§æ¢¯åº¦æµçš„è®¾è®¡ç»„æˆéƒ¨åˆ†ï¼Œå¯¹è¿™ç§æŠ½æ ·æ–¹æ³•åšå‡ºäº†ä¸‰ä¸ªè´¡çŒ®ã€‚æŠ½æ ·çš„ä»»ä½•å®ä¾‹åŒ–éƒ½éœ€è¦ä¸€ä¸ªèƒ½é‡æ³›å‡½å’Œä¸€ä¸ªåº¦é‡æ¥ç¡®å®šæµåŠ¨ï¼Œä»¥åŠæµåŠ¨çš„æ•°å€¼è¿‘ä¼¼æ¥æ¨å¯¼ç®—æ³•ã€‚æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªè´¡çŒ®æ˜¯å±•ç¤ºäº†Kullback-Leibleræ•£åº¦ä½œä¸ºä¸€ä¸ªèƒ½é‡æ³›å‡½å…·æœ‰å”¯ä¸€çš„ç‰¹å¾ï¼ˆåœ¨æ‰€æœ‰f-æ•£åº¦ä¸­ï¼‰ï¼Œå³ç”±å…¶å¾—åˆ°çš„æ¢¯åº¦æµä¸ä¾èµ–äºç›®æ ‡åˆ†å¸ƒçš„å½’ä¸€åŒ–å¸¸æ•°ã€‚æˆ‘ä»¬çš„ç¬¬äºŒä¸ªè´¡çŒ®æ˜¯ä»ä¸å˜æ€§çš„è§’åº¦ç ”ç©¶åº¦é‡çš„é€‰æ‹©ã€‚Fisher-Raoåº¦é‡è¢«ç§°ä¸ºt

    Sampling a target probability distribution with an unknown normalization constant is a fundamental challenge in computational science and engineering. Recent work shows that algorithms derived by considering gradient flows in the space of probability measures open up new avenues for algorithm development. This paper makes three contributions to this sampling approach by scrutinizing the design components of such gradient flows. Any instantiation of a gradient flow for sampling needs an energy functional and a metric to determine the flow, as well as numerical approximations of the flow to derive algorithms. Our first contribution is to show that the Kullback-Leibler divergence, as an energy functional, has the unique property (among all f-divergences) that gradient flows resulting from it do not depend on the normalization constant of the target distribution. Our second contribution is to study the choice of metric from the perspective of invariance. The Fisher-Rao metric is known as t
    
[^6]: é€šè¿‡ç¥ç»åéªŒä¸»æˆåˆ†è¿›è¡Œä¸ç¡®å®šæ€§é‡åŒ–

    Uncertainty Quantification via Neural Posterior Principal Components. (arXiv:2309.15533v1 [cs.CV])

    [http://arxiv.org/abs/2309.15533](http://arxiv.org/abs/2309.15533)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ç¥ç»ç½‘ç»œåœ¨å•æ¬¡å‰å‘ä¼ é€’ä¸­é¢„æµ‹ä»»æ„è¾“å…¥å›¾åƒåéªŒåˆ†å¸ƒçš„ä¸»æˆåˆ†çš„æ–¹æ³•ï¼Œä»¥å®ç°ä¸ç¡®å®šæ€§é‡åŒ–ã€‚

    

    ä¸ç¡®å®šæ€§é‡åŒ–å¯¹äºåœ¨è‡ªåŠ¨é©¾é©¶å’Œç”Ÿç‰©æˆåƒç­‰å®‰å…¨å…³é”®é¢†åŸŸä¸­éƒ¨ç½²å›¾åƒæ¢å¤æ¨¡å‹è‡³å…³é‡è¦ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œå…³äºä¸ç¡®å®šæ€§å¯è§†åŒ–çš„æ–¹æ³•ä¸»è¦é›†ä¸­åœ¨æ¯åƒç´ ä¼°è®¡ä¸Šã€‚ç„¶è€Œï¼Œæ¯åƒç´ æ–¹å·®çš„çƒ­å›¾é€šå¸¸åœ¨å®é™…ä¸­ç”¨é€”æœ‰é™ï¼Œå› ä¸ºå®ƒæ— æ³•æ•æ‰åƒç´ ä¹‹é—´çš„å¼ºç›¸å…³æ€§ã€‚æ›´è‡ªç„¶çš„ä¸ç¡®å®šæ€§åº¦é‡å¯¹åº”äºåéªŒåˆ†å¸ƒçš„ä¸»æˆåˆ†ï¼ˆPCsï¼‰ä¸Šçš„æ–¹å·®ã€‚ç†è®ºä¸Šï¼Œå¯ä»¥é€šè¿‡å¯¹è¾“å…¥å›¾åƒçš„æ¡ä»¶ç”Ÿæˆæ¨¡å‹ç”Ÿæˆçš„æ ·æœ¬åº”ç”¨PCAæ¥è®¡ç®—PCsã€‚ç„¶è€Œï¼Œè¿™éœ€è¦åœ¨æµ‹è¯•æ—¶ç”Ÿæˆå¤§é‡çš„æ ·æœ¬ï¼Œè€Œåœ¨ç›®å‰çš„æœ€å…ˆè¿›ï¼ˆæ‰©æ•£ï¼‰æ¨¡å‹ä¸‹éå¸¸ç¼“æ…¢ã€‚åœ¨è¯¥å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•æ¥åœ¨ç¥ç»ç½‘ç»œçš„å•æ¬¡å‰å‘ä¼ é€’ä¸­é¢„æµ‹åéªŒåˆ†å¸ƒçš„PCsï¼Œé€‚ç”¨äºä»»æ„è¾“å…¥å›¾åƒã€‚

    Uncertainty quantification is crucial for the deployment of image restoration models in safety-critical domains, like autonomous driving and biological imaging. To date, methods for uncertainty visualization have mainly focused on per-pixel estimates. However, a heatmap of per-pixel variances is typically of little practical use, as it does not capture the strong correlations between pixels. A more natural measure of uncertainty corresponds to the variances along the principal components (PCs) of the posterior distribution. Theoretically, the PCs can be computed by applying PCA on samples generated from a conditional generative model for the input image. However, this requires generating a very large number of samples at test time, which is painfully slow with the current state-of-the-art (diffusion) models. In this work, we present a method for predicting the PCs of the posterior distribution for any input image, in a single forward pass of a neural network. Our method can either wrap
    
[^7]: PolyLUT: ç”¨äºè¶…ä½å»¶è¿ŸFPGAåŸºäºæŸ¥æ‰¾è¡¨æ¨ç†çš„åˆ†æ®µå¤šé¡¹å¼å­¦ä¹ 

    PolyLUT: Learning Piecewise Polynomials for Ultra-Low Latency FPGA LUT-based Inference. (arXiv:2309.02334v1 [cs.LG])

    [http://arxiv.org/abs/2309.02334](http://arxiv.org/abs/2309.02334)

    æå‡ºäº†ä¸€ç§åä¸ºPolyLUTçš„æ–°æ–¹æ³•ï¼Œç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œåœ¨FPGAä¸Šè¿›è¡Œéƒ¨ç½²ã€‚è¯¥æ–¹æ³•åˆ©ç”¨å¤šå˜é‡å¤šé¡¹å¼ä½œä¸ºåŸºæœ¬æ¨¡å—ï¼Œå¹¶åˆ©ç”¨è½¯é€»è¾‘å°†å¤šé¡¹å¼è¯„ä¼°éšè—åœ¨FPGAçš„æŸ¥æ‰¾è¡¨ä¸­ï¼Œä»è€Œå®ç°è¶…ä½å»¶è¿Ÿæ¨ç†ï¼Œå¹¶å‡å°‘äº†è½¯ä»¶é€»è¾‘çš„å±‚æ•°ã€‚

    

    å¯ç¼–ç¨‹é—¨é˜µåˆ—ï¼ˆFPGAï¼‰è¢«å¹¿æ³›ç”¨äºå®ç°æ·±åº¦å­¦ä¹ æ¨ç†ã€‚æ ‡å‡†çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¨ç†æ¶‰åŠäº¤é”™çº¿æ€§æ˜ å°„å’Œéçº¿æ€§æ¿€æ´»å‡½æ•°çš„è®¡ç®—ã€‚ä»¥å¾€çš„è¶…ä½å»¶è¿Ÿå®ç°å·¥ä½œåœ¨FPGAæŸ¥æ‰¾è¡¨ï¼ˆLUTï¼‰ä¸­ç¡¬ç¼–ç äº†çº¿æ€§æ˜ å°„å’Œéçº¿æ€§æ¿€æ´»çš„ç»„åˆã€‚æˆ‘ä»¬çš„å·¥ä½œå—åˆ°è¿™ä¸ªæƒ³æ³•çš„å¯å‘ï¼Œå³FPGAä¸­çš„LUTå¯ä»¥ç”¨æ¥å®ç°æ¯”è¿™æ›´å¤šæ ·åŒ–çš„å‡½æ•°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ–¹æ³•æ¥è®­ç»ƒç”¨äºFPGAéƒ¨ç½²çš„ç¥ç»ç½‘ç»œï¼Œä»¥å¤šå˜é‡å¤šé¡¹å¼ä½œä¸ºåŸºæœ¬æ¨¡å—ã€‚æˆ‘ä»¬çš„æ–¹æ³•åˆ©ç”¨è½¯ä»¶é€»è¾‘æä¾›çš„çµæ´»æ€§ï¼Œå°†å¤šé¡¹å¼è¯„ä¼°éšè—åœ¨LUTä¸­ä¸”æ²¡æœ‰ä»»ä½•å¼€é”€ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œé€šè¿‡ä½¿ç”¨å¤šé¡¹å¼æ¨¡å—ï¼Œæˆ‘ä»¬å¯ä»¥å®ç°ç›¸åŒçš„å‡†ç¡®åº¦ï¼Œè€Œä½¿ç”¨çš„è½¯ä»¶é€»è¾‘å±‚æ•°è¦æ¯”ä½¿ç”¨çº¿æ€§å‡½æ•°è¦å°‘å¾—å¤šï¼Œä»è€Œå¸¦æ¥æ˜¾è‘—çš„å»¶è¿Ÿå’Œé¢ç§¯çš„å‡å°‘ã€‚

    Field-programmable gate arrays (FPGAs) are widely used to implement deep learning inference. Standard deep neural network inference involves the computation of interleaved linear maps and nonlinear activation functions. Prior work for ultra-low latency implementations has hardcoded the combination of linear maps and nonlinear activations inside FPGA lookup tables (LUTs). Our work is motivated by the idea that the LUTs in an FPGA can be used to implement a much greater variety of functions than this. In this paper, we propose a novel approach to training neural networks for FPGA deployment using multivariate polynomials as the basic building block. Our method takes advantage of the flexibility offered by the soft logic, hiding the polynomial evaluation inside the LUTs with zero overhead. We show that by using polynomial building blocks, we can achieve the same accuracy using considerably fewer layers of soft logic than by using linear functions, leading to significant latency and area i
    
[^8]: ç¥ç»åŠ¨åŠ›å­¦çš„ä½é˜¶å¼ é‡ç§©å­¦ä¹ 

    Low Tensor Rank Learning of Neural Dynamics. (arXiv:2308.11567v1 [q-bio.NC])

    [http://arxiv.org/abs/2308.11567](http://arxiv.org/abs/2308.11567)

    ç ”ç©¶å‘ç°é€šè¿‡å­¦ä¹ è¿‡ç¨‹ä¸­çš„å¼ é‡ç§©æ¼”åŒ–æ¥ç†è§£ç¥ç»å…ƒè¿æ¥åœ¨å­¦ä¹ ä¸­çš„åè°ƒå˜åŒ–ã€‚ç ”ç©¶è¡¨æ˜è®­ç»ƒè¿‡çš„é€’å½’ç¥ç»ç½‘ç»œçš„æƒé‡çŸ©é˜µé€šå¸¸å…·æœ‰ä½ç§©ç»“æ„ï¼Œè€Œè¿™ç§ç»“æ„åœ¨æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸­ä¿æŒåœ¨ä¸€ä¸ªå›ºå®šçš„ä½ç»´å­ç©ºé—´ä¸­ã€‚å¯¹çœŸå®æƒé‡è¿›è¡Œä½ç§©åˆ†è§£éªŒè¯äº†è¿™ä¸€è§‚å¯Ÿç»“æœã€‚

    

    å­¦ä¹ ä¾èµ–äºç¥ç»å…ƒç¾¤ä½“ä¸­çš„åè°ƒçªè§¦å˜åŒ–ã€‚å› æ­¤ï¼Œäº†è§£å­¦ä¹ è¿‡ç¨‹ä¸­çªè§¦è¿æ¥çš„é›†ä½“æ¼”åŒ–æ˜¯ç¥ç»ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªå…³é”®æŒ‘æˆ˜ã€‚è¿‘æœŸçš„ç ”ç©¶è¡¨æ˜ï¼Œç»è¿‡è®­ç»ƒçš„é€’å½’ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰çš„æƒé‡çŸ©é˜µé€šå¸¸æ˜¯ä½ç§©çš„ï¼Œä½†æ˜¯è¿™ç§ä½ç§©ç»“æ„å¦‚ä½•åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­å±•å¼€è¿˜ä¸æ¸…æ¥šã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸­ç”±æƒé‡çŸ©é˜µå½¢æˆçš„3é˜¶å¼ é‡çš„ç§©ã€‚é€šè¿‡ç”¨ä¸åŒç§©çš„RNNæ‹Ÿåˆå¤§è§„æ¨¡ç¥ç»è®°å½•çš„è¿åŠ¨å­¦ä¹ ä»»åŠ¡ï¼Œæˆ‘ä»¬å‘ç°æ¨æ–­çš„æƒé‡æ˜¯ä½é˜¶å¼ é‡ç§©çš„ï¼Œå› æ­¤åœ¨æ•´ä¸ªå­¦ä¹ è¿‡ç¨‹ä¸­åœ¨ä¸€ä¸ªå›ºå®šçš„ä½ç»´å­ç©ºé—´ä¸­æ¼”åŒ–ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬é€šè¿‡åœ¨çœŸå®æƒé‡ä¸Šç›´æ¥è¿›è¡Œä½é˜¶å¼ é‡ç§©åˆ†è§£ï¼Œå¹¶å±•ç¤ºæˆ‘ä»¬æ‰€ä½¿ç”¨çš„æ–¹æ³•ï¼ŒéªŒè¯äº†ä½é˜¶å¼ é‡ç§©å­¦ä¹ çš„è§‚å¯Ÿç»“è®ºã€‚

    Learning relies on coordinated synaptic changes in recurrently connected populations of neurons. Therefore, understanding the collective evolution of synaptic connectivity over learning is a key challenge in neuroscience and machine learning. In particular, recent work has shown that the weight matrices of task-trained RNNs are typically low rank, but how this low rank structure unfolds over learning is unknown. To address this, we investigate the rank of the 3-tensor formed by the weight matrices throughout learning. By fitting RNNs of varying rank to large-scale neural recordings during a motor learning task, we find that the inferred weights are low-tensor-rank and therefore evolve over a fixed low-dimensional subspace throughout the entire course of learning. We next validate the observation of low-tensor-rank learning on an RNN trained to solve the same task by performing a low-tensor-rank decomposition directly on the ground truth weights, and by showing that the method we applie
    
[^9]: ä½¿ç”¨ä¼ æ„Ÿå™¨æ•°æ®ã€æ–¹ç¨‹å’Œè‡ªç„¶è¯­è¨€æç¤ºä¸Šä¸‹æ–‡ä¸­çš„è¿ç®—ç¬¦å­¦ä¹ 

    Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language. (arXiv:2308.05061v1 [cs.LG])

    [http://arxiv.org/abs/2308.05061](http://arxiv.org/abs/2308.05061)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨ä¼ æ„Ÿå™¨æ•°æ®ã€æ–¹ç¨‹å’Œè‡ªç„¶è¯­è¨€æç¤ºä¸Šä¸‹æ–‡ä¸­è¿ç®—ç¬¦å­¦ä¹ çš„æ–¹æ³•ã€‚é€šè¿‡æ•´åˆäººç±»çŸ¥è¯†å’Œè¯­è¨€æè¿°ï¼Œè¯¥æ–¹æ³•ä¸ä»…æ‰©å±•äº†ç‰©ç†ä¿¡æ¯å­¦ä¹ çš„çµæ´»æ€§å’Œæ™®é€‚æ€§ï¼Œè€Œä¸”æ˜¾è‘—æé«˜äº†å­¦ä¹ æ€§èƒ½å’Œå‡å°‘äº†æ•°æ®éœ€æ±‚ã€‚

    

    åœ¨ç§‘å­¦æœºå™¨å­¦ä¹ é¢†åŸŸä¸­ï¼Œä¸Šä¸‹æ–‡ä¸­çš„è¿ç®—ç¬¦å­¦ä¹ å·²ç»å±•ç¤ºå‡ºäº†åœ¨æ¨ç†é˜¶æ®µä»æç¤ºæ•°æ®ä¸­å­¦ä¹ è¿ç®—ç¬¦çš„æ˜¾è‘—æ½œåŠ›ï¼Œè€Œæ— éœ€è¿›è¡Œæƒé‡æ›´æ–°ã€‚ç„¶è€Œï¼Œå½“å‰æ¨¡å‹å¯¹ä¼ æ„Ÿå™¨æ•°æ®çš„è¿‡åº¦ä¾èµ–å¯èƒ½ä¼šæ— æ„ä¸­å¿½è§†è¿ç®—ç¬¦çš„å®è´µçš„äººç±»æ´å¯ŸåŠ›ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å°†ä¸Šä¸‹æ–‡ä¸­çš„è¿ç®—ç¬¦å­¦ä¹ è½¬åŒ–ä¸ºä¸€ç§å¤šæ¨¡å¼èŒƒå¼ã€‚æˆ‘ä»¬æå‡ºä½¿ç”¨â€œæ ‡é¢˜â€æ¥æ•´åˆé€šè¿‡è‡ªç„¶è¯­è¨€æè¿°å’Œæ–¹ç¨‹å¼è¡¨è¾¾çš„è¿ç®—ç¬¦çš„äººç±»çŸ¥è¯†ã€‚æˆ‘ä»¬æ¼”ç¤ºäº†è¿™ç§æ–¹æ³•ä¸ä»…æ‰©å±•äº†ç‰©ç†ä¿¡æ¯å­¦ä¹ çš„çµæ´»æ€§å’Œæ™®éæ€§ï¼Œè€Œä¸”è¿˜æ˜¾è‘—æé«˜äº†å­¦ä¹ æ€§èƒ½å¹¶å‡å°‘äº†æ•°æ®éœ€æ±‚ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ›´é«˜æ•ˆçš„å¤šæ¨¡å¼ä¸Šä¸‹æ–‡è¿ç®—ç¬¦å­¦ä¹ çš„ç¥ç»ç½‘ç»œæ¶æ„ï¼Œç§°ä¸ºâ€œICON-LMâ€ï¼ŒåŸºäºç±»ä¼¼äºè¯­è¨€æ¨¡å‹çš„æ¶æ„ã€‚

    In the growing domain of scientific machine learning, in-context operator learning has demonstrated notable potential in learning operators from prompted data during inference stage without weight updates. However, the current model's overdependence on sensor data, may inadvertently overlook the invaluable human insight into the operator. To address this, we present a transformation of in-context operator learning into a multi-modal paradigm. We propose the use of "captions" to integrate human knowledge about the operator, expressed through natural language descriptions and equations. We illustrate how this method not only broadens the flexibility and generality of physics-informed learning, but also significantly boosts learning performance and reduces data needs. Furthermore, we introduce a more efficient neural network architecture for multi-modal in-context operator learning, referred to as "ICON-LM", based on a language-model-like architecture. We demonstrate the viability of "ICO
    
[^10]: åœ¨é©¬å°”å¯å¤«é‡‡æ ·ä¸‹ï¼Œç”¨äºéšæœºæ¢¯åº¦ä¸‹é™çš„åœ¨çº¿åæ–¹å·®ä¼°è®¡

    Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])

    [http://arxiv.org/abs/2308.01481](http://arxiv.org/abs/2308.01481)

    æœ¬æ–‡ç ”ç©¶äº†åœ¨é©¬å°”å¯å¤«é‡‡æ ·ä¸‹çš„éšæœºæ¢¯åº¦ä¸‹é™ä¸­çš„åœ¨çº¿é‡å æ‰¹æ¬¡å‡å€¼åæ–¹å·®ä¼°è®¡å™¨ï¼Œå¹¶è¯æ˜äº†å…¶æ”¶æ•›é€Ÿç‡ä¸º$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$å’Œ$O\big(\sqrt{d}\,n^{-1/8}\big)$ï¼Œåˆ†åˆ«å¯¹åº”äºçŠ¶æ€ç›¸å…³å’ŒçŠ¶æ€æ— å…³çš„é©¬å°”å¯å¤«é‡‡æ ·ã€‚è¿™äº›é€Ÿç‡ä¸ç‹¬ç«‹åŒåˆ†å¸ƒæƒ…å†µä¸‹çš„æœ€ä½³æ”¶æ•›é€Ÿç‡ç›¸åŒ¹é…ï¼Œå¹¶ä¸”å…‹æœäº†ç”±äºé©¬å°”å¯å¤«é‡‡æ ·è€Œå¼•èµ·çš„æŒ‘æˆ˜ã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†ç”¨äºé©¬å°”å¯å¤«é‡‡æ ·ä¸‹éšæœºæ¢¯åº¦ä¸‹é™çš„åœ¨çº¿é‡å æ‰¹æ¬¡å‡å€¼åæ–¹å·®ä¼°è®¡å™¨ã€‚æˆ‘ä»¬è¯æ˜äº†åæ–¹å·®ä¼°è®¡å™¨çš„æ”¶æ•›é€Ÿç‡åˆ†åˆ«ä¸º$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$å’Œ$O\big(\sqrt{d}\,n^{-1/8}\big)$ï¼Œå…¶ä¸­$d$ä»£è¡¨ç»´åº¦ï¼Œ$n$è¡¨ç¤ºè§‚æµ‹æ•°é‡æˆ–SGDè¿­ä»£æ¬¡æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›é€Ÿç‡ä¸å…ˆå‰ç”±\cite{zhu2021online}åœ¨ç‹¬ç«‹åŒåˆ†å¸ƒ($\iid$)æƒ…å†µä¸‹å»ºç«‹çš„æœ€ä½³æ”¶æ•›é€Ÿç‡ç›¸åŒ¹é…ï¼Œé™¤äº†å¯¹æ•°å› å­ã€‚æˆ‘ä»¬çš„åˆ†æå…‹æœäº†ç”±äºé©¬å°”å¯å¤«é‡‡æ ·è€Œäº§ç”Ÿçš„é‡è¦æŒ‘æˆ˜ï¼Œå¼•å…¥äº†é¢å¤–çš„è¯¯å·®é¡¹å’Œæ‰¹æ¬¡å‡å€¼åæ–¹å·®ä¼°è®¡å™¨çš„å¤æ‚ä¾èµ–å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å»ºç«‹äº†SGDåŠ¨æ€è¯¯å·®$\ell_2$èŒƒæ•°çš„å‰å››é˜¶çŸ©çš„æ”¶æ•›é€Ÿç‡ã€‚

    We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and $O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\iid$) case by \cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\ell_2$ norm of the error of SGD dynamics u
    
[^11]: è¯­è¨€æ¨¡å‹ä¸­å¤æ‚æŠ€èƒ½äº§ç”Ÿçš„ç†è®º

    A Theory for Emergence of Complex Skills in Language Models. (arXiv:2307.15936v1 [cs.LG])

    [http://arxiv.org/abs/2307.15936](http://arxiv.org/abs/2307.15936)

    æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªç»Ÿè®¡æ¡†æ¶ï¼Œé€šè¿‡åˆ†æè¯­è¨€æ¨¡å‹çš„äº¤å‰ç†µæŸå¤±ä¸åŸºæœ¬è¯­è¨€ä»»åŠ¡çš„èƒ½åŠ›ä¹‹é—´çš„å…³ç³»ï¼Œæ­ç¤ºäº†è¯­è¨€æ¨¡å‹ä¸­å¤æ‚æŠ€èƒ½äº§ç”Ÿçš„æœºåˆ¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡æ‰©å±•å®šå¾‹ï¼Œé¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿé«˜æ•ˆå­¦ä¹ ï¼Œå¹¶è¡¨ç°å‡ºè¿åé€šå¸¸æ³›åŒ–ç†è®ºçš„èƒ½åŠ›ã€‚

    

    å½“è¯­è¨€æ¨¡å‹çš„å‚æ•°é›†åˆå’Œè®­ç»ƒè¯­æ–™åº“æ‰©å¤§æ—¶ï¼Œæ–°çš„æŠ€èƒ½å°†åœ¨ AI äº§å“ä¸­å‡ºç°çš„ä¸»è¦é©±åŠ¨å› ç´ ã€‚è¿™ç§ç°è±¡å°šä¸ä¸ºäººæ‰€ç†è§£ï¼Œå¹¶ä¸”é€šè¿‡å¯¹åŸºäºæ¢¯åº¦è®­ç»ƒçš„æ•°å­¦åˆ†ææä¾›æœºæ¢°è§£é‡Šä¼¼ä¹å¾ˆå›°éš¾ã€‚æœ¬æ–‡é‡‡ç”¨ä¸åŒçš„æ–¹æ³•ï¼Œä½¿ç”¨è‘—åçš„ï¼ˆå’Œç»éªŒæ€§çš„ï¼‰LLMæ‰©å±•å®šå¾‹å’Œç®€å•çš„ç»Ÿè®¡æ¡†æ¶æ¥åˆ†æå‡ºç°ã€‚è´¡çŒ®åŒ…æ‹¬ï¼šï¼ˆaï¼‰ä¸€ä¸ªç»Ÿè®¡æ¡†æ¶å°†LLMçš„äº¤å‰ç†µæŸå¤±ä¸è¯­è¨€ä»»åŠ¡åŸºæœ¬æŠ€èƒ½çš„èƒ½åŠ›ç›¸å…³è”ã€‚ï¼ˆbï¼‰æ•°å­¦åˆ†æè¡¨æ˜ï¼Œæ‰©å±•å®šå¾‹æ„å‘³ç€å¼ºçƒˆçš„å½’çº³åè§ï¼Œä½¿é¢„è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿå­¦ä¹ å¾—éå¸¸é«˜æ•ˆã€‚æˆ‘ä»¬éæ­£å¼åœ°ç§°ä¹‹ä¸ºâ€œå¼¹å¼“æ³›åŒ–â€ï¼Œå› ä¸ºè¡¨é¢ä¸Šçœ‹ï¼Œå®ƒä¼¼ä¹æä¾›äº†åœ¨æŠ€èƒ½æ°´å¹³ä¸Šè¿åé€šå¸¸æ³›åŒ–ç†è®ºçš„èƒ½åŠ›ã€‚ï¼ˆcï¼‰å¼¹å¼“æ³›åŒ–çš„ä¸€ä¸ªå…³é”®ä¾‹å­ï¼Œå³åœ¨æ‰§è¡Œä»»åŠ¡æ—¶çš„èƒ½åŠ›ã€‚

    A major driver of AI products today is the fact that new skills emerge in language models when their parameter set and training corpora are scaled up. This phenomenon is poorly understood, and a mechanistic explanation via mathematical analysis of gradient-based training seems difficult. The current paper takes a different approach, analysing emergence using the famous (and empirical) Scaling Laws of LLMs and a simple statistical framework. Contributions include: (a) A statistical framework that relates cross-entropy loss of LLMs to competence on the basic skills that underlie language tasks. (b) Mathematical analysis showing that the Scaling Laws imply a strong form of inductive bias that allows the pre-trained model to learn very efficiently. We informally call this {\em slingshot generalization} since naively viewed it appears to give competence levels at skills that violate usual generalization theory. (c) A key example of slingshot generalization, that competence at executing task
    
[^12]: è‡ªé€‚åº”çº¿æ€§ä¼°è®¡æ–¹ç¨‹

    Adaptive Linear Estimating Equations. (arXiv:2307.07320v1 [math.ST])

    [http://arxiv.org/abs/2307.07320](http://arxiv.org/abs/2307.07320)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§è§£å†³è‡ªé€‚åº”çº¿æ€§å›å½’æ¨¡å‹ä¸­éæ­£æ€æ¸è¿‘è¡Œä¸ºçš„æ–¹æ³•ï¼Œä½¿ç”¨è‡ªé€‚åº”çº¿æ€§ä¼°è®¡æ–¹ç¨‹æ„å»ºå»åä¼°è®¡é‡ï¼Œå¹¶åœ¨å¤šè‡‚è€è™æœºçš„èƒŒæ™¯ä¸‹ä¿æŒäº†æœ€å°äºŒä¹˜ä¼°è®¡é‡çš„éæ¸è¿‘æ€§èƒ½ã€‚

    

    é¡ºåºæ•°æ®æ”¶é›†å·²æˆä¸ºå¢å¼ºæ•°æ®æ”¶é›†è¿‡ç¨‹æ•ˆç‡çš„å¹¿æ³›é‡‡ç”¨çš„æŠ€æœ¯ã€‚å°½ç®¡å…·æœ‰ä¼˜åŠ¿ï¼Œä½†è¿™ç§æ•°æ®æ”¶é›†æœºåˆ¶å¸¸å¸¸ç»™ç»Ÿè®¡æ¨æ–­è¿‡ç¨‹å¼•å…¥å¤æ‚æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨è‡ªé€‚åº”çº¿æ€§å›å½’æ¨¡å‹ä¸­ï¼Œæ™®é€šæœ€å°äºŒä¹˜ï¼ˆOLSï¼‰ä¼°è®¡é‡å¯èƒ½è¡¨ç°å‡ºéæ­£æ€çš„æ¸è¿‘è¡Œä¸ºï¼Œä»è€Œå¯¹å‡†ç¡®çš„æ¨æ–­å’Œè§£é‡Šæå‡ºæŒ‘æˆ˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ„å»ºå»åä¼°è®¡é‡çš„é€šç”¨æ–¹æ³•ï¼Œè¯¥æ–¹æ³•é‡‡ç”¨è‡ªé€‚åº”çº¿æ€§ä¼°è®¡æ–¹ç¨‹çš„æ€æƒ³ï¼Œå¹¶åœ¨ç†è®ºä¸Šä¿è¯äº†æ¸è¿‘æ­£æ€æ€§ï¼Œå¹¶è®¨è®ºäº†å®ç°è¿‘ä¼¼æœ€ä¼˜æ¸è¿‘æ–¹å·®çš„é—®é¢˜ã€‚æˆ‘ä»¬çš„ä¼°è®¡é‡çš„ä¸€ä¸ªæ˜¾è‘—ç‰¹ç‚¹æ˜¯ï¼Œåœ¨å¤šè‡‚è€è™æœºçš„èƒŒæ™¯ä¸‹ï¼Œæˆ‘ä»¬çš„ä¼°è®¡é‡ä¿ç•™äº†æœ€å°äºŒä¹˜ä¼°è®¡é‡çš„éæ¸è¿‘æ€§èƒ½ï¼ŒåŒæ—¶è·å¾—äº†æ¸è¿‘æ­£æ€æ€§ã€‚å› æ­¤ï¼Œæœ¬å·¥ä½œè§£å†³äº†è‡ªé€‚åº”çº¿æ€§å›å½’æ¨¡å‹ä¸­éæ­£æ€æ¸è¿‘è¡Œä¸ºçš„é—®é¢˜ï¼Œå¹¶ä¸ºç»Ÿè®¡æ¨æ–­æä¾›äº†å¯é çš„æ–¹æ³•ã€‚

    Sequential data collection has emerged as a widely adopted technique for enhancing the efficiency of data gathering processes. Despite its advantages, such data collection mechanism often introduces complexities to the statistical inference procedure. For instance, the ordinary least squares (OLS) estimator in an adaptive linear regression model can exhibit non-normal asymptotic behavior, posing challenges for accurate inference and interpretation. In this paper, we propose a general method for constructing debiased estimator which remedies this issue. It makes use of the idea of adaptive linear estimating equations, and we establish theoretical guarantees of asymptotic normality, supplemented by discussions on achieving near-optimal asymptotic variance. A salient feature of our estimator is that in the context of multi-armed bandits, our estimator retains the non-asymptotic performance of the least square estimator while obtaining asymptotic normality property. Consequently, this work
    
[^13]: è¿è¾“ã€å˜åˆ†æ¨æ–­å’Œæ‰©æ•£ï¼šåº”ç”¨äºå›ç«æµå’Œè–›å®šè°”æ¡¥çš„è®ºæ–‡ç ”ç©¶

    Transport, Variational Inference and Diffusions: with Applications to Annealed Flows and Schr\"odinger Bridges. (arXiv:2307.01050v1 [stat.ML])

    [http://arxiv.org/abs/2307.01050](http://arxiv.org/abs/2307.01050)

    æœ¬æ–‡ç ”ç©¶äº†æœ€ä¼˜è¿è¾“å’Œå˜åˆ†æ¨æ–­ä¹‹é—´çš„è”ç³»ï¼Œå¹¶æå‡ºäº†ä¸€ç§åŸºäºè·¯å¾„ç©ºé—´æ•£åº¦çš„é‡‡æ ·å’Œç”Ÿæˆå»ºæ¨¡æ¡†æ¶ã€‚é€šè¿‡å¼€å‘æ–°é¢–çš„åŸºäºå¾—åˆ†çš„å›ç«æµæŠ€æœ¯å’Œæ­£åˆ™åŒ–çš„è¿­ä»£æ¯”ä¾‹æ‹Ÿåˆç›®æ ‡ï¼Œæœ¬æ–‡å±•ç¤ºäº†è¿™äº›æ–¹æ³•çš„æ½œåŠ›ã€‚

    

    æœ¬æ–‡æ¢è®¨äº†æœ€ä¼˜è¿è¾“ä¸å˜åˆ†æ¨æ–­ä¹‹é—´çš„è”ç³»ï¼Œé‡ç‚¹ç ”ç©¶äº†æ­£å‘å’Œåå‘éšæœºå¾®åˆ†æ–¹ç¨‹ä»¥åŠGirsanovå˜æ¢ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºäºè·¯å¾„ç©ºé—´æ•£åº¦çš„é‡‡æ ·å’Œç”Ÿæˆå»ºæ¨¡çš„åŸåˆ™æ€§å’Œç³»ç»Ÿæ€§æ¡†æ¶ã€‚æˆ‘ä»¬çš„å·¥ä½œæœ€ç»ˆå‘å±•å‡ºä¸€ä¸ªæ–°é¢–çš„åŸºäºå¾—åˆ†çš„å›ç«æµæŠ€æœ¯ï¼ˆä¸ç»Ÿè®¡ç‰©ç†ä¸­çš„Jarzynskiå’ŒCrooksæ’ç­‰å¼æœ‰å…³ï¼‰å’Œä¸€ä¸ªæ­£åˆ™åŒ–çš„è¿­ä»£æ¯”ä¾‹æ‹Ÿåˆï¼ˆIPFï¼‰å‹ç›®æ ‡ï¼Œä¸åŒäºæ ‡å‡†IPFçš„é¡ºåºæ€§ã€‚é€šè¿‡ä¸€ç³»åˆ—çš„ç”Ÿæˆå»ºæ¨¡ç¤ºä¾‹å’ŒåŸºäºåŒäº•çš„ç¨€æœ‰äº‹ä»¶ä»»åŠ¡ï¼Œæˆ‘ä»¬å±•ç¤ºäº†æ‰€ææ–¹æ³•çš„æ½œåŠ›ã€‚

    This paper explores the connections between optimal transport and variational inference, with a focus on forward and reverse time stochastic differential equations and Girsanov transformations.We present a principled and systematic framework for sampling and generative modelling centred around divergences on path space. Our work culminates in the development of a novel score-based annealed flow technique (with connections to Jarzynski and Crooks identities from statistical physics) and a regularised iterative proportional fitting (IPF)-type objective, departing from the sequential nature of standard IPF. Through a series of generative modelling examples and a double-well-based rare event task, we showcase the potential of the proposed methods.
    
[^14]: RLHFæ˜¯å¦æ¯”æ ‡å‡†RLæ›´å›°éš¾ï¼Ÿ

    Is RLHF More Difficult than Standard RL?. (arXiv:2306.14111v1 [cs.LG])

    [http://arxiv.org/abs/2306.14111](http://arxiv.org/abs/2306.14111)

    æœ¬æ–‡è¯æ˜äº†å¯¹äºå¹¿æ³›çš„åå¥½æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç°æœ‰çš„ç®—æ³•å’ŒæŠ€æœ¯ç›´æ¥è§£å†³åŸºäºåå¥½çš„RLé—®é¢˜ï¼Œè€Œå‡ ä¹ä¸éœ€è¦é¢å¤–çš„æˆæœ¬ã€‚

    

    ä»äººç±»åé¦ˆå­¦ä¹ çš„å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰æ˜¯ä»åå¥½ä¿¡å·å­¦ä¹ ï¼Œè€Œæ ‡å‡†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰åˆ™ç›´æ¥ä»å¥–åŠ±ä¿¡å·å­¦ä¹ ã€‚åå¥½ä¿¡å·å¯èƒ½åŒ…å«çš„ä¿¡æ¯æ¯”å¥–åŠ±ä¿¡å·å°‘ï¼Œè¿™ä½¿å¾—åŸºäºåå¥½çš„RLä¼¼ä¹æ›´åŠ å›°éš¾ã€‚æœ¬æ–‡ç†è®ºä¸Šè¯æ˜ï¼Œå¯¹äºå¹¿æ³›çš„åå¥½æ¨¡å‹ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ç°æœ‰çš„ç®—æ³•å’ŒæŠ€æœ¯ç›´æ¥è§£å†³åŸºäºåå¥½çš„RLé—®é¢˜ï¼Œè€Œå‡ ä¹ä¸éœ€è¦é¢å¤–çš„æˆæœ¬ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å°†é—®é¢˜åˆ†ä¸ºä¸¤ç±»ï¼šï¼ˆ1ï¼‰åŸºäºå¥–åŠ±æ¦‚ç‡æ¨¡å‹çš„åå¥½ï¼Œæ­¤æ—¶å¯ä»¥å°†é—®é¢˜ç®€åŒ–ä¸ºå®¹å¿å¥–åŠ±å°è¯¯å·®çš„é²æ£’å¥–åŠ±RLé—®é¢˜ï¼›ï¼ˆ2ï¼‰å¯¹äºä¸€èˆ¬çš„ä»»æ„åå¥½ä¸”ç›®æ ‡æ˜¯æ‰¾åˆ°von Neumannè·èƒœè€…çš„æƒ…å†µï¼Œæˆ‘ä»¬å°†é—®é¢˜ç®€åŒ–ä¸ºå¤šæ™ºèƒ½ä½“å¥–åŠ±RLé—®é¢˜ï¼Œè¯¥é—®é¢˜å¯ä»¥åœ¨ä¸€ç»„å—é™åˆ¶çš„ç­–ç•¥ä¸‹æ‰¾åˆ°é©¬å°”å¯å¤«åšå¼ˆçš„å› å­çº³ä»€å¹³è¡¡è§£ã€‚åä¸€ç§æƒ…å†µå¯ä»¥è¿›ä¸€æ­¥é™ä½æˆå¯¹å…³ç³»çš„MDPã€‚

    Reinforcement learning from Human Feedback (RLHF) learns from preference signals, while standard Reinforcement Learning (RL) directly learns from reward signals. Preferences arguably contain less information than rewards, which makes preference-based RL seemingly more difficult. This paper theoretically proves that, for a wide range of preference models, we can solve preference-based RL directly using existing algorithms and techniques for reward-based RL, with small or no extra costs. Specifically, (1) for preferences that are drawn from reward-based probabilistic models, we reduce the problem to robust reward-based RL that can tolerate small errors in rewards; (2) for general arbitrary preferences where the objective is to find the von Neumann winner, we reduce the problem to multiagent reward-based RL which finds Nash equilibria for factored Markov games under a restricted set of policies. The latter case can be further reduce to adversarial MDP when preferences only depend on the f
    
[^15]: é€šè¿‡å…³ç³»æ¡ä»¶ç¥ç»è¿‡ç¨‹å®ç°å®ç”¨çš„ç­‰å˜æ€§

    Practical Equivariances via Relational Conditional Neural Processes. (arXiv:2306.10915v1 [stat.ML])

    [http://arxiv.org/abs/2306.10915](http://arxiv.org/abs/2306.10915)

    æœ¬æ–‡æå‡ºçš„å…³ç³»æ¡ä»¶ç¥ç»è¿‡ç¨‹ï¼ˆRCNPsï¼‰æ˜¯ä¸€ç§æœ‰æ•ˆå°†ç­‰å˜æ€§çº³å…¥ä»»ä½•ç¥ç»è¿‡ç¨‹æ¨¡å‹çš„æ–¹æ³•ï¼Œå¹¶æ‰©å±•äº†ç­‰å˜ç¥ç»è¿‡ç¨‹çš„é€‚ç”¨æ€§å’Œå½±å“åŠ›åˆ°æ›´é«˜çš„ç»´åº¦ã€‚

    

    æ¡ä»¶ç¥ç»è¿‡ç¨‹ï¼ˆCNPsï¼‰æ˜¯ä¸€ç±»å…ƒå­¦ä¹ æ¨¡å‹ï¼Œä»¥å…¶ç»¼åˆè¿è¡Œæ—¶æ•ˆç‡å’Œå¯é çš„ä¸ç¡®å®šæ€§é‡åŒ–è€Œå—æ¬¢è¿ã€‚è®¸å¤šç›¸å…³çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ï¼Œä¾‹å¦‚æ—¶ç©ºå»ºæ¨¡ã€è´å¶æ–¯ä¼˜åŒ–å’Œè¿ç»­æ§åˆ¶ï¼ŒåŒ…å«ç­‰å˜æ€§ï¼Œä¾‹å¦‚å¯¹äºå¹³ç§»ï¼Œæ¨¡å‹å¯ä»¥åˆ©ç”¨æœ€å¤§çš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œå…ˆå‰è¯•å›¾åœ¨CNPsä¸­åŒ…å«ç­‰å˜æ€§åœ¨è¶…è¿‡ä¸¤ä¸ªè¾“å…¥ç»´åº¦ä¹‹å¤–çš„å°ºåº¦ä¸Šæ— æ³•æœ‰æ•ˆæ‰©å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†å…³ç³»æ¡ä»¶ç¥ç»è¿‡ç¨‹ï¼ˆRCNPsï¼‰ï¼Œè¿™æ˜¯ä¸€ç§æœ‰æ•ˆå°†ç­‰å˜æ€§çº³å…¥ä»»ä½•ç¥ç»è¿‡ç¨‹æ¨¡å‹çš„æ–¹æ³•ã€‚æˆ‘ä»¬æå‡ºçš„æ–¹æ³•æ‰©å±•äº†ç­‰å˜ç¥ç»è¿‡ç¨‹çš„é€‚ç”¨æ€§å’Œå½±å“åŠ›åˆ°æ›´é«˜çš„ç»´åº¦ã€‚æˆ‘ä»¬åœ¨è‡ªç„¶åŒ…å«ç­‰å˜æ€§ä»»åŠ¡çš„å¤§é‡ä»»åŠ¡ä¸Šç»éªŒè¯å®äº†RCNPsçš„ç«äº‰æ€§èƒ½ã€‚

    Conditional Neural Processes (CNPs) are a class of metalearning models popular for combining the runtime efficiency of amortized inference with reliable uncertainty quantification. Many relevant machine learning tasks, such as spatio-temporal modeling, Bayesian Optimization and continuous control, contain equivariances -- for example to translation -- which the model can exploit for maximal performance. However, prior attempts to include equivariances in CNPs do not scale effectively beyond two input dimensions. In this work, we propose Relational Conditional Neural Processes (RCNPs), an effective approach to incorporate equivariances into any neural process model. Our proposed method extends the applicability and impact of equivariant neural processes to higher dimensions. We empirically demonstrate the competitive performance of RCNPs on a large array of tasks naturally containing equivariances.
    
[^16]: å¾…å”®ï¼šåŸºäºçŠ¶æ€-åŠ¨ä½œè¡¨ç¤ºå­¦ä¹ çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ 

    For SALE: State-Action Representation Learning for Deep Reinforcement Learning. (arXiv:2306.02451v1 [cs.LG])

    [http://arxiv.org/abs/2306.02451](http://arxiv.org/abs/2306.02451)

    SALEæ˜¯ä¸€ç§åŸºäºçŠ¶æ€-åŠ¨ä½œè¡¨ç¤ºå­¦ä¹ çš„æ–°æ–¹æ³•ï¼Œå¯ä»¥æœ‰æ•ˆåœ°ä»ä½çº§çŠ¶æ€ä¸­å®ç°è¡¨ç¤ºå­¦ä¹ ï¼ŒTD7ç®—æ³•å¼•å…¥äº†è¯¥æ–¹æ³•å¹¶åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ã€‚

    

    åœ¨å¼ºåŒ–å­¦ä¹ é¢†åŸŸä¸­ï¼Œè¡¨ç¤ºå­¦ä¹ æ˜¯å¤„ç†å¤æ‚åŸºäºå›¾åƒä»»åŠ¡çš„æœ‰æ•ˆå·¥å…·ï¼Œä½†é€šå¸¸è¢«å¿½ç•¥äº†ä½çº§çŠ¶æ€ï¼ˆä¾‹å¦‚ç‰©ç†æ§åˆ¶é—®é¢˜ï¼‰çš„ç¯å¢ƒã€‚æœ¬æ–‡ä»‹ç»äº†ä¸€ç§åä¸ºSALEçš„æ–°æ–¹æ³•ï¼Œå®ƒå¯ä»¥å­¦ä¹ åµŒå…¥æ¥å»ºæ¨¡çŠ¶æ€å’ŒåŠ¨ä½œä¹‹é—´å¾®å¦™çš„ç›¸äº’ä½œç”¨ï¼Œä»ä½çº§çŠ¶æ€ä¸­å®ç°æœ‰æ•ˆçš„è¡¨ç¤ºå­¦ä¹ ã€‚æˆ‘ä»¬å¹¿æ³›ç ”ç©¶äº†è¿™äº›åµŒå…¥çš„è®¾è®¡ç©ºé—´ï¼Œå¹¶å¼ºè°ƒäº†é‡è¦çš„è®¾è®¡è€ƒè™‘å› ç´ ã€‚æˆ‘ä»¬å°†SALEå’ŒRLçš„æ£€æŸ¥ç‚¹è‡ªé€‚åº”æ–¹æ³•æ•´åˆåˆ°TD3ä¸­ï¼Œå½¢æˆTD7ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è¿ç»­æ§åˆ¶ä»»åŠ¡ä¸­çš„è¡¨ç°æ˜æ˜¾ä¼˜äºç°æœ‰ç®—æ³•ã€‚åœ¨OpenAI gymåŸºå‡†ä»»åŠ¡ä¸­ï¼ŒTD7åœ¨300kå’Œ5Mæ—¶é—´æ­¥éª¤ä¸‹çš„å¹³å‡æ€§èƒ½å¢ç›Šåˆ†åˆ«ä¸º276.7ï¼…å’Œ50.7ï¼…ï¼Œå¯ä»¥åœ¨åœ¨çº¿å’Œç¦»çº¿è®¾ç½®ä¸­ä½¿ç”¨ã€‚

    In the field of reinforcement learning (RL), representation learning is a proven tool for complex image-based tasks, but is often overlooked for environments with low-level states, such as physical control problems. This paper introduces SALE, a novel approach for learning embeddings that model the nuanced interaction between state and action, enabling effective representation learning from low-level states. We extensively study the design space of these embeddings and highlight important design considerations. We integrate SALE and an adaptation of checkpoints for RL into TD3 to form the TD7 algorithm, which significantly outperforms existing continuous control algorithms. On OpenAI gym benchmark tasks, TD7 has an average performance gain of 276.7% and 50.7% over TD3 at 300k and 5M time steps, respectively, and works in both the online and offline settings.
    
[^17]: DiffLoad:æ‰©æ•£æ¨¡å‹ä¸­çš„è´Ÿè·é¢„æµ‹ä¸ç¡®å®šæ€§é‡åŒ–

    DiffLoad: Uncertainty Quantification in Load Forecasting with Diffusion Model. (arXiv:2306.01001v1 [cs.LG])

    [http://arxiv.org/abs/2306.01001](http://arxiv.org/abs/2306.01001)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ‰©æ•£æ¨¡å‹ä¸­çš„è´Ÿè·é¢„æµ‹ä¸ç¡®å®šæ€§é‡åŒ–æ–¹æ³•ï¼Œé‡‡ç”¨Seq2Seqç½‘ç»œç»“æ„æ¥åˆ†ç¦»ä¸¤ç§ç±»å‹çš„ä¸ç¡®å®šæ€§å¹¶å¤„ç†å¼‚å¸¸æƒ…å†µï¼Œä¸ä»…ç€çœ¼äºé¢„æµ‹æ¡ä»¶æœŸæœ›å€¼ã€‚

    

    ç”µåŠ›è´Ÿè·é¢„æµ‹å¯¹ç”µåŠ›ç³»ç»Ÿçš„å†³ç­–åˆ¶å®šï¼Œå¦‚æœºç»„æŠ•å…¥å’Œèƒ½æºç®¡ç†ç­‰å…·æœ‰é‡è¦æ„ä¹‰ã€‚è¿‘å¹´æ¥ï¼Œå„ç§åŸºäºè‡ªç›‘ç£ç¥ç»ç½‘ç»œçš„æ–¹æ³•å·²ç»è¢«åº”ç”¨äºç”µåŠ›è´Ÿè·é¢„æµ‹ï¼Œä»¥æé«˜é¢„æµ‹å‡†ç¡®æ€§å’Œæ•æ‰ä¸ç¡®å®šæ€§ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°ç°æœ‰çš„æ–¹æ³•æ˜¯åŸºäºé«˜æ–¯ä¼¼ç„¶æ–¹æ³•çš„ï¼Œå®ƒæ—¨åœ¨åœ¨ç»™å®šçš„åå˜é‡ä¸‹å‡†ç¡®ä¼°è®¡åˆ†å¸ƒæœŸæœ›å€¼ã€‚è¿™ç§æ–¹æ³•å¾ˆéš¾é€‚åº”å­˜åœ¨åˆ†å¸ƒåç§»å’Œå¼‚å¸¸å€¼çš„æ—¶é—´æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºæ‰©æ•£çš„Seq2seqç»“æ„æ¥ä¼°è®¡æœ¬ä½“ä¸ç¡®å®šæ€§ï¼Œå¹¶ä½¿ç”¨é²æ£’çš„åŠ æ€§æŸ¯è¥¿åˆ†å¸ƒæ¥ä¼°è®¡ç‰©è±¡ä¸ç¡®å®šæ€§ã€‚æˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿåˆ†ç¦»ä¸¤ç§ç±»å‹çš„ä¸ç¡®å®šæ€§å¹¶å¤„ç†çªå˜æƒ…å†µï¼Œè€Œä¸æ˜¯å‡†ç¡®é¢„æµ‹æ¡ä»¶æœŸæœ›ã€‚

    Electrical load forecasting is of great significance for the decision makings in power systems, such as unit commitment and energy management. In recent years, various self-supervised neural network-based methods have been applied to electrical load forecasting to improve forecasting accuracy and capture uncertainties. However, most current methods are based on Gaussian likelihood methods, which aim to accurately estimate the distribution expectation under a given covariate. This kind of approach is difficult to adapt to situations where temporal data has a distribution shift and outliers. In this paper, we propose a diffusion-based Seq2seq structure to estimate epistemic uncertainty and use the robust additive Cauchy distribution to estimate aleatoric uncertainty. Rather than accurately forecasting conditional expectations, we demonstrate our method's ability in separating two types of uncertainties and dealing with the mutant scenarios.
    
[^18]: ï¼ˆæ­£åˆ™åŒ–ï¼‰Wassersteinåˆ†å¸ƒå¼å¼ºæœ€ä¼˜æ¨¡å‹çš„ç¡®åˆ‡æ³›åŒ–ä¿è¯

    Exact Generalization Guarantees for (Regularized) Wasserstein Distributionally Robust Models. (arXiv:2305.17076v1 [cs.LG])

    [http://arxiv.org/abs/2305.17076](http://arxiv.org/abs/2305.17076)

    æœ¬æ–‡è¡¨æ˜Wassersteinåˆ†å¸ƒå¼å¼ºé²æ£’ä¼°è®¡å™¨çš„æ³›åŒ–ä¿è¯é€‚ç”¨äºä¸€èˆ¬æ¨¡å‹ç±»åˆ«ï¼Œä¸å—ç»´æ•°ç¾éš¾æ‰€å›°æ‰°ï¼Œç”šè‡³å¯ä»¥æ¶µç›–æµ‹è¯•æ—¶çš„åˆ†å¸ƒå˜åŒ–ã€‚

    

    Wassersteinåˆ†å¸ƒå¼å¼ºé²æ£’ä¼°è®¡å™¨å·²ç»æˆä¸ºé¢å¯¹ä¸ç¡®å®šæ€§çš„é¢„æµ‹å’Œå†³ç­–çš„å¼ºå¤§æ¨¡å‹ã€‚è¿™äº›ä¼°è®¡å™¨æä¾›äº†æœ‰å¸å¼•åŠ›çš„æ³›åŒ–ä¿è¯ï¼šè®­ç»ƒåˆ†å¸ƒå¾—åˆ°çš„å¼ºé²æ£’ç›®æ ‡æ˜¯çœŸå®é£é™©çš„ä¸€ä¸ªç²¾ç¡®ä¸Šç•Œï¼Œå¹¶ä¸”é«˜æ¦‚ç‡æˆç«‹ã€‚ç„¶è€Œï¼Œç°æœ‰çš„ä¿è¯è¦ä¹ˆå—åˆ°ç»´æ•°ç¾éš¾çš„å›°æ‰°ï¼Œè¦ä¹ˆä»…é™äºç‰¹å®šçš„è®¾ç½®ï¼Œæˆ–è€…ä¼šå¯¼è‡´è™šå‡çš„é”™è¯¯æœ¯è¯­ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬è¡¨æ˜è¿™äº›æ³›åŒ–ä¿è¯å®é™…ä¸Šé€‚ç”¨äºä¸€èˆ¬çš„æ¨¡å‹ç±»åˆ«ï¼Œä¸å—ç»´æ•°ç¾éš¾æ‰€å›°æ‰°ï¼Œç”šè‡³å¯ä»¥æ¶µç›–æµ‹è¯•æ—¶çš„åˆ†å¸ƒå˜åŒ–ã€‚æˆ‘ä»¬è¿˜è¯æ˜ï¼Œè¿™äº›ç»“æœå¯ä»¥æ¨å¹¿åˆ°æ–°å¼•å…¥çš„Wassersteinåˆ†å¸ƒå¼å¼ºæœ€ä¼˜é—®é¢˜çš„æ­£åˆ™åŒ–ç‰ˆæœ¬ã€‚

    Wasserstein distributionally robust estimators have emerged as powerful models for prediction and decision-making under uncertainty. These estimators provide attractive generalization guarantees: the robust objective obtained from the training distribution is an exact upper bound on the true risk with high probability. However, existing guarantees either suffer from the curse of dimensionality, are restricted to specific settings, or lead to spurious error terms. In this paper, we show that these generalization guarantees actually hold on general classes of models, do not suffer from the curse of dimensionality, and can even cover distribution shifts at testing. We also prove that these results carry over to the newly-introduced regularized versions of Wasserstein distributionally robust problems.
    
[^19]: å¸¦æ‰°åŠ¨ç”Ÿæˆæ ‘çš„å¯å¾®èšç±»æ–¹æ³•

    Differentiable Clustering with Perturbed Spanning Forests. (arXiv:2305.16358v1 [cs.LG])

    [http://arxiv.org/abs/2305.16358](http://arxiv.org/abs/2305.16358)

    ä»‹ç»äº†ä¸€ç§åŸºäºæ‰°åŠ¨ç”Ÿæˆæ ‘çš„å¯å¾®èšç±»æ–¹æ³•ï¼Œä¾èµ–äºçº¿æ€§è§„åˆ’è§£çš„éšæœºæ‰°åŠ¨ï¼Œå…·æœ‰è‰¯å¥½çš„æ€§èƒ½ã€‚

    

    æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§åŸºäºæœ€å°æƒé‡ç”Ÿæˆæ ‘çš„å¯å¾®èšç±»æ–¹æ³•ï¼Œå®ƒæ˜¯ç”Ÿæˆæ ‘çš„ä¸€ç§å˜ä½“ï¼Œå…·æœ‰å¤šä¸ªè¿é€šåˆ†é‡ã€‚æˆ‘ä»¬çš„æ–¹æ³•ä¾èµ–äºçº¿æ€§è§„åˆ’è§£çš„éšæœºæ‰°åŠ¨ï¼Œä»¥å®ç°å¹³æ»‘å’Œé«˜æ•ˆçš„æ¢¯åº¦è®¡ç®—ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨ç«¯åˆ°ç«¯å¯è®­ç»ƒçš„æµæ°´çº¿ä¸­åŒ…å«èšç±»ã€‚æˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•å³ä½¿åœ¨å˜ˆæ‚çš„æ•°æ®é›†å’Œå…·æœ‰æŒ‘æˆ˜æ€§çš„å‡ ä½•ç¯å¢ƒä¸‹ä¹Ÿèƒ½è‰¯å¥½åœ°å·¥ä½œã€‚æˆ‘ä»¬è¿˜åˆ©ç”¨è¿™ç§æ–¹æ³•åˆ¶å®šäº†ä¸€ä¸ªç‰¹åˆ«çš„æŸå¤±ï¼Œä»¥æœ‰æ•ˆåœ°ä»éƒ¨åˆ†èšç±»æ•°æ®å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨å‡ ä¸ªç°å®ä¸–ç•Œçš„æ•°æ®é›†ä¸Šå±•ç¤ºäº†å®ƒåœ¨ç›‘ç£å’ŒåŠç›‘ç£ä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚

    We introduce a differentiable clustering method based on minimum-weight spanning forests, a variant of spanning trees with several connected components. Our method relies on stochastic perturbations of solutions of linear programs, for smoothing and efficient gradient computations. This allows us to include clustering in end-to-end trainable pipelines. We show that our method performs well even in difficult settings, such as datasets with high noise and challenging geometries. We also formulate an ad hoc loss to efficiently learn from partial clustering data using this operation. We demonstrate its performance on several real world datasets for supervised and semi-supervised tasks.
    
[^20]: ä¸€ç§ç”¨äºç¡®å®šå›ºå®šç½®ä¿¡åº¦å’Œä»¥ä¸Šçš„ $\varepsilon$-æœ€ä½³è‡‚è¾¨è¯†ç®—æ³•

    An $\varepsilon$-Best-Arm Identification Algorithm for Fixed-Confidence and Beyond. (arXiv:2305.16041v1 [stat.ML])

    [http://arxiv.org/abs/2305.16041](http://arxiv.org/abs/2305.16041)

    æå‡ºä¸€ç§æ–°é¢–çš„é‡‡æ ·è§„åˆ™EB-TC $\varepsilon$ï¼Œç”¨äºéšæœºèµŒåšæœºä¸­çš„$\varepsilon$-æœ€ä½³è‡‚çš„è¾¨è¯†ã€‚è¯¥è§„åˆ™å¯ç”¨äºç¡®å®šå›ºå®šç½®ä¿¡åº¦æˆ–å›ºå®šé¢„ç®—æ ‡è¯†ä¸”å…·å¤‡è‡ªé€‚åº”è°ƒæ•´å‹˜æ¢å‚æ•°çš„æ¸è¿‘æœ€ä¼˜æ€§ã€‚åœ¨ä»¿çœŸå®éªŒä¸­è¡¨ç°è‰¯å¥½ï¼Œé€‚ç”¨äºä¸åŒé—®é¢˜é¢†åŸŸã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„é‡‡æ ·è§„åˆ™EB-TC $\varepsilon$ï¼Œè¯¥è§„åˆ™ç”¨äºéšæœºèµŒåšæœºä¸­çš„$\varepsilon$-æœ€ä½³è‡‚çš„è¾¨è¯†ã€‚è¿™æ˜¯ç¬¬ä¸€ç§ç”¨äºè¿‘ä¼¼æœ€ä½³è‡‚è¾¨è¯†çš„Top Twoç®—æ³•åˆ†æå®ä¾‹ã€‚ EB-TC $\varepsilon$ æ˜¯ä¸€ç§â€œéšæ—¶å¯ç”¨â€çš„é‡‡æ ·è§„åˆ™ï¼Œå› æ­¤å¯ä»¥åœ¨æ²¡æœ‰é¢„ç®—çŸ¥è¯†çš„æƒ…å†µä¸‹ç”¨äºç¡®å®šå›ºå®šç½®ä¿¡åº¦æˆ–å›ºå®šé¢„ç®—æ ‡è¯†ï¼ˆæ— éœ€ä¿®æ”¹ï¼‰ã€‚æˆ‘ä»¬ä¸ºEB-TC $\varepsilon$ æä¾›äº†ä¸‰ç§ç†è®ºä¿è¯ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è¯æ˜å…¶åœ¨å›ºå®šç½®ä¿¡åº¦è®¾ç½®ä¸­é¢„æœŸçš„æ ·æœ¬å¤æ‚åº¦ä¸Šæœ‰ç•Œï¼Œç‰¹åˆ«æ˜¯åœ¨å…¶å‹˜æ¢å‚æ•°çš„è‡ªé€‚åº”è°ƒæ•´ä¸ç»„åˆçš„æƒ…å†µä¸‹å‘ˆç°å…¶æ¸è¿‘æœ€ä¼˜æ€§ã€‚æˆ‘ä»¬é€šè¿‡åœ¨ä»»ä½•æ—¶é—´å’Œå¯¹äºä»»ä½•è¯¯å·®å‚æ•°çš„æ¦‚ç‡ä¸Šç•Œæ¥è¡¥å……è¿™äº›å‘ç°ï¼Œè¿™è¿›ä¸€æ­¥äº§ç”Ÿå…¶ä»»ä½•æ—¶é—´çš„ç®€å•é—æ†¾ä¸Šç•Œã€‚æœ€åï¼Œæˆ‘ä»¬é€šè¿‡æ•°å€¼æ¨¡æ‹Ÿè¡¨æ˜ï¼Œä¸ç°æœ‰ç®—æ³•ç›¸æ¯”ï¼ŒEB-TC $\varepsilon$ çš„æ€§èƒ½è¡¨ç°ä¼˜ç§€ï¼Œä¸”é€‚ç”¨äºä¸åŒé—®é¢˜é¢†åŸŸã€‚

    We propose EB-TC$\varepsilon$, a novel sampling rule for $\varepsilon$-best arm identification in stochastic bandits. It is the first instance of Top Two algorithm analyzed for approximate best arm identification. EB-TC$\varepsilon$ is an *anytime* sampling rule that can therefore be employed without modification for fixed confidence or fixed budget identification (without prior knowledge of the budget). We provide three types of theoretical guarantees for EB-TC$\varepsilon$. First, we prove bounds on its expected sample complexity in the fixed confidence setting, notably showing its asymptotic optimality in combination with an adaptive tuning of its exploration parameter. We complement these findings with upper bounds on its probability of error at any time and for any error parameter, which further yield upper bounds on its simple regret at any time. Finally, we show through numerical simulations that EB-TC$\varepsilon$ performs favorably compared to existing algorithms, in different
    
[^21]: ä»ç†è®ºè§’åº¦æ­ç¤ºâ€œæ€ç»´é“¾â€èƒŒåçš„å¥¥ç§˜

    Towards Revealing the Mystery behind Chain of Thought: a Theoretical Perspective. (arXiv:2305.15408v1 [cs.LG])

    [http://arxiv.org/abs/2305.15408](http://arxiv.org/abs/2305.15408)

    æœ¬æ–‡ä»ç†è®ºå±‚é¢æ¢ç©¶äº†å¸¦æœ‰â€œæ€ç»´é“¾â€æç¤ºçš„å¤§å‹è¯­è¨€æ¨¡å‹åœ¨è§£å†³åŸºæœ¬æ•°å­¦å’Œå†³ç­–é—®é¢˜ä¸­çš„èƒ½åŠ›ï¼Œå‘ç°è‡ªå›å½’Transformerå¤§å°æ’å®šå³å¯è§£å†³ä»»åŠ¡ï¼Œæ­ç¤ºäº†â€œæ€ç»´é“¾â€æç¤ºçš„èƒŒåæœºåˆ¶ã€‚

    

    æœ€è¿‘çš„ç ”ç©¶å‘ç°ï¼Œ"æ€ç»´é“¾"æç¤ºèƒ½å¤Ÿæ˜¾è‘—æé«˜å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠæ•°å­¦æˆ–æ¨ç†çš„å¤æ‚ä»»åŠ¡ä¸­ã€‚å°½ç®¡è·å¾—äº†å·¨å¤§çš„å®è¯æˆåŠŸï¼Œä½†â€œæ€ç»´é“¾â€èƒŒåçš„æœºåˆ¶ä»¥åŠå®ƒå¦‚ä½•é‡Šæ”¾LLMsçš„æ½œåŠ›ä»ç„¶æ˜¯ç¥ç§˜çš„ã€‚æœ¬æ–‡é¦–æ¬¡ä»ç†è®ºä¸Šå›ç­”äº†è¿™äº›é—®é¢˜ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ç ”ç©¶äº†LLMså¸¦æœ‰â€œæ€ç»´é“¾â€åœ¨è§£å†³åŸºæœ¬æ•°å­¦å’Œå†³ç­–é—®é¢˜ä¸­çš„èƒ½åŠ›ã€‚æˆ‘ä»¬é¦–å…ˆç»™å‡ºä¸€ä¸ªä¸å¯èƒ½çš„ç»“æœï¼Œè¡¨æ˜ä»»ä½•æœ‰é™æ·±åº¦çš„Transformeréƒ½ä¸èƒ½ç›´æ¥è¾“å‡ºæ­£ç¡®çš„åŸºæœ¬ç®—æœ¯/æ–¹ç¨‹ä»»åŠ¡çš„ç­”æ¡ˆï¼Œé™¤éæ¨¡å‹å¤§å°éšç€è¾“å…¥é•¿åº¦çš„å¢åŠ å‘ˆè¶…å¤šé¡¹å¼å¢é•¿ã€‚ç›¸åï¼Œæˆ‘ä»¬é€šè¿‡æ„é€ è¯æ˜ï¼Œå¤§å°æ’å®šçš„è‡ªå›å½’Transformerè¶³ä»¥é€šè¿‡ä½¿ç”¨å¸¸ç”¨çš„æ•°å­¦è¯­è¨€å½¢å¼ç”Ÿæˆâ€œæ€ç»´é“¾â€æ¨å¯¼æ¥è§£å†³è¿™ä¸¤ä¸ªä»»åŠ¡ã€‚

    Recent studies have discovered that Chain-of-Thought prompting (CoT) can dramatically improve the performance of Large Language Models (LLMs), particularly when dealing with complex tasks involving mathematics or reasoning. Despite the enormous empirical success, the underlying mechanisms behind CoT and how it unlocks the potential of LLMs remain elusive. In this paper, we take a first step towards theoretically answering these questions. Specifically, we examine the capacity of LLMs with CoT in solving fundamental mathematical and decision-making problems. We start by giving an impossibility result showing that any bounded-depth Transformer cannot directly output correct answers for basic arithmetic/equation tasks unless the model size grows super-polynomially with respect to the input length. In contrast, we then prove by construction that autoregressive Transformers of a constant size suffice to solve both tasks by generating CoT derivations using a commonly-used math language forma
    
[^22]: æ— éœ€Lipschitznesså’ŒSmoothnessçš„åœ¨çº¿æŠ•èµ„ç»„åˆé€‰æ‹©çš„æ•°æ®ç›¸å…³ä¸Šç•Œ

    Data-Dependent Bounds for Online Portfolio Selection Without Lipschitzness and Smoothness. (arXiv:2305.13946v1 [cs.LG])

    [http://arxiv.org/abs/2305.13946](http://arxiv.org/abs/2305.13946)

    æœ¬æ–‡æå‡ºäº†åœ¨çº¿æŠ•èµ„ç»„åˆé€‰æ‹©çš„ç¬¬ä¸€ä¸ªæ•°æ®ç›¸å…³ä¸Šç•Œï¼Œç®—æ³•æ˜¾ç¤ºäºšçº¿æ€§é—æ†¾ç‡ï¼Œå¹¶åœ¨æ•°æ®â€œå®¹æ˜“â€æ—¶å®ç°å¯¹æ•°é—æ†¾ã€‚

    

    æœ¬æ–‡ä»‹ç»äº†åœ¨çº¿æŠ•èµ„ç»„åˆé€‰æ‹©ä¸­çš„ç¬¬ä¸€ç§å°æŸå¤±å’Œå¹³ç¨³å˜åŒ–çš„é—æ†¾ä¸Šç•Œï¼Œå¹¶æ ‡å¿—ç€åœ¨çº¿å‡¸ä¼˜åŒ–å…·æœ‰éLipschitzã€éå…‰æ»‘æŸå¤±çš„æ•°æ®ç›¸å…³ä¸Šç•Œçš„é¦–æ¬¡å®ä¾‹ã€‚æˆ‘ä»¬æå‡ºçš„ç®—æ³•åœ¨æœ€åæƒ…å†µä¸‹æ˜¾ç¤ºå‡ºäºšçº¿æ€§é—æ†¾ç‡ï¼Œå¹¶åœ¨æ•°æ®â€œå®¹æ˜“â€æ—¶å®ç°å¯¹æ•°é—æ†¾ï¼Œæ¯æ¬¡è¿­ä»£çš„æ—¶é—´å‡ ä¹æ˜¯æŠ•èµ„é€‰æ‹©æ•°é‡çš„çº¿æ€§ã€‚é—æ†¾ä¸Šç•Œæ˜¯ä½¿ç”¨å¯¹æ•°æŸå¤±çš„æ–°å‹å…‰æ»‘æ€§è¡¨å¾ã€éµå¾ªå…·æœ‰è‡ªå…±è½­æ­£åˆ™åŒ–å™¨çš„æ­£åˆ™åŒ–é¢†è¢–ï¼ˆFTRLï¼‰çš„å±€éƒ¨èŒƒæ•°åˆ†æã€å®ƒä»¬ä¸ä¸€å®šæ˜¯éšœç¢çš„å’Œå…·æœ‰logéšœç¢çš„ä¹è§‚FTRLçš„éšå¼å˜ä½“æ¥æ¨å¯¼çš„ã€‚

    This work introduces the first small-loss and gradual-variation regret bounds for online portfolio selection, marking the first instances of data-dependent bounds for online convex optimization with non-Lipschitz, non-smooth losses. The algorithms we propose exhibit sublinear regret rates in the worst cases and achieve logarithmic regrets when the data is "easy," with per-iteration time almost linear in the number of investment alternatives. The regret bounds are derived using novel smoothness characterizations of the logarithmic loss, a local norm-based analysis of following the regularized leader (FTRL) with self-concordant regularizers, which are not necessarily barriers, and an implicit variant of optimistic FTRL with the log-barrier.
    
[^23]: é€šè¿‡RKHMå’ŒPerron-Frobeniusç®—å­çš„æ·±åº¦å­¦ä¹ 

    Deep Learning with Kernels through RKHM and the Perron-Frobenius Operator. (arXiv:2305.13588v1 [stat.ML])

    [http://arxiv.org/abs/2305.13588](http://arxiv.org/abs/2305.13588)

    è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºæ ¸æ–¹æ³•çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼šæ·±åº¦RKHMï¼Œé€šè¿‡ä½¿ç”¨$C^*$ä»£æ•°è·å¾—æ›´æ¸©å’Œçš„ç•Œé™ï¼Œå¹¶æä¾›äº†è‰¯æ€§è¿‡æ‹Ÿåˆçš„ç†è®ºè§£é‡Šã€‚

    

    é‡ç°æ ¸å¸Œå°”ä¼¯ç‰¹$C^*$-æ¨¡(RKHM)é€šè¿‡$C^*$ä»£æ•°å¯¹é‡ç°æ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´(RKHS)è¿›è¡Œäº†æ³›åŒ–ï¼Œè€ŒPerron-Frobeniusç®—å­æ˜¯ä¸å‡½æ•°ç»„åˆç›¸å…³çš„çº¿æ€§ç®—å­ã€‚å°†è¿™ä¸¤ä¸ªæ¦‚å¿µç»“åˆèµ·æ¥ï¼Œæˆ‘ä»¬æå‡ºäº†æ·±åº¦RKHMï¼Œä¸€ç§åŸºäºæ ¸æ–¹æ³•çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚æˆ‘ä»¬åœ¨è¿™ä¸ªè®¾ç½®ä¸­æ¨å¯¼äº†ä¸€ä¸ªæ–°çš„Rademacherå¹¿ä¹‰ç•Œé™ï¼Œå¹¶é€šè¿‡Perron-Frobeniusç®—å­æä¾›äº†è‰¯æ€§è¿‡æ‹Ÿåˆçš„ç†è®ºè§£é‡Šã€‚ç”±äº$C^*$ä»£æ•°çš„ä¼˜åŠ¿ï¼Œè¯¥ç•Œé™å¯¹è¾“å‡ºç»´åº¦çš„ä¾èµ–æ€§è¾ƒç°æœ‰ç•Œé™æ›´åŠ æ¸©å’Œã€‚æˆ‘ä»¬å±•ç¤ºäº†$C^*$ä»£æ•°æ˜¯æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒå·¥å…·ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ©ç”¨ç®—å­çš„ä¹˜ç§¯ç»“æ„ï¼Œå¹¶æä¾›ä¸å·ç§¯ç¥ç»ç½‘ç»œçš„æ˜ç¡®è”ç³»ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æä¸ºè®¾è®¡å’Œåˆ†ææ·±åº¦æ ¸æ–¹æ³•æä¾›äº†ä¸€ä¸ªæ–°çš„è§†è§’ã€‚

    Reproducing kernel Hilbert $C^*$-module (RKHM) is a generalization of reproducing kernel Hilbert space (RKHS) by means of $C^*$-algebra, and the Perron-Frobenius operator is a linear operator related to the composition of functions. Combining these two concepts, we present deep RKHM, a deep learning framework for kernel methods. We derive a new Rademacher generalization bound in this setting and provide a theoretical interpretation of benign overfitting by means of Perron-Frobenius operators. By virtue of $C^*$-algebra, the dependency of the bound on output dimension is milder than existing bounds. We show that $C^*$-algebra is a suitable tool for deep learning with kernels, enabling us to take advantage of the product structure of operators and to provide a clear connection with convolutional neural networks. Our theoretical analysis provides a new lens through which one can design and analyze deep kernel methods.
    
[^24]: åŠ¨é‡åŒ¹é…å»å™ªGibbsé‡‡æ ·

    Moment Matching Denoising Gibbs Sampling. (arXiv:2305.11650v1 [stat.ML])

    [http://arxiv.org/abs/2305.11650](http://arxiv.org/abs/2305.11650)

    æœ¬æ–‡æå‡ºäº†åŠ¨é‡åŒ¹é…å»å™ªGibbsé‡‡æ ·æ–¹æ³•ï¼Œå¯ä»¥åœ¨ç»™å®šâ€˜å˜ˆæ‚â€™çš„æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä»å¹²å‡€çš„æ¨¡å‹ä¸­æœ‰æ•ˆåœ°è¿›è¡Œé‡‡æ ·ã€‚

    

    èƒ½é‡åŸºæ¨¡å‹ï¼ˆEBMsï¼‰ä¸ºå»ºæ¨¡å¤æ‚æ•°æ®åˆ†å¸ƒæä¾›äº†ä¸€ä¸ªé€šç”¨çš„æ¡†æ¶ã€‚ç„¶è€Œï¼ŒEBMs çš„è®­ç»ƒå’Œé‡‡æ ·ä»ç„¶é¢ä¸´é‡å¤§æŒ‘æˆ˜ã€‚ç”¨äºå¯æ‰©å±• EBM è®­ç»ƒçš„å¹¿æ³›ä½¿ç”¨çš„å»å™ªåˆ†æ•°åŒ¹é…ï¼ˆDSMï¼‰æ–¹æ³•å­˜åœ¨ä¸ä¸€è‡´æ€§é—®é¢˜ï¼Œå¯¼è‡´èƒ½é‡æ¨¡å‹å­¦ä¹ åˆ°â€œå˜ˆæ‚â€çš„æ•°æ®åˆ†å¸ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœ‰æ•ˆçš„é‡‡æ ·æ¡†æ¶ï¼šï¼ˆä¼ªï¼‰Gibbsé‡‡æ ·ä¸åŠ¨é‡åŒ¹é…ï¼Œå¯ä»¥åœ¨ç»™å®šç»è¿‡DSMè®­ç»ƒè‰¯å¥½çš„â€œå˜ˆæ‚â€æ¨¡å‹çš„æƒ…å†µä¸‹ï¼Œä»åŸºç¡€â€œå¹²å‡€â€æ¨¡å‹ä¸­æœ‰æ•ˆåœ°è¿›è¡Œé‡‡æ ·ã€‚æˆ‘ä»¬æ¢è®¨äº†æˆ‘ä»¬çš„æ–¹æ³•ç›¸å¯¹äºç›¸å…³æ–¹æ³•çš„ä¼˜åŠ¿ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•å°†è¯¥æ–¹æ³•æ‰©å±•åˆ°é«˜ç»´æ•°æ®é›†ã€‚

    Energy-Based Models (EBMs) offer a versatile framework for modeling complex data distributions. However, training and sampling from EBMs continue to pose significant challenges. The widely-used Denoising Score Matching (DSM) method for scalable EBM training suffers from inconsistency issues, causing the energy model to learn a `noisy' data distribution. In this work, we propose an efficient sampling framework: (pseudo)-Gibbs sampling with moment matching, which enables effective sampling from the underlying clean model when given a `noisy' model that has been well-trained via DSM. We explore the benefits of our approach compared to related methods and demonstrate how to scale the method to high-dimensional datasets.
    
[^25]: ç”¨æœ‰æ•ˆçš„è§†é‡è¿æ¥å¼ºåŒ–å­¦ä¹ ç†è®ºå’Œå®è·µ

    Bridging RL Theory and Practice with the Effective Horizon. (arXiv:2304.09853v1 [cs.LG])

    [http://arxiv.org/abs/2304.09853](http://arxiv.org/abs/2304.09853)

    æœ¬è®ºæ–‡é€šè¿‡å¯¹å¸¸è§æ·±åº¦å¼ºåŒ–å­¦ä¹ æµ‹è¯•åŸºå‡†ä¸­155ä¸ªMDPçš„æ•°æ®é›†è¿›è¡Œåˆ†æï¼Œå‘ç°å½“æœ€é«˜Qå€¼çš„åŠ¨ä½œåœ¨éšæœºç­–ç•¥ä¸‹Qå€¼æœ€é«˜æ—¶ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ å¾€å¾€ä¼šæˆåŠŸï¼›åä¹‹ï¼Œåˆ™å¤±è´¥çš„å¯èƒ½æ€§è¾ƒé«˜ã€‚

    

    æ·±åº¦å¼ºåŒ–å­¦ä¹ åœ¨æŸäº›ç¯å¢ƒä¸­è¡¨ç°å‡ºè‰²ï¼Œä½†åœ¨å…¶ä»–ç¯å¢ƒä¸­å´å¤±è´¥å¾—éå¸¸ä¸¥é‡ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œå¼ºåŒ–å­¦ä¹ ç†è®ºåº”è¯¥èƒ½å¤Ÿè§£é‡Šè¿™ç§ç°è±¡ï¼Œæä¾›é¢„æµ‹å®é™…æ€§èƒ½çš„ç•Œé™ã€‚ä¸å¹¸çš„æ˜¯ï¼Œå½“å‰çš„ç†è®ºè¿˜æ²¡æœ‰è¿™ç§èƒ½åŠ›ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥åŒ…å«155ä¸ªMDPçš„æ–°æ•°æ®é›†BRIDGEï¼Œå°†æ ‡å‡†çš„æ·±åº¦å¼ºåŒ–å­¦ä¹ ç®—æ³•ä¸ä¹‹å‰çš„æ ·æœ¬å¤æ‚åº¦å…ˆå‰ç•Œè¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å‘ç°äº†ä¸€ä¸ªæ„æƒ³ä¸åˆ°çš„æ€§è´¨ï¼šå½“æœ€é«˜Qå€¼çš„åŠ¨ä½œåœ¨éšæœºç­–ç•¥ä¸‹çš„Qå€¼ä¹Ÿæ˜¯æœ€é«˜çš„æ—¶ï¼Œæ·±åº¦å¼ºåŒ–å­¦ä¹ å¾€å¾€ä¼šæˆåŠŸï¼›åä¹‹ï¼Œå¤±è´¥çš„å¯èƒ½æ€§è¾ƒé«˜ã€‚åŸºäºè¿™ä¸€æ€§è´¨ï¼Œæˆ‘ä»¬å°†å…¶æ¦‚æ‹¬ä¸ºä¸€ä¸ªæ–°çš„MDPå¤æ‚åº¦åº¦é‡ï¼Œç§°ä¸ºæœ‰æ•ˆçš„è§†é‡ã€‚

    Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity prior bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the random policy also have the highest Q-values under the optimal policy, deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the eff
    
[^26]: å¸¦å¸æ”¶çš„æ³›æ´ªï¼šå¤æ‚ç½‘ç»œä¸Šå¼‚æ„èµŒåšæœºçš„é«˜æ•ˆåè®®

    Flooding with Absorption: An Efficient Protocol for Heterogeneous Bandits over Complex Networks. (arXiv:2303.05445v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.05445](http://arxiv.org/abs/2303.05445)

    è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸ºå¸¦å¸æ”¶çš„æ³›æ´ªï¼ˆFwAï¼‰çš„æ–°åè®®ï¼Œç”¨äºè§£å†³å¤æ‚ç½‘ç»œä¸Šçš„å¼‚æ„èµŒåšæœºé—®é¢˜ã€‚é€šè¿‡ä¸¥æ ¼çš„é—æ†¾åˆ†æï¼Œè¯æ˜äº†è¯¥åè®®çš„æœ‰æ•ˆæ€§ã€‚

    

    å¤šè‡‚èµŒåšæœºå¹¿æ³›ç”¨äºå»ºæ¨¡é¡ºåºå†³ç­–ï¼Œåœ¨è®¸å¤šç°å®åº”ç”¨ä¸­å¦‚åœ¨çº¿æ¨èç³»ç»Ÿå’Œæ— çº¿ç½‘ç»œä¸­æ— å¤„ä¸åœ¨ã€‚æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªå¤šä»£ç†çš„åœºæ™¯ï¼Œæ¯ä¸ªä»£ç†è§£å†³è‡ªå·±çš„èµŒåšæœºé—®é¢˜ï¼ŒèµŒåšæœºæ‹¥æœ‰ä¸åŒçš„è‡‚ã€‚ä»–ä»¬çš„ç›®æ ‡æ˜¯åœ¨é€šè¿‡ç»™å®šç½‘ç»œçš„é€šä¿¡åè®®åä½œçš„åŒæ—¶æœ€å°åŒ–ä»–ä»¬çš„é›†ä½“é—æ†¾ã€‚å…ˆå‰å…³äºæ­¤é—®é¢˜çš„æ–‡çŒ®åªè€ƒè™‘äº†è‡‚çš„å¼‚è´¨æ€§å’Œç½‘ç»œåŒ–ä»£ç†é—®é¢˜ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªåŒæ—¶åŒ…å«è¿™ä¸¤ä¸ªç‰¹æ€§çš„è®¾ç½®ã€‚é’ˆå¯¹è¿™ä¸€æ–°é¢–çš„è®¾ç½®ï¼Œæˆ‘ä»¬é¦–å…ˆå¯¹æ ‡å‡†æ³›æ´ªåè®®ç»“åˆç»å…¸çš„ä¸Šç½®ä¿¡ç•Œç­–ç•¥æä¾›äº†ä¸¥æ ¼çš„é—æ†¾åˆ†æã€‚ç„¶åï¼Œä¸ºäº†å‡è½»åœ¨å¤æ‚ç½‘ç»œä¸­æ³›æ´ªé€ æˆçš„é«˜é€šä¿¡æˆæœ¬é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„åè®®ï¼Œç§°ä¸ºå¸¦å¸æ”¶çš„æ³›æ´ªï¼ˆFwAï¼‰ã€‚æˆ‘ä»¬å¯¹ç”±æ­¤äº§ç”Ÿçš„é—æ†¾ä¸Šç•Œè¿›è¡Œäº†ç†è®ºåˆ†æï¼Œå¹¶è®¨è®ºäº†è¯¥åè®®çš„ä¼˜ç‚¹ã€‚

    Multi-armed bandits are extensively used to model sequential decision-making, making them ubiquitous in many real-life applications such as online recommender systems and wireless networking. We consider a multi-agent setting where each agent solves their own bandit instance endowed with a different set of arms. Their goal is to minimize their group regret while collaborating via some communication protocol over a given network. Previous literature on this problem only considered arm heterogeneity and networked agents separately. In this work, we introduce a setting that encompasses both features. For this novel setting, we first provide a rigorous regret analysis for a standard flooding protocol combined with the classic UCB policy. Then, to mitigate the issue of high communication costs incurred by flooding in complex networks, we propose a new protocol called Flooding with Absorption (FwA). We provide a theoretical analysis of the resulting regret bound and discuss the advantages of
    
[^27]: å¤šçº§æ‰©æ•£ï¼šå›¾åƒç”Ÿæˆçš„æ— é™ç»´åº¦åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹

    Multilevel Diffusion: Infinite Dimensional Score-Based Diffusion Models for Image Generation. (arXiv:2303.04772v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.04772](http://arxiv.org/abs/2303.04772)

    æœ¬æ–‡ä»‹ç»äº†æ— é™ç»´åº¦å¾—åˆ†æ‰©æ•£æ¨¡å‹åœ¨å¤šä¸ªåˆ†è¾¨ç‡æ°´å¹³ä¸Šçš„ç¦»æ•£åŒ–æ–¹æ³•ï¼Œå¹¶ä½¿ç”¨å¤šçº§æ‰©æ•£ç®—æ³•åœ¨å¤šä¸ªåˆ†è¾¨ç‡ä¸Šé«˜æ•ˆåœ°å­¦ä¹ ã€‚å®è¯è¡¨æ˜ï¼Œè¯¥æ¨¡å‹åœ¨ç›¸åŒæˆ–æ›´é«˜åˆ†è¾¨ç‡ä¸‹äº§ç”Ÿæ¯”ä¼ ç»ŸåŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹æ›´é«˜è´¨é‡çš„æ ·æœ¬ï¼Œå¹¶å¯ä»¥ç”Ÿæˆä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒå¹¶å¤„ç†çŸ©å½¢åŸŸã€‚

    

    åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹æ˜¯è¿‘å¹´æ¥å›¾åƒç”Ÿæˆçš„æœ€å…ˆè¿›æ–¹æ³•ä¹‹ä¸€ã€‚ç°æœ‰çš„åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹é€šå¸¸åœ¨æœ‰é™ç»´åº¦è®¾ç½®ä¸­è¡¨è¿°ï¼Œå…¶ä¸­å›¾åƒè¢«è§†ä¸ºå…·æœ‰æœ‰é™å°ºå¯¸çš„å¼ é‡ã€‚æœ¬æ–‡åœ¨æ— é™ç»´åº¦è®¾ç½®ä¸­å¼€å‘äº†åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹ï¼Œå³æˆ‘ä»¬å°†è®­ç»ƒæ•°æ®å»ºæ¨¡ä¸ºæ”¯æ’‘åœ¨çŸ©å½¢åŸŸä¸Šçš„å‡½æ•°ã€‚é™¤äº†è¿½æ±‚åœ¨æ›´é«˜åˆ†è¾¨ç‡ä¸‹ç”Ÿæˆå›¾åƒä¹‹å¤–ï¼Œæˆ‘ä»¬çš„ä¸»è¦åŠ¨æœºæ˜¯åˆ›å»ºä¸€ä¸ªè‰¯å¥½å®šä¹‰çš„æ— é™ç»´åº¦å­¦ä¹ é—®é¢˜ï¼Œä»¥ä¾¿å¯ä»¥åœ¨å¤šä¸ªåˆ†è¾¨ç‡æ°´å¹³ä¸Šä¸€è‡´åœ°ç¦»æ•£åŒ–å®ƒã€‚æˆ‘ä»¬å¸Œæœ›è·å¾—èƒ½å¤Ÿæ¨ªè·¨ä¸åŒåˆ†è¾¨ç‡çº§åˆ«çš„æ‰©æ•£æ¨¡å‹ï¼Œå¹¶æé«˜è®­ç»ƒè¿‡ç¨‹çš„æ•ˆç‡ã€‚æˆ‘ä»¬å±•ç¤ºäº†å¦‚ä½•å…‹æœå½“å‰åŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹åœ¨æ— é™ç»´åº¦è®¾ç½®ä¸­å­˜åœ¨çš„ä¸¤ä¸ªç¼ºç‚¹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ä¿®æ”¹äº†å‰å‘è¿‡ç¨‹ä»¥ç¡®ä¿åœ¨æ— é™ç»´åº¦è®¾ç½®ä¸­æ½œåœ¨åˆ†å¸ƒæ˜¯è‰¯å¥½å®šä¹‰çš„ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§å¤šçº§æ‰©æ•£ç®—æ³•ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåœ¨å¤šä¸ªåˆ†è¾¨ç‡ä¸Šé«˜æ•ˆåœ°å­¦ä¹ ã€‚æˆ‘ä»¬å®è¯è¡¨æ˜ï¼Œæˆ‘ä»¬çš„å¤šçº§æ¨¡å‹åœ¨ç›¸åŒæˆ–æ›´é«˜åˆ†è¾¨ç‡ä¸‹äº§ç”Ÿæ¯”ä¼ ç»ŸåŸºäºå¾—åˆ†çš„æ‰©æ•£æ¨¡å‹æ›´é«˜è´¨é‡çš„æ ·æœ¬ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥æ— ç¼åœ°ç”Ÿæˆä¸åŒåˆ†è¾¨ç‡çš„å›¾åƒå¹¶å¤„ç†çŸ©å½¢åŸŸã€‚

    Score-based diffusion models (SBDM) have recently emerged as state-of-the-art approaches for image generation. Existing SBDMs are typically formulated in a finite-dimensional setting, where images are considered as tensors of a finite size. This papers develops SBDMs in the infinite-dimensional setting, that is, we model the training data as functions supported on a rectangular domain. Besides the quest for generating images at ever higher resolution our primary motivation is to create a well-posed infinite-dimensional learning problem so that we can discretize it consistently on multiple resolution levels. We thereby hope to obtain diffusion models that generalize across different resolution levels and improve the efficiency of the training process. We demonstrate how to overcome two shortcomings of current SBDM approaches in the infinite-dimensional setting. First, we modify the forward process to ensure that the latent distribution is well-defined in the infinite-dimensional setting
    
[^28]: æ•°æ®ä¿®å‰ªå’Œç¥ç»ç¼©æ”¾å®šå¾‹ï¼šåŸºäºè¯„åˆ†çš„ç®—æ³•çš„åŸºæœ¬é™åˆ¶

    Data pruning and neural scaling laws: fundamental limitations of score-based algorithms. (arXiv:2302.06960v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.06960](http://arxiv.org/abs/2302.06960)

    è¯„åˆ†æ•°æ®ä¿®å‰ªç®—æ³•åœ¨é«˜å‹ç¼©åŒºåŸŸå¤±è´¥ï¼Œé€šè¿‡éšæœºåŒ–çš„æ ¡å‡†åè®®å¯ä»¥æé«˜ç°æœ‰ä¿®å‰ªç®—æ³•åœ¨è¯¥åŒºåŸŸçš„æ€§èƒ½ã€‚

    

    æ•°æ®ä¿®å‰ªç®—æ³•å¸¸ç”¨äºå‡å°‘ä¼˜åŒ–è¿‡ç¨‹çš„å†…å­˜å’Œè®¡ç®—æˆæœ¬ã€‚æœ€è¿‘çš„å®è¯ç»“æœè¡¨æ˜ï¼Œéšæœºæ•°æ®ä¿®å‰ªä»ç„¶æ˜¯ä¸€ä¸ªå¼ºå¤§çš„åŸºå‡†ï¼Œå¹¶åœ¨é«˜å‹ç¼©åŒºåŸŸä¼˜äºå¤§å¤šæ•°ç°æœ‰çš„æ•°æ®ä¿®å‰ªæ–¹æ³•ï¼Œå³ä¿ç•™äº†ä¸åˆ°æ•°æ®çš„30ï¼…çš„éƒ¨åˆ†ã€‚è¿™ç§å‹ç¼©åŒºåŸŸæœ€è¿‘å¼•èµ·äº†å¾ˆå¤šå…³æ³¨ï¼Œå› ä¸ºæ•°æ®ä¿®å‰ªåœ¨æé«˜æ‰€è°“çš„ç¥ç»ç¼©æ”¾å®šå¾‹ä¸­çš„ä½œç”¨ï¼›åœ¨[Sorscher et al.]ä¸­ï¼Œä½œè€…å±•ç¤ºäº†éœ€è¦é«˜è´¨é‡çš„æ•°æ®ä¿®å‰ªç®—æ³•æ‰èƒ½å‡»è´¥æ ·æœ¬åŠ¿å¾‹ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å…³æ³¨è¯„åˆ†æ•°æ®ä¿®å‰ªç®—æ³•ï¼Œå¹¶åœ¨ç†è®ºä¸Šå’Œå®é™…ä¸Šå±•ç¤ºäº†ä¸ºä»€ä¹ˆè¿™æ ·çš„ç®—æ³•åœ¨é«˜å‹ç¼©åŒºåŸŸå¤±è´¥ã€‚æˆ‘ä»¬è¯æ˜äº†æ•°æ®ä¿®å‰ªçš„â€œæ²¡æœ‰å…è´¹åˆé¤â€å®šç†ï¼Œå¹¶é€šè¿‡éšæœºåŒ–æå‡ºäº†æ ¡å‡†åè®®ï¼Œä»¥æé«˜ç°æœ‰ä¿®å‰ªç®—æ³•åœ¨é«˜å‹ç¼©åŒºåŸŸçš„æ€§èƒ½ã€‚

    Data pruning algorithms are commonly used to reduce the memory and computational cost of the optimization process. Recent empirical results reveal that random data pruning remains a strong baseline and outperforms most existing data pruning methods in the high compression regime, i.e., where a fraction of $30\%$ or less of the data is kept. This regime has recently attracted a lot of interest as a result of the role of data pruning in improving the so-called neural scaling laws; in [Sorscher et al.], the authors showed the need for high-quality data pruning algorithms in order to beat the sample power law.  In this work, we focus on score-based data pruning algorithms and show theoretically and empirically why such algorithms fail in the high compression regime. We demonstrate ``No Free Lunch" theorems for data pruning and present calibration protocols that enhance the performance of existing pruning algorithms in this high compression regime using randomization.
    
[^29]: ä¸€ç§ç»Ÿä¸€çš„æ–¹æ³•æ¨å¯¼ï¼ˆæ—¶é—´å‡åŒ€çš„ï¼‰PAC-Bayesç•Œé™

    A unified recipe for deriving (time-uniform) PAC-Bayes bounds. (arXiv:2302.03421v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.03421](http://arxiv.org/abs/2302.03421)

    è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§ç”¨äºæ¨å¯¼PAC-Bayesianæ³›åŒ–ç•Œé™çš„ç»Ÿä¸€æ¡†æ¶ï¼Œä¸åŒäºä¼ ç»Ÿçš„å›ºå®šæ ·æœ¬å¤§å°æ–¹å¼ï¼Œè¯¥æ¡†æ¶é€‚ç”¨äºæ‰€æœ‰åœæ­¢æ—¶é—´ã€‚åŒæ—¶ï¼Œè¯¥è®ºæ–‡è¿˜æå‡ºäº†æ–°çš„è¾¹ç•Œæ–¹æ³•ï¼Œä¹Ÿå¯ä»¥åº”ç”¨äºéå¹³ç¨³æŸå¤±å‡½æ•°å’Œéç‹¬ç«‹åŒåˆ†å¸ƒçš„æ•°æ®ã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ¡†æ¶ï¼Œç”¨äºæ¨å¯¼PAC-Bayesianæ³›åŒ–ç•Œé™ã€‚ä¸å¤§å¤šæ•°å…³äºæ­¤ä¸»é¢˜çš„æ–‡çŒ®ä¸åŒï¼Œæˆ‘ä»¬çš„ç•Œé™æ˜¯ä»»ä½•æ—¶é—´éƒ½æœ‰æ•ˆçš„ï¼ˆå³æ—¶é—´å‡åŒ€çš„ï¼‰ï¼Œè¿™æ„å‘³ç€å®ƒä»¬é€‚ç”¨äºæ‰€æœ‰åœæ­¢æ—¶é—´ï¼Œè€Œä¸ä»…ä»…æ˜¯å›ºå®šçš„æ ·æœ¬å¤§å°ã€‚æˆ‘ä»¬çš„æ–¹æ³•æŒ‰ç…§ä»¥ä¸‹é¡ºåºç»“åˆäº†å››ç§å·¥å…·ï¼šï¼ˆaï¼‰éè´Ÿè¶…é©¬ä¸æ ¼å°”æˆ–åå‘äºšé©¬é€Šï¼Œï¼ˆbï¼‰æ··åˆæ³•ï¼Œï¼ˆcï¼‰Donsker-Varadhanå…¬å¼ï¼ˆæˆ–å…¶å®ƒå‡¸æ€§å¯¹å¶åŸç†ï¼‰å’Œï¼ˆdï¼‰Villeä¸ç­‰å¼ã€‚æˆ‘ä»¬çš„ä¸»è¦æˆæœæ˜¯ä¸€ä¸ªPAC-Bayeså®šç†ï¼Œé€‚ç”¨äºå¹¿æ³›çš„ç¦»æ•£éšæœºè¿‡ç¨‹ç±»ã€‚æˆ‘ä»¬å±•ç¤ºäº†è¿™ä¸ªç»“æœå¦‚ä½•æ¨å‡ºçŸ¥åçš„ç»å…¸PAC-Bayesç•Œé™ï¼Œä¾‹å¦‚Seegerã€McAllesterã€Maurerå’ŒCatoniçš„ç•Œé™ï¼Œä»¥åŠè®¸å¤šæœ€æ–°çš„ç•Œé™ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†å‡ ä¸ªæ–°çš„ç•Œé™ã€‚æˆ‘ä»¬çš„æ¡†æ¶è¿˜ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ”¾æ¾ä¼ ç»Ÿçš„å‡è®¾ï¼›ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬è€ƒè™‘éå¹³ç¨³æŸå¤±å‡½æ•°å’Œéç‹¬ç«‹åŒåˆ†å¸ƒçš„æ•°æ®ã€‚

    We present a unified framework for deriving PAC-Bayesian generalization bounds. Unlike most previous literature on this topic, our bounds are anytime-valid (i.e., time-uniform), meaning that they hold at all stopping times, not only for a fixed sample size. Our approach combines four tools in the following order: (a) nonnegative supermartingales or reverse submartingales, (b) the method of mixtures, (c) the Donsker-Varadhan formula (or other convex duality principles), and (d) Ville's inequality. Our main result is a PAC-Bayes theorem which holds for a wide class of discrete stochastic processes. We show how this result implies time-uniform versions of well-known classical PAC-Bayes bounds, such as those of Seeger, McAllester, Maurer, and Catoni, in addition to many recent bounds. We also present several novel bounds. Our framework also enables us to relax traditional assumptions; in particular, we consider nonstationary loss functions and non-i.i.d. data. In sum, we unify the derivati
    
[^30]: é¢„æ¡ä»¶å¯¹è¶…å‚åŒ–ä½ç§©çŸ©é˜µæ„ŸçŸ¥çš„å½±å“

    The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01186](http://arxiv.org/abs/2302.01186)

    è¯¥ç ”ç©¶æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç§©çŸ©é˜µæ„ŸçŸ¥é—®é¢˜æ—¶å…·æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚

    

    æœ¬æ–‡æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•æ¥è§£å†³ä½ç§©çŸ©é˜µæ„ŸçŸ¥ä¸­çŸ©é˜µå¯èƒ½ç—…æ€ä»¥åŠçœŸå®ç§©æœªçŸ¥çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è¶…å‚å¼è¡¨ç¤ºï¼Œä»ä¸€ä¸ªå°çš„éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œé€šè¿‡ä½¿ç”¨ç‰¹å®šå½¢å¼çš„é˜»å°¼é¢„æ¡ä»¶æ¢¯åº¦ä¸‹é™æ¥å¯¹æŠ—è¶…å‚åŒ–å’Œç—…æ€æ›²ç‡çš„å½±å“ã€‚ä¸åŸºå‡†æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰ç›¸æ¯”ï¼Œå°½ç®¡é¢„å¤„ç†éœ€è¦è½»å¾®çš„è®¡ç®—å¼€é”€ï¼Œä½†ScaledGDï¼ˆğœ†ï¼‰åœ¨é¢å¯¹ç—…æ€é—®é¢˜æ—¶è¡¨ç°å‡ºäº†å‡ºè‰²çš„é²æ£’æ€§ã€‚åœ¨é«˜æ–¯è®¾è®¡ä¸‹ï¼ŒScaledGD($\lambda$) ä¼šåœ¨ä»…è¿­ä»£æ•°å¯¹æ•°çº§åˆ«çš„æƒ…å†µä¸‹ï¼Œä»¥çº¿æ€§é€Ÿç‡æ”¶æ•›åˆ°çœŸå®çš„ä½ç§©çŸ©é˜µã€‚

    We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi
    
[^31]: ä½ æ˜¯å¦æ­£ç¡®ä½¿ç”¨äº†æµ‹è¯•å¯¹æ•°ä¼¼ç„¶ï¼Ÿ

    Are you using test log-likelihood correctly?. (arXiv:2212.00219v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2212.00219](http://arxiv.org/abs/2212.00219)

    ä½¿ç”¨æµ‹è¯•å¯¹æ•°ä¼¼ç„¶è¿›è¡Œæ¯”è¾ƒå¯èƒ½ä¸å…¶ä»–æŒ‡æ ‡ç›¸çŸ›ç›¾ï¼Œå¹¶ä¸”é«˜æµ‹è¯•å¯¹æ•°ä¼¼ç„¶ä¸æ„å‘³ç€æ›´å‡†ç¡®çš„åéªŒè¿‘ä¼¼ã€‚

    

    æµ‹è¯•å¯¹æ•°ä¼¼ç„¶å¸¸è¢«ç”¨æ¥æ¯”è¾ƒä¸åŒæ¨¡å‹çš„åŒä¸€æ•°æ®ï¼Œæˆ–è€…æ¯”è¾ƒæ‹ŸåˆåŒä¸€æ¦‚ç‡æ¨¡å‹çš„ä¸åŒè¿‘ä¼¼æ¨æ–­ç®—æ³•ã€‚æˆ‘ä»¬é€šè¿‡ç®€å•çš„ä¾‹å­å±•ç¤ºäº†å¦‚ä½•åŸºäºæµ‹è¯•å¯¹æ•°ä¼¼ç„¶çš„æ¯”è¾ƒå¯èƒ½ä¸å…¶ä»–ç›®æ ‡ç›¸çŸ›ç›¾ã€‚å…·ä½“æ¥è¯´ï¼Œæˆ‘ä»¬çš„ä¾‹å­è¡¨æ˜ï¼šï¼ˆiï¼‰è¾¾åˆ°æ›´é«˜æµ‹è¯•å¯¹æ•°ä¼¼ç„¶çš„è¿‘ä¼¼è´å¶æ–¯æ¨æ–­ç®—æ³•ä¸å¿…æ„å‘³ç€èƒ½å¤Ÿäº§ç”Ÿæ›´å‡†ç¡®çš„åéªŒè¿‘ä¼¼ï¼Œï¼ˆiiï¼‰åŸºäºæµ‹è¯•å¯¹æ•°ä¼¼ç„¶æ¯”è¾ƒçš„é¢„æµ‹å‡†ç¡®æ€§ç»“è®ºå¯èƒ½ä¸åŸºäºå‡æ–¹æ ¹è¯¯å·®çš„ç»“è®ºä¸ä¸€è‡´ã€‚

    Test log-likelihood is commonly used to compare different models of the same data or different approximate inference algorithms for fitting the same probabilistic model. We present simple examples demonstrating how comparisons based on test log-likelihood can contradict comparisons according to other objectives. Specifically, our examples show that (i) approximate Bayesian inference algorithms that attain higher test log-likelihoods need not also yield more accurate posterior approximations and (ii) conclusions about forecast accuracy based on test log-likelihood comparisons may not agree with conclusions based on root mean squared error.
    
[^32]: é«˜ç»´åˆ†ä½æ•°å›å½’ä¸­çš„è½¬ç§»å­¦ä¹ ç»Ÿè®¡æ¨æ–­

    Statistical inference for transfer learning with high-dimensional quantile regression. (arXiv:2211.14578v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2211.14578](http://arxiv.org/abs/2211.14578)

    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§é«˜ç»´åˆ†ä½æ•°å›å½’æ¨¡å‹ä¸­çš„è½¬ç§»å­¦ä¹ æ–¹æ³•ï¼Œä»¥é€‚åº”æºåŸŸå’Œç›®æ ‡åŸŸä¸­çš„å¼‚è´¨æ€§å’Œé‡å°¾åˆ†å¸ƒã€‚æ ¹æ®ç²¾å¿ƒé€‰æ‹©çš„å¯è½¬ç§»æºåŸŸå»ºç«‹äº†è½¬ç§»å­¦ä¹ ä¼°è®¡é‡çš„è¯¯å·®ç•Œé™ï¼Œå¹¶æå‡ºäº†æœ‰æ•ˆçš„ç½®ä¿¡åŒºé—´å’Œå‡è®¾æ£€éªŒç¨‹åºï¼Œä»¥å®ç°ä¸€æ­¥å®Œæˆã€‚

    

    è½¬ç§»å­¦ä¹ å·²ç»æˆä¸ºä¸€ç§é‡è¦çš„æŠ€æœ¯ï¼Œç”¨äºåˆ©ç”¨æºåŸŸä¸­çš„ä¿¡æ¯æ¥æé«˜ç›®æ ‡ä»»åŠ¡çš„æ€§èƒ½ã€‚å°½ç®¡é«˜ç»´æ•°æ®æ™®éå­˜åœ¨å¼‚è´¨æ€§å’Œ/æˆ–é‡å°¾åˆ†å¸ƒï¼Œä½†ç›®å‰çš„è½¬ç§»å­¦ä¹ æ–¹æ³•æœªèƒ½å……åˆ†è€ƒè™‘è¿™äº›é—®é¢˜ï¼Œå¯èƒ½ä¼šå½±å“ç»“æœçš„æ€§èƒ½ã€‚æˆ‘ä»¬åœ¨é«˜ç»´åˆ†ä½æ•°å›å½’æ¨¡å‹æ¡†æ¶ä¸‹æå‡ºäº†ä¸€ç§è½¬ç§»å­¦ä¹ è¿‡ç¨‹ï¼Œä»¥é€‚åº”æºåŸŸå’Œç›®æ ‡åŸŸä¸­çš„å¼‚è´¨æ€§å’Œé‡å°¾åˆ†å¸ƒã€‚æˆ‘ä»¬æ ¹æ®ç²¾å¿ƒé€‰æ‹©çš„å¯è½¬ç§»æºåŸŸå»ºç«‹äº†è½¬ç§»å­¦ä¹ ä¼°è®¡é‡çš„è¯¯å·®ç•Œé™ï¼Œæ˜¾ç¤ºåœ¨å…³é”®é€‰æ‹©æ ‡å‡†å’Œè¾ƒå¤§çš„æºä»»åŠ¡æ ·æœ¬é‡ä¸‹å¯ä»¥å®ç°æ›´ä½çš„è¯¯å·®ç•Œé™ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥æå‡ºäº†ä¸€ä¸ªæœ‰æ•ˆçš„ç½®ä¿¡åŒºé—´å’Œå‡è®¾æ£€éªŒç¨‹åºï¼Œç”¨äºé«˜ç»´åˆ†ä½æ•°å›å½’ç³»æ•°çš„å„ä¸ªåˆ†é‡ï¼Œé€šè¿‡å€¡å¯¼åŒé‡è½¬ç§»å­¦ä¹ ä¼°è®¡é‡ï¼Œå®ç°ä¸€æ­¥å®Œæˆã€‚

    Transfer learning has become an essential technique to exploit information from the source domain to boost performance of the target task. Despite the prevalence in high-dimensional data, heterogeneity and/or heavy tails are insufficiently accounted for by current transfer learning approaches and thus may undermine the resulting performance. We propose a transfer learning procedure in the framework of high-dimensional quantile regression models to accommodate the heterogeneity and heavy tails in the source and target domains. We establish error bounds of the transfer learning estimator based on delicately selected transferable source domains, showing that lower error bounds can be achieved for critical selection criterion and larger sample size of source tasks. We further propose valid confidence interval and hypothesis test procedures for individual component of high-dimensional quantile regression coefficients by advocating a double transfer learning estimator, which is the one-step 
    
[^33]: é€šè¿‡ä½¿ç”¨Cover Treesçš„æœ€å°é—´éš”å®ç°æ•°å€¼ç¨³å®šçš„ç¨€ç–é«˜æ–¯è¿‡ç¨‹

    Numerically Stable Sparse Gaussian Processes via Minimum Separation using Cover Trees. (arXiv:2210.07893v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2210.07893](http://arxiv.org/abs/2210.07893)

    æœ¬æ–‡é’ˆå¯¹é«˜æ–¯è¿‡ç¨‹æ¨¡å‹çš„æ•°å€¼ç¨³å®šæ€§è¿›è¡Œäº†ç ”ç©¶ï¼Œé€šè¿‡æ„Ÿå…´è¶£ç‚¹çš„é€‰æ‹©å’Œè®¡ç®—ï¼Œæä¾›äº†ç¨³å®šå¯é çš„ç¨€ç–é€¼è¿‘æ–¹æ³•ã€‚

    

    é«˜æ–¯è¿‡ç¨‹å¸¸ç”¨äºè¾ƒå¤§çš„æœºå™¨å­¦ä¹ å’Œå†³ç­–ç³»ç»Ÿä¸­ï¼Œä¾‹å¦‚åœ°ç†ç©ºé—´å»ºæ¨¡ã€è´å¶æ–¯ä¼˜åŒ–æˆ–æ½œåœ¨é«˜æ–¯æ¨¡å‹ä¸­ã€‚åœ¨ä¸€ä¸ªç³»ç»Ÿä¸­ï¼Œé«˜æ–¯è¿‡ç¨‹æ¨¡å‹éœ€è¦ä»¥ç¨³å®šå¯é çš„æ–¹å¼è¿è¡Œï¼Œä»¥ç¡®ä¿ä¸ç³»ç»Ÿçš„å…¶ä»–éƒ¨åˆ†æ­£ç¡®äº¤äº’ã€‚æœ¬æ–‡ç ”ç©¶äº†åŸºäºæ„Ÿå…´è¶£ç‚¹çš„å¯æ‰©å±•ç¨€ç–é€¼è¿‘çš„æ•°å€¼ç¨³å®šæ€§ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆå›é¡¾äº†æ•°å€¼ç¨³å®šæ€§ï¼Œå¹¶é˜è¿°äº†é«˜æ–¯è¿‡ç¨‹æ¨¡å‹å¯èƒ½ä¸ç¨³å®šçš„å…¸å‹æƒ…å†µã€‚åœ¨æ’å€¼æ–‡çŒ®ä¸­åŸå§‹å¼€å‘çš„ç¨³å®šæ€§ç†è®ºçš„åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯¼å‡ºäº†å¯¹æ„Ÿå…´è¶£ç‚¹è¿›è¡Œè®¡ç®—çš„æ•°å€¼ç¨³å®šæ€§çš„å……åˆ†æ¡ä»¶å’ŒæŸäº›æƒ…å†µä¸‹çš„å¿…è¦æ¡ä»¶ã€‚å¯¹äºåœ°ç†ç©ºé—´å»ºæ¨¡ç­‰ä½ç»´ä»»åŠ¡ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è‡ªåŠ¨è®¡ç®—æ»¡è¶³è¿™äº›æ¡ä»¶çš„æ„Ÿå…´è¶£ç‚¹çš„æ–¹æ³•ã€‚

    Gaussian processes are frequently deployed as part of larger machine learning and decision-making systems, for instance in geospatial modeling, Bayesian optimization, or in latent Gaussian models. Within a system, the Gaussian process model needs to perform in a stable and reliable manner to ensure it interacts correctly with other parts of the system. In this work, we study the numerical stability of scalable sparse approximations based on inducing points. To do so, we first review numerical stability, and illustrate typical situations in which Gaussian process models can be unstable. Building on stability theory originally developed in the interpolation literature, we derive sufficient and in certain cases necessary conditions on the inducing points for the computations performed to be numerically stable. For low-dimensional tasks such as geospatial modeling, we propose an automated method for computing inducing points satisfying these conditions. This is done via a modification of t
    
[^34]: ä½¿ç”¨å¤šç¯å¢ƒæ–¹æ³•æ£€æµ‹è§‚æµ‹æ•°æ®ä¸­çš„éšå¼æ··æ·†

    Detecting hidden confounding in observational data using multiple environments. (arXiv:2205.13935v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2205.13935](http://arxiv.org/abs/2205.13935)

    ä½¿ç”¨ç‹¬ç«‹æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸‹çš„å¤šç¯å¢ƒæ–¹æ³•ï¼Œå¯ä»¥æ£€æµ‹è§‚æµ‹æ•°æ®ä¸­çš„æœªè§‚å¯Ÿåˆ°çš„æ··æ·†å› ç´ ï¼Œå¹¶æå‡ºäº†æµ‹è¯•ç‹¬ç«‹æ€§çš„ç¨‹åºã€‚

    

    åœ¨å› æœæ¨æ–­ä¸­ï¼Œå¸¸è§çš„å‡è®¾æ˜¯æ²¡æœ‰éšå¼æ··æ·†ã€‚ç„¶è€Œï¼Œåœ¨å•ä¸ªæ•°æ®é›†ä¸­ä¸èƒ½ç¡®å®šè¿™ä¸ªå‡è®¾é€šå¸¸æ˜¯ä¸å¯èƒ½çš„ã€‚åœ¨ç‹¬ç«‹çš„æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¸‹ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç§æ–¹æ³•æ¥åœ¨å¤šä¸ªæ¥è‡ªä¸åŒç¯å¢ƒçš„è§‚æµ‹æ•°æ®é›†ä¸­æ£€æµ‹æœªè§‚å¯Ÿåˆ°çš„æ··æ·†å› ç´ ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æµ‹è¯•å¯éªŒè¯çš„æ¡ä»¶ç‹¬ç«‹æ€§çš„ç†è®ºï¼Œè¿™ç§ç‹¬ç«‹æ€§ä»…å½“å­˜åœ¨æ··æ·†å› ç´ æ—¶æ‰ä¸å­˜åœ¨ï¼Œå¹¶æ£€æŸ¥äº†è¿åå…¶å‡è®¾çš„æƒ…å†µï¼šé€€åŒ–å’Œä¾èµ–æœºåˆ¶ä»¥åŠå¿ å®åº¦è¿åã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç¨‹åºæ¥æµ‹è¯•è¿™äº›ç‹¬ç«‹æ€§ï¼Œå¹¶ä½¿ç”¨åŸºäºçœŸå®ä¸–ç•Œæ•°æ®çš„åŠåˆæˆæ•°æ®å’Œæ¨¡æ‹Ÿç ”ç©¶ç ”ç©¶å…¶ç»éªŒæœ‰é™æ ·æœ¬è¡Œä¸ºã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæå‡ºçš„ç¨‹åºèƒ½å¤Ÿæ­£ç¡®é¢„æµ‹å­˜åœ¨éšå¼æ··æ·†ï¼Œç‰¹åˆ«æ˜¯å½“æ··æ·†åå·®å¾ˆå¤§æ—¶ã€‚

    A common assumption in causal inference from observational data is that there is no hidden confounding. Yet it is, in general, impossible to verify this assumption from a single dataset. Under the assumption of independent causal mechanisms underlying the data-generating process, we demonstrate a way to detect unobserved confounders when having multiple observational datasets coming from different environments. We present a theory for testable conditional independencies that are only absent when there is hidden confounding and examine cases where we violate its assumptions: degenerate & dependent mechanisms, and faithfulness violations. Additionally, we propose a procedure to test these independencies and study its empirical finite-sample behavior using simulation studies and semi-synthetic data based on a real-world dataset. In most cases, the proposed procedure correctly predicts the presence of hidden confounding, particularly when the confounding bias is large.
    
[^35]: ä¸åŒè¾“å…¥ç»´åº¦æ•°æ®é›†ä¹‹é—´çš„è¿ç§»å­¦ä¹ ï¼šçº¿æ€§å›å½’æƒ…å†µä¸‹çš„ç®—æ³•å’Œåˆ†æ

    Transfer-Learning Across Datasets with Different Input Dimensions: An Algorithm and Analysis for the Linear Regression Case. (arXiv:2202.05069v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.05069](http://arxiv.org/abs/2202.05069)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§é€‚ç”¨äºçº¿æ€§å›å½’æƒ…å†µçš„è¿ç§»å­¦ä¹ ç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿå°†æ–°æ•°æ®ä¸å†å²æ•°æ®ç›¸ç»“åˆï¼Œç‰¹åˆ«åœ¨æ–°æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å…·æœ‰ç›Šå¤„ï¼Œå¹¶ä¸”åœ¨å®éªŒéªŒè¯ä¸­è¡¨ç°å‡ºå¯¹è´Ÿè¿ç§»å­¦ä¹ çš„é²æ£’æ€§ã€‚

    

    éšç€æ–°ä¼ æ„Ÿå™¨å’Œç›‘æµ‹è®¾å¤‡çš„å‘å±•ï¼Œè¶Šæ¥è¶Šå¤šçš„æ•°æ®æºå¯ä»¥ä½œä¸ºæœºå™¨å­¦ä¹ æ¨¡å‹çš„è¾“å…¥ã€‚è¿™äº›æ•°æ®æ—¢å¯ä»¥å¸®åŠ©æé«˜æ¨¡å‹çš„å‡†ç¡®æ€§ï¼Œä½†å°†è¿™äº›æ–°è¾“å…¥ä¸å†å²æ•°æ®ç›¸ç»“åˆä»ç„¶æ˜¯ä¸€ä¸ªå°šæœªè¯¦ç»†ç ”ç©¶çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è¿ç§»å­¦ä¹ ç®—æ³•ï¼Œå°†æ–°æ•°æ®å’Œå†å²æ•°æ®ç»“åˆèµ·æ¥ï¼Œç‰¹åˆ«åœ¨æ–°æ•°æ®ç¨€ç¼ºçš„æƒ…å†µä¸‹å…·æœ‰ç›Šå¤„ã€‚æˆ‘ä»¬å°†é‡ç‚¹æ”¾åœ¨çº¿æ€§å›å½’æƒ…å†µä¸‹ï¼Œè¿™ä½¿å¾—æˆ‘ä»¬èƒ½å¤Ÿå¯¹è¯¥æ–¹æ³•çš„ç›Šå¤„è¿›è¡Œä¸¥æ ¼çš„ç†è®ºç ”ç©¶ã€‚æˆ‘ä»¬è¡¨æ˜æˆ‘ä»¬çš„æ–¹æ³•å¯¹è´Ÿè¿ç§»å­¦ä¹ æ˜¯å…·æœ‰é²æ£’æ€§çš„ï¼Œå¹¶é€šè¿‡çœŸå®å’Œæ¨¡æ‹Ÿæ•°æ®è¿›è¡Œäº†å®è¯éªŒè¯ã€‚

    With the development of new sensors and monitoring devices, more sources of data become available to be used as inputs for machine learning models. These can on the one hand help to improve the accuracy of a model. On the other hand however, combining these new inputs with historical data remains a challenge that has not yet been studied in enough detail. In this work, we propose a transfer-learning algorithm that combines the new and the historical data, that is especially beneficial when the new data is scarce. We focus the approach on the linear regression case, which allows us to conduct a rigorous theoretical study on the benefits of the approach. We show that our approach is robust against negative transfer-learning, and we confirm this result empirically with real and simulated data.
    
[^36]: å¯è§£é‡Šçš„åºåˆ—åˆ†ç±»é€šè¿‡åŸå‹è½¨è¿¹

    Interpretable Sequence Classification Via Prototype Trajectory. (arXiv:2007.01777v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.01777](http://arxiv.org/abs/2007.01777)

    ProtoryNetæ˜¯ä¸€ç§åŸºäºåŸå‹è½¨è¿¹çš„å¯è§£é‡Šæ·±åº¦ç¥ç»ç½‘ç»œï¼Œå®ƒé€šè¿‡æ•æ‰æ—¶é—´æ¨¡å¼å’ŒåŸå‹çš„è¿‘ä¼¼ç¨‹åº¦æ¥è¿›è¡Œæ–‡æœ¬åˆ†ç±»ï¼Œå¹¶å®ç°äº†ç›´è§‚å’Œç»†è‡´çš„æ¨ç†è¿‡ç¨‹è§£é‡Šã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„ç”¨äºæ–‡æœ¬åˆ†ç±»çš„å¯è§£é‡Šæ·±åº¦ç¥ç»ç½‘ç»œï¼Œç§°ä¸ºProtoryNetï¼Œå®ƒåŸºäºåŸå‹è½¨è¿¹çš„æ–°æ¦‚å¿µã€‚å—ç°ä»£è¯­è¨€å­¦ä¸­çš„åŸå‹ç†è®ºçš„å¯å‘ï¼ŒProtoryNeté€šè¿‡ä¸ºæ–‡æœ¬åºåˆ—ä¸­çš„æ¯ä¸ªå¥å­æ‰¾åˆ°æœ€ç›¸ä¼¼çš„åŸå‹ï¼Œå¹¶å°†æ¯ä¸ªå¥å­ä¸ç›¸åº”çš„æ´»åŠ¨åŸå‹çš„æ¥è¿‘ç¨‹åº¦è¾“å…¥åˆ°RNNä¸»å¹²ä¸­è¿›è¡Œé¢„æµ‹ã€‚ç„¶åï¼ŒRNNä¸»å¹²æ•æ‰åˆ°åŸå‹çš„æ—¶é—´æ¨¡å¼ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºåŸå‹è½¨è¿¹ã€‚åŸå‹è½¨è¿¹èƒ½å¤Ÿç›´è§‚è€Œç»†è‡´åœ°è§£é‡ŠRNNæ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ï¼Œç±»ä¼¼äºäººç±»åˆ†ææ–‡æœ¬çš„æ–¹å¼ã€‚æˆ‘ä»¬è¿˜è®¾è®¡äº†åŸå‹ä¿®å‰ªè¿‡ç¨‹ï¼Œä»¥å‡å°‘æ¨¡å‹ä½¿ç”¨çš„åŸå‹æ€»æ•°ï¼Œä»¥æé«˜è§£é‡Šæ€§ã€‚åœ¨å¤šä¸ªå…¬å…±æ•°æ®é›†ä¸Šçš„å®éªŒè¯æ˜ï¼ŒProtoryNetæ¯”åŸºçº¿çš„åŸºäºåŸå‹çš„æ·±åº¦ç¥ç»ç½‘ç»œæ›´å‡†ç¡®ï¼Œå¹¶å‡å°‘äº†ä¸ç°æœ‰æ¨¡å‹ç›¸æ¯”çš„æ€§èƒ½å·®è·ã€‚

    We propose a novel interpretable deep neural network for text classification, called ProtoryNet, based on a new concept of prototype trajectories. Motivated by the prototype theory in modern linguistics, ProtoryNet makes a prediction by finding the most similar prototype for each sentence in a text sequence and feeding an RNN backbone with the proximity of each sentence to the corresponding active prototype. The RNN backbone then captures the temporal pattern of the prototypes, which we refer to as prototype trajectories. Prototype trajectories enable intuitive and fine-grained interpretation of the reasoning process of the RNN model, in resemblance to how humans analyze texts. We also design a prototype pruning procedure to reduce the total number of prototypes used by the model for better interpretability. Experiments on multiple public data sets show that ProtoryNet is more accurate than the baseline prototype-based deep neural net and reduces the performance gap compared to state-o
    

