# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Stochastic Gradient Descent for Additive Nonparametric Regression](https://arxiv.org/abs/2401.00691) | æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè®­ç»ƒåŠ æ€§æ¨¡å‹çš„éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„å†…å­˜å­˜å‚¨å’Œè®¡ç®—è¦æ±‚ã€‚åœ¨è§„èŒƒå¾ˆå¥½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä»”ç»†é€‰æ‹©å­¦ä¹ ç‡ï¼Œå¯ä»¥å®ç°æœ€å°å’Œæœ€ä¼˜çš„é£é™©ã€‚ |
| [^2] | [Are Ensembles Getting Better all the Time?](https://arxiv.org/abs/2311.17885) | åªæœ‰å½“è€ƒè™‘çš„æŸå¤±å‡½æ•°ä¸ºå‡¸å‡½æ•°æ—¶ï¼Œé›†æˆæ¨¡å‹ä¸€ç›´åœ¨å˜å¾—æ›´å¥½ï¼Œå½“æŸå¤±å‡½æ•°ä¸ºéå‡¸å‡½æ•°æ—¶ï¼Œå¥½æ¨¡å‹çš„é›†æˆå˜å¾—æ›´å¥½ï¼Œåæ¨¡å‹çš„é›†æˆå˜å¾—æ›´ç³Ÿã€‚ |
| [^3] | [Generative Modelling of L\'{e}vy Area for High Order SDE Simulation.](http://arxiv.org/abs/2308.02452) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹LÃ©vyGANï¼Œç”¨äºç”Ÿæˆæ¡ä»¶äºå¸ƒæœ—å¢é‡çš„LÃ©vyåŒºåŸŸçš„è¿‘ä¼¼æ ·æœ¬ã€‚é€šè¿‡â€œæ¡¥ç¿»è½¬â€æ“ä½œï¼Œè¾“å‡ºçš„æ ·æœ¬å¯ä»¥ç²¾ç¡®åŒ¹é…æ‰€æœ‰å¥‡æ•°é˜¶çŸ©ï¼Œè§£å†³äº†éé«˜æ–¯æ€§è´¨ä¸‹çš„æŠ½æ ·å›°éš¾é—®é¢˜ã€‚ |
| [^4] | [The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing.](http://arxiv.org/abs/2302.01186) | è¯¥ç ”ç©¶æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç§©çŸ©é˜µæ„ŸçŸ¥é—®é¢˜æ—¶å…·æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚ |

# è¯¦ç»†

[^1]: æ·»åŠ éå‚æ•°å›å½’çš„éšæœºæ¢¯åº¦ä¸‹é™

    Stochastic Gradient Descent for Additive Nonparametric Regression

    [https://arxiv.org/abs/2401.00691](https://arxiv.org/abs/2401.00691)

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè®­ç»ƒåŠ æ€§æ¨¡å‹çš„éšæœºæ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œå…·æœ‰è‰¯å¥½çš„å†…å­˜å­˜å‚¨å’Œè®¡ç®—è¦æ±‚ã€‚åœ¨è§„èŒƒå¾ˆå¥½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡ä»”ç»†é€‰æ‹©å­¦ä¹ ç‡ï¼Œå¯ä»¥å®ç°æœ€å°å’Œæœ€ä¼˜çš„é£é™©ã€‚

    

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºè®­ç»ƒåŠ æ€§æ¨¡å‹çš„è¿­ä»£ç®—æ³•ï¼Œè¯¥ç®—æ³•å…·æœ‰è‰¯å¥½çš„å†…å­˜å­˜å‚¨å’Œè®¡ç®—è¦æ±‚ã€‚è¯¥ç®—æ³•å¯ä»¥çœ‹ä½œæ˜¯å¯¹ç»„ä»¶å‡½æ•°çš„æˆªæ–­åŸºæ‰©å±•çš„ç³»æ•°åº”ç”¨éšæœºæ¢¯åº¦ä¸‹é™çš„å‡½æ•°å¯¹åº”ç‰©ã€‚æˆ‘ä»¬è¯æ˜äº†å¾—åˆ°çš„ä¼°è®¡é‡æ»¡è¶³ä¸€ä¸ªå¥¥æ‹‰å…‹ä¸ç­‰å¼ï¼Œå…è®¸æ¨¡å‹é”™è¯¯è§„èŒƒã€‚åœ¨è§„èŒƒå¾ˆå¥½çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡åœ¨è®­ç»ƒçš„ä¸‰ä¸ªä¸åŒé˜¶æ®µä»”ç»†é€‰æ‹©å­¦ä¹ ç‡ï¼Œæˆ‘ä»¬è¯æ˜äº†å…¶é£é™©åœ¨æ•°æ®ç»´åº¦å’Œè®­ç»ƒæ ·æœ¬å¤§å°çš„ä¾èµ–æ–¹é¢æ˜¯æœ€å°å’Œæœ€ä¼˜çš„ã€‚é€šè¿‡åœ¨ä¸¤ä¸ªå®é™…æ•°æ®é›†ä¸Šå°†è¯¥æ–¹æ³•ä¸ä¼ ç»Ÿçš„åå‘æ‹Ÿåˆè¿›è¡Œæ¯”è¾ƒï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥è¯´æ˜äº†è®¡ç®—ä¼˜åŠ¿ã€‚

    This paper introduces an iterative algorithm for training additive models that enjoys favorable memory storage and computational requirements. The algorithm can be viewed as the functional counterpart of stochastic gradient descent, applied to the coefficients of a truncated basis expansion of the component functions. We show that the resulting estimator satisfies an oracle inequality that allows for model mis-specification. In the well-specified setting, by choosing the learning rate carefully across three distinct stages of training, we demonstrate that its risk is minimax optimal in terms of the dependence on the dimensionality of the data and the size of the training sample. We further illustrate the computational benefits by comparing the approach with traditional backfitting on two real-world datasets.
    
[^2]: é›†æˆæ¨¡å‹æ˜¯å¦ä¸€ç›´åœ¨ä¸æ–­è¿›æ­¥ï¼Ÿ

    Are Ensembles Getting Better all the Time?

    [https://arxiv.org/abs/2311.17885](https://arxiv.org/abs/2311.17885)

    åªæœ‰å½“è€ƒè™‘çš„æŸå¤±å‡½æ•°ä¸ºå‡¸å‡½æ•°æ—¶ï¼Œé›†æˆæ¨¡å‹ä¸€ç›´åœ¨å˜å¾—æ›´å¥½ï¼Œå½“æŸå¤±å‡½æ•°ä¸ºéå‡¸å‡½æ•°æ—¶ï¼Œå¥½æ¨¡å‹çš„é›†æˆå˜å¾—æ›´å¥½ï¼Œåæ¨¡å‹çš„é›†æˆå˜å¾—æ›´ç³Ÿã€‚

    

    é›†æˆæ–¹æ³•ç»“åˆäº†å‡ ä¸ªåŸºç¡€æ¨¡å‹çš„é¢„æµ‹ã€‚æœ¬ç ”ç©¶æ¢è®¨äº†æ˜¯å¦å§‹ç»ˆå°†æ›´å¤šæ¨¡å‹çº³å…¥é›†æˆä¼šæå‡å…¶å¹³å‡æ€§èƒ½ã€‚è¿™ä¸ªé—®é¢˜å–å†³äºæ‰€è€ƒè™‘çš„é›†æˆç±»å‹ï¼Œä»¥åŠé€‰æ‹©çš„é¢„æµ‹åº¦é‡ã€‚æˆ‘ä»¬ä¸“æ³¨äºæ‰€æœ‰é›†æˆæˆå‘˜è¢«é¢„æœŸè¡¨ç°ç›¸åŒçš„æƒ…å†µï¼Œè¿™æ˜¯å‡ ç§æµè¡Œæ–¹æ³•ï¼ˆå¦‚éšæœºæ£®æ—æˆ–æ·±åº¦é›†æˆï¼‰çš„æƒ…å†µã€‚åœ¨è¿™ç§è®¾å®šä¸‹ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œåªæœ‰å½“è€ƒè™‘çš„æŸå¤±å‡½æ•°ä¸ºå‡¸å‡½æ•°æ—¶ï¼Œé›†æˆæ‰ä¼šä¸€ç›´å˜å¾—æ›´å¥½ã€‚æ›´å…·ä½“åœ°è¯´ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œé›†æˆçš„å¹³å‡æŸå¤±æ˜¯æ¨¡å‹æ•°é‡çš„å‡å‡½æ•°ã€‚å½“æŸå¤±å‡½æ•°ä¸ºéå‡¸å‡½æ•°æ—¶ï¼Œæˆ‘ä»¬å±•ç¤ºäº†ä¸€ç³»åˆ—ç»“æœï¼Œå¯ä»¥æ€»ç»“ä¸ºï¼šå¥½æ¨¡å‹çš„é›†æˆä¼šå˜å¾—æ›´å¥½ï¼Œåæ¨¡å‹çš„é›†æˆä¼šå˜å¾—æ›´ç³Ÿã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬è¯æ˜äº†å…³äºå°¾æ¦‚ç‡å•è°ƒæ€§çš„æ–°ç»“æœã€‚

    arXiv:2311.17885v2 Announce Type: replace-cross  Abstract: Ensemble methods combine the predictions of several base models. We study whether or not including more models always improves their average performance. This question depends on the kind of ensemble considered, as well as the predictive metric chosen. We focus on situations where all members of the ensemble are a priori expected to perform as well, which is the case of several popular methods such as random forests or deep ensembles. In this setting, we show that ensembles are getting better all the time if, and only if, the considered loss function is convex. More precisely, in that case, the average loss of the ensemble is a decreasing function of the number of models. When the loss function is nonconvex, we show a series of results that can be summarised as: ensembles of good models keep getting better, and ensembles of bad models keep getting worse. To this end, we prove a new result on the monotonicity of tail probabiliti
    
[^3]: å¯¹é«˜é˜¶SDEæ¨¡æ‹Ÿçš„LÃ©vyåŒºåŸŸè¿›è¡Œç”Ÿæˆå»ºæ¨¡

    Generative Modelling of L\'{e}vy Area for High Order SDE Simulation. (arXiv:2308.02452v1 [stat.ML])

    [http://arxiv.org/abs/2308.02452](http://arxiv.org/abs/2308.02452)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹LÃ©vyGANï¼Œç”¨äºç”Ÿæˆæ¡ä»¶äºå¸ƒæœ—å¢é‡çš„LÃ©vyåŒºåŸŸçš„è¿‘ä¼¼æ ·æœ¬ã€‚é€šè¿‡â€œæ¡¥ç¿»è½¬â€æ“ä½œï¼Œè¾“å‡ºçš„æ ·æœ¬å¯ä»¥ç²¾ç¡®åŒ¹é…æ‰€æœ‰å¥‡æ•°é˜¶çŸ©ï¼Œè§£å†³äº†éé«˜æ–¯æ€§è´¨ä¸‹çš„æŠ½æ ·å›°éš¾é—®é¢˜ã€‚

    

    ä¼—æ‰€å‘¨çŸ¥ï¼Œå½“æ•°å€¼æ¨¡æ‹ŸSDEçš„è§£æ—¶ï¼Œè¦å®ç°å¼ºæ”¶æ•›é€Ÿç‡è¶…è¿‡O(\sqrt{h})ï¼ˆå…¶ä¸­hä¸ºæ­¥é•¿ï¼‰ï¼Œéœ€è¦ä½¿ç”¨æŸäº›å¸ƒæœ—è¿åŠ¨çš„è¿­ä»£ç§¯åˆ†ï¼Œé€šå¸¸ç§°ä¸ºå…¶â€œLÃ©vyåŒºåŸŸâ€ã€‚ç„¶è€Œï¼Œç”±äºå…¶éé«˜æ–¯æ€§è´¨ï¼Œå¯¹äºdç»´å¸ƒæœ—è¿åŠ¨ï¼ˆd>2ï¼‰ï¼Œç›®å‰æ²¡æœ‰å¿«é€Ÿè¿‘ä¼¼æŠ½æ ·ç®—æ³•ã€‚æœ¬æ–‡æå‡ºäº†LÃ©vyGANï¼Œä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„æ¨¡å‹ï¼Œç”¨äºç”Ÿæˆæ¡ä»¶äºå¸ƒæœ—å¢é‡çš„LÃ©vyåŒºåŸŸçš„è¿‘ä¼¼æ ·æœ¬ã€‚é€šè¿‡â€œæ¡¥ç¿»è½¬â€æ“ä½œï¼Œè¾“å‡ºçš„æ ·æœ¬å¯ä»¥ç²¾ç¡®åŒ¹é…æ‰€æœ‰å¥‡æ•°é˜¶çŸ©ã€‚æˆ‘ä»¬çš„ç”Ÿæˆå™¨é‡‡ç”¨ç»è¿‡é‡èº«å®šåˆ¶çš„GNN-inspiredæ¶æ„ï¼Œå¼ºåˆ¶è¾“å‡ºåˆ†å¸ƒä¸æ¡ä»¶å˜é‡ä¹‹é—´çš„æ­£ç¡®ä¾èµ–ç»“æ„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ç»“åˆäº†åŸºäºç‰¹å¾å‡½æ•°çš„æ•°å­¦åŸç†çš„åˆ¤åˆ«æ€§å½’ä¸€åŒ–æ“ä½œã€‚

    It is well known that, when numerically simulating solutions to SDEs, achieving a strong convergence rate better than O(\sqrt{h}) (where h is the step size) requires the use of certain iterated integrals of Brownian motion, commonly referred to as its "L\'{e}vy areas". However, these stochastic integrals are difficult to simulate due to their non-Gaussian nature and for a d-dimensional Brownian motion with d > 2, no fast almost-exact sampling algorithm is known.  In this paper, we propose L\'{e}vyGAN, a deep-learning-based model for generating approximate samples of L\'{e}vy area conditional on a Brownian increment. Due to our "Bridge-flipping" operation, the output samples match all joint and conditional odd moments exactly. Our generator employs a tailored GNN-inspired architecture, which enforces the correct dependency structure between the output distribution and the conditioning variable. Furthermore, we incorporate a mathematically principled characteristic-function based discrim
    
[^4]: é¢„æ¡ä»¶å¯¹è¶…å‚åŒ–ä½ç§©çŸ©é˜µæ„ŸçŸ¥çš„å½±å“

    The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01186](http://arxiv.org/abs/2302.01186)

    è¯¥ç ”ç©¶æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç§©çŸ©é˜µæ„ŸçŸ¥é—®é¢˜æ—¶å…·æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚

    

    æœ¬æ–‡æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•æ¥è§£å†³ä½ç§©çŸ©é˜µæ„ŸçŸ¥ä¸­çŸ©é˜µå¯èƒ½ç—…æ€ä»¥åŠçœŸå®ç§©æœªçŸ¥çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è¶…å‚å¼è¡¨ç¤ºï¼Œä»ä¸€ä¸ªå°çš„éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œé€šè¿‡ä½¿ç”¨ç‰¹å®šå½¢å¼çš„é˜»å°¼é¢„æ¡ä»¶æ¢¯åº¦ä¸‹é™æ¥å¯¹æŠ—è¶…å‚åŒ–å’Œç—…æ€æ›²ç‡çš„å½±å“ã€‚ä¸åŸºå‡†æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰ç›¸æ¯”ï¼Œå°½ç®¡é¢„å¤„ç†éœ€è¦è½»å¾®çš„è®¡ç®—å¼€é”€ï¼Œä½†ScaledGDï¼ˆğœ†ï¼‰åœ¨é¢å¯¹ç—…æ€é—®é¢˜æ—¶è¡¨ç°å‡ºäº†å‡ºè‰²çš„é²æ£’æ€§ã€‚åœ¨é«˜æ–¯è®¾è®¡ä¸‹ï¼ŒScaledGD($\lambda$) ä¼šåœ¨ä»…è¿­ä»£æ•°å¯¹æ•°çº§åˆ«çš„æƒ…å†µä¸‹ï¼Œä»¥çº¿æ€§é€Ÿç‡æ”¶æ•›åˆ°çœŸå®çš„ä½ç§©çŸ©é˜µã€‚

    We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi
    

