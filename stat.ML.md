# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Classify and Generate Reciprocally: Simultaneous Positive-Unlabelled Learning and Conditional Generation with Extra Data](https://arxiv.org/abs/2006.07841) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŒæ—¶åˆ©ç”¨æ­£æ•°æ®-æ— æ ‡ç­¾å­¦ä¹ å’Œæœ‰æ¡ä»¶ç”Ÿæˆçš„è®­ç»ƒæ¡†æ¶ï¼Œä»¥åŠé¢å¤–æ— æ ‡ç­¾æ•°æ®çš„æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨ä¸€ä¸ªå¯¹å™ªå£°æ ‡ç­¾å…·æœ‰é²æ£’æ€§çš„åˆ†ç±»å™¨å™ªå£°ä¸å˜æœ‰æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥æé«˜PUåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨PUåˆ†ç±»å™¨é¢„æµ‹çš„æ ‡ç­¾å’Œé¢å¤–æ•°æ®æ¥å¸®åŠ©ç”Ÿæˆã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ |
| [^2] | [Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences.](http://arxiv.org/abs/2309.03791) | æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ARMOR_Dæ¥åŠ å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ï¼Œè¯¥æ–¹æ³•åŸºäºæœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–å·®å¼‚ï¼Œé€šè¿‡åœ¨åˆ†å¸ƒçš„é‚»åŸŸä¸Šè¿›è¡Œæœ€å¤§åŒ–æœŸæœ›æŸå¤±æ¥å®ç°ã€‚å®éªŒè¯æ˜ï¼ŒARMOR_Dæ–¹æ³•åœ¨æ¶æ„è½¯ä»¶æ£€æµ‹å’Œå›¾åƒè¯†åˆ«åº”ç”¨ä¸­èƒ½å¤Ÿä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„é²æ£’æ€§æ–¹é¢å…·æœ‰è¾ƒå¥½çš„æ•ˆæœã€‚ |
| [^3] | [Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning.](http://arxiv.org/abs/2308.01358) | æœ¬æ–‡ç ”ç©¶äº†å‹ç¼©å¯¹åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒçš„æ— åå‹ç¼©æ“ä½œç¬¦çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚é’ˆå¯¹æœ€å°äºŒä¹˜å›å½’ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªéšæœºé€¼è¿‘ç®—æ³•ï¼Œå¹¶è€ƒè™‘äº†éšæœºåœºçš„ä¸€èˆ¬å‡è®¾å’Œå™ªå£°åæ–¹å·®çš„é™åˆ¶ï¼Œä»¥åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ã€‚ |
| [^4] | [Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift.](http://arxiv.org/abs/2302.10160) | è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å…³äºæ ¸å²­å›å½’çš„åå˜é‡è½¬ç§»ç­–ç•¥ï¼Œé€šè¿‡ä½¿ç”¨ä¼ªæ ‡ç­¾è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒç‰¹å¾åˆ†å¸ƒä¸‹çš„å­¦ä¹ ï¼Œå®ç°å‡æ–¹è¯¯å·®æœ€å°åŒ–ã€‚ |

# è¯¦ç»†

[^1]: åŒæ—¶è¿›è¡Œæ­£æ•°æ®-æ— æ ‡ç­¾å­¦ä¹ å’Œæœ‰æ¡ä»¶ç”Ÿæˆï¼Œåˆ©ç”¨é¢å¤–æ•°æ®æ¥åˆ†ç±»å’Œç”Ÿæˆ

    Classify and Generate Reciprocally: Simultaneous Positive-Unlabelled Learning and Conditional Generation with Extra Data

    [https://arxiv.org/abs/2006.07841](https://arxiv.org/abs/2006.07841)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŒæ—¶åˆ©ç”¨æ­£æ•°æ®-æ— æ ‡ç­¾å­¦ä¹ å’Œæœ‰æ¡ä»¶ç”Ÿæˆçš„è®­ç»ƒæ¡†æ¶ï¼Œä»¥åŠé¢å¤–æ— æ ‡ç­¾æ•°æ®çš„æ–¹æ³•ã€‚é€šè¿‡ä½¿ç”¨ä¸€ä¸ªå¯¹å™ªå£°æ ‡ç­¾å…·æœ‰é²æ£’æ€§çš„åˆ†ç±»å™¨å™ªå£°ä¸å˜æœ‰æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œæ¥æé«˜PUåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå¹¶åˆ©ç”¨PUåˆ†ç±»å™¨é¢„æµ‹çš„æ ‡ç­¾å’Œé¢å¤–æ•°æ®æ¥å¸®åŠ©ç”Ÿæˆã€‚å®éªŒç»“æœè¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

    

    åœ¨è®¸å¤šæœºå™¨å­¦ä¹ é—®é¢˜ä¸­ï¼Œæ ‡è®°ç±»åˆ«æ•°æ®çš„ç¨€ç¼ºæ€§æ˜¯ä¸€ä¸ªæ™®éå­˜åœ¨çš„ç“¶é¢ˆã€‚è™½ç„¶å­˜åœ¨ä¸°å¯Œçš„æ— æ ‡ç­¾æ•°æ®å¹¶æä¾›æ½œåœ¨çš„è§£å†³æ–¹æ¡ˆï¼Œä½†åˆ©ç”¨å®ƒä»¬æ˜¯éå¸¸å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚æœ¬æ–‡é€šè¿‡åŒæ—¶åˆ©ç”¨æ­£æ•°æ®-æ— æ ‡ç­¾ï¼ˆPositive-Unlabeledï¼ŒPUï¼‰åˆ†ç±»å’Œæœ‰æ¡ä»¶ç”Ÿæˆï¼Œä»¥åŠé¢å¤–çš„æ— æ ‡ç­¾æ•°æ®ï¼Œè§£å†³äº†è¿™ä¸ªé—®é¢˜ã€‚ç‰¹åˆ«åœ°ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„è®­ç»ƒæ¡†æ¶ï¼Œä½¿å¾—åœ¨é¢å¯¹é¢å¤–æ•°æ®ï¼ˆå°¤å…¶æ˜¯åˆ†å¸ƒå¤–çš„æ— æ ‡ç­¾æ•°æ®ï¼‰æ—¶ï¼ŒåŒæ—¶è¿›è¡ŒPUåˆ†ç±»å’Œæœ‰æ¡ä»¶ç”Ÿæˆæˆä¸ºå¯èƒ½ï¼Œé€šè¿‡æ¢ç´¢å®ƒä»¬ä¹‹é—´çš„ç›¸äº’ä½œç”¨ï¼š1ï¼‰é€šè¿‡ä¸€ä¸ªå¯¹å™ªå£°æ ‡ç­¾å…·æœ‰é²æ£’æ€§çš„æ–°å‹åˆ†ç±»å™¨å™ªå£°ä¸å˜æœ‰æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆClassifier-Noise-Invariant Conditional GANï¼ŒCNI-CGANï¼‰æ¥æé«˜PUåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œ2ï¼‰åˆ©ç”¨PUåˆ†ç±»å™¨é¢„æµ‹çš„æ ‡ç­¾å’Œé¢å¤–æ•°æ®æ¥å¸®åŠ©ç”Ÿæˆã€‚ä»ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†CNI-CGANçš„æœ€ä¼˜æ¡ä»¶ï¼Œå¹¶åœ¨å®éªŒä¸­é€šè¿‡å¹¿æ³›çš„è¯„ä¼°æ¥éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ã€‚

    The scarcity of class-labeled data is a ubiquitous bottleneck in many machine learning problems. While abundant unlabeled data typically exist and provide a potential solution, it is highly challenging to exploit them. In this paper, we address this problem by leveraging Positive-Unlabeled~(PU) classification and the conditional generation with extra unlabeled data \emph{simultaneously}. In particular, we present a novel training framework to jointly target both PU classification and conditional generation when exposed to extra data, especially out-of-distribution unlabeled data, by exploring the interplay between them: 1) enhancing the performance of PU classifiers with the assistance of a novel Classifier-Noise-Invariant Conditional GAN~(CNI-CGAN) that is robust to noisy labels, 2) leveraging extra data with predicted labels from a PU classifier to help the generation. Theoretically, we prove the optimal condition of CNI-CGAN, and experimentally, we conducted extensive evaluations on
    
[^2]: ä½¿ç”¨æœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–å·®å¼‚æ¥æé«˜å¯¹æŠ—æ€§é²æ£’æ·±åº¦å­¦ä¹ 

    Adversarially Robust Deep Learning with Optimal-Transport-Regularized Divergences. (arXiv:2309.03791v1 [cs.LG])

    [http://arxiv.org/abs/2309.03791](http://arxiv.org/abs/2309.03791)

    æœ¬è®ºæ–‡ä»‹ç»äº†ä¸€ç§æ–°çš„æ–¹æ³•ARMOR_Dæ¥åŠ å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¯¹æŠ—é²æ£’æ€§ï¼Œè¯¥æ–¹æ³•åŸºäºæœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–å·®å¼‚ï¼Œé€šè¿‡åœ¨åˆ†å¸ƒçš„é‚»åŸŸä¸Šè¿›è¡Œæœ€å¤§åŒ–æœŸæœ›æŸå¤±æ¥å®ç°ã€‚å®éªŒè¯æ˜ï¼ŒARMOR_Dæ–¹æ³•åœ¨æ¶æ„è½¯ä»¶æ£€æµ‹å’Œå›¾åƒè¯†åˆ«åº”ç”¨ä¸­èƒ½å¤Ÿä¼˜äºç°æœ‰æ–¹æ³•ï¼Œåœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„é²æ£’æ€§æ–¹é¢å…·æœ‰è¾ƒå¥½çš„æ•ˆæœã€‚

    

    æˆ‘ä»¬å¼•å…¥äº†ARMOR_Dæ–¹æ³•ä½œä¸ºå¢å¼ºæ·±åº¦å­¦ä¹ æ¨¡å‹å¯¹æŠ—æ€§é²æ£’æ€§çš„åˆ›æ–°æ–¹æ³•ã€‚è¿™äº›æ–¹æ³•åŸºäºä¸€ç§æ–°çš„æœ€ä¼˜ä¼ è¾“æ­£åˆ™åŒ–å·®å¼‚ç±»ï¼Œé€šè¿‡ä¿¡æ¯å·®å¼‚å’Œæœ€ä¼˜ä¼ è¾“æˆæœ¬ä¹‹é—´çš„infimalå·ç§¯æ„å»ºã€‚æˆ‘ä»¬ä½¿ç”¨è¿™äº›æ–¹æ³•æ¥å¢å¼ºå¯¹æŠ—æ€§é²æ£’æ€§ï¼Œé€šè¿‡åœ¨åˆ†å¸ƒçš„é‚»åŸŸä¸Šæœ€å¤§åŒ–æœŸæœ›æŸå¤±ï¼Œè¿™è¢«ç§°ä¸ºåˆ†å¸ƒé²æ£’ä¼˜åŒ–æŠ€æœ¯ã€‚ä½œä¸ºæ„å»ºå¯¹æŠ—æ ·æœ¬çš„å·¥å…·ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å…è®¸æ ·æœ¬æ ¹æ®æœ€ä¼˜ä¼ è¾“æˆæœ¬è¿›è¡Œä¼ è¾“ï¼Œå¹¶æ ¹æ®ä¿¡æ¯å·®å¼‚è¿›è¡Œé‡æ–°åŠ æƒã€‚æˆ‘ä»¬åœ¨æ¶æ„è½¯ä»¶æ£€æµ‹å’Œå›¾åƒè¯†åˆ«åº”ç”¨ä¸Šè¯æ˜äº†æˆ‘ä»¬æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶å‘ç°åœ¨å¢å¼ºå¯¹æŠ—æ”»å‡»é²æ£’æ€§æ–¹é¢ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå®ƒä¼˜äºç°æœ‰æ–¹æ³•ã€‚ARMOR_Dåœ¨FGSMæ”»å‡»ä¸‹çš„robustifiedå‡†ç¡®ç‡è¾¾åˆ°98.29%ï¼Œåœ¨å…¶ä»–æ”»å‡»ä¸‹è¾¾åˆ°98.18%ã€‚

    We introduce the $ARMOR_D$ methods as novel approaches to enhancing the adversarial robustness of deep learning models. These methods are based on a new class of optimal-transport-regularized divergences, constructed via an infimal convolution between an information divergence and an optimal-transport (OT) cost. We use these as tools to enhance adversarial robustness by maximizing the expected loss over a neighborhood of distributions, a technique known as distributionally robust optimization. Viewed as a tool for constructing adversarial samples, our method allows samples to be both transported, according to the OT cost, and re-weighted, according to the information divergence. We demonstrate the effectiveness of our method on malware detection and image recognition applications and find that, to our knowledge, it outperforms existing methods at enhancing the robustness against adversarial attacks. $ARMOR_D$ yields the robustified accuracy of $98.29\%$ against $FGSM$ and $98.18\%$ aga
    
[^3]: å‹ç¼©å’Œåˆ†å¸ƒå¼æœ€å°äºŒä¹˜å›å½’ï¼šæ”¶æ•›é€Ÿåº¦åŠå…¶åœ¨è”é‚¦å­¦ä¹ ä¸­çš„åº”ç”¨

    Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])

    [http://arxiv.org/abs/2308.01358](http://arxiv.org/abs/2308.01358)

    æœ¬æ–‡ç ”ç©¶äº†å‹ç¼©å¯¹åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒçš„æ— åå‹ç¼©æ“ä½œç¬¦çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚é’ˆå¯¹æœ€å°äºŒä¹˜å›å½’ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªéšæœºé€¼è¿‘ç®—æ³•ï¼Œå¹¶è€ƒè™‘äº†éšæœºåœºçš„ä¸€èˆ¬å‡è®¾å’Œå™ªå£°åæ–¹å·®çš„é™åˆ¶ï¼Œä»¥åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†åœ¨æœºå™¨å­¦ä¹ ä¸­å¹¿æ³›åº”ç”¨çš„åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­ï¼Œå‹ç¼©å¯¹éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å‡ ç§æ— åå‹ç¼©æ“ä½œç¬¦ä¹‹é—´çš„æ”¶æ•›é€Ÿåº¦å·®å¼‚ï¼Œè¿™äº›æ“ä½œç¬¦éƒ½æ»¡è¶³ç›¸åŒçš„æ–¹å·®æ¡ä»¶ï¼Œä»è€Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæœ€å°äºŒä¹˜å›å½’ï¼ˆLSRï¼‰çš„æƒ…å†µï¼Œå¹¶åˆ†æäº†ä¸€ä¸ªä¾èµ–äºéšæœºåœºçš„æœ€å°äºŒä¹˜å›å½’çš„éšæœºé€¼è¿‘ç®—æ³•ã€‚æˆ‘ä»¬å¯¹éšæœºåœºçš„ä¸€èˆ¬æ€§å‡è®¾è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼ˆç‰¹åˆ«æ˜¯æœŸæœ›çš„HÃ¶lderæ­£åˆ™æ€§ï¼‰å¹¶å¯¹å™ªå£°åæ–¹å·®è¿›è¡Œäº†é™åˆ¶ï¼Œä»¥ä¾¿åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ï¼ŒåŒ…æ‹¬å‹ç¼©ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç»“æœæ‰©å±•åˆ°è”é‚¦å­¦ä¹ çš„æƒ…å†µä¸‹ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å¯¹åŠ æ€§å™ªå£°çš„åæ–¹å·®ğ–¢ğ– ğ–­ğ–¨ğ– å¯¹æ”¶æ•›æ€§çš„å½±å“ã€‚

    In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H\"older regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning.  More formally, we highlight the impact on the convergence of the covariance $\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced 
    
[^4]: æ ¸å²­å›å½’ä¸‹ä¼ªæ ‡ç­¾çš„åå˜é‡è½¬ç§»ç­–ç•¥

    Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift. (arXiv:2302.10160v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2302.10160](http://arxiv.org/abs/2302.10160)

    è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§å…³äºæ ¸å²­å›å½’çš„åå˜é‡è½¬ç§»ç­–ç•¥ï¼Œé€šè¿‡ä½¿ç”¨ä¼ªæ ‡ç­¾è¿›è¡Œæ¨¡å‹é€‰æ‹©ï¼Œèƒ½å¤Ÿé€‚åº”ä¸åŒç‰¹å¾åˆ†å¸ƒä¸‹çš„å­¦ä¹ ï¼Œå®ç°å‡æ–¹è¯¯å·®æœ€å°åŒ–ã€‚

    

    æˆ‘ä»¬æå‡ºå¹¶åˆ†æäº†ä¸€ç§åŸºäºåå˜é‡è½¬ç§»çš„æ ¸å²­å›å½’æ–¹æ³•ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯åœ¨ç›®æ ‡åˆ†å¸ƒä¸Šå­¦ä¹ ä¸€ä¸ªå‡æ–¹è¯¯å·®æœ€å°çš„å›å½’å‡½æ•°ï¼ŒåŸºäºä»ç›®æ ‡åˆ†å¸ƒé‡‡æ ·çš„æœªæ ‡è®°æ•°æ®å’Œå¯èƒ½å…·æœ‰ä¸åŒç‰¹å¾åˆ†å¸ƒçš„å·²æ ‡è®°æ•°æ®ã€‚æˆ‘ä»¬å°†å·²æ ‡è®°æ•°æ®åˆ†æˆä¸¤ä¸ªå­é›†ï¼Œå¹¶åˆ†åˆ«è¿›è¡Œæ ¸å²­å›å½’ï¼Œä»¥è·å¾—å€™é€‰æ¨¡å‹é›†åˆå’Œä¸€ä¸ªå¡«å……æ¨¡å‹ã€‚æˆ‘ä»¬ä½¿ç”¨åè€…å¡«å……ç¼ºå¤±çš„æ ‡ç­¾ï¼Œç„¶åç›¸åº”åœ°é€‰æ‹©æœ€ä½³çš„å€™é€‰æ¨¡å‹ã€‚æˆ‘ä»¬çš„éæ¸è¿‘æ€§è¿‡é‡é£é™©ç•Œè¡¨æ˜ï¼Œåœ¨ç›¸å½“ä¸€èˆ¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ä¼°è®¡å™¨èƒ½å¤Ÿé€‚åº”ç›®æ ‡åˆ†å¸ƒä»¥åŠåå˜é‡è½¬ç§»çš„ç»“æ„ã€‚å®ƒèƒ½å¤Ÿå®ç°æ¸è¿‘æ­£æ€è¯¯å·®ç‡ç›´åˆ°å¯¹æ•°å› å­çš„æœ€å°æé™ä¼˜åŒ–ã€‚åœ¨æ¨¡å‹é€‰æ‹©ä¸­ä½¿ç”¨ä¼ªæ ‡ç­¾ä¸ä¼šäº§ç”Ÿä¸»è¦è´Ÿé¢å½±å“ã€‚

    We develop and analyze a principled approach to kernel ridge regression under covariate shift. The goal is to learn a regression function with small mean squared error over a target distribution, based on unlabeled data from there and labeled data that may have a different feature distribution. We propose to split the labeled data into two subsets and conduct kernel ridge regression on them separately to obtain a collection of candidate models and an imputation model. We use the latter to fill the missing labels and then select the best candidate model accordingly. Our non-asymptotic excess risk bounds show that in quite general scenarios, our estimator adapts to the structure of the target distribution as well as the covariate shift. It achieves the minimax optimal error rate up to a logarithmic factor. The use of pseudo-labels in model selection does not have major negative impacts.
    

