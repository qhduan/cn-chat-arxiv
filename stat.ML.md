# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures.](http://arxiv.org/abs/2306.03801) | æœ¬ç ”ç©¶æå‡ºäº†ä½¿ç”¨ç­¾åæ¡ç æ¥ç¨³å®šå‘é‡åŒ–å¤šå‚æ•°æŒä¹…åŒè°ƒï¼Œå°†å¤šå‚æ•°æŒä¹…åŒè°ƒçš„ä¸°å¯Œä¿¡æ¯å’Œç¨³å®šå‘é‡åŒ–çš„ä¼˜åŠ¿ç›¸ç»“åˆã€‚ |
| [^2] | [Asymptotics of Bayesian Uncertainty Estimation in Random Features Regression.](http://arxiv.org/abs/2306.03783) | è®ºæ–‡æ¯”è¾ƒå’Œå¯¹æ¯”äº†åéªŒé¢„æµ‹åˆ†å¸ƒå’Œæœ€å¤§åéªŒä¼°è®¡çš„é£é™©ï¼Œé‡ç‚¹å…³æ³¨äº†æ¨¡å‹ç»´åº¦å¢é•¿é€Ÿåº¦å¤§äºä»»ä½•å¸¸æ•°å€çš„æ ·æœ¬æ•°æ—¶å®ƒä»¬ä¹‹é—´çš„æ¸è¿‘ä¸€è‡´æ€§ã€‚æ•°å€¼æ¨¡æ‹Ÿè¡¨æ˜è¿™ä¸¤ä¸ªæ•°é‡åœ¨é™å®šç»´åº¦ä¸Šå…·æœ‰é«˜æ–¯æ³¢åŠ¨ï¼Œå¹¶è¡¨ç°å‡ºç›¸ä¼¼çš„å±æ€§ã€‚ |
| [^3] | [Graph Classification Gaussian Processes via Spectral Features.](http://arxiv.org/abs/2306.03770) | æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨è°±ç‰¹å¾çš„é«˜æ–¯è¿‡ç¨‹æ¨¡å‹æ¥è§£å†³å›¾åˆ†ç±»é—®é¢˜ï¼Œå³ä½¿æ˜¯åŸºäºèŠ‚ç‚¹ç‰¹å¾ä¿¡å·åœ¨å›¾è°±ä¸Šèƒ½é‡åˆ†å¸ƒè¿™æ ·ç®€å•çš„æ–¹æ³•ä¹Ÿæœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚åŒæ—¶ï¼Œæ›´å¤æ‚çš„å˜ä½“ä½¿ç”¨è°±å›¾å°æ³¢æ»¤æ³¢å™¨å¯ä»¥åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè·å¾—æ”¹è¿›ï¼Œè€Œä¸¤ç§æ¨¡å‹éƒ½èƒ½å¤Ÿäº§ç”Ÿè®¡ç®—ä¼°è®¡ï¼Œèƒ½å¤Ÿå¯é åœ°åšå‡ºå†³ç­–ã€‚ |
| [^4] | [Human-imperceptible, Machine-recognizable Images.](http://arxiv.org/abs/2306.03679) | æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éšç§ä¿æŠ¤å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡åŠ å¯†å›¾åƒå®ç°äººç±»ä¸å¯æ„ŸçŸ¥ä½†æœºå™¨å¯è¯†åˆ«ï¼Œå¹¶ä½¿ç”¨ç»è¿‡æœ€å°é€‚é…çš„è§†è§‰è½¬æ¢å™¨å®Œæˆè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•å‡†ç¡®æ€§ä¸ç«äº‰æ–¹æ³•ç›¸å½“ã€‚ |
| [^5] | [Provable convergence guarantees for black-box variational inference.](http://arxiv.org/abs/2306.03638) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯†é›†é«˜æ–¯å˜åˆ†æ—çš„æ¢¯åº¦ä¼°è®¡å™¨ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šä½¿ç”¨è¿‘ç«¯å’ŒæŠ•å½±éšæœºæ¢¯åº¦ä¸‹é™ï¼Œæä¾›äº†é»‘ç›’å˜åˆ†æ¨æ–­æ”¶æ•›äºé€¼çœŸæ¨æ–­é—®é¢˜çš„ç¬¬ä¸€ä¸ªä¸¥æ ¼ä¿è¯ã€‚ |
| [^6] | [Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning.](http://arxiv.org/abs/2306.03625) | æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¬å¹³ä¸”å¥å£®çš„å¼‚è´¨æ²»ç–—æ•ˆæœçš„ä¼°è®¡æ¡†æ¶ï¼Œå¯ä»¥åœ¨å…¬å¹³çº¦æŸä¸‹éå‚æ•°åœ°ä¼°è®¡ï¼Œå¹¶å¯ç”¨äºæƒè¡¡å…¬å¹³å’Œæœ€å¤§ç¦åˆ©ä¹‹é—´çš„å…³ç³»ã€‚ |
| [^7] | [Entropic covariance models.](http://arxiv.org/abs/2306.03590) | æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„çº¿æ€§çº¦æŸåæ–¹å·®çŸ©é˜µå˜æ¢çš„æ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¼°è®¡æ–¹æ³•ï¼Œè§£å†³äº†ä¸€ä¸ªå‡¸é—®é¢˜ï¼Œå…è®¸ç›¸å¯¹ç®€å•çš„æ¸è¿‘æ€§å’Œæœ‰é™æ ·æœ¬åˆ†æã€‚ç ”ç©¶çš„é‡ç‚¹æ˜¯å…³äºå»ºæ¨¡ç›¸å…³çŸ©é˜µå’Œç¨€ç–æ€§æ–¹é¢çš„å†…å®¹ã€‚ |
| [^8] | [How does over-squashing affect the power of GNNs?.](http://arxiv.org/abs/2306.03589) | æœ¬æ–‡é€šè¿‡æµ‹é‡èŠ‚ç‚¹ä¹‹é—´æˆå¯¹äº¤äº’çš„æ°´å¹³ï¼Œæä¾›äº†ä¸¥æ ¼çš„åˆ†æï¼Œä»¥ç¡®å®šå…·æœ‰ä¸€å®šå®¹é‡çš„MPNNå¯ä»¥å­¦ä¹ å“ªäº›èŠ‚ç‚¹ç‰¹å¾çš„å‡½æ•°ç±»åˆ«ã€‚ç»“æœè¡¨æ˜ï¼Œä¸ºäº†ä¿è¯èŠ‚ç‚¹å¯¹ä¹‹é—´çš„å……åˆ†é€šä¿¡ï¼ŒMPNNçš„å®¹é‡å¿…é¡»æ˜¯... |
| [^9] | [L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference.](http://arxiv.org/abs/2306.03580) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º L-C2ST çš„åŸºäºæœ¬åœ°è¯Šæ–­å®ç°æ¨¡æ‹Ÿæ¨æ–­ä¸­åéªŒè¿‘ä¼¼çš„æ–°æ–¹æ³•ï¼Œå…¶å¯ä»¥åœ¨ä»»ä½•ç»™å®šçš„è§‚æµ‹ä¸‹æœ¬åœ°è¯„ä¼°åéªŒä¼°è®¡å™¨ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ç›®å‰è¯„ä¼°åéªŒä¼°è®¡å™¨é™åˆ¶è§£å†³æ–¹æ³•çš„é—®é¢˜ã€‚ |
| [^10] | [Memory-Based Dual Gaussian Processes for Sequential Learning.](http://arxiv.org/abs/2306.03566) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè®°å¿†çš„åŒé«˜æ–¯è¿‡ç¨‹ç”¨äºåºåˆ—å­¦ä¹ çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ§åˆ¶è¯¯å·®å¹¶æ”¹å–„å­¦ä¹ ã€‚ |
| [^11] | [A Functional Data Perspective and Baseline On Multi-Layer Out-of-Distribution Detection.](http://arxiv.org/abs/2306.03522) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½‘ç»œçš„å‡½æ•°æ•°æ®è§†è§’çš„åŸåˆ›æ–¹æ³•ï¼Œåˆ©ç”¨æ ·æœ¬é€šè¿‡å„å±‚çš„è½¨è¿¹åŠå…¶ç»Ÿè®¡ä¸Šçš„ä¾èµ–å…³ç³»ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå®ç°äº†å¤šå±‚æ¬¡å¸¦åŸºå‡†çš„ODDæ£€æµ‹ã€‚ |
| [^12] | [On the Role of Attention in Prompt-tuning.](http://arxiv.org/abs/2306.03435) | æœ¬è®ºæ–‡ç ”ç©¶äº†Prompt-tuningåœ¨æ³¨æ„åŠ›æ¶æ„ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡æ¢ç´¢ä¸Šä¸‹æ–‡æ··åˆæ¨¡å‹ï¼Œè¡¨æ˜softmax-prompt-attentionåœ¨è¡¨è¾¾ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥é«˜æ•ˆçš„ä½¿ç”¨æ•°æ®å­¦ä¹ æç¤ºã€‚ |
| [^13] | [Binary Classification with Instance and Label Dependent Label Noise.](http://arxiv.org/abs/2306.03402) | æœ¬æ–‡ç ”ç©¶è§£å†³å¸¦æœ‰å®ä¾‹å’Œæ ‡ç­¾ç›¸å…³çš„æ ‡ç­¾å™ªå£°å¯¹äºäºŒåˆ†ç±»é—®é¢˜çš„å›°éš¾ï¼Œé€šè¿‡ç†è®ºåˆ†æå¾—åˆ°ç»éªŒé£é™©æœ€å°åŒ–å¯ä»¥å®ç°æœ€ä¼˜çš„è¶…é¢é£é™©ç•Œé™ã€‚ |
| [^14] | [A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging.](http://arxiv.org/abs/2306.03401) | æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§æ–¹æ³•æ¥è°ƒæ•´è”é‚¦å¹³å‡ä¸­çš„èšåˆæƒé‡ï¼Œé€šè¿‡æ ¹æ®æ¯ä¸ªå®¢æˆ·çš„å‚ä¸å†å²æ¥å¤„ç†å…·æœ‰ä¸åŒå‚ä¸ç‡çš„å®¢æˆ·ï¼Œè§£å†³äº†åœ¨è”é‚¦å­¦ä¹ ä¸­æœªçŸ¥å‚ä¸æ¦‚ç‡çš„é—®é¢˜ã€‚ |
| [^15] | [Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret.](http://arxiv.org/abs/2306.03372) | æœ¬æ–‡æå‡ºäº†åœ¨çº¿é»æ›¼æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œç”¨äºåœ¨åœ¨çº¿æƒ…å†µä¸‹ä¼°è®¡æ½œåœ¨çš„ä½ç§©å¼ é‡ã€‚å…¶ä¸­ï¼Œæˆ‘ä»¬åœ¨å¤„ç†è¿ç»­æˆ–åˆ†ç±»å˜é‡æ—¶æä¾›äº†çµæ´»çš„æ–¹æ³•ï¼Œå¹¶åœ¨åœ¨çº¿æƒ…å†µä¸‹å°è¯•äº†ä¸¤ä¸ªå…·ä½“çš„åº”ç”¨ï¼Œå³åœ¨çº¿å¼ é‡è¡¥å…¨å’Œåœ¨çº¿äºŒå…ƒå¼ é‡å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†é€ä¸ªæ¡ç›®çš„ç²¾ç¡®é”™è¯¯ç•Œé™ï¼Œè¿™æ˜¯åœ¨åœ¨çº¿å¼ é‡è¡¥å…¨ä¸­é¦–æ¬¡çº³å…¥å™ªå£°ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨å­˜åœ¨å™ªå£°çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—å’Œç»Ÿè®¡æ–¹é¢å­˜åœ¨ç€ä»¤äººæƒŠè®¶çš„æƒè¡¡ã€‚ |
| [^16] | [Unraveling Projection Heads in Contrastive Learning: Insights from Expansion and Shrinkage.](http://arxiv.org/abs/2306.03335) | æœ¬æ–‡ç ”ç©¶äº†å¯¹æ¯”å­¦ä¹ ä¸­çš„æŠ•å½±å¤´ï¼Œåœ¨ç†è®ºå’Œå®è·µä¸­æ‰¾åˆ°äº†ä¸¤ä¸ªå…³é”®æ•ˆåº”ï¼šä¿¡å·æ–¹å‘çš„æ‰©å±•å’Œæ”¶ç¼©ï¼Œæå‡ºäº†ä¸€ç³»åˆ—çº¿æ€§å˜æ¢æ¥æ”¹å–„ä¸‹æ¸¸åˆ†ç±»å‡†ç¡®æ€§ã€‚ |
| [^17] | [Global universal approximation of functional input maps on weighted spaces.](http://arxiv.org/abs/2306.03303) | æœ¬æ–‡æå‡ºäº†åŠŸèƒ½æ€§è¾“å…¥ç¥ç»ç½‘ç»œï¼Œå¯ä»¥åœ¨å¸¦æƒé‡ç©ºé—´ä¸Šå®Œæˆå…¨å±€å‡½æ•°é€¼è¿‘ã€‚è¿™ä¸€æ–¹æ³•é€‚ç”¨äºè¿ç»­å‡½æ•°çš„æ¨å¹¿ï¼Œè¿˜å¯ç”¨äºè·¯å¾„ç©ºé—´å‡½æ•°çš„é€¼è¿‘ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥é€¼è¿‘çº¿æ€§å‡½æ•°ç­¾åã€‚ |
| [^18] | [Switching Autoregressive Low-rank Tensor Models.](http://arxiv.org/abs/2306.03291) | è¯¥æ–‡æå‡ºäº†ä¸€ç§åˆ‡æ¢è‡ªå›å½’ä½ç§©å¼ é‡ï¼ˆSALTï¼‰æ¨¡å‹ï¼Œå®ƒå°†è‡ªå›å½’éšMarkovæ¨¡å‹ï¼ˆARHMMï¼‰å’Œåˆ‡æ¢çº¿æ€§åŠ¨æ€ç³»ç»Ÿï¼ˆSLDSï¼‰çš„ä¼˜ç‚¹ç»“åˆèµ·æ¥ï¼Œé€šè¿‡ä½ç§©å‚æ•°åŒ–æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚ |
| [^19] | [Deep Learning From Crowdsourced Labels: Coupled Cross-entropy Minimization, Identifiability, and Regularization.](http://arxiv.org/abs/2306.03288) | æœ¬æ–‡æå‡ºäº†ä½¿ç”¨ä¼—åŒ…æ ‡ç­¾è¿›è¡Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡è€¦åˆäº¤å‰ç†µæœ€å°åŒ–å’Œæ­£åˆ™åŒ–ä½¿å­¦ä¹ è¿‡ç¨‹æ›´åŠ é²æ£’ï¼ŒåŒæ—¶æå‡ºäº†æ€§èƒ½ä¿è¯ã€‚ |
| [^20] | [Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman.](http://arxiv.org/abs/2306.03266) | æœ¬æ–‡æå‡ºäº†$(k, t)$-FWLå’Œ$k$-FWL+ä¸¤ç§æ–¹æ³•ï¼Œç†è®ºä¸Šè¯æ˜äº†å®ƒä»¬å¯ä»¥åœ¨$O(n^2)$çš„ç©ºé—´å¤æ‚åº¦ä¸‹ï¼Œè§£å†³å›¾åŒæ„é—®é¢˜ã€‚ |
| [^21] | [Explaining and Adapting Graph Conditional Shift.](http://arxiv.org/abs/2306.03256) | æœ¬ç ”ç©¶é€šè¿‡é‡åŒ–è¾“å…¥ç‰¹å¾å’Œè¾“å‡ºæ ‡ç­¾ä¹‹é—´çš„æ¡ä»¶åç§»é‡ï¼Œå¯¹å›¾ç¥ç»ç½‘ç»œæ˜“å—åˆ†å¸ƒåç§»å½±å“çš„é—®é¢˜è¿›è¡Œç†è®ºåˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œå›¾å½¢å¼‚è´¨æ€§å’Œæ¨¡å‹æ¶æ„éƒ½ä¼šå¯¼è‡´æ¡ä»¶åç§»ï¼Œå½±å“æ€§èƒ½ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡æ¡ä»¶åç§»çš„ä¼°è®¡å’Œæœ€å°åŒ–æ¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–¹æ³•åœ¨èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚ |
| [^22] | [Nonlinear Distributionally Robust Optimization.](http://arxiv.org/abs/2306.03202) | æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„éçº¿æ€§åˆ†å¸ƒé²æ£’ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºå¤„ç†ä¸€ç±»åˆ†å¸ƒé²æ£’ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡ Gateaux Derivative å¤„ç†ä¸€èˆ¬é£é™©åº¦é‡ã€‚ç»è¿‡å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•æˆåŠŸå¤„ç†åˆ†å¸ƒçš„éçº¿æ€§ç›®æ ‡å‡½æ•°ã€‚ |
| [^23] | [Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning.](http://arxiv.org/abs/2306.03175) | LatFormeræ˜¯ä¸€ç§å°†æ ¼ç‚¹å¯¹ç§°å…ˆéªŒèå…¥åˆ°æ³¨æ„åŠ›æ©ç ä¸­çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿç”¨å·ç§¯ç½‘ç»œç”Ÿæˆè½¯æ©ç æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ã€‚è¯¥æ¨¡å‹åœ¨åˆæˆå‡ ä½•æ¨ç†ä¸­å–å¾—äº†è¾ƒå¥½æ•ˆæœã€‚ |
| [^24] | [Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences.](http://arxiv.org/abs/2306.03111) | æœ¬æ–‡æå‡ºäº†ä¸€ç§BootGenç®—æ³•ï¼Œä½¿ç”¨ä»£ç†å¾—åˆ†å‡½æ•°å¢å¼ºç”Ÿç‰©åºåˆ—ç”Ÿæˆå™¨çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶äº§ç”Ÿå¤šæ ·åŒ–çš„è®¾è®¡ï¼Œå°†å…¶åº”ç”¨äºä¼˜åŒ–ç”Ÿç‰©åºåˆ—ï¼Œå–å¾—äº†æ¯”ç«äº‰å¯¹æ‰‹æ›´å¥½çš„ç»“æœã€‚ |
| [^25] | [Provable Benefit of Mixup for Finding Optimal Decision Boundaries.](http://arxiv.org/abs/2306.00267) | æœ¬ç ”ç©¶è¯æ˜äº†ä½¿ç”¨Mixupè®­ç»ƒå…·æœ‰å¯è¯å®çš„ç›Šå¤„ï¼Œå¯ä»¥æ˜¾è‘—é™ä½åœ¨æ›´å¯åˆ†ç¦»æ•°æ®åˆ†å¸ƒä¸­å¯»æ‰¾æœ€ä½³å†³ç­–è¾¹ç•Œçš„æ ·æœ¬å¤æ‚åº¦ã€‚ |
| [^26] | [Linear Neural Network Layers Promote Learning Single- and Multiple-Index Models.](http://arxiv.org/abs/2305.15598) | æœ¬ç ”ç©¶æ¢ç©¶äº†è¿‡åº¦å‚æ•°åŒ–çš„æ·±åº¦ç¥ç»ç½‘ç»œçš„åè§ï¼Œå‘ç°åœ¨ReLUç½‘ç»œä¸­æ·»åŠ çº¿æ€§å±‚æœ‰åŠ©äºé€¼è¿‘å…·æœ‰ä½ç§©çº¿æ€§ç®—å­å’Œä½è¡¨ç¤ºæˆæœ¬å‡½æ•°ç»„æˆçš„å‡½æ•°ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªä¸ä½ç»´å­ç©ºé—´å‚ç›´æ–¹å‘è¿‘ä¹æ’å®šçš„æ’å€¼å‡½æ•°ã€‚ |
| [^27] | [Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern.](http://arxiv.org/abs/2305.11640) | æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç”¨ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„ä¸¢å¤±æ¨¡å¼ä¸‹æœ‰æ•ˆåœ°ä¿è¯è¦†ç›–ç‡çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é‡åŒ–äº†ç¼ºå¤±å¯¹é¢„æµ‹ç²¾åº¦çš„å½±å“ã€‚ |
| [^28] | [From Random Search to Bandit Learning in Metric Measure Spaces.](http://arxiv.org/abs/2305.11509) | æœ¬æ–‡ä»‹ç»äº†éšæœºæœç´¢åŠå…¶æ€§èƒ½ï¼Œå¼•å…¥äº†â€œæ•£å°„ç»´åº¦â€çš„æ¦‚å¿µï¼Œæè¿°äº†åº•å±‚å‡½æ•°çš„çŠ¶æ€ï¼Œé‡åŒ–äº†éšæœºæœç´¢çš„æ€§èƒ½ï¼Œå¹¶è¯æ˜äº†åœ¨æ— å™ªå£°å’Œæœ‰ç•Œå™ªå£°æƒ…å†µä¸‹çš„è¾“å‡ºåˆ†åˆ«ä»¥ä¸€å®šæ¦‚ç‡æ”¶æ•›åˆ°æœ€ä¼˜å€¼ã€‚ |
| [^29] | [Fisher Information Embedding for Node and Graph Learning.](http://arxiv.org/abs/2305.07580) | æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å›¾èŠ‚ç‚¹åµŒå…¥æ¡†æ¶ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„GNNã€‚ |
| [^30] | [Communication-Constrained Bandits under Additive Gaussian Noise.](http://arxiv.org/abs/2304.12680) | æœ¬æ–‡ç ”ç©¶äº†åœ¨å—é™é€šä¿¡å’ŒåŠ æ€§é«˜æ–¯å™ªå£°ä¸‹çš„å¤šè‡‚èµŒåšæœºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¤šé˜¶æ®µèµŒåšç®—æ³•ï¼Œå¹¶ç»™å‡ºäº†ä¿¡æ¯ç†è®ºä¸‹é™ã€‚ |
| [^31] | [Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts.](http://arxiv.org/abs/2304.09836) | æœ¬ç ”ç©¶é€šè¿‡æœ‰é™æ ·æœ¬å’ŒåŠŸç‡åˆ†æç¡®å®šäº†å¤šå…ƒæ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹è¯„åˆ†è§„åˆ™çš„å¯é æ€§åŒºåŸŸï¼Œå¹¶åœ¨ç”µåŠ›ç”Ÿäº§é—®é¢˜ä¸Šè¯„ä¼°äº†ç»“æœå¯¹çœŸå®ä¸–ç•Œä»»åŠ¡çš„æ™®é€‚æ€§ã€‚ |
| [^32] | [Fast Rates for Maximum Entropy Exploration.](http://arxiv.org/abs/2303.08059) | æœ¬æ–‡è§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸­ç¨€ç–å¥–åŠ±ä¸‹çš„æœ€å¤§ç†µæ¢ç´¢é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§ç±»å‹çš„æœ€å¤§ç†µæ¢ç´¢æ–¹æ³•ï¼Œå¹¶æé«˜äº†å…¶æ ·æœ¬å¤æ‚åº¦ã€‚ |
| [^33] | [Safe Peeling for L0-Regularized Least-Squares with supplementary material.](http://arxiv.org/abs/2302.14471) | å¼•å…¥â€œå®‰å…¨å‰¥ç¦»â€æ–¹æ³•åŠ é€Ÿè§£å†³L0æ­£åˆ™åŒ–æœ€å°äºŒä¹˜é—®é¢˜ï¼Œé€šè¿‡æ”¶ç¼©æ¾å¼›åº¦å…è®¸æ›´æ¿€è¿›çš„å‰ªæï¼Œæ˜¾è‘—é™ä½æ±‚è§£æ—¶é—´ã€‚ |
| [^34] | [Causal isotonic calibration for heterogeneous treatment effects.](http://arxiv.org/abs/2302.14011) | æå‡ºäº†å› æœç­‰ä¿æ ¡å‡†æ–¹æ³•åŠå…¶æ•°æ®æœ‰æ•ˆçš„å˜ä½“äº¤å‰æ ¡å‡†ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½å¿«é€Ÿç¨³å¥åœ°æ ¡å‡†å¼‚è´¨æ€§å¤„ç†æ•ˆåº”çš„é¢„æµ‹å™¨ï¼Œè€Œä¸”å¯ä»¥åº”ç”¨äºä»»ä½•é»‘ç›’å­¦ä¹ ç®—æ³•ã€‚ |
| [^35] | [Aligning Language Models with Preferences through f-divergence Minimization.](http://arxiv.org/abs/2302.08215) | æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„æ–¹æ³•f-DPGï¼Œç”¨äºå¯¹é½è¯­è¨€æ¨¡å‹å’Œåå¥½ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºè¯„ä¼°ä»»ä½•ç›®æ ‡åˆ†å¸ƒï¼Œç»Ÿä¸€äº†ç°æœ‰çš„å„ç§æ¡†æ¶å’Œé€¼è¿‘æ–¹æ³•ã€‚ |
| [^36] | [In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation.](http://arxiv.org/abs/2302.02923) | æœ¬æ–‡ç ”ç©¶åœ¨å…·æœ‰é«˜é£é™©åº”ç”¨çš„ä¸ªæ€§åŒ–å¤„ç†æ•ˆåº”ä¼°è®¡ä¸­ï¼Œä¸åŒæ¨¡å‹é€‰æ‹©æ ‡å‡†çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œå¹¶æå‡ºæœªæ¥ç ”ç©¶æ–¹å‘ã€‚ |
| [^37] | [The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing.](http://arxiv.org/abs/2302.01186) | è¯¥ç ”ç©¶æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç§©çŸ©é˜µæ„ŸçŸ¥é—®é¢˜æ—¶å…·æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚ |
| [^38] | [Revisiting Bellman Errors for Offline Model Selection.](http://arxiv.org/abs/2302.00141) | æœ¬æ–‡é‡æ–°å®¡è§† Bellman Errorsï¼Œå‘ç°ä¹‹å‰çš„Bellman Errors æ–¹æ³•éœ€è¦åœ¨ç‰¹å®šæ¡ä»¶ä¸‹æ‰èƒ½è¡¨ç°è‰¯å¥½ï¼ŒåŒæ—¶æå‡ºäº†æ›´å‡†ç¡®çš„ MSBE ä¼°è®¡å™¨ï¼Œåœ¨ç¦»æ•£æ§åˆ¶ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚ |
| [^39] | [On the Correctness of Automatic Differentiation for Neural Networks with Machine-Representable Parameters.](http://arxiv.org/abs/2301.13370) | æœ¬è®ºæ–‡ç ”ç©¶äº†ç¥ç»ç½‘ç»œå‚æ•°ä¸ºæœºå™¨å¯è¡¨ç¤ºæ•°å­—æ—¶è‡ªåŠ¨å¾®åˆ†çš„æ­£ç¡®æ€§é—®é¢˜ï¼Œè¯æ˜äº†ç¥ç»ç½‘ç»œå¸¦åç½®å‚æ•°æ—¶è‡ªåŠ¨å¾®åˆ†å§‹ç»ˆæ­£ç¡®ï¼Œç»™å‡ºäº†é™åˆ¶ä¸å¯å¾®æ€§åœ¨æ¿€æ´»å‡½æ•°ä¸­æ•°ç›®çš„ç•Œï¼Œå¹¶æä¾›äº†åˆ¤æ–­å‚æ•°æ˜¯å¦åœ¨ä¸å¯å¾®å‚æ•°ç»„ä¸­çš„æ¡ä»¶ã€‚ |
| [^40] | [Rigid body flows for sampling molecular crystal structures.](http://arxiv.org/abs/2301.11355) | æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ­£åˆ™åŒ–æµï¼Œä¸“ä¸ºä¸‰ç»´ç©ºé—´ä¸­å¤šä¸ªç‰©ä½“çš„ä½ç½®å’Œæ–¹å‘å»ºæ¨¡è€Œè®¾è®¡ã€‚é€šè¿‡åœ¨å•ä½å››å…ƒæ•°ç¾¤ä¸Šå®šä¹‰å¹³æ»‘å’Œè¡¨ç°åŠ›å¼ºçš„æµä»¥åŠå®šä¹‰é€‚å½“çš„å¯†åº¦ï¼Œåœ¨æ—‹è½¬ç¾¤ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥æˆåŠŸåœ°é‡‡æ ·åˆ†å­æ™¶ä½“ç»“æ„ã€‚ |
| [^41] | [Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors.](http://arxiv.org/abs/2301.08987) | æœ¬ç ”ç©¶æå‡ºäº†å±‚æ¬¡å¹³è¡¡çš„æ¦‚å¿µï¼Œè¯¥æ¦‚å¿µæ•æ‰äº†æœªè§‚å¯Ÿåˆ°çš„æ½œåœ¨å› æœå› ç´ çš„æƒ…å†µå˜åŒ–ï¼Œå¹¶æ¢è®¨äº†åŠ¨æ€å…¬å¹³æ€§çš„å®ç°ã€‚åœ¨æŒ‡å®šçš„åŠ¨æ€ä¸‹ï¼Œé€šå¸¸ä¸èƒ½é€šè¿‡ä¸€æ­¥å¹²é¢„æ¥å®ç°é•¿æœŸå…¬å¹³ç›®æ ‡ã€‚ |
| [^42] | [I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data.](http://arxiv.org/abs/2210.13954) | è¯¥è®ºæ–‡ç ”ç©¶äº†ä¸ªäººå¯ä»¥é€‰æ‹©ä¸å†³ç­–ç³»ç»Ÿå…±äº«å¯é€‰ä¸ªäººä¿¡æ¯çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¿æŠ¤ç”¨æˆ·åŒæ„çš„PUCæ¦‚å¿µï¼Œä¸ºç”¨æˆ·éšç§ä¿æŠ¤æä¾›äº†æœ‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚ |
| [^43] | ["Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts.](http://arxiv.org/abs/2210.10769) | æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å°†æ¨¡å‹æ€§èƒ½å˜åŒ–å½’å› äºåº•å±‚æ•°æ®ç”Ÿæˆæœºåˆ¶çš„åˆ†å¸ƒåç§»çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡æ¨å¯¼ä¸€ç§é‡è¦æ€§æƒé‡æ–¹æ³•æ¥è®¡ç®—ä»»æ„ä¸€ç»„åˆ†å¸ƒçš„ä»·å€¼ã€‚ |
| [^44] | [Sparsity by Redundancy: Solving $L_1$ with SGD.](http://arxiv.org/abs/2210.01212) | è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å†—ä½™é‡å‚æ•°åŒ–å’Œç®€å•çš„éšæœºæ¢¯åº¦ä¸‹é™æ¥æœ€å°åŒ–å¸¦æœ‰$L_1$æƒ©ç½šçš„é€šç”¨å¯å¾®æŸå¤±å‡½æ•°çš„æ–¹æ³•ï¼Œç§°ä¸º\textit{spred}ï¼Œæ˜¯$L_1$çš„ç²¾ç¡®æ±‚è§£å™¨ï¼Œå¯ç”¨äºè®­ç»ƒç¨€ç–ç¥ç»ç½‘ç»œä»¥æ‰§è¡ŒåŸºå› é€‰æ‹©ä»»åŠ¡å’Œç¥ç»ç½‘ç»œå‹ç¼©ä»»åŠ¡ï¼Œå¼¥åˆäº†æ·±åº¦å­¦ä¹ ä¸­çš„ç¨€ç–æ€§å’Œä¼ ç»Ÿç»Ÿè®¡å­¦ä¹ ä¹‹é—´çš„å·®è·ã€‚ |
| [^45] | [Transfer Learning for Individual Treatment Effect Estimation.](http://arxiv.org/abs/2210.00380) | æœ¬è®ºæ–‡æ¢è®¨äº†ä¸ªä½“æ²»ç–—æ•ˆæœä¼°è®¡ä¸­è¿ç§»å› æœçŸ¥è¯†çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªä½¿ç”¨CITAåº¦é‡è¿›è¡ŒITEçŸ¥è¯†è½¬ç§»çš„æ¡†æ¶ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ |
| [^46] | [Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions.](http://arxiv.org/abs/2207.12067) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæ€è‡ªç¼–ç å™¨æ–¹æ³•ï¼Œä½¿æœºå™¨èƒ½å¤Ÿåœ¨è¡ŒåŠ¨ä¸­å­¦ä¹ åˆ°ä¸å…¶è¡Œä¸ºç›¸ä¸€è‡´çš„æ„ŸçŸ¥ä¿¡æ¯çš„å†…éƒ¨è¡¨ç¤ºï¼Œå¹¶æ•è·ç¯å¢ƒä¸­çš„è½¬æ¢ç»“æ„ã€‚ |
| [^47] | [When are Post-hoc Conceptual Explanations Identifiable?.](http://arxiv.org/abs/2206.13872) | æœ¬è®ºæ–‡æå‡ºäº†å¯è¯†åˆ«çš„æ¦‚å¿µå‘ç°æ–¹æ³•ï¼Œå¯ä»¥æ¢å¤å‡ºå¤šä¸ªå·²çŸ¥çš„æ¦‚å¿µï¼Œä»¥ç¡®ä¿è§£é‡Šçš„å¯é æ€§ã€‚å¯¹äºå…·æœ‰ä¾èµ–å…³ç³»çš„æ¦‚å¿µï¼Œæå‡ºäº†ä¸¤ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„åŠŸèƒ½ç»„åˆæ€§è´¨ã€‚è¯¥æ–¹æ³•æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ã€‚ |
| [^48] | [Beyond Uniform Lipschitz Condition in Differentially Private Optimization.](http://arxiv.org/abs/2206.10713) | æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å·®åˆ†éšç§ä¼˜åŒ–ç®—æ³•æ¥å¤„ç†å…¶å®ƒç®—æ³•æ— æ³•å¤„ç†çš„éå‡åŒ€ææ™®å¸ŒèŒ¨æƒ…å½¢ï¼Œå¹¶ä¸”åœ¨å…·ä½“åº”ç”¨ä¸­æä¾›äº†ç›¸åº”çš„å‚æ•°è°ƒæ•´æ–¹æ¡ˆã€‚ |
| [^49] | [Optimally tackling covariate shift in RKHS-based nonparametric regression.](http://arxiv.org/abs/2205.02986) | æœ¬æ–‡ç ”ç©¶äº†åœ¨RKHSçš„éå‚æ•°å›å½’ä¸­çš„åå˜é‡è½¬ç§»é—®é¢˜ï¼Œé’ˆå¯¹ä¸¤ä¸ªä¸åŒçš„ä¼¼ç„¶æ¯”æ—ï¼Œè¯æ˜äº†ä½¿ç”¨KRRä¼°è®¡é‡å…·æœ‰æå°åŒ–ç‡æœ€ä¼˜çš„ç‰¹ç‚¹ï¼Œå°¤å…¶æ˜¯åœ¨ä¼¼ç„¶æ¯”è¢«å‡åŒ€æœ‰ç•Œæ—¶ã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿè¯æ˜äº†ï¼Œåœ¨åå˜é‡è½¬ç§»ä¸‹ä¸€ä¸ªnaiveçš„ä¼°è®¡å™¨ç›¸æ¯”äºKRRæ˜¯ä¸¥æ ¼æ¬¡ä¼˜çš„ã€‚ |
| [^50] | [A Symmetric Loss Perspective of Reliable Machine Learning.](http://arxiv.org/abs/2101.01366) | å¯¹ç§°æŸå¤±æ˜¯ä¸€ç§æ–°å‹çš„ä»£ç†æŸå¤±ï¼Œèƒ½å¤Ÿä½¿å¾—å­¦ä¹ è¿‡ç¨‹å¯¹äºå—æŸæ ‡ç­¾æ›´åŠ é²æ£’ï¼Œä»è€Œæé«˜åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚ |
| [^51] | [Growing Efficient Deep Networks by Structured Continuous Sparsification.](http://arxiv.org/abs/2007.15353) | æœ¬æ–‡æå‡ºä¸€ç§ç»“æ„åŒ–è¿ç»­ç¨€ç–åŒ–çš„æ·±åº¦ç½‘ç»œç»“æ„ç”Ÿé•¿æ–¹æ³•ï¼Œé€šè¿‡è¿ç»­æ¾å¼›å’Œé‡‡æ ·ç¨€ç–å­ç½‘ç»œï¼Œå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¾¾åˆ°ç´§å‡‘çš„ä¿®å‰ªç½‘ç»œç»“æ„ï¼ŒåŒæ—¶å¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦å¹¶ä¿æŒè¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚ |
| [^52] | [Conditional Sampling with Monotone GANs: from Generative Models to Likelihood-Free Inference.](http://arxiv.org/abs/2006.06755) | æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¦‚ç‡æµ‹åº¦æ¡ä»¶é‡‡æ ·æ¡†æ¶ï¼Œä½¿ç”¨å•è°ƒGANå­¦ä¹ å—çŠ¶ä¸‰è§’å½¢æ˜ å°„ï¼Œä»…ä½¿ç”¨æ¥è‡ªåº•å±‚è”åˆæ¦‚ç‡æµ‹åº¦çš„æ ·æœ¬å®ç°æ— ä¼¼ç„¶æ¨æ–­ã€‚ |
| [^53] | [Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices.](http://arxiv.org/abs/2004.13612) | Deniseæ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ç®—æ³•ï¼Œç”¨äºå¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œä½ç§©åŠ ç¨€ç–åˆ†è§£ï¼Œè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½è€Œä¸”è¿‘ä¹æ¥è¿‘20å€çš„åŠ é€Ÿã€‚ |
| [^54] | [Certified Reinforcement Learning with Logic Guidance.](http://arxiv.org/abs/1902.00778) | æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿä½¿ç”¨çº¿æ€§æ—¶æ€é€»è¾‘æ¥åˆ¶å®šé©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹çš„ç›®æ ‡ï¼Œå°†LTLå±æ€§è½¬åŒ–ä¸ºLDGBAè‡ªåŠ¨æœºï¼Œé€šè¿‡è°ƒæ•´åŒæ­¥å¥–åŠ±å‡½æ•°æœ€å¤§æ¦‚ç‡è·å¾—æ»¡è¶³LTLè§„å®šè¦æ±‚çš„æ§åˆ¶ç­–ç•¥ã€‚ |
| [^55] | [Orthogonal Statistical Learning.](http://arxiv.org/abs/1901.09036) | æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ ·æœ¬æ‹†åˆ†çš„å…ƒç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿåœ¨è¯„ä¼°æ€»ä½“é£é™©æ—¶è€ƒè™‘å¹²æ‰°å‚æ•°ï¼Œå¹¶ä¸”å®ç°çš„è¶…é¢é£é™©ç•Œçš„å½±å“ä¸ºäºŒæ¬¡ã€‚ |

# è¯¦ç»†

[^1]: ç­¾åæ¡ç ä½œä¸ºåº¦é‡çš„å¤šå‚æ•°æŒä¹…åŒè°ƒçš„ç¨³å®šå‘é‡åŒ–

    Stable Vectorization of Multiparameter Persistent Homology using Signed Barcodes as Measures. (arXiv:2306.03801v1 [cs.LG])

    [http://arxiv.org/abs/2306.03801](http://arxiv.org/abs/2306.03801)

    æœ¬ç ”ç©¶æå‡ºäº†ä½¿ç”¨ç­¾åæ¡ç æ¥ç¨³å®šå‘é‡åŒ–å¤šå‚æ•°æŒä¹…åŒè°ƒï¼Œå°†å¤šå‚æ•°æŒä¹…åŒè°ƒçš„ä¸°å¯Œä¿¡æ¯å’Œç¨³å®šå‘é‡åŒ–çš„ä¼˜åŠ¿ç›¸ç»“åˆã€‚

    

    æŒä¹…åŒè°ƒï¼ˆPHï¼‰æä¾›äº†å‡ ä½•æ•°æ®ï¼ˆä¾‹å¦‚åŠ æƒå›¾ï¼‰çš„æ‹“æ‰‘æè¿°ç¬¦ï¼Œå®ƒä»¬æ˜¯å¯è§£é‡Šçš„ï¼Œå¯¹æ‰°åŠ¨ç¨³å®šï¼Œå¹¶å…·æœ‰è¯¸å¦‚é‡æ ‡è®°ç­‰ä¸å˜æ€§ã€‚å¤§å¤šæ•°PHåº”ç”¨å…³æ³¨ä¸€å‚æ•°æƒ…å†µâ€”â€”æè¿°ç¬¦æ€»ç»“æ•°æ®çš„æ‹“æ‰‘éšç€å•ä¸ªæ„Ÿå…´è¶£å› ç´ çš„æ»¤æ³¢è€Œå‘ç”Ÿå˜åŒ–ï¼›ç°åœ¨ï¼Œæœ‰å„ç§æ–¹æ³•ä½¿å¾—ä¸€å‚æ•°PHæè¿°ç¬¦åœ¨æ•°æ®ç§‘å­¦ä¸­å¾—åˆ°åº”ç”¨ï¼Œå¹¶ä¸”ä¾èµ–äºå°†è¿™äº›æè¿°ç¬¦ç¨³å®šå‘é‡åŒ–ä¸ºå¸Œå°”ä¼¯ç‰¹ç©ºé—´çš„å…ƒç´ ã€‚è™½ç„¶ç”±å‡ ä¸ªæ„Ÿå…´è¶£å› ç´ è¿‡æ»¤çš„æ•°æ®çš„å¤šå‚æ•°PHï¼ˆMPHï¼‰ç¼–ç æ¯”å…¶ä¸€å‚æ•°åŒå‹çš„ä¿¡æ¯æ›´ä¸°å¯Œï¼Œä½†è¿„ä»Šä¸ºæ­¢ï¼ŒMPHæè¿°ç¬¦çš„ç¨³å®šæ€§ç»“æœçš„ç¨€ç¼ºæ€§å·²ç»é™åˆ¶äº†MPHçš„ç¨³å®šå‘é‡åŒ–çš„å¯ç”¨é€‰é¡¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡å±•ç¤ºå¦‚ä½•è§£é‡Šç­¾åæ¡ç æ¥é›†ç»“ä¸¤æ–¹é¢çš„ä¼˜ç‚¹ã€‚

    Persistent homology (PH) provides topological descriptors for geometric data, such as weighted graphs, which are interpretable, stable to perturbations, and invariant under, e.g., relabeling. Most applications of PH focus on the one-parameter case -- where the descriptors summarize the changes in topology of data as it is filtered by a single quantity of interest -- and there is now a wide array of methods enabling the use of one-parameter PH descriptors in data science, which rely on the stable vectorization of these descriptors as elements of a Hilbert space. Although the multiparameter PH (MPH) of data that is filtered by several quantities of interest encodes much richer information than its one-parameter counterpart, the scarceness of stability results for MPH descriptors has so far limited the available options for the stable vectorization of MPH. In this paper, we aim to bring together the best of both worlds by showing how the interpretation of signed barcodes -- a recent famil
    
[^2]: éšæœºç‰¹å¾å›å½’ä¸­è´å¶æ–¯ä¸ç¡®å®šæ€§ä¼°è®¡çš„æ¸è¿‘æ€§

    Asymptotics of Bayesian Uncertainty Estimation in Random Features Regression. (arXiv:2306.03783v1 [stat.ML])

    [http://arxiv.org/abs/2306.03783](http://arxiv.org/abs/2306.03783)

    è®ºæ–‡æ¯”è¾ƒå’Œå¯¹æ¯”äº†åéªŒé¢„æµ‹åˆ†å¸ƒå’Œæœ€å¤§åéªŒä¼°è®¡çš„é£é™©ï¼Œé‡ç‚¹å…³æ³¨äº†æ¨¡å‹ç»´åº¦å¢é•¿é€Ÿåº¦å¤§äºä»»ä½•å¸¸æ•°å€çš„æ ·æœ¬æ•°æ—¶å®ƒä»¬ä¹‹é—´çš„æ¸è¿‘ä¸€è‡´æ€§ã€‚æ•°å€¼æ¨¡æ‹Ÿè¡¨æ˜è¿™ä¸¤ä¸ªæ•°é‡åœ¨é™å®šç»´åº¦ä¸Šå…·æœ‰é«˜æ–¯æ³¢åŠ¨ï¼Œå¹¶è¡¨ç°å‡ºç›¸ä¼¼çš„å±æ€§ã€‚

    

    æœ¬æ–‡æ¯”è¾ƒå’Œå¯¹æ¯”äº†è´å¶æ–¯å›å½’æ¨¡å‹ä¸­åéªŒé¢„æµ‹åˆ†å¸ƒå’Œæœ€å¤§åéªŒä¼°è®¡ï¼ˆMAPï¼‰é£é™©åœ¨è¶…å‚æ•°åŒ–åŒºåŸŸä¸­çš„è¡Œä¸ºã€‚æˆ‘ä»¬å°†é‡ç‚¹å…³æ³¨åéªŒé¢„æµ‹åˆ†å¸ƒï¼ˆè´å¶æ–¯æ¨¡å‹å¹³å‡ï¼‰çš„æ–¹å·®ï¼Œå¹¶å°†å…¶æ¸è¿‘æ€§ä¸MAPä¼°è®¡å™¨çš„é£é™©è¿›è¡Œæ¯”è¾ƒã€‚å½“æ¨¡å‹ç»´åº¦å¢é•¿é€Ÿåº¦å¤§äºä»»ä½•å¸¸æ•°å€çš„æ ·æœ¬æ•°æ—¶ï¼Œå®ƒä»¬ä¹‹é—´çš„æ¸è¿‘ä¸€è‡´æ€§å—åˆ°ä¿¡å™ªæ¯”çš„ç›¸å˜çš„æ§åˆ¶ã€‚å½“æ ·æœ¬æ•°å¢é•¿é€Ÿåº¦å¤§äºä»»ä½•å¸¸æ•°å€çš„æ¨¡å‹ç»´åº¦æ—¶ï¼Œå®ƒä»¬ä¹Ÿä¼šæ¸è¿‘ä¸€è‡´ã€‚æ•°å€¼æ¨¡æ‹Ÿè¯´æ˜äº†ä¸¤ä¸ªæ•°é‡çš„æœ‰é™ç»´åˆ†å¸ƒæ€§è´¨ã€‚æˆ‘ä»¬æ¨æµ‹å®ƒä»¬å…·æœ‰é«˜æ–¯æ³¢åŠ¨ï¼Œå¹¶è¡¨ç°å‡ºä¸ä¹‹å‰åœ¨é«˜æ–¯åºåˆ—æ¨¡å‹ä¸­å‘ç°çš„ç±»ä¼¼å±æ€§ã€‚

    In this paper we compare and contrast the behavior of the posterior predictive distribution to the risk of the maximum a posteriori estimator for the random features regression model in the overparameterized regime. We will focus on the variance of the posterior predictive distribution (Bayesian model average) and compare its asymptotics to that of the risk of the MAP estimator. In the regime where the model dimensions grow faster than any constant multiple of the number of samples, asymptotic agreement between these two quantities is governed by the phase transition in the signal-to-noise ratio. They also asymptotically agree with each other when the number of samples grow faster than any constant multiple of model dimensions. Numerical simulations illustrate finer distributional properties of the two quantities for finite dimensions. We conjecture they have Gaussian fluctuations and exhibit similar properties as found by previous authors in a Gaussian sequence model, which is of inde
    
[^3]: é€šè¿‡è°±ç‰¹å¾çš„é«˜æ–¯è¿‡ç¨‹è¿›è¡Œå›¾åˆ†ç±»

    Graph Classification Gaussian Processes via Spectral Features. (arXiv:2306.03770v1 [cs.LG])

    [http://arxiv.org/abs/2306.03770](http://arxiv.org/abs/2306.03770)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§ä½¿ç”¨è°±ç‰¹å¾çš„é«˜æ–¯è¿‡ç¨‹æ¨¡å‹æ¥è§£å†³å›¾åˆ†ç±»é—®é¢˜ï¼Œå³ä½¿æ˜¯åŸºäºèŠ‚ç‚¹ç‰¹å¾ä¿¡å·åœ¨å›¾è°±ä¸Šèƒ½é‡åˆ†å¸ƒè¿™æ ·ç®€å•çš„æ–¹æ³•ä¹Ÿæœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚åŒæ—¶ï¼Œæ›´å¤æ‚çš„å˜ä½“ä½¿ç”¨è°±å›¾å°æ³¢æ»¤æ³¢å™¨å¯ä»¥åœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œæ•°æ®é›†ä¸Šè·å¾—æ”¹è¿›ï¼Œè€Œä¸¤ç§æ¨¡å‹éƒ½èƒ½å¤Ÿäº§ç”Ÿè®¡ç®—ä¼°è®¡ï¼Œèƒ½å¤Ÿå¯é åœ°åšå‡ºå†³ç­–ã€‚

    

    å›¾åˆ†ç±»æ—¨åœ¨æ ¹æ®å…¶ç»“æ„å’ŒèŠ‚ç‚¹å±æ€§å¯¹å›¾è¿›è¡Œåˆ†ç±»ã€‚æœ¬æ–‡æå‡ºä½¿ç”¨å›¾ä¿¡å·å¤„ç†å·¥å…·ï¼Œé€šè¿‡å¾—å‡ºè°±ç‰¹å¾æ¥è®¾è®¡ä¸¤ç§å˜ä½“çš„é«˜æ–¯è¿‡ç¨‹æ¨¡å‹æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚ç¬¬ä¸€ç§å˜ä½“ä½¿ç”¨åŸºäºèŠ‚ç‚¹ç‰¹å¾ä¿¡å·åœ¨å›¾è°±ä¸Šèƒ½é‡åˆ†å¸ƒçš„è°±ç‰¹å¾ã€‚æˆ‘ä»¬å±•ç¤ºå³ä½¿ä½¿ç”¨æ²¡æœ‰å­¦ä¹ å‚æ•°çš„å¦‚æ­¤ç®€å•çš„æ–¹æ³•ï¼Œä¹Ÿå¯ä»¥ä¸å¼ºå¤§çš„ç¥ç»ç½‘ç»œå’Œå›¾å†…æ ¸åŸºçº¿ç›¸æ¯”å…·æœ‰ç«äº‰åŠ›çš„è¡¨ç°ã€‚ç¬¬äºŒç§æ›´å¤æ‚çš„å˜ä½“é€šè¿‡å­¦ä¹ è°±å›¾å°æ³¢æ»¤æ³¢å™¨æ¥æ•æ‰å›¾ä¸­çš„å¤šå°ºåº¦å’Œå±€éƒ¨æ¨¡å¼ï¼Œåœ¨åˆæˆå’ŒçœŸå®ä¸–ç•Œçš„æ•°æ®é›†ä¸Šè·å¾—äº†æ”¹è¿›ã€‚æœ€åï¼Œæˆ‘ä»¬å±•ç¤ºä¸¤ç§æ¨¡å‹éƒ½å¯ä»¥äº§ç”Ÿè‰¯å¥½çš„è®¡ç®—ä¼°è®¡ï¼Œä»è€ŒåŸºäºæ¨¡å‹é¢„æµ‹å¯é åœ°åšå‡ºå†³ç­–ã€‚

    Graph classification aims to categorise graphs based on their structure and node attributes. In this work, we propose to tackle this task using tools from graph signal processing by deriving spectral features, which we then use to design two variants of Gaussian process models for graph classification. The first variant uses spectral features based on the distribution of energy of a node feature signal over the spectrum of the graph. We show that even such a simple approach, having no learned parameters, can yield competitive performance compared to strong neural network and graph kernel baselines. A second, more sophisticated variant is designed to capture multi-scale and localised patterns in the graph by learning spectral graph wavelet filters, obtaining improved performance on synthetic and real-world data sets. Finally, we show that both models produce well calibrated uncertainty estimates, enabling reliable decision making based on the model predictions.
    
[^4]: äººç±»ä¸å¯æ„ŸçŸ¥ã€æœºå™¨å¯è¯†åˆ«çš„å›¾åƒ

    Human-imperceptible, Machine-recognizable Images. (arXiv:2306.03679v1 [cs.CV])

    [http://arxiv.org/abs/2306.03679](http://arxiv.org/abs/2306.03679)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éšç§ä¿æŠ¤å­¦ä¹ èŒƒå¼ï¼Œé€šè¿‡åŠ å¯†å›¾åƒå®ç°äººç±»ä¸å¯æ„ŸçŸ¥ä½†æœºå™¨å¯è¯†åˆ«ï¼Œå¹¶ä½¿ç”¨ç»è¿‡æœ€å°é€‚é…çš„è§†è§‰è½¬æ¢å™¨å®Œæˆè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚å®éªŒè¯æ˜è¯¥æ–¹æ³•å‡†ç¡®æ€§ä¸ç«äº‰æ–¹æ³•ç›¸å½“ã€‚

    

    å¤§é‡ä¸äººç±»ç›¸å…³çš„æ•°æ®è¢«æ”¶é›†ç”¨äºè®­ç»ƒç¥ç»ç½‘ç»œè¿›è¡Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ã€‚æˆ‘ä»¬å‘ç°äº†ä¸€ç§å…³äºè½¯ä»¶å·¥ç¨‹å¸ˆçš„é‡å¤§å†²çªï¼šåœ¨æ›´å¥½åœ°å¼€å‘äººå·¥æ™ºèƒ½ç³»ç»Ÿä¸è¿œç¦»æ•æ„Ÿè®­ç»ƒæ•°æ®ä¹‹é—´å­˜åœ¨ä¸€å¤§çŸ›ç›¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ¬æ–‡æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„éšç§ä¿æŠ¤å­¦ä¹ èŒƒå¼ï¼Œå…¶ä¸­å›¾åƒè¢«åŠ å¯†æˆâ€œäººç±»ä¸å¯æ„ŸçŸ¥ï¼Œæœºå™¨å¯è¯†åˆ«â€çŠ¶æ€ï¼Œé€šè¿‡ä»¥ä¸‹ä¸¤ç§åŠ å¯†ç­–ç•¥ä¹‹ä¸€æ¥å®ç°ï¼š(1) å°†å›¾åƒéšæœºæ´—ç‰Œæˆä¸€ç»„ç›¸ç­‰å¤§å°çš„å°å—ï¼Œ(2) å¯¹å›¾åƒçš„å­å—è¿›è¡Œæ··åˆã€‚ç„¶åï¼Œå¯¹è§†è§‰è½¬æ¢å™¨è¿›è¡Œæœ€å°çš„é€‚é…ï¼Œä½¿å…¶èƒ½å¤Ÿå­¦ä¹ åŠ å¯†å›¾åƒçš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ŒåŒ…æ‹¬å›¾åƒåˆ†ç±»å’Œç›®æ ‡æ£€æµ‹ã€‚åœ¨ ImageNet å’Œ COCO ä¸Šè¿›è¡Œäº†å¹¿æ³›çš„å®éªŒï¼Œç»“æœè¡¨æ˜ï¼Œæ‰€æå‡ºçš„æ–¹æ³•åœ¨å‡†ç¡®æ€§æ–¹é¢ä¸ç«äº‰æ–¹æ³•ç›¸å½“ã€‚è§£å¯†åŠ å¯†å›¾åƒéœ€è¦è§£å†³ NP éš¾çš„æ‹¼å›¾é—®é¢˜æˆ–ç—…æ€çš„åé—®é¢˜ï¼Œè¿™æ˜¯ç»éªŒè¯æ˜çš„ã€‚

    Massive human-related data is collected to train neural networks for computer vision tasks. A major conflict is exposed relating to software engineers between better developing AI systems and distancing from the sensitive training data. To reconcile this conflict, this paper proposes an efficient privacy-preserving learning paradigm, where images are first encrypted to become ``human-imperceptible, machine-recognizable'' via one of the two encryption strategies: (1) random shuffling to a set of equally-sized patches and (2) mixing-up sub-patches of the images. Then, minimal adaptations are made to vision transformer to enable it to learn on the encrypted images for vision tasks, including image classification and object detection. Extensive experiments on ImageNet and COCO show that the proposed paradigm achieves comparable accuracy with the competitive methods. Decrypting the encrypted images requires solving an NP-hard jigsaw puzzle or an ill-posed inverse problem, which is empirical
    
[^5]: é»‘ç›’å˜åˆ†æ¨æ–­çš„æ”¶æ•›æ€§ä¿è¯

    Provable convergence guarantees for black-box variational inference. (arXiv:2306.03638v1 [cs.LG])

    [http://arxiv.org/abs/2306.03638](http://arxiv.org/abs/2306.03638)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºå¯†é›†é«˜æ–¯å˜åˆ†æ—çš„æ¢¯åº¦ä¼°è®¡å™¨ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šä½¿ç”¨è¿‘ç«¯å’ŒæŠ•å½±éšæœºæ¢¯åº¦ä¸‹é™ï¼Œæä¾›äº†é»‘ç›’å˜åˆ†æ¨æ–­æ”¶æ•›äºé€¼çœŸæ¨æ–­é—®é¢˜çš„ç¬¬ä¸€ä¸ªä¸¥æ ¼ä¿è¯ã€‚

    

    å°½ç®¡é»‘ç›’å˜åˆ†æ¨æ–­è¢«å¹¿æ³›åº”ç”¨ï¼Œä½†æ²¡æœ‰è¯æ˜å…¶éšæœºä¼˜åŒ–æˆåŠŸçš„è¯æ˜ã€‚æˆ‘ä»¬æå‡ºè¿™æ˜¯ç°æœ‰éšæœºä¼˜åŒ–è¯æ˜ä¸­çš„ç†è®ºå·®è·ï¼Œå³å…·æœ‰å¼‚å¸¸å™ªå£°è¾¹ç•Œå’Œå¤åˆéå¹³æ»‘ç›®æ ‡çš„æ¢¯åº¦ä¼°è®¡å™¨çš„æŒ‘æˆ˜ã€‚å¯¹äºå¯†é›†çš„é«˜æ–¯å˜åˆ†æ—ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°ç°æœ‰çš„åŸºäºå†å‚æ•°åŒ–çš„æ¢¯åº¦ä¼°è®¡å™¨æ»¡è¶³äºŒæ¬¡å™ªå£°ç•Œï¼Œå¹¶ä¸ºä½¿ç”¨è¯¥ç•Œé™çš„è¿‘ç«¯å’ŒæŠ•å½±éšæœºæ¢¯åº¦ä¸‹é™æä¾›æ–°çš„æ”¶æ•›ä¿è¯ã€‚è¿™æä¾›äº†ç¬¬ä¸€ä¸ªé»‘ç›’å˜åˆ†æ¨æ–­æ”¶æ•›äºé€¼çœŸæ¨æ–­é—®é¢˜çš„ä¸¥æ ¼ä¿è¯ã€‚

    While black-box variational inference is widely used, there is no proof that its stochastic optimization succeeds. We suggest this is due to a theoretical gap in existing stochastic optimization proofs-namely the challenge of gradient estimators with unusual noise bounds, and a composite non-smooth objective. For dense Gaussian variational families, we observe that existing gradient estimators based on reparameterization satisfy a quadratic noise bound and give novel convergence guarantees for proximal and projected stochastic gradient descent using this bound. This provides the first rigorous guarantee that black-box variational inference converges for realistic inference problems.
    
[^6]: å…¬å¹³ä¸”å¥å£®çš„å¼‚è´¨æ²»ç–—æ•ˆæœæ”¿ç­–å­¦ä¹ ä¼°è®¡

    Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy Learning. (arXiv:2306.03625v1 [stat.ME])

    [http://arxiv.org/abs/2306.03625](http://arxiv.org/abs/2306.03625)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§å…¬å¹³ä¸”å¥å£®çš„å¼‚è´¨æ²»ç–—æ•ˆæœçš„ä¼°è®¡æ¡†æ¶ï¼Œå¯ä»¥åœ¨å…¬å¹³çº¦æŸä¸‹éå‚æ•°åœ°ä¼°è®¡ï¼Œå¹¶å¯ç”¨äºæƒè¡¡å…¬å¹³å’Œæœ€å¤§ç¦åˆ©ä¹‹é—´çš„å…³ç³»ã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§ç®€å•ä¸”é€šç”¨çš„æ¡†æ¶ï¼Œç”¨äºåœ¨å…¬å¹³çº¦æŸæ¡ä»¶ä¸‹éå‚æ•°ä¼°è®¡å¼‚è´¨æ²»ç–—æ•ˆæœã€‚åœ¨æ ‡å‡†æ­£åˆ™æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†æ‰€å¾—åˆ°çš„ä¼°è®¡å™¨å…·æœ‰åŒé‡å¥å£®æ€§ã€‚æˆ‘ä»¬åˆ©ç”¨æ­¤æ¡†æ¶æ¥è¡¨å¾å…¬å¹³å’Œæœ€ä½³æ”¿ç­–å¯å®ç°çš„æœ€å¤§ç¦åˆ©ä¹‹é—´çš„æƒè¡¡ã€‚æˆ‘ä»¬åœ¨æ¨¡æ‹Ÿç ”ç©¶ä¸­è¯„ä¼°äº†è¯¥æ–¹æ³•ï¼Œå¹¶åœ¨ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„æ¡ˆä¾‹ç ”ç©¶ä¸­è¿›è¡Œäº†è¯´æ˜ã€‚

    We propose a simple and general framework for nonparametric estimation of heterogeneous treatment effects under fairness constraints. Under standard regularity conditions, we show that the resulting estimators possess the double robustness property. We use this framework to characterize the trade-off between fairness and the maximum welfare achievable by the optimal policy. We evaluate the methods in a simulation study and illustrate them in a real-world case study.
    
[^7]: ç†µåæ–¹å·®æ¨¡å‹

    Entropic covariance models. (arXiv:2306.03590v1 [math.ST])

    [http://arxiv.org/abs/2306.03590](http://arxiv.org/abs/2306.03590)

    æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„çº¿æ€§çº¦æŸåæ–¹å·®çŸ©é˜µå˜æ¢çš„æ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä¼°è®¡æ–¹æ³•ï¼Œè§£å†³äº†ä¸€ä¸ªå‡¸é—®é¢˜ï¼Œå…è®¸ç›¸å¯¹ç®€å•çš„æ¸è¿‘æ€§å’Œæœ‰é™æ ·æœ¬åˆ†æã€‚ç ”ç©¶çš„é‡ç‚¹æ˜¯å…³äºå»ºæ¨¡ç›¸å…³çŸ©é˜µå’Œç¨€ç–æ€§æ–¹é¢çš„å†…å®¹ã€‚

    

    åœ¨åæ–¹å·®çŸ©é˜µä¼°è®¡ä¸­ï¼Œæ‰¾åˆ°åˆé€‚çš„æ¨¡å‹å’Œæœ‰æ•ˆçš„ä¼°è®¡æ–¹æ³•æ˜¯ä¸€é¡¹æŒ‘æˆ˜ã€‚æ–‡çŒ®ä¸­é€šå¸¸é‡‡ç”¨ä¸¤ç§æ–¹æ³•ï¼Œä¸€ç§æ˜¯å¯¹åæ–¹å·®çŸ©é˜µæˆ–å…¶é€†æ–½åŠ çº¿æ€§çº¦æŸï¼Œå¦ä¸€ç§æ˜¯è€ƒè™‘æ–½åŠ åœ¨åæ–¹å·®çŸ©é˜µçš„çŸ©é˜µå¯¹æ•°ä¸Šçš„çº¿æ€§çº¦æŸã€‚æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨çš„çº¿æ€§çº¦æŸåæ–¹å·®çŸ©é˜µå˜æ¢çš„æ¡†æ¶ï¼ŒåŒ…æ‹¬ä¸Šè¿°ä¾‹å­ã€‚æˆ‘ä»¬æå‡ºçš„ä¼°è®¡æ–¹æ³•è§£å†³äº†ä¸€ä¸ªå‡¸é—®é¢˜ï¼Œå¹¶äº§ç”Ÿäº†ä¸€ä¸ªMä¼°è®¡é‡ï¼Œå…è®¸ç›¸å¯¹ç®€å•çš„æ¸è¿‘æ€§å’Œæœ‰é™æ ·æœ¬åˆ†æã€‚åœ¨å¼€å‘äº†ä¸€èˆ¬ç†è®ºä¹‹åï¼Œæˆ‘ä»¬é›†ä¸­åœ¨å»ºæ¨¡ç›¸å…³çŸ©é˜µå’Œç¨€ç–æ€§æ–¹é¢ã€‚æˆ‘ä»¬çš„å‡ ä½•æ´å¯ŸåŠ›å…è®¸æˆ‘ä»¬æ‰©å±•åæ–¹å·®çŸ©é˜µå»ºæ¨¡ä¸­çš„ä¸€äº›æœ€æ–°ç»“æœã€‚è¿™åŒ…æ‹¬æä¾›ç›¸å…³çŸ©é˜µç©ºé—´çš„æ— é™åˆ¶å‚æ•°åŒ–ï¼Œè¿™æ˜¯ä¸€ç§æ›¿ä»£åˆ©ç”¨å˜æ¢çš„æœ€æ–°ç»“æœã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†å¦‚ä½•å¯¹åæ–¹å·®çŸ©é˜µçš„Choleskyå› å­æ–½åŠ ç¨€ç–æ€§é™åˆ¶ï¼Œè¿™ä¸ç°æœ‰æ–¹æ³•ä¸åŒã€‚

    In covariance matrix estimation, one of the challenges lies in finding a suitable model and an efficient estimation method. Two commonly used approaches in the literature involve imposing linear restrictions on the covariance matrix or its inverse. Another approach considers linear restrictions on the matrix logarithm of the covariance matrix. In this paper, we present a general framework for linear restrictions on different transformations of the covariance matrix, including the mentioned examples. Our proposed estimation method solves a convex problem and yields an M-estimator, allowing for relatively straightforward asymptotic and finite sample analysis. After developing the general theory, we focus on modelling correlation matrices and on sparsity. Our geometric insights allow to extend various recent results in covariance matrix modelling. This includes providing unrestricted parametrizations of the space of correlation matrices, which is alternative to a recent result utilizing t
    
[^8]: è¿‡åº¦å‹ç¼©å¦‚ä½•å½±å“GNNçš„èƒ½åŠ›ï¼Ÿ

    How does over-squashing affect the power of GNNs?. (arXiv:2306.03589v1 [cs.LG])

    [http://arxiv.org/abs/2306.03589](http://arxiv.org/abs/2306.03589)

    æœ¬æ–‡é€šè¿‡æµ‹é‡èŠ‚ç‚¹ä¹‹é—´æˆå¯¹äº¤äº’çš„æ°´å¹³ï¼Œæä¾›äº†ä¸¥æ ¼çš„åˆ†æï¼Œä»¥ç¡®å®šå…·æœ‰ä¸€å®šå®¹é‡çš„MPNNå¯ä»¥å­¦ä¹ å“ªäº›èŠ‚ç‚¹ç‰¹å¾çš„å‡½æ•°ç±»åˆ«ã€‚ç»“æœè¡¨æ˜ï¼Œä¸ºäº†ä¿è¯èŠ‚ç‚¹å¯¹ä¹‹é—´çš„å……åˆ†é€šä¿¡ï¼ŒMPNNçš„å®¹é‡å¿…é¡»æ˜¯...

    

    å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰æ˜¯å¤„ç†å›¾ç»“æ„æ•°æ®çš„æœºå™¨å­¦ä¹ çš„æœ€å…ˆè¿›æ¨¡å‹ã€‚æœ€æµè¡Œçš„GNNç±»åˆ«æ˜¯é€šè¿‡ç›¸é‚»èŠ‚ç‚¹é—´çš„ä¿¡æ¯äº¤æ¢æ¥æ“ä½œçš„ï¼Œç§°ä¸ºæ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNï¼‰ã€‚é‰´äºå®ƒä»¬çš„å¹¿æ³›åº”ç”¨ï¼Œäº†è§£MPNNçš„è¡¨è¾¾èƒ½åŠ›æ˜¯ä¸€ä¸ªå…³é”®é—®é¢˜ã€‚ç„¶è€Œï¼Œç°æœ‰ç»“æœé€šå¸¸è€ƒè™‘å…·æœ‰æ— ä¿¡æ¯èŠ‚ç‚¹ç‰¹å¾çš„ç¯å¢ƒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§ä¸¥æ ¼çš„åˆ†ææ–¹æ³•ï¼Œä»¥ç¡®å®šå…·æœ‰ä¸€å®šå®¹é‡çš„MPNNå¯ä»¥å­¦ä¹ å“ªäº›èŠ‚ç‚¹ç‰¹å¾çš„å‡½æ•°ç±»åˆ«ã€‚æˆ‘ä»¬é€šè¿‡æµ‹é‡MPNNå…è®¸çš„èŠ‚ç‚¹ä¹‹é—´çš„æˆå¯¹äº¤äº’æ°´å¹³æ¥å®ç°æ­¤ç›®çš„ã€‚è¯¥æµ‹é‡æä¾›äº†ä¸€ç§æ–°çš„é‡åŒ–ç‰¹æ€§ï¼Œå³æ‰€è°“çš„è¿‡åº¦å‹ç¼©æ•ˆåº”ï¼Œè¯¥æ•ˆåº”è¢«è§‚å¯Ÿåˆ°æ˜¯å½“å¤§é‡çš„ä¿¡æ¯èšåˆæˆå›ºå®šå¤§å°çš„å‘é‡æ—¶å‘ç”Ÿçš„ã€‚ä½¿ç”¨æˆ‘ä»¬çš„æµ‹é‡ï¼Œæˆ‘ä»¬è¯æ˜ï¼Œä¸ºäº†ä¿è¯èŠ‚ç‚¹å¯¹ä¹‹é—´çš„å……åˆ†é€šä¿¡ï¼ŒMPNNçš„å®¹é‡å¿…é¡»æ˜¯...

    Graph Neural Networks (GNNs) are the state-of-the-art model for machine learning on graph-structured data. The most popular class of GNNs operate by exchanging information between adjacent nodes, and are known as Message Passing Neural Networks (MPNNs). Given their widespread use, understanding the expressive power of MPNNs is a key question. However, existing results typically consider settings with uninformative node features. In this paper, we provide a rigorous analysis to determine which function classes of node features can be learned by an MPNN of a given capacity. We do so by measuring the level of pairwise interactions between nodes that MPNNs allow for. This measure provides a novel quantitative characterization of the so-called over-squashing effect, which is observed to occur when a large volume of messages is aggregated into fixed-size vectors. Using our measure, we prove that, to guarantee sufficient communication between pairs of nodes, the capacity of the MPNN must be l
    
[^9]: L-C2ST: åŸºäºæœ¬åœ°è¯Šæ–­å®ç°æ¨¡æ‹Ÿæ¨æ–­ä¸­åéªŒè¿‘ä¼¼

    L-C2ST: Local Diagnostics for Posterior Approximations in Simulation-Based Inference. (arXiv:2306.03580v1 [stat.ML])

    [http://arxiv.org/abs/2306.03580](http://arxiv.org/abs/2306.03580)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸º L-C2ST çš„åŸºäºæœ¬åœ°è¯Šæ–­å®ç°æ¨¡æ‹Ÿæ¨æ–­ä¸­åéªŒè¿‘ä¼¼çš„æ–°æ–¹æ³•ï¼Œå…¶å¯ä»¥åœ¨ä»»ä½•ç»™å®šçš„è§‚æµ‹ä¸‹æœ¬åœ°è¯„ä¼°åéªŒä¼°è®¡å™¨ï¼Œæœ‰æ•ˆåœ°è§£å†³äº†ç›®å‰è¯„ä¼°åéªŒä¼°è®¡å™¨é™åˆ¶è§£å†³æ–¹æ³•çš„é—®é¢˜ã€‚

    

    æœ€è¿‘è®¸å¤šæ¨¡æ‹Ÿæ¨æ–­ï¼ˆSBIï¼‰çš„å·¥ä½œéƒ½ä¾èµ–äºæ·±åº¦ç”Ÿæˆæ¨¡å‹æ¥è¿‘ä¼¼å¤æ‚ã€é«˜ç»´åº¦çš„åéªŒåˆ†å¸ƒã€‚ç„¶è€Œï¼Œè¯„ä¼°è¿™äº›è¿‘ä¼¼æ˜¯å¦å¯ä¿¡ä»æ˜¯ä¸€ä¸ªæŒ‘æˆ˜ã€‚å¤§å¤šæ•°æ–¹æ³•ä»…åœ¨è§‚æµ‹ç©ºé—´æœŸæœ›ä¸‹è¯„ä¼°åéªŒä¼°è®¡å™¨ã€‚è¿™é™åˆ¶äº†å®ƒä»¬çš„å¯è§£é‡Šæ€§ï¼Œå¹¶ä¸èƒ½è¶³å¤Ÿåœ°ç¡®å®šå“ªäº›è§‚æµ‹ç»“æœå¯ä»¥ä¿¡ä»»è¿™äº›è¿‘ä¼¼æˆ–åº”è¯¥æ”¹è¿›ã€‚æˆ‘ä»¬åŸºäºè‘—åçš„åˆ†ç±»å™¨ä¸¤æ ·æœ¬æ£€éªŒ (C2ST)ï¼Œå¼•å…¥ L-C2STï¼Œä¸€ä¸ªæ–°æ–¹æ³•ï¼Œå…è®¸åœ¨ä»»ä½•ç»™å®šçš„è§‚æµ‹ä¸‹æœ¬åœ°è¯„ä¼°åéªŒä¼°è®¡å™¨ã€‚å®ƒæä¾›æœ‰ç†è®ºåŸºç¡€å’Œæ˜“äºè§£é‡Šçš„ï¼Œå¦‚å›¾ç¤ºè¯Šæ–­ã€‚ä¸ C2ST ä¸åŒçš„æ˜¯ï¼ŒL-C2ST ä¸éœ€è¦è®¿é—®çœŸå®åéªŒçš„æ ·æœ¬ã€‚å¯¹äºåŸºäºå½’ä¸€åŒ–æµçš„åéªŒä¼°è®¡å™¨ï¼ŒL-C2ST å¯ä»¥ä¸“é—¨æä¾›æ›´å¥½çš„ç»Ÿè®¡åŠŸç‡ï¼ŒåŒæ—¶è®¡ç®—æ•ˆç‡æ›´é«˜ã€‚

    Many recent works in simulation-based inference (SBI) rely on deep generative models to approximate complex, high-dimensional posterior distributions. However, evaluating whether or not these approximations can be trusted remains a challenge. Most approaches evaluate the posterior estimator only in expectation over the observation space. This limits their interpretability and is not sufficient to identify for which observations the approximation can be trusted or should be improved. Building upon the well-known classifier two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a local evaluation of the posterior estimator at any given observation. It offers theoretically grounded and easy to interpret - e.g. graphical - diagnostics, and unlike C2ST, does not require access to samples from the true posterior. In the case of normalizing flow-based posterior estimators, L-C2ST can be specialized to offer better statistical power, while being computationally more efficien
    
[^10]: åŸºäºè®°å¿†çš„åŒé«˜æ–¯è¿‡ç¨‹ç”¨äºåºåˆ—å­¦ä¹ 

    Memory-Based Dual Gaussian Processes for Sequential Learning. (arXiv:2306.03566v1 [cs.LG])

    [http://arxiv.org/abs/2306.03566](http://arxiv.org/abs/2306.03566)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè®°å¿†çš„åŒé«˜æ–¯è¿‡ç¨‹ç”¨äºåºåˆ—å­¦ä¹ çš„æ–¹æ³•ï¼Œèƒ½å¤Ÿæ§åˆ¶è¯¯å·®å¹¶æ”¹å–„å­¦ä¹ ã€‚

    

    åœ¨è¿ç»­å’Œä¸»åŠ¨å­¦ä¹ ä¸­ï¼Œè®¿é—®è¿‡å»æ•°æ®çš„èƒ½åŠ›æœ‰é™ï¼Œå› æ­¤ä½¿ç”¨é«˜æ–¯è¿‡ç¨‹ï¼ˆGPsï¼‰è¿›è¡Œåºåˆ—å­¦ä¹ æ˜¯å…·æœ‰æŒ‘æˆ˜æ€§çš„ã€‚åœ¨åéªŒã€è¶…å‚æ•°å’Œè¯±å¯¼ç‚¹çš„ä¸å‡†ç¡®æ€§å¯¼è‡´é”™è¯¯éšæ—¶é—´ç´¯ç§¯çš„æƒ…å†µä¸‹ï¼Œå‡†ç¡®å­¦ä¹ å˜å¾—å›°éš¾ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ä½¿ç”¨æœ€è¿‘æå‡ºçš„åŒç¨€ç–å˜åˆ†é«˜æ–¯è¿‡ç¨‹æ¥æ§åˆ¶æ‰€æœ‰è¿™äº›è¯¯å·®çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æ–¹æ³•èƒ½å¤Ÿè¿›è¡Œé€šç”¨ä¼¼ç„¶çš„å‡†ç¡®æ¨æ–­ï¼Œå¹¶é€šè¿‡ä¸»åŠ¨å»ºç«‹å’Œæ›´æ–°è¿‡å»æ•°æ®çš„è®°å¿†æ¥æ”¹å–„å­¦ä¹ ã€‚æˆ‘ä»¬åœ¨æ¶‰åŠè´å¶æ–¯ä¼˜åŒ–ã€ä¸»åŠ¨å­¦ä¹ å’Œè¿ç»­å­¦ä¹ çš„å‡ ä¸ªåº”ç”¨ä¸­å±•ç¤ºäº†å…¶æœ‰æ•ˆæ€§ã€‚

    Sequential learning with Gaussian processes (GPs) is challenging when access to past data is limited, for example, in continual and active learning. In such cases, errors can accumulate over time due to inaccuracies in the posterior, hyperparameters, and inducing points, making accurate learning challenging. Here, we present a method to keep all such errors in check using the recently proposed dual sparse variational GP. Our method enables accurate inference for generic likelihoods and improves learning by actively building and updating a memory of past data. We demonstrate its effectiveness in several applications involving Bayesian optimization, active learning, and continual learning.
    
[^11]: å¤šå±‚æ¬¡å¸¦åŸºå‡†çš„å¼‚å¸¸æ£€æµ‹çš„å‡½æ•°æ•°æ®è§†è§’

    A Functional Data Perspective and Baseline On Multi-Layer Out-of-Distribution Detection. (arXiv:2306.03522v1 [cs.LG])

    [http://arxiv.org/abs/2306.03522](http://arxiv.org/abs/2306.03522)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½‘ç»œçš„å‡½æ•°æ•°æ®è§†è§’çš„åŸåˆ›æ–¹æ³•ï¼Œåˆ©ç”¨æ ·æœ¬é€šè¿‡å„å±‚çš„è½¨è¿¹åŠå…¶ç»Ÿè®¡ä¸Šçš„ä¾èµ–å…³ç³»ï¼Œä¼˜äºç°æœ‰æœ€å…ˆè¿›æ–¹æ³•ï¼Œå®ç°äº†å¤šå±‚æ¬¡å¸¦åŸºå‡†çš„ODDæ£€æµ‹ã€‚

    

    å®ç°å¤–æ ·æœ¬æ£€æµ‹çš„å…³é”®ç‰¹å¾æ˜¯é€šè¿‡å¤šå±‚åˆ†ç±»å™¨æå–ç»Ÿè®¡æ¨¡å¼å’Œæ•°æ®é—´çš„å…³ç³»ï¼Œä»¥æ£€æµ‹é¢„æœŸè¾“å…¥æ•°æ®åˆ†å¸ƒçš„å˜åŒ–ã€‚ä½†æ˜¯ï¼Œç°æœ‰çš„ä¸€äº›æœ€å…ˆè¿›çš„æ–¹æ³•ä»…ä½¿ç”¨å€’æ•°ç¬¬äºŒå±‚æˆ–æœ€åä¸€å±‚çš„è¾“å‡ºï¼Œç•™ä¸‹äº†ç”¨äºODDæ£€æµ‹çš„æœ‰ä»·å€¼çš„ä¿¡æ¯ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºç½‘ç»œçš„å‡½æ•°è§†è§’çš„åŸåˆ›æ–¹æ³•ï¼Œåˆ©ç”¨æ ·æœ¬é€šè¿‡å„å±‚çš„è½¨è¿¹åŠå…¶ç»Ÿè®¡ä¸Šçš„ä¾èµ–å…³ç³»ã€‚å®ƒè¶…è¶Šäº†å¤šå…ƒç‰¹å¾èšåˆï¼Œå¹¶å¼•å…¥äº†åŸºäºå‡½æ•°å¼‚å¸¸æ£€æµ‹çš„åŸºå‡†ã€‚åœ¨è¿™ä¸ªæ–°çš„æ¡†æ¶ä¸­ï¼ŒODDæ£€æµ‹è½¬åŒ–ä¸ºæ£€æµ‹æ ·æœ¬çš„è½¨è¿¹ä¸è®­ç»ƒé›†æ‰€è¡¨ç°çš„å…¸å‹è¡Œä¸ºä¸åŒçš„æƒ…å†µã€‚æˆ‘ä»¬åœ¨å„ç§åŸºå‡†æµ‹è¯•ä¸­éªŒè¯äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå¹¶å±•ç¤ºäº†é€šè¿‡åˆ©ç”¨ç½‘ç»œçš„æ‰€æœ‰å±‚çš„ä¿¡æ¯ï¼Œå…¶åœ¨æ€§èƒ½ä¸Šä¼˜äºç°æœ‰çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚

    A key feature of out-of-distribution (OOD) detection is to exploit a trained neural network by extracting statistical patterns and relationships through the multi-layer classifier to detect shifts in the expected input data distribution. Despite achieving solid results, several state-of-the-art methods rely on the penultimate or last layer outputs only, leaving behind valuable information for OOD detection. Methods that explore the multiple layers either require a special architecture or a supervised objective to do so. This work adopts an original approach based on a functional view of the network that exploits the sample's trajectories through the various layers and their statistical dependencies. It goes beyond multivariate features aggregation and introduces a baseline rooted in functional anomaly detection. In this new framework, OOD detection translates into detecting samples whose trajectories differ from the typical behavior characterized by the training set. We validate our me
    
[^12]: å…³æ³¨ç‚¹å¯¹Prompt-tuningçš„ä½œç”¨

    On the Role of Attention in Prompt-tuning. (arXiv:2306.03435v1 [cs.LG])

    [http://arxiv.org/abs/2306.03435](http://arxiv.org/abs/2306.03435)

    æœ¬è®ºæ–‡ç ”ç©¶äº†Prompt-tuningåœ¨æ³¨æ„åŠ›æ¶æ„ä¸­çš„åº”ç”¨ï¼Œé€šè¿‡æ¢ç´¢ä¸Šä¸‹æ–‡æ··åˆæ¨¡å‹ï¼Œè¡¨æ˜softmax-prompt-attentionåœ¨è¡¨è¾¾ä¸Šä¼˜äºå…¶ä»–æ¨¡å‹ï¼ŒåŒæ—¶ä¹Ÿè¯æ˜äº†è¯¥æ–¹æ³•å¯ä»¥é«˜æ•ˆçš„ä½¿ç”¨æ•°æ®å­¦ä¹ æç¤ºã€‚

    

    Prompt-tuning æ˜¯ä¸€ç§æ–°å…´çš„ç­–ç•¥ï¼Œé€šè¿‡ä»æ•°æ®ä¸­å­¦ä¹  (è½¯) æç¤ºå‚æ•°ï¼Œä½¿å¤§å‹è¯­è¨€æ¨¡å‹ (LLM) é€‚åº”ä¸‹æ¸¸ä»»åŠ¡ã€‚å°½ç®¡å…¶åœ¨ LLM ä¸­å–å¾—äº†æˆåŠŸï¼Œä½†å¯¹äº Prompt-tuning çš„èƒ½åŠ›åŠå…³æ³¨æœºåˆ¶åœ¨æç¤ºä¸­çš„ä½œç”¨ï¼Œç†è®ºç†è§£å°šæœ‰é™ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æ¢ç´¢ä¸€ä¸ªæ³¨æ„åŠ›æ¶æ„çš„ Prompt-tuningï¼Œå¹¶ç ”ç©¶ä¸Šä¸‹æ–‡æ··åˆæ¨¡å‹ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å…¥è¡¨ç¤ºå±äºä¸Šä¸‹æ–‡ç›¸å…³æˆ–æ— å…³é›†åˆã€‚æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªè‡ªåŒ…å«çš„æç¤º-æ³¨æ„åŠ›æ¨¡å‹æ¥éš”ç¦» Prompt-tuning çš„ä½œç”¨ã€‚æˆ‘ä»¬çš„è´¡çŒ®å¦‚ä¸‹ï¼š(1) æˆ‘ä»¬è¡¨æ˜åœ¨æˆ‘ä»¬çš„ä¸Šä¸‹æ–‡æ•°æ®æ¨¡å‹ä¸‹ï¼Œsoftmax-prompt-attention åœ¨å¯è¯æ˜åœ°æ¯”softmax-self-attention å’Œçº¿æ€§æç¤ºæ³¨æ„åŠ›æ›´å…·è¡¨è¾¾åŠ›ã€‚(2) æˆ‘ä»¬åˆ†æäº†æ¸å˜ä¸‹é™çš„åˆå§‹è½¨è¿¹ï¼Œå¹¶å±•ç¤ºå¯ä»¥é€šè¿‡è¿‘ä¹æœ€ä¼˜çš„æ ·æœ¬å¤æ‚åº¦å­¦ä¹ æç¤ºå’Œé¢„æµ‹å¤´ï¼Œä»è€Œè¯æ˜äº†æç¤ºå¯ä»¥è¯æ˜åœ°æ³¨æ„åˆ°ç¨€ç–çš„ä¸Šä¸‹æ–‡ç›¸å…³ä¿¡æ¯ã€‚(3)

    Prompt-tuning is an emerging strategy to adapt large language models (LLM) to downstream tasks by learning a (soft-)prompt parameter from data. Despite its success in LLMs, there is limited theoretical understanding of the power of prompt-tuning and the role of the attention mechanism in prompting. In this work, we explore prompt-tuning for one-layer attention architectures and study contextual mixture-models where each input token belongs to a context-relevant or -irrelevant set. We isolate the role of prompt-tuning through a self-contained prompt-attention model. Our contributions are as follows: (1) We show that softmax-prompt-attention is provably more expressive than softmax-self-attention and linear-prompt-attention under our contextual data model. (2) We analyze the initial trajectory of gradient descent and show that it learns the prompt and prediction head with near-optimal sample complexity and demonstrate how prompt can provably attend to sparse context-relevant tokens. (3) 
    
[^13]: å¸¦æœ‰å®ä¾‹å’Œæ ‡ç­¾ç›¸å…³çš„æ ‡ç­¾å™ªå£°çš„äºŒåˆ†ç±»é—®é¢˜

    Binary Classification with Instance and Label Dependent Label Noise. (arXiv:2306.03402v1 [stat.ML])

    [http://arxiv.org/abs/2306.03402](http://arxiv.org/abs/2306.03402)

    æœ¬æ–‡ç ”ç©¶è§£å†³å¸¦æœ‰å®ä¾‹å’Œæ ‡ç­¾ç›¸å…³çš„æ ‡ç­¾å™ªå£°å¯¹äºäºŒåˆ†ç±»é—®é¢˜çš„å›°éš¾ï¼Œé€šè¿‡ç†è®ºåˆ†æå¾—åˆ°ç»éªŒé£é™©æœ€å°åŒ–å¯ä»¥å®ç°æœ€ä¼˜çš„è¶…é¢é£é™©ç•Œé™ã€‚

    

    å­¦ä¹ å¸¦æœ‰æ ‡ç­¾ç›¸å…³çš„æ ‡ç­¾å™ªå£°åœ¨ç†è®ºå’Œå®è·µä¸­å¾—åˆ°äº†å¹¿æ³›æ¢è®¨ï¼Œç„¶è€Œå¤„ç†å¸¦æœ‰å®ä¾‹å’Œæ ‡ç­¾ç›¸å…³çš„æ ‡ç­¾å™ªå£°ä»ç„¶æ˜¯ä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚è¿™ç§å›°éš¾åœ¨äºå™ªå£°ç‡å› æ¯ä¸ªå®ä¾‹è€Œå¼‚ï¼Œä½¿å¾—å‡†ç¡®ä¼°è®¡å™ªå£°ç‡æˆä¸ºä¸€é¡¹å…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ã€‚ç›®å‰è¿˜æ²¡æœ‰è§£å†³èƒ½å¦ä»…ä½¿ç”¨å«æœ‰å™ªå£°æ ·æœ¬æ¥å­¦ä¹ å¯é æ¨¡å‹çš„é—®é¢˜ã€‚æˆ‘ä»¬é€šè¿‡ç†è®ºåˆ†æå›ç­”äº†è¿™ä¸ªé—®é¢˜ï¼Œæä¾›äº†åŒ¹é…çš„ä¸Šç•Œå’Œä¸‹ç•Œã€‚ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œæˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œä¸éœ€è¦ä»»ä½•é¢å¤–çš„å‡è®¾ï¼Œç»éªŒé£é™©æœ€å°åŒ–å¯ä»¥å®ç°æœ€ä¼˜çš„è¶…é¢é£é™©ç•Œé™ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬é€šè¿‡æ¯”è¾ƒä»å¹²å‡€æ ·æœ¬å’Œå™ªå£°æ ·æœ¬ä¸­å¾—åˆ°çš„ç»éªŒé£é™©æœ€å°åŒ–å™¨æ¥å¯¼å‡ºä¸€ç§ä¸å™ªå£°æ°´å¹³æˆæ¯”ä¾‹çš„æ–°çš„è¶…é¢é£é™©ç•Œé™ï¼Œåœ¨éå¸¸ä¸€èˆ¬çš„æƒ…å†µä¸‹éƒ½æˆç«‹ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è¡¨æ˜äº†0-1æŸå¤±çš„æå°æå¤§ä¸‹ç•Œæ˜¯ä¸€ä¸ªä¸æ ‡ç­¾æ•°æˆæ¯”ä¾‹çš„å¸¸æ•°ã€‚

    Learning with label dependent label noise has been extensively explored in both theory and practice; however, dealing with instance (i.e., feature) and label dependent label noise continues to be a challenging task. The difficulty arises from the fact that the noise rate varies for each instance, making it challenging to estimate accurately. The question of whether it is possible to learn a reliable model using only noisy samples remains unresolved. We answer this question with a theoretical analysis that provides matching upper and lower bounds. Surprisingly, our results show that, without any additional assumptions, empirical risk minimization achieves the optimal excess risk bound. Specifically, we derive a novel excess risk bound proportional to the noise level, which holds in very general settings, by comparing the empirical risk minimizers obtained from clean samples and noisy samples. Second, we show that the minimax lower bound for the 0-1 loss is a constant proportional to the
    
[^14]: å¤„ç†è”é‚¦å¹³å‡ä¸­æœªçŸ¥å‚ä¸æ¦‚ç‡çš„è½»é‡çº§æ–¹æ³•

    A Lightweight Method for Tackling Unknown Participation Probabilities in Federated Averaging. (arXiv:2306.03401v1 [cs.LG])

    [http://arxiv.org/abs/2306.03401](http://arxiv.org/abs/2306.03401)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§è½»é‡çº§æ–¹æ³•æ¥è°ƒæ•´è”é‚¦å¹³å‡ä¸­çš„èšåˆæƒé‡ï¼Œé€šè¿‡æ ¹æ®æ¯ä¸ªå®¢æˆ·çš„å‚ä¸å†å²æ¥å¤„ç†å…·æœ‰ä¸åŒå‚ä¸ç‡çš„å®¢æˆ·ï¼Œè§£å†³äº†åœ¨è”é‚¦å­¦ä¹ ä¸­æœªçŸ¥å‚ä¸æ¦‚ç‡çš„é—®é¢˜ã€‚

    

    åœ¨è”é‚¦å­¦ä¹ ä¸­ï¼Œå®¢æˆ·ç«¯é€šå¸¸å…·æœ‰å…ˆéªŒæœªçŸ¥çš„ä¸åŒå‚ä¸ç‡ï¼Œå¦‚æœä¸é€‚å½“å¤„ç†ï¼Œåˆ™å¯èƒ½ä¼šå¯¹è”é‚¦å­¦ä¹ çš„æ€§èƒ½é€ æˆé‡å¤§å½±å“ã€‚ç°æœ‰çš„è§£å†³æ–¹æ³•é€šå¸¸åŸºäºå…¨å±€æ–¹å·®ç¼©å‡ï¼Œè¿™éœ€è¦å¤§é‡é¢å¤–çš„å†…å­˜ï¼Œå…¶ä¹˜æ³•å› å­ç­‰äºå®¢æˆ·æ€»æ•°ã€‚ä¸€ä¸ªé‡è¦çš„æœªè§£å†³é—®é¢˜æ˜¯æ‰¾åˆ°ä¸€ç§è½»é‡çº§æ–¹æ³•æ¥å¤„ç†å…·å¤‡ä¸åŒå‚ä¸ç‡å®¢æˆ·çš„è”é‚¦å­¦ä¹ ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡æ ¹æ®æ¯ä¸ªå®¢æˆ·çš„å‚ä¸å†å²æ¥è°ƒæ•´è”é‚¦å¹³å‡ï¼ˆFedAvgï¼‰ä¸­çš„èšåˆæƒé‡æ¥è§£å†³æ­¤é—®é¢˜ã€‚æˆ‘ä»¬é¦–å…ˆå±•ç¤ºäº†åœ¨å…·æœ‰å¼‚æ„å‚ä¸æ¦‚ç‡çš„æƒ…å†µä¸‹ï¼Œéæœ€ä¼˜èšåˆæƒé‡çš„FedAvgå¯èƒ½ä¼šä»åŸå§‹FLç›®æ ‡çš„æœ€ä¼˜è§£åç¦»ï¼Œè¿™è¡¨æ˜éœ€è¦æ‰¾åˆ°æœ€ä¼˜èšåˆæƒé‡ã€‚ç„¶è€Œï¼Œå½“å‚ä¸æ¦‚ç‡ä¸å¯çŸ¥æ—¶è®¡ç®—æœ€ä¼˜æƒé‡éå¸¸å›°éš¾ã€‚

    In federated learning (FL), clients usually have diverse participation probabilities that are unknown a priori, which can significantly harm the performance of FL if not handled properly. Existing works aiming at addressing this problem are usually based on global variance reduction, which requires a substantial amount of additional memory in a multiplicative factor equal to the total number of clients. An important open problem is to find a lightweight method for FL in the presence of clients with unknown participation rates. In this paper, we address this problem by adapting the aggregation weights in federated averaging (FedAvg) based on the participation history of each client. We first show that, with heterogeneous participation probabilities, FedAvg with non-optimal aggregation weights can diverge from the optimal solution of the original FL objective, indicating the need of finding optimal aggregation weights. However, it is difficult to compute the optimal weights when the part
    
[^15]: åœ¨çº¿å¼ é‡å­¦ä¹ ï¼šè®¡ç®—å’Œç»Ÿè®¡æƒè¡¡ï¼Œé€‚åº”æ€§å’Œæœ€ä¼˜é—æ†¾

    Online Tensor Learning: Computational and Statistical Trade-offs, Adaptivity and Optimal Regret. (arXiv:2306.03372v1 [stat.ML])

    [http://arxiv.org/abs/2306.03372](http://arxiv.org/abs/2306.03372)

    æœ¬æ–‡æå‡ºäº†åœ¨çº¿é»æ›¼æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œç”¨äºåœ¨åœ¨çº¿æƒ…å†µä¸‹ä¼°è®¡æ½œåœ¨çš„ä½ç§©å¼ é‡ã€‚å…¶ä¸­ï¼Œæˆ‘ä»¬åœ¨å¤„ç†è¿ç»­æˆ–åˆ†ç±»å˜é‡æ—¶æä¾›äº†çµæ´»çš„æ–¹æ³•ï¼Œå¹¶åœ¨åœ¨çº¿æƒ…å†µä¸‹å°è¯•äº†ä¸¤ä¸ªå…·ä½“çš„åº”ç”¨ï¼Œå³åœ¨çº¿å¼ é‡è¡¥å…¨å’Œåœ¨çº¿äºŒå…ƒå¼ é‡å­¦ä¹ ã€‚æˆ‘ä»¬è¿˜å»ºç«‹äº†é€ä¸ªæ¡ç›®çš„ç²¾ç¡®é”™è¯¯ç•Œé™ï¼Œè¿™æ˜¯åœ¨åœ¨çº¿å¼ é‡è¡¥å…¨ä¸­é¦–æ¬¡çº³å…¥å™ªå£°ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œåœ¨å­˜åœ¨å™ªå£°çš„æƒ…å†µä¸‹ï¼Œè®¡ç®—å’Œç»Ÿè®¡æ–¹é¢å­˜åœ¨ç€ä»¤äººæƒŠè®¶çš„æƒè¡¡ã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†ä¸€ä¸ªå¹¿ä¹‰æ¡†æ¶ï¼Œç”¨äºåœ¨çº¿æƒ…å†µä¸‹ä¼°è®¡æ½œåœ¨çš„ä½ç§©å¼ é‡ï¼ŒåŒ…æ‹¬çº¿æ€§å’Œå¹¿ä¹‰çº¿æ€§æ¨¡å‹ã€‚è¯¥æ¡†æ¶æä¾›äº†ä¸€ç§å¤„ç†è¿ç»­æˆ–åˆ†ç±»å˜é‡çš„çµæ´»æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸¤ä¸ªå…·ä½“çš„åº”ç”¨ï¼šåœ¨çº¿å¼ é‡è¡¥å…¨å’Œåœ¨çº¿äºŒå…ƒå¼ é‡å­¦ä¹ ã€‚ä¸ºäº†åº”å¯¹è¿™äº›æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†åœ¨çº¿é»æ›¼æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼Œåœ¨æ‰€æœ‰åº”ç”¨ç¨‹åºä¸­éƒ½å¯ä»¥æ ¹æ®é€‚å½“çš„æ¡ä»¶çº¿æ€§æ”¶æ•›å¹¶æ¢å¤ä½ç§©ç»„ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¸ºåœ¨çº¿å¼ é‡è¡¥å…¨å»ºç«‹äº†ç²¾ç¡®çš„é€ä¸ªæ¡ç›®é”™è¯¯ç•Œé™ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬çš„å·¥ä½œä»£è¡¨äº†é¦–æ¬¡å°è¯•åœ¨åœ¨çº¿ä½ç§©å¼ é‡æ¢å¤ä»»åŠ¡ä¸­çº³å…¥å™ªå£°çš„åŠªåŠ›ã€‚æœ‰è¶£çš„æ˜¯ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°åœ¨å­˜åœ¨å™ªå£°çš„æƒ…å†µä¸‹ï¼Œåœ¨è®¡ç®—å’Œç»Ÿè®¡æ–¹é¢å­˜åœ¨ç€ä»¤äººæƒŠè®¶çš„æƒè¡¡ã€‚å¢åŠ æ­¥é•¿å¯ä»¥åŠ å¿«æ”¶æ•›ï¼Œä½†ä¼šå¯¼è‡´æ›´é«˜çš„ç»Ÿè®¡è¯¯å·®ã€‚

    We investigate a generalized framework for estimating latent low-rank tensors in an online setting, encompassing both linear and generalized linear models. This framework offers a flexible approach for handling continuous or categorical variables. Additionally, we investigate two specific applications: online tensor completion and online binary tensor learning. To address these challenges, we propose the online Riemannian gradient descent algorithm, which demonstrates linear convergence and the ability to recover the low-rank component under appropriate conditions in all applications. Furthermore, we establish a precise entry-wise error bound for online tensor completion. Notably, our work represents the first attempt to incorporate noise in the online low-rank tensor recovery task. Intriguingly, we observe a surprising trade-off between computational and statistical aspects in the presence of noise. Increasing the step size accelerates convergence but leads to higher statistical error
    
[^16]: å¯¹æ¯”å­¦ä¹ ä¸­çš„æŠ•å½±å¤´ï¼šæ‰©å±•å’Œæ”¶ç¼©çš„å¯ç¤º

    Unraveling Projection Heads in Contrastive Learning: Insights from Expansion and Shrinkage. (arXiv:2306.03335v1 [stat.ML])

    [http://arxiv.org/abs/2306.03335](http://arxiv.org/abs/2306.03335)

    æœ¬æ–‡ç ”ç©¶äº†å¯¹æ¯”å­¦ä¹ ä¸­çš„æŠ•å½±å¤´ï¼Œåœ¨ç†è®ºå’Œå®è·µä¸­æ‰¾åˆ°äº†ä¸¤ä¸ªå…³é”®æ•ˆåº”ï¼šä¿¡å·æ–¹å‘çš„æ‰©å±•å’Œæ”¶ç¼©ï¼Œæå‡ºäº†ä¸€ç³»åˆ—çº¿æ€§å˜æ¢æ¥æ”¹å–„ä¸‹æ¸¸åˆ†ç±»å‡†ç¡®æ€§ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†å¯¹æ¯”å­¦ä¹ ä¸­ç¼–ç å™¨-æŠ•å½±å™¨æ¡†æ¶ï¼ˆä¾‹å¦‚SimCLRï¼‰ä¸­çš„æŠ•å½±å¤´ï¼Œä¹Ÿç§°ä¸ºæŠ•å½±ä»ªï¼Œçš„ä½œç”¨ã€‚æˆ‘ä»¬æ—¨åœ¨æ­ç¤ºä¸€ä¸ªè§‚å¯Ÿç°è±¡çš„çœŸç›¸ï¼šé€šè¿‡ä¸‹æ¸¸çº¿æ€§åˆ†ç±»å‡†ç¡®åº¦çš„è¡¡é‡ï¼Œå³ä½¿åœ¨æŠ•å½±å¤´æœ¬èº«æ˜¯çº¿æ€§çš„æƒ…å†µä¸‹ï¼Œä¹Ÿå¯ä»¥å­¦ä¹ å‡ºåœ¨æŠ•å½±å™¨ä¹‹å‰çš„è¡¨ç¤ºä¼˜äºä¹‹åã€‚é€šè¿‡å®è¯å’Œç†è®ºåˆ†æï¼Œæˆ‘ä»¬é¦–å…ˆç¡®å®šäº†ä¸¤ä¸ªç”±å¯¹æ¯”æŸå¤±å¼•èµ·çš„å…³é”®æ•ˆåº”ã€‚æœ¬è´¨ä¸Šï¼Œå¯¹æ¯”æŸå¤±ä¼šæ‰©å±•æˆ–æ”¶ç¼©ç¼–ç å™¨å­¦ä¹ çš„è¡¨ç¤ºä¸­çš„ä¿¡å·æ–¹å‘ï¼Œå…·ä½“å–å†³äºå¦‚å¢å¼ºå¼ºåº¦ï¼Œå¯¹æ¯”æŸå¤±ä¸­ä½¿ç”¨çš„æ¸©åº¦ç­‰å› ç´ ã€‚å…¶æ¬¡ï¼Œå—åˆ°æ‰©å±•å’Œæ”¶ç¼©ç°è±¡çš„å¯ç¤ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç³»åˆ—çº¿æ€§å˜æ¢æ¥å‡†ç¡®å»ºæ¨¡æŠ•å½±å¤´ã€‚æˆ‘ä»¬æå‡ºçš„å˜æ¢å¯ä»¥æé«˜ä¸‹æ¸¸çš„åˆ†ç±»å‡†ç¡®åº¦ï¼Œè€Œä¸”è®¡ç®—æˆæœ¬ä½ï¼Œæ˜“äºå®ç°ã€‚

    We investigate the role of projection heads, also known as projectors, within the encoder-projector framework (e.g., SimCLR) used in contrastive learning. We aim to demystify the observed phenomenon where representations learned before projectors outperform those learned after -- measured using the downstream linear classification accuracy, even when the projectors themselves are linear.  In this paper, we make two significant contributions towards this aim. Firstly, through empirical and theoretical analysis, we identify two crucial effects -- expansion and shrinkage -- induced by the contrastive loss on the projectors. In essence, contrastive loss either expands or shrinks the signal direction in the representations learned by an encoder, depending on factors such as the augmentation strength, the temperature used in contrastive loss, etc. Secondly, drawing inspiration from the expansion and shrinkage phenomenon, we propose a family of linear transformations to accurately model the p
    
[^17]: å¸¦æƒé‡ç©ºé—´ä¸ŠåŠŸèƒ½æ€§è¾“å…¥æ˜ å°„çš„å…¨å±€æ™®é€‚é€¼è¿‘

    Global universal approximation of functional input maps on weighted spaces. (arXiv:2306.03303v1 [stat.ML])

    [http://arxiv.org/abs/2306.03303](http://arxiv.org/abs/2306.03303)

    æœ¬æ–‡æå‡ºäº†åŠŸèƒ½æ€§è¾“å…¥ç¥ç»ç½‘ç»œï¼Œå¯ä»¥åœ¨å¸¦æƒé‡ç©ºé—´ä¸Šå®Œæˆå…¨å±€å‡½æ•°é€¼è¿‘ã€‚è¿™ä¸€æ–¹æ³•é€‚ç”¨äºè¿ç»­å‡½æ•°çš„æ¨å¹¿ï¼Œè¿˜å¯ç”¨äºè·¯å¾„ç©ºé—´å‡½æ•°çš„é€¼è¿‘ï¼ŒåŒæ—¶ä¹Ÿå¯ä»¥é€¼è¿‘çº¿æ€§å‡½æ•°ç­¾åã€‚

    

    æˆ‘ä»¬å¼•å…¥äº†æ‰€è°“çš„åŠŸèƒ½æ€§è¾“å…¥ç¥ç»ç½‘ç»œï¼Œå®šä¹‰åœ¨å¯èƒ½æ˜¯æ— é™ç»´å¸¦æƒé‡ç©ºé—´ä¸Šï¼Œå…¶å€¼ä¹Ÿåœ¨å¯èƒ½æ˜¯æ— é™ç»´çš„è¾“å‡ºç©ºé—´ä¸­ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªåŠ æ€§æ—ä½œä¸ºéšè—å±‚æ˜ å°„ï¼Œä»¥åŠä¸€ä¸ªéçº¿æ€§æ¿€æ´»å‡½æ•°åº”ç”¨äºæ¯ä¸ªéšè—å±‚ã€‚ä¾é å¸¦æƒé‡ç©ºé—´ä¸Šçš„Stone-Weierstrasså®šç†ï¼Œæˆ‘ä»¬å¯ä»¥è¯æ˜è¿ç»­å‡½æ•°çš„æ¨å¹¿çš„å…¨å±€æ™®é€‚é€¼è¿‘ç»“æœï¼Œè¶…è¶Šäº†å¸¸è§„ç´§é›†é€¼è¿‘ã€‚è¿™ç‰¹åˆ«é€‚ç”¨äºé€šè¿‡åŠŸèƒ½æ€§è¾“å…¥ç¥ç»ç½‘ç»œé€¼è¿‘ï¼ˆéå…ˆè§ä¹‹æ˜çš„ï¼‰è·¯å¾„ç©ºé—´å‡½æ•°ã€‚ä½œä¸ºå¸¦æƒStone-Weierstrasså®šç†çš„è¿›ä¸€æ­¥åº”ç”¨ï¼Œæˆ‘ä»¬è¯æ˜äº†çº¿æ€§å‡½æ•°ç­¾åçš„å…¨å±€æ™®é€‚é€¼è¿‘ç»“æœã€‚æˆ‘ä»¬è¿˜åœ¨è¿™ä¸ªè®¾ç½®ä¸­å¼•å…¥äº†é«˜æ–¯è¿‡ç¨‹å›å½’çš„è§‚ç‚¹ï¼Œå¹¶å±•ç¤ºäº†ç­¾åå†…æ ¸çš„å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´æ˜¯æŸäº›é«˜æ–¯è¿‡ç¨‹çš„Cameron-Martinç©ºé—´ã€‚

    We introduce so-called functional input neural networks defined on a possibly infinite dimensional weighted space with values also in a possibly infinite dimensional output space. To this end, we use an additive family as hidden layer maps and a non-linear activation function applied to each hidden layer. Relying on Stone-Weierstrass theorems on weighted spaces, we can prove a global universal approximation result for generalizations of continuous functions going beyond the usual approximation on compact sets. This then applies in particular to approximation of (non-anticipative) path space functionals via functional input neural networks. As a further application of the weighted Stone-Weierstrass theorem we prove a global universal approximation result for linear functions of the signature. We also introduce the viewpoint of Gaussian process regression in this setting and show that the reproducing kernel Hilbert space of the signature kernels are Cameron-Martin spaces of certain Gauss
    
[^18]: åˆ‡æ¢è‡ªå›å½’ä½ç§©å¼ é‡æ¨¡å‹

    Switching Autoregressive Low-rank Tensor Models. (arXiv:2306.03291v1 [cs.LG])

    [http://arxiv.org/abs/2306.03291](http://arxiv.org/abs/2306.03291)

    è¯¥æ–‡æå‡ºäº†ä¸€ç§åˆ‡æ¢è‡ªå›å½’ä½ç§©å¼ é‡ï¼ˆSALTï¼‰æ¨¡å‹ï¼Œå®ƒå°†è‡ªå›å½’éšMarkovæ¨¡å‹ï¼ˆARHMMï¼‰å’Œåˆ‡æ¢çº¿æ€§åŠ¨æ€ç³»ç»Ÿï¼ˆSLDSï¼‰çš„ä¼˜ç‚¹ç»“åˆèµ·æ¥ï¼Œé€šè¿‡ä½ç§©å‚æ•°åŒ–æé«˜äº†æ¨¡å‹æ€§èƒ½ã€‚

    

    æ—¶åºåˆ†æä¸­ä¸€ä¸ªé‡è¦çš„é—®é¢˜æ˜¯å¯¹å…·æœ‰æ—¶å˜åŠ¨åŠ›å­¦çš„ç³»ç»Ÿè¿›è¡Œå»ºæ¨¡ã€‚å…±åŒè¿ç»­å’Œç¦»æ•£æ½œæ€çš„æ¦‚ç‡æ¨¡å‹ä¸ºè¿™æ ·çš„æ•°æ®æä¾›äº†å¯è§£é‡Šã€é«˜æ•ˆå’Œå®éªŒæ€§æœ‰ç”¨çš„æè¿°ã€‚å¸¸ç”¨çš„æ¨¡å‹åŒ…æ‹¬è‡ªå›å½’éšMarkovæ¨¡å‹ï¼ˆARHMMï¼‰å’Œåˆ‡æ¢çº¿æ€§åŠ¨æ€ç³»ç»Ÿï¼ˆSLDSï¼‰ï¼Œå®ƒä»¬å„æœ‰ä¼˜ç¼ºç‚¹ã€‚ARHMMå…è®¸ç²¾ç¡®æ¨ç†å’Œç®€å•çš„å‚æ•°ä¼°è®¡ï¼Œä½†åœ¨å¯¹é•¿ä¾èµ–å…³ç³»å»ºæ¨¡æ—¶å…·æœ‰å‚æ•°å¯†é›†æ€§ï¼Œå› æ­¤å®¹æ˜“å‡ºç°è¿‡æ‹Ÿåˆã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œé€šè¿‡é©¬å°”å¯å¤«æ½œæ€åŠ¨åŠ›å­¦ï¼ŒSLDSå¯ä»¥ä»¥å‚æ•°é«˜æ•ˆçš„æ–¹å¼æ•æ‰é•¿è·ç¦»ä¾èµ–æ€§ï¼Œä½†å›°éš¾çš„å‚æ•°ä¼°è®¡ä»»åŠ¡å’Œä¸€ä¸ªéš¾ä»¥å¤„ç†çš„ä¼¼ç„¶å‡½æ•°å´æ˜¯å…¶å…·æœ‰æŒ‘æˆ˜æ€§çš„åœ°æ–¹ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†åˆ‡æ¢è‡ªå›å½’ä½ç§©å¼ é‡ï¼ˆSALTï¼‰æ¨¡å‹ï¼Œè¯¥æ¨¡å‹ä¿ç•™äº†ä¸¤ç§æ–¹æ³•çš„ä¼˜ç‚¹ï¼ŒåŒæ—¶æ”¹å–„äº†å…¶å±€é™æ€§ã€‚SALTå°†ARHMMçš„å¼ é‡å‚æ•°åŒ–ä¸ºä½ç§©å½¢å¼ã€‚

    An important problem in time-series analysis is modeling systems with time-varying dynamics. Probabilistic models with joint continuous and discrete latent states offer interpretable, efficient, and experimentally useful descriptions of such data. Commonly used models include autoregressive hidden Markov models (ARHMMs) and switching linear dynamical systems (SLDSs), each with its own advantages and disadvantages. ARHMMs permit exact inference and easy parameter estimation, but are parameter intensive when modeling long dependencies, and hence are prone to overfitting. In contrast, SLDSs can capture long-range dependencies in a parameter efficient way through Markovian latent dynamics, but present an intractable likelihood and a challenging parameter estimation task. In this paper, we propose switching autoregressive low-rank tensor (SALT) models, which retain the advantages of both approaches while ameliorating the weaknesses. SALT parameterizes the tensor of an ARHMM with a low-rank 
    
[^19]: ä»ä¼—åŒ…æ ‡ç­¾è¿›è¡Œæ·±åº¦å­¦ä¹ ï¼šè€¦åˆäº¤å‰ç†µæœ€å°åŒ–ã€å¯è¯†åˆ«æ€§å’Œæ­£åˆ™åŒ–

    Deep Learning From Crowdsourced Labels: Coupled Cross-entropy Minimization, Identifiability, and Regularization. (arXiv:2306.03288v1 [cs.LG])

    [http://arxiv.org/abs/2306.03288](http://arxiv.org/abs/2306.03288)

    æœ¬æ–‡æå‡ºäº†ä½¿ç”¨ä¼—åŒ…æ ‡ç­¾è¿›è¡Œæ·±åº¦å­¦ä¹ çš„æ–¹æ³•ï¼Œé€šè¿‡è€¦åˆäº¤å‰ç†µæœ€å°åŒ–å’Œæ­£åˆ™åŒ–ä½¿å­¦ä¹ è¿‡ç¨‹æ›´åŠ é²æ£’ï¼ŒåŒæ—¶æå‡ºäº†æ€§èƒ½ä¿è¯ã€‚

    

    ä½¿ç”¨å¤šä¸ªæ³¨é‡Šè€…æä¾›çš„æœ‰å™ªå£°çš„ä¼—åŒ…æ ‡ç­¾ï¼Œä¸€ä¸ªåŸºäºæ·±åº¦å­¦ä¹ çš„ç«¯åˆ°ç«¯ (E2E) ç³»ç»Ÿæ—¨åœ¨åŒæ—¶å­¦ä¹ æ ‡ç­¾æ ¡æ­£æœºåˆ¶å’Œç¥ç»åˆ†ç±»å™¨ã€‚è€¦åˆäº¤å‰ç†µæœ€å°åŒ– (CCEM) ç±»å‹å‡†åˆ™ç›´è§‚ä¸”åœ¨å®è·µä¸­è¡¨ç°è‰¯å¥½ã€‚æœ¬æ–‡æå‡ºäº†å¯¹ CCEM å‡†åˆ™çš„æ€§èƒ½ä¿è¯ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ­£åˆ™åŒ–æ–¹æ³•ï¼Œä»è€Œä½¿å­¦ä¹ è¿‡ç¨‹æ›´åŠ é²æ£’ã€‚

    Using noisy crowdsourced labels from multiple annotators, a deep learning-based end-to-end (E2E) system aims to learn the label correction mechanism and the neural classifier simultaneously. To this end, many E2E systems concatenate the neural classifier with multiple annotator-specific ``label confusion'' layers and co-train the two parts in a parameter-coupled manner. The formulated coupled cross-entropy minimization (CCEM)-type criteria are intuitive and work well in practice. Nonetheless, theoretical understanding of the CCEM criterion has been limited. The contribution of this work is twofold: First, performance guarantees of the CCEM criterion are presented. Our analysis reveals for the first time that the CCEM can indeed correctly identify the annotators' confusion characteristics and the desired ``ground-truth'' neural classifier under realistic conditions, e.g., when only incomplete annotator labeling and finite samples are available. Second, based on the insights learned from
    
[^20]: é€šè¿‡é‡æ–°æ€è€ƒæ°‘é—´å¨æ–¯è´¹å‹’-è±æ›¼ç®—æ³•ï¼Œå®ç°$O(n^2)$ç©ºé—´å†…ä»»æ„è¡¨è¾¾èƒ½åŠ›çš„GNNs

    Towards Arbitrarily Expressive GNNs in $O(n^2)$ Space by Rethinking Folklore Weisfeiler-Lehman. (arXiv:2306.03266v1 [cs.LG])

    [http://arxiv.org/abs/2306.03266](http://arxiv.org/abs/2306.03266)

    æœ¬æ–‡æå‡ºäº†$(k, t)$-FWLå’Œ$k$-FWL+ä¸¤ç§æ–¹æ³•ï¼Œç†è®ºä¸Šè¯æ˜äº†å®ƒä»¬å¯ä»¥åœ¨$O(n^2)$çš„ç©ºé—´å¤æ‚åº¦ä¸‹ï¼Œè§£å†³å›¾åŒæ„é—®é¢˜ã€‚

    

    è¿‘å¹´æ¥ï¼Œæ¶ˆæ¯ä¼ é€’ç¥ç»ç½‘ç»œï¼ˆMPNNsï¼‰å·²æˆä¸ºå›¾ç¥ç»ç½‘ç»œï¼ˆGNNsï¼‰ä¸­æœ€å—æ¬¢è¿çš„æ¡†æ¶ã€‚ç„¶è€Œï¼Œå…¶è¡¨è¾¾èƒ½åŠ›å—åˆ°ä¸€ç»´å¨æ–¯è´¹å‹’-è±æ›¼ï¼ˆ1-WLï¼‰æµ‹è¯•çš„é™åˆ¶ã€‚ä¸€äº›ç ”ç©¶å—åˆ°$k$-WL/FWLï¼ˆæ°‘é—´WLï¼‰çš„å¯å‘å¹¶è®¾è®¡å…¶ç›¸åº”çš„ç¥ç»ç‰ˆæœ¬ã€‚å°½ç®¡å…·æœ‰å¾ˆé«˜çš„è¡¨è¾¾èƒ½åŠ›ï¼Œä½†è¿™ä¸€ç ”ç©¶æ–¹å‘å­˜åœ¨ä¸¥é‡å±€é™æ€§ã€‚ä¸ºè§£å†³è¿™äº›é—®é¢˜ï¼Œä½œè€…æå‡ºäº†$(k, t)$-FWLå’Œ$k$-FWL+ï¼Œå¹¶åœ¨ç†è®ºä¸Šè¯æ˜äº†å®ƒä»¬çš„æœ‰æ•ˆæ€§ã€‚

    Message passing neural networks (MPNNs) have emerged as the most popular framework of graph neural networks (GNNs) in recent years. However, their expressive power is limited by the 1-dimensional Weisfeiler-Lehman (1-WL) test. Some works are inspired by $k$-WL/FWL (Folklore WL) and design the corresponding neural versions. Despite the high expressive power, there are serious limitations in this line of research. In particular, (1) $k$-WL/FWL requires at least $O(n^k)$ space complexity, which is impractical for large graphs even when $k=3$; (2) The design space of $k$-WL/FWL is rigid, with the only adjustable hyper-parameter being $k$. To tackle the first limitation, we propose an extension, $(k, t)$-FWL. We theoretically prove that even if we fix the space complexity to $O(n^2)$ in $(k, t)$-FWL, we can construct an expressiveness hierarchy up to solving the graph isomorphism problem. To tackle the second problem, we propose $k$-FWL+, which considers any equivariant set as neighbors ins
    
[^21]: è§£é‡Šä¸è°ƒæ•´å›¾å½¢æ¡ä»¶è½¬ç§»

    Explaining and Adapting Graph Conditional Shift. (arXiv:2306.03256v1 [cs.LG])

    [http://arxiv.org/abs/2306.03256](http://arxiv.org/abs/2306.03256)

    æœ¬ç ”ç©¶é€šè¿‡é‡åŒ–è¾“å…¥ç‰¹å¾å’Œè¾“å‡ºæ ‡ç­¾ä¹‹é—´çš„æ¡ä»¶åç§»é‡ï¼Œå¯¹å›¾ç¥ç»ç½‘ç»œæ˜“å—åˆ†å¸ƒåç§»å½±å“çš„é—®é¢˜è¿›è¡Œç†è®ºåˆ†æã€‚ç ”ç©¶å‘ç°ï¼Œå›¾å½¢å¼‚è´¨æ€§å’Œæ¨¡å‹æ¶æ„éƒ½ä¼šå¯¼è‡´æ¡ä»¶åç§»ï¼Œå½±å“æ€§èƒ½ã€‚ä½œè€…æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡æ¡ä»¶åç§»çš„ä¼°è®¡å’Œæœ€å°åŒ–æ¥åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œè¯¥æ–¹æ³•åœ¨èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡ä¸Šè¡¨ç°æ›´ä¼˜ã€‚

    

    å›¾ç¥ç»ç½‘ç»œåœ¨å›¾ç»“æ„æ•°æ®ä¸Šè¡¨ç°å‡ºå“è¶Šçš„æ€§èƒ½ã€‚ç„¶è€Œï¼Œæœ€è¿‘çš„å®è¯ç ”ç©¶è¡¨æ˜ï¼ŒGNNéå¸¸å®¹æ˜“å—åˆ°åˆ†å¸ƒåç§»çš„å½±å“ã€‚ç›®å‰å…³äºä¸ºä»€ä¹ˆåŸºäºå›¾å½¢çš„æ¨¡å‹ä¼¼ä¹æ›´å®¹æ˜“å—åˆ°è¿™äº›åç§»å½±å“çš„é—®é¢˜è¿˜å­˜åœ¨æ˜¾è‘—çš„æ­§ä¹‰ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡é‡åŒ–è¾“å…¥ç‰¹å¾å’Œè¾“å‡ºæ ‡ç­¾ä¹‹é—´çš„æ¡ä»¶åç§»é‡ï¼Œå¯¹å®ƒè¿›è¡Œäº†å½»åº•çš„ç†è®ºåˆ†æã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå›¾å½¢å¼‚è´¨æ€§å’Œæ¨¡å‹æ¶æ„éƒ½åŠ å‰§äº†æ¡ä»¶åç§»ï¼Œå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚ä¸ºäº†åº”å¯¹è¿™ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–¹æ³•ï¼Œæ¶‰åŠå¯¹å›¾å½¢ä¸Šçš„æ— ç›‘ç£åŸŸé€‚åº”æ€§è¿›è¡Œæ¡ä»¶åç§»çš„ä¼°è®¡å’Œæœ€å°åŒ–ã€‚åœ¨æˆ‘ä»¬çš„æ§åˆ¶æ€§ç»¼åˆå®éªŒä¸­ï¼Œæˆ‘ä»¬çš„ç®—æ³•è¡¨ç°å‡ºå¯¹åˆ†å¸ƒåç§»çš„é²æ£’æ€§ï¼Œç›¸å¯¹ç¬¬äºŒä¼˜ç®—æ³•å®ç°äº†é«˜è¾¾10%çš„ROC AUCç»å¯¹æ”¹å–„ã€‚æ­¤å¤–ï¼Œå¯¹èŠ‚ç‚¹åˆ†ç±»å’Œå›¾åˆ†ç±»ä»»åŠ¡çš„å…¨é¢å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å§‹ç»ˆä¼˜äºæœ€å…ˆè¿›çš„åŸŸé€‚åº”æ–¹æ³•ã€‚

    Graph Neural Networks (GNNs) have shown remarkable performance on graph-structured data. However, recent empirical studies suggest that GNNs are very susceptible to distribution shift. There is still significant ambiguity about why graph-based models seem more vulnerable to these shifts. In this work we provide a thorough theoretical analysis on it by quantifying the magnitude of conditional shift between the input features and the output label. Our findings show that both graph heterophily and model architecture exacerbate conditional shifts, leading to performance degradation. To address this, we propose an approach that involves estimating and minimizing the conditional shift for unsupervised domain adaptation on graphs. In our controlled synthetic experiments, our algorithm demonstrates robustness towards distribution shift, resulting in up to 10% absolute ROC AUC improvement versus the second-best algorithm. Furthermore, comprehensive experiments on both node classification and gr
    
[^22]: éçº¿æ€§åˆ†å¸ƒé²æ£’ä¼˜åŒ–

    Nonlinear Distributionally Robust Optimization. (arXiv:2306.03202v1 [stat.ML])

    [http://arxiv.org/abs/2306.03202](http://arxiv.org/abs/2306.03202)

    æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„éçº¿æ€§åˆ†å¸ƒé²æ£’ä¼˜åŒ–ç®—æ³•ï¼Œç”¨äºå¤„ç†ä¸€ç±»åˆ†å¸ƒé²æ£’ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡ Gateaux Derivative å¤„ç†ä¸€èˆ¬é£é™©åº¦é‡ã€‚ç»è¿‡å®éªŒéªŒè¯ï¼Œè¯¥æ–¹æ³•æˆåŠŸå¤„ç†åˆ†å¸ƒçš„éçº¿æ€§ç›®æ ‡å‡½æ•°ã€‚

    

    æœ¬æ–‡å…³æ³¨ä¸€ç±»åˆ†å¸ƒé²æ£’ä¼˜åŒ–ï¼ˆDROï¼‰é—®é¢˜ï¼Œå…¶ä¸­ç›®æ ‡å‡½æ•°åœ¨åˆ†å¸ƒä¸Šå¯èƒ½æ˜¯éçº¿æ€§çš„ï¼Œè¿™ä¸ç°æœ‰çš„æ–‡çŒ®æœ‰æ‰€ä¸åŒã€‚ä¸ºè§£å†³åœ¨æ¦‚ç‡ç©ºé—´ä¸­ä¼˜åŒ–éçº¿æ€§å‡½æ•°é¢ä¸´çš„ç†è®ºå’Œè®¡ç®—æŒ‘æˆ˜ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§Derivativeå’Œç›¸åº”çš„å¹³æ»‘åº¦æ¦‚å¿µï¼ŒåŸºäºGateaux Derivativeæ¥å¤„ç†ä¸€èˆ¬é£é™©åº¦é‡ã€‚æˆ‘ä»¬é€šè¿‡Varã€entropic riskå’Œæœ‰é™æ”¯æŒé›†ä¸Šçš„ä¸‰ä¸ªè¿è¡Œé£é™©åº¦é‡ç¤ºä¾‹æ¥è§£é‡Šè¿™äº›æ¦‚å¿µã€‚ç„¶åï¼Œæˆ‘ä»¬ä¸ºæ¦‚ç‡ç©ºé—´ä¸­ä¸€èˆ¬éçº¿æ€§ä¼˜åŒ–é—®é¢˜æå‡ºäº†ä¸€ç§åŸºäºG-derivativeçš„Frank-Wolfeï¼ˆFWï¼‰ç®—æ³•ï¼Œå¹¶ä»¥å®Œå…¨ç‹¬ç«‹äºèŒƒæ•°çš„æ–¹å¼æ¨å¯¼å‡ºå…¶æ”¶æ•›æ€§åœ¨æå‡ºçš„å¹³æ»‘åº¦æ¦‚å¿µä¸‹ã€‚æˆ‘ä»¬åˆ©ç”¨FWç®—æ³•çš„è®¾ç½®æ¥è®¾è®¡ä¸€ç§è®¡ç®—éçº¿æ€§DROé—®é¢˜éç‚¹çš„æ–¹æ³•ã€‚æˆ‘ä»¬é€šè¿‡æ•°å€¼å®éªŒå±•ç¤ºäº†æˆ‘ä»¬æ–¹æ³•å¤„ç†åˆ†å¸ƒçš„éçº¿æ€§ç›®æ ‡å‡½æ•°çš„æˆåŠŸã€‚

    This article focuses on a class of distributionally robust optimization (DRO) problems where, unlike the growing body of the literature, the objective function is potentially non-linear in the distribution. Existing methods to optimize nonlinear functions in probability space use the Frechet derivatives, which present both theoretical and computational challenges. Motivated by this, we propose an alternative notion for the derivative and corresponding smoothness based on Gateaux (G)-derivative for generic risk measures. These concepts are explained via three running risk measure examples of variance, entropic risk, and risk on finite support sets. We then propose a G-derivative based Frank-Wolfe~(FW) algorithm for generic non-linear optimization problems in probability spaces and establish its convergence under the proposed notion of smoothness in a completely norm-independent manner. We use the set-up of the FW algorithm to devise a methodology to compute a saddle point of the non-lin
    
[^23]: åŸºäºæ ¼ç‚¹å¯¹æ³¨æ„æœºåˆ¶è¿›è¡Œå…ˆéªŒåŠ å…¥ï¼Œä»¥æé«˜æŠ½è±¡å‡ ä½•æ¨ç†çš„æ ·æœ¬æ•ˆç‡

    Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning. (arXiv:2306.03175v1 [cs.AI])

    [http://arxiv.org/abs/2306.03175](http://arxiv.org/abs/2306.03175)

    LatFormeræ˜¯ä¸€ç§å°†æ ¼ç‚¹å¯¹ç§°å…ˆéªŒèå…¥åˆ°æ³¨æ„åŠ›æ©ç ä¸­çš„æ¨¡å‹ï¼Œèƒ½å¤Ÿç”¨å·ç§¯ç½‘ç»œç”Ÿæˆè½¯æ©ç æ¥è°ƒæ•´æ³¨æ„åŠ›æƒé‡ã€‚è¯¥æ¨¡å‹åœ¨åˆæˆå‡ ä½•æ¨ç†ä¸­å–å¾—äº†è¾ƒå¥½æ•ˆæœã€‚

    

    æŠ½è±¡å’Œæ¨ç†è¯­æ–™åº“ï¼ˆARCï¼‰åŠå…¶æœ€è¿‘çš„è¯­è¨€å®Œæ•´å®ä¾‹ï¼ˆLARCï¼‰è¢«è®¤ä¸ºæ˜¯é€šå¾€é€šç”¨äººå·¥æ™ºèƒ½çš„é‡è¦ä¸€æ­¥ã€‚ç„¶è€Œï¼Œå³ä½¿æ˜¯æœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨è¿™äº›é—®é¢˜ä¸Šä¹Ÿéš¾ä»¥å®ç°æœ‰æ„ä¹‰çš„æ€§èƒ½ï¼Œè½åäºéå­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬è®¤ä¸ºè§£å†³è¿™äº›ä»»åŠ¡éœ€è¦æç«¯çš„æ³›åŒ–èƒ½åŠ›ï¼Œåªæœ‰é€šè¿‡é€‚å½“è€ƒè™‘æ ¸å¿ƒçŸ¥è¯†å…ˆéªŒæ‰èƒ½å®ç°ã€‚ä¸ºäº†è¾¾åˆ°è¿™ä¸ªç›®æ ‡ï¼Œæˆ‘ä»¬èšç„¦äºå‡ ä½•å…ˆéªŒï¼Œå¹¶å¼•å…¥LatFormeræ¨¡å‹ï¼Œå°†æ ¼ç‚¹å¯¹ç§°å…ˆéªŒèå…¥åˆ°æ³¨æ„åŠ›æ©ç ä¸­ã€‚æˆ‘ä»¬è¯æ˜äº†å¯¹äºè¶…ç«‹æ–¹æ ¼çš„ä»»ä½•å˜æ¢ï¼Œéƒ½å­˜åœ¨ä¸€ä¸ªäºŒå€¼æ³¨æ„åŠ›æ©ç æ¥å®ç°è¯¥ç¾¤ä½œç”¨ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç ”ç©¶æ¿€å‘äº†å¯¹æ ‡å‡†æ³¨æ„åŠ›æœºåˆ¶çš„ä¿®æ”¹ï¼Œå…¶ä¸­ä½¿ç”¨å·ç§¯ç½‘ç»œç”Ÿæˆçš„è½¯æ©ç æ¥è°ƒæ•´å…³æ³¨æƒé‡ã€‚åœ¨åˆæˆå‡ ä½•æ¨ç†æ–¹é¢çš„å®éªŒè¡¨æ˜ï¼ŒLatFormer

    The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer 
    
[^24]: é’ˆå¯¹ç¦»çº¿è®¾è®¡ç”Ÿç‰©åºåˆ—çš„å¾—åˆ†æ¡ä»¶ç”Ÿæˆå™¨çš„è‡ªåŠ©å¢å¼ºè®­ç»ƒ

    Bootstrapped Training of Score-Conditioned Generator for Offline Design of Biological Sequences. (arXiv:2306.03111v1 [q-bio.QM])

    [http://arxiv.org/abs/2306.03111](http://arxiv.org/abs/2306.03111)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§BootGenç®—æ³•ï¼Œä½¿ç”¨ä»£ç†å¾—åˆ†å‡½æ•°å¢å¼ºç”Ÿç‰©åºåˆ—ç”Ÿæˆå™¨çš„è®­ç»ƒæ•°æ®é›†ï¼Œå¹¶äº§ç”Ÿå¤šæ ·åŒ–çš„è®¾è®¡ï¼Œå°†å…¶åº”ç”¨äºä¼˜åŒ–ç”Ÿç‰©åºåˆ—ï¼Œå–å¾—äº†æ¯”ç«äº‰å¯¹æ‰‹æ›´å¥½çš„ç»“æœã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†ä¼˜åŒ–ç”Ÿç‰©åºåˆ—ï¼ˆå¦‚è›‹ç™½è´¨ã€DNAå’ŒRNAï¼‰ä»¥æœ€å¤§åŒ–ä»…åœ¨ç¦»çº¿æ•°æ®é›†ä¸­è¯„ä¼°çš„é»‘åŒ£å­å¾—åˆ†å‡½æ•°çš„é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°é¢–çš„è§£å†³æ–¹æ¡ˆâ€”â€”å¾—åˆ†æ¡ä»¶ç”Ÿæˆå™¨çš„è‡ªåŠ©å¢å¼ºè®­ç»ƒï¼ˆBootGenï¼‰ç®—æ³•ã€‚æˆ‘ä»¬çš„ç®—æ³•é‡å¤äº†ä¸€ä¸ªä¸¤é˜¶æ®µè¿‡ç¨‹ã€‚åœ¨ç¬¬ä¸€é˜¶æ®µï¼Œæˆ‘ä»¬çš„ç®—æ³•ä½¿ç”¨æ’ååŠ æƒæ³•è®­ç»ƒç”Ÿç‰©åºåˆ—ç”Ÿæˆå™¨ï¼Œä»¥æé«˜åŸºäºé«˜åˆ†æ•°çš„åºåˆ—ç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚æ¥ä¸‹æ¥çš„é˜¶æ®µæ¶‰åŠåˆ°è‡ªåŠ©å¢å¼ºï¼Œé€šè¿‡è‡ªåŠ¨ç”Ÿæˆçš„æ•°æ®å¹¶æ ‡è®°ä»£ç†å¾—åˆ†å‡½æ•°ï¼Œæ¥å¢å¼ºè®­ç»ƒæ•°æ®é›†ã€‚æˆ‘ä»¬çš„å…³é”®æ€æƒ³æ˜¯å°†åŸºäºå¾—åˆ†çš„ç”Ÿæˆä¸ä»£ç†å¾—åˆ†å‡½æ•°å¯¹é½ï¼Œå°†ä»£ç†å¾—åˆ†å‡½æ•°çš„çŸ¥è¯†ä¼ é€’ç»™ç”Ÿæˆå™¨ã€‚è®­ç»ƒåï¼Œæˆ‘ä»¬èšåˆæ¥è‡ªå¤šä¸ªè‡ªåŠ©å¢å¼ºç”Ÿæˆå™¨å’Œä»£ç†çš„æ ·æœ¬ï¼Œäº§ç”Ÿå¤šæ ·åŒ–çš„è®¾è®¡ã€‚å¤§é‡å®éªŒè¡¨æ˜ï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨ç”Ÿç‰©åºåˆ—ä¼˜åŒ–æ–¹é¢èƒœè¿‡ç«äº‰å¯¹æ‰‹ã€‚

    We study the problem of optimizing biological sequences, e.g., proteins, DNA, and RNA, to maximize a black-box score function that is only evaluated in an offline dataset. We propose a novel solution, bootstrapped training of score-conditioned generator (BootGen) algorithm. Our algorithm repeats a two-stage process. In the first stage, our algorithm trains the biological sequence generator with rank-based weights to enhance the accuracy of sequence generation based on high scores. The subsequent stage involves bootstrapping, which augments the training dataset with self-generated data labeled by a proxy score function. Our key idea is to align the score-based generation with a proxy score function, which distills the knowledge of the proxy score function to the generator. After training, we aggregate samples from multiple bootstrapped generators and proxies to produce a diverse design. Extensive experiments show that our method outperforms competitive baselines on biological sequential
    
[^25]: Mixupåœ¨å¯»æ‰¾æœ€ä½³å†³ç­–è¾¹ç•Œä¸­çš„å¯è¯å®ç›Šå¤„

    Provable Benefit of Mixup for Finding Optimal Decision Boundaries. (arXiv:2306.00267v1 [cs.LG])

    [http://arxiv.org/abs/2306.00267](http://arxiv.org/abs/2306.00267)

    æœ¬ç ”ç©¶è¯æ˜äº†ä½¿ç”¨Mixupè®­ç»ƒå…·æœ‰å¯è¯å®çš„ç›Šå¤„ï¼Œå¯ä»¥æ˜¾è‘—é™ä½åœ¨æ›´å¯åˆ†ç¦»æ•°æ®åˆ†å¸ƒä¸­å¯»æ‰¾æœ€ä½³å†³ç­–è¾¹ç•Œçš„æ ·æœ¬å¤æ‚åº¦ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†åƒMixupè¿™æ ·çš„æˆå¯¹æ•°æ®å¢å¼ºæŠ€æœ¯å¦‚ä½•å½±å“åœ¨äºŒå…ƒçº¿æ€§åˆ†ç±»é—®é¢˜ä¸­å¯»æ‰¾æœ€ä½³å†³ç­–è¾¹ç•Œçš„æ ·æœ¬å¤æ‚åº¦ã€‚é’ˆå¯¹ä¸€ç±»å…·æœ‰å¯åˆ†ç¦»å¸¸æ•°$\kappa$çš„æ•°æ®åˆ†å¸ƒï¼Œæˆ‘ä»¬åˆ†æäº†è®­ç»ƒæŸå¤±æœ€ä¼˜åˆ†ç±»å™¨ä¸æµ‹è¯•å‡†ç¡®ç‡æœ€ä¼˜åˆ†ç±»å™¨ï¼ˆå³è´å¶æ–¯æœ€ä¼˜åˆ†ç±»å™¨ï¼‰ä¹‹é—´çš„å¯¹é½ç¨‹åº¦ã€‚å¯¹äºæ²¡æœ‰å¢å¼ºçš„æ™®é€šè®­ç»ƒï¼Œæˆ‘ä»¬å‘ç°äº†ä¸€ç§æœ‰è¶£çš„ç°è±¡ï¼Œç§°ä¸ºå¯åˆ†ç¦»æ€§çš„è¯…å’’ã€‚éšç€æˆ‘ä»¬å¢åŠ $\kappa$ä½¿æ•°æ®åˆ†å¸ƒæ›´åŠ å¯åˆ†ç¦»ï¼Œæ™®é€šè®­ç»ƒçš„æ ·æœ¬å¤æ‚åº¦ä¼šåœ¨$\kappa$ä¸­å‘ˆæŒ‡æ•°å¢é•¿ã€‚ä¹Ÿè®¸æ›´ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œå¯¹äºæ›´å¯åˆ†ç¦»çš„æ•°æ®åˆ†å¸ƒè€Œè¨€ï¼Œå¯»æ‰¾æœ€ä½³å†³ç­–è¾¹ç•Œçš„ä»»åŠ¡å˜å¾—æ›´åŠ å›°éš¾ã€‚é’ˆå¯¹Mixupè®­ç»ƒï¼Œæˆ‘ä»¬å±•ç¤ºäº†Mixupå‡è½»äº†è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡æ˜¾è‘—é™ä½æ ·æœ¬å¤æ‚åº¦ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¼€å‘äº†é€‚ç”¨äºMixupè€ƒè™‘çš„$n^2$æˆå¯¹å¢å¼ºæ•°æ®ç‚¹çš„æ–°çš„é›†ä¸­ç»“æœã€‚æˆ‘ä»¬çš„ç»“æœæä¾›äº†å…³äºMixupçš„æ³›åŒ–ç›Šå¤„çš„å¯è¯ä¿è¯ï¼Œå¹¶æä¾›äº†ç†è§£Mixupä¸ºä»€ä¹ˆåœ¨å®è·µä¸­è¡¨ç°è‰¯å¥½çš„è§è§£ã€‚

    We investigate how pair-wise data augmentation techniques like Mixup affect the sample complexity of finding optimal decision boundaries in a binary linear classification problem. For a family of data distributions with a separability constant $\kappa$, we analyze how well the optimal classifier in terms of training loss aligns with the optimal one in test accuracy (i.e., Bayes optimal classifier). For vanilla training without augmentation, we uncover an interesting phenomenon named the curse of separability. As we increase $\kappa$ to make the data distribution more separable, the sample complexity of vanilla training increases exponentially in $\kappa$; perhaps surprisingly, the task of finding optimal decision boundaries becomes harder for more separable distributions. For Mixup training, we show that Mixup mitigates this problem by significantly reducing the sample complexity. To this end, we develop new concentration results applicable to $n^2$ pair-wise augmented data points cons
    
[^26]: çº¿æ€§ç¥ç»ç½‘ç»œå±‚ä¿ƒè¿›å­¦ä¹ å•æŒ‡æ•°å’Œå¤šæŒ‡æ•°æ¨¡å‹

    Linear Neural Network Layers Promote Learning Single- and Multiple-Index Models. (arXiv:2305.15598v1 [cs.LG])

    [http://arxiv.org/abs/2305.15598](http://arxiv.org/abs/2305.15598)

    æœ¬ç ”ç©¶æ¢ç©¶äº†è¿‡åº¦å‚æ•°åŒ–çš„æ·±åº¦ç¥ç»ç½‘ç»œçš„åè§ï¼Œå‘ç°åœ¨ReLUç½‘ç»œä¸­æ·»åŠ çº¿æ€§å±‚æœ‰åŠ©äºé€¼è¿‘å…·æœ‰ä½ç§©çº¿æ€§ç®—å­å’Œä½è¡¨ç¤ºæˆæœ¬å‡½æ•°ç»„æˆçš„å‡½æ•°ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªä¸ä½ç»´å­ç©ºé—´å‚ç›´æ–¹å‘è¿‘ä¹æ’å®šçš„æ’å€¼å‡½æ•°ã€‚

    

    æœ¬æ–‡æ¢ç©¶äº†æ·±åº¦å¤§äºä¸¤å±‚çš„è¿‡åº¦å‚æ•°åŒ–ç¥ç»ç½‘ç»œçš„éšå«åè§ã€‚æˆ‘ä»¬çš„æ¡†æ¶è€ƒè™‘äº†ä¸€ç±»æ·±åº¦ä¸åŒä½†å®¹é‡ç›¸åŒçš„ç½‘ç»œï¼Œå®ƒä»¬å…·æœ‰ä¸åŒçš„æ˜¾å¼å®šä¹‰çš„è¡¨ç¤ºæˆæœ¬ã€‚ç¥ç»ç½‘ç»œæ¶æ„è¯±å¯¼çš„å‡½æ•°çš„è¡¨ç¤ºæˆæœ¬æ˜¯ç½‘ç»œè¡¨ç¤ºè¯¥å‡½æ•°æ‰€éœ€çš„å¹³æ–¹æƒé‡ä¹‹å’Œçš„æœ€å°å€¼ï¼›å®ƒåæ˜ äº†ä¸è¯¥æ¶æ„ç›¸å…³çš„å‡½æ•°ç©ºé—´åå·®ã€‚ç»“æœè¡¨æ˜ï¼Œå°†çº¿æ€§å±‚æ·»åŠ åˆ°ReLUç½‘ç»œä¼šäº§ç”Ÿä¸€ä¸ªè¡¨ç¤ºæˆæœ¬ï¼Œè¿™æœ‰åˆ©äºä½¿ç”¨ä¸¤å±‚ç½‘ç»œæ¥é€¼è¿‘ç”±ä½ç§©çº¿æ€§ç®—å­å’Œå…·æœ‰ä½è¡¨ç¤ºæˆæœ¬çš„å‡½æ•°ç»„æˆçš„å‡½æ•°ã€‚å…·ä½“æ¥è¯´ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œä»¥æœ€å°çš„è¡¨ç¤ºæˆæœ¬æ‹Ÿåˆè®­ç»ƒæ•°æ®ä¼šå¾—åˆ°ä¸€ä¸ªä¸ä½ç»´å­ç©ºé—´å‚ç›´æ–¹å‘è¿‘ä¹æ’å®šçš„æ’å€¼å‡½æ•°ã€‚

    This paper explores the implicit bias of overparameterized neural networks of depth greater than two layers. Our framework considers a family of networks of varying depths that all have the same capacity but different implicitly defined representation costs. The representation cost of a function induced by a neural network architecture is the minimum sum of squared weights needed for the network to represent the function; it reflects the function space bias associated with the architecture. Our results show that adding linear layers to a ReLU network yields a representation cost that favors functions that can be approximated by a low-rank linear operator composed with a function with low representation cost using a two-layer network. Specifically, using a neural network to fit training data with minimum representation cost yields an interpolating function that is nearly constant in directions orthogonal to a low-dimensional subspace. This means that the learned network will approximate
    
[^27]: ä»»æ„ç¼ºå¤±æ¨¡å¼ä¸‹çš„æ— åˆ†å¸ƒçŸ©é˜µé¢„æµ‹

    Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern. (arXiv:2305.11640v1 [cs.LG])

    [http://arxiv.org/abs/2305.11640](http://arxiv.org/abs/2305.11640)

    æœ¬æ–‡æå‡ºäº†ä¸¤ç§å®ç”¨ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„ä¸¢å¤±æ¨¡å¼ä¸‹æœ‰æ•ˆåœ°ä¿è¯è¦†ç›–ç‡çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é‡åŒ–äº†ç¼ºå¤±å¯¹é¢„æµ‹ç²¾åº¦çš„å½±å“ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†åœ¨è¡Œ/åˆ—å¯äº¤æ¢çŸ©é˜µä¸­é¢„æµ‹ç¼ºå¤±æ¡ç›®çš„é—®é¢˜ã€‚è™½ç„¶çŸ©é˜µè®¾ç½®æå‡ºäº†æ–°é¢–å’Œç‹¬ç‰¹çš„æŒ‘æˆ˜ï¼Œä½†æ˜¯åœ¨è¿™ä¸ªæœ‰è¶£çš„ä¸»é¢˜ä¸Šå­˜åœ¨å¾ˆå°‘çš„å·¥ä½œã€‚æˆ‘ä»¬ç²¾ç»†åœ°å®šä¹‰äº†é—®é¢˜ï¼Œå°†å…¶ä¸å¯†åˆ‡ç›¸å…³çš„é—®é¢˜åŒºåˆ†å¼€æ¥ï¼Œå¹¶ä¸¥æ ¼åˆ’åˆ†äº†å¯è¾¾æˆå’Œä¸å¯èƒ½çš„ç›®æ ‡çš„è¾¹ç•Œã€‚ç„¶åæˆ‘ä»¬æå‡ºäº†ä¸¤ç§å®ç”¨ç®—æ³•ã€‚ç¬¬ä¸€ç§æ–¹æ³•æä¾›äº†å…¨é¢çš„é¢„æµ‹çš„å¿«é€Ÿä»¿çœŸï¼Œè€Œç¬¬äºŒç§æ–¹æ³•åˆ©ç”¨ç®—æ³•ç¨³å®šæ€§æŠ€æœ¯åŠ é€Ÿè®¡ç®—ã€‚è¿™ä¸¤ç§æ–¹æ³•è®¡ç®—æ•ˆç‡é«˜ï¼Œèƒ½å¤Ÿåœ¨ä»»æ„ä¸¢å¤±æ¨¡å¼ä¸‹æœ‰æ•ˆåœ°ä¿è¯è¦†ç›–ç‡çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡åŒ–äº†ç¼ºå¤±å¯¹é¢„æµ‹ç²¾åº¦çš„å½±å“ï¼Œå¹¶å»ºç«‹äº†åŸºæœ¬çš„æé™ç»“æœã€‚æ¥è‡ªåˆæˆå’ŒçœŸå®æ•°æ®é›†çš„ç»éªŒè¯æ®è¯å®äº†æˆ‘ä»¬æå‡ºçš„æ–¹æ³•çš„å“è¶Šæ€§èƒ½ã€‚

    This paper studies the open problem of conformalized entry prediction in a row/column-exchangeable matrix. The matrix setting presents novel and unique challenges, but there exists little work on this interesting topic. We meticulously define the problem, differentiate it from closely related problems, and rigorously delineate the boundary between achievable and impossible goals. We then propose two practical algorithms. The first method provides a fast emulation of the full conformal prediction, while the second method leverages the technique of algorithmic stability for acceleration. Both methods are computationally efficient and can effectively safeguard coverage validity in presence of arbitrary missing pattern. Further, we quantify the impact of missingness on prediction accuracy and establish fundamental limit results. Empirical evidence from synthetic and real-world data sets corroborates the superior performance of our proposed methods.
    
[^28]: ä»éšæœºæœç´¢åˆ°åº¦é‡æµ‹åº¦ç©ºé—´ä¸­çš„èµŒåšå­¦ä¹ 

    From Random Search to Bandit Learning in Metric Measure Spaces. (arXiv:2305.11509v1 [cs.LG])

    [http://arxiv.org/abs/2305.11509](http://arxiv.org/abs/2305.11509)

    æœ¬æ–‡ä»‹ç»äº†éšæœºæœç´¢åŠå…¶æ€§èƒ½ï¼Œå¼•å…¥äº†â€œæ•£å°„ç»´åº¦â€çš„æ¦‚å¿µï¼Œæè¿°äº†åº•å±‚å‡½æ•°çš„çŠ¶æ€ï¼Œé‡åŒ–äº†éšæœºæœç´¢çš„æ€§èƒ½ï¼Œå¹¶è¯æ˜äº†åœ¨æ— å™ªå£°å’Œæœ‰ç•Œå™ªå£°æƒ…å†µä¸‹çš„è¾“å‡ºåˆ†åˆ«ä»¥ä¸€å®šæ¦‚ç‡æ”¶æ•›åˆ°æœ€ä¼˜å€¼ã€‚

    

    éšæœºæœç´¢æ˜¯è¶…å‚æ•°ä¼˜åŒ–ä¸­æœ€å¸¸ç”¨çš„æ–¹æ³•ä¹‹ä¸€ï¼Œå¯¹äºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„æˆåŠŸè‡³å…³é‡è¦ã€‚å°½ç®¡å…¶æ€§èƒ½ä»¤äººæƒŠå¹ï¼Œä½†å¾ˆå°‘æœ‰éå¯å‘å¼çš„ç†è®ºç”¨äºæè¿°å…¶å·¥ä½œæœºåˆ¶ã€‚æœ¬æ–‡ç»™å‡ºäº†å…³äºéšæœºæœç´¢çš„ç†è®ºè§£é‡Šã€‚æˆ‘ä»¬å¼•å…¥äº†â€œæ•£å°„ç»´åº¦â€çš„æ¦‚å¿µï¼Œæè¿°äº†åº•å±‚å‡½æ•°çš„çŠ¶æ€ï¼Œå¹¶é‡åŒ–äº†éšæœºæœç´¢çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå½“ç¯å¢ƒæ²¡æœ‰å™ªå£°æ—¶ï¼Œéšæœºæœç´¢çš„è¾“å‡ºä»¥æ¦‚ç‡æ”¶æ•›åˆ°æœ€ä¼˜å€¼ï¼Œå…¶é€Ÿç‡ä¸º$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $ï¼Œå…¶ä¸­$ d_s \ge 0 $æ˜¯åº•å±‚å‡½æ•°çš„æ•£å°„ç»´åº¦ã€‚å½“è§‚å¯Ÿåˆ°çš„å‡½æ•°å€¼å—åˆ°æœ‰ç•Œçš„ç‹¬ç«‹åŒåˆ†å¸ƒå™ªå£°å½±å“æ—¶ï¼Œéšæœºæœç´¢çš„è¾“å‡ºä»¥æ¦‚ç‡æ”¶æ•›åˆ°æœ€ä¼˜å€¼ï¼Œé€Ÿç‡ä¸º$ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{2}{2+d_s} } \right) $ã€‚

    Random Search is one of the most widely-used method for Hyperparameter Optimization, and is critical to the success of deep learning models. Despite its astonishing performance, little non-heuristic theory has been developed to describe the underlying working mechanism. This paper gives a theoretical accounting of Random Search. We introduce the concept of \emph{scattering dimension} that describes the landscape of the underlying function, and quantifies the performance of random search. We show that, when the environment is noise-free, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering dimension of the underlying function. When the observed function values are corrupted by bounded $iid$ noise, the output of random search converges to the optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T} \rig
    
[^29]: åŸºäºFisherä¿¡æ¯åµŒå…¥çš„èŠ‚ç‚¹å’Œå›¾å­¦ä¹ 

    Fisher Information Embedding for Node and Graph Learning. (arXiv:2305.07580v1 [stat.ML])

    [http://arxiv.org/abs/2305.07580](http://arxiv.org/abs/2305.07580)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å›¾èŠ‚ç‚¹åµŒå…¥æ¡†æ¶ï¼Œå¯ä»¥æ›´å¥½åœ°ç†è§£åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„GNNã€‚

    

    åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰ï¼Œä¾‹å¦‚å›¾æ³¨æ„åŠ›ç½‘ç»œï¼ˆGATï¼‰ï¼Œå·²æˆä¸ºå¤„ç†å›¾ç»“æ„æ•°æ®å’Œå­¦ä¹ èŠ‚ç‚¹åµŒå…¥çš„æµè¡Œç¥ç»ç½‘ç»œç»“æ„ã€‚å°½ç®¡è¿™äº›æ¨¡å‹åœ¨ç»éªŒä¸Šå–å¾—äº†æˆåŠŸï¼Œä½†å®ƒä»¬ä¾èµ–äºæ ‡æ³¨æ•°æ®ï¼Œä¸”è¿™äº›æ¨¡å‹çš„ç†è®ºå±æ€§å°šæœªå®Œå…¨ç†è§£ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°é¢–çš„åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„å›¾èŠ‚ç‚¹åµŒå…¥æ¡†æ¶ã€‚æˆ‘ä»¬çš„æ¡†æ¶å»ºç«‹åœ¨ä¸€ç§å¤šé‡é›†åˆå†…èŠ‚ç‚¹å‘¨å›´å­å›¾çš„åˆ†å±‚æ ¸ä¹‹ä¸Šï¼ˆä¾‹å¦‚ï¼Œé‚»åŸŸï¼‰ï¼Œå¹¶ä¸”æ¯ä¸ªæ ¸åˆ©ç”¨å¹³æ»‘ç»Ÿè®¡æµå½¢çš„å‡ ä½•æ¥æ¯”è¾ƒå¤šé‡é›†åˆçš„æˆå¯¹å·®å¼‚ï¼Œé€šè¿‡å°†å¤šé‡é›†åˆâ€œæ˜ å°„â€åˆ°æµå½¢ä¸Šã€‚é€šè¿‡æ˜¾å¼è®¡ç®—é«˜æ–¯æ··åˆç‰©æµå½¢ä¸­çš„èŠ‚ç‚¹åµŒå…¥ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¼•å¯¼å‡ºäº†ä¸€ç§æ–°çš„å…³æ³¨æœºåˆ¶è¿›è¡Œé‚»åŸŸèšåˆã€‚æˆ‘ä»¬æä¾›äº†æœ‰å…³åµŒå…¥çš„æ³›åŒ–å’Œè¡¨è¾¾èƒ½åŠ›çš„ç†è®ºè§è§£ï¼Œä¸ºæ›´æ·±å…¥ç†è§£åŸºäºæ³¨æ„åŠ›æœºåˆ¶çš„GNNåšå‡ºäº†è´¡çŒ®ã€‚

    Attention-based graph neural networks (GNNs), such as graph attention networks (GATs), have become popular neural architectures for processing graph-structured data and learning node embeddings. Despite their empirical success, these models rely on labeled data and the theoretical properties of these models have yet to be fully understood. In this work, we propose a novel attention-based node embedding framework for graphs. Our framework builds upon a hierarchical kernel for multisets of subgraphs around nodes (e.g. neighborhoods) and each kernel leverages the geometry of a smooth statistical manifold to compare pairs of multisets, by "projecting" the multisets onto the manifold. By explicitly computing node embeddings with a manifold of Gaussian mixtures, our method leads to a new attention mechanism for neighborhood aggregation. We provide theoretical insights into genralizability and expressivity of our embeddings, contributing to a deeper understanding of attention-based GNNs. We p
    
[^30]: å—é™é€šä¿¡åŠ æ€§é«˜æ–¯å™ªå£°ä¸‹çš„å¤šè‡‚èµŒåšæœºé—®é¢˜ç ”ç©¶

    Communication-Constrained Bandits under Additive Gaussian Noise. (arXiv:2304.12680v1 [cs.LG])

    [http://arxiv.org/abs/2304.12680](http://arxiv.org/abs/2304.12680)

    æœ¬æ–‡ç ”ç©¶äº†åœ¨å—é™é€šä¿¡å’ŒåŠ æ€§é«˜æ–¯å™ªå£°ä¸‹çš„å¤šè‡‚èµŒåšæœºé—®é¢˜ï¼Œæå‡ºäº†ä¸€ä¸ªå¤šé˜¶æ®µèµŒåšç®—æ³•ï¼Œå¹¶ç»™å‡ºäº†ä¿¡æ¯ç†è®ºä¸‹é™ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†ä¸€ä¸ªåˆ†å¸ƒå¼éšæœºå¤šè‡‚èµŒåšæœº,å…¶ä¸­å®¢æˆ·ç«¯æ ¹æ®ç›¸åº”çš„æ‹‰è‡‚å¥–åŠ±æä¾›å—é™é€šä¿¡åé¦ˆç»™å­¦ä¹ è€…ã€‚åœ¨æˆ‘ä»¬çš„è®¾å®šä¸‹,å®¢æˆ·ç«¯å¿…é¡»ç¼–ç å¥–åŠ±ï¼Œä½¿å¾—ç¼–ç å¥–åŠ±çš„äºŒé˜¶çŸ©ä¸è¶…è¿‡Pï¼Œå¹¶ä¸”è¿™ä¸ªç¼–ç å¥–åŠ±ä¼šè¢«æ–¹å·®ä¸º$\sigma^2$çš„åŠ æ€§é«˜æ–¯å™ªå£°æ‰€æ±¡æŸ“ï¼›å­¦ä¹ è€…åªèƒ½è®¿é—®è¿™ä¸ªè¢«æ±¡æŸ“çš„å¥–åŠ±ã€‚æˆ‘ä»¬åœ¨è¿™ä¸ªè®¾ç½®ä¸­å¯¼å‡ºäº†ä»»ä½•æ–¹æ¡ˆçš„æœ€å°åŒ–åæ‚”çš„ä¿¡æ¯è®ºä¸‹é™$\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$ï¼Œå…¶ä¸­ $ \mathtt{SNR} := \frac{P}{\sigma^2}$ï¼Œ$K$å’Œ$T$åˆ†åˆ«æ˜¯è‡‚æ•°å’Œæ—¶é—´é•¿åº¦ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªå¤šé˜¶æ®µèµŒåšç®—æ³•$\mathtt{UE\text{-}UCB++}$ï¼Œå®ƒå¯ä»¥å°†è¿™ä¸ªä¸‹é™çš„å€¼åŠ ä¸Šä¸€ä¸ªå¾®å°çš„å¯åŠ æ€§å› å­ã€‚$\mathtt{UE\text{-}UCB++}$åœ¨å…¶åˆå§‹é˜¶æ®µæ‰§è¡Œå‡åŒ€æ¢ç´¢ï¼Œç„¶ååœ¨åç»­é˜¶æ®µä½¿ç”¨â€œä¸Šç½®ä¿¡ç•Œâ€(UCB)ç®—æ³•ã€‚æˆ‘ä»¬è¿˜å±•ç¤ºäº†æ•°å€¼ç»“æœï¼Œè¡¨æ˜åœ¨å®é™…æƒ…å†µä¸‹éœ€è¦è¿™æ ·çš„é€šä¿¡æœ‰æ•ˆç®—æ³•ã€‚

    We study a distributed stochastic multi-armed bandit where a client supplies the learner with communication-constrained feedback based on the rewards for the corresponding arm pulls. In our setup, the client must encode the rewards such that the second moment of the encoded rewards is no more than $P$, and this encoded reward is further corrupted by additive Gaussian noise of variance $\sigma^2$; the learner only has access to this corrupted reward. For this setting, we derive an information-theoretic lower bound of $\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$ on the minimax regret of any scheme, where $ \mathtt{SNR} := \frac{P}{\sigma^2}$, and $K$ and $T$ are the number of arms and time horizon, respectively. Furthermore, we propose a multi-phase bandit algorithm, $\mathtt{UE\text{-}UCB++}$, which matches this lower bound to a minor additive factor. $\mathtt{UE\text{-}UCB++}$ performs uniform exploration in its initial phases and then utilizes the {\em upper confidence
    
[^31]: å¤šå…ƒæ¦‚ç‡é¢„æµ‹è¯„ä¼°ä¸­çš„å¯é æ€§åŒºåŸŸç ”ç©¶

    Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts. (arXiv:2304.09836v1 [cs.LG])

    [http://arxiv.org/abs/2304.09836](http://arxiv.org/abs/2304.09836)

    æœ¬ç ”ç©¶é€šè¿‡æœ‰é™æ ·æœ¬å’ŒåŠŸç‡åˆ†æç¡®å®šäº†å¤šå…ƒæ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹è¯„åˆ†è§„åˆ™çš„å¯é æ€§åŒºåŸŸï¼Œå¹¶åœ¨ç”µåŠ›ç”Ÿäº§é—®é¢˜ä¸Šè¯„ä¼°äº†ç»“æœå¯¹çœŸå®ä¸–ç•Œä»»åŠ¡çš„æ™®é€‚æ€§ã€‚

    

    åœ¨å¤šå…ƒæ¦‚ç‡æ—¶é—´åºåˆ—é¢„æµ‹çš„è¯„ä¼°ä¸­ï¼Œé€šå¸¸ä½¿ç”¨é€‚å½“çš„è¯„åˆ†è§„åˆ™è¿›è¡Œè¯„ä¼°ï¼Œå³å¯¹äºåŸºå‡†åˆ†å¸ƒæœŸæœ›æœ€å°çš„å‡½æ•°ã€‚ç„¶è€Œï¼Œåœ¨éæ¸è¿›æƒ…å†µä¸‹ï¼Œè¿™ä¸€å±æ€§ä¸èƒ½ä¿è¯å…·æœ‰è‰¯å¥½çš„åŒºåˆ†åº¦ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ç¬¬ä¸€ç¯‡ç³»ç»Ÿçš„æœ‰é™æ ·æœ¬é€‚å½“è¯„åˆ†è§„åˆ™ç ”ç©¶ï¼Œé€šè¿‡åŠŸç‡åˆ†æï¼Œæˆ‘ä»¬ç¡®å®šäº†ä¸€ä¸ªåˆ†æ•°è§„åˆ™çš„â€œå¯é æ€§åŒºåŸŸâ€ï¼Œå³å®ƒå¯ä»¥å¯é åœ°è¯†åˆ«é¢„æµ‹è¯¯å·®çš„ä¸€ç»„å®é™…æ¡ä»¶ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªå…¨é¢çš„äººé€ åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œäº†åˆ†æï¼Œè¯¥æµ‹è¯•ä¸“é—¨è®¾è®¡ä»¥æµ‹è¯•åŸºå‡†åˆ†å¸ƒä¸é¢„æµ‹åˆ†å¸ƒä¹‹é—´çš„å‡ ä¸ªå…³é”®å·®å¼‚ï¼Œå¹¶é€šè¿‡åœ¨ç”µåŠ›ç”Ÿäº§é—®é¢˜ä¸Šåº”ç”¨æ¥è¯„ä¼°æˆ‘ä»¬çš„ç»“æœå¯¹çœŸå®ä¸–ç•Œä»»åŠ¡çš„æ™®é€‚æ€§ã€‚æˆ‘ä»¬çš„ç»“æœæ­ç¤ºäº†åœ¨å¤šå…ƒæ¦‚ç‡é¢„æµ‹çš„è¯„ä¼°ä¸­çš„é‡å¤§ç¼ºé™·ã€‚

    Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the "region of reliability" of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as co
    
[^32]: å¿«é€Ÿç‡çš„æœ€å¤§ç†µæ¢ç´¢æ–¹æ³•

    Fast Rates for Maximum Entropy Exploration. (arXiv:2303.08059v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2303.08059](http://arxiv.org/abs/2303.08059)

    æœ¬æ–‡è§£å†³äº†å¼ºåŒ–å­¦ä¹ ä¸­ç¨€ç–å¥–åŠ±ä¸‹çš„æœ€å¤§ç†µæ¢ç´¢é—®é¢˜ï¼Œæå‡ºäº†ä¸¤ç§ç±»å‹çš„æœ€å¤§ç†µæ¢ç´¢æ–¹æ³•ï¼Œå¹¶æé«˜äº†å…¶æ ·æœ¬å¤æ‚åº¦ã€‚

    

    å½“æ™ºèƒ½ä½“åœ¨ä¸€ä¸ªæœªçŸ¥çš„ã€ç¨€ç–æˆ–æ²¡æœ‰å¥–åŠ±çš„ç¯å¢ƒä¸­æ“ä½œæ—¶ï¼Œæˆ‘ä»¬è§£å†³äº†å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰ä¸­æ¢ç´¢çš„æŒ‘æˆ˜ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†ä¸¤ç§ä¸åŒç±»å‹çš„æœ€å¤§ç†µæ¢ç´¢é—®é¢˜ã€‚ç¬¬ä¸€ç§ç±»å‹æ˜¯å›è®¿ç†µæœ€å¤§åŒ–ï¼Œè¿™åœ¨æŠ˜æ‰£è®¾ç½®ä¸­å·²ç»ç”±Hazan et al.ï¼ˆ2019ï¼‰è€ƒè™‘è¿‡ã€‚å¯¹äºè¿™ç§ç±»å‹çš„æ¢ç´¢ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åšå¼ˆè®ºç®—æ³•ï¼Œå…¶æ ·æœ¬å¤æ‚æ€§ä¸º$\widetilde{\mathcal{O}}(H^3S^2A/\varepsilon^2)$ï¼Œä»è€Œæ”¹è¿›äº†ç°æœ‰ç»“æœçš„$\varepsilon$ä¾èµ–å…³ç³»ï¼Œå…¶ä¸­$S$æ˜¯çŠ¶æ€æ•°ï¼Œ$A$æ˜¯è¡ŒåŠ¨æ•°ï¼Œ$H$æ˜¯æ¯ä¸ªå›åˆçš„é•¿åº¦ï¼Œ$\varepsilon$æ˜¯æœŸæœ›çš„ç²¾åº¦ã€‚æˆ‘ä»¬ç ”ç©¶çš„ç¬¬äºŒç§ç†µæ˜¯è½¨è¿¹ç†µã€‚è¿™ä¸ªç›®æ ‡å‡½æ•°ä¸ç†µæ­£åˆ™åŒ–MDPså¯†åˆ‡ç›¸å…³ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªç®€å•çš„ç®—æ³•ï¼Œå…¶æ ·æœ¬å¤æ‚åº¦ä¸º$\widetilde{\mathcal{O}}(\mathrm{poly}(S,A,H)/\varepsilon)$ã€‚æœ‰è¶£çš„æ˜¯ï¼Œè¿™æ˜¯ç¬¬ä¸€ä¸ªåœ¨å…·æœ‰$\mathrm{poly}(S,A,H)$æ ·æœ¬å¤æ‚åº¦çš„æƒ…å†µä¸‹è§£å†³è½¨è¿¹ç†µæœ€å¤§åŒ–é—®é¢˜çš„ç®—æ³•ã€‚

    We address the challenge of exploration in reinforcement learning (RL) when the agent operates in an unknown environment with sparse or no rewards. In this work, we study the maximum entropy exploration problem of two different types. The first type is visitation entropy maximization previously considered by Hazan et al.(2019) in the discounted setting. For this type of exploration, we propose a game-theoretic algorithm that has $\widetilde{\mathcal{O}}(H^3S^2A/\varepsilon^2)$ sample complexity thus improving the $\varepsilon$-dependence upon existing results, where $S$ is a number of states, $A$ is a number of actions, $H$ is an episode length, and $\varepsilon$ is a desired accuracy. The second type of entropy we study is the trajectory entropy. This objective function is closely related to the entropy-regularized MDPs, and we propose a simple algorithm that has a sample complexity of order $\widetilde{\mathcal{O}}(\mathrm{poly}(S,A,H)/\varepsilon)$. Interestingly, it is the first th
    
[^33]: å®‰å…¨å‰¥ç¦»L0æ­£åˆ™åŒ–æœ€å°äºŒä¹˜é—®é¢˜

    Safe Peeling for L0-Regularized Least-Squares with supplementary material. (arXiv:2302.14471v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.14471](http://arxiv.org/abs/2302.14471)

    å¼•å…¥â€œå®‰å…¨å‰¥ç¦»â€æ–¹æ³•åŠ é€Ÿè§£å†³L0æ­£åˆ™åŒ–æœ€å°äºŒä¹˜é—®é¢˜ï¼Œé€šè¿‡æ”¶ç¼©æ¾å¼›åº¦å…è®¸æ›´æ¿€è¿›çš„å‰ªæï¼Œæ˜¾è‘—é™ä½æ±‚è§£æ—¶é—´ã€‚

    

    æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°çš„æ–¹æ³•ï¼Œç§°ä¸ºâ€œå®‰å…¨å‰¥ç¦»â€ï¼Œé€šè¿‡åˆ†æ”¯å®šç•Œç®—æ³•åŠ é€Ÿè§£å†³L0æ­£åˆ™åŒ–æœ€å°äºŒä¹˜é—®é¢˜ã€‚æˆ‘ä»¬çš„ç¨‹åºä½¿å¾—åœ¨BnBå†³ç­–æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹å¤„è€ƒè™‘åˆ°æ”¶ç¼©æ¾å¼›åº¦ï¼Œå› æ­¤å¯èƒ½å…è®¸æ›´åŠ æ¿€è¿›çš„å‰ªæã€‚æ•°å€¼æ¨¡æ‹Ÿè¡¨æ˜ï¼Œæˆ‘ä»¬æå‡ºçš„æ–¹æ³•åœ¨æ¢ç´¢èŠ‚ç‚¹æ•°é‡å’Œæ•´ä½“æ±‚è§£æ—¶é—´æ–¹é¢å…·æœ‰æ˜¾è‘—çš„ä¼˜åŠ¿ã€‚

    We introduce a new methodology dubbed ``safe peeling'' to accelerate the resolution of L0-regularized least-squares problems via a Branch-and-Bound (BnB) algorithm. Our procedure enables to tighten the convex relaxation considered at each node of the BnB decision tree and therefore potentially allows for more aggressive pruning. Numerical simulations show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.s show that our proposed methodology leads to significant gains in terms of number of nodes explored and overall solving time.
    
[^34]: å¼‚è´¨æ€§å¤„ç†æ•ˆåº”çš„å› æœç­‰ä¿æ ¡å‡†æ–¹æ³•

    Causal isotonic calibration for heterogeneous treatment effects. (arXiv:2302.14011v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.14011](http://arxiv.org/abs/2302.14011)

    æå‡ºäº†å› æœç­‰ä¿æ ¡å‡†æ–¹æ³•åŠå…¶æ•°æ®æœ‰æ•ˆçš„å˜ä½“äº¤å‰æ ¡å‡†ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½èƒ½å¿«é€Ÿç¨³å¥åœ°æ ¡å‡†å¼‚è´¨æ€§å¤„ç†æ•ˆåº”çš„é¢„æµ‹å™¨ï¼Œè€Œä¸”å¯ä»¥åº”ç”¨äºä»»ä½•é»‘ç›’å­¦ä¹ ç®—æ³•ã€‚

    

    æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„éå‚æ•°æ–¹æ³•â€”â€”å› æœç­‰ä¿æ ¡å‡†æ–¹æ³•ï¼Œç”¨äºæ ¡å‡†å¼‚è´¨æ€§å¤„ç†æ•ˆåº”çš„é¢„æµ‹å™¨ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä»‹ç»äº†äº¤å‰æ ¡å‡†ï¼Œè¿™æ˜¯ä¸€ç§æ•°æ®æœ‰æ•ˆçš„æ ¡å‡†å˜ä½“ï¼Œæ¶ˆé™¤äº†ä¿ç•™æ ¡å‡†é›†çš„éœ€è¦ã€‚äº¤å‰æ ¡å‡†åˆ©ç”¨äº¤å‰æ‹Ÿåˆçš„é¢„æµ‹å™¨ï¼Œå¹¶ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ•°æ®ç”Ÿæˆä¸€ä¸ªå•ä¸€çš„æ ¡å‡†é¢„æµ‹å™¨ã€‚åœ¨ä¸è¦æ±‚å•è°ƒæ€§çš„å¼±æ¡ä»¶ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜äº†å› æœç­‰ä¿æ ¡å‡†å’Œäº¤å‰æ ¡å‡†éƒ½èƒ½å®ç°å¿«é€ŸåŒé‡ç¨³å¥æ ¡å‡†é€Ÿç‡ï¼Œåªè¦åˆ©ç”¨ç±»ä¼¼æ„ä¹‰ä¸‹ç²¾ç¡®ä¼°è®¡äº†å€¾å‘å¾—åˆ†æˆ–åæœå›å½’ã€‚è¿™ç§å› æœç­‰ä¿æ ¡å‡†å™¨å¯ä»¥åŒ…è£…åœ¨ä»»ä½•é»‘ç›’å­¦ä¹ ç®—æ³•å‘¨å›´ï¼Œæä¾›å¼ºå¥å’Œåˆ†å¸ƒè‡ªç”±çš„æ ¡å‡†ä¿è¯ï¼ŒåŒæ—¶ä¿æŒé¢„æµ‹æ€§èƒ½ã€‚

    We propose causal isotonic calibration, a novel nonparametric method for calibrating predictors of heterogeneous treatment effects. Furthermore, we introduce cross-calibration, a data-efficient variant of calibration that eliminates the need for hold-out calibration sets. Cross-calibration leverages cross-fitted predictors and generates a single calibrated predictor using all available data. Under weak conditions that do not assume monotonicity, we establish that both causal isotonic calibration and cross-calibration achieve fast doubly-robust calibration rates, as long as either the propensity score or outcome regression is estimated accurately in a suitable sense. The proposed causal isotonic calibrator can be wrapped around any black-box learning algorithm, providing robust and distribution-free calibration guarantees while preserving predictive performance.
    
[^35]: é€šè¿‡f-æ•£åº¦æœ€å°åŒ–å¯¹é½è¯­è¨€æ¨¡å‹ä¸åå¥½

    Aligning Language Models with Preferences through f-divergence Minimization. (arXiv:2302.08215v2 [cs.CL] UPDATED)

    [http://arxiv.org/abs/2302.08215](http://arxiv.org/abs/2302.08215)

    æœ¬æ–‡æå‡ºä¸€ç§æ–°çš„æ–¹æ³•f-DPGï¼Œç”¨äºå¯¹é½è¯­è¨€æ¨¡å‹å’Œåå¥½ï¼Œè¯¥æ–¹æ³•é€‚ç”¨äºè¯„ä¼°ä»»ä½•ç›®æ ‡åˆ†å¸ƒï¼Œç»Ÿä¸€äº†ç°æœ‰çš„å„ç§æ¡†æ¶å’Œé€¼è¿‘æ–¹æ³•ã€‚

    

    å¯¹é½è¯­è¨€æ¨¡å‹å’Œåå¥½å¯ä»¥è¢«çœ‹ä½œæ˜¯å¯¹ç›®æ ‡åˆ†å¸ƒè¿›è¡Œé€¼è¿‘ï¼Œä»¥æœŸè¾¾åˆ°æŸç§æ‰€éœ€è¡Œä¸ºã€‚ç°æœ‰çš„æ–¹æ³•åœ¨ç›®æ ‡åˆ†å¸ƒçš„å‡½æ•°å½¢å¼å’Œç”¨äºé€¼è¿‘ç›®æ ‡åˆ†å¸ƒçš„ç®—æ³•ä¸Šå­˜åœ¨å·®å¼‚ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°æ–¹æ³•f-DPGï¼Œè¯¥æ–¹æ³•å…è®¸ä½¿ç”¨ä»»ä½•å¯è¯„ä¼°çš„f-æ•£åº¦é€¼è¿‘ä»»ä½•ç›®æ ‡åˆ†å¸ƒï¼Œä»è€Œç»Ÿä¸€äº†ç°æœ‰çš„å„ç§æ¡†æ¶å’Œé€¼è¿‘æ–¹æ³•ã€‚æˆ‘ä»¬å±•ç¤ºäº†å„ç§æ•£åº¦ç›®æ ‡çš„å®é™…å¥½å¤„ï¼Œå¹¶è¯æ˜äº†æ²¡æœ‰æ™®é€‚çš„æœ€ä½³é€‰æ‹©ã€‚

    Aligning language models with preferences can be posed as approximating a target distribution representing some desired behavior. Existing approaches differ both in the functional form of the target distribution and the algorithm used to approximate it. For instance, Reinforcement Learning from Human Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target distribution arising from a KL penalty in the objective. On the other hand, Generative Distributional Control (GDC) has an explicit target distribution and minimizes a forward KL from it using the Distributional Policy Gradient (DPG) algorithm. In this paper, we propose a new approach, f-DPG, which allows the use of any f-divergence to approximate any target distribution that can be evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation methods (DPG, RL with KL penalties). We show the practical benefits of various choices of divergence objectives and demonstrate that there is no universally o
    
[^36]: ä¸æ˜¯ç¥å¥‡è¯ä¸¸ï¼Œè€Œæ˜¯æ´å¯ŸåŠ›ä¹‹æœå¯»ï¼šæ¶ˆé™¤å¼‚è´¨æ€§å¤„ç†æ•ˆåº”ä¼°è®¡ä¸­çš„æ¨¡å‹é€‰æ‹©å›°å¢ƒ

    In Search of Insights, Not Magic Bullets: Towards Demystification of the Model Selection Dilemma in Heterogeneous Treatment Effect Estimation. (arXiv:2302.02923v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.02923](http://arxiv.org/abs/2302.02923)

    æœ¬æ–‡ç ”ç©¶åœ¨å…·æœ‰é«˜é£é™©åº”ç”¨çš„ä¸ªæ€§åŒ–å¤„ç†æ•ˆåº”ä¼°è®¡ä¸­ï¼Œä¸åŒæ¨¡å‹é€‰æ‹©æ ‡å‡†çš„ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼Œå¹¶æå‡ºæœªæ¥ç ”ç©¶æ–¹å‘ã€‚

    

    ä¸ªæ€§åŒ–å¤„ç†æ•ˆåº”ä¼°è®¡åœ¨é«˜é£é™©åº”ç”¨ä¸­ç»å¸¸å¤‡å—å…³æ³¨ï¼Œå› æ­¤ï¼Œåœ¨å®è·µä¸­éƒ¨ç½²ä¼°è®¡è¿™ç§æ•ˆåº”çš„æ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦ç¡®ä¿¡å·²ç»é€‰æ‹©äº†æœ€å¥½çš„æœºå™¨å­¦ä¹ å·¥å…·ç®±ä¸­çš„å€™é€‰æ¨¡å‹ã€‚ä¸å¹¸çš„æ˜¯ï¼Œç”±äºå®è·µä¸­ç¼ºä¹åäº‹å®ä¿¡æ¯ï¼Œé€šå¸¸æ— æ³•ä¾é æ ‡å‡†éªŒè¯æŒ‡æ ‡å®Œæˆæ­¤ä»»åŠ¡ï¼Œå¯¼è‡´äº†å¤„ç†æ•ˆåº”ä¼°è®¡æ–‡çŒ®ä¸­å·²çŸ¥çš„æ¨¡å‹é€‰æ‹©å›°å¢ƒã€‚è™½ç„¶æœ€è¿‘å·²ç»ç ”ç©¶äº†ä¸€äº›è§£å†³æ–¹æ¡ˆï¼Œä½†å¯¹ä¸åŒæ¨¡å‹é€‰æ‹©æ ‡å‡†çš„ä¼˜ç¼ºç‚¹çš„ç³»ç»Ÿç†è§£ä»ç„¶ç¼ºä¹ã€‚å› æ­¤ï¼Œåœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è¯•å›¾å®£å¸ƒå…¨å±€â€œèƒœè€…â€ï¼Œè€Œæ˜¯å¯¹ä¸åŒé€‰æ‹©æ ‡å‡†çš„æˆåŠŸå’Œå¤±è´¥æ¨¡å¼è¿›è¡Œäº†å®è¯ç ”ç©¶ã€‚æˆ‘ä»¬å¼ºè°ƒé€‰æ‹©ç­–ç•¥ï¼Œå€™é€‰ä¼°è®¡é‡å’Œç”¨äºæ¯”è¾ƒå®ƒä»¬çš„æ•°æ®ä¹‹é—´å­˜åœ¨å¤æ‚çš„ç›¸äº’ä½œç”¨ï¼Œå¹¶æå‡ºäº†æœªæ¥ç ”ç©¶çš„æ–¹å‘ã€‚

    Personalized treatment effect estimates are often of interest in high-stakes applications -- thus, before deploying a model estimating such effects in practice, one needs to be sure that the best candidate from the ever-growing machine learning toolbox for this task was chosen. Unfortunately, due to the absence of counterfactual information in practice, it is usually not possible to rely on standard validation metrics for doing so, leading to a well-known model selection dilemma in the treatment effect estimation literature. While some solutions have recently been investigated, systematic understanding of the strengths and weaknesses of different model selection criteria is still lacking. In this paper, instead of attempting to declare a global `winner', we therefore empirically investigate success- and failure modes of different selection criteria. We highlight that there is a complex interplay between selection strategies, candidate estimators and the data used for comparing them, an
    
[^37]: é¢„æ¡ä»¶å¯¹è¶…å‚åŒ–ä½ç§©çŸ©é˜µæ„ŸçŸ¥çš„å½±å“

    The Power of Preconditioning in Overparameterized Low-Rank Matrix Sensing. (arXiv:2302.01186v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.01186](http://arxiv.org/abs/2302.01186)

    è¯¥ç ”ç©¶æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•ï¼Œç›¸è¾ƒäºä¼ ç»Ÿæ¢¯åº¦ä¸‹é™æ³•æ›´åŠ é²æ£’ï¼Œå¹¶ä¸”åœ¨å¤„ç†ä½ç§©çŸ©é˜µæ„ŸçŸ¥é—®é¢˜æ—¶å…·æœ‰å¾ˆå¥½çš„è¡¨ç°ã€‚

    

    æœ¬æ–‡æå‡ºäº†ScaledGD(ğœ†)æ–¹æ³•æ¥è§£å†³ä½ç§©çŸ©é˜µæ„ŸçŸ¥ä¸­çŸ©é˜µå¯èƒ½ç—…æ€ä»¥åŠçœŸå®ç§©æœªçŸ¥çš„é—®é¢˜ã€‚è¯¥æ–¹æ³•ä½¿ç”¨è¶…å‚å¼è¡¨ç¤ºï¼Œä»ä¸€ä¸ªå°çš„éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œé€šè¿‡ä½¿ç”¨ç‰¹å®šå½¢å¼çš„é˜»å°¼é¢„æ¡ä»¶æ¢¯åº¦ä¸‹é™æ¥å¯¹æŠ—è¶…å‚åŒ–å’Œç—…æ€æ›²ç‡çš„å½±å“ã€‚ä¸åŸºå‡†æ¢¯åº¦ä¸‹é™ï¼ˆGDï¼‰ç›¸æ¯”ï¼Œå°½ç®¡é¢„å¤„ç†éœ€è¦è½»å¾®çš„è®¡ç®—å¼€é”€ï¼Œä½†ScaledGDï¼ˆğœ†ï¼‰åœ¨é¢å¯¹ç—…æ€é—®é¢˜æ—¶è¡¨ç°å‡ºäº†å‡ºè‰²çš„é²æ£’æ€§ã€‚åœ¨é«˜æ–¯è®¾è®¡ä¸‹ï¼ŒScaledGD($\lambda$) ä¼šåœ¨ä»…è¿­ä»£æ•°å¯¹æ•°çº§åˆ«çš„æƒ…å†µä¸‹ï¼Œä»¥çº¿æ€§é€Ÿç‡æ”¶æ•›åˆ°çœŸå®çš„ä½ç§©çŸ©é˜µã€‚

    We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent method to tackle the low-rank matrix sensing problem when the true rank is unknown, and when the matrix is possibly ill-conditioned. Using overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts from a small random initialization, and proceeds by gradient descent with a specific form of damped preconditioning to combat bad curvatures induced by overparameterization and ill-conditioning. At the expense of light computational overhead incurred by preconditioners, $\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning compared to vanilla gradient descent ($\textsf{GD}$) even with overprameterization. Specifically, we show that, under the Gaussian design, $\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a constant linear rate after a small number of iterations that scales only logarithmically with respect to the condition number and the problem dimensi
    
[^38]: é‡æ–°å®¡è§† Bellman Errors ç”¨äºç¦»çº¿æ¨¡å‹é€‰æ‹©

    Revisiting Bellman Errors for Offline Model Selection. (arXiv:2302.00141v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.00141](http://arxiv.org/abs/2302.00141)

    æœ¬æ–‡é‡æ–°å®¡è§† Bellman Errorsï¼Œå‘ç°ä¹‹å‰çš„Bellman Errors æ–¹æ³•éœ€è¦åœ¨ç‰¹å®šæ¡ä»¶ä¸‹æ‰èƒ½è¡¨ç°è‰¯å¥½ï¼ŒåŒæ—¶æå‡ºäº†æ›´å‡†ç¡®çš„ MSBE ä¼°è®¡å™¨ï¼Œåœ¨ç¦»æ•£æ§åˆ¶ä»»åŠ¡æ–¹é¢è¡¨ç°å‡ºè‰²ã€‚

    

    ç¦»çº¿æ¨¡å‹é€‰æ‹©ï¼ˆOMSï¼‰å³åœ¨åªæœ‰å·²è®°å½•æ•°æ®çš„æƒ…å†µä¸‹ä»ä¼—å¤šç­–ç•¥ä¸­é€‰æ‹©æœ€ä½³ç­–ç•¥ï¼Œå¯¹äºåœ¨å®é™…ç¯å¢ƒä¸‹åº”ç”¨ç¦»çº¿RLè‡³å…³é‡è¦ã€‚ä¸€ä¸ªç»è¿‡å¹¿æ³›æ¢è®¨çš„æƒ³æ³•æ˜¯æ ¹æ®ç›¸å…³Qå‡½æ•°çš„å‡æ–¹Bellmanè¯¯å·®ï¼ˆMSBEï¼‰é€‰æ‹©ç­–ç•¥ã€‚ç„¶è€Œï¼Œä¹‹å‰çš„ç ”ç©¶ä¸€ç›´åœ¨ä½¿ç”¨Bellmanè¯¯å·®æ—¶æ— æ³•è·å¾—è¶³å¤Ÿçš„OMSæ€§èƒ½ï¼Œå¯¼è‡´è®¸å¤šç ”ç©¶äººå‘˜æ”¾å¼ƒæ­¤æƒ³æ³•ã€‚ä¸ºæ­¤ï¼Œæœ¬æ–‡é˜è¿°äº†ä¸ºä»€ä¹ˆä¹‹å‰çš„ç»“æœä½¿ç”¨Bellmanè¯¯å·®æ—¶ä¼šçœ‹åˆ°æ‚²è§‚çš„ç»“æœï¼Œå¹¶ç¡®å®šäº†åŸºäºBellmanè¯¯å·®çš„OMSç®—æ³•å°†è¡¨ç°è‰¯å¥½çš„æ¡ä»¶ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ä¸ªæ¯”ä¹‹å‰æ–¹æ³•æ›´å‡†ç¡®çš„MSBEçš„æ–°çš„ä¼°è®¡å™¨ã€‚æˆ‘ä»¬çš„ä¼°è®¡å™¨åœ¨ä¸åŒçš„ç¦»æ•£æ§åˆ¶ä»»åŠ¡ï¼ˆåŒ…æ‹¬ Atari æ¸¸æˆï¼‰ä¸Šè·å¾—äº†å‡ºè‰²çš„OMSæ€§èƒ½ã€‚

    Offline model selection (OMS), that is, choosing the best policy from a set of many policies given only logged data, is crucial for applying offline RL in real-world settings. One idea that has been extensively explored is to select policies based on the mean squared Bellman error (MSBE) of the associated Q-functions. However, previous work has struggled to obtain adequate OMS performance with Bellman errors, leading many researchers to abandon the idea. To this end, we elucidate why previous work has seen pessimistic results with Bellman errors and identify conditions under which OMS algorithms based on Bellman errors will perform well. Moreover, we develop a new estimator of the MSBE that is more accurate than prior methods. Our estimator obtains impressive OMS performance on diverse discrete control tasks, including Atari games.
    
[^39]: å…³äºå…·æœ‰æœºå™¨å¯è¡¨ç¤ºå‚æ•°çš„ç¥ç»ç½‘ç»œè‡ªåŠ¨å¾®åˆ†æ­£ç¡®æ€§çš„ç ”ç©¶

    On the Correctness of Automatic Differentiation for Neural Networks with Machine-Representable Parameters. (arXiv:2301.13370v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.13370](http://arxiv.org/abs/2301.13370)

    æœ¬è®ºæ–‡ç ”ç©¶äº†ç¥ç»ç½‘ç»œå‚æ•°ä¸ºæœºå™¨å¯è¡¨ç¤ºæ•°å­—æ—¶è‡ªåŠ¨å¾®åˆ†çš„æ­£ç¡®æ€§é—®é¢˜ï¼Œè¯æ˜äº†ç¥ç»ç½‘ç»œå¸¦åç½®å‚æ•°æ—¶è‡ªåŠ¨å¾®åˆ†å§‹ç»ˆæ­£ç¡®ï¼Œç»™å‡ºäº†é™åˆ¶ä¸å¯å¾®æ€§åœ¨æ¿€æ´»å‡½æ•°ä¸­æ•°ç›®çš„ç•Œï¼Œå¹¶æä¾›äº†åˆ¤æ–­å‚æ•°æ˜¯å¦åœ¨ä¸å¯å¾®å‚æ•°ç»„ä¸­çš„æ¡ä»¶ã€‚

    

    æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå®æ•°åŸŸä¸Šçš„å‰å‘å’Œåå‘æ¨¡å¼è‡ªåŠ¨å¾®åˆ†å‡ ä¹å§‹ç»ˆåœ¨æ•°å­¦ä¸Šæ˜¯å‡†ç¡®çš„ã€‚ç„¶è€Œï¼Œå®é™…ç¼–ç¨‹ä½¿ç”¨çš„æ˜¯æœºå™¨å¯è¡¨ç¤ºçš„æ•°å­—ï¼ˆä¾‹å¦‚æµ®ç‚¹æ•°ï¼‰ï¼Œè€Œä¸æ˜¯å®æ•°ã€‚æœ¬æ–‡ç ”ç©¶äº†å½“ç¥ç»ç½‘ç»œçš„å‚æ•°ç©ºé—´ä»…ç”±æœºå™¨å¯è¡¨ç¤ºçš„æ•°å­—ç»„æˆæ—¶ï¼Œè‡ªåŠ¨å¾®åˆ†çš„æ­£ç¡®æ€§ã€‚æˆ‘ä»¬åˆ†æäº†ä¸¤ç»„å¯èƒ½å¯¼è‡´è‡ªåŠ¨å¾®åˆ†ä¸æ­£ç¡®çš„å‚æ•°ï¼šä¸€ç»„æ˜¯ç½‘ç»œå¯å¾®ä½†è‡ªåŠ¨å¾®åˆ†æ— æ³•è®¡ç®—å…¶å¯¼æ•°çš„å‚æ•°ç»„ï¼Œå¦ä¸€ç»„æ˜¯ç½‘ç»œä¸å¯å¾®çš„å‚æ•°ç»„ã€‚å¯¹äºå¸¦æœ‰åç½®å‚æ•°çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬é¦–å…ˆè¯æ˜äº†ç¬¬ä¸€ç»„å‚æ•°ç»„å§‹ç»ˆä¸ºç©ºã€‚ç„¶åæˆ‘ä»¬ç»™å‡ºäº†ä¸€ä¸ªçº¿æ€§ä¸Šé™æ¥é™åˆ¶ç¬¬äºŒç»„å‚æ•°ç»„ä¸­ä¸å¯å¾®æ€§åœ¨æ¿€æ´»å‡½æ•°ä¸­çš„æ•°ç›®ï¼Œå¹¶ç»™å‡ºäº†ä¸€ä¸ªç®€å•çš„å¿…è¦å’Œå……åˆ†æ¡ä»¶æ¥åˆ¤æ–­ä¸€ä¸ªå‚æ•°æ˜¯å¦åœ¨è¿™ä¸ªå‚æ•°ç»„ä¸­ã€‚

    Recent work has shown that forward- and reverse- mode automatic differentiation (AD) over the reals is almost always correct in a mathematically precise sense. However, actual programs work with machine-representable numbers (e.g., floating-point numbers), not reals. In this paper, we study the correctness of AD when the parameter space of a neural network consists solely of machine-representable numbers. In particular, we analyze two sets of parameters on which AD can be incorrect: the incorrect set on which the network is differentiable but AD does not compute its derivative, and the non-differentiable set on which the network is non-differentiable. For a neural network with bias parameters, we first prove that the incorrect set is always empty. We then prove a tight bound on the size of the non-differentiable set, which is linear in the number of non-differentiabilities in activation functions, and give a simple necessary and sufficient condition for a parameter to be in this set. W
    
[^40]: ç”¨äºé‡‡æ ·åˆ†å­æ™¶ä½“ç»“æ„çš„åˆšä½“æµ

    Rigid body flows for sampling molecular crystal structures. (arXiv:2301.11355v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.11355](http://arxiv.org/abs/2301.11355)

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ­£åˆ™åŒ–æµï¼Œä¸“ä¸ºä¸‰ç»´ç©ºé—´ä¸­å¤šä¸ªç‰©ä½“çš„ä½ç½®å’Œæ–¹å‘å»ºæ¨¡è€Œè®¾è®¡ã€‚é€šè¿‡åœ¨å•ä½å››å…ƒæ•°ç¾¤ä¸Šå®šä¹‰å¹³æ»‘å’Œè¡¨ç°åŠ›å¼ºçš„æµä»¥åŠå®šä¹‰é€‚å½“çš„å¯†åº¦ï¼Œåœ¨æ—‹è½¬ç¾¤ä¸Šè¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬å¯ä»¥æˆåŠŸåœ°é‡‡æ ·åˆ†å­æ™¶ä½“ç»“æ„ã€‚

    

    æ­£åˆ™åŒ–æµ(NF)æ˜¯ä¸€ç±»å¼ºå¤§çš„ç”Ÿæˆæ¨¡å‹ï¼Œç”±äºå…¶é«˜åº¦çµæ´»å’Œè¡¨ç°åŠ›ï¼Œè¿‘å¹´æ¥å¹¿å—æ¬¢è¿ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°å‹çš„æ­£åˆ™åŒ–æµï¼Œä¸“ä¸ºä¸‰ç»´ç©ºé—´ä¸­å¤šä¸ªç‰©ä½“çš„ä½ç½®å’Œæ–¹å‘å»ºæ¨¡è€Œè®¾è®¡ï¼Œä¾‹å¦‚æ™¶ä½“ä¸­çš„åˆ†å­ã€‚æˆ‘ä»¬çš„æ–¹æ³•åŸºäºä¸¤ä¸ªå…³é”®æ€æƒ³:é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨å•ä½å››å…ƒæ•°ç¾¤ä¸Šå®šä¹‰å¹³æ»‘å’Œè¡¨ç°åŠ›å¼ºçš„æµï¼Œä»è€Œå¯ä»¥æ•æ‰åˆšä½“çš„è¿ç»­æ—‹è½¬è¿åŠ¨;å…¶æ¬¡ï¼Œæˆ‘ä»¬åˆ©ç”¨å•ä½å››å…ƒæ•°çš„åŒè¦†ç›–ç‰¹æ€§ï¼Œåœ¨æ—‹è½¬ç¾¤ä¸Šå®šä¹‰ä¸€ä¸ªé€‚å½“çš„å¯†åº¦ã€‚è¿™ç¡®ä¿æˆ‘ä»¬çš„æ¨¡å‹å¯ä»¥ä½¿ç”¨æ ‡å‡†çš„åŸºäºä¼¼ç„¶æ–¹æ³•æˆ–åŸºäºçƒ­åŠ›å­¦ç›®æ ‡å¯†åº¦çš„å˜åˆ†æ¨æ–­è¿›è¡Œè®­ç»ƒã€‚æˆ‘ä»¬é€šè¿‡è®­ç»ƒä¸¤ä¸ªåˆ†å­ç¤ºä¾‹çš„Boltzmannç”Ÿæˆå™¨æ¥è¯„ä¼°è¯¥æ–¹æ³•ï¼Œå³å››é¢ä½“ç³»ç»Ÿçš„å¤šæ¨¡æ€å¯†åº¦ã€‚

    Normalizing flows (NF) are a class of powerful generative models that have gained popularity in recent years due to their ability to model complex distributions with high flexibility and expressiveness. In this work, we introduce a new type of normalizing flow that is tailored for modeling positions and orientations of multiple objects in three-dimensional space, such as molecules in a crystal. Our approach is based on two key ideas: first, we define smooth and expressive flows on the group of unit quaternions, which allows us to capture the continuous rotational motion of rigid bodies; second, we use the double cover property of unit quaternions to define a proper density on the rotation group. This ensures that our model can be trained using standard likelihood-based methods or variational inference with respect to a thermodynamic target density. We evaluate the method by training Boltzmann generators for two molecular examples, namely the multi-modal density of a tetrahedral system 
    
[^41]: å±‚æ¬¡å¹³è¡¡ï¼šå®ç°åŸºäºæ½œåœ¨å› æœå› ç´ çš„åŠ¨æ€å…¬å¹³æ€§

    Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors. (arXiv:2301.08987v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2301.08987](http://arxiv.org/abs/2301.08987)

    æœ¬ç ”ç©¶æå‡ºäº†å±‚æ¬¡å¹³è¡¡çš„æ¦‚å¿µï¼Œè¯¥æ¦‚å¿µæ•æ‰äº†æœªè§‚å¯Ÿåˆ°çš„æ½œåœ¨å› æœå› ç´ çš„æƒ…å†µå˜åŒ–ï¼Œå¹¶æ¢è®¨äº†åŠ¨æ€å…¬å¹³æ€§çš„å®ç°ã€‚åœ¨æŒ‡å®šçš„åŠ¨æ€ä¸‹ï¼Œé€šå¸¸ä¸èƒ½é€šè¿‡ä¸€æ­¥å¹²é¢„æ¥å®ç°é•¿æœŸå…¬å¹³ç›®æ ‡ã€‚

    

    å®ç°é•¿æœŸå…¬å¹³æ€§éœ€è¦å†³ç­–å’Œåº•å±‚æ•°æ®ç”Ÿæˆè¿‡ç¨‹ä¹‹é—´çš„ç›¸äº’ä½œç”¨ã€‚æœ¬æ–‡é€šè¿‡åœ¨å†³ç­–å’Œåˆ†å¸ƒäº¤äº’ä½œç”¨ä¸Šè¿›è¡Œå› æœå»ºæ¨¡ï¼Œå¹¶æå‡ºå±‚æ¬¡å¹³è¡¡çš„æ¦‚å¿µï¼Œä»åŠ¨æ€çš„è§’åº¦æ¢è®¨å®ç°é•¿æœŸå…¬å¹³æ€§çš„å¯èƒ½æ€§ã€‚ä¸ä¹‹å‰ä»…åœ¨è§‚å¯Ÿå˜é‡ä¸Šå®šä¹‰å…¬å¹³æ¦‚å¿µçš„æ–¹æ³•ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•åœ¨è¿›ä¸€æ­¥æ•æ‰æœªè§‚å¯Ÿåˆ°çš„æ½œåœ¨å› æœå› ç´ æ–¹é¢æ›´ä¸ºç²¾ç¡®ã€‚åœ¨æŒ‡å®šçš„åŠ¨æ€ä¸‹ï¼Œæˆ‘ä»¬è¯æ˜é€šå¸¸ä¸èƒ½é€šè¿‡ä¸€æ­¥å¹²é¢„æ¥å®ç°é•¿æœŸå…¬å¹³ç›®æ ‡ã€‚æ­¤å¤–ï¼Œåœ¨åŠªåŠ›å®ç°é•¿æœŸå…¬å¹³æ€§çš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘è°ƒèŠ‚å…¬å¹³åº¦å’Œæ€§èƒ½åº¦ä¹‹é—´çš„æƒè¡¡ã€‚

    The pursuit of long-term fairness involves the interplay between decision-making and the underlying data generating process. In this paper, through causal modeling with a directed acyclic graph (DAG) on the decision-distribution interplay, we investigate the possibility of achieving long-term fairness from a dynamic perspective. We propose Tier Balancing, a technically more challenging but more natural notion to achieve in the context of long-term, dynamic fairness analysis. Different from previous fairness notions that are defined purely on observed variables, our notion goes one step further, capturing behind-the-scenes situation changes on the unobserved latent causal factors that directly carry out the influence from the current decision to the future data distribution. Under the specified dynamics, we prove that in general one cannot achieve the long-term fairness goal only through one-step interventions. Furthermore, in the effort of approaching long-term fairness, we consider th
    
[^42]: æˆ‘ä¸æƒ³è¯´ï¼šåœ¨å¯é€‰ä¸ªäººæ•°æ®æ¨¡å‹ä¸­ä¿æŠ¤ç”¨æˆ·åŒæ„

    I Prefer not to Say: Protecting User Consent in Models with Optional Personal Data. (arXiv:2210.13954v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.13954](http://arxiv.org/abs/2210.13954)

    è¯¥è®ºæ–‡ç ”ç©¶äº†ä¸ªäººå¯ä»¥é€‰æ‹©ä¸å†³ç­–ç³»ç»Ÿå…±äº«å¯é€‰ä¸ªäººä¿¡æ¯çš„æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¿æŠ¤ç”¨æˆ·åŒæ„çš„PUCæ¦‚å¿µï¼Œä¸ºç”¨æˆ·éšç§ä¿æŠ¤æä¾›äº†æœ‰åŠ›çš„è§£å†³æ–¹æ¡ˆã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†ä¸€ç§æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå…¶ä¸­ä¸ªäººå¯ä»¥é€‰æ‹©ä¸å†³ç­–ç³»ç»Ÿå…±äº«å¯é€‰ä¸ªäººä¿¡æ¯ï¼Œè¿™åœ¨ç°ä»£ä¿é™©å®šä»·æ¨¡å‹ä¸­å¾ˆå¸¸è§ã€‚ä¸€äº›ç”¨æˆ·åŒæ„ä½¿ç”¨ä»–ä»¬çš„æ•°æ®ï¼Œè€Œå…¶ä»–äººåˆ™åå¯¹å¹¶ä¿æŒå…¶æ•°æ®æœªå…¬å¼€ã€‚æœ¬æ–‡è¡¨æ˜ï¼Œä¸å…±äº«æ•°æ®çš„å†³å®šæœ¬èº«å¯ä»¥è¢«è§†ä¸ºä¿¡æ¯ï¼Œåº”è¯¥å—åˆ°ä¿æŠ¤ï¼Œä»¥å°Šé‡ç”¨æˆ·çš„éšç§ã€‚è¿™ä¸€è§‚å¯Ÿç»“æœå¼•å‘äº†ä¸€ä¸ªè¢«å¿½è§†çš„é—®é¢˜ï¼Œå³å¦‚ä½•ç¡®ä¿ä¿æŠ¤å…¶ä¸ªäººæ•°æ®çš„ç”¨æˆ·ä¸ä¼šå› æ­¤å—åˆ°ä»»ä½•ä¸åˆ©å½±å“ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å¯¹ä»…ä½¿ç”¨è·å¾—ç§¯æç”¨æˆ·åŒæ„çš„ä¿¡æ¯çš„æ¨¡å‹è¿›è¡Œäº†ä¿æŠ¤è¦æ±‚çš„æ­£å¼åŒ–ã€‚è¿™æ’é™¤äº†ä½œå‡ºå…±äº«æ•°æ®ä¸å¦å†³å®šæ‰€åŒ…å«çš„éšå«ä¿¡æ¯ã€‚æˆ‘ä»¬æå‡ºäº†Protected User Consent (PUC)æ¦‚å¿µï¼Œè¿™æ˜¯æˆ‘ä»¬è¯æ˜åœ¨ä¿æŠ¤è¦æ±‚ä¸‹æŸå¤±æœ€å°çš„è§£å†³æ–¹æ¡ˆã€‚

    We examine machine learning models in a setup where individuals have the choice to share optional personal information with a decision-making system, as seen in modern insurance pricing models. Some users consent to their data being used whereas others object and keep their data undisclosed. In this work, we show that the decision not to share data can be considered as information in itself that should be protected to respect users' privacy. This observation raises the overlooked problem of how to ensure that users who protect their personal data do not suffer any disadvantages as a result. To address this problem, we formalize protection requirements for models which only use the information for which active user consent was obtained. This excludes implicit information contained in the decision to share data or not. We offer the first solution to this problem by proposing the notion of Protected User Consent (PUC), which we prove to be loss-optimal under our protection requirement. To
    
[^43]: ä¸ºä»€ä¹ˆæ¨¡å‹ä¼šå¤±è´¥ï¼Ÿå°†æ¨¡å‹æ€§èƒ½å˜åŒ–å½’å› äºåˆ†å¸ƒåç§»

    "Why did the Model Fail?": Attributing Model Performance Changes to Distribution Shifts. (arXiv:2210.10769v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.10769](http://arxiv.org/abs/2210.10769)

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å°†æ¨¡å‹æ€§èƒ½å˜åŒ–å½’å› äºåº•å±‚æ•°æ®ç”Ÿæˆæœºåˆ¶çš„åˆ†å¸ƒåç§»çš„æ–¹æ³•ï¼Œå¹¶é€šè¿‡æ¨å¯¼ä¸€ç§é‡è¦æ€§æƒé‡æ–¹æ³•æ¥è®¡ç®—ä»»æ„ä¸€ç»„åˆ†å¸ƒçš„ä»·å€¼ã€‚

    

    æœºå™¨å­¦ä¹ æ¨¡å‹åœ¨åˆ†å¸ƒåç§»ä¸‹ç»å¸¸ä¼šå‡ºç°æ€§èƒ½ä¸‹é™çš„æƒ…å†µã€‚è¿™ç§åç§»çš„æ ¹æœ¬åŸå› å¯èƒ½æ˜¯å¤šé‡çš„å› ç´ ï¼Œæ¯”å¦‚æ•°æ®è´¨é‡çš„å˜åŒ–ã€ç‰¹å®šåå˜é‡åˆ†å¸ƒçš„å·®å¼‚æˆ–è€…æ ‡ç­¾ä¸ç‰¹å¾ä¹‹é—´çš„å…³ç³»å˜åŒ–ç­‰ã€‚å½“æ¨¡å‹åœ¨éƒ¨ç½²æ—¶å¤±è´¥æ—¶ï¼Œå°†æ€§èƒ½å˜åŒ–å½’å› äºè¿™äº›å› ç´ å¯¹äºæ¨¡å‹å¼€å‘äººå‘˜æ¥è¯´è‡³å…³é‡è¦ï¼Œä»¥è¯†åˆ«æ ¹æœ¬åŸå› å¹¶é‡‡å–ç¼“è§£æªæ–½ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†å°†ç¯å¢ƒä¹‹é—´çš„æ€§èƒ½å·®å¼‚å½’å› äºåº•å±‚æ•°æ®ç”Ÿæˆæœºåˆ¶çš„åˆ†å¸ƒåç§»é—®é¢˜ã€‚æˆ‘ä»¬å°†è¯¥é—®é¢˜æ„é€ ä¸ºä¸€ç§åˆä½œåšå¼ˆçš„å½¢å¼ï¼Œå…¶ä¸­ç©å®¶æ˜¯åˆ†å¸ƒã€‚æˆ‘ä»¬å®šä¹‰ä¸€ç»„åˆ†å¸ƒçš„ä»·å€¼ä¸ºå½“åªæœ‰è¿™ç»„åˆ†å¸ƒåœ¨ç¯å¢ƒä¹‹é—´å‘ç”Ÿå˜åŒ–æ—¶æ¨¡å‹æ€§èƒ½çš„å˜åŒ–ï¼Œå¹¶æ¨å¯¼å‡ºä¸€ç§é‡è¦æ€§æƒé‡æ–¹æ³•ä»¥è®¡ç®—ä»»æ„ä¸€ç»„åˆ†å¸ƒçš„ä»·å€¼ã€‚

    Machine learning models frequently experience performance drops under distribution shifts. The underlying cause of such shifts may be multiple simultaneous factors such as changes in data quality, differences in specific covariate distributions, or changes in the relationship between label and features. When a model does fail during deployment, attributing performance change to these factors is critical for the model developer to identify the root cause and take mitigating actions. In this work, we introduce the problem of attributing performance differences between environments to distribution shifts in the underlying data generating mechanisms. We formulate the problem as a cooperative game where the players are distributions. We define the value of a set of distributions to be the change in model performance when only this set of distributions has changed between environments, and derive an importance weighting method for computing the value of an arbitrary set of distributions. The
    
[^44]: é€šè¿‡å†—ä½™æ€§å®ç°ç¨€ç–æ€§ï¼šç”¨SGDæ±‚è§£$L_1$

    Sparsity by Redundancy: Solving $L_1$ with SGD. (arXiv:2210.01212v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01212](http://arxiv.org/abs/2210.01212)

    è¯¥è®ºæ–‡æå‡ºäº†ä¸€ç§é€šè¿‡å†—ä½™é‡å‚æ•°åŒ–å’Œç®€å•çš„éšæœºæ¢¯åº¦ä¸‹é™æ¥æœ€å°åŒ–å¸¦æœ‰$L_1$æƒ©ç½šçš„é€šç”¨å¯å¾®æŸå¤±å‡½æ•°çš„æ–¹æ³•ï¼Œç§°ä¸º\textit{spred}ï¼Œæ˜¯$L_1$çš„ç²¾ç¡®æ±‚è§£å™¨ï¼Œå¯ç”¨äºè®­ç»ƒç¨€ç–ç¥ç»ç½‘ç»œä»¥æ‰§è¡ŒåŸºå› é€‰æ‹©ä»»åŠ¡å’Œç¥ç»ç½‘ç»œå‹ç¼©ä»»åŠ¡ï¼Œå¼¥åˆäº†æ·±åº¦å­¦ä¹ ä¸­çš„ç¨€ç–æ€§å’Œä¼ ç»Ÿç»Ÿè®¡å­¦ä¹ ä¹‹é—´çš„å·®è·ã€‚

    This paper proposes a method called "spred" to minimize a generic differentiable loss function with $L_1$ penalty using redundant reparametrization and straightforward stochastic gradient descent. It is an exact solver of $L_1$ and can be used to train sparse neural networks for gene selection tasks and neural network compression tasks, bridging the gap between sparsity in deep learning and conventional statistical learning.

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§é€šè¿‡å†—ä½™é‡å‚æ•°åŒ–å’Œç®€å•çš„éšæœºæ¢¯åº¦ä¸‹é™æ¥æœ€å°åŒ–å¸¦æœ‰$L_1$æƒ©ç½šçš„é€šç”¨å¯å¾®æŸå¤±å‡½æ•°çš„æ–¹æ³•ã€‚æˆ‘ä»¬çš„æè®®æ˜¯$L_1$æƒ©ç½šç­‰ä»·äºå¸¦æœ‰æƒé‡è¡°å‡çš„å¯å¾®é‡å‚æ•°åŒ–çš„ç›´æ¥æ¨å¹¿ã€‚æˆ‘ä»¬è¯æ˜äº†æ‰€æå‡ºçš„æ–¹æ³•ï¼Œå³\textit{spred}ï¼Œæ˜¯$L_1$çš„ç²¾ç¡®æ±‚è§£å™¨ï¼Œå¹¶ä¸”å¯¹äºé€šç”¨çš„éå‡¸å‡½æ•°ï¼Œé‡å‚æ•°åŒ–æŠ€å·§æ˜¯å®Œå…¨â€œè‰¯æ€§â€çš„ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥æ–¹æ³•çš„å®ç”¨æ€§ï¼ŒåŒ…æ‹¬(1)è®­ç»ƒç¨€ç–ç¥ç»ç½‘ç»œä»¥æ‰§è¡ŒåŸºå› é€‰æ‹©ä»»åŠ¡ï¼Œå…¶ä¸­æ¶‰åŠåœ¨éå¸¸é«˜ç»´ç©ºé—´ä¸­æ‰¾åˆ°ç›¸å…³ç‰¹å¾ï¼Œä»¥åŠ(2)ç¥ç»ç½‘ç»œå‹ç¼©ä»»åŠ¡ï¼Œå…ˆå‰å°è¯•åº”ç”¨$L_1$æƒ©ç½šçš„æ–¹æ³•å‡æœªæˆåŠŸã€‚ä»æ¦‚å¿µä¸Šè®²ï¼Œæˆ‘ä»¬çš„ç»“æœå¼¥åˆäº†æ·±åº¦å­¦ä¹ ä¸­çš„ç¨€ç–æ€§å’Œä¼ ç»Ÿç»Ÿè®¡å­¦ä¹ ä¹‹é—´çš„å·®è·ã€‚

    We propose to minimize a generic differentiable loss function with $L_1$ penalty with a redundant reparametrization and straightforward stochastic gradient descent. Our proposal is the direct generalization of a series of previous ideas that the $L_1$ penalty may be equivalent to a differentiable reparametrization with weight decay. We prove that the proposed method, \textit{spred}, is an exact solver of $L_1$ and that the reparametrization trick is completely ``benign" for a generic nonconvex function. Practically, we demonstrate the usefulness of the method in (1) training sparse neural networks to perform gene selection tasks, which involves finding relevant features in a very high dimensional space, and (2) neural network compression task, to which previous attempts at applying the $L_1$-penalty have been unsuccessful. Conceptually, our result bridges the gap between the sparsity in deep learning and conventional statistical learning.
    
[^45]: ä¸ªä½“æ²»ç–—æ•ˆæœä¼°è®¡çš„è¿ç§»å­¦ä¹ 

    Transfer Learning for Individual Treatment Effect Estimation. (arXiv:2210.00380v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.00380](http://arxiv.org/abs/2210.00380)

    æœ¬è®ºæ–‡æ¢è®¨äº†ä¸ªä½“æ²»ç–—æ•ˆæœä¼°è®¡ä¸­è¿ç§»å› æœçŸ¥è¯†çš„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªä½¿ç”¨CITAåº¦é‡è¿›è¡ŒITEçŸ¥è¯†è½¬ç§»çš„æ¡†æ¶ï¼Œå®éªŒè¯æ˜è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

    

    æœ¬ç ”ç©¶æ¢è®¨äº†åœ¨ä¸ªä½“æ²»ç–—æ•ˆæœä¼°è®¡ä¸­é€šè¿‡ä»»åŠ¡ä¹‹é—´è½¬ç§»å› æœçŸ¥è¯†çš„é—®é¢˜ã€‚æˆ‘ä»¬ä»ç†è®ºä¸Šè¯„ä¼°äº†è½¬ç§»ä¸ªä½“æ²»ç–—æ•ˆæœçŸ¥è¯†çš„å¯è¡Œæ€§ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªå®ç”¨çš„æ¡†æ¶æ¥å®ç°æœ‰æ•ˆçš„è½¬ç§»ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªä¸‹ç•Œæ¥è¯´æ˜ç›®æ ‡ä»»åŠ¡çš„ä¸ªä½“æ²»ç–—æ•ˆæœè¯¯å·®å­˜åœ¨æŒ‘æˆ˜ï¼Œå› ä¸ºç¼ºä¹åäº‹å®ä¿¡æ¯ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæˆ‘ä»¬å»ºç«‹äº†ç›®æ ‡ä»»åŠ¡åäº‹å®æŸå¤±å’Œä¸ªä½“æ²»ç–—æ•ˆæœè¯¯å·®çš„æ³›åŒ–ä¸Šç•Œï¼Œè¯æ˜äº†ä¸ªä½“æ²»ç–—æ•ˆæœçŸ¥è¯†è½¬ç§»çš„å¯è¡Œæ€§ã€‚éšåï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ¡†æ¶ï¼Œå…¶ä¸­ä½¿ç”¨æ–°çš„å› æœæ¨æ–­ä»»åŠ¡äº²å’Œåº¦ï¼ˆCITAï¼‰åº¦é‡è¿›è¡ŒITEçŸ¥è¯†è½¬ç§»ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ä½¿ç”¨CITAæ¥æ‰¾åˆ°æœ€æ¥è¿‘ç›®æ ‡ä»»åŠ¡çš„æºä»»åŠ¡ï¼Œå¹¶åˆ©ç”¨å®ƒæ¥è¿›è¡ŒITEçŸ¥è¯†è½¬ç§»ã€‚æˆ‘ä»¬æä¾›äº†å®è¯ç ”ç©¶ï¼Œè¯æ˜äº†æ‰€æå‡ºæ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼ŒITEçŸ¥è¯†è½¬ç§»å¯ä»¥æ˜¾è‘—ï¼ˆ

    This work considers the problem of transferring causal knowledge between tasks for Individual Treatment Effect (ITE) estimation. To this end, we theoretically assess the feasibility of transferring ITE knowledge and present a practical framework for efficient transfer. A lower bound is introduced on the ITE error of the target task to demonstrate that ITE knowledge transfer is challenging due to the absence of counterfactual information. Nevertheless, we establish generalization upper bounds on the counterfactual loss and ITE error of the target task, demonstrating the feasibility of ITE knowledge transfer. Subsequently, we introduce a framework with a new Causal Inference Task Affinity (CITA) measure for ITE knowledge transfer. Specifically, we use CITA to find the closest source task to the target task and utilize it for ITE knowledge transfer. Empirical studies are provided, demonstrating the efficacy of the proposed method. We observe that ITE knowledge transfer can significantly (
    
[^46]: åŒæ€è‡ªç¼–ç å™¨ - ä»è§‚å¯Ÿåˆ°è½¬åŒ–å­¦ä¹ ç¾¤ç»„ç»“æ„è¡¨ç°

    Homomorphism Autoencoder -- Learning Group Structured Representations from Observed Transitions. (arXiv:2207.12067v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2207.12067](http://arxiv.org/abs/2207.12067)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŒæ€è‡ªç¼–ç å™¨æ–¹æ³•ï¼Œä½¿æœºå™¨èƒ½å¤Ÿåœ¨è¡ŒåŠ¨ä¸­å­¦ä¹ åˆ°ä¸å…¶è¡Œä¸ºç›¸ä¸€è‡´çš„æ„ŸçŸ¥ä¿¡æ¯çš„å†…éƒ¨è¡¨ç¤ºï¼Œå¹¶æ•è·ç¯å¢ƒä¸­çš„è½¬æ¢ç»“æ„ã€‚

    

    å¦‚ä½•è®©æœºå™¨å­¦ä¹ ç³»ç»Ÿå­¦ä¹ åˆ°å‡†ç¡®è¡¨ç¤ºå…¶ä¸çœŸå®ä¸–ç•Œäº¤äº’çš„å†…åœ¨æ¨¡å‹æ˜¯ä¸€ä¸ªå°šå¾…è§£å†³çš„é—®é¢˜ã€‚ä¸ºäº†æ„å»ºä¸ä»…åŒ…å«è§‚å¯Ÿæ€§çŸ¥è¯†ï¼Œä¹ŸåŒ…å«å¹²é¢„æ€§çŸ¥è¯†çš„è¡¨ç°å­¦ä¹ æ¡†æ¶ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†è¡¨ç¤ºå­¦ä¹ å’Œç¾¤è®ºçš„æ–¹æ³•æ¥ç ”ç©¶è¯¥é—®é¢˜ã€‚æˆ‘ä»¬æå‡ºä¸€ç§æ–¹æ³•ï¼Œä½¿æœºå™¨èƒ½å¤Ÿåœ¨è¡ŒåŠ¨è¿‡ç¨‹ä¸­å­¦ä¹ åˆ°ä¸ä¹‹ç›¸ä¸€è‡´çš„æ„ŸçŸ¥ä¿¡æ¯çš„å†…éƒ¨è¡¨ç¤ºï¼Œè€Œè¿™äº›è¡ŒåŠ¨å®é™…ä¸Šæ˜¯å˜æ¢è¿™äº›ä¿¡æ¯çš„ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸€ä¸ªè‡ªç¼–ç å™¨å¹¶åœ¨å…¶æ½œåœ¨ç©ºé—´ä¸Šåº”ç”¨ç¾¤ç»„è¡¨ç¤ºï¼Œé€šè¿‡åˆ©ç”¨ç­‰å˜æŸå¤±å¼ºåˆ¶å®æ–½é€‚å½“çš„åŒæ€æ€§è´¨ä»¥å®Œæˆè®­ç»ƒã€‚ä¸ç°æœ‰æ–¹æ³•ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬çš„æ–¹æ³•ä¸éœ€è¦å…ˆéªŒç¾¤ç»„çŸ¥è¯†ï¼Œå¹¶ä¸”ä¸é™åˆ¶ä»£ç†å¯æ‰§è¡Œçš„è¡ŒåŠ¨é›†åˆã€‚ç†è®ºä¸Šï¼Œæˆ‘ä»¬è¯æ˜äº†è¯¥æ–¹æ³•çš„æœ‰æ•ˆæ€§ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†å…¶èƒ½å¤Ÿå­¦ä¹ åˆ°è¡ŒåŠ¨çš„ç¾¤ç»„è¡¨ç¤ºï¼Œä»è€Œæ•è·äº†ç¯å¢ƒä¸­çš„è½¬æ¢ç»“æ„ã€‚

    How can agents learn internal models that veridically represent interactions with the real world is a largely open question. As machine learning is moving towards representations containing not just observational but also interventional knowledge, we study this problem using tools from representation learning and group theory. We propose methods enabling an agent acting upon the world to learn internal representations of sensory information that are consistent with actions that modify it. We use an autoencoder equipped with a group representation acting on its latent space, trained using an equivariance-derived loss in order to enforce a suitable homomorphism property on the group representation. In contrast to existing work, our approach does not require prior knowledge of the group and does not restrict the set of actions the agent can perform. We motivate our method theoretically, and show empirically that it can learn a group representation of the actions, thereby capturing the str
    
[^47]: åéªŒæ¦‚å¿µè§£é‡Šä½•æ—¶å¯è¯†åˆ«ï¼Ÿ

    When are Post-hoc Conceptual Explanations Identifiable?. (arXiv:2206.13872v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2206.13872](http://arxiv.org/abs/2206.13872)

    æœ¬è®ºæ–‡æå‡ºäº†å¯è¯†åˆ«çš„æ¦‚å¿µå‘ç°æ–¹æ³•ï¼Œå¯ä»¥æ¢å¤å‡ºå¤šä¸ªå·²çŸ¥çš„æ¦‚å¿µï¼Œä»¥ç¡®ä¿è§£é‡Šçš„å¯é æ€§ã€‚å¯¹äºå…·æœ‰ä¾èµ–å…³ç³»çš„æ¦‚å¿µï¼Œæå‡ºäº†ä¸¤ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„åŠŸèƒ½ç»„åˆæ€§è´¨ã€‚è¯¥æ–¹æ³•æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

    

    å­¦ä¹ åµŒå…¥é€šå¸¸éœ€è¦é€šè¿‡æ¦‚å¿µè§£é‡Šæ¥ç†è§£å’Œåˆ†è§£ï¼Œè¿™ç§éœ€æ±‚åœ¨è§£é‡Šä¸­ä¸åŒ…å«æœ‰æ•ˆæ¦‚å¿µæ ‡ç­¾çš„æƒ…å†µä¸‹å°¤ä¸ºæ˜¾è‘—ã€‚ä¸ºäº†æä¾›åéªŒè§£é‡Šï¼Œæ¦‚å¿µå‘ç°æ–¹æ³•ä¼šåœ¨å·²è®­ç»ƒçš„åµŒå…¥ç©ºé—´ä¸­æœç´¢è§£é‡Šæ€§å¼ºçš„æ¦‚å¿µï¼Œä¾‹å¦‚ç‰©ä½“å½¢çŠ¶æˆ–é¢œè‰²ã€‚ä¸ä¹‹å‰çš„å·¥ä½œä¸åŒï¼Œæˆ‘ä»¬è®¤ä¸ºæ¦‚å¿µå‘ç°åº”è¯¥æ˜¯å¯è¯†åˆ«çš„ï¼Œè¿™æ„å‘³ç€å¯ä»¥è¢«è¯æ˜åœ°æ¢å¤å‡ºå¤šä¸ªå·²çŸ¥çš„æ¦‚å¿µï¼Œä»¥ç¡®ä¿è§£é‡Šçš„å¯é æ€§ã€‚ä¸ºäº†ä½œä¸ºä¸€ä¸ªèµ·ç‚¹ï¼Œæˆ‘ä»¬æ˜ç¡®åœ°å°†æ¦‚å¿µå‘ç°ä¸ä¼ ç»Ÿæ–¹æ³•ï¼ˆä¾‹å¦‚ä¸»æˆåˆ†åˆ†æå’Œç‹¬ç«‹æˆåˆ†åˆ†æï¼‰è”ç³»èµ·æ¥ï¼Œå¹¶é€šè¿‡è¡¨æ˜å®ƒä»¬å¯ä»¥æ¢å¤å…·æœ‰éé«˜æ–¯åˆ†å¸ƒçš„ç‹¬ç«‹æ¦‚å¿µæ¥é˜æ˜è¿™ä¸€ç‚¹ã€‚å¯¹äºå…·æœ‰ä¾èµ–å…³ç³»çš„æ¦‚å¿µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ç§æ–°çš„æ–¹æ³•ï¼Œåˆ©ç”¨å›¾åƒç”Ÿæˆè¿‡ç¨‹çš„åŠŸèƒ½ç»„åˆæ€§è´¨ã€‚æˆ‘ä»¬çš„å¯è¯æ˜å¯è¯†åˆ«çš„æ¦‚å¿µå‘ç°æ–¹æ³•æ˜æ˜¾ä¼˜äºç°æœ‰æ–¹æ³•ã€‚

    Interest in understanding and factorizing learned embedding spaces through conceptual explanations is steadily growing. When no human concept labels are available, concept discovery methods search trained embedding spaces for interpretable concepts like object shape or color that can be used to provide post-hoc explanations for decisions. Unlike previous work, we argue that concept discovery should be identifiable, meaning that a number of known concepts can be provably recovered to guarantee reliability of the explanations. As a starting point, we explicitly make the connection between concept discovery and classical methods like Principal Component Analysis and Independent Component Analysis by showing that they can recover independent concepts with non-Gaussian distributions. For dependent concepts, we propose two novel approaches that exploit functional compositionality properties of image-generating processes. Our provably identifiable concept discovery methods substantially outpe
    
[^48]: å·®åˆ†éšç§ä¼˜åŒ–ä¸­è¶…è¶Šç»Ÿä¸€ææ™®å¸ŒèŒ¨æ¡ä»¶

    Beyond Uniform Lipschitz Condition in Differentially Private Optimization. (arXiv:2206.10713v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2206.10713](http://arxiv.org/abs/2206.10713)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„å·®åˆ†éšç§ä¼˜åŒ–ç®—æ³•æ¥å¤„ç†å…¶å®ƒç®—æ³•æ— æ³•å¤„ç†çš„éå‡åŒ€ææ™®å¸ŒèŒ¨æƒ…å½¢ï¼Œå¹¶ä¸”åœ¨å…·ä½“åº”ç”¨ä¸­æä¾›äº†ç›¸åº”çš„å‚æ•°è°ƒæ•´æ–¹æ¡ˆã€‚

    

    å¤§å¤šæ•°å…³äºå·®åˆ†éšç§éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆDP-SGDï¼‰çš„å…ˆå‰ç»“æœéƒ½æ˜¯åœ¨ç»Ÿä¸€ææ™®å¸ŒèŒ¨æ€§çš„ç®€åŒ–å‡è®¾ä¸‹å¯¼å‡ºçš„ï¼Œå³æ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦éƒ½æ˜¯å‡åŒ€æœ‰ç•Œçš„ã€‚æˆ‘ä»¬é€šè¿‡å‡å®šæ¯ä¸ªæ ·æœ¬çš„æ¢¯åº¦å…·æœ‰æ ·æœ¬ç›¸å…³çš„ä¸Šç•Œï¼Œå³æ¯ä¸ªæ ·æœ¬çš„ææ™®å¸ŒèŒ¨å¸¸æ•°ï¼Œä»è€Œæ¨å¹¿äº†ç»Ÿä¸€ææ™®å¸ŒèŒ¨æ€§ã€‚è¿™äº›æœ¬èº«å¯èƒ½æ˜¯æ— ç•Œçš„ã€‚å½“æ¯ä¸ªæ ·æœ¬çš„ææ™®å¸ŒèŒ¨å¸¸æ•°æ˜¯æœ‰ç•Œçš„æ—¶ï¼Œæˆ‘ä»¬ä¸ºDP-SGDåœ¨å‡¸è¶…å‚æ•°åŒ–è®¾ç½®ä¸­é€‰æ‹©å‰ªè¾‘èŒƒæ•°æä¾›äº†åŸåˆ™æ€§æŒ‡å¯¼ï¼›å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å»ºè®®ä»…è°ƒæ•´å‰ªè¾‘èŒƒæ•°ï¼Œç›´åˆ°æœ€å°æ¯ä¸ªæ ·æœ¬ææ™®å¸ŒèŒ¨å¸¸æ•°çš„å€¼ã€‚è¿™åœ¨æ·±åº¦ç½‘ç»œä¸Šé¢„å…ˆè®­ç»ƒå…¬å…±æ•°æ®çš„ softmax å±‚çš„ç§äººè®­ç»ƒä¸­æœ‰åº”ç”¨ã€‚æˆ‘ä»¬é€šè¿‡å¯¹8ä¸ªæ•°æ®é›†çš„å®éªŒéªŒè¯äº†æˆ‘ä»¬çš„å»ºè®®çš„åŠŸæ•ˆã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜ä¸ºDP-SGDåœ¨å‡¸å’Œéå‡¸å‡½æ•°ä¸Šæä¾›äº†æ–°çš„æ”¶æ•›ç»“æœã€‚

    Most prior results on differentially private stochastic gradient descent (DP-SGD) are derived under the simplistic assumption of uniform Lipschitzness, i.e., the per-sample gradients are uniformly bounded. We generalize uniform Lipschitzness by assuming that the per-sample gradients have sample-dependent upper bounds, i.e., per-sample Lipschitz constants, which themselves may be unbounded. We provide principled guidance on choosing the clip norm in DP-SGD for convex over-parameterized settings satisfying our general version of Lipschitzness when the per-sample Lipschitz constants are bounded; specifically, we recommend tuning the clip norm only till values up to the minimum per-sample Lipschitz constant. This finds application in the private training of a softmax layer on top of a deep network pre-trained on public data. We verify the efficacy of our recommendation via experiments on 8 datasets. Furthermore, we provide new convergence results for DP-SGD on convex and nonconvex function
    
[^49]: åœ¨RKHSçš„éå‚æ•°å›å½’ä¸­æœ€ä¼˜è§£å†³åå˜é‡è½¬ç§»é—®é¢˜

    Optimally tackling covariate shift in RKHS-based nonparametric regression. (arXiv:2205.02986v2 [math.ST] UPDATED)

    [http://arxiv.org/abs/2205.02986](http://arxiv.org/abs/2205.02986)

    æœ¬æ–‡ç ”ç©¶äº†åœ¨RKHSçš„éå‚æ•°å›å½’ä¸­çš„åå˜é‡è½¬ç§»é—®é¢˜ï¼Œé’ˆå¯¹ä¸¤ä¸ªä¸åŒçš„ä¼¼ç„¶æ¯”æ—ï¼Œè¯æ˜äº†ä½¿ç”¨KRRä¼°è®¡é‡å…·æœ‰æå°åŒ–ç‡æœ€ä¼˜çš„ç‰¹ç‚¹ï¼Œå°¤å…¶æ˜¯åœ¨ä¼¼ç„¶æ¯”è¢«å‡åŒ€æœ‰ç•Œæ—¶ã€‚ä¸æ­¤åŒæ—¶ï¼Œæœ¬æ–‡ä¹Ÿè¯æ˜äº†ï¼Œåœ¨åå˜é‡è½¬ç§»ä¸‹ä¸€ä¸ªnaiveçš„ä¼°è®¡å™¨ç›¸æ¯”äºKRRæ˜¯ä¸¥æ ¼æ¬¡ä¼˜çš„ã€‚

    

    åœ¨å†ç”Ÿæ ¸å¸Œå°”ä¼¯ç‰¹ç©ºé—´ï¼ˆRKHSï¼‰ä¸Šç ”ç©¶äº†éå‚æ•°å›å½’ä¸­çš„åå˜é‡è½¬ç§»é—®é¢˜ã€‚æˆ‘ä»¬å…³æ³¨ä¸¤ä¸ªä½¿ç”¨æºå’Œç›®æ ‡åˆ†å¸ƒä¹‹é—´çš„ä¼¼ç„¶æ¯”å®šä¹‰çš„è‡ªç„¶åå˜é‡è½¬ç§»é—®é¢˜æ—ã€‚å½“ä¼¼ç„¶æ¯”è¢«å‡åŒ€æœ‰ç•Œæ—¶ï¼Œæˆ‘ä»¬è¯æ˜å¸¦æœ‰ç²¾å¿ƒé€‰æ‹©çš„æ­£åˆ™åŒ–å‚æ•°çš„æ ¸å²­å›å½’(KRR)ä¼°è®¡é‡æ˜¯æå°åŒ–ç‡æœ€ä¼˜çš„ï¼ˆæœ€å¤šå·®ä¸€ä¸ªå¯¹æ•°å› å­ï¼‰ï¼Œå¯¹äºä¸€å¤§ç±»å…·æœ‰æ­£åˆ™æ ¸ç‰¹å¾å€¼çš„RKHSè€Œè¨€ã€‚æœ‰è¶£çš„æ˜¯ï¼Œé™¤äº†ä¼¼ç„¶æ¯”ä¸Šç•Œä¹‹å¤–ï¼ŒKRRä¸éœ€è¦å¯¹ä¼¼ç„¶æ¯”æœ‰å®Œå…¨çš„çŸ¥è¯†ã€‚ä¸æ²¡æœ‰åå˜é‡è½¬ç§»çš„æ ‡å‡†ç»Ÿè®¡è®¾ç½®å½¢æˆé²œæ˜å¯¹æ¯”çš„æ˜¯ï¼Œæˆ‘ä»¬è¿˜è¯æ˜äº†ä¸€ä¸ªç®€å•ä¼°è®¡å™¨ï¼Œå³åœ¨å‡½æ•°ç±»ä¸­æœ€å°åŒ–ç»éªŒé£é™©ï¼Œä¸KRRç›¸æ¯”ï¼Œåœ¨åå˜é‡è½¬ç§»ä¸‹æ˜¯ä¸¥æ ¼æ¬¡ä¼˜çš„ã€‚ç„¶åï¼Œæˆ‘ä»¬è§£å†³äº†æ›´å¤§çš„åå˜é‡è½¬ç§»é—®é¢˜ç±»ï¼Œå…¶ä¸­ä¼¼ç„¶æ¯”å¯èƒ½æ˜¯æ— ç•Œçš„ã€‚

    We study the covariate shift problem in the context of nonparametric regression over a reproducing kernel Hilbert space (RKHS). We focus on two natural families of covariate shift problems defined using the likelihood ratios between the source and target distributions. When the likelihood ratios are uniformly bounded, we prove that the kernel ridge regression (KRR) estimator with a carefully chosen regularization parameter is minimax rate-optimal (up to a log factor) for a large family of RKHSs with regular kernel eigenvalues. Interestingly, KRR does not require full knowledge of likelihood ratios apart from an upper bound on them. In striking contrast to the standard statistical setting without covariate shift, we also demonstrate that a naive estimator, which minimizes the empirical risk over the function class, is strictly sub-optimal under covariate shift as compared to KRR. We then address the larger class of covariate shift problems where the likelihood ratio is possibly unbounde
    
[^50]: å¯é æœºå™¨å­¦ä¹ çš„å¯¹ç§°æŸå¤±è§†è§’

    A Symmetric Loss Perspective of Reliable Machine Learning. (arXiv:2101.01366v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2101.01366](http://arxiv.org/abs/2101.01366)

    å¯¹ç§°æŸå¤±æ˜¯ä¸€ç§æ–°å‹çš„ä»£ç†æŸå¤±ï¼Œèƒ½å¤Ÿä½¿å¾—å­¦ä¹ è¿‡ç¨‹å¯¹äºå—æŸæ ‡ç­¾æ›´åŠ é²æ£’ï¼Œä»è€Œæé«˜åˆ†ç±»å™¨çš„æ€§èƒ½ã€‚

    

    å½“åœ¨äºŒå…ƒåˆ†ç±»ä¸­æœ€å°åŒ–ç»éªŒé£é™©æ—¶ï¼Œå¸¸å¸¸å°†é›¶ä¸€æŸå¤±æ›¿æ¢ä¸ºä»£ç†æŸå¤±ï¼Œä»¥ä½¿å­¦ä¹ ç›®æ ‡æ˜“äºä¼˜åŒ–ã€‚äºŒå…ƒåˆ†ç±»çš„ä»£ç†æŸå¤±ä¾‹å¦‚é€»è¾‘æŸå¤±ï¼ŒhingeæŸå¤±å’ŒsigmoidæŸå¤±å¹¿ä¸ºäººçŸ¥ã€‚å·²çŸ¥ä»£ç†æŸå¤±çš„é€‰æ‹©ä¼šæå¤§åœ°å½±å“è®­ç»ƒåˆ†ç±»å™¨çš„æ€§èƒ½ï¼Œå› æ­¤åº”è¯¥ä»”ç»†é€‰æ‹©ã€‚æœ€è¿‘ï¼Œæ»¡è¶³æŸäº›å¯¹ç§°æ¡ä»¶ï¼ˆç§°ä¸ºå¯¹ç§°æŸå¤±ï¼‰çš„ä»£ç†æŸå¤±å·²ç»è¯æ˜äº†å®ƒä»¬åœ¨å­¦ä¹ æ¥è‡ªæŸåæ ‡ç­¾çš„æ•°æ®æ–¹é¢çš„å®ç”¨æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æä¾›å¯¹ç§°æŸå¤±åŠå…¶åº”ç”¨çš„æ¦‚è¿°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å›é¡¾äº†å¯¹ç§°æŸå¤±å¦‚ä½•åœ¨å¹³è¡¡è¯¯å·®ç‡ï¼ˆBERï¼‰æœ€å°åŒ–å’Œæ“ä½œç‰¹å¾æ›²çº¿ä¸‹é¢ç§¯ï¼ˆAUCï¼‰æœ€å¤§åŒ–ä¸­äº§ç”Ÿé²æ£’åˆ†ç±»çš„æ–¹æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬æ¼”ç¤ºäº†é²æ£’AUCæœ€å¤§åŒ–æ–¹æ³•å¯ä»¥å—ç›Šäºå—æŸæ ‡ç­¾ï¼Œå°¤å…¶æ˜¯ä¸å…¶ä»–ä»£ç†æŸå¤±ç›¸æ¯”ã€‚

    When minimizing the empirical risk in binary classification, it is a common practice to replace the zero-one loss with a surrogate loss to make the learning objective feasible to optimize. Examples of well-known surrogate losses for binary classification include the logistic loss, hinge loss, and sigmoid loss. It is known that the choice of a surrogate loss can highly influence the performance of the trained classifier and therefore it should be carefully chosen. Recently, surrogate losses that satisfy a certain symmetric condition (aka., symmetric losses) have demonstrated their usefulness in learning from corrupted labels. In this article, we provide an overview of symmetric losses and their applications. First, we review how a symmetric loss can yield robust classification from corrupted labels in balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization. Then, we demonstrate how the robust AUC maximization method can benefi
    
[^51]: ç»“æ„åŒ–è¿ç»­ç¨€ç–åŒ–å¢å¼ºæ·±åº¦ç½‘ç»œçš„è®­ç»ƒæ•ˆç‡

    Growing Efficient Deep Networks by Structured Continuous Sparsification. (arXiv:2007.15353v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2007.15353](http://arxiv.org/abs/2007.15353)

    æœ¬æ–‡æå‡ºä¸€ç§ç»“æ„åŒ–è¿ç»­ç¨€ç–åŒ–çš„æ·±åº¦ç½‘ç»œç»“æ„ç”Ÿé•¿æ–¹æ³•ï¼Œé€šè¿‡è¿ç»­æ¾å¼›å’Œé‡‡æ ·ç¨€ç–å­ç½‘ç»œï¼Œå¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è¾¾åˆ°ç´§å‡‘çš„ä¿®å‰ªç½‘ç»œç»“æ„ï¼ŒåŒæ—¶å¤§å¹…é™ä½è®¡ç®—å¤æ‚åº¦å¹¶ä¿æŒè¾ƒé«˜çš„å‡†ç¡®ç‡ã€‚

    

    æˆ‘ä»¬å¼€å‘äº†ä¸€ç§åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä»¥ç²¾åº¦å’Œç¨€ç–æ€§ä¸ºé©±åŠ¨çš„æ·±åº¦ç½‘ç»œç»“æ„ç”Ÿé•¿æ–¹æ³•ã€‚ä¸ç°æœ‰çš„åŸºäºå®Œæ•´æ¨¡å‹æˆ–è¶…ç½‘æ ¼æ¶æ„çš„å‰ªææˆ–æ¶æ„æœç´¢æŠ€æœ¯ä¸åŒï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯ä»¥ä»ä¸€ä¸ªå°è€Œç®€å•çš„ç§å­æ¶æ„å¼€å§‹ï¼ŒåŠ¨æ€åœ°å¢é•¿å’Œä¿®å‰ªå±‚å’Œè¿‡æ»¤å™¨ã€‚é€šè¿‡å°†ç¦»æ•£ç½‘ç»œç»“æ„ä¼˜åŒ–çš„è¿ç»­æ¾å¼›ä¸é‡‡æ ·ç¨€ç–å­ç½‘ç»œæ–¹æ¡ˆç›¸ç»“åˆï¼Œæˆ‘ä»¬å¯ä»¥äº§ç”Ÿç´§å‡‘çš„ä¿®å‰ªç½‘ç»œï¼ŒåŒæ—¶æ˜¾è‘—é™ä½è®­ç»ƒçš„è®¡ç®—å¤æ‚åº¦ã€‚ä¾‹å¦‚ï¼Œåœ¨ImageNetä¸Šï¼Œä¸åŸºçº¿ResNet-50ç›¸æ¯”ï¼Œæˆ‘ä»¬å®ç°äº†49.7ï¼…çš„æ¨ç†FLOPså’Œ47.4ï¼…çš„è®­ç»ƒFLOPsèŠ‚çœï¼ŒåŒæ—¶ä¿æŒ75.2ï¼…çš„top-1ç²¾åº¦--æ‰€æœ‰è¿™äº›éƒ½æ²¡æœ‰ä»»ä½•ä¸“é—¨çš„å¾®è°ƒé˜¶æ®µã€‚åœ¨CIFARï¼ŒImageNetï¼ŒPASCAL VOCå’ŒPenn Treebankä¸Šè¿›è¡Œå®éªŒï¼Œä½¿ç”¨å·ç§¯ç½‘ç»œè¿›è¡Œå›¾åƒåˆ†ç±»å’Œè¯­ä¹‰åˆ†å‰²ã€‚

    We develop an approach to growing deep network architectures over the course of training, driven by a principled combination of accuracy and sparsity objectives. Unlike existing pruning or architecture search techniques that operate on full-sized models or supernet architectures, our method can start from a small, simple seed architecture and dynamically grow and prune both layers and filters. By combining a continuous relaxation of discrete network structure optimization with a scheme for sampling sparse subnetworks, we produce compact, pruned networks, while also drastically reducing the computational expense of training. For example, we achieve $49.7\%$ inference FLOPs and $47.4\%$ training FLOPs savings compared to a baseline ResNet-50 on ImageNet, while maintaining $75.2\%$ top-1 accuracy -- all without any dedicated fine-tuning stage. Experiments across CIFAR, ImageNet, PASCAL VOC, and Penn Treebank, with convolutional networks for image classification and semantic segmentation, 
    
[^52]: åŸºäºå•è°ƒGANçš„æ¡ä»¶é‡‡æ ·ï¼šä»ç”Ÿæˆæ¨¡å‹åˆ°æ— ä¼¼ç„¶æ¨æ–­

    Conditional Sampling with Monotone GANs: from Generative Models to Likelihood-Free Inference. (arXiv:2006.06755v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2006.06755](http://arxiv.org/abs/2006.06755)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°çš„æ¦‚ç‡æµ‹åº¦æ¡ä»¶é‡‡æ ·æ¡†æ¶ï¼Œä½¿ç”¨å•è°ƒGANå­¦ä¹ å—çŠ¶ä¸‰è§’å½¢æ˜ å°„ï¼Œä»…ä½¿ç”¨æ¥è‡ªåº•å±‚è”åˆæ¦‚ç‡æµ‹åº¦çš„æ ·æœ¬å®ç°æ— ä¼¼ç„¶æ¨æ–­ã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°çš„æ¦‚ç‡æµ‹åº¦æ¡ä»¶é‡‡æ ·æ¡†æ¶ï¼Œä½¿ç”¨äº†å—çŠ¶ä¸‰è§’å½¢ä¼ è¾“æ˜ å°„ã€‚æˆ‘ä»¬åœ¨Banachç©ºé—´è®¾ç½®ä¸‹å¼€å‘äº†å—çŠ¶ä¸‰è§’å½¢ä¼ è¾“çš„ç†è®ºåŸºç¡€ï¼Œå»ºç«‹äº†å¯ä»¥å®ç°æ¡ä»¶é‡‡æ ·çš„ä¸€èˆ¬æ¡ä»¶ï¼Œå¹¶åœ¨å•è°ƒå—çŠ¶ä¸‰è§’å½¢æ˜ å°„ä¸æœ€ä¼˜ä¼ è¾“ä¹‹é—´å»ºç«‹è”ç³»ã€‚åŸºäºè¯¥ç†è®ºï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§è®¡ç®—æ–¹æ³•ï¼Œç§°ä¸ºå•è°ƒç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆM-GANï¼‰ï¼Œç”¨äºå­¦ä¹ åˆé€‚çš„å—çŠ¶ä¸‰è§’å½¢æ˜ å°„ã€‚æˆ‘ä»¬çš„ç®—æ³•ä»…ä½¿ç”¨æ¥è‡ªåº•å±‚è”åˆæ¦‚ç‡æµ‹åº¦çš„æ ·æœ¬ï¼Œå› æ­¤æ— éœ€ä¼¼ç„¶ã€‚M-GANçš„æ•°å€¼å®éªŒè¯æ˜äº†åœ¨åˆæˆç¤ºä¾‹ã€æ¶‰åŠå¸¸å¾®åˆ†æ–¹ç¨‹å’Œåå¾®åˆ†æ–¹ç¨‹çš„è´å¶æ–¯åé—®é¢˜ï¼Œä»¥åŠæ¦‚ç‡å›¾åƒä¿®å¤ä¸­å‡†ç¡®é‡‡æ ·æ¡ä»¶æµ‹åº¦çš„èƒ½åŠ›ã€‚

    We present a novel framework for conditional sampling of probability measures, using block triangular transport maps. We develop the theoretical foundations of block triangular transport in a Banach space setting, establishing general conditions under which conditional sampling can be achieved and drawing connections between monotone block triangular maps and optimal transport. Based on this theory, we then introduce a computational approach, called monotone generative adversarial networks (M-GANs), to learn suitable block triangular maps. Our algorithm uses only samples from the underlying joint probability measure and is hence likelihood-free. Numerical experiments with M-GAN demonstrate accurate sampling of conditional measures in synthetic examples, Bayesian inverse problems involving ordinary and partial differential equations, and probabilistic image in-painting.
    
[^53]: Denise: é¢å‘åŠæ­£å®šçŸ©é˜µçš„æ·±åº¦å¥å£®ä¸»æˆåˆ†åˆ†æ

    Denise: Deep Robust Principal Component Analysis for Positive Semidefinite Matrices. (arXiv:2004.13612v4 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2004.13612](http://arxiv.org/abs/2004.13612)

    Deniseæ˜¯ä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„ç®—æ³•ï¼Œç”¨äºå¯¹åæ–¹å·®çŸ©é˜µè¿›è¡Œä½ç§©åŠ ç¨€ç–åˆ†è§£ï¼Œè¾¾åˆ°äº†ä¸æœ€å…ˆè¿›æŠ€æœ¯ç›¸å½“çš„æ€§èƒ½è€Œä¸”è¿‘ä¹æ¥è¿‘20å€çš„åŠ é€Ÿã€‚

    

    åæ–¹å·®çŸ©é˜µçš„å¥å£®ä¸»æˆåˆ†åˆ†æåœ¨éš”ç¦»å…³é”®è§£é‡Šç‰¹å¾æ–¹é¢å‘æŒ¥ç€è‡³å…³é‡è¦çš„ä½œç”¨ã€‚ç›®å‰å¯ç”¨çš„æ‰§è¡Œä½ç§©åŠ ç¨€ç–åˆ†è§£çš„æ–¹æ³•æ˜¯é’ˆå¯¹ç‰¹å®šçŸ©é˜µçš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¿™äº›ç®—æ³•å¿…é¡»é’ˆå¯¹æ¯ä¸ªæ–°çš„çŸ©é˜µé‡æ–°è¿è¡Œã€‚ç”±äºè¿™äº›ç®—æ³•çš„è®¡ç®—æˆæœ¬å¾ˆé«˜ï¼Œå› æ­¤æœ€å¥½å­¦ä¹ å’Œå­˜å‚¨ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨è¯„ä¼°æ—¶å‡ ä¹ç«‹å³æ‰§è¡Œæ­¤åˆ†è§£ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¼•å…¥äº† Deniseï¼Œä¸€ç§åŸºäºæ·±åº¦å­¦ä¹ çš„åæ–¹å·®çŸ©é˜µçš„å¥å£®ä¸»æˆåˆ†åˆ†æç®—æ³•ï¼Œæˆ–æ›´ä¸€èˆ¬åœ°è¯´ï¼Œå¯¹ç§°åŠæ­£å®šçŸ©é˜µï¼Œå®ƒå­¦ä¹ åˆ°äº†è¿™æ ·ä¸€ä¸ªå‡½æ•°ã€‚æˆ‘ä»¬æä¾›äº† Denise çš„ç†è®ºä¿è¯ã€‚è¿™äº›åŒ…æ‹¬ä¸€ä¸ªæ–°çš„é€šç”¨é€¼è¿‘å®šç†ï¼Œé€‚ç”¨äºæˆ‘ä»¬çš„å‡ ä½•æ·±åº¦å­¦ä¹ é—®é¢˜ï¼Œå¹¶è¶‹äºå­¦ä¹ é—®é¢˜çš„æœ€ä¼˜è§£ã€‚æˆ‘ä»¬çš„å®éªŒè¡¨æ˜ï¼ŒDenise åœ¨åˆ†è§£è´¨é‡æ–¹é¢ä¸æœ€å…ˆè¿›çš„æ€§èƒ½ç›¸åŒ¹é…ï¼ŒåŒæ—¶è¿‘ä¹æ¥è¿‘20å€çš„åŠ é€Ÿã€‚

    The robust PCA of covariance matrices plays an essential role when isolating key explanatory features. The currently available methods for performing such a low-rank plus sparse decomposition are matrix specific, meaning, those algorithms must re-run for every new matrix. Since these algorithms are computationally expensive, it is preferable to learn and store a function that nearly instantaneously performs this decomposition when evaluated. Therefore, we introduce Denise, a deep learning-based algorithm for robust PCA of covariance matrices, or more generally, of symmetric positive semidefinite matrices, which learns precisely such a function. Theoretical guarantees for Denise are provided. These include a novel universal approximation theorem adapted to our geometric deep learning problem and convergence to an optimal solution to the learning problem. Our experiments show that Denise matches state-of-the-art performance in terms of decomposition quality, while being approximately $20
    
[^54]: é€šè¿‡é€»è¾‘æŒ‡å¯¼çš„è®¤è¯å¼ºåŒ–å­¦ä¹ 

    Certified Reinforcement Learning with Logic Guidance. (arXiv:1902.00778v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/1902.00778](http://arxiv.org/abs/1902.00778)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ¨¡å‹æ— å…³çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œèƒ½å¤Ÿä½¿ç”¨çº¿æ€§æ—¶æ€é€»è¾‘æ¥åˆ¶å®šé©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹çš„ç›®æ ‡ï¼Œå°†LTLå±æ€§è½¬åŒ–ä¸ºLDGBAè‡ªåŠ¨æœºï¼Œé€šè¿‡è°ƒæ•´åŒæ­¥å¥–åŠ±å‡½æ•°æœ€å¤§æ¦‚ç‡è·å¾—æ»¡è¶³LTLè§„å®šè¦æ±‚çš„æ§åˆ¶ç­–ç•¥ã€‚

    

    å¼ºåŒ–å­¦ä¹ åœ¨å„ç§æ§åˆ¶é—®é¢˜ä¸Šå¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚ç„¶è€Œï¼Œåœ¨å®‰å…¨å…³é”®é¢†åŸŸçš„åº”ç”¨éœ€è¦ä¸€ä¸ªç³»ç»Ÿå’Œæ­£å¼çš„æ–¹æ³•æ¥æŒ‡å®šä»»åŠ¡æˆ–ç›®æ ‡ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ç§æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ç®—æ³•ï¼Œå®ƒèƒ½å¤Ÿä½¿ç”¨çº¿æ€§æ—¶æ€é€»è¾‘ï¼ˆLTLï¼‰æ¥åˆ¶å®šæœªçŸ¥è¿ç»­çŠ¶æ€/åŠ¨ä½œé©¬å°”ç§‘å¤«å†³ç­–è¿‡ç¨‹ï¼ˆMDPsï¼‰çš„ç›®æ ‡ã€‚ç»™å®šçš„LTLå±æ€§è¢«è½¬åŒ–ä¸ºæé™ç¡®å®šåŒ–å¹¿ä¹‰å¸ƒæ°è‡ªåŠ¨æœºï¼ˆLDGBAï¼‰ï¼Œé€šè¿‡LDGBAåœ¨è¡Œè¿›è¿‡ç¨‹ä¸­ä¸æ–­è°ƒæ•´åŒæ­¥å¥–åŠ±å‡½æ•°ã€‚åœ¨æŸäº›å‡è®¾ä¸‹ï¼Œè¯¥ç®—æ³•å°†ä¿è¯åˆæˆå‡ºä¸€ä¸ªæ§åˆ¶ç­–ç•¥ï¼Œå…¶è½¨è¿¹æœ€å¤§æ¦‚ç‡æ»¡è¶³LTLè§„å®šçš„è¦æ±‚ã€‚

    Reinforcement Learning (RL) is a widely employed machine learning architecture that has been applied to a variety of control problems. However, applications in safety-critical domains require a systematic and formal approach to specifying requirements as tasks or goals. We propose a model-free RL algorithm that enables the use of Linear Temporal Logic (LTL) to formulate a goal for unknown continuous-state/action Markov Decision Processes (MDPs). The given LTL property is translated into a Limit-Deterministic Generalised Buchi Automaton (LDGBA), which is then used to shape a synchronous reward function on-the-fly. Under certain assumptions, the algorithm is guaranteed to synthesise a control policy whose traces satisfy the LTL specification with maximal probability.
    
[^55]: æ­£äº¤ç»Ÿè®¡å­¦ä¹ 

    Orthogonal Statistical Learning. (arXiv:1901.09036v4 [math.ST] UPDATED)

    [http://arxiv.org/abs/1901.09036](http://arxiv.org/abs/1901.09036)

    æœ¬æ–‡æå‡ºäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ ·æœ¬æ‹†åˆ†çš„å…ƒç®—æ³•ï¼Œè¯¥ç®—æ³•èƒ½å¤Ÿåœ¨è¯„ä¼°æ€»ä½“é£é™©æ—¶è€ƒè™‘å¹²æ‰°å‚æ•°ï¼Œå¹¶ä¸”å®ç°çš„è¶…é¢é£é™©ç•Œçš„å½±å“ä¸ºäºŒæ¬¡ã€‚

    

    æˆ‘ä»¬åœ¨ä¸€ä¸ªç»Ÿè®¡å­¦ä¹ çš„è®¾ç½®ä¸‹æä¾›äº†å…³äºéæ¸è¿‘è¶…é¢é£é™©ä¿è¯ï¼Œå…¶ä¸­ç›®æ ‡å‚æ•°æ‰€è¯„ä¼°çš„æ€»ä½“é£é™©å–å†³äºå¿…é¡»ä»æ•°æ®ä¸­ä¼°è®¡çš„æœªçŸ¥å¹²æ‰°å‚æ•°ã€‚æˆ‘ä»¬åˆ†æäº†ä¸€ä¸ªä¸¤é˜¶æ®µæ ·æœ¬æ‹†åˆ†çš„å…ƒç®—æ³•ï¼Œè¯¥ç®—æ³•å°†ä»»æ„ä¼°è®¡ç›®æ ‡å‚æ•°å’Œå¹²æ‰°å‚æ•°çš„ç®—æ³•ä½œä¸ºè¾“å…¥ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œå¦‚æœæ€»ä½“é£é™©æ»¡è¶³ä¸€ä¸ªç§°ä¸ºNeymanæ­£äº¤æ€§çš„æ¡ä»¶ï¼Œåˆ™å¹²æ‰°ä¼°è®¡è¯¯å·®å¯¹å…ƒç®—æ³•å®ç°çš„è¶…é¢é£é™©ç•Œçš„å½±å“ä¸ºäºŒæ¬¡ã€‚æˆ‘ä»¬çš„å®šç†ä¸å…³å¿ƒç”¨äºç›®æ ‡å’Œå¹²æ‰°çš„ç‰¹å®šç®—æ³•ï¼Œåªåšå‡ºäº†æœ‰å…³å®ƒä»¬å„è‡ªæ€§èƒ½çš„å‡è®¾ã€‚è¿™æ ·ï¼Œå°±å¯ä»¥åˆ©ç”¨ç°æœ‰æœºå™¨å­¦ä¹ çš„å¤§é‡ç»“æœï¼Œä¸ºå¸¦æœ‰å¹²æ‰°ç»„æˆçš„å­¦ä¹ æä¾›æ–°çš„ä¿è¯ã€‚æ­¤å¤–ï¼Œé€šè¿‡å…³æ³¨è¶…é¢é£é™©è€Œä¸æ˜¯å‚æ•°ä¼°è®¡ï¼Œæˆ‘ä»¬å¯ä»¥æä¾›ä¸€ä¸ªå¼±åŒ–çš„é€Ÿç‡ã€‚

    We provide non-asymptotic excess risk guarantees for statistical learning in a setting where the population risk with respect to which we evaluate the target parameter depends on an unknown nuisance parameter that must be estimated from data. We analyze a two-stage sample splitting meta-algorithm that takes as input arbitrary estimation algorithms for the target parameter and nuisance parameter. We show that if the population risk satisfies a condition called Neyman orthogonality, the impact of the nuisance estimation error on the excess risk bound achieved by the meta-algorithm is of second order. Our theorem is agnostic to the particular algorithms used for the target and nuisance and only makes an assumption on their individual performance. This enables the use of a plethora of existing results from machine learning to give new guarantees for learning with a nuisance component. Moreover, by focusing on excess risk rather than parameter estimation, we can provide rates under weaker a
    

