# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Signature Isolation Forest](https://arxiv.org/abs/2403.04405) | ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å¼‚å¸¸æ£€æµ‹ç®—æ³•"Signature Isolation Forest"ï¼Œåˆ©ç”¨ç²—è·¯å¾„ç†è®ºçš„ç­¾åå˜æ¢å»é™¤äº†Functional Isolation Forestçš„çº¿æ€§å†…ç§¯å’Œè¯å…¸é€‰æ‹©æ–¹é¢çš„é™åˆ¶ã€‚ |
| [^2] | [Nonparametric logistic regression with deep learning.](http://arxiv.org/abs/2401.12482) | æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥åˆ†æéå‚æ•° logistic å›å½’é—®é¢˜ï¼Œé€šè¿‡åœ¨æ¸©å’Œçš„å‡è®¾ä¸‹ï¼Œåœ¨ Hellinger è·ç¦»ä¸‹æ¨å¯¼å‡ºäº†æœ€å¤§ä¼¼ç„¶ä¼°è®¡å™¨çš„æ”¶æ•›é€Ÿç‡ã€‚ |
| [^3] | [Enhancing selectivity using Wasserstein distance based reweighing.](http://arxiv.org/abs/2401.11562) | æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä½¿ç”¨Wassersteinè·ç¦»è¿›è¡ŒåŠ æƒçš„ç®—æ³•ï¼Œåœ¨æ ‡è®°çš„æ•°æ®é›†ä¸Šè®­ç»ƒç¥ç»ç½‘ç»œå¯ä»¥é€¼è¿‘åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„ç»“æœã€‚æˆ‘ä»¬è¯æ˜äº†ç®—æ³•å¯ä»¥è¾“å‡ºæ¥è¿‘æœ€ä¼˜çš„åŠ æƒï¼Œä¸”ç®—æ³•ç®€å•å¯æ‰©å±•ã€‚æˆ‘ä»¬çš„ç®—æ³•å¯ä»¥æœ‰æ„åœ°å¼•å…¥åˆ†å¸ƒåç§»è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚ä½œä¸ºåº”ç”¨å®ä¾‹ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¯†åˆ«å¯¹ç»†èƒä¿¡å·ä¼ å¯¼çš„MAPæ¿€é…¶å…·æœ‰éç»“åˆæ€§çš„å°åˆ†å­ç»“åˆç‰©ã€‚ |
| [^4] | [The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties.](http://arxiv.org/abs/2304.09310) | æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é²æ£’çš„è‡ªé€‚åº” $\tau$-Lasso ä¼°è®¡å™¨ï¼ŒåŒæ—¶é‡‡ç”¨è‡ªé€‚åº” $\ell_1$-èŒƒæ•°æƒ©ç½šé¡¹ä»¥é™ä½çœŸå®å›å½’ç³»æ•°çš„åå·®ã€‚å®ƒå…·æœ‰å˜é‡é€‰æ‹©ä¸€è‡´æ€§å’ŒçœŸå®æ”¯æŒä¸‹å›å½’å‘é‡çš„æ¸è¿‘æ­£æ€æ€§çš„æœ€ä¼˜æ€§è´¨ï¼Œå‡å®šå·²çŸ¥çœŸå®å›å½’å‘é‡çš„æ”¯æŒã€‚ |

# è¯¦ç»†

[^1]: Signature Isolation Forest

    Signature Isolation Forest

    [https://arxiv.org/abs/2403.04405](https://arxiv.org/abs/2403.04405)

    ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å¼‚å¸¸æ£€æµ‹ç®—æ³•"Signature Isolation Forest"ï¼Œåˆ©ç”¨ç²—è·¯å¾„ç†è®ºçš„ç­¾åå˜æ¢å»é™¤äº†Functional Isolation Forestçš„çº¿æ€§å†…ç§¯å’Œè¯å…¸é€‰æ‹©æ–¹é¢çš„é™åˆ¶ã€‚

    

    Functional Isolation Forest (FIF)æ˜¯ä¸€ç§é’ˆå¯¹åŠŸèƒ½æ•°æ®è®¾è®¡çš„æœ€æ–°ä¸€æµå¼‚å¸¸æ£€æµ‹(AD)ç®—æ³•ã€‚å®ƒä¾èµ–äºä¸€ç§æ ‘åˆ†åŒºè¿‡ç¨‹ï¼Œé€šè¿‡å°†æ¯ä¸ªæ›²çº¿è§‚æµ‹æŠ•å½±åˆ°é€šè¿‡çº¿æ€§å†…ç§¯ç»˜åˆ¶çš„è¯å…¸ä¸Šæ¥è®¡ç®—å¼‚å¸¸å¾—åˆ†ã€‚æœ¬æ–‡é€šè¿‡å¼•å…¥â€œSignature Isolation Forestâ€ï¼Œä¸€ç§åˆ©ç”¨ç²—è·¯å¾„ç†è®ºç­¾åå˜æ¢çš„æ–°é¢–ADç®—æ³•ç±»ï¼Œæ¥è§£å†³è¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡æå‡ºä¸¤ç§ç®—æ³•æ¥æ¶ˆé™¤FIFæ–½åŠ çš„é™åˆ¶ï¼Œè¿™ä¸¤ç§ç®—æ³•ç‰¹åˆ«é’ˆå¯¹FIFå†…ç§¯çš„çº¿æ€§æ€§å’Œè¯å…¸çš„é€‰æ‹©ã€‚

    arXiv:2403.04405v1 Announce Type: cross  Abstract: Functional Isolation Forest (FIF) is a recent state-of-the-art Anomaly Detection (AD) algorithm designed for functional data. It relies on a tree partition procedure where an abnormality score is computed by projecting each curve observation on a drawn dictionary through a linear inner product. Such linear inner product and the dictionary are a priori choices that highly influence the algorithm's performances and might lead to unreliable results, particularly with complex datasets. This work addresses these challenges by introducing \textit{Signature Isolation Forest}, a novel AD algorithm class leveraging the rough path theory's signature transform. Our objective is to remove the constraints imposed by FIF through the proposition of two algorithms which specifically target the linearity of the FIF inner product and the choice of the dictionary. We provide several numerical experiments, including a real-world applications benchmark sho
    
[^2]: éå‚æ•° logistic å›å½’ä¸æ·±åº¦å­¦ä¹ 

    Nonparametric logistic regression with deep learning. (arXiv:2401.12482v1 [math.ST])

    [http://arxiv.org/abs/2401.12482](http://arxiv.org/abs/2401.12482)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§ç®€å•çš„æ–¹æ³•æ¥åˆ†æéå‚æ•° logistic å›å½’é—®é¢˜ï¼Œé€šè¿‡åœ¨æ¸©å’Œçš„å‡è®¾ä¸‹ï¼Œåœ¨ Hellinger è·ç¦»ä¸‹æ¨å¯¼å‡ºäº†æœ€å¤§ä¼¼ç„¶ä¼°è®¡å™¨çš„æ”¶æ•›é€Ÿç‡ã€‚

    

    è€ƒè™‘éå‚æ•° logistic å›å½’é—®é¢˜ã€‚åœ¨ logistic å›å½’ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸è€ƒè™‘æœ€å¤§ä¼¼ç„¶ä¼°è®¡å™¨ï¼Œè€Œè¿‡åº¦é£é™©æ˜¯çœŸå®æ¡ä»¶ç±»æ¦‚ç‡å’Œä¼°è®¡æ¡ä»¶ç±»æ¦‚ç‡ä¹‹é—´ Kullback-Leibler (KL) æ•£åº¦çš„æœŸæœ›ã€‚ç„¶è€Œï¼Œåœ¨éå‚æ•° logistic å›å½’ä¸­ï¼ŒKL æ•£åº¦å¾ˆå®¹æ˜“å‘æ•£ï¼Œå› æ­¤ï¼Œè¿‡åº¦é£é™©çš„æ”¶æ•›å¾ˆéš¾è¯æ˜æˆ–ä¸æˆç«‹ã€‚è‹¥å¹²ç°æœ‰ç ”ç©¶è¡¨æ˜ï¼Œåœ¨å¼ºå‡è®¾ä¸‹ KL æ•£åº¦çš„æ”¶æ•›æ€§ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¼°è®¡çœŸå®çš„æ¡ä»¶ç±»æ¦‚ç‡ã€‚å› æ­¤ï¼Œä¸éœ€è¦åˆ†æè¿‡åº¦é£é™©æœ¬èº«ï¼Œåªéœ€åœ¨æŸäº›åˆé€‚çš„åº¦é‡ä¸‹è¯æ˜æœ€å¤§ä¼¼ç„¶ä¼°è®¡å™¨çš„ä¸€è‡´æ€§å³å¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ç®€å•ç»Ÿä¸€çš„æ–¹æ³•åˆ†æéå‚æ•°æœ€å¤§ä¼¼ç„¶ä¼°è®¡å™¨ (NPMLE)ï¼Œç›´æ¥æ¨å¯¼å‡º NPMLE åœ¨ Hellinger è·ç¦»ä¸‹çš„æ”¶æ•›é€Ÿç‡ï¼Œåœ¨æ¸©å’Œçš„å‡è®¾ä¸‹æˆç«‹ã€‚

    Consider the nonparametric logistic regression problem. In the logistic regression, we usually consider the maximum likelihood estimator, and the excess risk is the expectation of the Kullback-Leibler (KL) divergence between the true and estimated conditional class probabilities. However, in the nonparametric logistic regression, the KL divergence could diverge easily, and thus, the convergence of the excess risk is difficult to prove or does not hold. Several existing studies show the convergence of the KL divergence under strong assumptions. In most cases, our goal is to estimate the true conditional class probabilities. Thus, instead of analyzing the excess risk itself, it suffices to show the consistency of the maximum likelihood estimator in some suitable metric. In this paper, using a simple unified approach for analyzing the nonparametric maximum likelihood estimator (NPMLE), we directly derive the convergence rates of the NPMLE in the Hellinger distance under mild assumptions. 
    
[^3]: ä½¿ç”¨Wassersteinè·ç¦»è¿›è¡ŒåŠ æƒä»¥å¢å¼ºé€‰æ‹©æ€§

    Enhancing selectivity using Wasserstein distance based reweighing. (arXiv:2401.11562v1 [stat.ML])

    [http://arxiv.org/abs/2401.11562](http://arxiv.org/abs/2401.11562)

    æˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä½¿ç”¨Wassersteinè·ç¦»è¿›è¡ŒåŠ æƒçš„ç®—æ³•ï¼Œåœ¨æ ‡è®°çš„æ•°æ®é›†ä¸Šè®­ç»ƒç¥ç»ç½‘ç»œå¯ä»¥é€¼è¿‘åœ¨å…¶ä»–æ•°æ®é›†ä¸Šè®­ç»ƒå¾—åˆ°çš„ç»“æœã€‚æˆ‘ä»¬è¯æ˜äº†ç®—æ³•å¯ä»¥è¾“å‡ºæ¥è¿‘æœ€ä¼˜çš„åŠ æƒï¼Œä¸”ç®—æ³•ç®€å•å¯æ‰©å±•ã€‚æˆ‘ä»¬çš„ç®—æ³•å¯ä»¥æœ‰æ„åœ°å¼•å…¥åˆ†å¸ƒåç§»è¿›è¡Œå¤šç›®æ ‡ä¼˜åŒ–ã€‚ä½œä¸ºåº”ç”¨å®ä¾‹ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¯†åˆ«å¯¹ç»†èƒä¿¡å·ä¼ å¯¼çš„MAPæ¿€é…¶å…·æœ‰éç»“åˆæ€§çš„å°åˆ†å­ç»“åˆç‰©ã€‚

    

    ç»™å®šä¸¤ä¸ªæ ‡è®°æ•°æ®é›†ğ’®å’Œğ’¯ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€å•é«˜æ•ˆçš„è´ªå©ªç®—æ³•æ¥å¯¹æŸå¤±å‡½æ•°è¿›è¡ŒåŠ æƒï¼Œä½¿å¾—åœ¨ğ’®ä¸Šè®­ç»ƒå¾—åˆ°çš„ç¥ç»ç½‘ç»œæƒé‡çš„æé™åˆ†å¸ƒé€¼è¿‘åœ¨ğ’¯ä¸Šè®­ç»ƒå¾—åˆ°çš„æé™åˆ†å¸ƒã€‚åœ¨ç†è®ºæ–¹é¢ï¼Œæˆ‘ä»¬è¯æ˜äº†å½“è¾“å…¥æ•°æ®é›†çš„åº¦é‡ç†µæœ‰ç•Œæ—¶ï¼Œæˆ‘ä»¬çš„è´ªå©ªç®—æ³•è¾“å‡ºæ¥è¿‘æœ€ä¼˜çš„åŠ æƒï¼Œå³ç½‘ç»œæƒé‡çš„ä¸¤ä¸ªä¸å˜åˆ†å¸ƒåœ¨æ€»å˜å·®è·ç¦»ä¸Šå¯ä»¥è¯æ˜æ¥è¿‘ã€‚æ­¤å¤–ï¼Œè¯¥ç®—æ³•ç®€å•å¯æ‰©å±•ï¼Œå¹¶ä¸”æˆ‘ä»¬è¿˜è¯æ˜äº†ç®—æ³•çš„æ•ˆç‡ä¸Šç•Œã€‚æˆ‘ä»¬çš„ç®—æ³•å¯ä»¥æœ‰æ„åœ°å¼•å…¥åˆ†å¸ƒåç§»ä»¥è¿›è¡Œï¼ˆè½¯ï¼‰å¤šç›®æ ‡ä¼˜åŒ–ã€‚ä½œä¸ºä¸€ä¸ªåŠ¨æœºåº”ç”¨ï¼Œæˆ‘ä»¬è®­ç»ƒäº†ä¸€ä¸ªç¥ç»ç½‘ç»œæ¥è¯†åˆ«å¯¹MNK2ï¼ˆä¸€ç§ç»†èƒä¿¡å·ä¼ å¯¼çš„MAPæ¿€é…¶ï¼‰å…·æœ‰éç»“åˆæ€§çš„å°åˆ†å­ç»“åˆç‰©ã€‚

    Given two labeled data-sets $\mathcal{S}$ and $\mathcal{T}$, we design a simple and efficient greedy algorithm to reweigh the loss function such that the limiting distribution of the neural network weights that result from training on $\mathcal{S}$ approaches the limiting distribution that would have resulted by training on $\mathcal{T}$.  On the theoretical side, we prove that when the metric entropy of the input data-sets is bounded, our greedy algorithm outputs a close to optimal reweighing, i.e., the two invariant distributions of network weights will be provably close in total variation distance. Moreover, the algorithm is simple and scalable, and we prove bounds on the efficiency of the algorithm as well.  Our algorithm can deliberately introduce distribution shift to perform (soft) multi-criteria optimization. As a motivating application, we train a neural net to recognize small molecule binders to MNK2 (a MAP Kinase, responsible for cell signaling) which are non-binders to MNK1
    
[^4]: è‡ªé€‚åº” $\tau$-Lassoï¼šå…¶å¥å£®æ€§å’Œæœ€ä¼˜æ€§è´¨ã€‚

    The Adaptive $\tau$-Lasso: Its Robustness and Oracle Properties. (arXiv:2304.09310v1 [stat.ML])

    [http://arxiv.org/abs/2304.09310](http://arxiv.org/abs/2304.09310)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§æ–°å‹é²æ£’çš„è‡ªé€‚åº” $\tau$-Lasso ä¼°è®¡å™¨ï¼ŒåŒæ—¶é‡‡ç”¨è‡ªé€‚åº” $\ell_1$-èŒƒæ•°æƒ©ç½šé¡¹ä»¥é™ä½çœŸå®å›å½’ç³»æ•°çš„åå·®ã€‚å®ƒå…·æœ‰å˜é‡é€‰æ‹©ä¸€è‡´æ€§å’ŒçœŸå®æ”¯æŒä¸‹å›å½’å‘é‡çš„æ¸è¿‘æ­£æ€æ€§çš„æœ€ä¼˜æ€§è´¨ï¼Œå‡å®šå·²çŸ¥çœŸå®å›å½’å‘é‡çš„æ”¯æŒã€‚

    

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§ç”¨äºåˆ†æé«˜ç»´æ•°æ®é›†çš„æ–°å‹æ­£åˆ™åŒ–é²æ£’ $\tau$-å›å½’ä¼°è®¡å™¨ï¼Œä»¥åº”å¯¹å“åº”å˜é‡å’Œåå˜é‡çš„ä¸¥é‡æ±¡æŸ“ã€‚æˆ‘ä»¬ç§°è¿™ç§ä¼°è®¡å™¨ä¸ºè‡ªé€‚åº” $\tau$-Lassoï¼Œå®ƒå¯¹å¼‚å¸¸å€¼å’Œé«˜æ æ†ç‚¹å…·æœ‰é²æ£’æ€§ï¼ŒåŒæ—¶é‡‡ç”¨è‡ªé€‚åº” $\ell_1$-èŒƒæ•°æƒ©ç½šé¡¹æ¥å‡å°‘çœŸå®å›å½’ç³»æ•°çš„åå·®ã€‚å…·ä½“è€Œè¨€ï¼Œè¯¥è‡ªé€‚åº” $\ell_1$-èŒƒæ•°æƒ©ç½šé¡¹ä¸ºæ¯ä¸ªå›å½’ç³»æ•°åˆ†é…ä¸€ä¸ªæƒé‡ã€‚å¯¹äºå›ºå®šæ•°é‡çš„é¢„æµ‹å˜é‡ $p$ï¼Œæˆ‘ä»¬æ˜¾ç¤ºå‡ºè‡ªé€‚åº” $\tau$-Lasso å…·æœ‰å˜é‡é€‰æ‹©ä¸€è‡´æ€§å’ŒçœŸå®æ”¯æŒä¸‹å›å½’å‘é‡çš„æ¸è¿‘æ­£æ€æ€§çš„æœ€ä¼˜æ€§è´¨ï¼Œå‡å®šå·²çŸ¥çœŸå®å›å½’å‘é‡çš„æ”¯æŒã€‚ç„¶åæˆ‘ä»¬é€šè¿‡æœ‰é™æ ·æœ¬æ–­ç‚¹å’Œå½±å“å‡½æ•°æ¥è¡¨å¾å…¶å¥å£®æ€§ã€‚æˆ‘ä»¬è¿›è¡Œäº†å¹¿æ³›çš„æ¨¡æ‹Ÿæ¥æ¯”è¾ƒä¸åŒçš„ä¼°è®¡å™¨çš„æ€§èƒ½ã€‚

    This paper introduces a new regularized version of the robust $\tau$-regression estimator for analyzing high-dimensional data sets subject to gross contamination in the response variables and covariates. We call the resulting estimator adaptive $\tau$-Lasso that is robust to outliers and high-leverage points and simultaneously employs adaptive $\ell_1$-norm penalty term to reduce the bias associated with large true regression coefficients. More specifically, this adaptive $\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\tau$-Lasso has the oracle property with respect to variable-selection consistency and asymptotic normality for the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We then characterize its robustness via the finite-sample breakdown point and the influence function. We carry-out extensive simulations to compare the per
    

