# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory.](http://arxiv.org/abs/2308.01853) | è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†ç»Ÿè®¡ä¼°è®¡ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œä¸»è¦å…³æ³¨Wassersteinåˆ†å¸ƒåç§»ï¼Œæå‡ºäº†è”åˆåˆ†å¸ƒåç§»æ¦‚å¿µï¼Œå¹¶åˆ†æäº†å‡ ä¸ªç»Ÿè®¡é—®é¢˜çš„è§£å†³æ–¹æ³•ã€‚è®ºæ–‡å‘ç°äº†æœ€ä¼˜çš„æå°æå¤§é£é™©å’Œæœ€ä¸åˆ©çš„æ‰°åŠ¨ï¼Œå¹¶è¯æ˜äº†æ ·æœ¬å‡å€¼å’Œæœ€å°äºŒä¹˜ä¼°è®¡é‡çš„ä¼˜è¶Šæ€§ã€‚ |
| [^2] | [Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data.](http://arxiv.org/abs/2308.01839) | æœ¬æ–‡æå‡ºäº†ä¸€ç§è°±æµå½¢å¯¹é½å’Œæ¨æ–­ï¼ˆSMAIï¼‰æ¡†æ¶ï¼Œé€šè¿‡æä¾›ä¸€ç§ç»Ÿè®¡æ£€éªŒæ–¹æ³•æ¥ç¡®å®šå•ç»†èƒæ•°æ®é›†ä¹‹é—´çš„å¯¹é½æ€§ï¼Œé¿å…è¯¯å¯¼æ€§æ¨æ–­ï¼Œå¹¶ä¿æŒæ•°æ®æ•´åˆçš„ç»“æ„å’Œå¯è§£é‡Šæ€§ã€‚ |
| [^3] | [Distribution-Free Inference for the Regression Function of Binary Classification.](http://arxiv.org/abs/2308.01835) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ†å¸ƒæ— å…³çš„æ–¹æ³•æ¥æ¨æ–­äºŒå…ƒåˆ†ç±»é—®é¢˜ä¸­çš„å›å½’å‡½æ•°ï¼Œé€šè¿‡æ„å»ºç½®ä¿¡åŒºé—´æ¥è§£å†³è¯¥é—®é¢˜ï¼Œç›¸å…³ç®—æ³•ç»è¿‡éªŒè¯å…·æœ‰å¯é æ€§ã€‚ |
| [^4] | [Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data.](http://arxiv.org/abs/2308.01729) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè”åˆç²¾ç®—ç¥ç»ç½‘ç»œæ¡†æ¶çš„æ¨ªæ–­é¢å’Œçºµå‘ç´¢èµ”è®¡æ•°æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆä¼ ç»Ÿç²¾ç®—æ¨¡å‹å’Œç¥ç»ç½‘ç»œï¼Œå……åˆ†åˆ©ç”¨äº†ä¸¤ä¸ªæ¨¡å‹çš„ä¼˜åŠ¿ã€‚ |
| [^5] | [Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity.](http://arxiv.org/abs/2308.01677) | æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºå¼ é‡æ ¸èŒƒæ•°çš„çº¦æŸæœ€å°åŒ–æ–¹æ³•åœ¨ä½ç§©å¼ é‡æ¢å¤ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºäº†é€‚å½“çš„ä¸¥æ ¼äº’è¡¥æ€§æ¡ä»¶ï¼Œå¹¶ä¸”å¾—åˆ°äº†åœ¨æ­¤æ¡ä»¶ä¸‹çš„ä¸»è¦ç»“æœï¼š1.å¯¹äºç‰¹å®šå½¢å¼çš„ç›®æ ‡å‡½æ•°ï¼Œæ ‡å‡†æŠ•å½±æ¢¯åº¦æ–¹æ³•å…·æœ‰çº¿æ€§æ”¶æ•›é€Ÿåº¦ï¼Œå°½ç®¡ç›®æ ‡å‡½æ•°ä¸ä¸€å®šæ˜¯å¼ºå‡¸çš„ã€‚2.å¯¹äºå…‰æ»‘çš„ç›®æ ‡å‡½æ•°ï¼Œæ ‡å‡†æ¢¯åº¦æ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦å’Œæ¯æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´å¯èƒ½æ˜¾è‘—æé«˜ã€‚ |
| [^6] | [Causal thinking for decision making on Electronic Health Records: why and how.](http://arxiv.org/abs/2308.01605) | æœ¬æ–‡ä»‹ç»äº†åœ¨ç”µå­å¥åº·è®°å½•ä¸­ä½¿ç”¨å› æœæ€ç»´è¿›è¡Œå†³ç­–çš„å¿…è¦æ€§å’Œæ–¹æ³•ã€‚é€šè¿‡æ¨¡æ‹Ÿéšæœºè¯•éªŒæ¥ä¸ªæ€§åŒ–å†³ç­–ï¼Œä»¥å‡å°‘æ•°æ®ä¸­çš„åè§ã€‚è¿™å¯¹äºåˆ†æç”µå­å¥åº·è®°å½•æˆ–ç´¢èµ”æ•°æ®ä»¥å¾—å‡ºå› æœç»“è®ºçš„æœ€é‡è¦é™·é˜±å’Œè€ƒè™‘å› ç´ è¿›è¡Œäº†é‡ç‚¹å¼ºè°ƒã€‚ |
| [^7] | [Fast Slate Policy Optimization: Going Beyond Plackett-Luce.](http://arxiv.org/abs/2308.01566) | æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¿«é€ŸSlateç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡æå‡ºä¸€ç§æ–°çš„ç­–ç•¥ç±»ï¼Œå¯ä»¥åœ¨å¤§è§„æ¨¡å†³ç­–ç³»ç»Ÿä¸­æœ‰æ•ˆåœ°ä¼˜åŒ–ä»»æ„å¥–åŠ±å‡½æ•°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç™¾ä¸‡çº§åˆ«åŠ¨ä½œç©ºé—´é—®é¢˜ä¸Šå…·æœ‰å¾ˆå¥½çš„æ•ˆæœã€‚ |
| [^8] | [Non-equilibrium physics: from spin glasses to machine and neural learning.](http://arxiv.org/abs/2308.01538) | æœ¬è®ºæ–‡ç ”ç©¶äº†æ— åºç³»ç»Ÿä¸­çš„æ–°å…´æ™ºèƒ½è¡Œä¸ºï¼Œå¹¶é€šè¿‡ç»Ÿè®¡ç‰©ç†å­¦æ¢ç´¢å­¦ä¹ æœºåˆ¶å’Œç‰©ç†åŠ¨åŠ›å­¦ä¹‹é—´çš„å…³ç³»ï¼Œä»¥æ­¤ä¸ºæŒ‡å¯¼åŸåˆ™è®¾è®¡æ™ºèƒ½ç³»ç»Ÿã€‚ |
| [^9] | [Minimax Optimal $Q$ Learning with Nearest Neighbors.](http://arxiv.org/abs/2308.01490) | æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–°çš„å…·æœ‰æœ€è¿‘é‚»çš„$Q$å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè§£å†³è¿ç»­çŠ¶æ€ç©ºé—´ä¸‹æ”¶æ•›é€Ÿåº¦å·®çš„é—®é¢˜ã€‚ |
| [^10] | [Online covariance estimation for stochastic gradient descent under Markovian sampling.](http://arxiv.org/abs/2308.01481) | æœ¬æ–‡ç ”ç©¶äº†åœ¨é©¬å°”å¯å¤«é‡‡æ ·ä¸‹çš„éšæœºæ¢¯åº¦ä¸‹é™ä¸­çš„åœ¨çº¿é‡å æ‰¹æ¬¡å‡å€¼åæ–¹å·®ä¼°è®¡å™¨ï¼Œå¹¶è¯æ˜äº†å…¶æ”¶æ•›é€Ÿç‡ä¸º$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$å’Œ$O\big(\sqrt{d}\,n^{-1/8}\big)$ï¼Œåˆ†åˆ«å¯¹åº”äºçŠ¶æ€ç›¸å…³å’ŒçŠ¶æ€æ— å…³çš„é©¬å°”å¯å¤«é‡‡æ ·ã€‚è¿™äº›é€Ÿç‡ä¸ç‹¬ç«‹åŒåˆ†å¸ƒæƒ…å†µä¸‹çš„æœ€ä½³æ”¶æ•›é€Ÿç‡ç›¸åŒ¹é…ï¼Œå¹¶ä¸”å…‹æœäº†ç”±äºé©¬å°”å¯å¤«é‡‡æ ·è€Œå¼•èµ·çš„æŒ‘æˆ˜ã€‚ |
| [^11] | [Interpretable Machine Learning for Discovery: Statistical Challenges \& Opportunities.](http://arxiv.org/abs/2308.01475) | å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æŠ€æœ¯è¢«å¹¿æ³›ç”¨äºå¤„ç†å¤§æ•°æ®é›†ã€å¯è§†åŒ–é¢„æµ‹å’Œæ•°æ®é©±åŠ¨çš„å‘ç°ï¼Œè¯¥è®ºæ–‡å›é¡¾äº†è¿™ä¸€é¢†åŸŸå¹¶æ¢è®¨äº†éªŒè¯å‘ç°çš„æŒ‘æˆ˜ã€‚ |
| [^12] | [Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning.](http://arxiv.org/abs/2308.01358) | æœ¬æ–‡ç ”ç©¶äº†å‹ç¼©å¯¹åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒçš„æ— åå‹ç¼©æ“ä½œç¬¦çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚é’ˆå¯¹æœ€å°äºŒä¹˜å›å½’ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªéšæœºé€¼è¿‘ç®—æ³•ï¼Œå¹¶è€ƒè™‘äº†éšæœºåœºçš„ä¸€èˆ¬å‡è®¾å’Œå™ªå£°åæ–¹å·®çš„é™åˆ¶ï¼Œä»¥åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ã€‚ |
| [^13] | [An efficient, provably exact algorithm for the 0-1 loss linear classification problem.](http://arxiv.org/abs/2306.12344) | è¯¥ç ”ç©¶è¯¦ç»†ä»‹ç»äº†ä¸€ç§åä¸ºå¢é‡å•å…ƒæšä¸¾ï¼ˆICEï¼‰çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥ç²¾ç¡®è§£å†³å®šç»´åº¦0-1æŸå¤±çº¿æ€§åˆ†ç±»é—®é¢˜ã€‚ |
| [^14] | [Robust, randomized preconditioning for kernel ridge regression.](http://arxiv.org/abs/2304.12465) | é’ˆå¯¹æ ¸å²­å›å½’é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸¤ç§å¼ºå¥çš„éšæœºé¢„å¤„ç†æŠ€æœ¯ï¼Œåˆ†åˆ«è§£å†³äº†å…¨æ•°æ®KRRé—®é¢˜å’Œé™åˆ¶ç‰ˆKRRé—®é¢˜ï¼Œå…‹æœäº†ä»¥å¾€é¢„å¤„ç†å™¨çš„æ•…éšœæ¨¡å¼ã€‚ |
| [^15] | [Normative framework for deriving neural networks with multi-compartmental neurons and non-Hebbian plasticity.](http://arxiv.org/abs/2302.10051) | è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç›¸ä¼¼æ€§åŒ¹é…æ–¹æ³•çš„è§„èŒƒæ¡†æ¶ï¼Œå¯ä»¥è§£å†³å¯¼å‡ºç¥ç»ç½‘ç»œä¸­å¤šå®¤ç¥ç»å…ƒå’Œéæµ·åšå‹å¯å¡‘æ€§çš„æŒ‘æˆ˜ã€‚ |
| [^16] | [Optimal Training of Mean Variance Estimation Neural Networks.](http://arxiv.org/abs/2302.08875) | æœ¬æ–‡ç ”ç©¶äº†å‡æ–¹å·®ä¼°è®¡ç½‘ç»œçš„æœ€ä¼˜å®ç°ï¼Œå¹¶å‘ç°é€šè¿‡ä½¿ç”¨é¢„çƒ­æœŸå¯ä»¥é¿å…æ”¶æ•›å›°éš¾ã€‚ |
| [^17] | [Matrix Estimation for Individual Fairness.](http://arxiv.org/abs/2302.02096) | æœ¬æ–‡ç ”ç©¶äº†ä¸ªä½“å…¬å¹³æ€§(IF)å’ŒçŸ©é˜µä¼°è®¡(ME)ä¹‹é—´çš„è”ç³»ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨é€‚å½“æ¡ä»¶ä¸‹ä½¿ç”¨MEæ–¹æ³•è¿›è¡Œæ•°æ®é¢„å¤„ç†å¯ä»¥æ”¹å–„ç®—æ³•çš„ä¸ªä½“å…¬å¹³æ€§ï¼Œå¹¶ä¸”ä¸ä¼šç‰ºç‰²æ€§èƒ½ã€‚ |
| [^18] | [Confident Neural Network Regression with Bootstrapped Deep Ensembles.](http://arxiv.org/abs/2202.10903) | æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºBootstrapped Deep Ensemblesçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç»å…¸çš„æœ‰é™æ•°æ®æ•ˆåº”ï¼Œæ˜ç¡®è€ƒè™‘ç¥ç»ç½‘ç»œå›å½’ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚ |
| [^19] | [How to Evaluate Uncertainty Estimates in Machine Learning for Regression?.](http://arxiv.org/abs/2106.03395) | æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•è¯„ä¼°æœºå™¨å­¦ä¹ å›å½’ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå‘ç°ç›®å‰çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°ä¼°è®¡è´¨é‡å’Œé¢„æµ‹åŒºé—´çš„å…³ç³»ã€‚ |
| [^20] | [Random Planted Forest: a directly interpretable tree ensemble.](http://arxiv.org/abs/2012.14563) | æå‡ºäº†ä¸€ç§åä¸º"éšæœºç§æ¤æ£®æ—"çš„ç®—æ³•ï¼Œé€šè¿‡ä¿®æ”¹éšæœºæ£®æ—ç®—æ³•ï¼Œä¿ç•™åˆ‡åˆ†åçš„æŸäº›å¶å­ï¼Œå½¢æˆéäºŒè¿›åˆ¶æ ‘ï¼Œå®ç°ç›´æ¥å¯è§£é‡Šçš„æ ‘é›†ç®—æ³•ã€‚è¯¥ç®—æ³•å…·æœ‰è¾ƒå¥½çš„é¢„æµ‹å’Œå¯è§†åŒ–ç‰¹æ€§ã€‚ |
| [^21] | [Adversarial Meta-Learning of Gamma-Minimax Estimators That Leverage Prior Knowledge.](http://arxiv.org/abs/2012.05465) | æœ¬æ–‡æå‡ºå¯¹æŠ—å…ƒå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®¡ç®—åœ¨ä¸€ç»„ä¸å¯ç”¨çŸ¥è¯†ç›¸å®¹çš„å…ˆéªŒåˆ†å¸ƒä¸­æœ€å°åŒ–æœ€åæƒ…å†µçš„ Bayes é£é™©çš„ Gamma-Minimax ä¼°è®¡å™¨ï¼Œæ–‡ä¸­è¿˜æå‡ºäº†ä¸€ç§ç¥ç»ç½‘ç»œç±»ç”¨äºæä¾›ä¼°è®¡å™¨ç±»ï¼Œä»¥åŠä¸¤ä¸ªå®éªŒç¯èŠ‚ç”¨äºè¯´æ˜è¯¥æ–¹æ³•çš„åº”ç”¨ã€‚ |
| [^22] | [Stable and consistent density-based clustering via multiparameter persistence.](http://arxiv.org/abs/2005.09048) | è¿™ç¯‡è®ºæ–‡é€šè¿‡å¼•å…¥ä¸€ç§åº¦é‡å±‚æ¬¡èšç±»çš„å¯¹åº”äº¤é”™è·ç¦»ï¼Œç ”ç©¶äº†ä¸€ç§ç¨³å®šä¸€è‡´çš„å¯†åº¦-basedèšç±»ç®—æ³•ï¼Œæä¾›äº†ä¸€ä¸ªä»ä¸€å‚æ•°å±‚æ¬¡èšç±»ä¸­æå–å•ä¸ªèšç±»çš„ç®—æ³•ï¼Œå¹¶è¯æ˜äº†è¯¥ç®—æ³•çš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§ã€‚ |
| [^23] | [RAB: Provable Robustness Against Backdoor Attacks.](http://arxiv.org/abs/2003.08904) | æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯å®æœºå™¨å­¦ä¹ æ¨¡å‹é²æ£’æ€§çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡éšæœºå¹³æ»‘æŠ€æœ¯å®ç°å¯¹è§„é¿å’Œåé—¨æ”»å‡»çš„é²æ£’æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†é²æ£’è®­ç»ƒè¿‡ç¨‹RABï¼Œå¹¶è¯æ˜å…¶æœ‰æ•ˆæ€§å’Œç´§å¯†æ€§ã€‚åœ¨ç†è®ºä¸Šè¯æ˜äº†å¯¹åé—¨æ”»å‡»è¿›è¡Œé²æ£’æ€§ä¿æŠ¤çš„å¯è¡Œæ€§ã€‚ |

# è¯¦ç»†

[^1]: ç»Ÿè®¡ä¼°è®¡ä¸­çš„åˆ†å¸ƒåç§»: Wassersteinæ‰°åŠ¨ä¸æå°æå¤§ç†è®º

    Statistical Estimation Under Distribution Shift: Wasserstein Perturbations and Minimax Theory. (arXiv:2308.01853v1 [stat.ML])

    [http://arxiv.org/abs/2308.01853](http://arxiv.org/abs/2308.01853)

    è¿™ç¯‡è®ºæ–‡ç ”ç©¶äº†ç»Ÿè®¡ä¼°è®¡ä¸­çš„åˆ†å¸ƒåç§»é—®é¢˜ï¼Œä¸»è¦å…³æ³¨Wassersteinåˆ†å¸ƒåç§»ï¼Œæå‡ºäº†è”åˆåˆ†å¸ƒåç§»æ¦‚å¿µï¼Œå¹¶åˆ†æäº†å‡ ä¸ªç»Ÿè®¡é—®é¢˜çš„è§£å†³æ–¹æ³•ã€‚è®ºæ–‡å‘ç°äº†æœ€ä¼˜çš„æå°æå¤§é£é™©å’Œæœ€ä¸åˆ©çš„æ‰°åŠ¨ï¼Œå¹¶è¯æ˜äº†æ ·æœ¬å‡å€¼å’Œæœ€å°äºŒä¹˜ä¼°è®¡é‡çš„ä¼˜è¶Šæ€§ã€‚

    

    åˆ†å¸ƒåç§»æ˜¯ç°ä»£ç»Ÿè®¡å­¦ä¹ ä¸­çš„ä¸€ä¸ªä¸¥é‡é—®é¢˜ï¼Œå› ä¸ºå®ƒä»¬å¯ä»¥å°†æ•°æ®çš„ç‰¹æ€§ä»çœŸå®æƒ…å†µä¸­ç³»ç»Ÿåœ°æ”¹å˜ã€‚æˆ‘ä»¬ä¸“æ³¨äºWassersteinåˆ†å¸ƒåç§»ï¼Œå…¶ä¸­æ¯ä¸ªæ•°æ®ç‚¹å¯èƒ½ä¼šå‘ç”Ÿè½»å¾®æ‰°åŠ¨ï¼Œè€Œä¸æ˜¯Huberæ±¡æŸ“æ¨¡å‹ï¼Œå…¶ä¸­ä¸€éƒ¨åˆ†è§‚æµ‹å€¼æ˜¯å¼‚å¸¸å€¼ã€‚æˆ‘ä»¬æå‡ºå¹¶ç ”ç©¶äº†è¶…å‡ºç‹¬ç«‹æ‰°åŠ¨çš„åç§»ï¼Œæ¢ç´¢äº†è”åˆåˆ†å¸ƒåç§»ï¼Œå…¶ä¸­æ¯ä¸ªè§‚æµ‹ç‚¹çš„æ‰°åŠ¨å¯ä»¥åè°ƒè¿›è¡Œã€‚æˆ‘ä»¬åˆ†æäº†å‡ ä¸ªé‡è¦çš„ç»Ÿè®¡é—®é¢˜ï¼ŒåŒ…æ‹¬ä½ç½®ä¼°è®¡ã€çº¿æ€§å›å½’å’Œéå‚æ•°å¯†åº¦ä¼°è®¡ã€‚åœ¨å‡å€¼ä¼°è®¡å’Œçº¿æ€§å›å½’çš„é¢„æµ‹è¯¯å·®æ–¹å·®ä¸‹ï¼Œæˆ‘ä»¬æ‰¾åˆ°äº†ç²¾ç¡®çš„æå°æå¤§é£é™©ã€æœ€ä¸åˆ©çš„æ‰°åŠ¨ï¼Œå¹¶è¯æ˜äº†æ ·æœ¬å‡å€¼å’Œæœ€å°äºŒä¹˜ä¼°è®¡é‡åˆ†åˆ«æ˜¯æœ€ä¼˜çš„ã€‚è¿™é€‚ç”¨äºç‹¬ç«‹å’Œè”åˆåç§»ï¼Œä½†æœ€ä¸åˆ©çš„æ‰°åŠ¨å’Œæå°æå¤§é£é™©æ˜¯ä¸åŒçš„ã€‚

    Distribution shifts are a serious concern in modern statistical learning as they can systematically change the properties of the data away from the truth. We focus on Wasserstein distribution shifts, where every data point may undergo a slight perturbation, as opposed to the Huber contamination model where a fraction of observations are outliers. We formulate and study shifts beyond independent perturbations, exploring Joint Distribution Shifts, where the per-observation perturbations can be coordinated. We analyze several important statistical problems, including location estimation, linear regression, and non-parametric density estimation. Under a squared loss for mean estimation and prediction error in linear regression, we find the exact minimax risk, a least favorable perturbation, and show that the sample mean and least squares estimators are respectively optimal. This holds for both independent and joint shifts, but the least favorable perturbations and minimax risks differ. For
    
[^2]: æ‚¨çš„æ•°æ®å¯å¯¹é½å—ï¼ŸåŸºäºåŸåˆ™å’Œå¯è§£é‡Šæ€§çš„å•ç»†èƒæ•°æ®å¯¹é½æ€§æµ‹è¯•å’Œæ•´åˆ

    Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data. (arXiv:2308.01839v1 [q-bio.QM])

    [http://arxiv.org/abs/2308.01839](http://arxiv.org/abs/2308.01839)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§è°±æµå½¢å¯¹é½å’Œæ¨æ–­ï¼ˆSMAIï¼‰æ¡†æ¶ï¼Œé€šè¿‡æä¾›ä¸€ç§ç»Ÿè®¡æ£€éªŒæ–¹æ³•æ¥ç¡®å®šå•ç»†èƒæ•°æ®é›†ä¹‹é—´çš„å¯¹é½æ€§ï¼Œé¿å…è¯¯å¯¼æ€§æ¨æ–­ï¼Œå¹¶ä¿æŒæ•°æ®æ•´åˆçš„ç»“æ„å’Œå¯è§£é‡Šæ€§ã€‚

    

    å•ç»†èƒæ•°æ®æ•´åˆå¯ä»¥æä¾›ç»†èƒçš„å…¨é¢åˆ†å­è§†å›¾ï¼Œå¹¶ä¸”å·²ç»å¼€å‘å‡ºè®¸å¤šç®—æ³•æ¥æ¶ˆé™¤ä¸éœ€è¦çš„æŠ€æœ¯æˆ–ç”Ÿç‰©å˜å¼‚ï¼Œå¹¶æ•´åˆå¼‚è´¨çš„å•ç»†èƒæ•°æ®é›†ã€‚å°½ç®¡è¿™äº›æ–¹æ³•è¢«å¹¿æ³›ä½¿ç”¨ï¼Œä½†ç°æœ‰æ–¹æ³•å­˜åœ¨ä¸€äº›åŸºæœ¬é™åˆ¶ã€‚ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬ç¼ºä¹ä¸€ç§ä¸¥è°¨çš„ç»Ÿè®¡æ£€éªŒæ–¹æ³•ï¼Œç”¨äºåˆ¤æ–­ä¸¤ä¸ªé«˜ç»´å•ç»†èƒæ•°æ®é›†æ˜¯å¦å¯ä»¥å¯¹é½ï¼ˆå› æ­¤æ˜¯å¦åº”è¯¥è¿›è¡Œå¯¹é½ï¼‰ã€‚æ­¤å¤–ï¼Œæµè¡Œçš„æ–¹æ³•åœ¨å¯¹é½è¿‡ç¨‹ä¸­å¯èƒ½ä¼šæ˜æ˜¾ç•¸å˜æ•°æ®ï¼Œä½¿å¾—å¯¹é½åçš„æ•°æ®å’Œä¸‹æ¸¸åˆ†æéš¾ä»¥è§£é‡Šã€‚ä¸ºäº†å…‹æœè¿™äº›é™åˆ¶ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºè°±æµå½¢å¯¹é½å’Œæ¨æ–­ï¼ˆSMAIï¼‰æ¡†æ¶ï¼Œè¯¥æ¡†æ¶å¯ä»¥å®ç°åŸºäºåŸåˆ™å’Œå¯è§£é‡Šçš„å•ç»†èƒæ•°æ®å¯¹é½æ€§æµ‹è¯•å’Œä¿æŒç»“æ„çš„æ•´åˆã€‚SMAIæä¾›äº†ä¸€ç§ç»Ÿè®¡æ£€éªŒæ–¹æ³•ï¼Œèƒ½å¤Ÿç¨³å¥åœ°ç¡®å®šæ•°æ®é›†ä¹‹é—´çš„å¯¹é½æ€§ï¼Œä»¥é¿å…è¯¯å¯¼æ€§æ¨æ–­ï¼ŒåŒæ—¶å…¶æ–¹æ³•ä¹Ÿç»è¿‡äº†é«˜ç»´æ•°æ®çš„æ­£å½“æ€§è¯æ˜ã€‚

    Single-cell data integration can provide a comprehensive molecular view of cells, and many algorithms have been developed to remove unwanted technical or biological variations and integrate heterogeneous single-cell datasets. Despite their wide usage, existing methods suffer from several fundamental limitations. In particular, we lack a rigorous statistical test for whether two high-dimensional single-cell datasets are alignable (and therefore should even be aligned). Moreover, popular methods can substantially distort the data during alignment, making the aligned data and downstream analysis difficult to interpret. To overcome these limitations, we present a spectral manifold alignment and inference (SMAI) framework, which enables principled and interpretable alignability testing and structure-preserving integration of single-cell data. SMAI provides a statistical test to robustly determine the alignability between datasets to avoid misleading inference, and is justified by high-dimen
    
[^3]: åˆ†å¸ƒæ— å…³æ¨æ–­äºŒå…ƒåˆ†ç±»çš„å›å½’å‡½æ•°

    Distribution-Free Inference for the Regression Function of Binary Classification. (arXiv:2308.01835v1 [stat.ML])

    [http://arxiv.org/abs/2308.01835](http://arxiv.org/abs/2308.01835)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åˆ†å¸ƒæ— å…³çš„æ–¹æ³•æ¥æ¨æ–­äºŒå…ƒåˆ†ç±»é—®é¢˜ä¸­çš„å›å½’å‡½æ•°ï¼Œé€šè¿‡æ„å»ºç½®ä¿¡åŒºé—´æ¥è§£å†³è¯¥é—®é¢˜ï¼Œç›¸å…³ç®—æ³•ç»è¿‡éªŒè¯å…·æœ‰å¯é æ€§ã€‚

    

    äºŒå…ƒåˆ†ç±»çš„ä¸€ä¸ªå…³é”®å¯¹è±¡æ˜¯å›å½’å‡½æ•°ï¼Œå³ç»™å®šè¾“å…¥çš„ç±»åˆ«æ ‡ç­¾çš„æ¡ä»¶æœŸæœ›ã€‚é€šè¿‡å›å½’å‡½æ•°ï¼Œä¸ä»…å¯ä»¥å®šä¹‰è´å¶æ–¯æœ€ä¼˜åˆ†ç±»å™¨ï¼Œè¿˜å¯ä»¥ç¼–ç å¯¹åº”çš„é”™è¯¯åˆ†ç±»æ¦‚ç‡ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§é‡é‡‡æ ·æ¡†æ¶ï¼Œç”¨äºæ„å»ºç²¾ç¡®ã€åˆ†å¸ƒæ— å…³ä¸”éæ¸è¿‘ä¿è¯çš„çœŸå®å›å½’å‡½æ•°çš„ç½®ä¿¡åŒºé—´ï¼Œæ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ç½®ä¿¡æ°´å¹³ã€‚ç„¶åï¼Œæå‡ºäº†ç‰¹å®šçš„ç®—æ³•æ¥æ¼”ç¤ºè¯¥æ¡†æ¶ã€‚è¯æ˜äº†æ„å»ºçš„ç½®ä¿¡åŒºé—´æ˜¯å¼ºä¸€è‡´çš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œä»»ä½•é”™è¯¯çš„æ¨¡å‹æœ€ç»ˆè¢«æ’é™¤çš„æ¦‚ç‡ä¸º1ã€‚æ’é™¤çš„ç¨‹åº¦ä¹Ÿé€šè¿‡å¯èƒ½è¿‘ä¼¼æ­£ç¡®ç±»å‹çš„ç•Œé™è¿›è¡Œäº†é‡åŒ–ã€‚æœ€åï¼Œé€šè¿‡æ•°å€¼å®éªŒéªŒè¯äº†ç®—æ³•ï¼Œå¹¶å°†æ–¹æ³•ä¸è¿‘ä¼¼æ¸è¿‘ç½®ä¿¡æ¤­åœ†è¿›è¡Œäº†æ¯”è¾ƒã€‚

    One of the key objects of binary classification is the regression function, i.e., the conditional expectation of the class labels given the inputs. With the regression function not only a Bayes optimal classifier can be defined, but it also encodes the corresponding misclassification probabilities. The paper presents a resampling framework to construct exact, distribution-free and non-asymptotically guaranteed confidence regions for the true regression function for any user-chosen confidence level. Then, specific algorithms are suggested to demonstrate the framework. It is proved that the constructed confidence regions are strongly consistent, that is, any false model is excluded in the long run with probability one. The exclusion is quantified with probably approximately correct type bounds, as well. Finally, the algorithms are validated via numerical experiments, and the methods are compared to approximate asymptotic confidence ellipsoids.
    
[^4]: åŸºäºè”åˆç²¾ç®—ç¥ç»ç½‘ç»œçš„æ¨ªæ–­é¢å’Œçºµå‘ç´¢èµ”è®¡æ•°æ•°æ®çš„è½¦è½½é€šä¿¡æŠ€æœ¯

    Telematics Combined Actuarial Neural Networks for Cross-Sectional and Longitudinal Claim Count Data. (arXiv:2308.01729v1 [stat.ML])

    [http://arxiv.org/abs/2308.01729](http://arxiv.org/abs/2308.01729)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ç§åŸºäºè”åˆç²¾ç®—ç¥ç»ç½‘ç»œæ¡†æ¶çš„æ¨ªæ–­é¢å’Œçºµå‘ç´¢èµ”è®¡æ•°æ¨¡å‹ï¼Œé€šè¿‡ç»“åˆä¼ ç»Ÿç²¾ç®—æ¨¡å‹å’Œç¥ç»ç½‘ç»œï¼Œå……åˆ†åˆ©ç”¨äº†ä¸¤ä¸ªæ¨¡å‹çš„ä¼˜åŠ¿ã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºMario W\"uthrichå’ŒMichael Merzæå‡ºçš„è”åˆç²¾ç®—ç¥ç»ç½‘ç»œï¼ˆCANNï¼‰æ¡†æ¶çš„æ¨ªæ–­é¢å’Œçºµå‘ç´¢èµ”è®¡æ•°æ¨¡å‹ã€‚CANNæ–¹æ³•å°†ä¼ ç»Ÿçš„ç²¾ç®—æ¨¡å‹ï¼ˆå¦‚å¹¿ä¹‰çº¿æ€§æ¨¡å‹ï¼‰ä¸ç¥ç»ç½‘ç»œç›¸ç»“åˆï¼Œå½¢æˆäº†ä¸€ä¸ªåŒ…å«ç»å…¸å›å½’æ¨¡å‹å’Œç¥ç»ç½‘ç»œéƒ¨åˆ†çš„åŒç»„ä»¶æ¨¡å‹ã€‚CANNæ¨¡å‹å……åˆ†åˆ©ç”¨äº†ä¸¤ä¸ªæ¨¡å‹çš„ä¼˜åŠ¿ï¼Œæ—¢å¯ä»¥æä¾›ç»å…¸æ¨¡å‹çš„å¯é æ€§å’Œå¯è§£é‡Šæ€§ï¼Œåˆå¯ä»¥åˆ©ç”¨ç¥ç»ç½‘ç»œçš„çµæ´»æ€§å’Œå¯¹å¤æ‚å…³ç³»å’Œäº¤äº’ä½œç”¨çš„æ•æ‰èƒ½åŠ›ã€‚åœ¨æˆ‘ä»¬æå‡ºçš„æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¹¿ä¸ºäººçŸ¥çš„å¯¹æ•°çº¿æ€§ç´¢èµ”è®¡æ•°å›å½’æ¨¡å‹ä½œä¸ºç»å…¸å›å½’éƒ¨åˆ†ï¼Œä½¿ç”¨äº†å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä½œä¸ºç¥ç»ç½‘ç»œéƒ¨åˆ†ã€‚MLPéƒ¨åˆ†ç”¨äºå¤„ç†ä»¥å‘é‡å½¢å¼è¡¨ç¤ºçš„è½¦è¾†é©¾é©¶è¡Œä¸ºçš„è½¦è½½é€šä¿¡æ•°æ®ã€‚

    We present novel cross-sectional and longitudinal claim count models for vehicle insurance built upon the Combined Actuarial Neural Network (CANN) framework proposed by Mario W\"uthrich and Michael Merz. The CANN approach combines a classical actuarial model, such as a generalized linear model, with a neural network. This blending of models results in a two-component model comprising a classical regression model and a neural network part. The CANN model leverages the strengths of both components, providing a solid foundation and interpretability from the classical model while harnessing the flexibility and capacity to capture intricate relationships and interactions offered by the neural network. In our proposed models, we use well-known log-linear claim count regression models for the classical regression part and a multilayer perceptron (MLP) for the neural network part. The MLP part is used to process telematics car driving data given as a vector characterizing the driving behavior 
    
[^5]: åˆ©ç”¨å¼ é‡æ ¸èŒƒæ•°å’Œä¸¥æ ¼äº’è¡¥æ€§è¿›è¡Œä½ç§©å¼ é‡æ¢å¤çš„ä¸€é˜¶æ–¹æ³•çš„æ•ˆç‡

    Efficiency of First-Order Methods for Low-Rank Tensor Recovery with the Tensor Nuclear Norm Under Strict Complementarity. (arXiv:2308.01677v1 [math.OC])

    [http://arxiv.org/abs/2308.01677](http://arxiv.org/abs/2308.01677)

    æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºå¼ é‡æ ¸èŒƒæ•°çš„çº¦æŸæœ€å°åŒ–æ–¹æ³•åœ¨ä½ç§©å¼ é‡æ¢å¤ä¸­çš„æœ‰æ•ˆæ€§ï¼Œæå‡ºäº†é€‚å½“çš„ä¸¥æ ¼äº’è¡¥æ€§æ¡ä»¶ï¼Œå¹¶ä¸”å¾—åˆ°äº†åœ¨æ­¤æ¡ä»¶ä¸‹çš„ä¸»è¦ç»“æœï¼š1.å¯¹äºç‰¹å®šå½¢å¼çš„ç›®æ ‡å‡½æ•°ï¼Œæ ‡å‡†æŠ•å½±æ¢¯åº¦æ–¹æ³•å…·æœ‰çº¿æ€§æ”¶æ•›é€Ÿåº¦ï¼Œå°½ç®¡ç›®æ ‡å‡½æ•°ä¸ä¸€å®šæ˜¯å¼ºå‡¸çš„ã€‚2.å¯¹äºå…‰æ»‘çš„ç›®æ ‡å‡½æ•°ï¼Œæ ‡å‡†æ¢¯åº¦æ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦å’Œæ¯æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´å¯èƒ½æ˜¾è‘—æé«˜ã€‚

    

    æˆ‘ä»¬è€ƒè™‘åŸºäºå¼ é‡æ ¸èŒƒæ•°è¯±å¯¼çš„çƒä¸Šçš„çº¦æŸæœ€å°åŒ–çš„å‡¸æ¾å¼›æ–¹æ³•ï¼Œç”¨äºä½ç§©å¼ é‡æ¢å¤ã€‚æˆ‘ä»¬å€Ÿé‰´äº†æœ€è¿‘çš„ä¸€ç³»åˆ—ç»“æœï¼Œè¿™äº›ç»“æœè€ƒè™‘äº†ç”¨äºæ¢å¤ä½ç§©çŸ©é˜µçš„å‡¸æ¾å¼›æ–¹æ³•ï¼Œå¹¶ä¸”å·²ç»å»ºç«‹äº†åœ¨ä¸¥æ ¼äº’è¡¥æ€§æ¡ä»¶ä¸‹ï¼Œæ ‡å‡†æ¢¯åº¦æ–¹æ³•çš„æ”¶æ•›é€Ÿåº¦å’Œæ¯æ¬¡è¿­ä»£çš„è¿è¡Œæ—¶é—´å¯èƒ½æ˜¾è‘—æé«˜ã€‚æˆ‘ä»¬é’ˆå¯¹å¼ é‡æ ¸èŒƒæ•°çƒä½“å¼€å‘äº†é€‚å½“çš„ä¸¥æ ¼äº’è¡¥æ€§æ¡ä»¶ï¼Œå¹¶è·å¾—äº†ä»¥ä¸‹ä¸»è¦ç»“æœï¼š1. å½“è¦æœ€å°åŒ–çš„ç›®æ ‡å…·æœ‰å½¢å¼$f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$ï¼Œå…¶ä¸­$g$æ˜¯å¼ºå‡¸å‡½æ•°ï¼Œ$\mA$æ˜¯ä¸€ä¸ªçº¿æ€§æ˜ å°„ï¼ˆä¾‹å¦‚æœ€å°äºŒä¹˜æ³•ï¼‰ï¼Œå­˜åœ¨äºŒæ¬¡å¢é•¿ç•Œï¼Œè¿™æ„å‘³ç€æ ‡å‡†æŠ•å½±æ¢¯åº¦æ–¹æ³•å…·æœ‰çº¿æ€§æ”¶æ•›é€Ÿåº¦ï¼Œå°½ç®¡$f$ä¸ä¸€å®šæ˜¯å¼ºå‡¸çš„ã€‚2.å¯¹äºå…‰æ»‘çš„ç›®æ ‡å‡½æ•°ï¼Œ

    We consider convex relaxations for recovering low-rank tensors based on constrained minimization over a ball induced by the tensor nuclear norm, recently introduced in \cite{tensor_tSVD}. We build on a recent line of results that considered convex relaxations for the recovery of low-rank matrices and established that under a strict complementarity condition (SC), both the convergence rate and per-iteration runtime of standard gradient methods may improve dramatically. We develop the appropriate strict complementarity condition for the tensor nuclear norm ball and obtain the following main results under this condition: 1. When the objective to minimize is of the form $f(\mX)=g(\mA\mX)+\langle{\mC,\mX}\rangle$ , where $g$ is strongly convex and $\mA$ is a linear map (e.g., least squares), a quadratic growth bound holds, which implies linear convergence rates for standard projected gradient methods, despite the fact that $f$ need not be strongly convex. 2. For a smooth objective function,
    
[^6]: ç”¨äºå†³ç­–çš„å› æœæ€ç»´åœ¨ç”µå­å¥åº·è®°å½•ä¸­çš„åº”ç”¨ï¼šä¸ºä»€ä¹ˆä»¥åŠå¦‚ä½•

    Causal thinking for decision making on Electronic Health Records: why and how. (arXiv:2308.01605v1 [stat.ME])

    [http://arxiv.org/abs/2308.01605](http://arxiv.org/abs/2308.01605)

    æœ¬æ–‡ä»‹ç»äº†åœ¨ç”µå­å¥åº·è®°å½•ä¸­ä½¿ç”¨å› æœæ€ç»´è¿›è¡Œå†³ç­–çš„å¿…è¦æ€§å’Œæ–¹æ³•ã€‚é€šè¿‡æ¨¡æ‹Ÿéšæœºè¯•éªŒæ¥ä¸ªæ€§åŒ–å†³ç­–ï¼Œä»¥å‡å°‘æ•°æ®ä¸­çš„åè§ã€‚è¿™å¯¹äºåˆ†æç”µå­å¥åº·è®°å½•æˆ–ç´¢èµ”æ•°æ®ä»¥å¾—å‡ºå› æœç»“è®ºçš„æœ€é‡è¦é™·é˜±å’Œè€ƒè™‘å› ç´ è¿›è¡Œäº†é‡ç‚¹å¼ºè°ƒã€‚

    

    å‡†ç¡®çš„é¢„æµ‹ï¼Œå¦‚åŒæœºå™¨å­¦ä¹ ä¸€æ ·ï¼Œå¯èƒ½æ— æ³•ä¸ºæ¯ä¸ªæ‚£è€…æä¾›æœ€ä½³åŒ»ç–—ä¿å¥ã€‚ç¡®å®ï¼Œé¢„æµ‹å¯èƒ½å—åˆ°æ•°æ®ä¸­çš„æ·å¾„ï¼ˆå¦‚ç§æ—åè§ï¼‰çš„é©±åŠ¨ã€‚ä¸ºæ•°æ®é©±åŠ¨çš„å†³ç­–éœ€è¦å› æœæ€ç»´ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä»‹ç»å…³é”®è¦ç´ ï¼Œé‡ç‚¹å…³æ³¨å¸¸è§„æ”¶é›†çš„æ•°æ®ï¼Œå³ç”µå­å¥åº·è®°å½•ï¼ˆEHRsï¼‰å’Œç´¢èµ”æ•°æ®ã€‚ä½¿ç”¨è¿™äº›æ•°æ®è¯„ä¼°å¹²é¢„çš„ä»·å€¼éœ€è¦è°¨æ…ï¼šæ—¶é—´ä¾èµ–æ€§å’Œç°æœ‰å®è·µå¾ˆå®¹æ˜“æ··æ·†å› æœæ•ˆåº”ã€‚æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªé€æ­¥æ¡†æ¶ï¼Œå¸®åŠ©ä»çœŸå®æ‚£è€…è®°å½•ä¸­æ„å»ºæœ‰æ•ˆçš„å†³ç­–ï¼Œé€šè¿‡æ¨¡æ‹Ÿéšæœºè¯•éªŒæ¥ä¸ªæ€§åŒ–å†³ç­–ï¼Œä¾‹å¦‚ä½¿ç”¨æœºå™¨å­¦ä¹ ã€‚æˆ‘ä»¬çš„æ¡†æ¶å¼ºè°ƒäº†åˆ†æEHRsæˆ–ç´¢èµ”æ•°æ®ä»¥å¾—å‡ºå› æœç»“è®ºæ—¶æœ€é‡è¦çš„é™·é˜±å’Œè€ƒè™‘å› ç´ ã€‚æˆ‘ä»¬åœ¨ç”¨äºé‡ç—‡åŒ»å­¦ä¿¡æ¯å¸‚åœºä¸­çš„è‚Œé…å¯¹è´¥è¡€ç—‡æ­»äº¡ç‡çš„å½±å“çš„ç ”ç©¶ä¸­è¯´æ˜äº†å„ç§é€‰æ‹©ã€‚

    Accurate predictions, as with machine learning, may not suffice to provide optimal healthcare for every patient. Indeed, prediction can be driven by shortcuts in the data, such as racial biases. Causal thinking is needed for data-driven decisions. Here, we give an introduction to the key elements, focusing on routinely-collected data, electronic health records (EHRs) and claims data. Using such data to assess the value of an intervention requires care: temporal dependencies and existing practices easily confound the causal effect. We present a step-by-step framework to help build valid decision making from real-life patient records by emulating a randomized trial before individualizing decisions, eg with machine learning. Our framework highlights the most important pitfalls and considerations in analysing EHRs or claims data to draw causal conclusions. We illustrate the various choices in studying the effect of albumin on sepsis mortality in the Medical Information Mart for Intensive C
    
[^7]: å¿«é€ŸSlateç­–ç•¥ä¼˜åŒ–ï¼šè¶…è¶ŠPlackett-Luce

    Fast Slate Policy Optimization: Going Beyond Plackett-Luce. (arXiv:2308.01566v1 [cs.LG])

    [http://arxiv.org/abs/2308.01566](http://arxiv.org/abs/2308.01566)

    æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¿«é€ŸSlateç­–ç•¥ä¼˜åŒ–æ–¹æ³•ï¼Œé€šè¿‡æå‡ºä¸€ç§æ–°çš„ç­–ç•¥ç±»ï¼Œå¯ä»¥åœ¨å¤§è§„æ¨¡å†³ç­–ç³»ç»Ÿä¸­æœ‰æ•ˆåœ°ä¼˜åŒ–ä»»æ„å¥–åŠ±å‡½æ•°ï¼Œç»“æœè¡¨æ˜è¯¥æ–¹æ³•åœ¨ç™¾ä¸‡çº§åˆ«åŠ¨ä½œç©ºé—´é—®é¢˜ä¸Šå…·æœ‰å¾ˆå¥½çš„æ•ˆæœã€‚

    

    å¤§è§„æ¨¡æœºå™¨å­¦ä¹ ç³»ç»Ÿä¸­ä¸€ä¸ªè¶Šæ¥è¶Šé‡è¦çš„æ„å»ºæ¨¡å—æ˜¯è¿”å›Slateï¼Œå³ç»™å®šä¸€ä¸ªæŸ¥è¯¢è¿”å›æœ‰åºçš„é¡¹ç›®åˆ—è¡¨ã€‚è¯¥æŠ€æœ¯çš„åº”ç”¨åŒ…æ‹¬æœç´¢ã€ä¿¡æ¯æ£€ç´¢å’Œæ¨èç³»ç»Ÿã€‚å½“è¡ŒåŠ¨ç©ºé—´å¾ˆå¤§æ—¶ï¼Œå†³ç­–ç³»ç»Ÿä¼šé™åˆ¶åœ¨ç‰¹å®šç»“æ„ä¸­ä»¥å¿«é€Ÿå®Œæˆåœ¨çº¿æŸ¥è¯¢ã€‚æœ¬æ–‡è§£å†³äº†è¿™äº›å¤§è§„æ¨¡å†³ç­–ç³»ç»Ÿåœ¨ç»™å®šä»»æ„å¥–åŠ±å‡½æ•°ä¸‹çš„ä¼˜åŒ–é—®é¢˜ã€‚æˆ‘ä»¬å°†è¿™ä¸ªå­¦ä¹ é—®é¢˜è½¬åŒ–ä¸ºç­–ç•¥ä¼˜åŒ–æ¡†æ¶ï¼Œå¹¶æå‡ºäº†ä¸€ç§æ–°çš„ç­–ç•¥ç±»ï¼Œå®ƒæºäºå†³ç­–å‡½æ•°çš„ä¸€ç§æ–°é¢–æ”¾æ¾ã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªç®€å•è€Œé«˜æ•ˆçš„å­¦ä¹ ç®—æ³•ï¼Œå¯ä»¥æ‰©å±•åˆ°å¤§è§„æ¨¡çš„åŠ¨ä½œç©ºé—´ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•ä¸å¸¸ç”¨çš„Plackett-Luceç­–ç•¥ç±»è¿›è¡Œæ¯”è¾ƒï¼Œå¹¶å±•ç¤ºäº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨åŠ¨ä½œç©ºé—´å¤§å°è¾¾åˆ°ç™¾ä¸‡çº§åˆ«çš„é—®é¢˜ä¸Šçš„æœ‰æ•ˆæ€§ã€‚

    An increasingly important building block of large scale machine learning systems is based on returning slates; an ordered lists of items given a query. Applications of this technology include: search, information retrieval and recommender systems. When the action space is large, decision systems are restricted to a particular structure to complete online queries quickly. This paper addresses the optimization of these large scale decision systems given an arbitrary reward function. We cast this learning problem in a policy optimization framework and propose a new class of policies, born from a novel relaxation of decision functions. This results in a simple, yet efficient learning algorithm that scales to massive action spaces. We compare our method to the commonly adopted Plackett-Luce policy class and demonstrate the effectiveness of our approach on problems with action space sizes in the order of millions.
    
[^8]: éå¹³è¡¡ç‰©ç†å­¦ï¼šä»è‡ªæ—‹ç»ç’ƒåˆ°æœºå™¨å’Œç¥ç»å­¦ä¹ 

    Non-equilibrium physics: from spin glasses to machine and neural learning. (arXiv:2308.01538v1 [cond-mat.dis-nn])

    [http://arxiv.org/abs/2308.01538](http://arxiv.org/abs/2308.01538)

    æœ¬è®ºæ–‡ç ”ç©¶äº†æ— åºç³»ç»Ÿä¸­çš„æ–°å…´æ™ºèƒ½è¡Œä¸ºï¼Œå¹¶é€šè¿‡ç»Ÿè®¡ç‰©ç†å­¦æ¢ç´¢å­¦ä¹ æœºåˆ¶å’Œç‰©ç†åŠ¨åŠ›å­¦ä¹‹é—´çš„å…³ç³»ï¼Œä»¥æ­¤ä¸ºæŒ‡å¯¼åŸåˆ™è®¾è®¡æ™ºèƒ½ç³»ç»Ÿã€‚

    

    æ— åºå¤šä½“ç³»ç»Ÿåœ¨ä¸åŒå°ºåº¦ä¸Šè¡¨ç°å‡ºäº†å„ç§å„æ ·çš„æ–°å…´ç°è±¡ã€‚è¿™äº›å¤æ‚è¡Œä¸ºå¯ä»¥ç”¨äºé”™è¯¯ä¿®æ­£ã€å­¦ä¹ å’Œä¼˜åŒ–ç­‰å„ç§ä¿¡æ¯å¤„ç†ä»»åŠ¡ã€‚å°½ç®¡åˆ©ç”¨è¿™äº›ç³»ç»Ÿè¿›è¡Œæ™ºèƒ½ä»»åŠ¡çš„ç»éªŒæˆæœæ˜¾è‘—ï¼Œä½†å…¶å‡ºç°çš„æ™ºèƒ½è¡Œä¸ºçš„åŸºæœ¬åŸåˆ™ä»ç„¶å¤§éƒ¨åˆ†æœªçŸ¥ã€‚åœ¨æœ¬è®ºæ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨é€šè¿‡ç»Ÿè®¡ç‰©ç†å­¦æ¥è¡¨å¾æ— åºç³»ç»Ÿä¸­çš„è¿™ç§æ–°å…´æ™ºèƒ½ã€‚æˆ‘ä»¬æ ¹æ®å­¦ä¹ æœºåˆ¶ï¼ˆé•¿æœŸè®°å¿† vs. å·¥ä½œè®°å¿†ï¼‰å’Œå­¦ä¹ åŠ¨åŠ›å­¦ï¼ˆäººå·¥ vs. è‡ªç„¶ï¼‰è¿™ä¸¤ä¸ªæ–¹é¢åˆ¶å®šäº†æˆ‘ä»¬åœ¨è®ºæ–‡ä¸­çš„åŠªåŠ›çš„è·¯çº¿å›¾ã€‚åœ¨æˆ‘ä»¬çš„ç ”ç©¶è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ­ç¤ºäº†å­¦ä¹ æœºåˆ¶å’Œç‰©ç†åŠ¨åŠ›å­¦ä¹‹é—´çš„å…³ç³»ï¼Œè¿™äº›å…³ç³»å¯ä»¥ä½œä¸ºè®¾è®¡æ™ºèƒ½ç³»ç»Ÿçš„æŒ‡å¯¼åŸåˆ™ã€‚æˆ‘ä»¬å¸Œæœ›é€šè¿‡å¯¹çœ‹ä¼¼ä¸ç›¸å…³çš„å­¦ä¹ ç³»ç»Ÿçš„æ–°å…´æ™ºèƒ½çš„ç ”ç©¶ï¼Œèƒ½å¤Ÿæ‰©å±•æˆ‘ä»¬å½“å‰çš„è®¤è¯†ã€‚

    Disordered many-body systems exhibit a wide range of emergent phenomena across different scales. These complex behaviors can be utilized for various information processing tasks such as error correction, learning, and optimization. Despite the empirical success of utilizing these systems for intelligent tasks, the underlying principles that govern their emergent intelligent behaviors remain largely unknown. In this thesis, we aim to characterize such emergent intelligence in disordered systems through statistical physics. We chart a roadmap for our efforts in this thesis based on two axes: learning mechanisms (long-term memory vs. working memory) and learning dynamics (artificial vs. natural). Throughout our journey, we uncover relationships between learning mechanisms and physical dynamics that could serve as guiding principles for designing intelligent systems. We hope that our investigation into the emergent intelligence of seemingly disparate learning systems can expand our current
    
[^9]: å…·æœ‰æœ€è¿‘é‚»çš„æå°æå¤§æœ€ä¼˜$Q$å­¦ä¹ 

    Minimax Optimal $Q$ Learning with Nearest Neighbors. (arXiv:2308.01490v1 [cs.LG])

    [http://arxiv.org/abs/2308.01490](http://arxiv.org/abs/2308.01490)

    æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–°çš„å…·æœ‰æœ€è¿‘é‚»çš„$Q$å­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè§£å†³è¿ç»­çŠ¶æ€ç©ºé—´ä¸‹æ”¶æ•›é€Ÿåº¦å·®çš„é—®é¢˜ã€‚

    

    $Q$å­¦ä¹ æ˜¯ä¸€ç§å¸¸è§çš„æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ æ–¹æ³•ã€‚ç°æœ‰çš„å¤§éƒ¨åˆ†å·¥ä½œé›†ä¸­åœ¨åˆ†ææœ‰é™çŠ¶æ€å’ŒåŠ¨ä½œç©ºé—´çš„$Q$å­¦ä¹ ã€‚å¦‚æœçŠ¶æ€ç©ºé—´æ˜¯è¿ç»­çš„ï¼Œé‚£ä¹ˆåŸå§‹çš„$Q$å­¦ä¹ æ–¹æ³•å°±æ— æ³•ç›´æ¥ä½¿ç”¨ã€‚(Shah and Xie, 2018) æå‡ºäº†åŸå§‹$Q$å­¦ä¹ æ–¹æ³•çš„ä¿®æ”¹ç‰ˆï¼Œç”¨æœ€è¿‘é‚»æ–¹æ³•ä¼°è®¡$Q$å€¼ã€‚è¿™ç§ä¿®æ”¹ä½¿å¾—$Q$å­¦ä¹ é€‚ç”¨äºè¿ç»­çŠ¶æ€ç©ºé—´ã€‚è¯¥è®ºæ–‡æŒ‡å‡ºä¼°è®¡$Q$å‡½æ•°çš„æ”¶æ•›é€Ÿåº¦ä¸º$\tilde{O}(T^{-1/(d+3)})$ï¼Œæ¯”æå°æå¤§ä¸‹ç•Œ$\tilde{\Omega}(T^{-1/(d+2)})$æ…¢ï¼Œè¯´æ˜è¯¥æ–¹æ³•æ•ˆç‡ä¸é«˜ã€‚æœ¬æ–‡æå‡ºäº†ä¸¤ç§æ–°çš„$Q$å­¦ä¹ æ–¹æ³•ï¼Œæ¥å¼¥åˆ(Shah and Xie, 2018)ä¸­çš„æ”¶æ•›é€Ÿåº¦å·®è·ï¼Œå…¶ä¸­ä¸€ç§æ˜¯ç¦»çº¿çš„ï¼Œå¦ä¸€ç§æ˜¯åœ¨çº¿çš„ã€‚å°½ç®¡æˆ‘ä»¬ä»ç„¶ä½¿ç”¨æœ€è¿‘é‚»æ–¹æ³•æ¥ä¼°è®¡$Q$å‡½æ•°ï¼Œä½†ç®—æ³•ä¸(Shah and Xie, 2018)æœ‰æ˜¾è‘—åŒºåˆ«ã€‚

    $Q$ learning is a popular model free reinforcement learning method. Most of existing works focus on analyzing $Q$ learning for finite state and action spaces. If the state space is continuous, then the original $Q$ learning method can not be directly used. A modification of the original $Q$ learning method was proposed in (Shah and Xie, 2018), which estimates $Q$ values with nearest neighbors. Such modification makes $Q$ learning suitable for continuous state space. (Shah and Xie, 2018) shows that the convergence rate of estimated $Q$ function is $\tilde{O}(T^{-1/(d+3)})$, which is slower than the minimax lower bound $\tilde{\Omega}(T^{-1/(d+2)})$, indicating that this method is not efficient. This paper proposes two new $Q$ learning methods to bridge the gap of convergence rates in (Shah and Xie, 2018), with one of them being offline, while the other is online. Despite that we still use nearest neighbor approach to estimate $Q$ function, the algorithms are crucially different from (Sh
    
[^10]: åœ¨é©¬å°”å¯å¤«é‡‡æ ·ä¸‹ï¼Œç”¨äºéšæœºæ¢¯åº¦ä¸‹é™çš„åœ¨çº¿åæ–¹å·®ä¼°è®¡

    Online covariance estimation for stochastic gradient descent under Markovian sampling. (arXiv:2308.01481v1 [math.ST])

    [http://arxiv.org/abs/2308.01481](http://arxiv.org/abs/2308.01481)

    æœ¬æ–‡ç ”ç©¶äº†åœ¨é©¬å°”å¯å¤«é‡‡æ ·ä¸‹çš„éšæœºæ¢¯åº¦ä¸‹é™ä¸­çš„åœ¨çº¿é‡å æ‰¹æ¬¡å‡å€¼åæ–¹å·®ä¼°è®¡å™¨ï¼Œå¹¶è¯æ˜äº†å…¶æ”¶æ•›é€Ÿç‡ä¸º$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$å’Œ$O\big(\sqrt{d}\,n^{-1/8}\big)$ï¼Œåˆ†åˆ«å¯¹åº”äºçŠ¶æ€ç›¸å…³å’ŒçŠ¶æ€æ— å…³çš„é©¬å°”å¯å¤«é‡‡æ ·ã€‚è¿™äº›é€Ÿç‡ä¸ç‹¬ç«‹åŒåˆ†å¸ƒæƒ…å†µä¸‹çš„æœ€ä½³æ”¶æ•›é€Ÿç‡ç›¸åŒ¹é…ï¼Œå¹¶ä¸”å…‹æœäº†ç”±äºé©¬å°”å¯å¤«é‡‡æ ·è€Œå¼•èµ·çš„æŒ‘æˆ˜ã€‚

    

    æˆ‘ä»¬ç ”ç©¶äº†ç”¨äºé©¬å°”å¯å¤«é‡‡æ ·ä¸‹éšæœºæ¢¯åº¦ä¸‹é™çš„åœ¨çº¿é‡å æ‰¹æ¬¡å‡å€¼åæ–¹å·®ä¼°è®¡å™¨ã€‚æˆ‘ä»¬è¯æ˜äº†åæ–¹å·®ä¼°è®¡å™¨çš„æ”¶æ•›é€Ÿç‡åˆ†åˆ«ä¸º$O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$å’Œ$O\big(\sqrt{d}\,n^{-1/8}\big)$ï¼Œå…¶ä¸­$d$ä»£è¡¨ç»´åº¦ï¼Œ$n$è¡¨ç¤ºè§‚æµ‹æ•°é‡æˆ–SGDè¿­ä»£æ¬¡æ•°ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè¿™äº›é€Ÿç‡ä¸å…ˆå‰ç”±\cite{zhu2021online}åœ¨ç‹¬ç«‹åŒåˆ†å¸ƒ($\iid$)æƒ…å†µä¸‹å»ºç«‹çš„æœ€ä½³æ”¶æ•›é€Ÿç‡ç›¸åŒ¹é…ï¼Œé™¤äº†å¯¹æ•°å› å­ã€‚æˆ‘ä»¬çš„åˆ†æå…‹æœäº†ç”±äºé©¬å°”å¯å¤«é‡‡æ ·è€Œäº§ç”Ÿçš„é‡è¦æŒ‘æˆ˜ï¼Œå¼•å…¥äº†é¢å¤–çš„è¯¯å·®é¡¹å’Œæ‰¹æ¬¡å‡å€¼åæ–¹å·®ä¼°è®¡å™¨çš„å¤æ‚ä¾èµ–å…³ç³»ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜å»ºç«‹äº†SGDåŠ¨æ€è¯¯å·®$\ell_2$èŒƒæ•°çš„å‰å››é˜¶çŸ©çš„æ”¶æ•›é€Ÿç‡ã€‚

    We study the online overlapping batch-means covariance estimator for Stochastic Gradient Descent (SGD) under Markovian sampling. We show that the convergence rates of the covariance estimator are $O\big(\sqrt{d}\,n^{-1/8}(\log n)^{1/4}\big)$ and $O\big(\sqrt{d}\,n^{-1/8}\big)$ under state-dependent and state-independent Markovian sampling, respectively, with $d$ representing dimensionality and $n$ denoting the number of observations or SGD iterations. Remarkably, these rates match the best-known convergence rate previously established for the independent and identically distributed ($\iid$) case by \cite{zhu2021online}, up to logarithmic factors. Our analysis overcomes significant challenges that arise due to Markovian sampling, leading to the introduction of additional error terms and complex dependencies between the blocks of the batch-means covariance estimator. Moreover, we establish the convergence rate for the first four moments of the $\ell_2$ norm of the error of SGD dynamics u
    
[^11]: å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ ç”¨äºå‘ç°ï¼šç»Ÿè®¡æŒ‘æˆ˜å’Œæœºé‡

    Interpretable Machine Learning for Discovery: Statistical Challenges \& Opportunities. (arXiv:2308.01475v1 [stat.ML])

    [http://arxiv.org/abs/2308.01475](http://arxiv.org/abs/2308.01475)

    å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æŠ€æœ¯è¢«å¹¿æ³›ç”¨äºå¤„ç†å¤§æ•°æ®é›†ã€å¯è§†åŒ–é¢„æµ‹å’Œæ•°æ®é©±åŠ¨çš„å‘ç°ï¼Œè¯¥è®ºæ–‡å›é¡¾äº†è¿™ä¸€é¢†åŸŸå¹¶æ¢è®¨äº†éªŒè¯å‘ç°çš„æŒ‘æˆ˜ã€‚

    

    æ–°æŠ€æœ¯å¯¼è‡´äº†è®¸å¤šç§‘å­¦é¢†åŸŸå’Œè¡Œä¸šçš„åºå¤§ã€å¤æ‚çš„æ•°æ®é›†ã€‚äººä»¬ç»å¸¸ä½¿ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯æ¥å¤„ç†ã€å¯è§†åŒ–å’Œé¢„æµ‹è¿™äº›å¤§æ•°æ®ï¼Œå¹¶é€šè¿‡å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ æ¨¡å‹å’ŒæŠ€æœ¯æ¥è¿›è¡Œæ•°æ®é©±åŠ¨çš„å‘ç°ã€‚æœ¬æ–‡è®¨è®ºå’Œå›é¡¾äº†å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ é¢†åŸŸï¼Œç‰¹åˆ«å…³æ³¨è¿™äº›æŠ€æœ¯åœ¨ä»å¤§æ•°æ®é›†ä¸­ç”Ÿæˆæ–°çŸ¥è¯†æˆ–è¿›è¡Œå‘ç°æ—¶çš„åº”ç”¨ã€‚æˆ‘ä»¬æ¦‚è¿°äº†å¯è§£é‡Šçš„æœºå™¨å­¦ä¹ åœ¨ç›‘ç£å’Œæ— ç›‘ç£åœºæ™¯ä¸‹å¯ä»¥å®ç°çš„å‘ç°ç±»å‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬é‡ç‚¹è®¨è®ºäº†å¦‚ä½•ä»¥æ•°æ®é©±åŠ¨çš„æ–¹å¼éªŒè¯è¿™äº›å‘ç°ï¼Œä»¥ä¿ƒè¿›å¯¹æœºå™¨å­¦ä¹ ç³»ç»Ÿçš„ä¿¡ä»»å’Œç§‘å­¦ä¸­çš„å¯é‡å¤æ€§ã€‚æˆ‘ä»¬è®¨è®ºäº†éªŒè¯çš„æŒ‘æˆ˜ã€‚

    New technologies have led to vast troves of large and complex datasets across many scientific domains and industries. People routinely use machine learning techniques to not only process, visualize, and make predictions from this big data, but also to make data-driven discoveries. These discoveries are often made using Interpretable Machine Learning, or machine learning models and techniques that yield human understandable insights. In this paper, we discuss and review the field of interpretable machine learning, focusing especially on the techniques as they are often employed to generate new knowledge or make discoveries from large data sets. We outline the types of discoveries that can be made using Interpretable Machine Learning in both supervised and unsupervised settings. Additionally, we focus on the grand challenge of how to validate these discoveries in a data-driven manner, which promotes trust in machine learning systems and reproducibility in science. We discuss validation f
    
[^12]: å‹ç¼©å’Œåˆ†å¸ƒå¼æœ€å°äºŒä¹˜å›å½’ï¼šæ”¶æ•›é€Ÿåº¦åŠå…¶åœ¨è”é‚¦å­¦ä¹ ä¸­çš„åº”ç”¨

    Compressed and distributed least-squares regression: convergence rates with applications to Federated Learning. (arXiv:2308.01358v1 [cs.LG])

    [http://arxiv.org/abs/2308.01358](http://arxiv.org/abs/2308.01358)

    æœ¬æ–‡ç ”ç©¶äº†å‹ç¼©å¯¹åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ï¼Œé€šè¿‡æ¯”è¾ƒä¸åŒçš„æ— åå‹ç¼©æ“ä½œç¬¦çš„æ”¶æ•›é€Ÿåº¦ï¼Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚é’ˆå¯¹æœ€å°äºŒä¹˜å›å½’ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªéšæœºé€¼è¿‘ç®—æ³•ï¼Œå¹¶è€ƒè™‘äº†éšæœºåœºçš„ä¸€èˆ¬å‡è®¾å’Œå™ªå£°åæ–¹å·®çš„é™åˆ¶ï¼Œä»¥åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†åœ¨æœºå™¨å­¦ä¹ ä¸­å¹¿æ³›åº”ç”¨çš„åˆ†å¸ƒå¼å’Œè”é‚¦å­¦ä¹ ä¸­ï¼Œå‹ç¼©å¯¹éšæœºæ¢¯åº¦ç®—æ³•çš„å½±å“ã€‚æˆ‘ä»¬å¼ºè°ƒäº†å‡ ç§æ— åå‹ç¼©æ“ä½œç¬¦ä¹‹é—´çš„æ”¶æ•›é€Ÿåº¦å·®å¼‚ï¼Œè¿™äº›æ“ä½œç¬¦éƒ½æ»¡è¶³ç›¸åŒçš„æ–¹å·®æ¡ä»¶ï¼Œä»è€Œè¶…è¶Šäº†ç»å…¸çš„æœ€åæƒ…å†µåˆ†æã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ä¸“æ³¨äºæœ€å°äºŒä¹˜å›å½’ï¼ˆLSRï¼‰çš„æƒ…å†µï¼Œå¹¶åˆ†æäº†ä¸€ä¸ªä¾èµ–äºéšæœºåœºçš„æœ€å°äºŒä¹˜å›å½’çš„éšæœºé€¼è¿‘ç®—æ³•ã€‚æˆ‘ä»¬å¯¹éšæœºåœºçš„ä¸€èˆ¬æ€§å‡è®¾è¿›è¡Œäº†è¯¦ç»†åˆ†æï¼ˆç‰¹åˆ«æ˜¯æœŸæœ›çš„HÃ¶lderæ­£åˆ™æ€§ï¼‰å¹¶å¯¹å™ªå£°åæ–¹å·®è¿›è¡Œäº†é™åˆ¶ï¼Œä»¥ä¾¿åˆ†æå„ç§éšæœºåŒ–æœºåˆ¶ï¼ŒåŒ…æ‹¬å‹ç¼©ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†ç»“æœæ‰©å±•åˆ°è”é‚¦å­¦ä¹ çš„æƒ…å†µä¸‹ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬å¼ºè°ƒäº†å¯¹åŠ æ€§å™ªå£°çš„åæ–¹å·®ğ–¢ğ– ğ–­ğ–¨ğ– å¯¹æ”¶æ•›æ€§çš„å½±å“ã€‚

    In this paper, we investigate the impact of compression on stochastic gradient algorithms for machine learning, a technique widely used in distributed and federated learning. We underline differences in terms of convergence rates between several unbiased compression operators, that all satisfy the same condition on their variance, thus going beyond the classical worst-case analysis. To do so, we focus on the case of least-squares regression (LSR) and analyze a general stochastic approximation algorithm for minimizing quadratic functions relying on a random field. We consider weak assumptions on the random field, tailored to the analysis (specifically, expected H\"older regularity), and on the noise covariance, enabling the analysis of various randomizing mechanisms, including compression. We then extend our results to the case of federated learning.  More formally, we highlight the impact on the convergence of the covariance $\mathfrak{C}_{\mathrm{ania}}$ of the additive noise induced 
    
[^13]: ä¸€ç§æœ‰æ•ˆä¸”å¯è¯æ˜ç²¾ç¡®çš„0-1æŸå¤±çº¿æ€§åˆ†ç±»é—®é¢˜ç®—æ³•

    An efficient, provably exact algorithm for the 0-1 loss linear classification problem. (arXiv:2306.12344v1 [cs.LG])

    [http://arxiv.org/abs/2306.12344](http://arxiv.org/abs/2306.12344)

    è¯¥ç ”ç©¶è¯¦ç»†ä»‹ç»äº†ä¸€ç§åä¸ºå¢é‡å•å…ƒæšä¸¾ï¼ˆICEï¼‰çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•å¯ä»¥ç²¾ç¡®è§£å†³å®šç»´åº¦0-1æŸå¤±çº¿æ€§åˆ†ç±»é—®é¢˜ã€‚

    

    è§£å†³çº¿æ€§åˆ†ç±»é—®é¢˜çš„ç®—æ³•å…·æœ‰æ‚ ä¹…çš„å†å²ï¼Œè‡³å°‘å¯ä»¥è¿½æº¯åˆ°1936å¹´çš„çº¿æ€§åˆ¤åˆ«åˆ†æã€‚å¯¹äºçº¿æ€§å¯åˆ†æ•°æ®ï¼Œè®¸å¤šç®—æ³•å¯ä»¥æœ‰æ•ˆåœ°å¾—åˆ°ç›¸åº”çš„0-1æŸå¤±åˆ†ç±»é—®é¢˜çš„ç²¾ç¡®è§£ï¼Œä½†å¯¹äºéçº¿æ€§å¯åˆ†æ•°æ®ï¼Œå·²ç»è¯æ˜è¿™ä¸ªé—®é¢˜åœ¨å®Œå…¨èŒƒå›´å†…æ˜¯NPéš¾çš„ã€‚æ‰€æœ‰æ›¿ä»£æ–¹æ³•éƒ½æ¶‰åŠæŸç§å½¢å¼çš„è¿‘ä¼¼ï¼ŒåŒ…æ‹¬ä½¿ç”¨0-1æŸå¤±çš„ä»£ç†ï¼ˆä¾‹å¦‚hingeæˆ–logisticæŸå¤±ï¼‰æˆ–è¿‘ä¼¼çš„ç»„åˆæœç´¢ï¼Œè¿™äº›éƒ½ä¸èƒ½ä¿è¯å®Œå…¨è§£å†³é—®é¢˜ã€‚æ‰¾åˆ°è§£å†³å®šç»´åº¦0-1æŸå¤±çº¿æ€§åˆ†ç±»é—®é¢˜çš„å…¨å±€æœ€ä¼˜è§£çš„æœ‰æ•ˆç®—æ³•ä»ç„¶æ˜¯ä¸€ä¸ªæœªè§£å†³çš„é—®é¢˜ã€‚åœ¨æœ¬ç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬è¯¦ç»†ä»‹ç»äº†ä¸€ä¸ªæ–°ç®—æ³•çš„æ„å»ºè¿‡ç¨‹ï¼Œå¢é‡å•å…ƒæšä¸¾ï¼ˆICEï¼‰ï¼Œå®ƒå¯ä»¥ç²¾ç¡®è§£å†³0-1æŸå¤±åˆ†ç±»é—®é¢˜ã€‚

    Algorithms for solving the linear classification problem have a long history, dating back at least to 1936 with linear discriminant analysis. For linearly separable data, many algorithms can obtain the exact solution to the corresponding 0-1 loss classification problem efficiently, but for data which is not linearly separable, it has been shown that this problem, in full generality, is NP-hard. Alternative approaches all involve approximations of some kind, including the use of surrogates for the 0-1 loss (for example, the hinge or logistic loss) or approximate combinatorial search, none of which can be guaranteed to solve the problem exactly. Finding efficient algorithms to obtain an exact i.e. globally optimal solution for the 0-1 loss linear classification problem with fixed dimension, remains an open problem. In research we report here, we detail the construction of a new algorithm, incremental cell enumeration (ICE), that can solve the 0-1 loss classification problem exactly in po
    
[^14]: å¼ºå¥çš„éšæœºé¢„å¤„ç†æ–¹æ³•è§£å†³æ ¸å²­å›å½’é—®é¢˜

    Robust, randomized preconditioning for kernel ridge regression. (arXiv:2304.12465v1 [math.NA])

    [http://arxiv.org/abs/2304.12465](http://arxiv.org/abs/2304.12465)

    é’ˆå¯¹æ ¸å²­å›å½’é—®é¢˜ï¼Œæœ¬æ–‡å¼•å…¥äº†ä¸¤ç§å¼ºå¥çš„éšæœºé¢„å¤„ç†æŠ€æœ¯ï¼Œåˆ†åˆ«è§£å†³äº†å…¨æ•°æ®KRRé—®é¢˜å’Œé™åˆ¶ç‰ˆKRRé—®é¢˜ï¼Œå…‹æœäº†ä»¥å¾€é¢„å¤„ç†å™¨çš„æ•…éšœæ¨¡å¼ã€‚

    

    æœ¬è®ºæ–‡ä»‹ç»äº†ä¸¤ç§éšæœºé¢„å¤„ç†æŠ€æœ¯ï¼Œç”¨äºå¼ºå¥åœ°è§£å†³å…·æœ‰ä¸­å¤§è§„æ¨¡æ•°æ®ç‚¹ï¼ˆ$10^4 \leq N \leq 10^7$ï¼‰çš„æ ¸å²­å›å½’ï¼ˆKRRï¼‰é—®é¢˜ã€‚ç¬¬ä¸€ç§æ–¹æ³•ï¼ŒRPCholeskyé¢„å¤„ç†ï¼Œèƒ½å¤Ÿåœ¨å‡è®¾æ ¸çŸ©é˜µç‰¹å¾å€¼æœ‰è¶³å¤Ÿå¿«é€Ÿçš„å¤šé¡¹å¼è¡°å‡çš„æƒ…å†µä¸‹ï¼Œä»¥$Oï¼ˆN ^ 2ï¼‰$ç®—æ³•æ“ä½œå‡†ç¡®åœ°è§£å†³å…¨æ•°æ®KRRé—®é¢˜ã€‚ç¬¬äºŒç§æ–¹æ³•ï¼ŒKRILLé¢„å¤„ç†ï¼Œä»¥$Oï¼ˆï¼ˆN + k ^ 2ï¼‰k \ logkï¼‰$çš„ä»£ä»·ï¼Œä¸ºKRRé—®é¢˜çš„é™åˆ¶ç‰ˆæœ¬æä¾›å‡†ç¡®çš„è§£å†³æ–¹æ¡ˆï¼Œè¯¥ç‰ˆæœ¬æ¶‰åŠ$k \ll N$é€‰æ‹©çš„æ•°æ®ä¸­å¿ƒã€‚æ‰€æå‡ºçš„æ–¹æ³•è§£å†³äº†å¹¿æ³›çš„KRRé—®é¢˜ï¼Œå…‹æœäº†ä»¥å‰çš„KRRé¢„å¤„ç†å™¨çš„æ•…éšœæ¨¡å¼ï¼Œä½¿å®ƒä»¬æˆä¸ºå®é™…åº”ç”¨çš„ç†æƒ³é€‰æ‹©ã€‚

    This paper introduces two randomized preconditioning techniques for robustly solving kernel ridge regression (KRR) problems with a medium to large number of data points ($10^4 \leq N \leq 10^7$). The first method, RPCholesky preconditioning, is capable of accurately solving the full-data KRR problem in $O(N^2)$ arithmetic operations, assuming sufficiently rapid polynomial decay of the kernel matrix eigenvalues. The second method, KRILL preconditioning, offers an accurate solution to a restricted version of the KRR problem involving $k \ll N$ selected data centers at a cost of $O((N + k^2) k \log k)$ operations. The proposed methods solve a broad range of KRR problems and overcome the failure modes of previous KRR preconditioners, making them ideal for practical applications.
    
[^15]: åŸºäºå¤šå®¤ç¥ç»å…ƒå’Œéæµ·å†€æ€§å¯å¡‘æ€§çš„ç¥ç»ç½‘ç»œå¯¼å‡ºçš„è§„èŒƒæ¡†æ¶

    Normative framework for deriving neural networks with multi-compartmental neurons and non-Hebbian plasticity. (arXiv:2302.10051v2 [q-bio.NC] UPDATED)

    [http://arxiv.org/abs/2302.10051](http://arxiv.org/abs/2302.10051)

    è¯¥è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåŸºäºç›¸ä¼¼æ€§åŒ¹é…æ–¹æ³•çš„è§„èŒƒæ¡†æ¶ï¼Œå¯ä»¥è§£å†³å¯¼å‡ºç¥ç»ç½‘ç»œä¸­å¤šå®¤ç¥ç»å…ƒå’Œéæµ·åšå‹å¯å¡‘æ€§çš„æŒ‘æˆ˜ã€‚

    

    ç†è§£ç¥ç»è®¡ç®—çš„ç®—æ³•åŸºç¡€çš„å·²å»ºç«‹çš„è§„èŒƒæ–¹æ³•æ˜¯ä»æœ‰åŸåˆ™çš„è®¡ç®—ç›®æ ‡å¯¼å‡ºåœ¨çº¿ç®—æ³•ï¼Œå¹¶è¯„ä¼°å®ƒä»¬ä¸è§£å‰–å’Œç”Ÿç†è§‚å¯Ÿçš„å…¼å®¹æ€§ã€‚ç›¸ä¼¼æ€§åŒ¹é…ç›®æ ‡å·²ç»æˆåŠŸä½œä¸ºå¯¼å‡ºä¸ç‚¹ç¥ç»å…ƒå’Œæµ·åš/åæµ·åšå¯å¡‘æ€§ç›¸åŒ¹é…çš„åœ¨çº¿ç®—æ³•çš„èµ·ç‚¹ã€‚è¿™äº›ç¥ç»ç½‘ç»œæ¨¡å‹è§£é‡Šäº†è®¸å¤šè§£å‰–å’Œç”Ÿç†è§‚å¯Ÿï¼Œä½†ç›®æ ‡çš„è®¡ç®—èƒ½åŠ›æœ‰é™ï¼Œå¯¼å‡ºçš„ç¥ç»ç½‘ç»œæ— æ³•è§£é‡Šå…¨è„‘æ™®éå­˜åœ¨çš„å¤šå®¤ç¥ç»å…ƒç»“æ„å’Œéæµ·åšå‹å¯å¡‘æ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ç»Ÿä¸€å’Œæ¨å¹¿äº†ç›¸ä¼¼æ€§åŒ¹é…æ–¹æ³•çš„æœ€è¿‘æ‰©å±•ï¼Œä»¥è§£å†³æ›´å¤æ‚çš„ç›®æ ‡ï¼ŒåŒ…æ‹¬å¯ä»¥è¡¨è¿°ä¸ºå¯¹ç§°å¹¿ä¹‰ç‰¹å¾å€¼é—®é¢˜çš„å¤§ç±»æ— ç›‘ç£å’Œè‡ªæˆ‘ç›‘ç£å­¦ä¹ ä»»åŠ¡ã€‚

    An established normative approach for understanding the algorithmic basis of neural computation is to derive online algorithms from principled computational objectives and evaluate their compatibility with anatomical and physiological observations. Similarity matching objectives have served as successful starting points for deriving online algorithms that map onto neural networks (NNs) with point neurons and Hebbian/anti-Hebbian plasticity. These NN models account for many anatomical and physiological observations; however, the objectives have limited computational power and the derived NNs do not explain multi-compartmental neuronal structures and non-Hebbian forms of plasticity that are prevalent throughout the brain. In this article, we unify and generalize recent extensions of the similarity matching approach to address more complex objectives, including a large class of unsupervised and self-supervised learning tasks that can be formulated as symmetric generalized eigenvalue probl
    
[^16]: æœ€ä¼˜è®­ç»ƒå‡æ–¹å·®ä¼°è®¡ç¥ç»ç½‘ç»œ

    Optimal Training of Mean Variance Estimation Neural Networks. (arXiv:2302.08875v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2302.08875](http://arxiv.org/abs/2302.08875)

    æœ¬æ–‡ç ”ç©¶äº†å‡æ–¹å·®ä¼°è®¡ç½‘ç»œçš„æœ€ä¼˜å®ç°ï¼Œå¹¶å‘ç°é€šè¿‡ä½¿ç”¨é¢„çƒ­æœŸå¯ä»¥é¿å…æ”¶æ•›å›°éš¾ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†å‡æ–¹å·®ä¼°è®¡ç½‘ç»œï¼ˆMVEç½‘ç»œï¼‰çš„æœ€ä¼˜å®ç°ã€‚è¿™ç§ç½‘ç»œç»å¸¸è¢«ç”¨ä½œå›å½’è®¾ç½®ä¸­ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•çš„æ„å»ºæ¨¡å—ï¼Œä¾‹å¦‚Concrete dropoutå’ŒDeep Ensemblesã€‚å…·ä½“è€Œè¨€ï¼ŒMVEç½‘ç»œå‡è®¾æ•°æ®æ˜¯ä»ä¸€ä¸ªå…·æœ‰å‡å€¼å‡½æ•°å’Œæ–¹å·®å‡½æ•°çš„æ­£æ€åˆ†å¸ƒäº§ç”Ÿçš„ã€‚MVEç½‘ç»œè¾“å‡ºå‡å€¼å’Œæ–¹å·®çš„ä¼°è®¡ï¼Œé€šè¿‡æœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶å‡½æ•°æ¥ä¼˜åŒ–ç½‘ç»œå‚æ•°ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸¤ä¸ªé‡è¦çš„è§è§£ã€‚é¦–å…ˆï¼Œæœ€è¿‘çš„ç ”ç©¶ä¸­æŠ¥å‘Šçš„æ”¶æ•›å›°éš¾å¯ä»¥é€šè¿‡éµå¾ªåŸå§‹ä½œè€…çš„ç®€å•ä½†ç»å¸¸è¢«å¿½è§†çš„å»ºè®®æ¥ç›¸å¯¹å®¹æ˜“åœ°é¿å…ï¼Œå³ä½¿ç”¨ä¸€ä¸ªé¢„çƒ­æœŸã€‚åœ¨è¿™ä¸ªæœŸé—´ï¼Œåªä¼˜åŒ–å‡å€¼ï¼Œæ–¹å·®ä¿æŒå›ºå®šã€‚æˆ‘ä»¬é€šè¿‡å®éªŒè¯æ˜äº†è¿™ä¸€æ­¥éª¤çš„æœ‰æ•ˆæ€§ã€‚

    This paper focusses on the optimal implementation of a Mean Variance Estimation network (MVE network) (Nix and Weigend, 1994). This type of network is often used as a building block for uncertainty estimation methods in a regression setting, for instance Concrete dropout (Gal et al., 2017) and Deep Ensembles (Lakshminarayanan et al., 2017). Specifically, an MVE network assumes that the data is produced from a normal distribution with a mean function and variance function. The MVE network outputs a mean and variance estimate and optimizes the network parameters by minimizing the negative loglikelihood. In our paper, we present two significant insights. Firstly, the convergence difficulties reported in recent work can be relatively easily prevented by following the simple yet often overlooked recommendation from the original authors that a warm-up period should be used. During this period, only the mean is optimized with a fixed variance. We demonstrate the effectiveness of this step thr
    
[^17]: ä¸ªä½“å…¬å¹³çš„çŸ©é˜µä¼°è®¡

    Matrix Estimation for Individual Fairness. (arXiv:2302.02096v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2302.02096](http://arxiv.org/abs/2302.02096)

    æœ¬æ–‡ç ”ç©¶äº†ä¸ªä½“å…¬å¹³æ€§(IF)å’ŒçŸ©é˜µä¼°è®¡(ME)ä¹‹é—´çš„è”ç³»ã€‚ç»“æœè¡¨æ˜ï¼Œåœ¨é€‚å½“æ¡ä»¶ä¸‹ä½¿ç”¨MEæ–¹æ³•è¿›è¡Œæ•°æ®é¢„å¤„ç†å¯ä»¥æ”¹å–„ç®—æ³•çš„ä¸ªä½“å…¬å¹³æ€§ï¼Œå¹¶ä¸”ä¸ä¼šç‰ºç‰²æ€§èƒ½ã€‚

    

    è¿‘å¹´æ¥ï¼Œå‡ºç°äº†å¤šç§ç®—æ³•å…¬å¹³æ€§çš„æ¦‚å¿µã€‚å…¶ä¸­ä¸€ç§æ¦‚å¿µæ˜¯ä¸ªä½“å…¬å¹³æ€§(IF)ï¼Œè¦æ±‚ç›¸ä¼¼çš„ä¸ªä½“æ¥å—ç›¸ä¼¼çš„å¯¹å¾…ã€‚ä¸æ­¤åŒæ—¶ï¼ŒçŸ©é˜µä¼°è®¡(ME)ä½œä¸ºå¤„ç†å…·æœ‰ç¼ºå¤±å€¼çš„å™ªå£°æ•°æ®çš„ä¸€ç§è‡ªç„¶èŒƒå¼ä¹Ÿå‡ºç°äº†ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†è¿™ä¸¤ä¸ªæ¦‚å¿µè¿›è¡Œäº†è”ç³»ã€‚æˆ‘ä»¬è¡¨æ˜ï¼Œä½¿ç”¨MEæ–¹æ³•å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†å¯ä»¥åœ¨ä¸ç‰ºç‰²æ€§èƒ½çš„æƒ…å†µä¸‹æ”¹å–„ç®—æ³•çš„IFã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨é€‚å½“çš„æ¡ä»¶ä¸‹ï¼Œä½¿ç”¨ä¸€ç§åä¸ºå¥‡å¼‚å€¼é˜ˆå€¼(SVT)çš„æµè¡ŒMEæ–¹æ³•å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†å¯ä»¥æä¾›å¼ºæœ‰åŠ›çš„IFä¿è¯ã€‚ç„¶åï¼Œæˆ‘ä»¬è¡¨æ˜ï¼Œåœ¨ç±»ä¼¼çš„æ¡ä»¶ä¸‹ï¼ŒSVTé¢„å¤„ç†è¿˜äº§ç”Ÿäº†ä¸€è‡´ä¸”è¿‘ä¼¼æœ€å°åŒ–æ•Œå¯¹é£é™©çš„ä¼°è®¡ã€‚å› æ­¤ï¼Œåœ¨æ‰€è¿°æ¡ä»¶ä¸‹ï¼ŒMEé¢„å¤„ç†æ­¥éª¤ä¸ä¼šå¢åŠ åŸºæœ¬ç®—æ³•çš„é¢„æµ‹è¯¯å·®ï¼Œå³ä¸ä¼šç»™å…¬å¹³æ€§ä¸æ€§èƒ½ä¹‹é—´å¸¦æ¥æƒè¡¡ã€‚æˆ‘ä»¬é€šè¿‡åˆæˆæ•°æ®å’ŒçœŸå®æ•°æ®é›†éªŒè¯äº†è¿™äº›ç»“æœã€‚

    In recent years, multiple notions of algorithmic fairness have arisen. One such notion is individual fairness (IF), which requires that individuals who are similar receive similar treatment. In parallel, matrix estimation (ME) has emerged as a natural paradigm for handling noisy data with missing values. In this work, we connect the two concepts. We show that pre-processing data using ME can improve an algorithm's IF without sacrificing performance. Specifically, we show that using a popular ME method known as singular value thresholding (SVT) to pre-process the data provides a strong IF guarantee under appropriate conditions. We then show that, under analogous conditions, SVT pre-processing also yields estimates that are consistent and approximately minimax optimal. As such, the ME pre-processing step does not, under the stated conditions, increase the prediction error of the base algorithm, i.e., does not impose a fairness-performance trade-off. We verify these results on synthetic a
    
[^18]: è‡ªä¿¡çš„ç¥ç»ç½‘ç»œå›å½’ä¸å¼•å¯¼æ·±åº¦é›†æˆ

    Confident Neural Network Regression with Bootstrapped Deep Ensembles. (arXiv:2202.10903v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2202.10903](http://arxiv.org/abs/2202.10903)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§ç§°ä¸ºBootstrapped Deep Ensemblesçš„æ–°æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥ç»å…¸çš„æœ‰é™æ•°æ®æ•ˆåº”ï¼Œæ˜ç¡®è€ƒè™‘ç¥ç»ç½‘ç»œå›å½’ä¸­çš„ä¸ç¡®å®šæ€§ï¼Œå¹¶é€šè¿‡å®éªŒè¯æ˜äº†è¯¥æ–¹æ³•çš„æ˜¾è‘—æ”¹è¿›ã€‚

    

    éšç€ç¥ç»ç½‘ç»œçš„æµè¡Œå’Œä½¿ç”¨å¢åŠ ï¼Œå¯ä¿¡çš„ä¸ç¡®å®šæ€§ä¼°è®¡å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚å…¶ä¸­ä¸€ä¸ªæœ€çªå‡ºçš„ä¸ç¡®å®šæ€§ä¼°è®¡æ–¹æ³•æ˜¯Deep Ensemblesï¼ˆLakshminarayananç­‰äººï¼Œ2017ï¼‰ã€‚ä¸€ä¸ªç»å…¸çš„å‚æ•°æ¨¡å‹ç”±äºå»ºæ¨¡æ•°æ®æ˜¯éšæœºæ ·æœ¬ï¼Œå› æ­¤å…¶å‚æ•°å­˜åœ¨ä¸ç¡®å®šæ€§ã€‚ç°ä»£ç¥ç»ç½‘ç»œç”±äºç½‘ç»œä¼˜åŒ–çš„éšæœºæ€§ä¹Ÿå…·æœ‰é¢å¤–çš„ä¸ç¡®å®šæ€§æˆåˆ†ã€‚Lakshminarayananç­‰äººï¼ˆ2017ï¼‰æŒ‡å‡ºï¼ŒDeep Ensemblesæœªè€ƒè™‘åˆ°ç”±æœ‰é™æ•°æ®æ•ˆåº”å¼•èµ·çš„ç»å…¸ä¸ç¡®å®šæ€§ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§ç”¨äºå›å½’è®¾ç½®çš„è®¡ç®—å»‰ä»·æ€§æ‰©å±•Deep Ensemblesçš„æ–¹æ³•ï¼Œç§°ä¸ºBootstrapped Deep Ensemblesï¼Œå®ƒä½¿ç”¨æ”¹è¿›ç‰ˆçš„å‚æ•°è‡ªåŠ©æ³•æ˜ç¡®è€ƒè™‘äº†æœ‰é™æ•°æ®çš„ç»å…¸æ•ˆåº”ã€‚é€šè¿‡å®éªŒç ”ç©¶ï¼Œæˆ‘ä»¬è¯æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨æ ‡å‡†æ–¹æ³•çš„åŸºç¡€ä¸Šæ˜æ˜¾æ”¹è¿›ã€‚

    With the rise of the popularity and usage of neural networks, trustworthy uncertainty estimation is becoming increasingly essential. One of the most prominent uncertainty estimation methods is Deep Ensembles (Lakshminarayanan et al., 2017) . A classical parametric model has uncertainty in the parameters due to the fact that the data on which the model is build is a random sample. A modern neural network has an additional uncertainty component since the optimization of the network is random. Lakshminarayanan et al. (2017) noted that Deep Ensembles do not incorporate the classical uncertainty induced by the effect of finite data. In this paper, we present a computationally cheap extension of Deep Ensembles for the regression setting, called Bootstrapped Deep Ensembles, that explicitly takes this classical effect of finite data into account using a modified version of the parametric bootstrap. We demonstrate through an experimental study that our method significantly improves upon standar
    
[^19]: å¦‚ä½•è¯„ä¼°æœºå™¨å­¦ä¹ å›å½’ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Ÿ

    How to Evaluate Uncertainty Estimates in Machine Learning for Regression?. (arXiv:2106.03395v2 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2106.03395](http://arxiv.org/abs/2106.03395)

    æœ¬æ–‡ç ”ç©¶äº†å¦‚ä½•è¯„ä¼°æœºå™¨å­¦ä¹ å›å½’ä¸­çš„ä¸ç¡®å®šæ€§ä¼°è®¡ï¼Œå‘ç°ç›®å‰çš„è¯„ä¼°æ–¹æ³•å­˜åœ¨ä¸¥é‡ç¼ºé™·ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°ä¼°è®¡è´¨é‡å’Œé¢„æµ‹åŒºé—´çš„å…³ç³»ã€‚

    

    éšç€ç¥ç»ç½‘ç»œçš„æ™®åŠï¼Œå¯¹äºç›¸åº”çš„ä¸ç¡®å®šæ€§ä¼°è®¡çš„éœ€æ±‚ä¹Ÿè¶Šæ¥è¶Šå¤§ã€‚ç›®å‰æœ‰ä¸¤ç§ä¸»è¦çš„æ–¹æ³•æ¥æµ‹è¯•è¿™äº›ä¼°è®¡çš„è´¨é‡ã€‚å¤§å¤šæ•°æ–¹æ³•è¾“å‡ºä¸€ä¸ªæ¦‚ç‡å¯†åº¦ï¼Œå¯ä»¥é€šè¿‡åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°å…¶å¯¹æ•°ä¼¼ç„¶æ¥è¿›è¡Œæ¯”è¾ƒã€‚å…¶ä»–æ–¹æ³•ç›´æ¥è¾“å‡ºä¸€ä¸ªé¢„æµ‹åŒºé—´ï¼Œé€šå¸¸é€šè¿‡æ£€æŸ¥è½å…¥ç›¸åº”é¢„æµ‹åŒºé—´çš„æµ‹è¯•ç‚¹çš„æ¯”ä¾‹æ¥è¿›è¡Œæµ‹è¯•ã€‚ç›´è§‚ä¸Šçœ‹ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æ˜¯åˆç†çš„ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬é€šè¿‡ç†è®ºè®ºè¯å’Œæ¨¡æ‹Ÿå®éªŒè¡¨æ˜ï¼Œè¯„ä¼°ä¸ç¡®å®šæ€§ä¼°è®¡è´¨é‡çš„è¿™ä¸¤ç§æ–¹å¼éƒ½å­˜åœ¨ä¸¥é‡ç¼ºé™·ã€‚é¦–å…ˆï¼Œè¿™ä¸¤ç§æ–¹æ³•æ— æ³•åˆ†ç¦»å…±åŒäº§ç”Ÿé¢„æµ‹ä¸ç¡®å®šæ€§çš„å„ä¸ªç»„æˆéƒ¨åˆ†ï¼Œä»è€Œéš¾ä»¥è¯„ä¼°è¿™äº›ç»„æˆéƒ¨åˆ†çš„ä¼°è®¡è´¨é‡ã€‚å…¶æ¬¡ï¼Œæ›´å¥½çš„å¯¹æ•°ä¼¼ç„¶å¹¶ä¸ä¿è¯æ›´å¥½çš„é¢„æµ‹åŒºé—´ï¼Œè€Œè¿™é€šå¸¸æ˜¯è¿™äº›æ–¹æ³•åœ¨å®è·µä¸­æ‰€ç”¨çš„ã€‚

    As neural networks become more popular, the need for accompanying uncertainty estimates increases. There are currently two main approaches to test the quality of these estimates. Most methods output a density. They can be compared by evaluating their loglikelihood on a test set. Other methods output a prediction interval directly. These methods are often tested by examining the fraction of test points that fall inside the corresponding prediction intervals. Intuitively both approaches seem logical. However, we demonstrate through both theoretical arguments and simulations that both ways of evaluating the quality of uncertainty estimates have serious flaws. Firstly, both approaches cannot disentangle the separate components that jointly create the predictive uncertainty, making it difficult to evaluate the quality of the estimates of these components. Secondly, a better loglikelihood does not guarantee better prediction intervals, which is what the methods are often used for in practice
    
[^20]: éšæœºç§æ¤æ£®æ—ï¼šä¸€ç§ç›´æ¥å¯è§£é‡Šçš„æ ‘é›†ç®—æ³•

    Random Planted Forest: a directly interpretable tree ensemble. (arXiv:2012.14563v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2012.14563](http://arxiv.org/abs/2012.14563)

    æå‡ºäº†ä¸€ç§åä¸º"éšæœºç§æ¤æ£®æ—"çš„ç®—æ³•ï¼Œé€šè¿‡ä¿®æ”¹éšæœºæ£®æ—ç®—æ³•ï¼Œä¿ç•™åˆ‡åˆ†åçš„æŸäº›å¶å­ï¼Œå½¢æˆéäºŒè¿›åˆ¶æ ‘ï¼Œå®ç°ç›´æ¥å¯è§£é‡Šçš„æ ‘é›†ç®—æ³•ã€‚è¯¥ç®—æ³•å…·æœ‰è¾ƒå¥½çš„é¢„æµ‹å’Œå¯è§†åŒ–ç‰¹æ€§ã€‚

    

    æˆ‘ä»¬ä»‹ç»äº†ä¸€ç§æ–°é¢–çš„å¯è§£é‡Šçš„åŸºäºæ ‘çš„å›å½’ç®—æ³•ã€‚æˆ‘ä»¬çš„åŠ¨æœºæ˜¯ä»åŠŸèƒ½åˆ†è§£çš„è§’åº¦ä¼°è®¡æœªçŸ¥çš„å›å½’å‡½æ•°ï¼Œå…¶ä¸­åŠŸèƒ½ç»„ä»¶å¯¹åº”äºä½é˜¶äº¤äº’é¡¹ã€‚æˆ‘ä»¬çš„æ€è·¯æ˜¯ä¿®æ”¹éšæœºæ£®æ—ç®—æ³•ï¼Œé€šè¿‡åœ¨åˆ‡åˆ†åä¿ç•™æŸäº›å¶å­è€Œä¸æ˜¯åˆ é™¤å®ƒä»¬ã€‚è¿™å¯¼è‡´éäºŒè¿›åˆ¶æ ‘ï¼Œæˆ‘ä»¬ç§°ä¹‹ä¸ºç§æ¤æ ‘ã€‚å°†å…¶æ‰©å±•ä¸ºä¸€ä¸ªæ£®æ—ï¼Œæˆ‘ä»¬å¾—åˆ°äº†æˆ‘ä»¬çš„éšæœºç§æ¤æ£®æ—ç®—æ³•ã€‚æ­¤å¤–ï¼Œå¯ä»¥å¯¹å¶å­å†…å¯ä»¥ç›¸äº’ä½œç”¨çš„åå˜é‡æ•°é‡è¿›è¡Œé™åˆ¶ã€‚å¦‚æœæˆ‘ä»¬å°†äº¤äº’é™åˆ¶è®¾ç½®ä¸º1ï¼Œå¾—åˆ°çš„ä¼°è®¡é‡æ˜¯ä¸€ç»´å‡½æ•°çš„å’Œã€‚åœ¨å¦ä¸€ä¸ªæç«¯æƒ…å†µä¸‹ï¼Œå¦‚æœæˆ‘ä»¬ä¸è®¾ç½®é™åˆ¶ï¼Œå¾—åˆ°çš„ä¼°è®¡é‡å’Œç›¸åº”çš„æ¨¡å‹å¯¹å›å½’å‡½æ•°çš„å½¢å¼ä¸åŠ é™åˆ¶ã€‚åœ¨æ¨¡æ‹Ÿç ”ç©¶ä¸­ï¼Œæˆ‘ä»¬å‘ç°æˆ‘ä»¬çš„éšæœºç§æ¤æ£®æ—ç®—æ³•å…·æœ‰é¼“åŠ±äººçš„é¢„æµ‹å’Œå¯è§†åŒ–ç‰¹æ€§ã€‚

    We introduce a novel interpretable tree based algorithm for prediction in a regression setting. Our motivation is to estimate the unknown regression function from a functional decomposition perspective in which the functional components correspond to lower order interaction terms. The idea is to modify the random forest algorithm by keeping certain leaves after they are split instead of deleting them. This leads to non-binary trees which we refer to as planted trees. An extension to a forest leads to our random planted forest algorithm. Additionally, the maximum number of covariates which can interact within a leaf can be bounded. If we set this interaction bound to one, the resulting estimator is a sum of one-dimensional functions. In the other extreme case, if we do not set a limit, the resulting estimator and corresponding model place no restrictions on the form of the regression function. In a simulation study we find encouraging prediction and visualisation properties of our rando
    
[^21]: åˆ©ç”¨å…ˆéªŒçŸ¥è¯†çš„ Gamma-Minimax ä¼°è®¡å™¨çš„å¯¹æŠ—å…ƒå­¦ä¹ 

    Adversarial Meta-Learning of Gamma-Minimax Estimators That Leverage Prior Knowledge. (arXiv:2012.05465v4 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2012.05465](http://arxiv.org/abs/2012.05465)

    æœ¬æ–‡æå‡ºå¯¹æŠ—å…ƒå­¦ä¹ æ–¹æ³•ï¼Œç”¨äºè®¡ç®—åœ¨ä¸€ç»„ä¸å¯ç”¨çŸ¥è¯†ç›¸å®¹çš„å…ˆéªŒåˆ†å¸ƒä¸­æœ€å°åŒ–æœ€åæƒ…å†µçš„ Bayes é£é™©çš„ Gamma-Minimax ä¼°è®¡å™¨ï¼Œæ–‡ä¸­è¿˜æå‡ºäº†ä¸€ç§ç¥ç»ç½‘ç»œç±»ç”¨äºæä¾›ä¼°è®¡å™¨ç±»ï¼Œä»¥åŠä¸¤ä¸ªå®éªŒç¯èŠ‚ç”¨äºè¯´æ˜è¯¥æ–¹æ³•çš„åº”ç”¨ã€‚

    

    è´å¶æ–¯ä¼°è®¡æä¾›äº†ä¸€ç§å°†èƒ½å¤Ÿä»¥å•ä¸ªå…ˆéªŒåˆ†å¸ƒçš„å½¢å¼è¡¨è¾¾çš„å…ˆéªŒçŸ¥è¯†ç»“åˆèµ·æ¥çš„æ–¹å¼ã€‚ç„¶è€Œï¼Œå½“è¿™ç§çŸ¥è¯†å¤ªæ¨¡ç³Šï¼Œæ— æ³•ç”¨å•ä¸ªå…ˆéªŒè¡¨ç¤ºæ—¶ï¼Œå°±éœ€è¦å¦ä¸€ç§æ–¹æ³•ã€‚Gamma-minimax ä¼°è®¡å™¨æä¾›äº†è¿™æ ·ä¸€ç§æ–¹æ³•ã€‚è¿™äº›ä¼°è®¡å™¨å°†åœ¨ä¸å¯ç”¨çŸ¥è¯†ç›¸å®¹çš„ä¸€ç»„å…ˆéªŒåˆ†å¸ƒ $\Gamma$ ä¸Šæœ€å°åŒ–æœ€åæƒ…å†µçš„ Bayes é£é™©ã€‚ä¼ ç»Ÿä¸Šï¼ŒGamma-minimax æ€§è´¨æ˜¯ä¸ºå‚æ•°æ¨¡å‹å®šä¹‰çš„ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¸ºä¸€èˆ¬æ¨¡å‹å®šä¹‰ Gamma-minimax ä¼°è®¡å™¨ï¼Œå¹¶æå‡ºäº†åˆ©ç”¨ä¸€èˆ¬åŒ–çŸ©é™åˆ¶çš„å¯¹æŠ—å…ƒå­¦ä¹ ç®—æ³•æ¥è®¡ç®—å®ƒä»¬ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ç¥ç»ç½‘ç»œç±»ï¼Œå®ƒæä¾›äº†ä¸€ç§ä¸°å¯Œä½†æœ‰é™ç»´åº¦çš„ä¼°è®¡å™¨ç±»ï¼Œå¯ä»¥ä»ä¸­é€‰æ‹© Gamma-minimax ä¼°è®¡å™¨ã€‚æˆ‘ä»¬åœ¨ä¸¤ä¸ªç¯èŠ‚ä¸­è¯´æ˜äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œå³ä¼°è®¡æœªçŸ¥æ”¯æŒåˆ†å¸ƒçš„æ ·æœ¬ç†µå’Œååˆ†å±‚ä¼°è®¡ã€‚

    Bayes estimators are well known to provide a means to incorporate prior knowledge that can be expressed in terms of a single prior distribution. However, when this knowledge is too vague to express with a single prior, an alternative approach is needed. Gamma-minimax estimators provide such an approach. These estimators minimize the worst-case Bayes risk over a set $\Gamma$ of prior distributions that are compatible with the available knowledge. Traditionally, Gamma-minimaxity is defined for parametric models. In this work, we define Gamma-minimax estimators for general models and propose adversarial meta-learning algorithms to compute them when the set of prior distributions is constrained by generalized moments. Accompanying convergence guarantees are also provided. We also introduce a neural network class that provides a rich, but finite-dimensional, class of estimators from which a Gamma-minimax estimator can be selected. We illustrate our method in two settings, namely entropy est
    
[^22]: ç¨³å®šä¸€è‡´çš„å¯†åº¦-basedèšç±»ç®—æ³•é€šè¿‡å¤šå‚æ•°æŒç»­æ€§

    Stable and consistent density-based clustering via multiparameter persistence. (arXiv:2005.09048v3 [math.ST] UPDATED)

    [http://arxiv.org/abs/2005.09048](http://arxiv.org/abs/2005.09048)

    è¿™ç¯‡è®ºæ–‡é€šè¿‡å¼•å…¥ä¸€ç§åº¦é‡å±‚æ¬¡èšç±»çš„å¯¹åº”äº¤é”™è·ç¦»ï¼Œç ”ç©¶äº†ä¸€ç§ç¨³å®šä¸€è‡´çš„å¯†åº¦-basedèšç±»ç®—æ³•ï¼Œæä¾›äº†ä¸€ä¸ªä»ä¸€å‚æ•°å±‚æ¬¡èšç±»ä¸­æå–å•ä¸ªèšç±»çš„ç®—æ³•ï¼Œå¹¶è¯æ˜äº†è¯¥ç®—æ³•çš„ä¸€è‡´æ€§å’Œç¨³å®šæ€§ã€‚

    

    æˆ‘ä»¬è€ƒè™‘äº†æ‹“æ‰‘æ•°æ®åˆ†æä¸­çš„åº¦-Ripsæ„é€ ï¼Œå®ƒæä¾›äº†ä¸€ç§å¯†åº¦æ•æ„Ÿçš„å¤šå‚æ•°å±‚æ¬¡èšç±»ç®—æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬å¼•å…¥çš„ä¸€ç§åº¦é‡å±‚æ¬¡èšç±»çš„å¯¹åº”äº¤é”™è·ç¦»ï¼Œåˆ†æäº†å®ƒå¯¹è¾“å…¥æ•°æ®çš„æ‰°åŠ¨çš„ç¨³å®šæ€§ã€‚ä»åº¦-Ripsä¸­å–æŸäº›ä¸€å‚æ•°åˆ‡ç‰‡å¯ä»¥æ¢å¤å‡ºå·²çŸ¥çš„åŸºäºå¯†åº¦çš„èšç±»æ–¹æ³•ï¼Œä½†æˆ‘ä»¬è¯æ˜äº†è¿™äº›æ–¹æ³•æ˜¯ä¸ç¨³å®šçš„ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬è¯æ˜äº†ä½œä¸ºå¤šå‚æ•°å¯¹è±¡çš„åº¦-Ripsæ˜¯ç¨³å®šçš„ï¼Œå¹¶æå‡ºäº†ä¸€ç§ä»åº¦-Ripsä¸­å–åˆ‡ç‰‡çš„æ›¿ä»£æ–¹æ³•ï¼Œè¯¥æ–¹æ³•äº§ç”Ÿä¸€ä¸ªå…·æœ‰æ›´å¥½ç¨³å®šæ€§å±æ€§çš„ä¸€å‚æ•°å±‚æ¬¡èšç±»ç®—æ³•ã€‚æˆ‘ä»¬ä½¿ç”¨å¯¹åº”äº¤é”™è·ç¦»è¯æ˜äº†è¯¥ç®—æ³•çš„ä¸€è‡´æ€§ã€‚æˆ‘ä»¬æä¾›äº†ä»ä¸€å‚æ•°å±‚æ¬¡èšç±»ä¸­æå–å•ä¸ªèšç±»çš„ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨å¯¹åº”äº¤é”™è·ç¦»æ–¹é¢æ˜¯ç¨³å®šçš„ã€‚

    We consider the degree-Rips construction from topological data analysis, which provides a density-sensitive, multiparameter hierarchical clustering algorithm. We analyze its stability to perturbations of the input data using the correspondence-interleaving distance, a metric for hierarchical clusterings that we introduce. Taking certain one-parameter slices of degree-Rips recovers well-known methods for density-based clustering, but we show that these methods are unstable. However, we prove that degree-Rips, as a multiparameter object, is stable, and we propose an alternative approach for taking slices of degree-Rips, which yields a one-parameter hierarchical clustering algorithm with better stability properties. We prove that this algorithm is consistent, using the correspondence-interleaving distance. We provide an algorithm for extracting a single clustering from one-parameter hierarchical clusterings, which is stable with respect to the correspondence-interleaving distance. And, we
    
[^23]: RAB: å¯è¯å®æŠµæŠ—åé—¨æ”»å‡»çš„æ–¹æ³•

    RAB: Provable Robustness Against Backdoor Attacks. (arXiv:2003.08904v8 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2003.08904](http://arxiv.org/abs/2003.08904)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§è¯å®æœºå™¨å­¦ä¹ æ¨¡å‹é²æ£’æ€§çš„ç»Ÿä¸€æ¡†æ¶ï¼Œé€šè¿‡éšæœºå¹³æ»‘æŠ€æœ¯å®ç°å¯¹è§„é¿å’Œåé—¨æ”»å‡»çš„é²æ£’æ€§ã€‚åŒæ—¶ï¼Œæˆ‘ä»¬æå‡ºäº†é²æ£’è®­ç»ƒè¿‡ç¨‹RABï¼Œå¹¶è¯æ˜å…¶æœ‰æ•ˆæ€§å’Œç´§å¯†æ€§ã€‚åœ¨ç†è®ºä¸Šè¯æ˜äº†å¯¹åé—¨æ”»å‡»è¿›è¡Œé²æ£’æ€§ä¿æŠ¤çš„å¯è¡Œæ€§ã€‚

    

    æœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œï¼ˆDNNsï¼‰å®¹æ˜“å—åˆ°å¯¹æŠ—æ€§æ”»å‡»ï¼ŒåŒ…æ‹¬è§„é¿æ”»å‡»å’Œåé—¨ï¼ˆæ¯’åŒ–ï¼‰æ”»å‡»ã€‚åœ¨é˜²å¾¡æ–¹é¢ï¼Œå¯¹äºè§„é¿æ”»å‡»å·²ç»è¿›è¡Œäº†å¯†é›†çš„æ”¹è¿›ï¼ŒåŒ…æ‹¬ç»éªŒå’Œå¯è¯å®çš„é²æ£’æ€§ï¼›ç„¶è€Œï¼Œå¯¹äºåé—¨æ”»å‡»çš„å¯è¯å®é²æ£’æ€§ä»ç„¶å¾ˆå°‘è¢«æ¢ç´¢ã€‚æœ¬æ–‡é’ˆå¯¹é€šç”¨å¨èƒæ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åé—¨æ”»å‡»ï¼Œæå‡ºäº†ä¸€ç§è¯å®æœºå™¨å­¦ä¹ æ¨¡å‹é²æ£’æ€§çš„ç»Ÿä¸€æ¡†æ¶ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•åˆ©ç”¨éšæœºå¹³æ»‘æŠ€æœ¯æ¥ä¿è¯å¯¹è§„é¿å’Œåé—¨æ”»å‡»çš„é²æ£’æ€§ã€‚æˆ‘ä»¬è¿˜æå‡ºäº†é¦–ä¸ªé²æ£’è®­ç»ƒè¿‡ç¨‹RABï¼Œä½¿è®­ç»ƒæ¨¡å‹å¹³æ»‘å¹¶ä¿è¯å…¶å¯¹åé—¨æ”»å‡»çš„é²æ£’æ€§ã€‚æˆ‘ä»¬è¯æ˜äº†ä½¿ç”¨RABè®­ç»ƒçš„æœºå™¨å­¦ä¹ æ¨¡å‹çš„é²æ£’æ€§ç•Œé™ï¼Œå¹¶è¯æ˜æˆ‘ä»¬çš„é²æ£’æ€§ç•Œé™æ˜¯ç´§å¯†çš„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬åœ¨ç†è®ºä¸Šå±•ç¤ºäº†å®ç°å¯¹åé—¨æ”»å‡»è¿›è¡Œé²æ£’æ€§ä¿æŠ¤æ˜¯å¯èƒ½çš„ã€‚

    Recent studies have shown that deep neural networks (DNNs) are vulnerable to adversarial attacks, including evasion and backdoor (poisoning) attacks. On the defense side, there have been intensive efforts on improving both empirical and provable robustness against evasion attacks; however, the provable robustness against backdoor attacks still remains largely unexplored. In this paper, we focus on certifying the machine learning model robustness against general threat models, especially backdoor attacks. We first provide a unified framework via randomized smoothing techniques and show how it can be instantiated to certify the robustness against both evasion and backdoor attacks. We then propose the first robust training process, RAB, to smooth the trained model and certify its robustness against backdoor attacks. We prove the robustness bound for machine learning models trained with RAB and prove that our robustness bound is tight. In addition, we theoretically show that it is possible
    

