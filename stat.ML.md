# 摘要

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [Theoretical Guarantees for the Subspace-Constrained Tyler's Estimator](https://arxiv.org/abs/2403.18658) | 该论文分析了子空间约束的Tyler估计器用于在高度受到离群值污染的数据集中恢复低维子空间的情况，并展示了当初始化条件得到满足时，该估计器可以有效地恢复潜在的子空间。 |
| [^2] | [Hidden Minima in Two-Layer ReLU Networks](https://arxiv.org/abs/2312.16819) | 本文研究了两层ReLU网络中的隐藏极小值现象，并提出方法来研究这些隐藏极小值的独特解析性质。 |

# 详细

[^1]: 对子空间约束的Tyler估计器的理论保证

    Theoretical Guarantees for the Subspace-Constrained Tyler's Estimator

    [https://arxiv.org/abs/2403.18658](https://arxiv.org/abs/2403.18658)

    该论文分析了子空间约束的Tyler估计器用于在高度受到离群值污染的数据集中恢复低维子空间的情况，并展示了当初始化条件得到满足时，该估计器可以有效地恢复潜在的子空间。

    

    本文分析了用于恢复可能受到严重污染的数据集中的低维子空间的子空间约束的Tyler估计器（STE）。它假设一个弱的内点-外点模型，并允许内点的比例小于导致鲁棒子空间恢复问题计算困难的比例。在这种情况下，它显示如果STE的初始化满足某些条件，那么STE可以有效地恢复潜在的子空间。此外，它还表明在广义的干草堆模型下，由Tyler的M-估计器（TME）初始化的STE可以在内点的比例太小以至于TME无法处理时恢复子空间。

    arXiv:2403.18658v1 Announce Type: cross  Abstract: This work analyzes the subspace-constrained Tyler's estimator (STE) designed for recovering a low-dimensional subspace within a dataset that may be highly corrupted with outliers. It assumes a weak inlier-outlier model and allows the fraction of inliers to be smaller than a fraction that leads to computational hardness of the robust subspace recovery problem. It shows that in this setting, if the initialization of STE, which is an iterative algorithm, satisfies a certain condition, then STE can effectively recover the underlying subspace. It further shows that under the generalized haystack model, STE initialized by the Tyler's M-estimator (TME), can recover the subspace when the fraction of iniliers is too small for TME to handle.
    
[^2]: 两层ReLU网络中的隐藏极小值

    Hidden Minima in Two-Layer ReLU Networks

    [https://arxiv.org/abs/2312.16819](https://arxiv.org/abs/2312.16819)

    本文研究了两层ReLU网络中的隐藏极小值现象，并提出方法来研究这些隐藏极小值的独特解析性质。

    

    本文考虑拟合具有$d$个输入、$k$个神经元以及由目标网络生成的标签的两层ReLU网络所涉及的优化问题。最近发现了两种无穷族的虚假极小值，每个$d$对应一个极小值。属于第一类的极小值的损失在$d$增加时收敛于零。在第二类中，损失保持远离于零。那么，如何避免属于后一类的极小值呢？幸运的是，这样的极小值从不会被标准优化方法检测到。受到此现象性质的问题的启发，我们开发了研究隐藏极小值独特解析性质的方法。根据现有的分析，两种类型的Hessian谱在$O(d^{-1/2})$项模意义下一致 -- 不太乐观。因此，我们的研究通过研究损失被最小化或最大化的曲线进行，通常称为切线。

    arXiv:2312.16819v2 Announce Type: replace  Abstract: The optimization problem associated to fitting two-layer ReLU networks having $d$~inputs, $k$~neurons, and labels generated by a target network, is considered. Two types of infinite families of spurious minima, giving one minimum per $d$, were recently found. The loss at minima belonging to the first type converges to zero as $d$ increases. In the second type, the loss remains bounded away from zero. That being so, how may one avoid minima belonging to the latter type? Fortunately, such minima are never detected by standard optimization methods. Motivated by questions concerning the nature of this phenomenon, we develop methods to study distinctive analytic properties of hidden minima.   By existing analyses, the Hessian spectrum of both types agree modulo $O(d^{-1/2})$-terms -- not promising. Thus, rather, our investigation proceeds by studying curves along which the loss is minimized or maximized, generally referred to as tangency 
    

