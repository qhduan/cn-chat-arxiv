# æ‘˜è¦

| Ref | Title | Summary |
| --- | --- | --- |
| [^1] | [SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem.](http://arxiv.org/abs/2309.15111) | æœ¬ç ”ç©¶é€šè¿‡åœ¨ä¸¤å±‚ç¥ç»ç½‘ç»œä¸Šä½¿ç”¨å°æ‰¹é‡SGDç®—æ³•ï¼Œåœ¨å…·æœ‰äºŒæ¬¡çœŸå®å‡½æ•°åˆ†éš”æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è®­ç»ƒæ•°é‡çº§ä¸º$d \:\text{polylog}(d)$çš„æ ·æœ¬ï¼Œå°†ç½‘ç»œè®­ç»ƒåˆ°äº†äººå£è¯¯å·®ä¸º$o(1)$çš„ç¨‹åº¦ã€‚è¿™æ˜¯é¦–æ¬¡åœ¨æ ‡å‡†ç¥ç»ç½‘ç»œä¸Šä»¥åŠæ ‡å‡†è®­ç»ƒä¸‹ï¼Œå±•ç¤ºäº†åœ¨å„å‘åŒæ€§æ•°æ®ä¸Šé«˜æ•ˆå­¦ä¹ XORå‡½æ•°çš„æ ·æœ¬å¤æ‚åº¦ä¸º$\tilde{O}(d)$ã€‚ |
| [^2] | [Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs.](http://arxiv.org/abs/2309.15096) | æœ¬æ–‡ä»ä¸¤ä¸ªæ–¹å‘å¯¹æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œç†è®ºåˆ†æï¼Œæä¾›äº†é€šè¿‡ç¥ç»åˆ‡çº¿æ ¸ï¼ˆNTKï¼‰å’Œé€šè¿‡å‡¸é‡å¡‘ReLUç½‘ç»œçš„å…¨å±€ä¼˜åŒ–è®­ç»ƒç›®æ ‡çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¸NTKç›¸è¿çš„å¤šæ ¸å­¦ä¹ æ¨¡å‹ï¼Œç§°ä¸ºé—¨æ§ReLUç½‘ç»œï¼Œé€šè¿‡åŠ æƒæ•°æ®å±è”½ç‰¹å¾æ˜ å°„æ¥å®ç°å…¨å±€ä¼˜åŒ–ã€‚ |
| [^3] | [On Excess Risk Convergence Rates of Neural Network Classifiers.](http://arxiv.org/abs/2309.15075) | æœ¬æ–‡ç ”ç©¶äº†åŸºäºç¥ç»ç½‘ç»œçš„æ’å€¼åˆ†ç±»å™¨åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­çš„æ€§èƒ½ï¼Œé€šè¿‡è¶…é¢é£é™©æ¥è¡¡é‡ã€‚ç ”ç©¶è€ƒè™‘äº†æ›´ä¸€èˆ¬çš„åœºæ™¯ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥è½»æ¾åº”ç”¨æ•°å€¼ä¼˜åŒ–æ–¹æ³•ã€‚è™½ç„¶å‡½æ•°ç±»å¾ˆå¤§ï¼Œä½†æ— ç»´åº¦é€Ÿç‡æ˜¯å¯èƒ½çš„ã€‚ |
| [^4] | [Reparameterized Variational Rejection Sampling.](http://arxiv.org/abs/2309.14612) | æœ¬æ–‡æå‡ºäº†ä¸€ç§é‡å‚æ•°åŒ–å˜åˆ†æ‹’ç»é‡‡æ ·ï¼ˆVRSï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†å‚æ•°åŒ–çš„æè®®åˆ†å¸ƒä¸æ‹’ç»é‡‡æ ·ç»“åˆï¼Œå®šä¹‰äº†ä¸€ä¸ªä¸°å¯Œçš„éå‚æ•°åˆ†å¸ƒæ—ï¼Œæ˜ç¡®åˆ©ç”¨å·²çŸ¥çš„ç›®æ ‡åˆ†å¸ƒï¼Œä¸ºå…·æœ‰è¿ç»­æ½œå˜é‡çš„æ¨¡å‹æä¾›äº†ä¸€ç§å¸å¼•äººçš„æ¨æ–­ç­–ç•¥ã€‚ |
| [^5] | [Decoding trust: A reinforcement learning perspective.](http://arxiv.org/abs/2309.14598) | è¿™é¡¹ç ”ç©¶é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ­ç¤ºäº†åœ¨æˆå¯¹åœºæ™¯ä¸­é«˜æ°´å¹³çš„ä¿¡ä»»å’Œå¯ä¿¡åº¦æ˜¯é€šè¿‡åŒæ—¶é‡è§†å†å²ç»éªŒå’Œæœªæ¥å›æŠ¥æ¥å½¢æˆçš„ã€‚ |
| [^6] | [Towards a statistical theory of data selection under weak supervision.](http://arxiv.org/abs/2309.14563) | æœ¬ç ”ç©¶é’ˆå¯¹å¼±ç›‘ç£ä¸‹çš„æ•°æ®é€‰æ‹©è¿›è¡Œäº†ç»Ÿè®¡ç†è®ºç ”ç©¶ï¼Œé€šè¿‡å®éªŒè¯æ˜æ•°æ®é€‰æ‹©å¯ä»¥éå¸¸æœ‰æ•ˆï¼Œæœ‰æ—¶ç”šè‡³å¯ä»¥æˆ˜èƒœå¯¹æ•´ä¸ªæ ·æœ¬çš„è®­ç»ƒã€‚å¹¶åˆ†æäº†åœ¨ä¸åŒæƒ…å†µä¸‹çš„æ•°æ®é€‰æ‹©é€‰æ‹©æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚ |
| [^7] | [Cluster-based Method for Eavesdropping Identification and Localization in Optical Links.](http://arxiv.org/abs/2309.14541) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å…‰çº¿ç³»ç»Ÿä¸­æ£€æµ‹å’Œå®šä½å°åŠŸç‡æŸå¤±çš„çªƒå¬äº‹ä»¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å…‰æ€§èƒ½ç›‘æµ‹æ•°æ®å¯ä»¥æ£€æµ‹è¿™ç§å¾®å°çš„çªƒå¬æŸå¤±ï¼ŒåŒæ—¶é€šè¿‡åœ¨çº¿æ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å®šä½è¿™ç±»äº‹ä»¶ã€‚ |
| [^8] | [Byzantine-Resilient Federated PCA and Low Rank Matrix Recovery.](http://arxiv.org/abs/2309.14512) | è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ‹œå åº­é²æ£’ã€é€šä¿¡é«˜æ•ˆå’Œç§å¯†çš„ç®—æ³•(Subspace-Median)æ¥è§£å†³åœ¨è”é‚¦ç¯å¢ƒä¸­ä¼°è®¡å¯¹ç§°çŸ©é˜µä¸»å­ç©ºé—´çš„é—®é¢˜ï¼ŒåŒæ—¶è¿˜ç ”ç©¶äº†è”é‚¦ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å’Œæ°´å¹³è”é‚¦ä½ç§©åˆ—æ„ŸçŸ¥ï¼ˆLRCCSï¼‰çš„ç‰¹æ®Šæƒ…å†µï¼Œå¹¶å±•ç¤ºäº†Subspace-Medianç®—æ³•çš„ä¼˜åŠ¿ã€‚ |
| [^9] | [On the expressivity of embedding quantum kernels.](http://arxiv.org/abs/2309.14419) | é‡å­æ ¸æ–¹æ³•æ˜¯é‡å­å’Œç»å…¸æœºå™¨å­¦ä¹ ä¹‹é—´æœ€è‡ªç„¶çš„è”ç³»ä¹‹ä¸€ã€‚æœ¬æ–‡æ¢è®¨äº†åµŒå…¥å¼é‡å­æ ¸çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶å¾—å‡ºç»“è®ºï¼šé€šè¿‡å¼•å…¥è®¡ç®—æ™®é€‚æ€§ï¼Œä»»ä½•æ ¸å‡½æ•°éƒ½å¯ä»¥è¡¨ç¤ºä¸ºé‡å­ç‰¹å¾æ˜ å°„å’ŒåµŒå…¥å¼é‡å­æ ¸ã€‚ |
| [^10] | [Pseudo Label Selection is a Decision Problem.](http://arxiv.org/abs/2309.13926) | ä¼ªæ ‡ç­¾é€‰æ‹©æ˜¯åŠç›‘ç£å­¦ä¹ ä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åµŒå…¥å†³ç­–ç†è®ºï¼Œæå‡ºäº†BPLSæ¡†æ¶æ¥è§£å†³ä¼ªæ ‡ç­¾é€‰æ‹©ä¸­çš„ç¡®è®¤åå·®é—®é¢˜ã€‚ |
| [^11] | [Resilient Constrained Learning.](http://arxiv.org/abs/2306.02426) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œæŠ—å¹²æ‰°çº¦æŸå­¦ä¹ â€çš„æ–¹æ³•æ¥è§£å†³åœ¨éƒ¨ç½²æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆæ—¶éœ€è¦æ»¡è¶³é™¤äº†å‡†ç¡®æ€§ä»¥å¤–çš„å¤šä¸ªè¦æ±‚ï¼Œå¹¶ä»¥å¹³è¡¡ä»æ”¾å®½ä¸­è·å¾—çš„æ€§èƒ½å¢ç›Šä¸ç”¨æˆ·å®šä¹‰çš„æ”¾å®½æˆæœ¬ä¹‹é—´çš„å…³ç³»çš„æ–¹å¼æ”¾æ¾å­¦ä¹ çº¦æŸã€‚ |
| [^12] | [Borda Regret Minimization for Generalized Linear Dueling Bandits.](http://arxiv.org/abs/2303.08816) | æœ¬æ–‡è§£å†³äº†é€šç”¨å¹¿ä¹‰çº¿æ€§å¯¹æŠ—æ€§æ’åé—®é¢˜ä¸­çš„åšå°”è¾¾åæ‚”æœ€å°åŒ–é—®é¢˜ï¼Œæå‡ºäº†é«˜åº¦è¡¨è¾¾åŠ›çš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ä¸€ç§æ–°çš„â€œå…ˆæ¢ç´¢å†æ‰§è¡Œâ€ç®—æ³•é¿å…äº†å›°éš¾çš„åæ‚”ä¸‹é™ã€‚ |
| [^13] | [Understanding the Diffusion Objective as a Weighted Integral of ELBOs.](http://arxiv.org/abs/2303.00848) | æœ¬æ–‡æ·±å…¥ç†è§£äº†æ‰©æ•£ç›®æ ‡ï¼Œå¹¶æ­ç¤ºäº†åŠ æƒæŸå¤±å’ŒELBOç›®æ ‡ä¹‹é—´çš„ç›´æ¥å…³ç³»ã€‚ |
| [^14] | [ddml: Double/debiased machine learning in Stata.](http://arxiv.org/abs/2301.09397) | ddmlæ˜¯Stataä¸­çš„åŒé‡/æ— åæœºå™¨å­¦ä¹ åŒ…ï¼Œæ”¯æŒäº”ç§ä¸åŒè®¡é‡æ¨¡å‹çš„å› æœå‚æ•°ä¼°è®¡ï¼Œå¯ä»¥çµæ´»ä¼°è®¡å†…ç”Ÿå˜é‡çš„å› æœæ•ˆåº”ï¼Œåœ¨è®¸å¤šç°æœ‰ç›‘ç£æœºå™¨å­¦ä¹ ç¨‹åºä¸­å…¼å®¹ã€‚æ¨èä¸å †å ä¼°è®¡ç»“åˆä½¿ç”¨ï¼Œæä¾›äº†è’™ç‰¹å¡æ´›è¯æ®æ”¯æŒã€‚ |
| [^15] | [Online Kernel CUSUM for Change-Point Detection.](http://arxiv.org/abs/2211.15070) | æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨çº¿å˜ç‚¹æ£€æµ‹çš„æ ¸CUSUMæ–¹æ³•ï¼Œç›¸æ¯”äºç°æœ‰æ–¹æ³•æ›´æ•æ„Ÿï¼Œæä¾›äº†å‡†ç¡®çš„å…³é”®æ€§èƒ½æŒ‡æ ‡åˆ†æï¼Œå¹¶å»ºç«‹äº†æœ€ä¼˜çª—å£é•¿åº¦ï¼Œå¼•å…¥äº†é€’å½’è®¡ç®—ç¨‹åºæ¥ç¡®ä¿è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦æ’å®šã€‚ |
| [^16] | [Finite-time analysis of single-timescale actor-critic.](http://arxiv.org/abs/2210.09921) | è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨çº¿å•æ—¶é—´å°ºåº¦æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ï¼Œé€šè¿‡çº¿æ€§å‡½æ•°é€¼è¿‘å’Œé©¬å°”å¯å¤«æ ·æœ¬æ›´æ–°ï¼Œåœ¨è¿ç»­çŠ¶æ€ç©ºé—´ä¸­æ‰¾åˆ°äº†ä¸€ä¸ª$\epsilon$-è¿‘ä¼¼çš„ç¨³å®šç‚¹ï¼Œå¹¶ä¸”åœ¨æ ·æœ¬å¤æ‚åº¦ä¸º$\widetilde{\mathcal{O}}(\epsilon^{-2})$çš„æƒ…å†µä¸‹è¯æ˜äº†å…¶æ”¶æ•›æ€§ã€‚ |
| [^17] | [Testing predictions of representation cost theory with CNNs.](http://arxiv.org/abs/2210.01257) | é€šè¿‡ç†è®ºå’Œå®éªŒè¯æ˜ï¼Œè®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å¯¹ä½é¢‘ä¿¡å·å…·æœ‰æ•æ„Ÿæ€§ï¼Œè¿™æ˜¯å› ä¸ºè‡ªç„¶å›¾åƒçš„é¢‘ç‡åˆ†å¸ƒä½¿å¤§éƒ¨åˆ†èƒ½é‡é›†ä¸­åœ¨ä½åˆ°ä¸­é¢‘ã€‚ |
| [^18] | [Combinatorial and algebraic perspectives on the marginal independence structure of Bayesian networks.](http://arxiv.org/abs/2210.00822) | æœ¬ç ”ç©¶é€šè¿‡ç»„åˆå’Œä»£æ•°è§†è§’æ¢è®¨äº†è´å¶æ–¯ç½‘ç»œçš„è¾¹é™…ç‹¬ç«‹ç»“æ„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŸºäº GrÃ¶bner åŸºç¡€çš„ MCMC æ–¹æ³• GrUESï¼Œè¯¥æ–¹æ³•åœ¨æ¢å¤çœŸå®ç»“æ„å’Œä¼°è®¡åéªŒä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚ |
| [^19] | [NN2Poly: A polynomial representation for deep feed-forward artificial neural networks.](http://arxiv.org/abs/2112.11397) | æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNN2Polyçš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå·²ç»è®­ç»ƒå¥½çš„å…¨è¿æ¥å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œçš„ç²¾ç¡®å¤šé¡¹å¼è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä»»æ„æ·±åº¦çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¸”è®¡ç®—æˆæœ¬ç›¸å¯¹è¾ƒä½ï¼Œèƒ½å¤Ÿåœ¨å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸­æä¾›éå¸¸å‡†ç¡®çš„é€¼è¿‘ç»“æœã€‚ |
| [^20] | [Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks.](http://arxiv.org/abs/2110.09548) | è·¯å¾„æ­£åˆ™åŒ–ä¸ºå¹¶è¡ŒReLUç½‘ç»œæä¾›äº†ä¸€ç§ç®€åŒ–çš„å‡¸ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡ç¾¤ç¨€ç–æ€§å¼•å¯¼å®ç°äº†å‡¸æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè¿‘ä¼¼ç®—æ³•ï¼Œåœ¨æ‰€æœ‰æ•°æ®ç»´åº¦ä¸Šå…·å¤‡å®Œå…¨å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦ã€‚ |
| [^21] | [Continuous Treatment Recommendation with Deep Survival Dose Response Function.](http://arxiv.org/abs/2108.10453) | æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨å…¬å¼ï¼Œç§°ä¸ºæ·±åº¦ç”Ÿå­˜å‰‚é‡ååº”å‡½æ•°ï¼ˆDeepSDRFï¼‰ï¼Œç”¨äºè§£å†³ä¸´åºŠç”Ÿå­˜æ•°æ®ä¸­çš„è¿ç»­æ²»ç–—æ¨èé—®é¢˜ã€‚é€šè¿‡æ ¡æ­£é€‰æ‹©åå·®ï¼ŒDeepSDRFä¼°è®¡çš„æ²»ç–—æ•ˆæœå¯ä»¥ç”¨äºå¼€å‘æ¨èç®—æ³•ã€‚åœ¨æ¨¡æ‹Ÿç ”ç©¶å’Œå®é™…åŒ»å­¦æ•°æ®åº“ä¸Šçš„æµ‹è¯•ä¸­ï¼ŒDeepSDRFè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚ |
| [^22] | [Optimal Experimental Design for Staggered Rollouts.](http://arxiv.org/abs/1911.03764) | æœ¬æ–‡ç ”ç©¶äº†éš”å¼€å¼è¯•éªŒçš„æœ€ä¼˜è®¾è®¡é—®é¢˜ã€‚å¯¹äºéè‡ªé€‚åº”å®éªŒï¼Œæå‡ºäº†ä¸€ä¸ªè¿‘ä¼¼æœ€ä¼˜è§£ï¼›å¯¹äºè‡ªé€‚åº”å®éªŒï¼Œæå‡ºäº†ä¸€ç§æ–°ç®—æ³•â€”â€”ç²¾åº¦å¯¼å‘çš„è‡ªé€‚åº”å®éªŒï¼ˆPGAEï¼‰ç®—æ³•ï¼Œå®ƒä½¿ç”¨è´å¶æ–¯å†³ç­–ç†è®ºæ¥æœ€å¤§åŒ–ä¼°è®¡æ²»ç–—æ•ˆæœçš„é¢„æœŸç²¾åº¦ã€‚ |

# è¯¦ç»†

[^1]: SGDåœ¨å…·æœ‰æ¥è¿‘æœ€ä¼˜æ ·æœ¬å¤æ‚åº¦çš„åŒå±‚ç¥ç»ç½‘ç»œä¸­å¯»æ‰¾å¹¶è°ƒæ•´ç‰¹å¾ï¼šä»¥XORé—®é¢˜ä¸ºæ¡ˆä¾‹ç ”ç©¶

    SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem. (arXiv:2309.15111v1 [cs.LG])

    [http://arxiv.org/abs/2309.15111](http://arxiv.org/abs/2309.15111)

    æœ¬ç ”ç©¶é€šè¿‡åœ¨ä¸¤å±‚ç¥ç»ç½‘ç»œä¸Šä½¿ç”¨å°æ‰¹é‡SGDç®—æ³•ï¼Œåœ¨å…·æœ‰äºŒæ¬¡çœŸå®å‡½æ•°åˆ†éš”æ•°æ®çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡è®­ç»ƒæ•°é‡çº§ä¸º$d \:\text{polylog}(d)$çš„æ ·æœ¬ï¼Œå°†ç½‘ç»œè®­ç»ƒåˆ°äº†äººå£è¯¯å·®ä¸º$o(1)$çš„ç¨‹åº¦ã€‚è¿™æ˜¯é¦–æ¬¡åœ¨æ ‡å‡†ç¥ç»ç½‘ç»œä¸Šä»¥åŠæ ‡å‡†è®­ç»ƒä¸‹ï¼Œå±•ç¤ºäº†åœ¨å„å‘åŒæ€§æ•°æ®ä¸Šé«˜æ•ˆå­¦ä¹ XORå‡½æ•°çš„æ ·æœ¬å¤æ‚åº¦ä¸º$\tilde{O}(d)$ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™ï¼ˆSGDï¼‰åœ¨å…·æœ‰äºŒæ¬¡çœŸå®å‡½æ•°åˆ†éš”æ•°æ®çš„åŒå±‚ç¥ç»ç½‘ç»œä¸Šçš„ä¼˜åŒ–è¿‡ç¨‹ã€‚æˆ‘ä»¬è¯æ˜ï¼Œå¯¹äºä»$d$ç»´å¸ƒå°”è¶…ç«‹æ–¹ä½“ä¸­ç”±äºŒæ¬¡â€œXORâ€å‡½æ•°$y = -x_ix_j$æ ‡è®°çš„æ•°æ®ï¼Œå¯ä»¥é€šè¿‡æ ‡å‡†å°æ‰¹é‡SGDåœ¨é€»è¾‘æŸå¤±ä¸ŠåŒæ—¶è®­ç»ƒä¸¤å±‚ReLUæ¿€æ´»çš„åŒå±‚ç¥ç»ç½‘ç»œï¼Œç”¨$d \:\text{polylog}(d)$ä¸ªæ ·æœ¬å°†å…¶è®­ç»ƒåˆ°äººå£è¯¯å·®ä¸º$o(1)$çš„ç¨‹åº¦ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡ç»™å‡ºäº†åœ¨æ ‡å‡†ç¥ç»ç½‘ç»œä¸Šä»¥åŠæ ‡å‡†è®­ç»ƒä¸‹ï¼Œå¯¹äºåœ¨å„å‘åŒæ€§æ•°æ®ä¸Šé«˜æ•ˆå­¦ä¹ XORå‡½æ•°çš„æ ·æœ¬å¤æ‚åº¦ä¸º$\tilde{O}(d)$ã€‚æˆ‘ä»¬çš„ä¸»è¦æŠ€æœ¯æ˜¯å±•ç¤ºç½‘ç»œæ¼”åŒ–æœ‰ä¸¤ä¸ªé˜¶æ®µï¼šä¸€ä¸ªâ€ä¿¡å·å‘ç°â€œé˜¶æ®µï¼Œåœ¨æ­¤ç½‘ç»œè§„æ¨¡è¾ƒå°ä¸”è®¸å¤šç¥ç»å…ƒç‹¬ç«‹æ¼”åŒ–ä»¥å¯»æ‰¾ç‰¹å¾ï¼Œä»¥åŠä¸€ä¸ªâ€ä¿¡å·å¯†é›†â€œé˜¶æ®µï¼Œå…¶ä¸­è®¸å¤šç¥ç»å…ƒç›¸äº’ä½œç”¨ä»¥ä¼˜åŒ–é¢„æµ‹ã€‚

    In this work, we consider the optimization process of minibatch stochastic gradient descent (SGD) on a 2-layer neural network with data separated by a quadratic ground truth function. We prove that with data drawn from the $d$-dimensional Boolean hypercube labeled by the quadratic ``XOR'' function $y = -x_ix_j$, it is possible to train to a population error $o(1)$ with $d \:\text{polylog}(d)$ samples. Our result considers simultaneously training both layers of the two-layer-neural network with ReLU activations via standard minibatch SGD on the logistic loss. To our knowledge, this work is the first to give a sample complexity of $\tilde{O}(d)$ for efficiently learning the XOR function on isotropic data on a standard neural network with standard training. Our main technique is showing that the network evolves in two phases: a $\textit{signal-finding}$ phase where the network is small and many of the neurons evolve independently to find features, and a $\textit{signal-heavy}$ phase, wher
    
[^2]: ä¿®å¤NTKï¼šä»ç¥ç»ç½‘ç»œçº¿æ€§åŒ–åˆ°ç²¾ç¡®çš„å‡¸ç¨‹åº

    Fixing the NTK: From Neural Network Linearizations to Exact Convex Programs. (arXiv:2309.15096v1 [cs.LG])

    [http://arxiv.org/abs/2309.15096](http://arxiv.org/abs/2309.15096)

    æœ¬æ–‡ä»ä¸¤ä¸ªæ–¹å‘å¯¹æ·±åº¦ç¥ç»ç½‘ç»œè¿›è¡Œç†è®ºåˆ†æï¼Œæä¾›äº†é€šè¿‡ç¥ç»åˆ‡çº¿æ ¸ï¼ˆNTKï¼‰å’Œé€šè¿‡å‡¸é‡å¡‘ReLUç½‘ç»œçš„å…¨å±€ä¼˜åŒ–è®­ç»ƒç›®æ ‡çš„æ–¹æ³•ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬è¿˜æå‡ºäº†ä¸€ç§ä¸NTKç›¸è¿çš„å¤šæ ¸å­¦ä¹ æ¨¡å‹ï¼Œç§°ä¸ºé—¨æ§ReLUç½‘ç»œï¼Œé€šè¿‡åŠ æƒæ•°æ®å±è”½ç‰¹å¾æ˜ å°„æ¥å®ç°å…¨å±€ä¼˜åŒ–ã€‚

    

    æœ€è¿‘ï¼Œæ·±åº¦ç¥ç»ç½‘ç»œçš„ç†è®ºåˆ†æä¸»è¦é›†ä¸­åœ¨ä¸¤ä¸ªæ–¹å‘ä¸Šï¼š1ï¼‰é€šè¿‡åœ¨éšè—å±‚å®½åº¦æ— é™å¤§å’Œå­¦ä¹ ç‡æ— ç©·å°çš„æƒ…å†µä¸‹è¿›è¡Œçš„SGDè®­ç»ƒçš„ç†è®ºæ´å¯ŸåŠ›ï¼ˆä¹Ÿç§°ä¸ºæ¢¯åº¦æµï¼‰é€šè¿‡ç¥ç»åˆ‡çº¿æ ¸ï¼ˆNTKï¼‰ï¼›2ï¼‰é€šè¿‡é”¥çº¦æŸå‡¸é‡å¡‘ReLUç½‘ç»œçš„å…¨å±€ä¼˜åŒ–è®­ç»ƒç›®æ ‡ã€‚åä¸€ç§ç ”ç©¶æ–¹å‘è¿˜æä¾›äº†ReLUç½‘ç»œçš„å¦ä¸€ç§å…¬å¼ï¼Œç§°ä¸ºé—¨æ§ReLUç½‘ç»œï¼Œå¯é€šè¿‡é«˜æ•ˆçš„æ— çº¦æŸå‡¸ç¨‹åºè¿›è¡Œå…¨å±€ä¼˜åŒ–ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†é—¨æ§ReLUç½‘ç»œçš„å‡¸é—®é¢˜è§£é‡Šä¸ºå…·æœ‰åŠ æƒæ•°æ®å±è”½ç‰¹å¾æ˜ å°„çš„å¤šæ ¸å­¦ä¹ ï¼ˆMKLï¼‰æ¨¡å‹ï¼Œå¹¶ä¸NTKå»ºç«‹äº†è¿æ¥ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬è¯æ˜äº†å¯¹äºé‚£äº›ä¸å­¦ä¹ ç›®æ ‡æ— å…³çš„ç‰¹å®šé€‰æ‹©çš„æ©ç æƒé‡ï¼Œè¯¥æ ¸ç­‰æ•ˆäºé—¨æ§ReLUç½‘ç»œåœ¨è®­ç»ƒæ ·æœ¬ä¸Šçš„NTKã€‚

    Recently, theoretical analyses of deep neural networks have broadly focused on two directions: 1) Providing insight into neural network training by SGD in the limit of infinite hidden-layer width and infinitesimally small learning rate (also known as gradient flow) via the Neural Tangent Kernel (NTK), and 2) Globally optimizing the regularized training objective via cone-constrained convex reformulations of ReLU networks. The latter research direction also yielded an alternative formulation of the ReLU network, called a gated ReLU network, that is globally optimizable via efficient unconstrained convex programs. In this work, we interpret the convex program for this gated ReLU network as a Multiple Kernel Learning (MKL) model with a weighted data masking feature map and establish a connection to the NTK. Specifically, we show that for a particular choice of mask weights that do not depend on the learning targets, this kernel is equivalent to the NTK of the gated ReLU network on the tra
    
[^3]: å…³äºç¥ç»ç½‘ç»œåˆ†ç±»å™¨è¶…é¢é£é™©æ”¶æ•›é€Ÿç‡çš„ç ”ç©¶

    On Excess Risk Convergence Rates of Neural Network Classifiers. (arXiv:2309.15075v1 [stat.ML])

    [http://arxiv.org/abs/2309.15075](http://arxiv.org/abs/2309.15075)

    æœ¬æ–‡ç ”ç©¶äº†åŸºäºç¥ç»ç½‘ç»œçš„æ’å€¼åˆ†ç±»å™¨åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­çš„æ€§èƒ½ï¼Œé€šè¿‡è¶…é¢é£é™©æ¥è¡¡é‡ã€‚ç ”ç©¶è€ƒè™‘äº†æ›´ä¸€èˆ¬çš„åœºæ™¯ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥è½»æ¾åº”ç”¨æ•°å€¼ä¼˜åŒ–æ–¹æ³•ã€‚è™½ç„¶å‡½æ•°ç±»å¾ˆå¤§ï¼Œä½†æ— ç»´åº¦é€Ÿç‡æ˜¯å¯èƒ½çš„ã€‚

    

    æœ€è¿‘ç¥ç»ç½‘ç»œåœ¨æ¨¡å¼è¯†åˆ«å’Œåˆ†ç±»é—®é¢˜ä¸Šçš„æˆåŠŸè¡¨æ˜ï¼Œä¸å…¶ä»–æ›´ç»å…¸çš„åˆ†ç±»å™¨ï¼ˆå¦‚SVMæˆ–boostingåˆ†ç±»å™¨ï¼‰ç›¸æ¯”ï¼Œç¥ç»ç½‘ç»œå…·æœ‰ç‹¬ç‰¹çš„ç‰¹ç‚¹ã€‚æœ¬æ–‡ç ”ç©¶äº†åŸºäºç¥ç»ç½‘ç»œçš„æ’å€¼åˆ†ç±»å™¨åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­çš„æ€§èƒ½ï¼Œé€šè¿‡å…¶è¶…é¢é£é™©æ¥è¡¡é‡ã€‚ä¸æ–‡çŒ®ä¸­æ‰€è§„å®šçš„å…¸å‹æ¡ä»¶ç›¸æ¯”ï¼Œæˆ‘ä»¬è€ƒè™‘äº†ä¸€ä¸ªæ›´ä¸€èˆ¬çš„åœºæ™¯ï¼Œå®ƒåœ¨ä¸¤ä¸ªæ–¹é¢ä¸å®é™…åº”ç”¨ç±»ä¼¼ï¼šé¦–å…ˆï¼Œè¦è¿‘ä¼¼çš„å‡½æ•°ç±»åŒ…æ‹¬äº†Barronå‡½æ•°ä½œä¸ºæ­£å­é›†ï¼›å…¶æ¬¡ï¼Œæ„å»ºçš„ç¥ç»ç½‘ç»œåˆ†ç±»å™¨æ˜¯é€šè¿‡æœ€å°åŒ–ä¸€ä¸ªæ›¿ä»£æŸå¤±å‡½æ•°è€Œä¸æ˜¯0-1æŸå¤±å‡½æ•°æ¥å®ç°çš„ï¼Œä»è€Œå¯ä»¥è½»æ¾åº”ç”¨åŸºäºæ¢¯åº¦ä¸‹é™çš„æ•°å€¼ä¼˜åŒ–æ–¹æ³•ã€‚è™½ç„¶æˆ‘ä»¬è€ƒè™‘çš„å‡½æ•°ç±»éå¸¸å¤§ï¼Œæœ€ä¼˜é€Ÿç‡ä¸èƒ½è¶…è¿‡$n^{-\frac{1}{3}}$ï¼Œä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ— ç»´åº¦é€Ÿç‡æ˜¯å¯èƒ½çš„ã€‚

    The recent success of neural networks in pattern recognition and classification problems suggests that neural networks possess qualities distinct from other more classical classifiers such as SVMs or boosting classifiers. This paper studies the performance of plug-in classifiers based on neural networks in a binary classification setting as measured by their excess risks. Compared to the typical settings imposed in the literature, we consider a more general scenario that resembles actual practice in two respects: first, the function class to be approximated includes the Barron functions as a proper subset, and second, the neural network classifier constructed is the minimizer of a surrogate loss instead of the $0$-$1$ loss so that gradient descent-based numerical optimizations can be easily applied. While the class of functions we consider is quite large that optimal rates cannot be faster than $n^{-\frac{1}{3}}$, it is a regime in which dimension-free rates are possible and approximat
    
[^4]: é‡å‚æ•°åŒ–å˜åˆ†æ‹’ç»é‡‡æ ·

    Reparameterized Variational Rejection Sampling. (arXiv:2309.14612v1 [stat.ML])

    [http://arxiv.org/abs/2309.14612](http://arxiv.org/abs/2309.14612)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§é‡å‚æ•°åŒ–å˜åˆ†æ‹’ç»é‡‡æ ·ï¼ˆVRSï¼‰æ–¹æ³•ï¼Œé€šè¿‡å°†å‚æ•°åŒ–çš„æè®®åˆ†å¸ƒä¸æ‹’ç»é‡‡æ ·ç»“åˆï¼Œå®šä¹‰äº†ä¸€ä¸ªä¸°å¯Œçš„éå‚æ•°åˆ†å¸ƒæ—ï¼Œæ˜ç¡®åˆ©ç”¨å·²çŸ¥çš„ç›®æ ‡åˆ†å¸ƒï¼Œä¸ºå…·æœ‰è¿ç»­æ½œå˜é‡çš„æ¨¡å‹æä¾›äº†ä¸€ç§å¸å¼•äººçš„æ¨æ–­ç­–ç•¥ã€‚

    

    ä¼ ç»Ÿçš„å˜åˆ†æ¨æ–­æ–¹æ³•ä¾èµ–äºå‚æ•°åŒ–çš„å˜åˆ†åˆ†å¸ƒæ—ï¼Œé€‰æ‹©çš„åˆ†å¸ƒæ—åœ¨ç¡®å®šåéªŒè¿‘ä¼¼çš„å‡†ç¡®æ€§æ–¹é¢èµ·ç€å…³é”®ä½œç”¨ã€‚ç®€å•çš„mean-fieldåˆ†å¸ƒæ—é€šå¸¸å¯¼è‡´è¾ƒå·®çš„è¿‘ä¼¼ï¼Œè€Œåƒå½’ä¸€åŒ–æµè¿™æ ·çš„ä¸°å¯Œåˆ†å¸ƒæ—å¾€å¾€éš¾ä»¥ä¼˜åŒ–ï¼Œå¹¶ä¸”é€šå¸¸ä¸åŒ…å«å·²çŸ¥ç›®æ ‡åˆ†å¸ƒçš„ç»“æ„ï¼Œå› ä¸ºå…¶æ˜¯é»‘ç®±çš„ã€‚ä¸ºäº†æ‰©å±•çµæ´»çš„å˜åˆ†åˆ†å¸ƒæ—ç©ºé—´ï¼Œæˆ‘ä»¬é‡æ–°è€ƒè™‘å˜åˆ†æ‹’ç»é‡‡æ ·ï¼ˆVRSï¼‰[Grover et al., 2018]ï¼Œå®ƒå°†å‚æ•°åŒ–æè®®åˆ†å¸ƒä¸æ‹’ç»é‡‡æ ·ç»“åˆèµ·æ¥ï¼Œå®šä¹‰äº†ä¸€ä¸ªä¸°å¯Œçš„éå‚æ•°åˆ†å¸ƒæ—ï¼Œæ˜ç¡®åˆ©ç”¨å·²çŸ¥çš„ç›®æ ‡åˆ†å¸ƒã€‚é€šè¿‡å¼•å…¥å¯¹æè®®åˆ†å¸ƒå‚æ•°çš„ä½æ–¹å·®é‡å‚æ•°åŒ–æ¢¯åº¦ä¼°è®¡å™¨ï¼Œæˆ‘ä»¬ä½¿VRSæˆä¸ºå…·æœ‰è¿ç»­æ½œå˜é‡çš„å¸å¼•äººçš„æ¨æ–­ç­–ç•¥ã€‚

    Traditional approaches to variational inference rely on parametric families of variational distributions, with the choice of family playing a critical role in determining the accuracy of the resulting posterior approximation. Simple mean-field families often lead to poor approximations, while rich families of distributions like normalizing flows can be difficult to optimize and usually do not incorporate the known structure of the target distribution due to their black-box nature. To expand the space of flexible variational families, we revisit Variational Rejection Sampling (VRS) [Grover et al., 2018], which combines a parametric proposal distribution with rejection sampling to define a rich non-parametric family of distributions that explicitly utilizes the known target distribution. By introducing a low-variance reparameterized gradient estimator for the parameters of the proposal distribution, we make VRS an attractive inference strategy for models with continuous latent variables.
    
[^5]: è§£è¯»ä¿¡ä»»:å¼ºåŒ–å­¦ä¹ è§†è§’

    Decoding trust: A reinforcement learning perspective. (arXiv:2309.14598v1 [q-bio.PE])

    [http://arxiv.org/abs/2309.14598](http://arxiv.org/abs/2309.14598)

    è¿™é¡¹ç ”ç©¶é€šè¿‡ä½¿ç”¨å¼ºåŒ–å­¦ä¹ ç®—æ³•æ­ç¤ºäº†åœ¨æˆå¯¹åœºæ™¯ä¸­é«˜æ°´å¹³çš„ä¿¡ä»»å’Œå¯ä¿¡åº¦æ˜¯é€šè¿‡åŒæ—¶é‡è§†å†å²ç»éªŒå’Œæœªæ¥å›æŠ¥æ¥å½¢æˆçš„ã€‚

    

    å¯¹ä¿¡ä»»æ¸¸æˆçš„è¡Œä¸ºå®éªŒè¡¨æ˜ï¼Œä¿¡ä»»å’Œå¯ä¿¡åº¦åœ¨äººç±»ä¸­æ™®éå­˜åœ¨ï¼Œè¿™ä¸æ­£ç»Ÿç»æµå­¦ä¸­å‡è®¾çš„ç»æµäººçš„é¢„æµ‹ç›¸çŸ›ç›¾ã€‚è¿™æ„å‘³ç€ä¸€å®šå­˜åœ¨æŸç§æœºåˆ¶ä¿ƒä½¿ä»–ä»¬çš„å‡ºç°ã€‚ç„¶è€Œï¼Œå¤§å¤šæ•°å…ˆå‰çš„è§£é‡Šéƒ½éœ€è¦ä¾èµ–äºä¸€äº›åŸºäºæ¨¡ä»¿å­¦ä¹ çš„å› ç´ ï¼Œå³ç®€å•ç‰ˆæœ¬çš„ç¤¾ä¼šå­¦ä¹ ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è½¬å‘å¼ºåŒ–å­¦ä¹ çš„èŒƒå¼ï¼Œä¸ªä½“é€šè¿‡ç´¯ç§¯ç»éªŒè¯„ä¼°é•¿æœŸå›æŠ¥æ¥æ›´æ–°ä»–ä»¬çš„ç­–ç•¥ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬ä½¿ç”¨Q-learningç®—æ³•ç ”ç©¶ä¿¡ä»»æ¸¸æˆï¼Œæ¯ä¸ªå‚ä¸è€…åˆ†åˆ«ä¸ä¸¤ä¸ªä¸æ–­æ¼”åŒ–çš„Qè¡¨å…³è”ï¼ŒæŒ‡å¯¼ä»–ä»¬ä½œä¸ºä¿¡ä»»è€…å’Œæ‰˜ç®¡æ–¹çš„å†³ç­–ã€‚åœ¨æˆå¯¹çš„åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å‘ç°å½“ä¸ªä½“åŒæ—¶é‡è§†å†å²ç»éªŒå’Œæœªæ¥å›æŠ¥æ—¶ï¼Œä¿¡ä»»å’Œå¯ä¿¡åº¦æ°´å¹³è¾ƒé«˜ã€‚ä»æœºåˆ¶ä¸Šçœ‹ï¼ŒQçš„æ¼”åŒ–...

    Behavioral experiments on the trust game have shown that trust and trustworthiness are universal among human beings, contradicting the prediction by assuming \emph{Homo economicus} in orthodox Economics. This means some mechanism must be at work that favors their emergence. Most previous explanations however need to resort to some factors based upon imitative learning, a simple version of social learning. Here, we turn to the paradigm of reinforcement learning, where individuals update their strategies by evaluating the long-term return through accumulated experience. Specifically, we investigate the trust game with the Q-learning algorithm, where each participant is associated with two evolving Q-tables that guide one's decision making as trustor and trustee respectively. In the pairwise scenario, we reveal that high levels of trust and trustworthiness emerge when individuals appreciate both their historical experience and returns in the future. Mechanistically, the evolution of the Q
    
[^6]: é¢å‘å¼±ç›‘ç£ä¸‹çš„æ•°æ®é€‰æ‹©ç»Ÿè®¡ç†è®º

    Towards a statistical theory of data selection under weak supervision. (arXiv:2309.14563v1 [stat.ML])

    [http://arxiv.org/abs/2309.14563](http://arxiv.org/abs/2309.14563)

    æœ¬ç ”ç©¶é’ˆå¯¹å¼±ç›‘ç£ä¸‹çš„æ•°æ®é€‰æ‹©è¿›è¡Œäº†ç»Ÿè®¡ç†è®ºç ”ç©¶ï¼Œé€šè¿‡å®éªŒè¯æ˜æ•°æ®é€‰æ‹©å¯ä»¥éå¸¸æœ‰æ•ˆï¼Œæœ‰æ—¶ç”šè‡³å¯ä»¥æˆ˜èƒœå¯¹æ•´ä¸ªæ ·æœ¬çš„è®­ç»ƒã€‚å¹¶åˆ†æäº†åœ¨ä¸åŒæƒ…å†µä¸‹çš„æ•°æ®é€‰æ‹©é€‰æ‹©æ–¹æ³•çš„æœ‰æ•ˆæ€§ã€‚

    

    å¯¹äºä¸€ä¸ªå¤§å°ä¸ºNçš„æ ·æœ¬ï¼Œé€‰æ‹©ä¸€ä¸ªæ›´å°çš„å¤§å°n<Nçš„å­æ ·æœ¬ç”¨äºç»Ÿè®¡ä¼°è®¡æˆ–å­¦ä¹ é€šå¸¸æ˜¯æœ‰ç”¨çš„ã€‚è¿™æ ·çš„æ•°æ®é€‰æ‹©æ­¥éª¤æœ‰åŠ©äºå‡å°‘æ•°æ®æ ‡è®°çš„è¦æ±‚å’Œå­¦ä¹ çš„è®¡ç®—å¤æ‚æ€§ã€‚æˆ‘ä»¬å‡è®¾ç»™å®šäº†Nä¸ªæœªæ ‡è®°çš„æ ·æœ¬{x_i}ï¼Œå¹¶ä¸”å¯ä»¥è®¿é—®ä¸€ä¸ªâ€œæ›¿ä»£æ¨¡å‹â€ï¼Œå®ƒå¯ä»¥æ¯”éšæœºçŒœæµ‹æ›´å¥½åœ°é¢„æµ‹æ ‡ç­¾y_iã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€‰æ‹©ä¸€ä¸ªå­æ ·æœ¬é›†{ğ±_i}ï¼Œå…¶å¤§å°ä¸º|G|=n<Nã€‚ç„¶åæˆ‘ä»¬ä¸ºè¿™ä¸ªé›†åˆè·å–æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬é€šè¿‡æ­£åˆ™åŒ–ç»éªŒé£é™©æœ€å°åŒ–æ¥è®­ç»ƒæ¨¡å‹ã€‚é€šè¿‡åœ¨çœŸå®å’Œåˆæˆæ•°æ®ä¸Šè¿›è¡Œæ··åˆçš„æ•°å€¼å®éªŒï¼Œå¹¶åœ¨ä½ç»´å’Œé«˜ç»´æ¸è¿‘æƒ…å†µä¸‹è¿›è¡Œæ•°å­¦æ¨å¯¼ï¼Œæˆ‘ä»¬è¯æ˜ï¼š(i) æ•°æ®é€‰æ‹©å¯ä»¥éå¸¸æœ‰æ•ˆï¼Œç‰¹åˆ«æ˜¯åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥å‡»è´¥å¯¹æ•´ä¸ªæ ·æœ¬çš„è®­ç»ƒï¼›(ii) åœ¨æ•°æ®é€‰æ‹©æ–¹é¢ï¼ŒæŸäº›æµè¡Œçš„é€‰æ‹©åœ¨ä¸€äº›æƒ…å†µä¸‹æ˜¯æœ‰æ•ˆçš„ï¼Œè€Œåœ¨å…¶ä»–æƒ…å†µä¸‹åˆ™ä¸æ˜¯ã€‚

    Given a sample of size $N$, it is often useful to select a subsample of smaller size $n<N$ to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given $N$ unlabeled samples $\{{\boldsymbol x}_i\}_{i\le N}$, and to be given access to a `surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by $\{{\boldsymbol x}_i\}_{i\in G}$, of size $|G|=n<N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization.  By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: $(i)$~Data selection can be very effective, in particular beating training on the full sample in some cases; $(ii)$~Certain popular choices in data selecti
    
[^7]: å…‰é“¾è·¯ä¸­çš„çªƒå¬è¯†åˆ«å’Œå®šä½çš„åŸºäºèšç±»çš„æ–¹æ³•

    Cluster-based Method for Eavesdropping Identification and Localization in Optical Links. (arXiv:2309.14541v1 [stat.ML])

    [http://arxiv.org/abs/2309.14541](http://arxiv.org/abs/2309.14541)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ³•ï¼Œç”¨äºåœ¨å…‰çº¿ç³»ç»Ÿä¸­æ£€æµ‹å’Œå®šä½å°åŠŸç‡æŸå¤±çš„çªƒå¬äº‹ä»¶ã€‚ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œé€šè¿‡å…‰æ€§èƒ½ç›‘æµ‹æ•°æ®å¯ä»¥æ£€æµ‹è¿™ç§å¾®å°çš„çªƒå¬æŸå¤±ï¼ŒåŒæ—¶é€šè¿‡åœ¨çº¿æ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å®šä½è¿™ç±»äº‹ä»¶ã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºèšç±»çš„æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å’Œå®šä½å…‰çº¿ç³»ç»Ÿä¸­å°åŠŸç‡æŸå¤±çš„çªƒå¬äº‹ä»¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶ç»“æœè¡¨æ˜ï¼Œå…‰æ€§èƒ½ç›‘æµ‹ï¼ˆOPMï¼‰æ•°æ®ä»…é€šè¿‡æ¥æ”¶å™¨æ”¶é›†å°±å¯ä»¥æ£€æµ‹åˆ°è¿™ç§å¾®å°çš„çªƒå¬æŸå¤±ã€‚å¦ä¸€æ–¹é¢ï¼Œé€šè¿‡åˆ©ç”¨åœ¨çº¿OPMæ•°æ®å¯ä»¥æœ‰æ•ˆåœ°å®ç°å¯¹è¿™ç±»äº‹ä»¶çš„å®šä½ã€‚

    We propose a cluster-based method to detect and locate eavesdropping events in optical line systems characterized by small power losses. Our findings indicate that detecting such subtle losses from eavesdropping can be accomplished solely through optical performance monitoring (OPM) data collected at the receiver. On the other hand, the localization of such events can be effectively achieved by leveraging in-line OPM data.
    
[^8]: æ‹œå åº­é²æ£’çš„è”é‚¦PCAå’Œä½ç§©çŸ©é˜µæ¢å¤

    Byzantine-Resilient Federated PCA and Low Rank Matrix Recovery. (arXiv:2309.14512v1 [cs.IT])

    [http://arxiv.org/abs/2309.14512](http://arxiv.org/abs/2309.14512)

    è¿™é¡¹å·¥ä½œæå‡ºäº†ä¸€ä¸ªæ‹œå åº­é²æ£’ã€é€šä¿¡é«˜æ•ˆå’Œç§å¯†çš„ç®—æ³•(Subspace-Median)æ¥è§£å†³åœ¨è”é‚¦ç¯å¢ƒä¸­ä¼°è®¡å¯¹ç§°çŸ©é˜µä¸»å­ç©ºé—´çš„é—®é¢˜ï¼ŒåŒæ—¶è¿˜ç ”ç©¶äº†è”é‚¦ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å’Œæ°´å¹³è”é‚¦ä½ç§©åˆ—æ„ŸçŸ¥ï¼ˆLRCCSï¼‰çš„ç‰¹æ®Šæƒ…å†µï¼Œå¹¶å±•ç¤ºäº†Subspace-Medianç®—æ³•çš„ä¼˜åŠ¿ã€‚

    

    åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†åœ¨è”é‚¦ç¯å¢ƒä¸­ä¼°è®¡å¯¹ç§°çŸ©é˜µçš„ä¸»å­ç©ºé—´ï¼ˆå‰rä¸ªå¥‡å¼‚å‘é‡çš„å¼ æˆï¼‰çš„é—®é¢˜ï¼Œå½“æ¯ä¸ªèŠ‚ç‚¹éƒ½å¯ä»¥è®¿é—®å¯¹è¿™ä¸ªçŸ©é˜µçš„ä¼°è®¡æ—¶ã€‚æˆ‘ä»¬ç ”ç©¶å¦‚ä½•ä½¿è¿™ä¸ªé—®é¢˜å…·æœ‰æ‹œå åº­é²æ£’æ€§ã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ç§æ–°é¢–çš„å¯è¯æ˜çš„æ‹œå åº­é²æ£’ã€é€šä¿¡é«˜æ•ˆå’Œç§å¯†çš„ç®—æ³•ï¼Œç§°ä¸ºå­ç©ºé—´ä¸­å€¼ç®—æ³•ï¼ˆSubspace-Medianï¼‰ï¼Œç”¨äºè§£å†³è¿™ä¸ªé—®é¢˜ã€‚æˆ‘ä»¬è¿˜ç ”ç©¶äº†è¿™ä¸ªé—®é¢˜çš„æœ€è‡ªç„¶çš„è§£æ³•ï¼ŒåŸºäºå‡ ä½•ä¸­å€¼çš„ä¿®æ”¹çš„è”é‚¦å¹‚æ–¹æ³•ï¼Œå¹¶è§£é‡Šä¸ºä»€ä¹ˆå®ƒæ˜¯æ— ç”¨çš„ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è€ƒè™‘äº†é²æ£’å­ç©ºé—´ä¼°è®¡å…ƒé—®é¢˜çš„ä¸¤ä¸ªç‰¹æ®Šæƒ…å†µ - è”é‚¦ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰å’Œæ°´å¹³è”é‚¦ä½ç§©åˆ—æ„ŸçŸ¥ï¼ˆLRCCSï¼‰çš„è°±åˆå§‹åŒ–æ­¥éª¤ã€‚å¯¹äºè¿™ä¸¤ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬å±•ç¤ºäº†å­ç©ºé—´ä¸­å€¼ç®—æ³•æä¾›äº†æ—¢å…·æœ‰é²æ£’æ€§åˆå…·æœ‰é«˜é€šä¿¡æ•ˆç‡çš„è§£å†³æ–¹æ¡ˆã€‚å‡å€¼çš„ä¸­ä½æ•°æ‰©å±•ä¹Ÿè¢«å¼€å‘å‡ºæ¥äº†ã€‚

    In this work we consider the problem of estimating the principal subspace (span of the top r singular vectors) of a symmetric matrix in a federated setting, when each node has access to estimates of this matrix. We study how to make this problem Byzantine resilient. We introduce a novel provably Byzantine-resilient, communication-efficient, and private algorithm, called Subspace-Median, to solve it. We also study the most natural solution for this problem, a geometric median based modification of the federated power method, and explain why it is not useful. We consider two special cases of the resilient subspace estimation meta-problem - federated principal components analysis (PCA) and the spectral initialization step of horizontally federated low rank column-wise sensing (LRCCS) in this work. For both these problems we show how Subspace Median provides a resilient solution that is also communication-efficient. Median of Means extensions are developed for both problems. Extensive simu
    
[^9]: å…³äºåµŒå…¥å¼é‡å­æ ¸çš„è¡¨è¾¾èƒ½åŠ›

    On the expressivity of embedding quantum kernels. (arXiv:2309.14419v1 [quant-ph])

    [http://arxiv.org/abs/2309.14419](http://arxiv.org/abs/2309.14419)

    é‡å­æ ¸æ–¹æ³•æ˜¯é‡å­å’Œç»å…¸æœºå™¨å­¦ä¹ ä¹‹é—´æœ€è‡ªç„¶çš„è”ç³»ä¹‹ä¸€ã€‚æœ¬æ–‡æ¢è®¨äº†åµŒå…¥å¼é‡å­æ ¸çš„è¡¨è¾¾èƒ½åŠ›ï¼Œå¹¶å¾—å‡ºç»“è®ºï¼šé€šè¿‡å¼•å…¥è®¡ç®—æ™®é€‚æ€§ï¼Œä»»ä½•æ ¸å‡½æ•°éƒ½å¯ä»¥è¡¨ç¤ºä¸ºé‡å­ç‰¹å¾æ˜ å°„å’ŒåµŒå…¥å¼é‡å­æ ¸ã€‚

    

    åœ¨æ ¸æ–¹æ³•çš„èƒŒæ™¯ä¸‹ï¼Œé‡å­æ ¸ä¸ç»å…¸æœºå™¨å­¦ä¹ ä¹‹é—´å»ºç«‹äº†æœ€è‡ªç„¶çš„è”ç³»ã€‚æ ¸æ–¹æ³•ä¾èµ–äºå†…ç§¯ç‰¹å¾å‘é‡ï¼Œè¿™äº›ç‰¹å¾å‘é‡å­˜åœ¨äºå¤§å‹ç‰¹å¾ç©ºé—´ä¸­ã€‚é‡å­æ ¸é€šå¸¸é€šè¿‡æ˜¾å¼æ„é€ é‡å­ç‰¹å¾æ€å¹¶è®¡ç®—å®ƒä»¬çš„å†…ç§¯æ¥è¯„ä¼°ï¼Œè¿™é‡Œç§°ä¸ºåµŒå…¥å¼é‡å­æ ¸ã€‚ç”±äºç»å…¸æ ¸é€šå¸¸åœ¨ä¸ä½¿ç”¨ç‰¹å¾å‘é‡çš„æƒ…å†µä¸‹è¿›è¡Œè¯„ä¼°ï¼Œæˆ‘ä»¬æƒ³çŸ¥é“åµŒå…¥å¼é‡å­æ ¸çš„è¡¨è¾¾èƒ½åŠ›å¦‚ä½•ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåŸºæœ¬é—®é¢˜ï¼šæ˜¯å¦æ‰€æœ‰çš„é‡å­æ ¸éƒ½å¯ä»¥è¡¨è¾¾ä¸ºé‡å­ç‰¹å¾æ€çš„å†…ç§¯ï¼Ÿæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªç»“æœæ˜¯è‚¯å®šçš„ï¼šé€šè¿‡è°ƒç”¨è®¡ç®—æ™®é€‚æ€§ï¼Œæˆ‘ä»¬å‘ç°å¯¹äºä»»ä½•æ ¸å‡½æ•°ï¼Œæ€»æ˜¯å­˜åœ¨å¯¹åº”çš„é‡å­ç‰¹å¾æ˜ å°„å’ŒåµŒå…¥å¼é‡å­æ ¸ã€‚ç„¶è€Œï¼Œé—®é¢˜æ›´å…³æ³¨çš„æ˜¯æœ‰æ•ˆçš„æ„é€ æ–¹å¼ã€‚åœ¨ç¬¬äºŒéƒ¨åˆ†ä¸­

    One of the most natural connections between quantum and classical machine learning has been established in the context of kernel methods. Kernel methods rely on kernels, which are inner products of feature vectors living in large feature spaces. Quantum kernels are typically evaluated by explicitly constructing quantum feature states and then taking their inner product, here called embedding quantum kernels. Since classical kernels are usually evaluated without using the feature vectors explicitly, we wonder how expressive embedding quantum kernels are. In this work, we raise the fundamental question: can all quantum kernels be expressed as the inner product of quantum feature states? Our first result is positive: Invoking computational universality, we find that for any kernel function there always exists a corresponding quantum feature map and an embedding quantum kernel. The more operational reading of the question is concerned with efficient constructions, however. In a second part
    
[^10]: ä¼ªæ ‡ç­¾é€‰æ‹©æ˜¯ä¸€ä¸ªå†³ç­–é—®é¢˜

    Pseudo Label Selection is a Decision Problem. (arXiv:2309.13926v1 [cs.LG])

    [http://arxiv.org/abs/2309.13926](http://arxiv.org/abs/2309.13926)

    ä¼ªæ ‡ç­¾é€‰æ‹©æ˜¯åŠç›‘ç£å­¦ä¹ ä¸­çš„ä¸€ç§æ–¹æ³•ï¼Œé€šè¿‡åµŒå…¥å†³ç­–ç†è®ºï¼Œæå‡ºäº†BPLSæ¡†æ¶æ¥è§£å†³ä¼ªæ ‡ç­¾é€‰æ‹©ä¸­çš„ç¡®è®¤åå·®é—®é¢˜ã€‚

    

    ä¼ªæ ‡ç­¾é€‰æ‹©æ˜¯åŠç›‘ç£å­¦ä¹ ä¸­ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„æ–¹æ³•ï¼Œå®ƒéœ€è¦ä¸€äº›å‡†åˆ™æ¥æŒ‡å¯¼ä¼ªæ ‡ç­¾æ•°æ®çš„é€‰æ‹©ã€‚è¿™äº›å‡†åˆ™è¢«è¯æ˜å¯ä»¥åœ¨å®è·µä¸­å·¥ä½œå¾—ç›¸å½“å¥½ã€‚ç„¶è€Œï¼Œå®ƒä»¬çš„æ€§èƒ½å¾€å¾€å–å†³äºæ ‡è®°æ•°æ®ä¸Šåˆå§‹æ¨¡å‹çš„æ‹Ÿåˆæƒ…å†µã€‚æ—©æœŸè¿‡æ‹Ÿåˆå¯èƒ½é€šè¿‡é€‰æ‹©å…·æœ‰è‡ªä¿¡ä½†é”™è¯¯é¢„æµ‹çš„å®ä¾‹ï¼ˆé€šå¸¸è¢«ç§°ä¸ºç¡®è®¤åå·®ï¼‰è€Œä¼ æ’­åˆ°æœ€ç»ˆæ¨¡å‹ã€‚åœ¨ä¸¤é¡¹æœ€è¿‘çš„å·¥ä½œä¸­ï¼Œæˆ‘ä»¬è¯æ˜äº†ä¼ªæ ‡ç­¾é€‰æ‹©ï¼ˆPLSï¼‰å¯ä»¥è‡ªç„¶åœ°åµŒå…¥åˆ°å†³ç­–ç†è®ºä¸­ã€‚è¿™ä¸ºBPLSé“ºå¹³äº†é“è·¯ï¼Œå®ƒæ˜¯ä¸€ç§ç”¨äºPLSçš„è´å¶æ–¯æ¡†æ¶ï¼Œå¯ä»¥ç¼“è§£ç¡®è®¤åå·®çš„é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ˜¯ä¸€ç§æ–°çš„é€‰æ‹©å‡†åˆ™ï¼šä¼ªæ ·æœ¬å’Œæ ‡è®°æ•°æ®çš„åéªŒé¢„æµ‹çš„è§£æè¿‘ä¼¼ã€‚æˆ‘ä»¬é€šè¿‡è¯æ˜è¿™ä¸ªâ€œä¼ªPOSâ€çš„è´å¶æ–¯æœ€ä¼˜æ€§æ¥æ¨å¯¼å‡ºè¿™ä¸ªé€‰æ‹©å‡†åˆ™ã€‚

    Pseudo-Labeling is a simple and effective approach to semi-supervised learning. It requires criteria that guide the selection of pseudo-labeled data. The latter have been shown to crucially affect pseudo-labeling's generalization performance. Several such criteria exist and were proven to work reasonably well in practice. However, their performance often depends on the initial model fit on labeled data. Early overfitting can be propagated to the final model by choosing instances with overconfident but wrong predictions, often called confirmation bias. In two recent works, we demonstrate that pseudo-label selection (PLS) can be naturally embedded into decision theory. This paves the way for BPLS, a Bayesian framework for PLS that mitigates the issue of confirmation bias. At its heart is a novel selection criterion: an analytical approximation of the posterior predictive of pseudo-samples and labeled data. We derive this selection criterion by proving Bayes-optimality of this "pseudo pos
    
[^11]: æŠ—å¹²æ‰°çº¦æŸå­¦ä¹ 

    Resilient Constrained Learning. (arXiv:2306.02426v1 [cs.LG])

    [http://arxiv.org/abs/2306.02426](http://arxiv.org/abs/2306.02426)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªåä¸ºâ€œæŠ—å¹²æ‰°çº¦æŸå­¦ä¹ â€çš„æ–¹æ³•æ¥è§£å†³åœ¨éƒ¨ç½²æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆæ—¶éœ€è¦æ»¡è¶³é™¤äº†å‡†ç¡®æ€§ä»¥å¤–çš„å¤šä¸ªè¦æ±‚ï¼Œå¹¶ä»¥å¹³è¡¡ä»æ”¾å®½ä¸­è·å¾—çš„æ€§èƒ½å¢ç›Šä¸ç”¨æˆ·å®šä¹‰çš„æ”¾å®½æˆæœ¬ä¹‹é—´çš„å…³ç³»çš„æ–¹å¼æ”¾æ¾å­¦ä¹ çº¦æŸã€‚

    

    åœ¨éƒ¨ç½²æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆæ—¶ï¼Œé™¤äº†å‡†ç¡®æ€§ä¹‹å¤–ï¼Œå®ƒä»¬å¿…é¡»æ»¡è¶³å¤šä¸ªè¦æ±‚ï¼Œå¦‚å…¬å¹³æ€§ã€é²æ£’æ€§æˆ–å®‰å…¨æ€§ã€‚è¿™äº›è¦æ±‚å¯ä»¥é€šè¿‡ä½¿ç”¨æƒ©ç½šæ¥éšå¼åœ°æ–½åŠ ï¼Œæˆ–è€…é€šè¿‡åŸºäºLagrangianå¯¹å¶çš„çº¦æŸä¼˜åŒ–æ–¹æ³•æ¥æ˜¾å¼åœ°æ–½åŠ ã€‚æ— è®ºå“ªç§æ–¹å¼ï¼ŒæŒ‡å®šè¦æ±‚éƒ½å—åˆ°å¦¥åå’Œæœ‰é™çš„æœ‰å…³æ•°æ®çš„å…ˆå‰çŸ¥è¯†çš„å½±å“ã€‚æ­¤å¤–ï¼Œå®ƒä»¬å¯¹æ€§èƒ½çš„å½±å“é€šå¸¸åªèƒ½é€šè¿‡å®é™…è§£å†³å­¦ä¹ é—®é¢˜æ¥è¯„ä¼°ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§çº¦æŸå­¦ä¹ æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨åŒæ—¶è§£å†³å­¦ä¹ ä»»åŠ¡çš„åŒæ—¶è°ƒæ•´è¦æ±‚ã€‚ä¸ºæ­¤ï¼Œå®ƒä»¥å¹³è¡¡ä»æ”¾å®½ä¸­è·å¾—çš„æ€§èƒ½å¢ç›Šä¸ç”¨æˆ·å®šä¹‰çš„æ”¾å®½æˆæœ¬ä¹‹é—´çš„å…³ç³»çš„æ–¹å¼æ”¾æ¾äº†å­¦ä¹ çº¦æŸã€‚æˆ‘ä»¬å°†æ­¤æ–¹æ³•ç§°ä¸ºå…·æœ‰å¼¹æ€§çš„çº¦æŸå­¦ä¹ ï¼Œè¿™æ˜¯å¯¹ç”¨äºæè¿°ç”Ÿæ€ç³»ç»Ÿçš„æœ¯è¯­çš„ä¸€ç§å€Ÿé‰´ã€‚

    When deploying machine learning solutions, they must satisfy multiple requirements beyond accuracy, such as fairness, robustness, or safety. These requirements are imposed during training either implicitly, using penalties, or explicitly, using constrained optimization methods based on Lagrangian duality. Either way, specifying requirements is hindered by the presence of compromises and limited prior knowledge about the data. Furthermore, their impact on performance can often only be evaluated by actually solving the learning problem. This paper presents a constrained learning approach that adapts the requirements while simultaneously solving the learning task. To do so, it relaxes the learning constraints in a way that contemplates how much they affect the task at hand by balancing the performance gains obtained from the relaxation against a user-defined cost of that relaxation. We call this approach resilient constrained learning after the term used to describe ecological systems tha
    
[^12]: Borda Regret Minimization for Generalized Linear Dueling Bandits (é€šç”¨å¹¿ä¹‰çº¿æ€§å¯¹æŠ—æ€§æ’åé—®é¢˜çš„åšå°”è¾¾åæ‚”æœ€å°åŒ–ç®—æ³•)

    Borda Regret Minimization for Generalized Linear Dueling Bandits. (arXiv:2303.08816v1 [cs.LG])

    [http://arxiv.org/abs/2303.08816](http://arxiv.org/abs/2303.08816)

    æœ¬æ–‡è§£å†³äº†é€šç”¨å¹¿ä¹‰çº¿æ€§å¯¹æŠ—æ€§æ’åé—®é¢˜ä¸­çš„åšå°”è¾¾åæ‚”æœ€å°åŒ–é—®é¢˜ï¼Œæå‡ºäº†é«˜åº¦è¡¨è¾¾åŠ›çš„æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ä¸€ç§æ–°çš„â€œå…ˆæ¢ç´¢å†æ‰§è¡Œâ€ç®—æ³•é¿å…äº†å›°éš¾çš„åæ‚”ä¸‹é™ã€‚

    

    å¯¹æŠ—æ€§æ’åé—®é¢˜(Dueling bandits)å¸¸è¢«ç”¨äºæœºå™¨å­¦ä¹ åº”ç”¨ï¼Œå¦‚æ¨èç³»ç»Ÿå’Œæ’åé—®é¢˜ã€‚æœ¬æ–‡ç ”ç©¶å¯¹æŠ—æ€§æ’åé—®é¢˜ä¸­åšå°”è¾¾åæ‚”æœ€å°åŒ–é—®é¢˜ï¼Œæ—¨åœ¨ç¡®å®šå…·æœ‰æœ€é«˜åšå°”è¾¾å¾—åˆ†çš„é¡¹ç›®ï¼Œå¹¶åŒæ—¶æœ€å°åŒ–ç´¯è®¡çš„åæ‚”ã€‚æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæ–°çš„ã€é«˜åº¦è¡¨è¾¾åŠ›çš„é€šç”¨å¹¿ä¹‰çº¿æ€§å¯¹æŠ—æ€§æ’åæ¨¡å‹ï¼Œå®ƒåŒ…æ‹¬è®¸å¤šç°æœ‰æ¨¡å‹ã€‚ ä»¤äººæƒŠè®¶çš„æ˜¯ï¼Œåšå°”è¾¾åæ‚”æœ€å°åŒ–é—®é¢˜æ˜¯å›°éš¾çš„ã€‚ æˆ‘ä»¬è¯æ˜äº†æ¸è¿‘æ—¶é—´å¤æ‚åº¦çš„åæ‚”ä¸‹é™æ˜¯$\Omega(d^{2/3} T^{2/3})$ï¼Œå…¶ä¸­$d$æ˜¯ä¸Šä¸‹æ–‡å‘é‡çš„ç»´æ•°ï¼Œ$T$æ˜¯æ—¶é—´è·¨åº¦ã€‚ä¸ºäº†è¾¾åˆ°ä¸‹é™ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§"å…ˆæ¢ç´¢å†æ‰§è¡Œ"çš„ç®—æ³•ï¼Œå®ƒå…·æœ‰å‡ ä¹åŒ¹é…çš„ä¸Šé™å›å½’è¯¯å·®$\tilde{O}(d^{2/3} T^{2/3})$ã€‚å½“é¡¹ç›®æ•°é‡$K$å¾ˆå°æ—¶ï¼Œæˆ‘ä»¬çš„ç®—æ³•å¯ä»¥é€šè¿‡é€‚å½“é€‰æ‹©è¶…å‚æ•°ä»¥è¾¾åˆ°æ›´å°çš„åæ‚”$\tilde{O}((d\log K)^{1/3}T^{2/3})$ã€‚

    Dueling bandits are widely used to model preferential feedback that is prevalent in machine learning applications such as recommendation systems and ranking. In this paper, we study the Borda regret minimization problem for dueling bandits, which aims to identify the item with the highest Borda score while minimizing the cumulative regret. We propose a new and highly expressive generalized linear dueling bandits model, which covers many existing models. Surprisingly, the Borda regret minimization problem turns out to be difficult, as we prove a regret lower bound of order $\Omega(d^{2/3} T^{2/3})$, where $d$ is the dimension of contextual vectors and $T$ is the time horizon. To attain the lower bound, we propose an explore-then-commit type algorithm, which has a nearly matching regret upper bound $\tilde{O}(d^{2/3} T^{2/3})$. When the number of items/arms $K$ is small, our algorithm can achieve a smaller regret $\tilde{O}( (d \log K)^{1/3} T^{2/3})$ with proper choices of hyperparamete
    
[^13]: ä»¥ELBOsçš„åŠ æƒç§¯åˆ†ç†è§£æ‰©æ•£ç›®æ ‡

    Understanding the Diffusion Objective as a Weighted Integral of ELBOs. (arXiv:2303.00848v2 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2303.00848](http://arxiv.org/abs/2303.00848)

    æœ¬æ–‡æ·±å…¥ç†è§£äº†æ‰©æ•£ç›®æ ‡ï¼Œå¹¶æ­ç¤ºäº†åŠ æƒæŸå¤±å’ŒELBOç›®æ ‡ä¹‹é—´çš„ç›´æ¥å…³ç³»ã€‚

    

    æ–‡çŒ®ä¸­çš„æ‰©æ•£æ¨¡å‹é‡‡ç”¨ä¸åŒçš„ç›®æ ‡è¿›è¡Œä¼˜åŒ–ï¼Œå¹¶ä¸”è¿™äº›ç›®æ ‡éƒ½æ˜¯åŠ æƒæŸå¤±çš„ç‰¹ä¾‹ï¼Œå…¶ä¸­åŠ æƒå‡½æ•°æŒ‡å®šæ¯ä¸ªå™ªå£°çº§åˆ«çš„æƒé‡ã€‚å‡åŒ€åŠ æƒå¯¹åº”äºæœ€å¤§ä¼¼ç„¶çš„åŸåˆ™æ€§è¿‘ä¼¼ELBOçš„æœ€å¤§åŒ–ã€‚ä½†æ˜¯å®é™…ä¸Šï¼Œç”±äºæ›´å¥½çš„æ ·æœ¬è´¨é‡ï¼Œç›®å‰çš„æ‰©æ•£æ¨¡å‹ä½¿ç”¨éå‡åŒ€åŠ æƒã€‚æœ¬æ–‡æ­ç¤ºäº†åŠ æƒæŸå¤±ï¼ˆå¸¦æœ‰ä»»ä½•åŠ æƒï¼‰å’ŒELBOç›®æ ‡ä¹‹é—´çš„ç›´æ¥å…³ç³»ã€‚æˆ‘ä»¬å±•ç¤ºäº†åŠ æƒæŸå¤±å¯ä»¥è¢«å†™æˆä¸€ç§ELBOsçš„åŠ æƒç§¯åˆ†å½¢å¼ï¼Œå…¶ä¸­æ¯ä¸ªå™ªå£°çº§åˆ«éƒ½æœ‰ä¸€ä¸ªELBOã€‚å¦‚æœæƒé‡å‡½æ•°æ˜¯å•è°ƒçš„ï¼Œé‚£ä¹ˆåŠ æƒæŸå¤±æ˜¯ä¸€ç§åŸºäºä¼¼ç„¶çš„ç›®æ ‡ï¼šå®ƒåœ¨ç®€å•çš„æ•°æ®å¢å¼ºä¸‹ï¼ˆå³é«˜æ–¯å™ªå£°æ‰°åŠ¨ï¼‰ä¸‹æœ€å¤§åŒ–ELBOã€‚æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯æ›´æ·±å…¥åœ°ç†è§£äº†æ‰©æ•£ç›®æ ‡ï¼Œä½†æˆ‘ä»¬è¿˜è¿›è¡Œäº†ä¸€äº›æ¯”è¾ƒå•è°ƒå’Œéå•è°ƒæƒé‡çš„å®éªŒã€‚

    Diffusion models in the literature are optimized with various objectives that are special cases of a weighted loss, where the weighting function specifies the weight per noise level. Uniform weighting corresponds to maximizing the ELBO, a principled approximation of maximum likelihood. In current practice diffusion models are optimized with non-uniform weighting due to better results in terms of sample quality. In this work we expose a direct relationship between the weighted loss (with any weighting) and the ELBO objective.  We show that the weighted loss can be written as a weighted integral of ELBOs, with one ELBO per noise level. If the weighting function is monotonic, then the weighted loss is a likelihood-based objective: it maximizes the ELBO under simple data augmentation, namely Gaussian noise perturbation. Our main contribution is a deeper theoretical understanding of the diffusion objective, but we also performed some experiments comparing monotonic with non-monotonic weight
    
[^14]: ddml: Stataä¸­çš„åŒé‡/æ— åæœºå™¨å­¦ä¹ 

    ddml: Double/debiased machine learning in Stata. (arXiv:2301.09397v2 [econ.EM] UPDATED)

    [http://arxiv.org/abs/2301.09397](http://arxiv.org/abs/2301.09397)

    ddmlæ˜¯Stataä¸­çš„åŒé‡/æ— åæœºå™¨å­¦ä¹ åŒ…ï¼Œæ”¯æŒäº”ç§ä¸åŒè®¡é‡æ¨¡å‹çš„å› æœå‚æ•°ä¼°è®¡ï¼Œå¯ä»¥çµæ´»ä¼°è®¡å†…ç”Ÿå˜é‡çš„å› æœæ•ˆåº”ï¼Œåœ¨è®¸å¤šç°æœ‰ç›‘ç£æœºå™¨å­¦ä¹ ç¨‹åºä¸­å…¼å®¹ã€‚æ¨èä¸å †å ä¼°è®¡ç»“åˆä½¿ç”¨ï¼Œæä¾›äº†è’™ç‰¹å¡æ´›è¯æ®æ”¯æŒã€‚

    

    æˆ‘ä»¬åœ¨Stataä¸­å¼•å…¥äº†ä¸€ä¸ªåä¸ºddmlçš„åŒ…ï¼Œç”¨äºåŒé‡/æ— åæœºå™¨å­¦ä¹ ï¼ˆDDMLï¼‰ã€‚æ”¯æŒäº”ç§ä¸åŒè®¡é‡æ¨¡å‹çš„å› æœå‚æ•°ä¼°è®¡ï¼Œå…è®¸åœ¨æœªçŸ¥å‡½æ•°å½¢å¼å’Œ/æˆ–è®¸å¤šå¤–ç”Ÿå˜é‡çš„è®¾ç½®ä¸­çµæ´»ä¼°è®¡å†…ç”Ÿå˜é‡çš„å› æœæ•ˆåº”ã€‚ddmlä¸Stataä¸­çš„è®¸å¤šç°æœ‰ç›‘ç£æœºå™¨å­¦ä¹ ç¨‹åºå…¼å®¹ã€‚æˆ‘ä»¬æ¨èå°†DDMLä¸å †å ä¼°è®¡ç»“åˆä½¿ç”¨ï¼Œå°†å¤šä¸ªæœºå™¨å­¦ä¹ å™¨ç»„åˆæˆæœ€ç»ˆé¢„æµ‹å™¨ã€‚æˆ‘ä»¬æä¾›äº†è’™ç‰¹å¡æ´›è¯æ®æ¥æ”¯æŒæˆ‘ä»¬çš„å»ºè®®ã€‚

    We introduce the package ddml for Double/Debiased Machine Learning (DDML) in Stata. Estimators of causal parameters for five different econometric models are supported, allowing for flexible estimation of causal effects of endogenous variables in settings with unknown functional forms and/or many exogenous variables. ddml is compatible with many existing supervised machine learning programs in Stata. We recommend using DDML in combination with stacking estimation which combines multiple machine learners into a final predictor. We provide Monte Carlo evidence to support our recommendation.
    
[^15]: åœ¨çº¿æ ¸CUSUMæ–¹æ³•è¿›è¡Œå˜ç‚¹æ£€æµ‹

    Online Kernel CUSUM for Change-Point Detection. (arXiv:2211.15070v3 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2211.15070](http://arxiv.org/abs/2211.15070)

    æœ¬ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨çº¿å˜ç‚¹æ£€æµ‹çš„æ ¸CUSUMæ–¹æ³•ï¼Œç›¸æ¯”äºç°æœ‰æ–¹æ³•æ›´æ•æ„Ÿï¼Œæä¾›äº†å‡†ç¡®çš„å…³é”®æ€§èƒ½æŒ‡æ ‡åˆ†æï¼Œå¹¶å»ºç«‹äº†æœ€ä¼˜çª—å£é•¿åº¦ï¼Œå¼•å…¥äº†é€’å½’è®¡ç®—ç¨‹åºæ¥ç¡®ä¿è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦æ’å®šã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ç§é«˜æ•ˆçš„åœ¨çº¿æ ¸Cumulative Sum (CUSUM)æ–¹æ³•ï¼Œç”¨äºå˜ç‚¹æ£€æµ‹ï¼Œåˆ©ç”¨æ ¸ç»Ÿè®¡é‡é›†åˆä¸­çš„æœ€å¤§å€¼æ¥è€ƒè™‘æœªçŸ¥çš„å˜ç‚¹ä½ç½®ã€‚ç›¸æ¯”äºç°æœ‰æ–¹æ³•ï¼Œå¦‚Scan-Bç»Ÿè®¡é‡ï¼Œå³å¯¹åº”äºéå‚æ•°Shewhartå›¾è¿‡ç¨‹çš„æ–¹æ³•ï¼Œæˆ‘ä»¬çš„æ–¹æ³•å¯¹äºå°å˜åŒ–å…·æœ‰æ›´é«˜çš„æ•æ„Ÿæ€§ã€‚æˆ‘ä»¬æä¾›äº†ä¸¤ä¸ªå…³é”®æ€§èƒ½æŒ‡æ ‡çš„å‡†ç¡®åˆ†æè¿‘ä¼¼å€¼ï¼šå¹³å‡è¿è¡Œé•¿åº¦ï¼ˆARLï¼‰å’Œé¢„æœŸæ£€æµ‹å»¶è¿Ÿï¼ˆEDDï¼‰ï¼Œè¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿå»ºç«‹ä¸€ä¸ªä¸ARLå¯¹æ•°åŒé˜¶çš„æœ€ä¼˜çª—å£é•¿åº¦ï¼Œä»¥ç¡®ä¿ç›¸å¯¹äºå…·æœ‰æ— é™å†…å­˜çš„ç†è®ºæ¨¡å‹èƒ½å¤Ÿä¿æŒæœ€å°åŠŸç‡æŸå¤±ã€‚è¿™ç±»ä¼¼äºå‚æ•°å˜ç‚¹æ£€æµ‹æ–‡çŒ®ä¸­çš„çª—å£é™åˆ¶å¹¿ä¹‰ä¼¼ç„¶æ¯”ï¼ˆGLRï¼‰è¿‡ç¨‹çš„ç»å…¸ç»“æœã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€ç§é€’å½’è®¡ç®—ç¨‹åºï¼Œç”¨äºæ£€æµ‹ç»Ÿè®¡é‡ï¼Œä»¥ç¡®ä¿è®¡ç®—å’Œå†…å­˜å¤æ‚åº¦æ’å®šã€‚

    We propose an efficient online kernel Cumulative Sum (CUSUM) method for change-point detection that utilizes the maximum over a set of kernel statistics to account for the unknown change-point location. Our approach exhibits increased sensitivity to small changes compared to existing methods, such as the Scan-B statistic, which corresponds to a non-parametric Shewhart chart-type procedure. We provide accurate analytic approximations for two key performance metrics: the Average Run Length (ARL) and Expected Detection Delay (EDD), which enable us to establish an optimal window length on the order of the logarithm of ARL to ensure minimal power loss relative to an oracle procedure with infinite memory. Such a finding parallels the classic result for window-limited Generalized Likelihood Ratio (GLR) procedure in parametric change-point detection literature. Moreover, we introduce a recursive calculation procedure for detection statistics to ensure constant computational and memory complexi
    
[^16]: å•æ—¶é—´å°ºåº¦æ¼”å‘˜-è¯„è®ºå®¶æ³•çš„æœ‰é™æ—¶é—´åˆ†æ

    Finite-time analysis of single-timescale actor-critic. (arXiv:2210.09921v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.09921](http://arxiv.org/abs/2210.09921)

    è¿™é¡¹ç ”ç©¶æå‡ºäº†ä¸€ç§åœ¨çº¿å•æ—¶é—´å°ºåº¦æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•ï¼Œé€šè¿‡çº¿æ€§å‡½æ•°é€¼è¿‘å’Œé©¬å°”å¯å¤«æ ·æœ¬æ›´æ–°ï¼Œåœ¨è¿ç»­çŠ¶æ€ç©ºé—´ä¸­æ‰¾åˆ°äº†ä¸€ä¸ª$\epsilon$-è¿‘ä¼¼çš„ç¨³å®šç‚¹ï¼Œå¹¶ä¸”åœ¨æ ·æœ¬å¤æ‚åº¦ä¸º$\widetilde{\mathcal{O}}(\epsilon^{-2})$çš„æƒ…å†µä¸‹è¯æ˜äº†å…¶æ”¶æ•›æ€§ã€‚

    

    åœ¨è®¸å¤šå…·æœ‰æŒ‘æˆ˜æ€§çš„åº”ç”¨ä¸­ï¼Œæ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•å–å¾—äº†æ˜¾ç€çš„æˆåŠŸã€‚ç„¶è€Œï¼Œåœ¨æœ€å®é™…çš„å•æ—¶é—´å°ºåº¦å½¢å¼ä¸‹ï¼Œå…¶æœ‰é™æ—¶é—´æ”¶æ•›æ€§ä»ç„¶ä¸å¤Ÿç†è§£ã€‚ç°æœ‰çš„å•æ—¶é—´å°ºåº¦æ¼”å‘˜-è¯„è®ºå®¶åˆ†æå·¥ä½œä»…é™äºç®€åŒ–çš„i.i.d.é‡‡æ ·æˆ–è¡¨æ ¼è®¾ç½®ã€‚æˆ‘ä»¬ç ”ç©¶äº†æ›´å®é™…çš„åœ¨çº¿å•æ—¶é—´å°ºåº¦æ¼”å‘˜-è¯„è®ºå®¶ç®—æ³•ï¼Œè¯¥ç®—æ³•åœ¨è¿ç»­çŠ¶æ€ç©ºé—´ä¸­ï¼Œè¯„è®ºå®¶é‡‡ç”¨çº¿æ€§å‡½æ•°é€¼è¿‘ï¼Œå¹¶åœ¨æ¯ä¸ªæ¼”å‘˜æ­¥éª¤ä¸­ä½¿ç”¨å•ä¸ªé©¬å°”å¯å¤«æ ·æœ¬è¿›è¡Œæ›´æ–°ã€‚å…ˆå‰çš„åˆ†ææ— æ³•åœ¨è¿™ç§å…·æœ‰æŒ‘æˆ˜æ€§çš„åœºæ™¯ä¸­å®ç°æ”¶æ•›ã€‚æˆ‘ä»¬è¯æ˜ï¼Œåœ¨æ ‡å‡†å‡è®¾ä¸‹ï¼Œåœ¨çº¿å•æ—¶é—´å°ºåº¦æ¼”å‘˜-è¯„è®ºå®¶æ–¹æ³•èƒ½å¤Ÿåœ¨æ ·æœ¬å¤æ‚åº¦ä¸º$\widetilde{\mathcal{O}}(\epsilon^{-2})$çš„æƒ…å†µä¸‹æ‰¾åˆ°ä¸€ä¸ª$\epsilon$-è¿‘ä¼¼çš„ç¨³å®šç‚¹ï¼Œè€Œåœ¨i.i.d.é‡‡æ ·ä¸‹ï¼Œè¿™ä¸ªå¤æ‚åº¦å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›ä¸º$\mathcal{O}(\epsilon^{-2})$ã€‚æˆ‘ä»¬çš„æ–°æ¡†æ¶ç³»ç»Ÿåœ°è¯„ä¼°äº†ä¸€ä¸ª

    Actor-critic methods have achieved significant success in many challenging applications. However, its finite-time convergence is still poorly understood in the most practical single-timescale form. Existing works on analyzing single-timescale actor-critic have been limited to i.i.d. sampling or tabular setting for simplicity. We investigate the more practical online single-timescale actor-critic algorithm on continuous state space, where the critic assumes linear function approximation and updates with a single Markovian sample per actor step. Previous analysis has been unable to establish the convergence for such a challenging scenario. We demonstrate that the online single-timescale actor-critic method provably finds an $\epsilon$-approximate stationary point with $\widetilde{\mathcal{O}}(\epsilon^{-2})$ sample complexity under standard assumptions, which can be further improved to $\mathcal{O}(\epsilon^{-2})$ under the i.i.d. sampling. Our novel framework systematically evaluates an
    
[^17]: ä½¿ç”¨CNNæ¥æµ‹è¯•è¡¨ç¤ºæˆæœ¬ç†è®ºçš„é¢„æµ‹

    Testing predictions of representation cost theory with CNNs. (arXiv:2210.01257v3 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2210.01257](http://arxiv.org/abs/2210.01257)

    é€šè¿‡ç†è®ºå’Œå®éªŒè¯æ˜ï¼Œè®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å¯¹ä½é¢‘ä¿¡å·å…·æœ‰æ•æ„Ÿæ€§ï¼Œè¿™æ˜¯å› ä¸ºè‡ªç„¶å›¾åƒçš„é¢‘ç‡åˆ†å¸ƒä½¿å¤§éƒ¨åˆ†èƒ½é‡é›†ä¸­åœ¨ä½åˆ°ä¸­é¢‘ã€‚

    

    ä¼—æ‰€å‘¨çŸ¥ï¼Œç»è¿‡è®­ç»ƒçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNsï¼‰å¯¹ä¸åŒé¢‘ç‡çš„ä¿¡å·å…·æœ‰ä¸åŒçš„æ•æ„Ÿæ€§ã€‚ç‰¹åˆ«æ˜¯ï¼Œè®¸å¤šå®è¯ç ”ç©¶å·²ç»è®°å½•äº†CNNså¯¹ä½é¢‘ä¿¡å·çš„æ•æ„Ÿæ€§ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡ç†è®ºå’Œå®éªŒè¯æ˜ï¼Œè¿™ç§è§‚å¯Ÿåˆ°çš„æ•æ„Ÿæ€§æ˜¯è‡ªç„¶å›¾åƒé¢‘ç‡åˆ†å¸ƒçš„ç»“æœï¼Œå·²çŸ¥å¤§éƒ¨åˆ†èƒ½é‡é›†ä¸­åœ¨ä½åˆ°ä¸­é¢‘ã€‚æˆ‘ä»¬çš„ç†è®ºåˆ†æä¾èµ–äºCNNçš„å±‚æ¬¡åœ¨é¢‘ç‡ç©ºé—´ä¸­çš„è¡¨ç¤ºï¼Œè¿™ä¸ªæƒ³æ³•ä¹‹å‰æ›¾è¢«ç”¨æ¥åŠ é€Ÿè®¡ç®—å’Œç ”ç©¶ç½‘ç»œè®­ç»ƒç®—æ³•çš„éšå¼åå·®ï¼Œä½†æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œå°šæœªåœ¨æ¨¡å‹é²æ£’æ€§é¢†åŸŸåº”ç”¨è¿‡ã€‚

    It is widely acknowledged that trained convolutional neural networks (CNNs) have different levels of sensitivity to signals of different frequency. In particular, a number of empirical studies have documented CNNs sensitivity to low-frequency signals. In this work we show with theory and experiments that this observed sensitivity is a consequence of the frequency distribution of natural images, which is known to have most of its power concentrated in low-to-mid frequencies. Our theoretical analysis relies on representations of the layers of a CNN in frequency space, an idea that has previously been used to accelerate computations and study implicit bias of network training algorithms, but to the best of our knowledge has not been applied in the domain of model robustness.
    
[^18]: å…³äºè´å¶æ–¯ç½‘ç»œè¾¹é™…ç‹¬ç«‹ç»“æ„çš„ç»„åˆå’Œä»£æ•°è§†è§’

    Combinatorial and algebraic perspectives on the marginal independence structure of Bayesian networks. (arXiv:2210.00822v2 [stat.ME] UPDATED)

    [http://arxiv.org/abs/2210.00822](http://arxiv.org/abs/2210.00822)

    æœ¬ç ”ç©¶é€šè¿‡ç»„åˆå’Œä»£æ•°è§†è§’æ¢è®¨äº†è´å¶æ–¯ç½‘ç»œçš„è¾¹é™…ç‹¬ç«‹ç»“æ„é—®é¢˜ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªåŸºäº GrÃ¶bner åŸºç¡€çš„ MCMC æ–¹æ³• GrUESï¼Œè¯¥æ–¹æ³•åœ¨æ¢å¤çœŸå®ç»“æ„å’Œä¼°è®¡åéªŒä¸Šå…·æœ‰ä¼˜åŠ¿ã€‚

    

    æˆ‘ä»¬è€ƒè™‘ä»è§‚æµ‹æ•°æ®ä¸­ä¼°è®¡è´å¶æ–¯ç½‘ç»œçš„è¾¹é™…ç‹¬ç«‹ç»“æ„çš„é—®é¢˜ï¼Œè¿™äº›æ•°æ®ä»¥ä¸€ä¸ªæ— å‘å›¾çš„å½¢å¼å‘ˆç°ï¼Œè¢«ç§°ä¸ºæ— æ¡ä»¶ä¾èµ–å›¾ã€‚æˆ‘ä»¬è¯æ˜äº†è´å¶æ–¯ç½‘ç»œçš„æ— æ¡ä»¶ä¾èµ–å›¾å¯¹åº”äºå…·æœ‰ç›¸ç­‰ç‹¬ç«‹æ€§å’Œäº¤é›†æ•°çš„å›¾ã€‚åŸºäºè¿™ä¸€è§‚å¯Ÿç»“æœï¼Œæˆ‘ä»¬ç»™å‡ºäº†ä¸è´å¶æ–¯ç½‘ç»œçš„æ— æ¡ä»¶ä¾èµ–å›¾ç›¸å…³çš„ä¸€ä¸ªæ‹“æ‰‘ç†æƒ³çš„ GrÃ¶bner åŸºç¡€ï¼Œç„¶åé€šè¿‡é¢å¤–çš„äºŒé¡¹å¼å…³ç³»å°†å…¶æ‰©å±•ä»¥è¿æ¥æ‰€æœ‰è¿™äº›å›¾çš„ç©ºé—´ã€‚æˆ‘ä»¬å®ç°äº†ä¸€ç§åä¸º GrUES (GrÃ¶bner-based Unconditional Equivalence Search) çš„ MCMC æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åŸºäºæ‰€å¾—çš„ç§»åŠ¨å¹¶åº”ç”¨äºåˆæˆé«˜æ–¯æ•°æ®ã€‚GrUES ä»¥æ¯”ç®€å•çš„ç‹¬ç«‹æ€§æµ‹è¯•æ›´é«˜çš„é€Ÿç‡æ¢å¤çœŸå®çš„è¾¹é™…ç‹¬ç«‹ç»“æ„ï¼ŒåŒæ—¶è¿˜äº§ç”Ÿäº†ä¸€ä¸ªåŒ…æ‹¬çœŸå®ç»“æ„çš„åéªŒä¼°è®¡ï¼Œå…¶ä¸­ $20\%$ çš„ HPD ç½®ä¿¡åŒºé—´åŒ…å«çœŸå®ç»“æ„ã€‚

    We consider the problem of estimating the marginal independence structure of a Bayesian network from observational data in the form of an undirected graph called the unconditional dependence graph. We show that unconditional dependence graphs of Bayesian networks correspond to the graphs having equal independence and intersection numbers. Using this observation, a Gr\"obner basis for a toric ideal associated to unconditional dependence graphs of Bayesian networks is given and then extended by additional binomial relations to connect the space of all such graphs. An MCMC method, called GrUES (Gr\"obner-based Unconditional Equivalence Search), is implemented based on the resulting moves and applied to synthetic Gaussian data. GrUES recovers the true marginal independence structure via a penalized maximum likelihood or MAP estimate at a higher rate than simple independence tests while also yielding an estimate of the posterior, for which the $20\%$ HPD credible sets include the true struc
    
[^19]: NN2Polyï¼šç”¨äºæ·±åº¦å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œçš„å¤šé¡¹å¼è¡¨ç¤ºæ–¹æ³•

    NN2Poly: A polynomial representation for deep feed-forward artificial neural networks. (arXiv:2112.11397v3 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2112.11397](http://arxiv.org/abs/2112.11397)

    æœ¬æ–‡æå‡ºäº†ä¸€ç§åä¸ºNN2Polyçš„æ–¹æ³•ï¼Œç”¨äºç”Ÿæˆå·²ç»è®­ç»ƒå¥½çš„å…¨è¿æ¥å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œçš„ç²¾ç¡®å¤šé¡¹å¼è¡¨ç¤ºã€‚è¯¥æ–¹æ³•é€‚ç”¨äºä»»æ„æ·±åº¦çš„å¤šå±‚æ„ŸçŸ¥å™¨ï¼ˆMLPï¼‰ä¸”è®¡ç®—æˆæœ¬ç›¸å¯¹è¾ƒä½ï¼Œèƒ½å¤Ÿåœ¨å›å½’å’Œåˆ†ç±»ä»»åŠ¡ä¸­æä¾›éå¸¸å‡†ç¡®çš„é€¼è¿‘ç»“æœã€‚

    

    å°½ç®¡æ·±åº¦å­¦ä¹ åº”ç”¨éå¸¸æˆåŠŸï¼Œä½†ç¥ç»ç½‘ç»œçš„å¯è§£é‡Šæ€§å’Œç†è®ºè¡Œä¸ºä»ç„¶æ˜¯ä¸€ä¸ªå¼€æ”¾çš„ç ”ç©¶é¢†åŸŸã€‚æœ¬æ–‡æå‡ºNN2Polyï¼šä¸€ç§ç†è®ºæ–¹æ³•ï¼Œç”¨äºè·å–ä¸€ä¸ªæ˜¾å¼å¤šé¡¹å¼æ¨¡å‹ï¼Œä»¥æä¾›å·²ç»è®­ç»ƒå¥½çš„å…¨è¿æ¥å‰é¦ˆäººå·¥ç¥ç»ç½‘ç»œï¼ˆå¤šå±‚æ„ŸçŸ¥å™¨æˆ–MLPï¼‰çš„ç²¾ç¡®è¡¨ç¤ºã€‚è¿™ç§æ–¹æ³•æ‰©å±•äº†æ–‡çŒ®ä¸­æå‡ºçš„å…ˆå‰æƒ³æ³•ï¼Œè¯¥æƒ³æ³•ä»…é™äºå•éšè—å±‚çš„ç½‘ç»œï¼Œå¹¶ä¸”é€‚ç”¨äºå›å½’å’Œåˆ†ç±»ä»»åŠ¡çš„ä»»æ„æ·±åº¦MLPã€‚æœ¬æ–‡çš„ç›®æ ‡æ˜¯é€šè¿‡åœ¨æ¯å±‚ä¸Šä½¿ç”¨æ¿€æ´»å‡½æ•°çš„æ³°å‹’å±•å¼€å¼ï¼Œç„¶åä½¿ç”¨å‡ ä¸ªç»„åˆæ€§è´¨æ¥è®¡ç®—æ‰€éœ€å¤šé¡¹å¼çš„ç³»æ•°ï¼Œä»è€Œå®ç°æ­¤ç›®æ ‡ã€‚ä½œè€…è®¨è®ºäº†æ­¤æ–¹æ³•çš„ä¸»è¦è®¡ç®—æŒ‘æˆ˜ä»¥åŠé€šè¿‡å¼•å…¥ä¸€äº›é€¼è¿‘æ¥å…‹æœè¿™äº›æŒ‘æˆ˜çš„æ–¹æ³•ï¼Œè€Œä¸ä¼šå½±å“å…¶å‡†ç¡®æ€§ã€‚é€šè¿‡å®éªŒéªŒè¯è¡¨æ˜ï¼Œå°½ç®¡NN2Polyæ–¹æ³•ç®€å•ä¸”è®¡ç®—æˆæœ¬ä½ï¼Œä½†å¯¹äºåˆæˆå’ŒçœŸå®æ•°æ®é›†ï¼Œæä¾›éå¸¸å‡†ç¡®çš„å¤šé¡¹å¼é€¼è¿‘ã€‚

    Interpretability of neural networks and their underlying theoretical behavior remain an open field of study even after the great success of their practical applications, particularly with the emergence of deep learning. In this work, NN2Poly is proposed: a theoretical approach to obtain an explicit polynomial model that provides an accurate representation of an already trained fully-connected feed-forward artificial neural network (a multilayer perceptron or MLP). This approach extends a previous idea proposed in the literature, which was limited to single hidden layer networks, to work with arbitrarily deep MLPs in both regression and classification tasks. The objective of this paper is to achieve this by using a Taylor expansion on the activation function, at each layer, and then using several combinatorial properties to calculate the coefficients of the desired polynomials. Discussion is presented on the main computational challenges of this method, and the way to overcome them by i
    
[^20]: è·¯å¾„æ­£åˆ™åŒ–ï¼šä¸€ç§å¯¹å¹¶è¡ŒReLUç½‘ç»œè¿›è¡Œå‡¸æ€§å’Œç¨€ç–æ€§å¼•å¯¼çš„æ­£åˆ™åŒ–æ–¹æ³•

    Path Regularization: A Convexity and Sparsity Inducing Regularization for Parallel ReLU Networks. (arXiv:2110.09548v4 [cs.LG] UPDATED)

    [http://arxiv.org/abs/2110.09548](http://arxiv.org/abs/2110.09548)

    è·¯å¾„æ­£åˆ™åŒ–ä¸ºå¹¶è¡ŒReLUç½‘ç»œæä¾›äº†ä¸€ç§ç®€åŒ–çš„å‡¸ä¼˜åŒ–é—®é¢˜ï¼Œé€šè¿‡ç¾¤ç¨€ç–æ€§å¼•å¯¼å®ç°äº†å‡¸æ¨¡å‹ï¼Œå¹¶æå‡ºäº†ä¸€ä¸ªè¿‘ä¼¼ç®—æ³•ï¼Œåœ¨æ‰€æœ‰æ•°æ®ç»´åº¦ä¸Šå…·å¤‡å®Œå…¨å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦ã€‚

    

    ç†è§£æ·±åº¦ç¥ç»ç½‘ç»œæˆåŠŸèƒŒåçš„åŸºæœ¬åŸç†æ˜¯å½“å‰æ–‡çŒ®ä¸­æœ€é‡è¦çš„å¼€æ”¾é—®é¢˜ä¹‹ä¸€ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ·±åº¦ç¥ç»ç½‘ç»œçš„è®­ç»ƒé—®é¢˜ï¼Œå¹¶å¼•å…¥äº†ä¸€ç§åˆ†ææ–¹æ³•æ¥æ­ç¤ºä¼˜åŒ–æ™¯è§‚ä¸­éšè—çš„å‡¸æ€§ã€‚æˆ‘ä»¬è€ƒè™‘äº†æ·±åº¦å¹¶è¡ŒReLUç½‘ç»œæ¶æ„ï¼Œå…¶ä¹ŸåŒ…æ‹¬æ ‡å‡†çš„æ·±åº¦ç½‘ç»œå’ŒResNetä½œä¸ºå…¶ç‰¹ä¾‹ã€‚ç„¶åæˆ‘ä»¬è¡¨æ˜ï¼ŒåŸºäºè·¯å¾„æ­£åˆ™åŒ–çš„è®­ç»ƒé—®é¢˜å¯ä»¥è¡¨ç¤ºä¸ºä¸€ä¸ªç²¾ç¡®çš„å‡¸ä¼˜åŒ–é—®é¢˜ã€‚æˆ‘ä»¬è¿›ä¸€æ­¥è¯æ˜ç­‰ä»·çš„å‡¸é—®é¢˜æ˜¯é€šè¿‡ä¸€ç§ç¾¤ç¨€ç–æ€§å¼•å¯¼çš„è§„èŒƒè¿›è¡Œæ­£åˆ™åŒ–çš„ã€‚å› æ­¤ï¼Œè·¯å¾„æ­£åˆ™åŒ–çš„å¹¶è¡ŒReLUç½‘ç»œå¯ä»¥è¢«è§†ä¸ºé«˜ç»´ä¸­ä¸€ç§ç®€åŒ–çš„å‡¸æ¨¡å‹ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œç”±äºåŸå§‹çš„è®­ç»ƒé—®é¢˜å¯èƒ½æ— æ³•åœ¨å¤šé¡¹å¼æ—¶é—´å†…è®­ç»ƒï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåœ¨æ‰€æœ‰æ•°æ®ç»´åº¦ä¸Šå…·æœ‰å®Œå…¨å¤šé¡¹å¼æ—¶é—´å¤æ‚åº¦çš„è¿‘ä¼¼ç®—æ³•ã€‚ç„¶åï¼Œæˆ‘ä»¬è¯æ˜äº†å¼ºå…¨å±€æ”¶æ•›æ€§ã€‚

    Understanding the fundamental principles behind the success of deep neural networks is one of the most important open questions in the current literature. To this end, we study the training problem of deep neural networks and introduce an analytic approach to unveil hidden convexity in the optimization landscape. We consider a deep parallel ReLU network architecture, which also includes standard deep networks and ResNets as its special cases. We then show that pathwise regularized training problems can be represented as an exact convex optimization problem. We further prove that the equivalent convex problem is regularized via a group sparsity inducing norm. Thus, a path regularized parallel ReLU network can be viewed as a parsimonious convex model in high dimensions. More importantly, since the original training problem may not be trainable in polynomial-time, we propose an approximate algorithm with a fully polynomial-time complexity in all data dimensions. Then, we prove strong glob
    
[^21]: æ·±åº¦ç”Ÿå­˜å‰‚é‡ååº”å‡½æ•°çš„è¿ç»­æ²»ç–—æ¨è

    Continuous Treatment Recommendation with Deep Survival Dose Response Function. (arXiv:2108.10453v5 [stat.ML] UPDATED)

    [http://arxiv.org/abs/2108.10453](http://arxiv.org/abs/2108.10453)

    æœ¬è®ºæ–‡æå‡ºäº†ä¸€ä¸ªé€šç”¨å…¬å¼ï¼Œç§°ä¸ºæ·±åº¦ç”Ÿå­˜å‰‚é‡ååº”å‡½æ•°ï¼ˆDeepSDRFï¼‰ï¼Œç”¨äºè§£å†³ä¸´åºŠç”Ÿå­˜æ•°æ®ä¸­çš„è¿ç»­æ²»ç–—æ¨èé—®é¢˜ã€‚é€šè¿‡æ ¡æ­£é€‰æ‹©åå·®ï¼ŒDeepSDRFä¼°è®¡çš„æ²»ç–—æ•ˆæœå¯ä»¥ç”¨äºå¼€å‘æ¨èç®—æ³•ã€‚åœ¨æ¨¡æ‹Ÿç ”ç©¶å’Œå®é™…åŒ»å­¦æ•°æ®åº“ä¸Šçš„æµ‹è¯•ä¸­ï¼ŒDeepSDRFè¡¨ç°å‡ºè‰¯å¥½çš„æ€§èƒ½ã€‚

    

    æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªåœ¨ä¸´åºŠç”Ÿå­˜æ•°æ®è®¾ç½®ä¸­çš„è¿ç»­æ²»ç–—æ¨èé—®é¢˜çš„é€šç”¨å…¬å¼ï¼Œç§°ä¸ºæ·±åº¦ç”Ÿå­˜å‰‚é‡ååº”å‡½æ•°ï¼ˆDeepSDRFï¼‰ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œæˆ‘ä»¬è€ƒè™‘ä»å†å²æ•°æ®ä¸­ä»…ä»…é€šè¿‡è§‚å¯Ÿåˆ°çš„å› ç´ ï¼ˆæ··æ‚å› å­ï¼‰å¯¹è§‚å¯Ÿåˆ°çš„æ²»ç–—å’Œäº‹ä»¶å‘ç”Ÿæ—¶é—´ç»“æœéƒ½æœ‰å½±å“çš„æ¡ä»¶å¹³å‡å‰‚é‡ååº”ï¼ˆCADRï¼‰å‡½æ•°çš„å­¦ä¹ é—®é¢˜ã€‚ä»DeepSDRFä¸­ä¼°è®¡çš„æ²»ç–—æ•ˆæœä½¿æˆ‘ä»¬èƒ½å¤Ÿå¼€å‘å…·æœ‰é€‰æ‹©åå·®æ ¡æ­£çš„æ¨èç®—æ³•ã€‚æˆ‘ä»¬æ¯”è¾ƒäº†åŸºäºéšæœºæœç´¢å’Œå¼ºåŒ–å­¦ä¹ çš„ä¸¤ç§æ¨èæ–¹æ³•ï¼Œå¹¶å‘ç°åœ¨æ‚£è€…ç»“æœæ–¹é¢è¡¨ç°ç›¸ä¼¼ã€‚æˆ‘ä»¬åœ¨å¤§é‡çš„æ¨¡æ‹Ÿç ”ç©¶å’ŒeICUç ”ç©¶æœºæ„ï¼ˆeRIï¼‰æ•°æ®åº“ä¸Šæµ‹è¯•äº†DeepSDRFå’Œç›¸åº”çš„æ¨èå™¨ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯é¦–æ¬¡åœ¨åŒ»å­¦èƒŒæ™¯ä¸‹ä½¿ç”¨å› æœæ¨¡å‹æ¥è§£å†³è§‚å¯Ÿæ•°æ®ä¸­çš„è¿ç»­æ²»ç–—æ•ˆåº”é—®é¢˜ã€‚

    We propose a general formulation for continuous treatment recommendation problems in settings with clinical survival data, which we call the Deep Survival Dose Response Function (DeepSDRF). That is, we consider the problem of learning the conditional average dose response (CADR) function solely from historical data in which observed factors (confounders) affect both observed treatment and time-to-event outcomes. The estimated treatment effect from DeepSDRF enables us to develop recommender algorithms with the correction for selection bias. We compared two recommender approaches based on random search and reinforcement learning and found similar performance in terms of patient outcome. We tested the DeepSDRF and the corresponding recommender on extensive simulation studies and the eICU Research Institute (eRI) database. To the best of our knowledge, this is the first time that causal models are used to address the continuous treatment effect with observational data in a medical context.
    
[^22]: éš”å¼€å¼è¯•éªŒçš„æœ€ä¼˜è®¾è®¡

    Optimal Experimental Design for Staggered Rollouts. (arXiv:1911.03764v5 [econ.EM] UPDATED)

    [http://arxiv.org/abs/1911.03764](http://arxiv.org/abs/1911.03764)

    æœ¬æ–‡ç ”ç©¶äº†éš”å¼€å¼è¯•éªŒçš„æœ€ä¼˜è®¾è®¡é—®é¢˜ã€‚å¯¹äºéè‡ªé€‚åº”å®éªŒï¼Œæå‡ºäº†ä¸€ä¸ªè¿‘ä¼¼æœ€ä¼˜è§£ï¼›å¯¹äºè‡ªé€‚åº”å®éªŒï¼Œæå‡ºäº†ä¸€ç§æ–°ç®—æ³•â€”â€”ç²¾åº¦å¯¼å‘çš„è‡ªé€‚åº”å®éªŒï¼ˆPGAEï¼‰ç®—æ³•ï¼Œå®ƒä½¿ç”¨è´å¶æ–¯å†³ç­–ç†è®ºæ¥æœ€å¤§åŒ–ä¼°è®¡æ²»ç–—æ•ˆæœçš„é¢„æœŸç²¾åº¦ã€‚

    

    æœ¬æ–‡ç ”ç©¶äº†åœ¨ä¸åŒæ—¶æœŸå†…æŸç»„æ•°æ®å•å…ƒçš„æ²»ç–—å¼€å§‹æ—¶é—´å­˜åœ¨å·®å¼‚æ—¶ï¼Œå¯¹å®éªŒè¿›è¡Œè®¾è®¡å’Œåˆ†æçš„é—®é¢˜ã€‚è®¾è®¡é—®é¢˜æ¶‰åŠé€‰æ‹©æ¯ä¸ªæ•°æ®å•å…ƒçš„åˆå§‹æ²»ç–—æ—¶é—´ä»¥ä¾¿æœ€ç²¾ç¡®åœ°ä¼°è®¡æ²»ç–—çš„ç¬æ—¶æ•ˆåº”å’Œç´¯ç§¯æ•ˆåº”ã€‚æˆ‘ä»¬é¦–å…ˆè€ƒè™‘éè‡ªé€‚åº”å®éªŒï¼Œå…¶ä¸­æ‰€æœ‰çš„æ²»ç–—åˆ†é…å†³ç­–éƒ½åœ¨å®éªŒå¼€å§‹ä¹‹å‰åšå‡ºã€‚é’ˆå¯¹è¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬è¯æ˜äº†ä¼˜åŒ–é—®é¢˜é€šå¸¸æ˜¯NPéš¾çš„ï¼Œå¹¶æå‡ºäº†ä¸€ç§è¿‘ä¼¼æœ€ä¼˜è§£ã€‚åœ¨è¯¥è§£å†³æ–¹æ¡ˆä¸‹ï¼Œæ¯ä¸ªæ—¶æœŸè¿›å…¥æ²»ç–—çš„åˆ†æ•°æœ€åˆè¾ƒä½ï¼Œç„¶åå˜é«˜ï¼Œæœ€åå†æ¬¡é™ä½ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬ç ”ç©¶äº†è‡ªé€‚åº”å®éªŒè®¾è®¡é—®é¢˜ï¼Œå…¶ä¸­åœ¨æ”¶é›†æ¯ä¸ªæ—¶æœŸçš„æ•°æ®åæ›´æ–°ç»§ç»­å®éªŒå’Œæ²»ç–—åˆ†é…å†³ç­–ã€‚å¯¹äºè‡ªé€‚åº”æƒ…å†µï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æ–°ç®—æ³•â€”â€”ç²¾åº¦å¯¼å‘çš„è‡ªé€‚åº”å®éªŒï¼ˆPGAEï¼‰ç®—æ³•ï¼Œå®ƒä½¿ç”¨è´å¶æ–¯å†³ç­–ç†è®ºæ¥æœ€å¤§åŒ–ä¼°è®¡æ²»ç–—æ•ˆæœçš„é¢„æœŸç²¾åº¦ã€‚æˆ‘ä»¬è¯æ˜äº†PGAEç®—æ³•è¾¾åˆ°äº†æ‚”æ¨çš„ä¸‹é™ï¼Œæ‚”æ¨å®šä¹‰ä¸ºæœŸæœ›ç´¯è®¡å¹³æ–¹æ ‡å‡†è¯¯å·®å’Œä»»æ„æ²»ç–—åˆ†é…ç­–ç•¥æ‰€èƒ½å®ç°çš„æœ€ä½³è¯¯å·®ä¹‹é—´çš„å·®å¼‚ã€‚

    In this paper, we study the design and analysis of experiments conducted on a set of units over multiple time periods where the starting time of the treatment may vary by unit. The design problem involves selecting an initial treatment time for each unit in order to most precisely estimate both the instantaneous and cumulative effects of the treatment. We first consider non-adaptive experiments, where all treatment assignment decisions are made prior to the start of the experiment. For this case, we show that the optimization problem is generally NP-hard, and we propose a near-optimal solution. Under this solution, the fraction entering treatment each period is initially low, then high, and finally low again. Next, we study an adaptive experimental design problem, where both the decision to continue the experiment and treatment assignment decisions are updated after each period's data is collected. For the adaptive case, we propose a new algorithm, the Precision-Guided Adaptive Experim
    

